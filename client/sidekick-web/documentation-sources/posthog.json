[
  {
    "title": "feature-flag-use-cases.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/feature-flag-use-cases.md",
    "content": "\ntitle: Feature Flag Use Cases\nsidebar: Docs\nshowTitle: true\n\nFeature flags are a very powerful piece of functionality that can be used in a wide variety of ways. How you use them will depend on your particular painpoints and internal best practices. \nFor us, here are some suggestions of use cases that could fit nicely with feature flags:\nA/B Testing\nIn simple terms, A/B testing is a method for determining how to provide the best user experience or meet other product goals by testing how different features perform. \nThis could be used to answer questions such as \"Do users click a button more often if it is blue or red?\" or more complex questions like \"How much more time do active users who have signed up for a free trial spend on our blog pages if we add a banner image?\".\nA great way to do this is by using Cohorts. If you filter your flags by cohort, you can then easily see the differences in behavior across different cohorts. \nHere's an example view of Trends in PostHog filtering pageview events that contain the term \"blog\" in the URL, showing a breakdown between Cohort A (Beta Feature On) and Cohort B (Beta Feature Off):\n\nGradual Rollouts\nThere are many occasions when you might want to roll out a feature to your users slowly. Maybe you only want to enable it for Beta users, or you simply want to give users a transition period.\nWhatever the case may be, feature flags let you easily roll out features in an incremental way, increasing the portion of users that have the feature as fast or slow as you wish. \n\"Master Toggles\"\nIf you have to ship a big piece of functionality, chances are that you'll be doing it across multiple PRs.\nAs such, rather than attempt to coordinate a merge spree to ensure everything is live at once, you can create a feature flag that wraps all the new logic in all the pull requests. Then, once everything is merged and ready to go, you can simply flip the switch to release it.\nBetter yet, you can then release it slowly to make sure nothing breaks, and, if it does, you can easily turn it off with one click.\nAnd this brings us to the next example.\nKill Switches\nYou don't need feature flags per se to implement kill switches, but having the ability to immediately turn a flag off is a nice add-on to the functionality.",
    "tag": "posthog"
  },
  {
    "title": "Features",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/product-direction.md",
    "content": "\ntitle: Product direction\nsidebar: Docs\nshowTitle: true\n\nFeatures\nTo see what we\u2019re working on, and what the software does, visit the feature list.\nLicensing\nFree edition (Open source \u2013 MIT)\nThis is designed for individual or smaller teams of developers to use for non-commercial or smaller commercial projects.\nThe repo can be found at the PostHog GitHub.\nGrowth edition (PostHog Growth license required)\nThis is designed for companies needing more sophisticated analytics such as ab testing or more scalable databases for high traffic.\nUltimate edition (PostHog Ultimate license required)\nThis is designed for companies with enterprise needs \u2013 SSO, complex permissions, etc. We are seeking beta customers for this.\nWhy we are open core?\nPostHog is founded on a desire to protect user privacy and to be the developer-friendly product analytics choice.\nProviding an open source offering \u2013 our Free edition \u2013 means that PostHog is as friendly as possible for developers to try out.  If you install it, it\u2019s yours \u2013 forever. Please give us lots of feedback, and we\u2019d love to have you contribute.\nWe have chosen not to be open source for our Growth and Ultimate editions. Doing so would only leave us with donations or providing a hosted service as ways to support the team. Donations would be unlikely to raise enough money, and providing a hosted services as our main offering doesn\u2019t fit what we do \u2013 since we provide self-hosted analytics, it feels illogical to only be able to offer to host the software for you.\nTelemetry\nPostHog tracks some usage of its own products. You can opt out of any telemetry at any time, and much is opt in by default.\nFree edition\nWe ask users if they wish to opt in to:\n\nOur Slack channel where we have a community\nEmail newsletter\nEmail security updates\n\nWe provide opt-out tracking on some basic high-level information on the product:\n\nAggregated usage data. We do not track any individual-user information.\nThe entity who initially uses the software:\nEmail address if provided\nCompany name if provided\nThe host\n\nWe do not connect entity information with usage data.\nNone of this data is passed to 3rd parties.\nThe reason we track these items is so we can understand how to improve our products. Aggregated usage data lets us work out which parts of the product are popular, where users get stuck, and what people ignore. Entity data helps us work out which features to build next \u2013 if we see people trying it out in similar industries, we can understand how to meet their use cases better.\nGrowth and Ultimate editions\nWe track product usage with our own product \u2013  the same you will be using \u2013 by default. However, we can opt you out \u2013 if you are an existing or prospective customer, talk to us about it.",
    "tag": "posthog"
  },
  {
    "title": "Story",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/posthog-direction.md",
    "content": "\ntitle: PostHog Direction\nsidebar: Docs\nshowTitle: true\n\nStory\nJames started as a developer, then used to run a multi-million dollar monthly online marketing budget. Tim ran R&D, has deep development experience, and used to work with James. Tim can build stuff fast.\nJames and Tim decided to start a startup, so put together an initial product focused on technical debt. In doing this, they wanted a way to understand who was using the product, which parts of the product were being used, and to be able to understand where all these individual users came from in the first place. This would help improve retention and it\u2019d reduce inefficient marketing spend.\nJames got frustrated at having to repeatedly add track(\u2018event\u2019) to every element, especially when the product and funnel was changing so frequently.\nTim felt it was wrong to send so much personally-identifiable event data to third parties, yet knew it was necessary to grow the business.\nIn the process of building their own product\u2019s tracking system, James and Tim fell in love with the concept of self-hosted product analytics, that you could deploy in one line, to capture every event automatically. They moved to San Francisco, raised some money, were joined by Aaron \u2013 to help run our marketing \u2013 and PostHog was born.\nWhy the name?\nThe \u2018Post\u2019 in PostHog comes from the concept of ex post facto laws \u2013 laws that are applied retroactively. The parallel is by auto-capturing event data, we can help you draw meanings after your users are long gone.",
    "tag": "posthog"
  },
  {
    "title": "disclaimer.mdx",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/self-host/open-source/disclaimer.mdx",
    "content": "\ntitle: Disclaimer for open-source self-hosted PostHog\nsidebarTitle: Disclaimer\nsidebar: Docs\nshowTitle: true\n\nSelf-hosted open-source deployment is made for hobbyists or hosting PostHog in weird and wonderful ways (like in your basement).\nIt's MIT licensed and provided without guarantee. You should be confident in your security knowledge to run it. It is unlikely to handle >100k events/month with a high risk of data loss. As a small team we can only provide limited support for open-source PostHog.\nFor most companies we'd recommend PostHog Cloud.",
    "tag": "posthog"
  },
  {
    "title": "Support for open-source deployments of PostHog",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/self-host/open-source/support.mdx",
    "content": "\ntitle: Open-source self-hosted support\nsidebar: Docs\nsidebarTitle: Support\nshowTitle: true\n\nimport Disclaimer from '../_snippets/disclaimer.mdx'\n\nSupport for open-source deployments of PostHog\nPostHog at production scale is a data-intensive application with a complex system architecture.\nFrequently, errors with self-hosted PostHog are due to incorrect configurations or management.\nUnfortunately, as a small team, we don't have the bandwidth for troubleshooting instance-specific issues for open-source users of PostHog. However, if you identify an issue with the open-source deployment PostHog please create a GitHub issue on the repo including the steps to reproduce, open a Pull Request to fix it, or visit the documentation.\nFor troubleshooting your own problems the troubleshooting guide and runbooks will likely be useful.\nIf you aren't able to fix the problem yourself here are some other options:\n\nMigrate from the self-hosted PostHog to PostHog Cloud\nGuide on how to migrate here\nIf you have any problems migrating feel free to message us in Slack or send us an email at hey@posthog.com and we will be happy to help\n\n\n3rd party support\nReach out to a relevant PostHog partner. Several customers have had a good experience with Altinity for Clickhouse and Kubernetes issues\nWork with a software engineering contractor\n\n\nUpgrade to PostHog Enterprise\n",
    "tag": "posthog"
  },
  {
    "title": "Requirements",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/self-host/open-source/deployment.mdx",
    "content": "\ntitle: Self-hosted open source PostHog\nsidebarTitle: Deployment\nsidebar: Docs\nshowTitle: true\n\nimport Disclaimer from '../_snippets/disclaimer.mdx'\n\nRequirements\n\nYou have deployed a Linux Ubuntu Virtual Machine.\nAn instance with 2GB of RAM can handle approximately 100k events spread over a month\nWe highly recommend an instance with at least 4GB of RAM to handle any surges in event volume\n\n\nYou have set up an `A` record to connect a custom domain to your instance.\nPostHog will automatically create an SSL certificate for your domain using LetsEncrypt\n\n\n\nNew deployments of PostHog open source using Kubernetes are no longer supported.\nConfiguration\nThere are various ways to configure and personalize your PostHog instance to better suit your needs. In this section you will find all the information you need about settings and options you can configure to get what you need out of PostHog.\n\nEnvironment variables\nUpgrading PostHog\nSecuring PostHog\nRunning behind proxy\nEmail configuration\n\nSetting up the stack\nTo get started, all we need to do is run the following command, which will spin up a fresh PostHog deployment for us automatically!\n`bash\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/posthog/posthog/HEAD/bin/deploy-hobby)\"`\nYou'll now be asked to provide the release tag you would like to use, as well as the domain you have connected to your instance.\nOnce everything has been setup, you should see the following message:\n```\nWe will need to wait ~5-10 minutes for things to settle down, migrations to finish, and TLS certs to be issued\n\u23f3 Waiting for PostHog web to boot (this will take a few minutes)\n```\nPostHog will wait here on a couple of tasks that need to be completed, which should only take a couple minutes.\nOnce this is complete, you should be able to see your PostHog dashboard on the domain you provided!\n\nIf you notice this step taking longer than 10 minutes, it's best to cancel it with `Ctrl+C` and take a look at the troubleshooting section.\n\nCustomizing your deployment (optional)\nBy default, the `docker-compose.yml` file that gets run comes with a series of default config values that should work for most deployments.\nIf you need to customize anything, you can take a look at the full list of environment variables.\nAfter making any changes, simply restart the stack with `docker-compose`.\nAdditionally, if you would like to run a different version of PostHog, you can change the tag for the web, worker, and plugins services.\nCheck out here for a list of all available tags.\nTroubleshooting\nIf you have already run the one-step deployment command above and something went wrong, this section covers a number of steps you can take to debug issues.\nChecking that all containers are running\nWe can use `docker ps` to check that all of our services are running.\n```bash\n$ docker ps\nCONTAINER ID   IMAGE                               COMMAND     CREATED    STATUS    PORTS   NAMES\n21a2f62d6e50   posthog/posthog:release-1.39.1      ...         1m ago     Up 1m     ...     ...\n77face12d3e2   posthog/posthog:release-1.39.1      ...         1m ago     Up 1m     ...     ...\n3b4bc7394049   posthog/posthog:release-1.39.1      ...         1m ago     Up 1m     ...     ...\n03f393c7aa84   caddy:2.6.1                         ...         1m ago     Up 1m     ...     ...\nf1060c3d8d73   clickhouse/clickhouse-server:22.3   ...         1m ago     Up 1m     ...     ...\n7d2353a6bddf   bitnami/kafka:2.8.1-debian-10-r99   ...         1m ago     Up 1m     ...     ...\n72051397040e   zookeeper:3.7.0                     ...         1m ago     Up 1m     ...     ...\nff42ccf14481   redis:6.2.7-alpine                  ...         1m ago     Up 1m     ...     ...\n402a0eef69ae   postgres:12-alpine                  ...         1m ago     Up 1m     ...     ...\nda0d115dd02e   minio/minio                         ...         1m ago     Up 1m     ...     ...\n```\nYou should see all the same containers as above. If any containers aren't showing up or show that they've restarted recently, it's worth checking their logs to see what the issue is.\nChecking the logs of each container\nWe can use the following command to check the logs for each of our containers.\n`bash\ndocker logs <container_name>`\nThe best place to start looking is in the `web` container, which runs all the database migrations and will produce an error if any have failed.\n\nRunning into issues with deployment? Ask a question here or check out our Slack to get help.\n\nUpgrading\nTo upgrade, you can run the `upgrade-hobby` script from the PostHog repo.\n`bash\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/posthog/posthog/HEAD/bin/upgrade-hobby)\"`\n\nWarning: Before upgrading, make sure you have created back-ups of all your data!\n\nMigrating",
    "tag": "posthog"
  },
  {
    "title": "General configuration",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/self-host/configure/email.md",
    "content": "\ntitle: Configuring email\nsidebar: Docs\nshowTitle: true\n\nPostHog's core relies on email messaging for certain functionality. For example:\n- Sending a reset link to a user that has forgotten their password.\n- Sending an invite link for new team members to join PostHog.\nBy default, PostHog will not send email messages until you enable an SMTP server. We very strongly recommend using an email service to act as email server (see examples below for most common providers). These providers are optimized to maximize email deliverability. \nTo prevent spam, most email providers have very complex systems in place that validate a myriad of factors before allowing an email through. Optimizing local servers for this is a bit like reinventing the wheel, so avoid this unless you have a very strong reason not to.\nGeneral configuration\nTo configure a remote email server, you will need to head over to Instance settings and set up the following parameters (please note that if you're running PostHog older than 1.33.0, you will need to configure these parameters via environment variables):\n- `EMAIL_HOST`: Defaults to `None`. Hostname to connect to for establishing SMTP connections.\n- `EMAIL_PORT`: Defaults to `25`. Port that should be used to connect to the host.\n- `EMAIL_HOST_USER`: Defaults to `null`. Credentials to connect to the host.\n- `EMAIL_HOST_PASSWORD`: Defaults to `null`. Credentials to connect to the host.\n- `EMAIL_USE_TLS`: Defaults to `false`. Whether to use TLS protocol when connecting to the host.\n- `EMAIL_USE_SSL`: Defaults to `false`. Whether to use SSL protocol when connecting to the host. If you're using AWS SES you should use this instead of TLS.\n- `EMAIL_DEFAULT_FROM`: Defaults to `root@localhost`. Email address that will appear as the sender in emails (`From` header).\n- `EMAIL_ENABLED`: Defaults to `true`. Whether email service is enabled or not.\n- `SITE_URL`: Defaults to `http://localhost:8000`. Not unique to the email service, but it needs to be set for emails to work properly. Principal URL of your PostHog instance.\nSample configuration:\n`yaml\nEMAIL_HOST: smtp.example.com\nEMAIL_PORT: 587\nEMAIL_HOST_USER: postmaster@example.com\nEMAIL_HOST_PASSWORD: password\nEMAIL_USE_TLS: false\nEMAIL_USE_SSL: true\nEMAIL_DEFAULT_FROM: no-reply@example.com\nSITE_URL: https://posthog.example.com`\nThe above sample configuration can also be set in your `values.yaml` this way, however configuring your email service with Instance settings instead is highly recommended (this is because environment variables are only read when there are no values set from the UI/API, i.e. the values set from the UI/API take precedence, as such setting them here can help you ensure you have a single source of truth):\n`yaml\nemail:\n  from_email: no-reply@example.com\n  host: smtp.example.com\n  port: 587\n  user: postmaster@example.com\n  password: password\n  use_tls: false\n  use_ssl: true`\nBelow you will find details on how to configure the most common email providers (not in any particular order). \nTwilio's Sendgrid\nWith Sendgrid you have 2 different configuration options. \nOption A: Domain authentication (Recommended)\nDomain authentication allows you to send emails from any address within your validated domain. It is the best option to guarantee email deliverability because it establishes DNS records on your domain that validate your identity.\n\n\nOn sender authentication, select the option to authenticate a domain, you can also go directly to https://app.sendgrid.com/settings/sender_auth/domain/create.\n\n\nFill out the required details for the domain you wish to configure. We recommend using the default configuration. If you do not use the advanced settings option please be sure to properly configure DKIM and SPF records to ensure deliverability.\n\n\nYou will receive now a list of DNS records that need to be added to your domain. After adding them, be sure to verify them on Sendgrid. You are now ready to start sending emails.\n\n\nOption B: Single sender authentication\nAs an alternative you can do single sender verification which is the easiest option to configure. You will only need to be able to receive emails on the address you want to use as sender. Please note this method is only recommended as a starting point, or for small scale usage.\n\n\nOn sender authentication, select the option to create a new sender profile, you can also go directly to https://app.sendgrid.com/settings/sender_auth/senders/new.\n\n\nFill out the form with the required details, see an example below.\n\n\n\n\nValidate the email address by clicking on the link you will receive.\n\nAfter you have set up your sending configuration, you can continue below to set up your credentials and configure to send emails with Sendgrid.\n\n\nTo create the required credentials, go to Settings > API keys and click on \"Create API key\".\n\n\nSet a name for your API key, we recommend using \"PostHog\", and select the \"Restricted Key\" option. You will need to enable the \"Mail Send\" permission as detailed below. Copy the key directly to your instance configuration.\n\n\n\n\n\nWith the key you created above, you can now set your instance configuration in PostHog:\n    `yaml\n    EMAIL_HOST: smtp.sendgrid.net\n    EMAIL_PORT: 587\n    EMAIL_HOST_USER: apikey # same for everyone\n    EMAIL_HOST_PASSWORD: SG.rqHsfjxZPiqE5lqXTgQ_lz7x7IVLv # obtained from step above\n    EMAIL_USE_TLS: true\n    EMAIL_USE_SSL: false\n    EMAIL_DEFAULT_FROM: hey@example.com # you can define this, just use your domain or single sender address\n    SITE_URL: https://posthog.example.com # this is the URL of your instance`\n\n\nTest your integration! Whenever updating email settings, we will attempt to send you a test email so you can assert everything is working as expected. Please note that if you update these settings via environment variables, you need to restart your server and test sending an email (e.g. request a password reset).\n    > Remember that you will need to restart both your web server and background worker for this to work properly.\n\n\nAs an additional optional step, we recommend turning off 'open & click tracking' to avoid having weird-looking links and increase deliverability (there's little value in having this data). You can do so by going to tracking settings.\n\n\n\nMailgun\n\nAfter you have created an account, go to Sending > Domains, and click on \"Add New Domain\".\n\nEnter a domain name that you own. Using a subdomain is recommended (e.g. `m.posthog.com` instead of `posthog.com`). We strongly recommend selecting \"Create DKIM Authority\" (and using 2048 bits) to prevent spoofing with your domains (read more about DKIM). See sample configuration below:\n\n\n\nYou will now be given instructions to set up certain DNS records in your domain. Please be sure to add all requested records to ensure proper email deliverability. If not provided, we also recommend adding the following SPF record to prevent email forgery with your domain.\n`TXT @ v=spf1 include:mailgun.org ~all`\n\n\nOnce you have added all records and verify them you can go to the domain settings > \"SMTP credentials\" section. You then need to create a set of SMTP credentials.\n\n\nWith the SMTP credentials, you can now set the required settings for email to work properly. You will also need to obtain the hostname from the credentials page. Your configuration should now look something like this.\n    `yaml\n    EMAIL_HOST: smtp.eu.mailgun.org # obtained from credentials page\n    EMAIL_PORT: 587\n    EMAIL_HOST_USER: postmaster@m.example.com # obtained from credentials page\n    EMAIL_HOST_PASSWORD: password # obtained from credentials page\n    EMAIL_USE_TLS: true\n    EMAIL_USE_SSL: false\n    EMAIL_DEFAULT_FROM: hey@example.com # you can define this, just use your domain\n    SITE_URL: https://posthog.example.com # this is the URL of your instance`\n\n\nTest your integration! Whenever updating email settings, we will attempt to send you a test email so you can assert everything is working as expected. Please note that if you update these settings via environment variables, you need to restart your server and test sending an email (e.g. request a password reset).\n    > Remember that you will need to restart both your web server and background worker for this to work properly.\n\n",
    "tag": "posthog"
  },
  {
    "title": "Instance settings",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/self-host/configure/environment-variables.md",
    "content": "\ntitle: Environment variables\nsidebar: Docs\nshowTitle: true\nhideAnchor: true\n\nimport Sunset from \"../_snippets/sunset-disclaimer.mdx\"\n\nAs of PostHog 1.33.0 some settings can now be managed directly in the app, without having to connect to your cluster and/or redeploy. If you are on version 1.33.0 or newer, please review Instance settings first.\nFor settings that can be managed with Instance Settings, you can either set the values via environment variables or through the Instance Settings page in your instance. However, it is strongly recommended to use Instance settings. Environment variables are only loaded when there are no values in Instance settings, which means that environment variables could reflect outdated values.\nFor other settings, there are various environment variables you can set to configure your instance. Below is a comprehensive list of all of them. However, for general use, you most likely do not have to worry about the vast majority of these.\nRows with a missing 'Default Value' usually default to an empty string. This is different from `None`.\nSome variables here are default Django variables. This Django Docs page has more information about them.\n| Variable                                                                                                 | Description                                                                                                                                                                                                                                         | Default Value                                                                                                 |\n| :------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------ |\n| `SECRET_KEY`                                                                                             | \u2757\ufe0f Always required. Used by Django for cryptography. Helps secure cookies, sessions, hashes, etc. Custom value required in production.                                       | `<randomly generated secret key>`                                                                             |\n| `SITE_URL` - should be an absolute URL and include the protocol (e.g. `https://posthog.your-domain.com`) | \u2757\ufe0f Always required. Principal/canonical URL of your PostHog instance. Needed for emails, webhooks and SSO to work properly. We currently do not support subpaths in this URL.                                                                  | `http://localhost:8000`                                                                                       |\n| `SECURE_COOKIES`                                                                                         | Determines if Django should use secure cookies. Insecure cookies do not work without HTTPS.                                                                            | `False` if PostHog is running in DEBUG or TEST mode, else `True`                                              |\n| `SENTRY_DSN`                                                                                             | Used to integrate with Sentry error and event tracking. Ignored when running tests.                                                                                                                                   | `None`                                                                                                        |\n| `IS_BEHIND_PROXY`                                                                                        | Specifies if PostHog is running behind a proxy like Apache, NGINX or ELB. Be sure to properly set trusted proxies.                                                                | `False`                                                                                                       |\n| `ALLOWED_IP_BLOCKS`                                                                                      | Specifies IP blocks allowed to connect to the PostHog instance for management (events will still be allowed from anywhere). Make sure to properly configure your proxy if running behind a proxy. | Empty                                                                                                       |\n| `TRUSTED_PROXIES`                                                                                        | Specifies the IPs of proxies that can be trusted.                                                                                                                                                                                                   | `None`                                                                                                        |\n| `TRUST_ALL_PROXIES`                                                                                      | Determines if all proxies can be trusted.                                                                                                                                                                                                           | `False`                                                                                                       |\n| `ALLOWED_HOSTS`                                                                                          | A list of strings representing the host/domain names that Django can serve. More info.                                                                                         | `*` (all)                                                                                                     |\n| `SKIP_SERVICE_VERSION_REQUIREMENTS`                                                                      | Set this to True if you want to disable checking for dependent service version requirements.                                                                                                                                                        | `False`                                                                                                       |\n| `ACTION_EVENT_MAPPING_INTERVAL_SECONDS`                                                                  | Specify how often (in seconds) PostHog should run a job to match events to actions.                                                                                                                                                                 | `300`                                                                                                         |\n| `ASYNC_EVENT_ACTION_MAPPING`                                                                             | If set to `False`, actions will be matched to events as they come. Otherwise, the matching will happen in batches through a periodic Celery task. Should only be toggled on by high load instances.                                                 | `False`                                                                                                       |\n| `CAPTURE_INTERNAL_METRICS`                                                                               | Send some internal instrumentation to your own posthog instance, exposed via `/instance/status` page. For EE only.                                                                                                                                  | `False`                                                                                                       |\n| `DATABASE_URL`                                                                                           | Database URL pointing to your PostgreSQL instance.                                                                                                                                        | `postgres://localhost:5432/posthog` if PostHog is running in DEBUG or TEST mode, must be specified otherwise. |\n| `DEBUG_QUERIES`                                                                                          | Whether debugging queries (ClickHouse) is enabled in the Command Palette.                                                                                                                                                                           | `False`                                                                                                       |\n| `DEBUG`                                                                                                  | Determines if PostHog should run in DEBUG mode. You can set this to a truthy value when developing, but disable this in production!                                        | `False`                                                                                                       |\n| `CLICKHOUSE_DISABLE_EXTERNAL_SCHEMAS`                                                                    | If set, disables using ProtoBuf schemas for kafka communication. Needs to be set when using an external ClickHouse service provider during initial deploy.                                                                                          | `False`                                                                                                       |\n| `DISABLE_PAID_FEATURE_SHOWCASING`                                                                        | Whether any showcasing of a paid feature should be disabled. Useful if running a free open source version of PostHog and are not interested in premium functionality.                                                                               | `False`                                                                                                       |\n| `DISABLE_SECURE_SSL_REDIRECT`                                                                            | Disables automatic redirect from port 80 (HTTP) to port 443 (HTTPS).                                                                                                                                                                                | `False`                                                                                                       |\n| `GITHUB_TOKEN`                                                                                           | GitHub personal access token, used to prevent rate limiting when using apps and to allow installation of apps from private repos                                                                                                                    | `None`                                                                                                        |\n| `GITLAB_TOKEN`                                                                                           | GitLab personal access token, used to prevent rate limiting when using apps and to allow installation of apps from private repos                                                                                                                    | `None`                                                                                                        |\n| `JS_URL`                                                                                                 | URL used by Webpack for loading external resources like images and files.                                                                                                                                                                           | `http://localhost:8234` if PostHog is running in DEBUG mode, must be specified otherwise.                     |\n| `KAFKA_URL`                                                                                              | Address used by the application to contact kafka                                                                                                                                                                                                    | `kafka://kafka`                                                                                               |\n| `KAFKA_URL_FOR_CLICKHOUSE`                                                                               | Address used by ClickHouse to read from kafka. Falls back to `KAFKA_URL`                                                                                                                                                                            | `None`                                                                                                        |\n| `MATERIALIZE_COLUMNS_ANALYSIS_PERIOD_HOURS`                                                              | Diagnostic for what columns to materialize                                                                                                                                                                                                          | `168`                                                                                                         |\n| `MATERIALIZE_COLUMNS_BACKFILL_PERIOD_DAYS`                                                               | How far back backfill materialized columns                                                                                                                                                                                                          | `90`                                                                                                          |\n| `MATERIALIZE_COLUMNS_MAX_AT_ONCE`                                                                        | How many columns to materialize at once                                                                                                                                                                                                             | `10`                                                                                                          |\n| `MATERIALIZE_COLUMNS_MINIMUM_QUERY_TIME`                                                                 | Diagnostic for what columns to materialize                                                                                                                                                                                                          | `3000`                                                                                                        |\n| `MATERIALIZE_COLUMNS_SCHEDULE_CRON`                                                                      | How frequently to run clickhouse column materialization.                                                                                                                                                                                            | `0 5 * * SAT`                                                                                                 |\n| `MULTI_ORG_ENABLED`                                                                                      | Allows creating multiple organizations in your instance (multi-tenancy). Requires a premium license.                                                                                                                                            | `False`                                                                                                       |\n| `NPM_TOKEN`                                                                                              | Access token for npm, used to allow installation of apps released as a private npm package                                                                                                            | `None`                                                                                                        |\n| `OPT_OUT_CAPTURING`                                                                                      | Disable sending product usage data to PostHog.                                                                                                                                                                                                      | `False`                                                                                                       |\n| `POSTHOG_DB_NAME`                                                                                        | Database name.                                                                                                                                                                                                                                      | Must be specified when `DATABASE_URL` is not set.                                                             |\n| `POSTHOG_DB_PASSWORD`                                                                                    | Database password.                                                                                                                                                                                                                                  | `\"\"` if PostHog is running in DEBUG or TEST mode. Must be specified when `DATABASE_URL` is not set.           |\n| `POSTHOG_DB_USER`                                                                                        | Database username.                                                                                                                                                                                                                                  | `postgres` if PostHog is running in DEBUG or TEST mode. Must be specified when `DATABASE_URL` is not set.     |\n| `POSTHOG_POSTGRES_CLI_SSL_CA`                                                                            | Location of the SSL root certificate file for PostgreSQL. More info.                                                                                                                        | `None`                                                                                                        |\n| `POSTHOG_POSTGRES_CLI_SSL_CRT`                                                                           | Location of the SSL certificate file for PostgreSQL. More info.                                                                                                                             | `None`                                                                                                        |\n| `POSTHOG_POSTGRES_CLI_SSL_KEY`                                                                           | Location of the SSL key file for PostgreSQL. More info.                                                                                                                                     | `None`                                                                                                        |\n| `POSTHOG_POSTGRES_HOST`                                                                                  | Host pointing to your PostgreSQL instance.                                                                                                                                                                                                          | `localhost` if PostHog is running in DEBUG or TEST mode. Must be specified when `DATABASE_URL` is not set.    |\n| `POSTHOG_POSTGRES_PORT`                                                                                  | Port pointing to your PostgreSQL instance.                                                                                                                                                                                                          | `5432` if PostHog is running in DEBUG or TEST mode. Must be specified when `DATABASE_URL` is not set.         |\n| `POSTHOG_POSTGRES_SSL_MODE`                                                                              | PostgreSQL SSL mode. More info.                                                                                                                                         | `None`                                                                                                        |\n| `REDIS_URL`                                                                                              | Redis URL pointing to your Redis instance.                                                                                                                              | `redis://localhost/` if PostHog is running in DEBUG or TEST mode, must be specified otherwise.                |\n| `SOCIAL_AUTH_GITHUB_KEY`                                                                                 | GitHub key for allowing sign up with GitHub.                                                                                                                                                                        | Empty                                                                                                       |\n| `SOCIAL_AUTH_GITHUB_SECRET`                                                                              | GitHub secret for allowing sign up with GitHub.                                                                                                                                                                     | Empty                                                                                                       |\n| `SOCIAL_AUTH_GITLAB_API_URL`                                                                             | Endpoint to be used for GitLab authentication. Changing this is only relevant for self-host GitLab users.                                                                                                           | `https://gitlab.com`                                                                                          |\n| `SOCIAL_AUTH_GITLAB_KEY`                                                                                 | GitLab key for allowing sign up with GitLab.                                                                                                                                                                        | Empty                                                                                                       |\n| `SOCIAL_AUTH_GITLAB_SECRET`                                                                              | GitLab secret for allowing sign up with GitLab.                                                                                                                                                                     | Empty                                                                                                       |\n| `SOCIAL_AUTH_GOOGLE_OAUTH2_KEY`                                                                          | Google client ID for allowing SSO with Google.                                                                                                                                                                      | Empty                                                                                                       |\n| `SOCIAL_AUTH_GOOGLE_OAUTH2_SECRET`                                                                       | Google client secret for allowing SSO with Google.                                                                                                                                                                  | Empty                                                                                                       |\n| `STATSD_HOST`                                                                                            | Host of a running StatsD daemon (e.g. 127.0.0.1)                                                                                                                                                                                                    | `None`                                                                                                        |\n| `STATSD_PORT`                                                                                            | Port for the running StatsD daemon                                                                                                                                                                                                                  | `8125`                                                                                                        |\n| `STATSD_PREFIX`                                                                                          | Prefix to be prepended to all stats used by StatsD. Useful for distinguishing environments using the same server.                                                                                                                                   | Empty                                                                                                       |\n| `CLEAR_CLICKHOUSE_REMOVED_DATA_SCHEDULE_CRON`                                                            | When data is (asynchronously) deleted from the events table                                                                                                                                                                                         | `0 5 * * SAT`                                                                                                 |\nInstance settings\nThe following settings should mainly be managed with Instance settings. However, if you can still set them via environment variables if you prefer. Please be mindful that if these settings are overridden in the settings page, the overridden values will prevail.\n| Variable                   | Description                                 | Default Value                               | Managed with Instance Settings |\n| :------------------------- | :------------------------------------------ | :------------------------------------------ | :----------------------------- |\n| `EMAIL_DEFAULT_FROM`       | Please see configuring email for details. | Please see configuring email for details. | \u2705 Yes                         |\n| `EMAIL_ENABLED`            | Please see configuring email for details. | Please see configuring email for details. | \u2705 Yes                         |\n| `EMAIL_HOST_PASSWORD`      | Please see configuring email for details. | Please see configuring email for details. | \u2705 Yes                         |\n| `EMAIL_HOST_USER`          | Please see configuring email for details. | Please see configuring email for details. | \u2705 Yes                         |\n| `EMAIL_HOST`               | Please see configuring email for details. | Please see configuring email for details. | \u2705 Yes                         |\n| `EMAIL_PORT`               | Please see configuring email for details. | Please see configuring email for details. | \u2705 Yes                         |\n| `EMAIL_USE_TLS`            | Please see configuring email for details. | Please see configuring email for details. | \u2705 Yes                         |\n| `EMAIL_USE_TLS`            | Please see configuring email for details. | Please see configuring email for details. | \u2705 Yes                         |\n| `SLACK_APP_CLIENT_ID`      | Please see [configuring slack] for details. | Please see [configuring slack] for details. | \u2705 Yes                         |\n| `SLACK_APP_CLIENT_SECRET`  | Please see [configuring slack] for details. | Please see [configuring slack] for details. | \u2705 Yes                         |\n| `SLACK_APP_SIGNING_SECRET` | Please see [configuring slack] for details. | Please see [configuring slack] for details. | \u2705 Yes                         |",
    "tag": "posthog"
  },
  {
    "title": "General configuration",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/self-host/configure/slack.md",
    "content": "\ntitle: Configuring Slack\nsidebar: Docs\nshowTitle: true\n\n\nNote - these instructions are for self-hosted PostHog instances only. If you are using PostHog Cloud or have already configured your instance per these instructions, check out our general Slack Integration Docs\n\nPostHog has built in support for sending Slack notifications via Actions or Subscriptions. For self-hosted PostHog instances, a Slack App is required so that Teams can add PostHog to their Slack workspace via the standard Oauth flows.\nGeneral configuration\n\n\ud83d\udea7 In addition to the Slack App configuration it is important that the instance `SITE_URL` is correctly set. This is used by Slack's Oauth flow to ensure that only your instance uses the Slack App.\n\nCreating a Slack App and configuring PostHog to use it is relatively straightforward thanks to Slack's App Template functionality. Visit the Slack section of your instance's Project Settings `https://MY_POSTHOG.DOMAIN/project/settings#slack` and follow the pre-templated instructions to setup your instance for Slack.",
    "tag": "posthog"
  },
  {
    "title": "Setup",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/self-host/configure/running-behind-proxy.md",
    "content": "\ntitle: Running behind a proxy\nsidebar: Docs\nshowTitle: true\n\nIf you're running PostHog behind a proxy, there are a few more things you need to do to make sure PostHog works. You usually need this if running behind a web server like Apache or NGINX, a load balancer (like ELB), or a DDoS protection service (like Cloudflare).\nSetup\nIf PostHog is running behind a proxy, you need to do the following:\n\nSet the `IS_BEHIND_PROXY` environment variable to `True`. This will make sure the client's IP address is properly calculated, and SSL is properly handled (e.g. for OAuth requests).\nSet your trusted proxies configuration.\nDepending on your setup, you might also need to set the `ALLOWED_HOSTS` environment variable. If you don't allow all hosts (i.e. you are whitelisting specific hosts), you will need to set the address(es) of your proxy here.\n\nNote: It is suggested to set up the proxy separately from PostHog's Docker Compose definition.\nTrusted proxies\nTrusted proxies are used to determine which proxies to consider as valid from the `X-Forwarded-For` HTTP header included in all requests to determine the end user's real IP address. Specifically whitelisting your proxy server's address prevents spoofing of the end user's IP address while ensuring your service works as expected. There are two ways of setting up trusted proxies.\n\nRecommended. Set a list of trusted IP addresses for your proxies via the `TRUSTED_PROXIES` environment variable (comma-separated list of IP addresses).\nTrust all proxies by setting `TRUST_ALL_PROXIES` environment variable to `True` (not recommended unless you have a strong reason for which whitelisting specific addresses wouldn't work for you).\n\nCommon issues\n\nSome users have reported getting infinite redirects when running behind a proxy. Make sure the `X-Forwarded-Proto` header is set to `https` if you have HTTPS enabled. Alternatively, you can set the `DISABLE_SECURE_SSL_REDIRECT` variable to make PostHog run using HTTP.\nIf you use a load balancer, it is recommended to terminate the SSL connection at the load balancer (remember to set `DISABLE_SECURE_SSL_REDIRECT` to `True`) and connect via HTTP to your PostHog container (make sure your container is behind a firewall or VPC to prevent unauthorized connections), you would then enforce SSL/TLS connections at the load balancer level.\n\n\nIf you have IP blocks that are not working and you're running behind a proxy, your instance may be misconfigured, preventing PostHog from determining the connecting IP address.\n\nPublic endpoints\nIf you're setting up a proxy to protect your PostHog instance and prevent access only through an authorized connection, you should consider there are some endpoints that must always be publicly accessible in order for event ingestion, session recording and feature flags to work properly. These endpoints are listed below.\n| Path               | Description                                                                                  |\n| ------------------ | -------------------------------------------------------------------------------------------- |\n| `/batch`           | Endpoint for ingesting/capturing events.                                                     |\n| `/decide`          | Endpoint that enables autocapture, session recording, feature flags & compression on events. |\n| `/capture`         | Endpoint for ingesting/capturing events.                                                     |\n| `/e`               | Endpoint for ingesting/capturing events.                                                     |\n| `/engage`          | Endpoint for ingesting/capturing events.                                                     |\n| `/s`               | Endpoint for capturing session recordings.                                                   |\n| `/static/array.js` | Frontend javascript code that loads `posthog-js`.                                            |\n| `/track`           | Endpoint for ingesting/capturing events.                                                     |\nNGINX config (Recommended)\nYou need to make sure your proxy server is sending `X-Forwarded-For` headers. For NGINX, that config should look something like this:\n`nginx\n    location / {\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header Host $http_host;\n        proxy_redirect off;\n        proxy_set_header X-Forwarded-Proto $scheme;\n        proxy_pass http://127.0.0.1:8000;\n    }`\nApache2 config\nYou need the `proxy` `proxy_http` and `proxy_html` modules enabled.\nTo do this, run `sudo a2enmod proxy proxy_http proxy_html`.\nMake sure SSL is enabled, and include the `X-Forwarded-Proto` header so that PostHog knows it.\n```apacheconf\n\n    ProxyPass / http://0.0.0.0:8000/\n    RequestHeader set X-Forwarded-Proto expr=%{REQUEST_SCHEME}\n    # SSL & other config here\n",
    "tag": "posthog"
  },
  {
    "title": "Prerequisites",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/self-host/configure/using-altinity-cloud.md",
    "content": "\ntitle: Deploying ClickHouse using Altinity.Cloud\nsidebar: Docs\nshowTitle: true\n\nimport Sunset from \"../_snippets/sunset-disclaimer.mdx\"\n\nThis document outlines how to deploy PostHog using Altinity Cloud ClickHouse clusters.\nPrerequisites\n\nAltinity.Cloud ClickHouse cluster:\nMinimum ClickHouse version: 21.8.13\nSingle shard and no data replication\nNo dashes (`-`) in cluster name\n\n\nPostHog helm chart version >= 16.1.1\nPostHog version >= 1.33.0\n\nDeployment instructions\nPostHog uses Kafka to send data from the app to ClickHouse. For that reason, Kafka needs to be accessible to ClickHouse during deployment.\nDeploying using external Kafka\n```yaml\nenv:\n    - name: CLICKHOUSE_DISABLE_EXTERNAL_SCHEMAS\n      value: '1'\nkafka:\n    enabled: false\nexternalKafka:\n    brokers:\n        - 'broker-1.posthog.kafka.us-east-1.amazonaws.com:9094'\n        - 'broker-2.posthog.kafka.us-east-1.amazonaws.com:9094'\n        - 'broker-3.posthog.kafka.us-east-1.amazonaws.com:9094'\nclickhouse:\n    enabled: false\nexternalClickhouse:\n    host: 'somecluster.demo.altinity.cloud'\n    user: 'admin'\n    password: 'password'\n    cluster: 'clustername'\n    secure: true\n```\nRead more about how to configure external Kafka in the chart in our deployment documentation.\nUsing internal Kafka\nTo deploy using a version of Kafka managed by the PostHog Helm chart, follow these three steps:\n\nDeploy your Helm chart initially with the following values.yaml:\n\n```yaml\nkafka:\n    enabled: true\n    externalAccess:\n        enabled: true\n        service:\n            type: LoadBalancer\n            ports:\n                external: 9094\n        autoDiscovery:\n            enabled: true\n    serviceAccount:\n        create: true\n    rbac:\n        create: true\nclickhouse:\n    enabled: false\nredis:\n    enabled: false\npostgresql:\n    enabled: false\npgbouncer:\n    enabled: false\nplugins:\n    enabled: false\nworker:\n    enabled: false\nweb:\n    enabled: false\nevents:\n    enabled: false\nmigrate:\n    enabled: false\n```\n\n\nGet the external Kafka IP via `kubectl get svc -n posthog | grep kafka-0-external`\n\n\nDeploy PostHog using helm with new values.yaml (fill in placeholder values)\n\n\n```yaml\nenv:\n    - name: KAFKA_URL_FOR_CLICKHOUSE\n      value: 'kafka://KAFKA_IP:9094'\n    - name: CLICKHOUSE_DISABLE_EXTERNAL_SCHEMAS\n      value: '1'\nclickhouse:\n    enabled: false\nexternalClickhouse:\n    host: 'somecluster.demo.altinity.cloud'\n    user: 'admin'\n    password: 'password'\n    cluster: 'clustername'\n    secure: true\nkafka:\n    enabled: true\n    externalAccess:\n        enabled: true\n        service:\n            type: LoadBalancer\n            ports:\n                external: 9094\n        autoDiscovery:\n            enabled: true\n    serviceAccount:\n        create: true\n    rbac:\n        create: true",
    "tag": "posthog"
  },
  {
    "title": "Updating settings",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/self-host/configure/instance-settings.md",
    "content": "\ntitle: Instance settings\nsidebar: Docs\nshowTitle: true\n\nimport Sunset from \"../_snippets/sunset-disclaimer.mdx\"\n\nWhen self-hosting PostHog there are several instance settings that can be adjusted according to your needs. These settings are available as of PostHog 1.33.0, if you're running an older version, settings can only be set using [Environment variables][env-vars].\nInstance settings can be managed by staff users by visiting the Instance settings page (`/instance/status/configuration`). Some setting configurations cannot be managed this way, and in particular, settings that determine how PostHog should behave at runtime must be set using [Environment variables][env-vars]. Please review the [Environment variables][env-vars] list for further details.\nThe actual list of settings that can be updated varies depending on which version of PostHog you're running. The Instance settings page will provide a detailed list and description of all settings available.\nUpdating settings\nSettings can easily be updated from PostHog's user interface. When updating, settings are applied immediately and used across your entire instance. Even if you have multiple pods running PostHog, all pods will use this same configuration. The settings updated here can be for advanced users and may have adverse consequences to your instance when not managed properly. Please review any warnings or additional information carefully that comes up when updating settings.\nStaff users\nStaff users are a special kind of instance-level permission that allows managing advanced instance-wide settings. A user can be a staff user regardless of their permission level to any organization(s) or project(s) in your instance. Only staff users can manage these settings.\nAs of PostHog 1.32.0, the first user in any instance is a staff user. This user can then add others if applicable. When possible, it is recommended to have multiple staff users to ensure your instance can always be properly maintained.\nStarting with version 1.34.0, staff users can also easily manage (add/remove) other staff users via the PostHog user interface or the API. You can visit the Instance status (`/instance/status/`) page and navigate to the \"Staff users\" tab to do this.\nIf you don't have any staff users (e.g. if you deployed PostHog before version 1.32.0), you can add your first staff user, by connecting to your instance (via a `web` pod), and then running the commands below.\n\nTo connect to your pod, follow these instructions\n\n`bash\npython manage.py shell_plus`\nOnce you access the Django interactive shell,\n`python\nuser = User.objects.get(email=\"email_of_the_user_to_add@example.com\")\nuser.is_staff = True\nuser.save()`",
    "tag": "posthog"
  },
  {
    "title": "Getting started",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/self-host/configure/monitoring-with-grafana.md",
    "content": "\ntitle: Monitoring with Prometheus and Grafana\nimport Sunset from \"../_snippets/sunset-disclaimer.mdx\"\n\nThis guide covers how to configure monitoring of your self-hosted deployment through Grafana.\nIf you are targeting a production use-case, we highly recommend setting up all of these options.\nGetting started\nBy default, the PostHog Helm chart does not come with Grafana enabled, so we will need to update our config values in order to install it.\n\nNote:  This guide requires you to be running a Helm chart version of at least `26.0.6`. If you are running an older version, take a look at our guide on how to upgrade PostHog\n\nSetting up cluster monitoring\nThis section covers setting up basic monitoring for the entire Kubernetes cluster, and provides basic metrics such as CPU usage, memory usage, and disk IOs.\nTo set up basic monitoring, we will need to enable the following two charts:\n\ngrafana/grafana\nprometheus-community/prometheus\n\nwhich can be done by adding the following lines to our `values.yaml`\n`yaml file=values.yaml\ngrafana:\n    enabled: true\nprometheus:\n    enabled: true`\nNext, we'll need to upgrade our deployment to spin-up the additional services, which can be done using the following command:\n`shell\nhelm upgrade -f values.yaml --timeout 30m --namespace posthog posthog posthog/posthog --atomic --wait --wait-for-jobs --debug`\nConnecting to Grafana\nOnce our deployment is back up and running, we can now log in to Grafana to see our dashboards.\nBy default, a single user is created with the username `admin` and an auto-generated password can be fetched by running:\n`shell\nkubectl -n posthog get secret posthog-grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode`\nNext, we'll connect to Grafana by port-forwarding into the `posthog-grafana` service:\n`shell\nkubectl -n posthog port-forward svc/posthog-grafana 8080:80`\nOur Grafana dashboard should now be available at `localhost:8080`, and we can log in using the username `admin` along with the password we just retrieved.\nFor information on exposing Grafana through an Nginx ingress, take a look at the configuration options for the upstream Grafana chart.\n\nFinally, if we now go to the list of Dashboards and navigate to PostHog > Kubernetes (cluster overview), you should see a pre-configured dashboard with a number of metrics related to our cluster!\nFor more information on configuring and using Grafana, check out the official docs\nSetting up service-specific monitoring\nWhile the basic cluster-overview monitoring is useful for monitoring overall cluster health, there is still a lot of important information about each service within PostHog that isn't available.\nTo fix this, PostHog includes a number of exporters for Prometheus that allow us to stream metrics from specific services into their own separate dashboard.\nFor more information on the configuration values for each service, check out ALL_VALUES.md for the full list of configuration options.\nKafka monitoring\nFor streaming information from Kafka, we use the prometheus-community/prometheus-kafka-exporter chart, which can be installed by setting `prometheus-kafka-exporter.enabled` to `true` in your `values.yaml` file.\nIf you are using an external Kafka service, you can use the `prometheus-kafka-exporter.kafkaServer` option to set the location for your managed service.\nPostgreSQL monitoring\nTo expose PostgreSQL metrics, we use the prometheus-community/prometheus-postgres-exporter chart, which can be installed by setting `prometheus-postgres-exporter.enabled` to `true` in your `values.yaml` file.\nIf you are using an external PostgreSQL deployment, you can use the `prometheus-postgres-exporter.config.datasource` option to set the location for your managed service.\n\nA sample of the default Postgres dashboard\nRedis monitoring\nTo expose metrics from Redis, we use the prometheus-community/prometheus-redis-exporter chart, which can be installed by setting `prometheus-redis-exporter.enabled` to `true` in your `values.yaml` file.\nIf you are using an external Redis service, you can use the `prometheus-redis-exporter.redisAddress` option to set the location for your managed service.\n\nA sample of the default Redis dashboard\nConfigure log-aggregation with Loki\nThis section covers setting up aggregation for logs using Loki, which allows you to query and explore logs from all of your services.\nThis step requires that you already have Grafana set-up, which you can do by following the instructions in the basic cluster monitoring section.\nTo set this up, we will need to enable the following two charts:\n\ngrafana/loki\ngrafana/promtail\n\nwhich can be done by adding the following values to your configuration:\n`yaml file=values.yaml\nloki:\n    enabled: true\npromtail:\n    enabled: true`\nOnce again, we will need to upgrade our deployment for the new changes go into effect, which can be done by following command:\n`shell\nhelm upgrade -f values.yaml --timeout 30m --namespace posthog posthog posthog/posthog --atomic --wait --wait-for-jobs --debug`\nAfter our deployment is finished, we can log back in to Grafana, and navigate to the 'Explore' tab to start viewing our logs.\nLet's try viewing the logs for Clickhouse.\nTo start, we'll switch our datasource in the top bar from 'Prometheus' to 'Loki', which will open up the Query Builder.\nWe can create a query for this by setting the `app` label to equal `chi-posthog-posthog-0-0`, which will show us only the logs which are labeled as part of the Clickhouse app.\n\nRunning this query, we'll see we get back a graph showing our log-volume over time, as well as a list of the log lines that matched our query!\n",
    "tag": "posthog"
  },
  {
    "title": "Restrict access by IP",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/self-host/configure/securing-posthog.mdx",
    "content": "\ntitle: Securing PostHog\nsidebar: Docs\nshowTitle: true\n\nRestrict access by IP\nYou can restrict access to PostHog by IP by passing `ALLOWED_IP_BLOCKS`. This is a comma separated list, and can either be individual IP addresses or subnets. For example:\n`bash\nALLOWED_IP_BLOCKS=192.168.0.0/31,127.0.0.0/25,128.0.0.1`\nIf you try to access your PostHog instance with a different IP, you will get an error message.\nThis restriction does not apply to the endpoints used to send events, like `batch`, `capture` etc.\nIf you're behind a proxy, you need to either set trusted proxies:\n`bash\nTRUSTED_PROXIES=ip1,ip2`\nOr you can implicitly trust all proxies:\n`bash\nTRUST_ALL_PROXIES=True`\n\nWhen using `TRUST_ALL_PROXIES`, make sure your proxy (like NGINX) is setting the header `X-Forwarded-For` like in the example above. If not, it would still be possible to spoof your IP address.\nIf you're on Heroku, you are behind a proxy by default, so you'll need to add `IS_BEHIND_PROXY=True`. Heroku automatically overrides `X-Forwarded-For`, so you can use `TRUST_ALL_PROXIES=True`.\n\nSecure cookies\nStarting with PostHog 1.13.0, we introduced a `SECURE_COOKIES` flag. This defaults to \"False\" when PostHog is running on `DEBUG` or `TEST` mode (generally the case when running locally) and \"True\" in production (when those modes are not on).\nWhile this defaults to \"True\" in environments that are not `TEST` or `DEBUG`, you may need to toggle this off for setup or testing purposes in a production instance. However, remember that secure cookies should always be on when handling live data (i.e. in production). This flag affects cookies used in Django sessions, CSRF tokens, and Toolbar login. In short, it ensures the security of your PostHog instance, hence it is so important.\nAs noted multiple times throughout our Docs, PostHog should always run on HTTPS (i.e. using TLS/SSL). Thus, if you are running on HTTPS (as you should) and `SECURE_COOKIES` is set to \"False\", browsers will likely throw warnings about cookies and you might have trouble logging in on some newer versions of Chrome, for example. Additionally, the toolbar login cookie will not work and you will be vulnerable to Man In the Middle (MITM) attacks when you accidentally open your app using HTTP and not HTTPS.\nFurthermore, if this flag is set to \"True\" and you are not running on HTTPS, you will not be able to log in to PostHog, since secure cookies are discarded in an unsafe environment.\nFor most users, toggling this flag will not be necessary, as PostHog handles most cases appropriately for you. However, if you need to set it manually, you can explicitly set `SECURE_COOKIES=False` or `SECURE_COOKIES=True` as an environment variable. The main use case for this is testing, where you may need secure cookies off while setting up a production environment, or you might want them on when developing locally with HTTPS.\nFor more information on Django security features, you can check out Django's Official Docs, which discuss secure cookies.\nSecret key\nImportant: PostHog will not work if you do not set your own unique `SECRET_KEY`.\nSecret keys are used to encrypt cookies and password reset emails, among other things. To generate a secret key, run:\n`bash\nopenssl rand -hex 32`\nThis `SECRET_KEY` must be passed to PostHog as an environment variable.\nSecuring PostHog Deployments\nimport Sunset from '../_snippets/sunset-disclaimer.mdx'\n\nAs part of self-hosting PostHog by deploying it to a Kubernetes cluster, we need to\nmake it is secure to threat actors. As every installation is unique, we provide\nin our documentation some general guidelines regarding how to make it secure but\nit is then up to each instance admin to make sure all the best practices are followed.\n\nmake sure your Kubernetes cluster is provisioned and configured using industries\n   best practices\nencrypt data at rest and in transit\nminimize access to worker nodes\ndeploy workers onto private subnets\nenable Kubernetes audit logs\nemploy least privileged access for all the resources\n...\n\n\nmake sure your installation is regularly maintained and up-to-date\nfor specific Helm chart configuration\n",
    "tag": "posthog"
  },
  {
    "title": "First we need to determine the name of the web pod \u2013 see \"How do I see logs for a pod?\" for more on this",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/self-host/deploy/troubleshooting.md",
    "content": "\ntitle: Troubleshooting and debugging your PostHog instance\nsidebar: Docs\nshowTitle: true\n\nimport Sunset from \"../_snippets/sunset-disclaimer.mdx\"\n\nIf you are looking for routine procedures and operations to manage PostHog installations like begin, stop, supervise, and debug a PostHog infrastructure, please take a look at the runbook section.\nTroubleshooting\nHelm failed for not enough resources\nWhile running `helm upgrade --install` you might run into an error like `timed out waiting for the condition`\nOne of the potential causes is that Kubernetes doesn't have enough resources to schedule all the services PostHog needs to run. To know if resources are a problem we can check pod status and errors while the `helm` command is still running:\n\n\ncheck the output for `kubectl get pods -n posthog` and if you see any pending pods for a long time then that could be the problem\n\n\ncheck if the pending pod has scheduling errors using `kubectl describe pod <podname> -n posthog`. For example, at the end of the events section we could see that we didn't have enough memory to schedule the pod.\n\n\n`Events:\n  Type     Reason             Age                  From                Message\n  ----     ------             ----                 ----                -------\n  Normal   NotTriggerScaleUp  3m23s                cluster-autoscaler  pod didn't trigger scale-up:\n  Warning  FailedScheduling   45s (x5 over 3m47s)  default-scheduler   0/3 nodes are available: 3 Insufficient memory.`\nHow to fix this: add more nodes to your Kubernetes cluster.\nConnection is not secure\nFirst, check that DNS is set up properly:\n`shell\nnslookup <your-hostname> 1.1.1.1`\nNote that when using a browser there are various layers of caching and other logic that could make the resolution work (temporarily) even if its not correctly set up.\nKafka crash looping (disk full)\nYou might see an error similar to this one in the Kafka pod:\n`Error while writing to checkpoint file /bitnami/kafka/data/...\njava.io.IOException: No space left on device`\nThis tells us that the data disk is full. To resize the disk, please follow the runbook.\nWhy did we run into this problem and how to avoid it in the future?\nThere isn't a way for us to say \"if there's less than X% of disk space left, then nuke the oldest data\". Instead we have two conditions that restrict, when stuff can be deleted:\n\nsize (`logRetentionBytes: _22_000_000_000`) for the minimum size of data on disk before allowed deletion.\ntime (`logRetentionHours: 24`) for the minimum age before allowed deletion.\n\nWe need to configure these well, but a disk monitoring utility can help catch this problem before we end up in a crash loop.\nSee more in these stack overflow questions (1, 2, 3).\nUpgrade failed due to cert-manager conflicts\nIf a deploy fails with the following error:\n`Error: UPGRADE FAILED: rendered manifests contain a resource that already exists. Unable to continue with update: CustomResourceDefinition \"certificaterequests.cert-manager.io\" in namespace \"\" exists and cannot be imported into the current release: invalid ownership metadata; label validation error: missing key \"app.kubernetes.io/managed-by\": must be set to \"Helm\"; annotation validation error: missing key \"meta.helm.sh/release-name\": must be set to \"posthog\"; annotation validation error: missing key \"meta.helm.sh/release-namespace\": must be set to \"posthog\"`\nThe issue might be with cert-manager custom resource definitions already existing and being unupgradable.\nTry running helm upgrade without `--atomic` to fix this issue.\nNamespace deletion stuck at `terminating`\nWhile deleting the namespace, if your Helm release uses `clickhouse.enabled: true` you might end up in the operation being indefinitely stuck.\nThis is a known behavior of the `clickhouse-operator` finalizer. Workaround:\n\n\npatch CHI removing the finalizer: `kubectl patch chi posthog -n posthog -p '{\"metadata\":{\"finalizers\":null}}' --type=merge`\n\n\ndelete CHI: `kubectl delete chi posthog -n posthog`\n\n\nFAQ\nHow can I increase storage size?\nTo increase the storage size of the ClickHouse, Kafka or PostgreSQL service, take a look at our runbook section.\nAre the errors I'm seeing important?\nHere are some examples of log spam that currently exists in our app and is safe to ignore:\nThe following messages in the ClickHouse pod happen when ClickHouse reshuffles how it consumes from the topics. So, anytime ClickHouse or Kafka restarts we'll get a bit of noise and the following log entries are safe to ignore:\n`<Error> TCPHandler: Code: 60, e.displayText() = DB::Exception: Table posthog.sharded_events doesn't exist.\n...\n<Warning> StorageKafka (kafka_session_recording_events): Can't get assignment. It can be caused by some issue with consumer group (not enough partitions?). Will keep trying.`\nThe following error is produced by some low-priority celery tasks and we haven't seen any actual impact so can safely be ignored. It shows up in Sentry as well.\n`TooManyConnections: too many connections\n  File \"posthog/celery.py\",\n  ...\n  File \"clickhouse_pool/pool.py\", line 102, in pull\n    raise TooManyConnections(\"too many connections\")`\nHow do I see logs for a pod?\n\n\nFind the name of the pod you want to get logs on:\n`shell\nkubectl get pods -n posthog`\nThis command will list all running pods. If you want app/plugin server logs, for example, look for a pod that has a name starting with `posthog-plugins`. This will be something like `posthog-plugins-54f324b649-66afm`\n\n\nGet the logs for that pod using the name from the previous step:\n`bash\nkubectl logs posthog-plugins-54f324b649-66afm -n posthog`\n\n\nHow do I connect to the web server's shell?\nPostHog is built on Django, which comes with some useful utilities. One of them is a Python shell.\nYou can connect to it like so:\n```bash\nFirst we need to determine the name of the web pod \u2013 see \"How do I see logs for a pod?\" for more on this\nPOSTHOG_WEB_POD_NAME=$(kubectl get pods -n posthog | grep -- '-web-' | awk '{print $1}')\nThen we connect to the interactive Django shell\nkubectl exec -n posthog -it $POSTHOG_WEB_POD_NAME -- python manage.py shell_plus\n```\nIn a moment you should see the shell load and finally a message like this appear:\n```\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n(InteractiveConsole)\n\n\n\n```\n\n\n\nThat means you can now type Python code and run it with PostHog context (such as models) already loaded!\nFor example, to see the number of users in your instance run:\n`python\nUser.objects.count()`\nHow do I connect to Postgres?\n\n\nFind out your Postgres password from the web pod:\n```bash\nFirst we need to determine the name of the web pod \u2013 see \"How do I see logs for a pod?\" for more on this\nPOSTHOG_WEB_POD_NAME=$(kubectl get pods -n posthog | grep -- '-web-' | awk '{print $1}')\nThen we can get the password from the pod's environment variables\nkubectl exec -n posthog -it $POSTHOG_WEB_POD_NAME -- sh -c 'echo The Postgres password is: $POSTHOG_DB_PASSWORD'\n```\n\n\nConnect to your Postgres pod's shell:\n```bash\nWe need to determine the name of the Postgres pod (usually it's 'posthog-posthog-postgresql-0')\nPOSTHOG_POSTGRES_POD_NAME=$(kubectl get pods -n posthog | grep -- '-postgresql-' | awk '{print $1}')\nWe'll connect straight to the Postgres pod's psql interface\nkubectl exec -n posthog -it $POSTHOG_POSTGRES_POD_NAME  -- /bin/bash\n```\n\n\nConnect to the `posthog` database:\n\nYou're connecting to your production database, proceed with caution!\n\n`bash\npsql -d posthog -U postgres`\nPostgres will ask you for the password. Use the value you found out in step 1.\nNow you can run SQL queries! Just remember that an SQL query needs to be terminated with a semicolon `;` to run.\n\n\nHow do I connect to ClickHouse?\n\nTip: Find out your pod names with `kubectl get pods -n posthog`\n\n\n\nFind out your ClickHouse user and password from the web pod:\n`shell\nkubectl exec -n posthog -it <your-posthog-web-pod> \\\n-- sh -c 'echo user:$CLICKHOUSE_USER password:$CLICKHOUSE_PASSWORD'`\n\n\nConnect to the `chi-posthog-posthog-0-0-0` pod:\n`shell\nkubectl exec -n posthog -it chi-posthog-posthog-0-0-0  -- /bin/bash`\n\n\nConnect to ClickHouse using `clickhouse-client`:\n\nNote: You're connecting to your production database, proceed with caution!\n\n`shell\nclickhouse-client -d posthog --user <user_from_step_1> --password <password_from_step_1>`\n\n\nHow do I restart all pods for a service?\n\nImportant: Not all services can be safely restarted this way. It is safe to do this for the app/plugin server. If you have any doubts, ask someone from the PostHog team.\n\n\n\nTerminate all running pods for the service:\n```shell\nsubstitute posthog-plugins for the desired service\nkubectl scale deployment posthog-plugins --replicas=0 -n posthog\n```\n\n\nStart new pods for the service:\n```shell\nsubstitute posthog-plugins for the desired service\nkubectl scale deployment posthog-plugins --replicas=1 -n posthog\n\n",
    "tag": "posthog"
  },
  {
    "title": "Note: those overrides are experimental as each installation and workload is unique",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/self-host/deploy/configuration.md",
    "content": "\ntitle: PostHog chart configuration\nsidebarTitle: Chart configuration\nsidebar: Docs\nshowTitle: true\n\nimport Sunset from '../_snippets/sunset-disclaimer.mdx'\n\nThis document outlines the most important configuration options available in the chart.\nDependencies\nBy default, the chart installs the following dependencies:\n\naltinity/clickhouse-operator\nbitnami/kafka\nbitnami/minio\nbitnami/postgresql\nbitnami/redis\nbitnami/zookeeper\n\nThere is optional support for the following additional dependencies:\n\ngrafana/grafana\ngrafana/loki\ngrafana/promtail\njetstack/cert-manager\nkubernetes/ingress-nginx\nprometheus-community/prometheus\nprometheus-community/prometheus-kafka-exporter\nprometheus-community/prometheus-postgres-exporter\nprometheus-community/prometheus-redis-exporter\nprometheus-community/prometheus-statsd-exporter\n\nChart configuration\nAll PostHog Helm chart configuration options can be found in the ALL_VALUES.md generated from the values.yaml file.\nDependent charts can also have values overwritten. See Chart.yaml for more info regarding the source shard and the namespace that can be used for the override.\nScaling up\nThe default configuration is geared towards minimizing costs. Here are example extra values overrides to use for scaling up:\n\n\nCustom overrides for {`<`} 1M events/month\n\n\n```yaml\n# Note: those overrides are experimental as each installation and workload is unique\n\n# Use larger storage for stateful services\nclickhouse:\n    persistence:\n        size: 60Gi\n\npostgresql:\n    persistence:\n        size: 30Gi\n\nkafka:\n    persistence:\n        size: 60Gi\n    logRetentionBytes: _45_000_000_000\n\n# Add additional replicas for the stateless services\nevents:\n    replicacount: 2\n\npgbouncer:\n    replicacount: 2\n\nplugins:\n    replicacount: 2\n\nweb:\n    replicacount: 2\n\nworker:\n    replicacount: 2\n```\n\n\n\n\nCustom overrides for > 1M events/month\n\n\n```yaml\n# Note: those overrides are experimental as each installation and workload is unique\n\n# Use larger storage for stateful services\nclickhouse:\n    persistence:\n        size: 200Gi\n\npostgresql:\n    persistence:\n        size: 100Gi\n\nkafka:\n    persistence:\n        size: 200Gi\n    logRetentionBytes: _150_000_000_000\n\n# Enable horizontal pod autoscaling for stateless services\nevents:\n    hpa:\n        enabled: true\n\npgbouncer:\n    hpa:\n        enabled: true\n\nplugins:\n    hpa:\n        enabled: true\n\nweb:\n    hpa:\n        enabled: true\n\nworker:\n    hpa:\n        enabled: true\n```\n\n\nUsing dedicated nodes for services\nFor the stateful services (ClickHouse, Kafka, Redis, PostgreSQL, Zookeeper), we suggest you to run them on nodes with dedicated CPU resources and fast drives (SSD/NVMe).\nIn order to do so, after having labeled your Kubernetes nodes, you can assign pods to them using the following overrides:\n\nClickHouse: `clickhouse.nodeSelector`\nKafka: `kafka.nodeSelector`\nRedis: `redis.master.nodeSelector`\nPostgreSQL: `postgresql.master.nodeSelector`\nZookeeper: `zookeeper.nodeSelector`\n\nExample:\n`yaml\nclickhouse.nodeSelector:\n    diskType: ssd\n    nodeType: fast`\nFor more fine grained options, `affinity` and `tolerations` overrides are also available for the majority of the stateful components. See the official Kubernetes documentation for more info.\nEmail (SMTP service)\nFor PostHog to be able to send emails we need a working SMTP service available. You can configure PostHog to use the service by editing the `email` section of your `values.yaml` file. Example:\n`yaml\nemail:\n    host: <SMTP service host>\n    port: <SMTP service port>`\nIf your SMTP services requires authentication (recommended) you can either:\n\ndirectly provide the SMTP login in the `values.yaml` by simply setting `email.user` and `email.password`\n\n\n\nExample\n\n\n```yaml\nemail:\n    host: \n    port: \n    user: \n    password: \n```\n\n\n\nprovide the password via a Kubernetes secret, by configuring `email.existingSecret` and `email.existingSecretKey` accordingly\n\n\n\nExample\n\n\n1. create the secret by running: `kubectl -n posthog create secret generic \"smtp-password\" --from-literal=\"password=\"`\n\n2. configure your `values.yaml` to reference the secret:\n\n```yaml\nemail:\n    host: \n    port: \n    user: \n    existingSecret: 'smtp-password'\n    existingSecretKey: 'password'\n```\n\n\nClickHouse\nClickHouse is the datastore system that does the bulk of heavy lifting with regards to storing and analyzing the analytics data.\nBy default, ClickHouse is installed as a part of the chart, powered by clickhouse-operator. You can also use a ClickHouse managed service like Altinity (see here for more info).\nSecuring ClickHouse\nBy default, the PostHog Helm Chart will provision a ClickHouse cluster using a default username and password. Please provide a unique login by overriding the `clickhouse.user` and `clickhouse.password` values.\nBy default, the PostHog Helm Chart uses a `ClusterIP` to expose the service\ninternally to the rest of the PostHog application. This should prevent any\nexternal access.\nIf however you decide you want to access the ClickHouse cluster external to the\nKubernetes cluster and need to expose it e.g. to the internet, keep in mind the\nfollowing:\n\n\nthe Helm Chart does not configure TLS for ClickHouse, thus we would\n    recommend that you ensure that you configure TLS e.g. within a load balancer\n    in front of the cluster.\n\n\nif exposing via a `LoadBalancer` or `NodePort` service type via\n    `clickhouse.serviceType`, these will both expose a port on your Kubernetes\n    nodes. We recommend you ensure that your Kubernetes worker nodes are within\n    a private network or in a public network with firewall rules in place.\n\n\nif exposing via a `LoadBalancer` service type, restrict the ingress network\n    access to the load balancer\n\n\nto restrict access to the ClickHouse cluster, ClickHouse offers settings for\n    restricting the IPs/hosts that can access the cluster. See the\n    user_name/networks\n    setting for details. We expose this setting via the Helm Chart as\n    `clickhouse.allowedNetworkIps`\n\n\nUse an external service\nTo use an external ClickHouse service, please set `clickhouse.enabled` to `false` and then configure the `externalClickhouse` values.\nFind out how to deploy PostHog using Altinity Cloud in our deployment configuration docs.\nCustom settings\nIt's possible to pass custom settings to ClickHouse. This might be needed to e.g. set query time limits or increase max memory usable by clickhouse.\nTo do so, you can override the `clickhouse.profiles` values as below. The `default` profile is used by PostHog for all queries.\n`yaml\nclickhouse:\n    profiles:\n        default/max_execution_time: '180'\n        default/max_memory_usage: '40000000000'`\nRead more about ClickHouse settings here.\nSee ALL_VALUES.md for full configuration options.\nMinIO\nBy default, `MinIO` is not installed as part of the chart. If you want to enable it, please set `minio.enabled` to `true`.\nMinIO provide a scalable, S3 compatible object storage system. You can customize all its settings by overriding `values.yaml` variables in the `minio` namespace.\nNote: please override the default user authentication by either passing `auth.rootUser` and `auth.rootPassword` or `auth.existingSecret`.\nUse an external service\nTo use an external S3 like/compatible object storage, please set `minio.enabled` to `false` and then configure the `externalObjectStorage` values.\nSee ALL_VALUES.md and the MinIO chart for full configuration options.\nPostgreSQL\n\nWhile ClickHouse powers the bulk of the analytics if you deploy PostHog using this chart, Postgres is still needed as a data store for PostHog to work.\n\nPostgreSQL is installed by default as part of the chart. You can customize all its settings by overriding `values.yaml` variables in the `postgresql` namespace.\nNote: to avoid issues when upgrading this chart, provide `postgresql.postgresqlPassword` for subsequent upgrades. This is due to an issue in the PostgreSQL upstream chart where password will be overwritten with randomly generated passwords otherwise. See PostgreSQL#upgrade for more details.\nUse an external service\nTo use an external PostgreSQL service, please set `postgresql.enabled` to `false` and then configure the `externalPostgresql` values.\nSee ALL_VALUES.md and the PostgreSQL chart for full configuration options.\nPgBouncer\nPgBouncer is a lightweight connection pooler for PostgreSQL and it is installed by default as part of the chart. It is currently required in order for the installation to work (see here for more info).\nIf you've configured your PostgreSQL instance to require the use of TLS, you'll need to pass an additional env variables to the PgBouncer deployment (see the official documentation for more info). Example:\n`yaml\npgbouncer:\n    env:\n        - name: SERVER_TLS_SSLMODE\n          value: 'your_value'`\nSee ALL_VALUES.md for full configuration options.\nRedis\nRedis is installed by default as part of the chart. You can customize all its settings by overriding `values.yaml` variables in the `redis` namespace.\nUse an external service\nTo use an external Redis service, please set `redis.enabled` to `false` and then configure the `externalRedis` values.\n\n\nExample\n\n\n```yaml\nredis:\n    enabled: false\n\nexternalRedis:\n    host: 'posthog.cache.us-east-1.amazonaws.com'\n    port: 6379\n```\n\n\nCredentials\nBy default, Redis doesn't use any password for authentication. If you want to configure it to use a password (recommended) see the options below.\n\n\nInternal Redis\n\n\n-   set `redis.auth.enabled` to `true`\n-   to directly provide the password value in the `values.yaml` simply set it in `redis.auth.password`\n-   if you want to provide the password via a Kubernetes secret, please configure `redis.auth.existingSecret` and `redis.auth.existingSecretPasswordKey` accordingly:\n\n    **Example**\n\n    1. create the secret by running: `kubectl -n posthog create secret generic \"redis-existing-secret\" --from-literal=\"redis-password=\"`\n\n    2. configure your `values.yaml` to reference the secret:\n\n    ```yaml\n    redis:\n        enabled: true\n        auth:\n            enabled: true\n            existingSecret: 'redis-existing-secret'\n            existingSecretPasswordKey: 'redis-password'\n    ```\n\n\n\n\nExternal Redis\n\n\n-   to directly provide the password value in the `values.yaml` simply set it in `externalRedis.password`\n\n-   if you want to provide a password via an existing secret, please configure `externalRedis.existingSecret` and `externalRedis.existingSecretPasswordKey` accordingly:\n\n    **Example**\n\n    1. create the secret by running: `kubectl -n posthog create secret generic \"redis-existing-secret\" --from-literal=\"redis-password=\"`\n\n    1. configure your `values.yaml` to reference the secret:\n\n        ```yaml\n        redis:\n            enable: false\n\n        externalRedis:\n            host: ''\n            port: \n            existingSecret: 'redis-existing-secret'\n            existingSecretPasswordKey: 'redis-password'\n        ```\n\n\nSee ALL_VALUES.md and the Redis chart for full configuration options.\nKafka\nKakfa is installed by default as part of the chart. You can customize all its settings by overriding `values.yaml` variables in the `kafka` namespace.\nUse an external service\nTo use an external Kafka service, please set `kafka.enabled` to `false` and then configure the `externalKafka` values.\n\n\nExample\n\n\n```yaml\nkafka:\n    enabled: false\n\nexternalKafka:\n    brokers:\n        - 'broker-1.posthog.kafka.us-east-1.amazonaws.com:9094'\n        - 'broker-2.posthog.kafka.us-east-1.amazonaws.com:9094'\n        - 'broker-3.posthog.kafka.us-east-1.amazonaws.com:9094'\n```\n\n\nSee ALL_VALUES.md and the Kafka chart for full configuration options.\nIngress\nThis chart provides support for the Ingress resource. If you have an available Ingress Controller such as Nginx or Traefik you maybe want to set `ingress.nginx.enabled` to true or `ingress.type` and choose an `ingress.hostname` for the URL. Then, you should be able to access the installation using that address.\nGrafana\nBy default, `grafana` is not installed as part of the chart. If you want to enable it, please set `grafana.enabled` to `true`.\nThe default settings provide a vanilla installation with an auto generated login. The username is `admin` and the auto-generated password can be fetched by running:\n`shell\nkubectl -n posthog get secret posthog-grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode`\nTo configure the stack (like expose the service via an ingress resource, manage users, ...) please look at the inputs provided by the upstream chart.\nSee ALL_VALUES.md and the grafana chart for full configuration options.\nLoki\nBy default, `loki` is not installed as part of the chart. If you want to enable it, please set `loki.enabled` to `true`.\nTo configure the stack (like expose the service via an ingress resource, ...) please look at the inputs provided by the upstream chart.\nSee ALL_VALUES.md and the loki chart for full configuration options.\nPromtail\nBy default, `promtail` is not installed as part of the chart. If you want to enable it, please set `promtail.enabled` to `true`.\nTo configure the stack (like expose the service via an ingress resource, ...) please look at the inputs provided by the upstream chart.\nSee ALL_VALUES.md and the promtail chart for full configuration options.\nPrometheus\nThis chart supports alerting. Set `prometheus.enabled` to true and set `prometheus.alertmanagerFiles` to the right configuration.\nRead more at Prometheus chart and Prometheus configuration\nExample configuration (PagerDuty)\n```yaml\nprometheus:\n    enabled: true\n\n\n```alertmanagerFiles:\n    alertmanager.yml:\n        receivers:\n            - name: default-receiver\n              pagerduty_configs:\n                  - routing_key: YOUR_ROUTING_KEY\n                    description: \"{{ range .Alerts }}{{ .Annotations.summary }}\\n{{ end }}\"\n\n        route:\n            group_by: [alertname]\n            receiver: default-receiver\n```\n\n\n```\nGetting access to the Prometheus UI\nThis might be useful when checking out metrics. Figure out your `prometheus-server` pod name via `kubectl get pods --namespace NS` and run:\n`kubectl --namespace NS port-forward posthog-prometheus-server-XXX 9090`.\nAfter this, you should be able to access Prometheus server on `localhost`.\nStatsd / prometheus-statsd-exporter\nBy default, StatsD is not installed as part of the chart. If you want to enable it, please set `prometheus-statsd-exporter.enabled` to `true`.\nUse an external service\nTo use an external StatsD service, please set `prometheus-statsd-exporter.enabled` to `false` and then configure the `externalStatsd` values.\nSee ALL_VALUES.md and prometheus-statsd-exporter chart for full configuration options.\nprometheus-kafka-exporter\nBy default, `prometheus-kafka-exporter` is not installed as part of the chart. If you want to enable it, please set `prometheus-kafka-exporter.enabled` to `true`. If you are using an external Kafka, please configure `prometheus-kafka-exporter.kafkaServer` accordingly.\nSee ALL_VALUES.md and prometheus-kafka-exporter chart for full configuration options.\nprometheus-postgres-exporter\nBy default, `prometheus-postgres-exporter` is not installed as part of the chart. If you want to enable it, please set `prometheus-postgres-exporter.enabled` to `true`. If you are using an external Kafka, please configure `prometheus-postgres-exporter.config.datasource` accordingly.\nSee ALL_VALUES.md and prometheus-postgres-exporter chart for full configuration options.\nprometheus-redis-exporter\nBy default, `prometheus-redis-exporter` is not installed as part of the chart. If you want to enable it, please set `prometheus-redis-exporter.enabled` to `true`. If you are using an external Redis, please configure `prometheus-redis-exporter.redisAddress` accordingly.",
    "tag": "posthog"
  },
  {
    "title": "sunset-disclaimer.mdx",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/self-host/_snippets/sunset-disclaimer.mdx",
    "content": "\n\ud83c\udf07 Sunsetting Kubernetes Deployments\n\nThis page covers our PostHog Kubernetes deployment, which we are currently in the process of sunsetting.\nExisting customers will receive support until May 31, 2023 and we will continue to provide security updates for the next year.\n\n\nFor existing customers\nWe highly recommend migrating to PostHog Cloud (US or EU). Take a look at this guide for more information on the migration process.\n\n\nLooking to continue self-hosting?\nWe still maintain our Open-source Docker Compose deployment. Instructions for deploying can be found here.\n",
    "tag": "posthog"
  },
  {
    "title": "get-installation-address.mdx",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/self-host/_snippets/get-installation-address.mdx",
    "content": "```shell\nPOSTHOG_IP=$(kubectl get --namespace posthog ingress posthog -o jsonpath=\"{.status.loadBalancer.ingress[0].ip}\" 2> /dev/null)\nPOSTHOG_HOSTNAME=$(kubectl get --namespace posthog ingress posthog -o jsonpath=\"{.status.loadBalancer.ingress[0].hostname}\" 2> /dev/null)\nif [ -n \"$POSTHOG_IP\" ]; then\n    POSTHOG_INSTALLATION=$POSTHOG_IP\nfi\nif [ -n \"$POSTHOG_HOSTNAME\" ]; then\n    POSTHOG_INSTALLATION=$POSTHOG_HOSTNAME\nfi\nif [ ! -z \"$POSTHOG_INSTALLATION\" ]; then\n    echo -e \"\\n----\\nYour PostHog installation is available at: http://${POSTHOG_INSTALLATION}\\n----\\n\"\nelse\n    echo -e \"\\n----\\nUnable to find the address of your PostHog installation\\n----\\n\"\nfi",
    "tag": "posthog"
  },
  {
    "title": "installing.mdx",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/self-host/_snippets/installing.mdx",
    "content": "To install the chart using Helm with the release name `posthog` in the `posthog` namespace, run the following:\n`shell\nhelm repo add posthog https://posthog.github.io/charts-clickhouse/\nhelm repo update\nhelm upgrade --install -f values.yaml --timeout 30m --create-namespace --namespace posthog posthog posthog/posthog --wait --wait-for-jobs --debug`\nNote: if you decide to use a different Helm release name or namespace, please keep in mind you might have to change several values in\nyour `values.yaml` in order to make the installation successful. This is because we build several Kubernetes resources",
    "tag": "posthog"
  },
  {
    "title": "expandable-volume.mdx",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/self-host/_snippets/expandable-volume.mdx",
    "content": "\n\nDetails\n\n\n\n`PersistentVolumes` can be configured to be expandable. This feature when set to `true`, allows the users to resize the volume by editing the corresponding `PersistentVolumeClaims` object.\n\nThis can become useful in case your storage usage grows and you want to resize the disk on-the-fly without having to resync data across PVCs.\n\nTo verify if your storage class allows volume expansion you can run:\n\n```shell\nkubectl get storageclass -o json | jq '.items[].allowVolumeExpansion'\ntrue\n```\n\nIn case it returns `false`, you can enable volume expansion capabilities for your storage class by running:\n\n```shell\nDEFAULT_STORAGE_CLASS=$(kubectl get storageclass -o=jsonpath='{.items[?(@.metadata.annotations.storageclass\\.kubernetes\\.io/is-default-class==\"true\")].metadata.name}')\nkubectl patch storageclass \"$DEFAULT_STORAGE_CLASS\" -p '{\"allowVolumeExpansion\": true}'\nstorageclass.storage.k8s.io/gp2 patched\n```\n\n> N.B:\n>  - expanding a persistent volume is a time consuming operation\n>  - some platforms have a per-volume quota of one modification every 6 hours\n>  - not all the volume types support this feature. Please take a look at the [official docs](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion) for more info",
    "tag": "posthog"
  },
  {
    "title": "upgrading.mdx",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/self-host/_snippets/upgrading.mdx",
    "content": "import CommandHelmGetRepoSnippet from './command-helm-get-repo'\nimport CommandHelmUpgradeSnippet from './command-helm-upgrade'\nTo upgrade the Helm release `posthog` in the `posthog` namespace:\n\nGet and update the Helm repo:\n\n\n\nVerify if the operation is going to be a major version upgrade:\n\n`shell\n  helm list -n posthog\n  helm search repo posthog`\nCompare the numbers of the Helm chart version (in the format `posthog-{major}.{minor}.{patch}` - for example, `posthog-19.15.1`) when running the commands above. If the upgrade is for a major version, check the upgrade notes before moving forward.\n\nRun the upgrade\n\n",
    "tag": "posthog"
  },
  {
    "title": "install-cluster-issuer.mdx",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/self-host/_snippets/install-cluster-issuer.mdx",
    "content": "Create a new cluster resource that will take care of signing your TLS certificates using Let\u2019s Encrypt.\n\nCreate a new file called `cluster-issuer.yaml` with the following content. Note: please remember to replace `your-name@domain.com`\nwith a valid email address as you will receive email notifications on certificate renewals:\n\n`yaml file=cluster-issuer.yaml\n  apiVersion: cert-manager.io/v1\n  kind: ClusterIssuer\n  metadata:\n    name: letsencrypt-prod\n  spec:\n    acme:\n      email: \"your-name@domain.com\"\n      server: https://acme-v02.api.letsencrypt.org/directory\n      privateKeySecretRef:\n        name: posthog-tls\n      solvers:\n      - http01:\n          ingress:\n            class: nginx`",
    "tag": "posthog"
  },
  {
    "title": "cluster-requirements.mdx",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/self-host/_snippets/cluster-requirements.mdx",
    "content": "import ExpandableVolumeSnippet from './expandable-volume'\nCluster requirements\n\nKubernetes version >=1.23 <= 1.25\nKubernetes nodes:\nensure you have enough resources available (we suggest a total minimum of 4 vcpu & 8GB of memory)\nensure you can run `x86-64`/`amd64` workloads. `arm64` architecture is currently not supported\n\n\nSuggestion: ensure `allowVolumeExpansion` is set to `True` in the storage class definition (this setting enables `PVC` resize)\n\n\n\n\nSuggestion: ensure `reclaimPolicy` is set to `Retain` in the storage class definition (this setting allows for manual reclamation of the resource)\n    \nThe `Retain` reclaim policy allows for manual reclamation of the resource. When the PersistentVolumeClaim is deleted, the PersistentVolume still exists and the volume is considered \"released\".\nBut it is not yet available for another claim because the previous claimant's data remains on the volume (see the official documentation).\nThis can become useful in case your need to reprovision a pod/statefulset but you don't want to lose the underlying data\nTo verify which `reclaimPolicy` your default storage class is using you can run:\n`shell\nkubectl get storageclass -o json | jq '.items[].reclaimPolicy'\n\"Retain\"`\nIf your storage class allows it, you can modify the `reclaimPolicy` by running:\n`shell\nDEFAULT_STORAGE_CLASS=$(kubectl get storageclass -o=jsonpath='{.items[?(@.metadata.annotations.storageclass\\.kubernetes\\.io/is-default-class==\"true\")].metadata.name}')\nkubectl patch storageclass \"$DEFAULT_STORAGE_CLASS\" -p '{\"reclaimPolicy\": \"Retain\"}'\nstorageclass.storage.k8s.io/gp2 patched`\n\n",
    "tag": "posthog"
  },
  {
    "title": "Insights counting unique persons",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/how-posthog-works/queries.mdx",
    "content": "\ntitle: Querying data\nThis page provides a high-level overview of how queries are run when creating insights in PostHog.\n\nNote:  This page does not cover all the intricacies of how queries are run in PostHog.\n\nInsights counting unique persons\nThis section covers how PostHog determines the number of unique users who performed a certain action, such as when creating a Trends or Funnel insight.\nIn this case, PostHog determines this number by counting the total number of unique `person_id`'s on events that match your filters.\nAs an example, let's say that we have the following list of events:\n| ID  | Event       | `person_id` |\n| --- | ----------- | ----------- |\n| 1   | viewed page | `user1`     |\n| 2   | viewed page | `user2`     |\n| 3   | viewed page | `user1`     |\nIn this case, if we ran a query asking for the number of unique users who viewed a page, we would get a result of `2`, as our table contains 2 unique `person_id`'s.\nThe way we write the `person_id` to each event has some implications for the number of unique users that are displayed:\n\nSome users are counted twice on the trend graph. \n   The source of truth for data is the events table. Since this is point-in-time data, it is not possible to determine whether two `person_id`'s were later merged into a single user, which results in them being counted separately.\nNote: Before PostHog version 1.39.0, we would join with the persons table to get this result. However, as this does not scale well, we've decided to change to the new method of counting distinct `person_id`'s.\n\n\nIn the person modal, the count may be lower than the count displayed in the graph. \n   Persons who've been merged into one have one of their old IDs deleted. We remove these people from the persons modal, as there's no place to link them to.\n\nTo understand better how these scenarios can arise, let's take a look at some specific examples.\n| Day | Event    | distinct_id              | `person_id` |\n| --- | -------- | ------------------------ | ----------- |\n| 1   | other    | Alice                    | user-1      |\n| 2   | pageview | anon-1                   | user-2      |\n| 2   | identify | Alice (anon-id = anon-1) | user-1      |\nIn this case, we have a user Alice who sends an 'other' event on day `1` from her mobile phone.\nOn day 2, Alice decides to view the homepage from her desktop where she isn't logged in. This results in the pageview event being associated with a newly created Person (`user-2`).\nShe then logs in to her account, which sends an identify event that merges `user-2` into `user-1`.\nThis mean that we delete `user-2` from the persons table and all future events from `anon-1` will be tied to `user-1` (note that we never alter the events table to reflect this).\nIn this case, we\u2019d show 1 unique user in the trend graph for pageviews, but since `user-2` was deleted during the merge, we would show 0 users in the person modal.\nTo continue the example, let's say that Alice views the homepage again now that she is logged in.\n| Day | Event    | distinct_id                       | `person_id` |\n| --- | -------- | --------------------------------- | ----------- |\n| 1   | other    | Alice                             | user-1      |\n| 2   | pageview | anon-1                            | user-2      |\n| 2   | identify | Alice (anon_distinct_id = anon-1) | user-1      |\n| 2   | pageview | Alice                             | user-1      |\nIn this case, the trend graph would show 2 unique users (based on person_id = `user-1` and `user-2`) but the Person modal would only show `user-1` as `user-2` has been deleted.\nFiltering on person properties\nThis section covers how PostHog filters out events based on Person properties.\nSince all the properties for a person are stored on each event, the process is actually quite straightforward.\nLet's walk through a simple example to see how this works in practice. Let's say we have ingested the following events:\n| User ID | Event            | Subscription plan (Property on each person) |\n| ------- | ---------------- | --------------------------------------------- |\n| 1       | clicked login    | `premium`                                     |\n| 2       | refreshed table  | `premium`                                     |\n| 3       | viewed docs      | `free`                                        |\n| 3       | upgraded plan    | `enterprise`                                  |\n| 3       | viewed dashboard | `enterprise`                                  |\n| 4       | logged out       | `free`                                        |\n\nNote:  This isn't exactly how person properties are stored within the events table, but it will help us to keep things simple. For detailed information, check out our data model.\n\nIn this case, let's say we only want to see events from users while they were on the `premium` or `enterprise` plans.\nTo achieve this, we would filter based on the Subscription plan person property, which would match the following events.\n| User ID | Event            | Subscription plan (Property on each person) |\n| ------- | ---------------- | --------------------------------------------- |\n| 1       | clicked login    | `premium`                                     |\n| 2       | refreshed table  | `premium`                                     |\n| 3       | upgraded plan    | `enterprise`                                  |\n| 3       | viewed dashboard | `enterprise`                                  |\nYou may have noticed that over the course of this period, user `3` actually upgraded from the `free` plan to the `enterprise` plan.\nDespite this, the event they sent for when they viewed the docs still reflects that they were on the `free` plan at the time, and is thus filtered out.\nIn most cases this is exactly what we want, as it means that we can update the properties for a person without worrying about messing up our past data points!\nHowever, if instead you do want to filter based on a person's current properties, you can do so by creating a cohort.\nTo see this let's say we want to get all events for users who are currently on `enterprise` or `premium` plans.\nTo do this, we'll create a cohort called 'Paid users' that matches all persons who have their 'plan' property set as either `premium` or `enterprise`.\nThen on the insight, we can filter by the cohort, which would match the following events.\n| User ID | Event            | Subscription plan (Property on each person) |\n| ------- | ---------------- | --------------------------------------------- |\n| 1       | clicked login    | `premium`                                     |\n| 2       | refreshed table  | `premium`                                     |\n| 3       | viewed docs      | `free`                                        |\n| 3       | upgraded plan    | `enterprise`                                  |\n| 3       | viewed dashboard | `enterprise`                                  |\nFiltering on group properties",
    "tag": "posthog"
  },
  {
    "title": "Zooming right out",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/how-posthog-works/index.mdx",
    "content": "\ntitle: Architecture\nThe following pages in this section cover Data model,  Ingestion pipeline, ClickHouse and Querying data. \nThis document gives an overview of the general architecture. How our cloud or a deployed PostHog Helm chart works on Kubernetes.\nZooming right out\nThere are only a few systems to consider. A Website and API is presented for users. An API for Client Apps. A plugin service for processing events on ingestion. And a worker service for processing events in response to triggers (e.g. timers)\n`mermaid\ngraph LR\n    u[User]\n    sdk[Client Apps/SDKs]\n    ex[Export Sink]\n    dj[Web/API]\n    p[plugin/worker service]\n    c[Celery]\n    ds[(Data stores)]\n    u-->dj\n    sdk --> dj\n    p --> ex\n    dj --> p\n    p <--> ds\n    dj-->c\n    c <--> ds\n    dj <--> ds`\nZooming closer\nStarts to reveal the flow between parts of the system\n```mermaid\ngraph LR\n    u[User]\n    sdk[Client Apps/SDKs]\n    ex[Export Sink]\n    ds[(Data stores)]\n\n\n```subgraph Web/API\n    w[Web]\n    c[Capture API]\n    d[Decide API]\n    cr[cron]\n    ce[Celery]\nend\n\nsubgraph \"plugin/worker service\"\n    i[Ingestion]\n    a[Async]\n    t[timer]\nend\n\nu-->|views insights and more|w\nsdk-->|send events|c\nsdk-->|read|d\nw-->ds\nc-->|write events|i\ni-->|onEvent|a\ni-->|save events|ds\nt-->|onTimer|a\na-->|export events|ex\nd-->|e.g. read flags|ds\nds-->|read events|a\nw-->|start task|ce\ncr-->|on schedule|ce\nce-->ds\n```\n\n\n```\nZoomed right in\n```mermaid\nflowchart TD\n    classDef sgraph fill-opacity:0,stroke-width:3px,stroke-dasharray:5,stroke:#000\n\n\n```subgraph K8s [\"K8s PostHog namespace\"]\n    Ingress(Ingress)\n    PG[(Postgres Stateful service)]\n    Kafka[(Kafka Stateful Service)]\n\n    KafkaEvents[(Kafka Stateful Service)]\n    Redis[(Redis SS)]\n\n    ServiceLB([Service Load Balancer])\n    ServiceLBReads([Service Load Balancer])\n\n    subgraph ServicesDB [K8s Services]\n        PGBouncer\n    end\n\n    subgraph CH [\"ClickHouse Cluster (Operator Managed)\"]\n        CH1[(Replica 1 Shard 1)]\n        CH2[(Replica 1 Shard 2)]\n        CH3[(Replica 2 Shard 1)]\n        CH4[(Replica 2 Shard 2)]\n    end\n\n    subgraph ZK [K8s ZooKeeper cluster]\n        ZK1\n        ZK2\n        ZK3\n    end\n\n    subgraph AppServices [K8s Services]\n        Events(Events Service)\n        App(Web Service)\n\n        na %% Invisible helper node\n    end\n\n\n    subgraph WorkerServices [K8s Services]\n        Plugins[Plugin Service]\n        Worker[Worker Service]\n    end\nend\n\nAppServices --> ServiceLB --> ServicesDB --> PG\n\nEvents --> KafkaEvents --> Plugins --> Kafka --Write path--> CH\nWorkerServices --> ServiceLBReads\nWorkerServices --> ServiceLB\nServiceLBReads --Read path--> CH\n\n\nAppServices --> ServiceLBReads\n\nCH --> ZK\n\nCH1 <--> CH2\nCH3 <--> CH4\n\nClientApps(Client Apps)--- Ingress\n\n%% KLUDGE: Use invisible nodes for styling purposes\nIngress --- na[ ]\nna --Other traffic--> App\nna --Events endpoint --> Events\nstyle na height:0px,width:0px\n\nRedis -.- WorkerServices\nAppServices -.- Redis\n\nAppServices-.Optional Utilization telemetry.->Telemetry(Posthog License Telemetry service)\n\nclass K8s,ServicesDB,CH,ZK,AppServices,WorkerServices sgraph;\n```\n\n\n```\nNo communication is needed into or out of this namespace other than the ingress controller for app and collecting data.\nNote that the specifics of this may vary:\n\nClickHouse, Kafka, PostgreSQL and Redis services may be hosted outside of the namespace or configured differently.\nThe ClickHouse cluster is managed by clickhouse-operator and the\n",
    "tag": "posthog"
  },
  {
    "title": "Events",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/how-posthog-works/clickhouse.md",
    "content": "\ntitle: ClickHouse\nClickHouse is our main analytics backend.\nInstead of data being inserted directly into ClickHouse, it itself pulls data from Kafka. This makes our ingestion pipeline more resilient towards outages.\nThe following sections go more into depth in how this works exactly.\nEvents\nIn order to make PostHog easy to scale, we use a sharded ClickHouse setup.\n```mermaid\nflowchart LR\n    classDef table fill:#f6e486,stroke:#ffc45d;\n    kafka_events[\"kafka_events table(Kafka table engine)\"]:::table\n    eventsmv[\"events_mv table(Materialized view)\"]:::table\n    writable_events[\"writable_events table(Distributed table engine)\"]:::table\n    events[\"events table(Distributed table engine)\"]:::table\n    sharded_events[\"sharded_events table(ReplicatedReplacingMergeTree table engine)\"]:::table\n\n\n```kafka_events -.clickhouse_events_json topic.- Kafka\neventsmv --reads from--> kafka_events\neventsmv --pushes data to--> writable_events\nwritable_events -.pushes data to.-> sharded_events\n\nevents -.reads from.-> sharded_events\n```\n\n\n```\n`kafka_events` table\n`kafka_events` table uses the Kafka table engine\nTables using this engine set up Kafka consumers that consume data on read queries to the table, advancing the offset for the consumer group in Kafka.\n`events_mv` Materialized View\n`events_mv` table is a Materialized View.\nIn this case it acts as a data pipe which periodically pulls data from `kafka_events` and pushes it into the target (events) table.\n`writable_events` table\n`writable_events` table uses the Distributed table engine.\nThe schema looks something like as follows:\n`sql\nCREATE TABLE posthog.writable_events (\n    `uuid` UUID,\n    `event` String,\n    `properties` String,\n    `timestamp` DateTime64(6, 'UTC'),\n    `team_id` Int64,\n    `distinct_id` String,\n    `elements_hash` String,\n    `created_at` DateTime64(6, 'UTC'),\n    `_timestamp` DateTime,\n    `_offset` UInt64,\n    `elements_chain` String\n) ENGINE = Distributed('posthog', 'posthog', 'sharded_events', sipHash64(distinct_id))`\nThis table:\n\nGets pushed rows from `events_mv` table.\nFor every row, it calculates a hash based on the `distinct_id` column.\nBased on the hash, sends the row to the right shard on the `posthog` cluster into the `posthog.sharded_events` table.\nDoes not contain materialized columns as they would hinder INSERT queries.\n\n`sharded_events` table\n`sharded_events` table uses the ReplicatedReplacingMergeTree.\nThis table:\n\nStores the event data.\nIs sharded and replicated.\nIs queried indirectly via the `events` table.\n\n`events` table\nSimilar to `writable_events`, the `events` table uses the Distributed table engine.\nThis table is being queried from app and for every query, figures out what shard(s) to query and aggregates the results from shards.\nNote that even though the `ReplacingMergeTree` engine is used, we should avoid writing duplicate data into the table, as deduplication is not a guarantee.\nPersons\nThe source of truth for person info and person to distinct_id mappings is in PostgreSQL, but to speed up queries we replicate it to ClickHouse. Both tables use the ReplacingMergeTree and collapse by the `version` column, which is incremented every time a person is updated.\nNote that querying both tables requires handling duplicated rows. Check out PersonQuery code for an example of how it's done.",
    "tag": "posthog"
  },
  {
    "title": "Capture API",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/how-posthog-works/ingestion-pipeline.mdx",
    "content": "\ntitle: Ingestion pipeline\nIn its simplest form, the PostHog ingestion pipeline is a collection of services which listen for events as they are sent in, process them, and then store them for later analysis.\nThis document gives an overview of how data ingestion works, as well as some of the caveats to be aware of when sending events to PostHog.\n```mermaid\ngraph TD\n    CLIENT[Client Library]\n    DECIDE[\"/decide API\"]\n    CAPTURE[Capture API]\n    PLUGINS[Plugin server]\n    PERSONS[\"PostgreSQL (persons table)\"]\n    Kafka2[Kafka]\n\n\n```CLIENT -..-> DECIDE\n\nCLIENT -..-> CAPTURE\nCAPTURE --> Kafka\n\nKafka <-..- PLUGINS\n\nPLUGINS <--> PERSONS\nPLUGINS --> Kafka2\n\nKafka2 <-..- ClickHouse\n```\n\n\n```\nCapture API\nThe Capture API represents the user-facing side of the ingestion pipeline, and is exposed as a number of API routes where events can be sent.\nBefore an event reaches the Ingestion pipeline, there are a couple of preliminary checks and actions that we perform so that we can return a response immediately to the client.\nThese consist of:\n\nValidating API keys\nAnonymizing IPs according to project settings\nDecompressing and normalizing the shape of event data for the rest of the system\nSending processed data to `events_plugin_ingestion` Kafka topic\nIf any of these operations fail (other than checking for data validity), logging events to the Kafka `dead_letter_queue` table\n\nThe goal of this step is to be as simple as possible, so that we can reliably get events into the ingestion pipeline, where Kafka can persist them until they are able to be processed.\nEvents are written to the `events_plugin_ingestion` Kafka topic, which is then consumed by the plugin-server.\nPlugin server\nWithin the plugin server events go through a number of different steps here is an overview:\n```mermaid\nflowchart TD\n    STEP1[\"plugin-server - Event buffer\"]\n    STEP2[\"Apps - processEvent\"]\n    STEP3[\"plugin-server - Person processing\"]\n    STEP4[\"plugin-server - Event processing\"]\n    STEP5[\"plugin-server - Writing to ClickHouse\"]\n    STEP6[\"Apps - onEvent\"]\n    APPS1[/\"GeoIP\"/]\n    APPS3[/\"Property flattener\"/]\n    APPS2[/\"BigQuery Export\"/]\n    APPS4[/\"Snowflake Export\"/]\n    PERSONS[(\"PostgreSQL\")]\n    KafkaIn{{\"Kafka - events_plugin_ingestion\"}}\n    KafkaOut{{\"Kafka - clickhouse_events_json\"}}\n\n\n```KafkaIn --> STEP1\n\nSTEP1 --> STEP2\nSTEP1 --delay 60s--> STEP2\nSTEP2 --> STEP3\nSTEP3 --> STEP4\nSTEP4 --> STEP5\nSTEP5 --> STEP6\n\nSTEP2 <--> APPS1\nSTEP2 <--> APPS3\nSTEP6 --> APPS2\nSTEP6 --> APPS4\n\nSTEP5 --> KafkaOut\n\nSTEP3 <--> PERSONS\n```\n\n\n```\nIn the sections below we will dive deeper into each step:\n\nEvent buffer\nApps - processEvent\nPerson processing\nEvent processing\nWriting to ClickHouse\nApps - onEvent\n\nIf you would like to dive even deeper the related source code can be found here.\n1. Event buffer\nThe event buffer sits right at the beginning of the ingestion pipeline, and gives us the ability to selectively delay the processing of certain events. For more information, take a look at the all about the event buffer section in the appendix.\n2. Apps - `processEvent`\nAfter the event buffer, we start the first of a few steps that augment or transform our raw event before it gets written into ClickHouse.\nThis first step runs any workloads that come from Apps that you have installed and who have exported a `processEvent` function.\nThis is the only chance for apps to transform or exclude an event before it is written into ClickHouse.\nAn example of an app that uses the `processEvent` hook is the GeoIP Enricher. This app uses the `$ip` property to retrieve and add geographic information to each event as they are ingested.\n3. Person processing\nThe next step in the ingestion pipeline is processing the Person who sent the event, which is determined by the `distinct_id` field.\nA number of different actions can happen here depending both on if we've seen this `distinct_id` before, as well as which type of event is being sent.\nThis is one of the most complex steps in the entire pipeline, so to help make things easier we'll break down this step into a number of smaller sections:\n\nAssociate the event with a person\n`$identify` events\n`$create_alias` events\nAll other events\n\n\nUpdate person properties\n\nNote that in case there were any changes to persons we will update the persons info in ClickHouse too.\n1. Associate the event with a person\nBased on which type of event is currently being processed, we perform a number of different steps.\n3.1.1 - `$identify` events\nIn the case of an `$identify` event, the first step is to use the `$distinct_id` and `$anon_distinct_id` fields that are sent with the event to determine what actions we will need to take.\n\n`$anon_distinct_id` - The UUID associated with the client device that sent the event (Only included for events sent from client-side libraries)\n`$distinct_id` - The distinct identifier for whichever user sent the event (Email, UUID, etc.). This can be set by the sender or is defaulted to `$anon_distinct_id` if it is not set\n\nTo determine what to do at this stage, we need to make a call to PostgreSQL to determine which scenario we are in:\n|                                                                                            |                                                                                                                    |\n| ------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------ |\n| 1. Neither `$anon_distinct_id` nor `$distinct_id` have been associated with a Person       | Create a new Person and add a mapping in PostgreSQL to associate this `$distinct_id` with the new `person_id`      |\n| 2. Only one of `$anon_distinct_id` and `$distinct_id` have been associated with a Person | Create a new mapping to associate the `$distinct_id` and `$anon_distinct_id` with the already existing `person_id` |\n| 3. Both `$anon_distinct_id` and `$distinct_id` have been associated with a Person      | We will merge these two people and associate all future events with the `person_id` that was associated with the `$distinct_id` |\n\nNote: In the case the `$anon_distinct_id` is missing (e.g. events from backend libraries), we will treat this event like all other events.\n\nMerging two Persons\nIn the third scenario, where we have inadvertently created two Persons for the same user, we will need to merge them. Note that PostHog has a few built-in protections, in which case the merge will not aborted. (more info). \nIn the case of an `$identify` call, we will merge the person tied to `$anon_distinct_id` (`person_2`) into the person identified by `distinct_id` (`person_1`). \nThis means that we'll associate `$anon_distinct_id` with `person_1`, delete `person_2` and all future events for `$anon_distinct_id` will be associated with `person_1`.\nIf there are any conflicts when merging Person properties for these two persons, the values from the non-anonymous person (`person_1`) will take precedence.\nWe choose to prioritize the history of the non-anonymous person (`person_1`), as it is far more likely that this person will have a history of previous events associated with the user that we want to preserve.\nFor more information on exactly how the merging of properties is done, check out this overview of user properties.\nConsequences of merging\nThis scenario typically isn't ideal, as merging two users will only affect events that are sent in the future.\nAs a result, we will be left with two separate `person_id`'s in the events table that should be associated with the same user, but instead will be treated as entirely unique users.\nThe only way to fully 'merge' these two people would be to go back and rewrite the `person_id` field on past events, which is not practical or performant.\nFor more information on how these merges can affect the results of your queries, take a look at this page on how querying data works.\nAvoiding this scenario is the primary goal of the event buffer: by buffering certain events, we can avoid creating a duplicate Person when other events arrive in suboptimal order from the user's initial sign-up flow.\n3.1.2 - `$create_alias` events\nThe process of handling `$create_alias` events is almost identical to the process for `$identify` events, except that instead of merging `$anon_distinct_id` into `$distinct_id`, we allow you to pass in two arbitrary `$distinct_id`'s you would like to combine and merge the second one (`alias`) into `distinct_id`.\n3.1.3 - All other events\nFor all other types of events, the process is much more straightforward.\nIf we have determined that this is a new `$distinct_id`, then we will create a new Person within PostgreSQL and associate them with this `$distinct_id`. Otherwise, we will retrieve the person associated with this `$distinct_id`.\n3.2. Update person properties\nNow, once we have finished determining the Person who is associated with the event we are processing, we can finish by updating their properties within PostgreSQL.\nThis step takes into account any `$set`, `$set_once` or `$unset` arguments provided on the event, and merges these with any existing values for the Person.\nFor more information on exactly how this updating is done, check out this overview of user properties.\n4. Event processing\nFinally, now that we have our event and person all ready, we perform a few last processing steps before we write the event to ClickHouse.\nThis is our last chance to change anything about the event, which can include:\n\nAdding group properties if the event has been assigned to a Group\nAnonymizing IPs, if needed\n\n5. Writing to ClickHouse\nWe combine the fully-processed event and the person from Step 3 and send it to a separate Kafka topic that ClickHouse will consume from and then write to the events table.\nFor more information on exactly how data is stored in ClickHouse, check out this reference\n6. Apps - `onEvent`\nThe final step in the ingestion pipeline is calling the `onEvent` handler from any apps that we have enabled. This includes all of our export apps as well as some of our alerting/monitoring apps.\nIt's worth noting that since this event has already been written to ClickHouse, it is effectively immutable at this point as we do not allow apps to directly update events. Any apps that need to transform events should use the `processEvent` handler.\nAppendix\nAll about the event buffer\nDetermining if an event should be buffered\nAfter an event is consumed from Kafka, the plugin server will check a number of things in order to determine whether or not to buffer an event.\nIf any of these conditions are true the event will not be buffered and will immediately move on to the next step.\n\nIf the `distinct_id` on the event is already associated with an existing Person\nIf the event is anonymous (In this case this means the `distinct_id` matches the `device_id` generated by the library that sent the event)\nIf the event is an `$identify` or `$create_alias` call that merges ids (note that before version 1.42 non merging events were also processed immediately) \nIf the event is coming from one of our mobile libraries, as it is not easy to determine if an event is anonymous and we don't want to delay all events from mobile\n\nIf an event coming in satisfies none of these checks, then it will be added to the buffer and sent back through the ingestion pipeline after 60s.\nSince this means that events would be delayed, our goal is to use the buffer as sparingly as possible.\nChoosing to use the event buffer is a trade-off between events arriving quickly and events being associated the desired person.\nFor more detail on this step, check out this file from the plugin-server codebase.\nWhy are events buffered?\nSince events coming into PostHog can arrive in a suboptimal order, we sometimes decide to delay the processing of specific events.\nThere are two scenarios where events can arrive in suboptimal order that the event buffer is designed to handle:\n\nEvents that are sent from the same library and session may arrive out of order due to network uncertainty\nDuring the initial signup flow the backend event might arrive before frontend has identified the user\n\nIt will become more obvious why it is better to buffer events in these two cases as we dive deeper into the example below.\nLet's look at a initial sign-up flow (identify event is used to sign-up/log-in).\nIn the case that we're sending a signup event from the backend, this event will typically arrive before the identify event on the frontend.\nIf we did not use the buffer, here is the events table we would see.\n| ID  | Event          | `distinct_id`                         | person_id | Details                                |\n| --- | -------------- | ------------------------------------- | --------- | -------------------------------------- |\n| 1   | pageview       | `anon-1`                              | `user-1`  | New person with id `user-1` is created |\n| 2   | backend signup | `Alice`                               | `user-2`  | New person with id `user-2` is created |\n| 3   | identify       | `Alice` (anon_distinct_id = `anon-1`) | `user-2`  | Merge `user-1` into `user-2`           |\nAs you can see, we end up with two different `person_id`'s in the events table for the same user. See the impact this has on queries here.\nNow, let's take a look at how the event buffer can help to prevent this.\n| ID  | Event          | `distinct_id`                         | person_id | Details                                                  |\n| --- | -------------- | ------------------------------------- | --------- | -------------------------------------------------------- |\n| 1   | pageview       | `anon-1`                              | `user-1`  | New person with id `user-1` is created                   |\n| 3   | identify       | `Alice` (anon_distinct_id = `anon-1`) | `user-1`  | distinct_id `Alice` is associated with `user-1`          |\n| 2   | backend signup | `Alice`                               | `user-1`  | This event is now processed last because it was buffered |\nAs you can see, since we delayed the processing of event `2`, we were able to avoid creating an unnecessary Person.\nA similar scenario can occur when events with non-anonymous `distinct_id` arrive before an identify call due to network uncertainty.\nThe buffer is designed to help with the initial sign-up flow, and has no effect on events that are sent after this point.\nAs an example, let's take a look at the following series of events:\n| ID  | Event    | `distinct_id`                         | `person_id` | Details                                                            |\n| --- | -------- | ------------------------------------- | ----------- | ------------------------------------------------------------------ |\n| 1   | pageview | `anon-1`                              | `user-1`    | New person with id `user-1` is created                             |\n| 2   | identify | `Alice` (anon_distinct_id = `anon-1`) | `user-1`    | distinct_id `Alice` is associated with `user-1`                    |\n| 3   | pageview | `anon-2`                              | `user-2`    | Imagine this was the same user opening the page in incognito mode. |\n| 4   | identify | `Alice` (anon_distinct_id = `anon-2`) | `user-1`    | Alice logs in. Merge `user-1` into `user-2`                        |",
    "tag": "posthog"
  },
  {
    "title": "Event",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/how-posthog-works/data-model.mdx",
    "content": "\ntitle: Data model\nThis document provides a high-level overview of the various objects and primitives that make up the PostHog data model. For more information on exactly how data is stored in our database, check out the ClickHouse overview.\nThe two most-basic entities in PostHog are the Event and Person objects, and represent the core of our analytics functionalities.\nEvent\nAn event is the most important object in PostHog, and represents a single action that a user performed at a specific point in time. These events are sent either from one of our libraries or directly via our API.\nEach event contains the following base fields within ClickHouse:\n| Column                | Type                   | Description                                                                                                                                                     |\n| --------------------- | ---------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| uuid              | `UUID`                 | ID of the event                                                                                                                                                 |\n| team_id           | `Int64`                | Foreign key which links to the team                                                                                                                             |\n| event             | `VARCHAR`              | Name of the event                                                                                                                                               |\n| distinct_id       | `VARCHAR`              | The unique or anonymous id of the user that triggered the event.                                                                                                |\n| properties        | `VARCHAR`              | Any key: value pairs in a dict.- `$current_url` - we use this in a couple of places (like /paths, /events) as the url the user was visiting at that time. |\n| elements_*      | Various                | Columns used for `$autocapture` to track which DOM element was clicked on                                                                                       |\n| timestamp         | `DateTime64(6, 'UTC')` | Defaults to timezone.now at ingestion time if not set                                                                                                           |\n| created_at        | `DateTime64(6, 'UTC')` | The timestamp for when the event was ingested                                                                                                                   |\n| person_id         | `UUID`                 | This is the `id` of the Person that sent this event                                                                                                  |\n| person_created_at | `DateTime64(3)`        | The timestamp of the earliest event associated with this person                                                                                                 |\n| person_properties | `VARCHAR`              | A JSON object with all the properties for a user, which can be altered using the `$set`, `$set_once`, and `$unset` arguments                                    |\n| group*           | Various                | Columns used for group analytics                                                                                                     |\nEvents are only stored within ClickHouse, and once they have been written they can't be changed.\nThis limitation comes from a trade-off in the design of ClickHouse: inserting data and running queries on large tables is extremely fast, but updating or deleting specific rows is generally not efficient.\nPerson\nIn PostHog, a `Person` is an entity which sends events, and typically represents a 'User' in most implementations.\nEach person contains the following base fields within PostgreSQL:\n| Column         | Type          | Description                                                                                                                  |\n| -------------- | ------------- | ---------------------------------------------------------------------------------------------------------------------------- |\n| id         | `integer`     | Sequential ID for the person                                                                                                 |\n| team_id    | `integer`     | Foreign key which links to the team                                                                                          |\n| uuid       | `UUID`        | UUID of the person within ClickHouse. This is referenced by the `person_id` field on events                                  |\n| created_at | `timestamptz` | The timestamp of the earliest event associated with this person                                                              |\n| properties | `jsonb`       | A JSON object with all the properties for a user, which can be altered using the `$set`, `$set_once`, and `$unset` arguments |\n| version    | `bigint`      | Incremented every time a person is updated. Helps to keep ClickHouse and PostgreSQL in sync.                                 |\nPersons are stored in PostgreSQL, but are additionally replicated into ClickHouse for certain queries. For example, when viewing the global list of Persons from the dashboard, this information is retrieved from ClickHouse.\nAdditionally, person properties are also stored directly on each event. Their value is determined during ingestion by looking up the Person who sent the event in PostgreSQL, and combining these values with any updates from the event itself.\nThe `properties` field on each Person object can be updated at any time, and as a result the PostgreSQL table represents the one source of truth for the most up-to-date values for the properties of a Person.",
    "tag": "posthog"
  },
  {
    "title": "What data is protected under HIPAA?",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/privacy/hipaa-compliance.md",
    "content": "\ntitle: PostHog & HIPAA compliance\nsidebarTitle: PostHog & HIPAA\nsidebar: Docs\nshowTitle: true\n\nHIPAA is the Health Insurance Portability and Accountability Act. It\u2019s a piece of legislation that applies to certain covered entities operating in the United States of America (e.g. healthcare providers).\nA key goal of this legislation is to \u201cassure that individuals\u2019 health information is properly protected while allowing the flow of health information needed to provide and promote high quality health care and to protect the public's health and well being.\u201d \nIn other words, it stops anyone from using or sharing a persons data improperly.\nThe consequences of violating HIPAA are severe. It can lead to fines of over $1M and prison sentences of up to 10 years for the most egregious violations.\nWhat data is protected under HIPAA?\nData which is protected under HIPAA is called Protected Health Information (PHI), or ePHI if it exists specifically in electronic format. It includes any identifying information related to a past, present or future health status. That includes individual diagnoses, medical test results and prescription info, as well as birthdays, gender, ethnicity and contact information.\nIn short, any information which is tied to a specific individual can be considered PHI, from their social security number or license plate number to photos, emails, URLs or formal medical information. \nWhat is the impact of HIPAA on product analytics?\nMost product analytics tools require you to send your captured user data to a third-party system where the data is stored outside of your control. This is a problem under HIPAA, but there are two common ways to remain compliant:\n\n\nAnonymize the data: This involves either removing all traces of protected health information, including but not limited to email addresses, phone numbers, IP addresses, URLs etc., or following an expert determination to limit the data shared in such a way that the statistical risk of identifying an individual is mitigated\n\n\nSign a Business Associate Agreement (BAA): This is essentially a contract with your provider that ensures they are compliant and jointly liable for the protection of your data.\n\n\nThere are downsides to these two solutions:\n\n\nAnonymization: You can easily limit the data so much that it becomes meaningless and makes it impossible to perform standard and critical analyses of your product and users. There's no point reducing the data to an unusable state\n\n\nBusiness Associate Agreement: Business Associate Agreements are often expensive and/or require you to pay for a higher tier of product than you actually require.\n\n\nPostHog offers a third approach without either of these downsides: hosting the product analytics systems yourself.\nHow to set PostHog up for HIPAA compliant analytics\nPostHog enables you to self-host on your own infrastructure and maintain full control of the data. This means you don't need to anonymize the data, nor do you need to set up a Business Associate Agreement with PostHog because you never need to send any Protected Health Information (PHI) to us in the first place. The data stays on your systems, in its original form. \nYou may need to sign a BAA with your hosting provider, but major providers such as Google and AWS offer these for free.\nStep 1: Choose a hosting provider\nWe recommend hosting PostHog on your own infrastructure. If you\u2019re leveraging a private cloud you will need a Business Associate Agreement with your provider first. These are commonly and easily available with services such as Amazon Web Services, Google Cloud Platform, Microsoft Azure and many more, often for free.\nStep 2: Deploy PostHog\nDeploying PostHog onto your own infrastructure is straightforward and we provide support to solve any issues you encounter. You can follow our standard deployment guides to get started, or arrange a demo to see it in action first.\nStep 3: Security configuration\nWhen setting up a PostHog instance we strongly recommend that you use HTTPS to secure data in transmission, whether or not your instance has access to the wider internet. We also have a guide for securing PostHog which you should follow to further protect your instance.\nWe also strongly recommend that you limit access to PostHog and the infrastructure it is deployed on only to people who are authorized and need to access the data, including shared dashboard links. Although aggregate data in dashboards should not contain PHI, it may be possible for malicious users to infer PHI unless it is evaluated thoroughly via expert determination.\nFinally, we advise caution when installing, building and enabling apps for your PostHog instance. Apps are a great way to share and augment data from your instance with other systems, but it\u2019s essential to ensure you have the proper controls (e.g. BAA, anonymization or self-hosting) in place when sharing PHI outside of your self-hosted PostHog instance.\nDoes PostHog offer a BAA for PostHog Cloud?\nWe believe the most effective solution to HIPAA-compliant product analytics is to control the data yourself. That's why we recommend using the self-hosted versions of PostHog. As such, we do not offer a Business Associate Agreement (BAA) for PostHog Cloud - we recommend you self host PostHog instead. \nFurther reading\n\nA simple guide to personal data & PII\n",
    "tag": "posthog"
  },
  {
    "title": "What data is protected under GDPR?",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/privacy/gdpr-compliance.md",
    "content": "\ntitle: PostHog & GDPR compliance\nsidebarTitle: PostHog & GDPR\nsidebar: Docs\nshowTitle: true\n\nThe General Data Protection Regulation (GDPR) is a privacy and security law, drafted and passed by the European Union (EU). It imposes obligations onto organizations anywhere, so long as they target or collect data related to people in the EU.\nWe recommend that you read the full text of the GDPR and seek independent legal advice regarding your obligations. The consequences of violating GDPR are severe.\nIf you require robust GDPR compliance, we recommend using PostHog Cloud EU \u2013 a managed version of PostHog that's hosted on servers based in Frankfurt.\nWhat data is protected under GDPR?\nPersonal data is protected under GDPR, which means any information that relates to an individual who can be directly or indirectly identified. Names and email addresses are obviously personal data. Location information, ethnicity, gender, biometric data, religious beliefs, web cookies, and political opinions can also be personal data.\nWhat is the impact of GDPR on product analytics?\nThe number one rule is don\u2019t collect, store or use any personal data without a good reason for it, such as:\n\n\nThe person gave you specific, unambiguous consent to process the data (e.g. they\u2019ve opted in to your marketing email list)\n\n\nProcessing is necessary to enter into a contract to someone (e.g. you need to do a background check)\n\n\nYou need to process it to comply with a legal obligation of yours (e.g. you receive an order from the court in your jurisdiction)\n\n\nYou need to process the data to save somebody\u2019s life (e.g. well, you\u2019ll probably know when this one applies)\n\n\nProcessing is necessary to perform a task in the public interest or to carry out some official function (e.g. you\u2019re a private garbage collection company)\n\n\nYou have a legitimate interest to process someone\u2019s personal data. This is the most flexible lawful basis, though the \u201cfundamental rights and freedoms of the data subject\u201d always override your interests, especially if it\u2019s a minor's data\n\n\nYou must acquire \"Unambiguous Consent\"\nThere are specific rules about what consent means; hiding it away on page 73 or of your terms and conditions is not good enough:\n\n\nConsent must be \u201cfreely given, specific, informed and unambiguous\u201d\n\n\nRequests for consent must be \u201cclearly distinguishable from the other matters\u201d and presented in \u201cclear and plain language\u201d\n\n\nData subjects can withdraw previously given consent whenever they want, and you have to honor their decision\n\n\nChildren under 13 can only give consent with permission from their parent\n\n\nYou need to keep documentary evidence of consent\n\n\nSo, if you're tracking users in your product using PostHog to improve your product, you should explicitly ask for consent to use this data and explain exactly how you will use it when users sign up for your service.\nIf you use PostHog with cookies on your website (for logged out users), you should also use a cookie banner to enable people to give and withdraw their consent for using cookies.\nData must be handled securely\nYou\u2019re required to handle data securely by implementing \u201cappropriate technical and organizational measures.\u201d\nThis means both technical measures (like encrypting data) and organizational measures (like staff training and limiting access to personal data).\nIf you have a data breach, you have 72 hours to tell the data subjects or face penalties. (This notification requirement may be waived if you use technological safeguards, such as encryption, to render data useless to an attacker.)\nYou should not transfer EU users' personal data outside the EU\nIf you are self-hosting PostHog on a server outside the EU and are collecting EU user data, you should anonymize any of those users' personal data. \nIf you are using PostHog Cloud in any country, we also recommend you anonymize any EU user data, as PostHog Cloud is hosted in the US. \nThe PostHog Property Filter app allows you to anonymize user data to ensure you stay compliant with GDPR in both cases. \nHow to set PostHog up for GDPR compliance\nGDPR requirements differ depending on how your business interacts with personal data. Companies can be data controllers, data processors, or both a controller and a processor. Data controllers collect their end users\u2019 data and decide why and how it is processed. Data processors are businesses instructed to process customer data on behalf of other businesses.\nYou will be using PostHog in one of two ways:\n\nHosted and managed by us on PostHog Cloud\nSelf-hosted by you on a private cloud or your own infrastructure\n\nIf you are using PostHog Cloud then PostHog is the Data Processor and you are the Data Controller.\nIf you are self-hosting PostHog then you are both the Data Processor and the Data Controller because you are responsible for your PostHog instance.\nStep 1: Choose a hosting provider\nWe recommend using PostHog Cloud EU for GDPR compliance, though you can use PostHog Cloud (US) if you follow additional steps to protect user data. If self-hosting, the steps will depend on where you're hosting your data. \nStep 2: Deploy PostHog\nIf using PostHog Cloud EU, simply follow the steps in the onboarding process to start sending events. Read our integration documentation for more information on sending events to PostHog. \nDeploying PostHog onto your own infrastructure is straightforward and we provide support to help with any issues you encounter. You can follow our standard deployment guides to get started, or arrange a demo to see it in action first.\nStep 3: Security configuration\nWhen setting up a PostHog instance we strongly recommend that you use HTTPS to secure data in transmission, whether or not your instance has access to the wider internet. We also have a guide for securing PostHog which you should follow to further protect your instance.\nWe also strongly recommend that you limit access to PostHog and the infrastructure it is deployed on only to people who are authorized and need to access the data, including shared dashboard links. Although aggregate data in dashboards should not contain personal data, it may be possible for malicious users to infer personal data unless it is evaluated thoroughly via expert determination.\nFinally, we advise caution when installing, building and enabling plugins for your PostHog instance. Apps are a great way to share and augment data from your instance with other systems, but it\u2019s essential to ensure you have the proper controls in place when sharing personal data outside of your self-hosted PostHog instance.\nStep 4: Configure consent\nSince PostHog automatically captures data which can be personal data, you must provide a mechanism for the consensual capturing of that data. In the GDPR, this is called the right to be informed.\nWithin the consent you should identify the types of personal data that are being processed and what tools are being used to process them:\n\nIf you are using PostHog Cloud you should identify PostHog as a tool \nIf you are self-hosting you can either not list a tool or provide a generic description such as \"Product Analytics\".\n\nIf a user opts out then you must stop data capturing and processing. Here are some ways PostHog makes this possible:\n\n\nIf posthog-js has been initialized, call `posthog.opt_out_capturing()`. See the posthog-js docs\n\n\nEnsure posthog-js is configured not to auto-capture and do not make capture calls using the installed PostHog SDK on any client\n\n\nDo not load the posthog-js SDK. If you do this you should ensure your application logic always performs conditional checks for the availability of the PostHog SDK. This may not be possible in modern JavaScript applications.\n\n\nDo not initialize the posthog-js SDK via the call to `init`. If you do this you should ensure your application logic always performs conditional checks regarding the initialization state of the PostHog SDK.\n\n\n\nCookieless Tracking: It is possible to use PostHog without tracking cookies. In this mode, PostHog doesn't create permanent user profiles. Read How to use PostHog without cookie banners\n\nStep 5: Enable the Property Filter app (optional)\nIf you are self-hosting PostHog outside the EU, or are using PostHog Cloud, and are capturing EU users' data, you should enable the Property Filter app. This will allow you to anonymize user data. \nComplying with 'right to be forgotten' requests\nA user must be able to request that their data be removed from PostHog. How you facilitate that request is up to you. For example, you could accept requests via email or form submission.\nYou can remove a user from a PostHog instance via the PostHog user interface. To do this:\n\nSelect Persons from the left-hand menu\nSearch for the person via their unique ID. For example, their email\nClick view next to the person within the search results\nClick Delete this person to remove them and all their associated data from the PostHog instance. You will be prompted to confirm this action.\n\nFurther reading\n\nA simple guide to personal data and PII\nBuilding a tracking cookies opt out banner in React\n",
    "tag": "posthog"
  },
  {
    "title": "How to delete data from PostHog",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/privacy/data-deletion.md",
    "content": "\ntitle: Data deletion\nsidebarTitle: Data deletion\nsidebar: Docs\nshowTitle: true\n\nHow to delete data from PostHog\nOne can remove unwanted data from posthog by:\n1. Deleting teams/organizations\n2. Deleting persons\nThis can be done either under (1) the settings pages , (2) under persons & groups pages, or (3) using the API.\nWhen deleting individual persons, you can also choose to delete all of their events. When deleting teams, all data under the team\n(including events) are automatically removed.\nHow to delete persons and related events using the API\nPersons and events can be deleted using our API endpoints.\nTo query all persons in your project, use the GET Persons API endpoint.\nYou can filter for specific subsets of persons using the query parameters. For example, you can get a specific person by filtering by email:\n`GET https://posthog.example.com/api/projects/{YOUR_PROJECT_ID}/persons?email={EMAIL}`\nYou can paginate through the results using the `next` and `previous` parameters returned by this endpoint.\nTo delete the persons and their events, you can use the DELETE Persons API endpoint\nTo do that, iterate through the persons returned in the previous step, and use the person's `id` or `distinct_id` in the DELETE call. To delete the person's corresponding events in addition to the person, add a `delete_events=true` parameter. For example:\n`GET https://posthog.example.com/api/projects/{YOUR_PROJECT_ID}/persons/{ID}?delete_events=true`\nAsynchronous data deletion\nWhile most data in PostHog is deleted instantly, event data is not. Instead data is cleared asynchronously during non-peak usage times (weekends on PostHog Cloud).\nThis is done because data deletion in ClickHouse is expensive and it can impact performance for other users.",
    "tag": "posthog"
  },
  {
    "title": "Frequently asked questions",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/privacy/index.md",
    "content": "\ntitle: Privacy compliance\nsidebarTitle: Overview\nsidebar: Docs\nshowTitle: true\n\nPostHog offers considerable flexibility in hosting and configuration options to comply with privacy regulations around the world.\nIn these guides, we offer advice for using PostHog in a compliant manner under the following legal frameworks:\n\n\nThe General Data Protection Regulation (GDPR), which applies to all businesses collecting data on EU citizens\n\n\nThe Health Insurance Portability and Accountability Act (HIPAA), which applies to businesses capturing and processing health data in the US\n\n\nThe California Consumer Privacy Act (CCPA), which applies to qualifying for-profit businesses collecting personal information on residents of California\n\n\n\nPlease note: these guides do not constitute legal advice. We recommend seeking professional advice to ensure you remain compliant with relevant legislation.\n\nFrequently asked questions\nThis overview covers some frequently asked questions about PostHog and privacy. Have a question not covered here? Use the 'Ask a question' box at the bottom of the page.\nWhat is and isn't considered personal data?\nIt's hard to have a single legal definition of personal data because every legal privacy framework has different ideas, and even names, for it. The GDPR calls it 'personal data' but the US uses the term 'personally identifiable information' (PII) and others refer to it as 'personal information'.\nAccording to the GDPR, personal data is any information which: \n\nIdentifies a 'data subject' directly\nCan be used to identify a 'data subject' when combined with other information\n\nRead our simple guide to personal data and PII for more specific examples to help you identify what personal data you are collecting.\nHow does the GDPR impact analytics?\nThere are three key GDPR principles that impact your use PostHog and analytics in general:\n\nYou need to have a good reason to collect personal data\nYou need to acquire unambiguous consent\nData must be handled securely\n\nOur guide to personal data provides an overview of what's considered personal data under the GDPR, but suffice it to say that its definition is broad.\nIs PostHog GDPR compliant?\nWe have in-depth GDPR guidance documentation for advice on deploying PostHog in a GDPR-compliant way, including how to configure GDPR consent in PostHog and complying with 'right to be forgotten' requests.\nWe also offer PostHog Cloud EU \u2013 a managed version of PostHog with servers hosted in Frankfurt, ensuring user data never leaves EU jurisdiction. \nCan I use PostHog to collect user data under HIPAA?\nYes. You can self-host PostHog on your own infrastructure and maintain full control of your data, making it an excellent solution for analytics in healthcare settings. Because you maintain full control, you don't need to sign a Business Associate Agreement with us. Read our HIPAA guidance for more information.\nCan I use PostHog Cloud under HIPAA?\nNo. We believe self-hosting is the best solution for HIPAA compliance. Read our documentation for more on how to self-host PostHog.\nIs Google Analytics HIPAA compliant?",
    "tag": "posthog"
  },
  {
    "title": "What is the CCPA?",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/privacy/ccpa-compliance.md",
    "content": "\ntitle: PostHog & CCPA compliance\nsidebarTitle: PostHog & CCPA\nsidebar: Docs\nshowTitle: true\n\nIf you have users who are Californian residents, it's important to understand the implications of handling their data privately and securely. PostHog doesn't see any of your data and can be self-hosted on your existing infrastructure, making it one of the most CCPA-compliant product analytics platforms available. \nThis guide explains what the CCPA is, what data must be protected and what your options are for CCPA-compliant analytics. \nWhat is the CCPA?\nThe California Consumer Privacy Act of 2018 (CCPA) gives consumers control over the personal information that businesses collect about them:\n\nThe right to know about the personal information a business collects about them and how it is used and shared\nThe right to delete personal information collected from them (with some exceptions)\nThe right to opt-out of the sale of their personal information\nThe right to non-discrimination for exercising their CCPA rights.\n\nWhat data is protected under CCPA?\nOnly California residents have rights under the CCPA. A California resident is a person who resides in California, even if they temporarily outside of the state.\nThe CCPA applies to for-profit businesses that meet any (not all) of the following criteria:\n\nHave a gross annual revenue of over $25 million\nBuy, receive, or sell the personal information of 50,000 or more California residents, households, or devices\nDerive 50% or more of their annual revenue from selling California residents\u2019 personal information.\n\nPersonal information is protected under CCPA and is considered information that identifies or could be linked with a person.\nIn short, any information which is tied to a specific individual can be considered Personal Information, from their social security number or license plate number to photos, emails, URLs, IP addresses or even pseudonyms.\nWhat is the impact of CCPA on product analytics?\nThe CCPA requires businesses to give consumers certain information in a \u201cnotice at collection\u201d. This means that, when people sign-up to use your product, you need to explain how you intend to use their data to improve the product for them.\nA notice at collection must list the categories of personal information businesses collect about consumers and the purposes for which they use the categories of information. The notice must also contain a link to the business\u2019s privacy policy, where consumers can get more details on your privacy practices.\nThe right to delete personal information is also covered by CCPA. You have 45 days to respond to a request from a user to delete any personal information stored about them.\nHow to set PostHog up for CCPA compliance\nPostHog enables you to self-host your analytics on your own infrastructure and maintain full control of the data \u2013 i.e. you decide how and where to host any personal information. It also means you have full control of the underlying database, making it possible for you to easily share any information stored about an individual or delete any personal data. \nStep 1: Choose a hosting provider\nWe recommend hosting PostHog on your own infrastructure, or a private cloud such as AWS, Google Cloud Platform or Microsoft Azure.\nStep 2: Deploy PostHog\nDeploying PostHog onto your own infrastructure is straightforward and we provide support to help with any issues you encounter. You can follow our standard deployment guides to get started, or arrange a demo to see it in action first.\nStep 3: Security configuration\nWhen setting up a PostHog instance we strongly recommend that you use HTTPS to secure data in transmission, whether or not your instance has access to the wider internet. We also have a guide for securing PostHog which you should follow to further protect your instance.\nWe also strongly recommend that you limit access to PostHog and the infrastructure it is deployed on only to people who are authorized and need to access the data, including shared dashboard links. Although aggregate data in dashboards should not contain personal data, it may be possible for malicious users to infer personal data unless it is evaluated thoroughly via expert determination.\nFinally, we advise caution when installing, building and enabling apps for your PostHog instance. Apps are a great way to share and augment data from your instance with other systems, but it\u2019s essential to ensure you have the proper controls in place when sharing personal data outside of your self-hosted PostHog instance.\nDeleting personal information in PostHog\nIf a user request the deletion of their information under the CCPA, complete the following steps:\n\nSelect Persons from the left-hand menu\nSearch for the person via their unique ID. For example, their email\nClick view next to the person within the search results\n",
    "tag": "posthog"
  },
  {
    "title": "What data does PostHog collect from self-hosted instances?",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/privacy/egress.md",
    "content": "\ntitle: Data egress from self-hosted instances\nsidebarTitle: Egress and compliance\nsidebar: Docs\nshowTitle: true\n\nWhat data does PostHog collect from self-hosted instances?\nIn order to understand usage, build better products, and bill customers accurately, PostHog instances routinely send usage reports to our servers. In addition, this data helps us troubleshoot when we provide support. These reports do not contain any raw person, event, or group data \u2014 ie no identifiable or unique information from your users, only anonymized, aggregated data. \nWhat if I cannot allow data egress or need to run in an air gapped environment?\nWe are committed to supporting businesses whose user data is subject to privacy regulations, or whose compliance postures dictate that internally generated data be isolated from the internet. For customers that need our paid features, we offer the ability to run PostHog in air gapped environments, or without sending this status ping, on the Enterprise plan. \nWhat data is sent in the usage report?\nThe following is an example status report that would be sent from your instance to PostHog's servers.\n```json\n{\n    \"id\": \"01808211-23a9-0000-354b-a5da0697932b\",\n    \"timestamp\": \"2022-05-02T00:00:02.293000+00:00\",\n    \"event\": \"user instance status report\",\n    \"distinct_id\": \"VY1tssVQViiUWdVY1tssVQViiUWdVY1tssVQViiUWd\",\n    \"properties\": {\n        \"$geoip_city_name\": \"London\",\n        \"$geoip_continent_code\": \"EU\",\n        \"$geoip_continent_name\": \"Europe\",\n        \"$geoip_country_code\": \"GB\",\n        \"$geoip_country_name\": \"United Kingdom\",\n        \"$geoip_latitude\": 51.5368,\n        \"$geoip_longitude\": -0.6718,\n        \"$geoip_postal_code\": \"SL1\",\n        \"$geoip_subdivision_1_code\": \"ENG\",\n        \"$geoip_subdivision_1_name\": \"England\",\n        \"$geoip_time_zone\": \"Europe/London\",\n        \"$ip\": \"165.22.112.27\",\n        \"$lib\": \"posthog-python\",\n        \"$lib_version\": \"1.4.4\",\n        \"$plugins_deferred\": [\n            \"Hubspot (2788)\"\n        ],\n        \"$plugins_failed\": [],\n        \"$plugins_succeeded\": [\n            \"GeoIP (80)\",\n            \"Clearbit Enhance User (2911)\",\n            \"Property Flattener Plugin (93)\",\n            \"First Time Event Tracker (7272)\"\n        ],\n        \"$set\": {\n            \"$geoip_city_name\": \"London\",\n            \"$geoip_country_name\": \"United Kingdom\",\n            \"$geoip_country_code\": \"GB\",\n            \"$geoip_continent_name\": \"Europe\",\n            \"$geoip_continent_code\": \"EU\",\n            \"$geoip_postal_code\": \"SL1\",\n            \"$geoip_latitude\": 51.5368,\n            \"$geoip_longitude\": -0.6718,\n            \"$geoip_time_zone\": \"Europe/London\",\n            \"$geoip_subdivision_1_code\": \"ENG\",\n            \"$geoip_subdivision_1_name\": \"England\"\n        },\n        \"$set_once\": {\n            \"$initial_geoip_city_name\": \"London\",\n            \"$initial_geoip_country_name\": \"United Kingdom\",\n            \"$initial_geoip_country_code\": \"GB\",\n            \"$initial_geoip_continent_name\": \"Europe\",\n            \"$initial_geoip_continent_code\": \"EU\",\n            \"$initial_geoip_postal_code\": \"SL1\",\n            \"$initial_geoip_latitude\": 51.5368,\n            \"$initial_geoip_longitude\": -0.6718,\n            \"$initial_geoip_time_zone\": \"Europe/London\",\n            \"$initial_geoip_subdivision_1_code\": \"ENG\",\n            \"$initial_geoip_subdivision_1_name\": \"England\"\n        },\n        \"clickhouse_version\": \"21.6.5\",\n        \"deployment\": \"helm_do_ha\",\n        \"helm\": {\n            \"chart_version\": \"18.2.3\",\n            \"cloud\": \"do\",\n            \"deployment_type\": \"helm\",\n            \"hostname\": \"playground.posthog.net\",\n            \"ingress_type\": \"nginx\",\n            \"kube_version\": \"v1.22.7\",\n            \"operation\": \"upgrade\",\n            \"release_name\": \"posthog\",\n            \"release_revision\": 9\n        },\n        \"helm__chart_version\": \"18.2.3\",\n        \"helm__cloud\": \"do\",\n        \"helm__deployment_type\": \"helm\",\n        \"helm__hostname\": \"playground.posthog.net\",\n        \"helm__ingress_type\": \"nginx\",\n        \"helm__kube_version\": \"v1.22.7\",\n        \"helm__operation\": \"upgrade\",\n        \"helm__release_name\": \"posthog\",\n        \"helm__release_revision\": 9,\n        \"instance_usage_summary\": {\n            \"events_count_new_in_period\": 0,\n            \"persons_count_new_in_period\": 0,\n            \"persons_count_total\": 52113,\n            \"events_count_total\": 1627639,\n            \"dashboards_count\": 10,\n            \"ff_count\": 1,\n            \"using_groups\": true\n        },\n        \"instance_usage_summary__dashboards_count\": 10,\n        \"instance_usage_summary__events_count_new_in_period\": 0,\n        \"instance_usage_summary__events_count_total\": 1627639,\n        \"instance_usage_summary__ff_count\": 1,\n        \"instance_usage_summary__persons_count_new_in_period\": 0,\n        \"instance_usage_summary__persons_count_total\": 52113,\n        \"instance_usage_summary__using_groups\": true,\n        \"is_first_event_in_session\": true,\n        \"license_keys\": [\n            \"BGQQQQQQQQQQQQQQQQQQQQQQQQQQqqqqq\"\n        ],\n        \"license_keys__0\": \"BGQQQQQQQQQQQQQQQQQQQQQQQQQQqqqqq\",\n        \"period\": {\n            \"start_inclusive\": \"2022-04-25T00:00:00+00:00\",\n            \"end_inclusive\": \"2022-05-01T23:59:59.999999+00:00\"\n        },\n        \"period__end_inclusive\": \"2022-05-01T23:59:59.999999+00:00\",\n        \"period__start_inclusive\": \"2022-04-25T00:00:00+00:00\",\n        \"plugins_enabled\": {\n            \"GeoIP\": 37,\n            \"Avo\": 1\n        },\n        \"plugins_enabled__Avo\": 1,\n        \"plugins_enabled__GeoIP\": 37,\n        \"plugins_installed\": {\n            \"GeoIP\": 38,\n            \"job\": 1,\n            \"both servers\": 1,\n            \"onEvent\": 1,\n            \"Avo\": 2,\n            \"Avo New\": 1,\n            \"THE CRASH\": 1\n        },\n        \"plugins_installed__Avo\": 2,\n        \"plugins_installed__Avo New\": 1,\n        \"plugins_installed__GeoIP\": 38,\n        \"plugins_installed__THE CRASH\": 1,\n        \"plugins_installed__both servers\": 1,\n        \"plugins_installed__job\": 1,\n        \"plugins_installed__onEvent\": 1,\n        \"posthog_version\": \"1.35.0\",\n        \"realm\": \"hosted-clickhouse\",\n        \"scope\": \"user\",\n        \"site_url\": \"https://playground.posthog.net\",\n        \"table_sizes\": {\n            \"posthog_event\": 57344,\n            \"posthog_sessionrecordingevent\": 49152\n        },\n        \"table_sizes__posthog_event\": 57344,\n        \"table_sizes__posthog_sessionrecordingevent\": 49152,\n        \"teams\": {\n            \"2\": {\n                \"events_count_total\": 1616800,\n                \"events_count_new_in_period\": 0,\n                \"events_count_by_lib\": {\n                    \"web\": 370048,\n                    \"null\": 1246752\n                },\n                \"events_count_by_name\": {\n                    \"$autocapture\": 210240,\n                    \"$exception\": 11935,\n                    \"$identify\": 31362,\n                    \"Lead\": 33,\n                    \"$pageleave\": 16593,\n                    \"$pageview\": 122770\n                },\n                \"duplicate_distinct_ids\": {\n                    \"prev_total_ids_with_duplicates\": 0,\n                    \"prev_total_extra_distinct_id_rows\": 0,\n                    \"new_total_ids_with_duplicates\": 0,\n                    \"new_total_extra_distinct_id_rows\": 0\n                },\n                \"multiple_ids_per_person\": {\n                    \"total_persons_with_more_than_2_ids\": 0,\n                    \"max_distinct_ids_for_one_person\": 0\n                },\n                \"group_types_count\": 3,\n                \"persons_count_total\": 50350,\n                \"persons_count_new_in_period\": 0,\n                \"dashboards_count\": 1,\n                \"dashboards_template_count\": 0,\n                \"dashboards_shared_count\": 0,\n                \"dashboards_tagged_count\": 0,\n                \"ff_count\": 1,\n                \"ff_active_count\": 1\n            },\n            \"35\": {\n                \"events_count_total\": 0,\n                \"events_count_new_in_period\": 0,\n                \"events_count_by_lib\": {},\n                \"events_count_by_name\": {},\n                \"duplicate_distinct_ids\": {\n                    \"prev_total_ids_with_duplicates\": 0,\n                    \"prev_total_extra_distinct_id_rows\": 0,\n                    \"new_total_ids_with_duplicates\": 0,\n                    \"new_total_extra_distinct_id_rows\": 0\n                },\n                \"multiple_ids_per_person\": {\n                    \"total_persons_with_more_than_2_ids\": 0,\n                    \"max_distinct_ids_for_one_person\": 0\n                },\n                \"group_types_count\": 0,\n                \"persons_count_total\": 0,\n                \"persons_count_new_in_period\": 0,\n                \"dashboards_count\": 1,\n                \"dashboards_template_count\": 0,\n                \"dashboards_shared_count\": 0,\n                \"dashboards_tagged_count\": 0,\n                \"ff_count\": 0,\n                \"ff_active_count\": 0\n            },\n            \"36\": {\n                \"events_count_total\": 10839,\n                \"events_count_new_in_period\": 0,\n                \"events_count_by_lib\": {},\n                \"events_count_by_name\": {},\n                \"duplicate_distinct_ids\": {\n                    \"prev_total_ids_with_duplicates\": 0,\n                    \"prev_total_extra_distinct_id_rows\": 0,\n                    \"new_total_ids_with_duplicates\": 0,\n                    \"new_total_extra_distinct_id_rows\": 0\n                },\n                \"multiple_ids_per_person\": {\n                    \"total_persons_with_more_than_2_ids\": 0,\n                    \"max_distinct_ids_for_one_person\": 0\n                },\n                \"group_types_count\": 0,\n                \"persons_count_total\": 1763,\n                \"persons_count_new_in_period\": 0,\n                \"dashboards_count\": 3,\n                \"dashboards_template_count\": 0,\n                \"dashboards_shared_count\": 0,\n                \"dashboards_tagged_count\": 0,\n                \"ff_count\": 0,\n                \"ff_active_count\": 0\n            }\n        },\n        \"teams__2__dashboards_count\": 1,\n        \"teams__2__dashboards_shared_count\": 0,\n        \"teams__2__dashboards_tagged_count\": 0,\n        \"teams__2__dashboards_template_count\": 0,\n        \"teams__2__duplicate_distinct_ids__new_total_extra_distinct_id_rows\": 0,\n        \"teams__2__duplicate_distinct_ids__new_total_ids_with_duplicates\": 0,\n        \"teams__2__duplicate_distinct_ids__prev_total_extra_distinct_id_rows\": 0,\n        \"teams__2__duplicate_distinct_ids__prev_total_ids_with_duplicates\": 0,\n        \"teams__2__events_count_by_lib\": {},\n        \"teams__2__events_count_by_name\": {},\n        \"teams__2__events_count_new_in_period\": 0,\n        \"teams__2__events_count_total\": 1616800,\n        \"teams__2__ff_active_count\": 1,\n        \"teams__2__ff_count\": 1,\n        \"teams__2__group_types_count\": 3,\n        \"teams__2__multiple_ids_per_person__max_distinct_ids_for_one_person\": 0,\n        \"teams__2__multiple_ids_per_person__total_persons_with_more_than_2_ids\": 0,\n        \"teams__2__persons_count_new_in_period\": 0,\n        \"teams__2__persons_count_total\": 50350,\n        \"teams__35__dashboards_count\": 1,\n        \"teams__35__dashboards_shared_count\": 0,\n        \"teams__35__dashboards_tagged_count\": 0,\n        \"teams__35__dashboards_template_count\": 0,\n        \"teams__35__duplicate_distinct_ids__new_total_extra_distinct_id_rows\": 0,\n        \"teams__35__duplicate_distinct_ids__new_total_ids_with_duplicates\": 0,\n        \"teams__35__duplicate_distinct_ids__prev_total_extra_distinct_id_rows\": 0,\n        \"teams__35__duplicate_distinct_ids__prev_total_ids_with_duplicates\": 0,\n        \"teams__35__events_count_by_lib\": {},\n        \"teams__35__events_count_by_name\": {},\n        \"teams__35__events_count_new_in_period\": 0,\n        \"teams__35__events_count_total\": 0,\n        \"teams__35__ff_active_count\": 0,\n        \"teams__35__ff_count\": 0,\n        \"teams__35__group_types_count\": 0,\n        \"teams__35__multiple_ids_per_person__max_distinct_ids_for_one_person\": 0,\n        \"teams__35__multiple_ids_per_person__total_persons_with_more_than_2_ids\": 0,\n        \"teams__35__persons_count_new_in_period\": 0,\n        \"teams__35__persons_count_total\": 0,\n        \"teams__36__dashboards_count\": 3,\n        \"teams__36__dashboards_shared_count\": 0,\n        \"teams__36__dashboards_tagged_count\": 0,\n        \"teams__36__dashboards_template_count\": 0,\n        \"teams__36__duplicate_distinct_ids__new_total_extra_distinct_id_rows\": 0,\n        \"teams__36__duplicate_distinct_ids__new_total_ids_with_duplicates\": 0,\n        \"teams__36__duplicate_distinct_ids__prev_total_extra_distinct_id_rows\": 0,\n        \"teams__36__duplicate_distinct_ids__prev_total_ids_with_duplicates\": 0,\n        \"teams__36__events_count_by_lib\": {},\n        \"teams__36__events_count_by_name\": {},\n        \"teams__36__events_count_new_in_period\": 0,\n        \"teams__36__events_count_total\": 10839,\n        \"teams__36__ff_active_count\": 0,\n        \"teams__36__ff_count\": 0,\n        \"teams__36__group_types_count\": 0,\n        \"teams__36__multiple_ids_per_person__max_distinct_ids_for_one_person\": 0,\n        \"teams__36__multiple_ids_per_person__total_persons_with_more_than_2_ids\": 0,\n        \"teams__36__persons_count_new_in_period\": 0,\n        \"teams__36__persons_count_total\": 1763,\n        \"users_who_logged_in\": []\n    },\n    \"elements_chain\": \"\"\n}",
    "tag": "posthog"
  },
  {
    "title": "Installation",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/sdks/go/index.mdx",
    "content": "\ntitle: Go\nsidebarTitle: Go\nsidebar: Docs\nshowTitle: true\ngithub: https://github.com/PostHog/posthog-go\nicon: ../../../images/docs/integrate/go.svg\nfeatures:\n    eventCapture: true\n    userIdentification: true\n    autoCapture: false\n    sessionRecording: false\n    featureFlags: true\n    groupAnalytics: true\n\nimport CohortExpansionSnippet from '../_snippets/cohort-expansion'\nThis library uses an internal queue to make calls fast and non-blocking. It also batches requests and flushes asynchronously, making it perfect to use in any part of your web app or other server-side application that needs performance.\nInstallation\n`go\ngo get github.com/posthog/posthog-go`\nUsage\n```go\npackage main\nimport (\n    \"os\"\n    \"github.com/posthog/posthog-go\"\n)\nfunc main() {\n    client, _ := posthog.NewWithConfig(\n        os.Getenv(\"POSTHOG_API_KEY\"),\n        posthog.Config{\n            Endpoint:       \"\",\n            PersonalApiKey: \"your personal API key\", // needed for feature flags\n        },\n    )\n    defer client.Close()\n\n\n```// run commands\n```\n\n\n}\n```\nMaking calls\nCapture\nCapture allows you to capture anything a user does within your system, which you can later use in PostHog to find patterns in usage, work out which features to improve, or find out where people are giving up.\nA `capture` call requires:\n\n`distinct id` which uniquely identifies your user\n`event name` to specify the event\nWe recommend naming events with \"[noun] [verb]\", such as `movie played` or `movie updated`, in order to easily identify what your events mean later on (we know this from experience).\n\n\n\nOptionally you can submit:\n\n`properties`, which can be an array with any information you'd like to add\n\nFor example:\n`go\nclient.Enqueue(posthog.Capture{\n  DistinctId: \"test-user\",\n  Event:      \"test-snippet\",\n  Properties: posthog.NewProperties().\n    Set(\"plan\", \"Enterprise\").\n    Set(\"friends\", 42),\n})`\nSetting user properties via an event\nTo set properties on your users via an event, you can leverage the event properties `$set` and `$set_once`.\n$set\nExample\n`go\nclient.Enqueue(posthog.Capture{\n  DistinctId: \"test-user\",\n  Event:      \"test-snippet\",\n  Properties: map[string]interface{}{\n        \"eventProp\":    \"value1\",\n        \"$set\": map[string]interface{}{\n            \"userProp\": \"value2\",\n        },\n    }\n})`\nUsage\nWhen capturing an event, you can pass a property called `$set` as an event property, and specify its value to be an object with properties to be set on the user that will be associated with the user who triggered the event.\n$set_once\nExample\n`go\nclient.Enqueue(posthog.Capture{\n  DistinctId: \"test-user\",\n  Event:      \"test-snippet\",\n  Properties: map[string]interface{}{\n        \"eventProp\":    \"value1\",\n        \"$set_once\": map[string]interface{}{\n            \"userProp\": \"value2\",\n        },\n    }\n})`\nUsage\n`$set_once` works just like `$set`, except that it will only set the property if the user doesn't already have that property set.\nIdentify\n\nWe highly recommend reading our section on Identifying users to better understand how to correctly use this method.\n\nIdentify lets you add metadata to your users so you can easily identify who they are in PostHog, as well as do things\nlike segment users by these properties.\nAn identify call requires:\n\n`distinct id` which uniquely identifies your user\n`properties` with a dictionary of any key:value pairs you'd like to add\n\nFor example:\n`go\nclient.Enqueue(posthog.Identify{\n  DistinctId: \"user:123\",\n  Properties: posthog.NewProperties().\n    Set(\"email\", \"john@doe.com\").\n    Set(\"proUser\", false),\n})`\nThe most obvious place to make this call is whenever a user signs up, or when they update their information.\nAlias\nTo connect whatever a user does before they sign up or login with what they do after, you need to make an alias call. This will allow you to answer questions like \"Which marketing channels lead to users churning after a month?\" or \"What do users do on our website before signing up?\"\nIn a purely back-end implementation, this means whenever an anonymous user does something, you'll want to send a session ID with the capture call. Then, when that users signs up, you want to do an alias call with the session ID and the newly created user ID.\nThe same concept applies for when a user logs in.\nIf you're using PostHog in the front-end and back-end, doing the identify call in the frontend will be enough.\nAn alias call requires\n\n`DistinctId` \u2013 the user id\n`Alias` \u2013 the anonymous session distinct ID\n\nFor example:\n`go\nclient.Enqueue(posthog.Alias{\n  DistinctId: \"user:123\",\n  Alias: \"session:12345\",\n})`\nSending page views\nIf you're aiming for a full back-end implementation of PostHog, you can send page views from your backend, like so:\n`go\nclient.Enqueue(posthog.Capture{\n  DistinctId: \"test-user\",\n  Event:      \"$pageview\",\n  Properties: posthog.NewProperties().\n    Set(\"$current_url\", \"https://example.com\"),\n})`\nFeature flags\n\nNote that to use feature flags you must specify `PersonalApiKey` in the options passed to posthog.NewWithConfig.\n\nHow to check if a flag is enabled\n\nNote: Whenever we face an error computing the flag, the library returns `undefined`, instead of `true`, `false`, or a string variant value.\n\n```go\n// IsFeatureEnabled(FeatureFlagPayload) (interface{}, error)\nisFlagEnabledForUser, err := client.IsFeatureEnabled(\n        FeatureFlagPayload{\n            Key:        \"flag-key\",\n            DistinctId: \"distinct-id\",\n        },\n    )\nif (isFlagEnabledForUser) {\n  // Do something differently for this user\n}\n```\n\nIf your feature flag relies entirely on rollout percentage (i.e. it has no filters), `isFeatureEnabled` will provide a fast response, allowing it to be used in the logic for API endpoints, for example. Flags that depend on filters require a call to the PostHog API so will take longer.\nNote: Whenever we face an error computing the flag, the library returns `nil`, instead of `true` or `false` or a string variant value.\n\nGet a flag value\nIf you're using multivariate feature flags, you can also get the value of the flag, as well as whether or not it is enabled.\n\nNote: Whenever we face an error computing the flag, the library returns `None`, instead of `true` or `false` or a string variant value.\n\n`go\n// GetFeatureFlag(FeatureFlagPayload) (interface{}, error)\nenabledVariant, err := client.GetFeatureFlag(\n        FeatureFlagPayload{\n            Key:        \"multivariate-flag\",\n            DistinctId: \"distinct-id\",\n        },\n)`\nOverriding server properties\nSometimes, you might want to evaluate feature flags using properties that haven't been ingested yet, or were set incorrectly earlier. You can do so by setting properties the flag depends on with these calls.\nFor example, if the `beta-feature` depends on the `is_authorized` property, and you know the value of the property, you can tell PostHog to use this property, like so:\n`go\nenabledVariant, err := client.GetFeatureFlag(\n        FeatureFlagPayload{\n            Key:        \"multivariate-flag\",\n            DistinctId: \"distinct-id\",\n      PersonProperties: posthog.NewProperties().\n        Set(\"is_authorized\", true),\n        },\n)`\nThe same holds for groups. if you have a group name `organisation`, you can add properties like so:\n`go\nenabledVariant, err := client.GetFeatureFlag(\n        FeatureFlagPayload{\n            Key:        \"multivariate-flag\",\n            DistinctId: \"distinct-id\",\n      Groups:          Groups{\"organisation\": \"some-company\"},\n            GroupProperties: map[string]Properties{\"organisation\": NewProperties().Set(\"is_authorized\", true)},\n        },\n)`\nGetting all flag values\nYou can also get all known flag values as well. This is useful when you want to seed a frontend client with initial known flags. Like all methods above, this also takes optional person and group properties, if known.\n`go\nfeatureVariants, _ := client.GetAllFlags(FeatureFlagPayloadNoKey{\n        DistinctId: \"distinct-id\",\n})`\nLocal Evaluation\n\nNote: This feature requires version 2.0 of the library, which in turn requires a minimum PostHog version of 1.38\n\nAll feature flag evaluation requires an API request to your PostHog servers to get a response. However, where latency matters, you can evaluate flags locally. You must know all person or group properties the flag depends on.\nThe method call looks just like above\n`go\nenabledVariant, err := client.GetFeatureFlag(\n        FeatureFlagPayload{\n            Key:        \"multivariate-flag\",\n            DistinctId: \"distinct-id\",\n      PersonProperties: posthog.NewProperties().\n        Set(\"is_authorized\", true),\n        },\n)`\nThis works for `getAllFlags` as well. It evaluates all flags locally if possible. If even one flag isn't locally evaluable, it falls back to decide.\n`go\nfeatureVariants, _ := client.GetAllFlags(FeatureFlagPayloadNoKey{\n        DistinctId: \"distinct-id\",\n})`\nRestricting evaluation to local only\nSometimes, performance might matter to you so much that you never want an HTTP request roundtrip delay when computing flags. In this case, you can set the OnlyEvaluateLocally parameter to true, which tries to compute flags only with the properties it has. If it fails to compute a flag, it returns None, instead of going to PostHog's servers to get the value.\n\nReloading feature flags\nWhen initializing PostHog, you can configure the interval at which feature flags are polled (fetched from the server). However, if you need to force a reload, you can use `ReloadFeatureFlags`:\n```go\nclient.ReloadFeatureFlags()\n// Do something with feature flags here\n```\nGroup analytics\nGroup analytics allows you to associate an event with a group (e.g. teams, organizations, etc.). Read the Group Analytics guide for more information.\n\nNote:  This is a paid feature and is not available on the open-source or free cloud plan. Learn more here.\n\n\nSend an event associated with a group\n\n`go\nclient.Enqueue(posthog.Capture{\n    DistinctId: \"[distinct id]\",\n    Event:      \"some event\",\n    Groups: posthog.NewGroups().\n        Set(\"company\", \"42dlsfj23f\").\n})`\n\nUpdate properties on a group\n\n`go\nclient.Enqueue(posthog.GroupIdentify{\n    Type: \"company\",\n    Key:  \"42dlsfj23f\",\n    Properties: posthog.NewProperties().\n        Set(\"name\", \"Awesome Inc.\").\n        Set(\"employees\", 11),\n})`\nThe `name` is a special property which is used in the PostHog UI for the name of the Group. If you don't specify a `name` property, the group ID will be used instead.\nThank you",
    "tag": "posthog"
  },
  {
    "title": "Installation",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/sdks/react-native/index.mdx",
    "content": "\ntitle: React Native\nsidebarTitle: React Native\nsidebar: Docs\nshowTitle: true\ngithub: https://github.com/PostHog/posthog-react-native\nicon: ../../../images/docs/integrate/react.svg\nfeatures:\n    eventCapture: true\n    userIdentification: true\n    autoCapture: true\n    sessionRecording: false\n    featureFlags: true\n    groupAnalytics: true\n\nPurely built for React Native, this library allows you to integrate PostHog with your React Native project. For React Native projects built with Expo, there are no mobile native dependencies outside of supported Expo packages.\nInstallation\nimport ReactNativeInstall from './_snippets/install.mdx'\n\nConfiguration\nimport ReactNativeConfigure from './_snippets/configure.mdx'\n\nUsage\nAutocapture\nAutocapture tracks these events:\n\nApplication Opened - once when the App is opened from a closed state\nApplication Became Active - when the App comes to the foreground (e.g. from the app switcher)\nApplication Backgrounded - when the App is sent to the background by the user\n$screen - when the user navigates (if using `@react-navigation/native` or `react-native-navigation`)\n$autocapture - touch events when the user interacts with the screen\n\nWith Autocapture, all touch events for children of `PosthogProvider` will be tracked, capturing a snapshot of the view hierarchy at that point. This allows you to create Insights in PostHog without having to add many custom events.\nPostHog will try to generate a sensible name for the touched element based on the React component `displayName` or `name` but you can force this to something reliable (and also clearly marked for posthog tracking) using the `ph-label` prop.\n`jsx\n<View ph-label=\"my-special-label\"></View>`\nIf there are elements you don't want to be captured, you can add the `ph-no-capture` property like so. If this property is found anywhere in the view hierarchy, the entire touch event is ignored.\n`jsx\n<View ph-no-capture>Sensitive view here</View>`\nTracking Screen views and touches with `@react-navigation/native`:\n```jsx\n// App.(js|ts)\nimport { PostHogProvider } from 'posthog-react-native'\nimport { NavigationContainer } from '@react-navigation/native'\nexport function App() {\n    return (\n        \n\n                {/ Rest of app /}\n            \n\n    )\n}\n```\nTracking Screen views and touches with `react-native-navigation`:\n```jsx\nimport PostHog, { PostHogProvider } from 'posthog-react-native'\nimport { Navigation } from 'react-native-navigation';\nexport const posthogAsync = PostHog.initAsync('');\n// Simplify the wrapping of your Screens with a shared PostHogProvider\nexport const SharedPostHogProvider = (props: any) => {\n  return (\n    \n      {props.children}\n    \n  );\n};\nexport const MyScreen = () => {\n  return (\n    // Every screen needs to be wrapped with this provider if you want to capture touches or use the hook `usePostHog()`\n\n\n        ...\n      \n\n  );\n};\nNavigation.registerComponent('Screen', () => MyScreen);\nNavigation.events().registerAppLaunchedListener(async () => {\n  (await posthogAsync).initReactNativeNavigation({\n    navigation: {\n      // (Optional) Set the name based on the route. Defaults to the route name.\n      routeToName: (name, properties) => name,\n      // (Optional) Tracks all passProps as properties. Defaults to undefined\n      routeToProperties: (name, properties) => properties,\n    },\n  });\n});\n```\nAdditional configuration\nYou can tweak how autocapture works by passing custom options.\n```tsx\n<PostHogProvider apiKey=\"\" autocapture={{\n    captureTouches: true, // If you don't want to capture touch events set this to false\n    captureLifecycleEvents: true, // If you don't want to capture the Lifecycle Events (e.g. Application Opened) set this to false\n    captureScreens: true, // If you don't want to capture screen events set this to false\n    ignoreLabels: [], // Any labels here will be ignored from the stack in touch events\n    customLabelProp: \"ph-label\",\n    noCaptureProp: \"ph-no-capture\",\n    propsToCapture = [\"testID\"], // Limit which props are captured. By default, identifiers and text content are captured.\n\n\n```navigation: {\n    // By default only the Screen name is tracked but it is possible to track the\n    // params or modify the name by intercepting theautocapture like so\n    routeToName: (name, params) => {\n        if (params.id) return `${name}/${params.id}`\n        return name\n    },\n    routeToParams: (name, params) => {\n        if (name === \"SensitiveScreen\") return undefined\n        return params\n    },\n}\n```\n\n\n}}>\n    ...\n\n```\nIdentify\n\nWe highly recommend reading our section on Identifying users to better understand how to correctly use this method.\n\nimport ReactNativeIdentify from './_snippets/identify.mdx'\n\nCapture\nimport ReactNativeCapture from './_snippets/capture.mdx'\n\nSetting user properties via an event\nWhen capturing an event, you can pass a property called `$set` as an event property, and specify its value to be an object with properties to be set on the user that will be associated with the user who triggered the event.\n$set\nExample\n`js\nposthog.capture('some event', { $set: { userProperty: 'value' } })`\nUsage\nWhen capturing an event, you can pass a property called `$set` as an event property, and specify its value to be an object with properties to be set on the user that will be associated with the user who triggered the event.\n$set_once\nExample\n`js\nposthog.capture('some event', { $set_once: { userProperty: 'value' } })`\nUsage\n`$set_once` works just like `$set`, except that it will only set the property if the user doesn't already have that property set.\nSuper Properties\nSuper Properties are properties associated with events that are set once and then sent with every `capture` call, be it a $screen, an autocaptured touch, or anything else.\nThey are set using `posthog.register`, which takes a properties object as a parameter, and they persist across sessions.\nFor example, take a look at the following call:\n`js\nposthog.register({\n    'icecream pref': 'vanilla',\n    team_id: 22,\n})`\nThe call above ensures that every event sent by the user will include `\"icecream pref\": \"vanilla\"` and `\"team_id\": 22`. This way, if you filtered events by property using `icecream_pref = vanilla`, it would display all events captured on that user after the `posthog.register` call, since they all include the specified Super Property.\nHowever, please note that this does not store properties against the User, only against their events. To store properties against the User object, you should use `posthog.identify`. More information on this can be found on the Sending User Information section.\nRemoving stored Super Properties\nSuper Properties are persisted across sessions so you have to explicitly remove them if they are no longer relevant. In order to stop sending a Super Property with events, you can use `posthog.unregister`, like so:\n`js\nposthog.unregister('icecream pref'),`\nThis will remove the Super Property and subsequent events will not include it.\nIf you are doing this as part of a user logging out you can instead simply use `posthog.reset` which takes care of clearing all stored Super Properties and more.\nFlush\nYou can set the number of events in the configuration that should queue before flushing.\nSetting this to `1` will send events immediately and will use more battery. This is set to `20` by default.\nYou can also configure the flush interval. By default we flush all events after `30` seconds,\nno matter how many events have gathered.\nYou can also manually flush the queue, like so:\n`javascript\nposthog.flush()\n// or using async/await\nawait posthog.flushAsync()`\nReset\nTo reset the user's ID and anonymous ID, call `reset`. Usually you would do this right after the user logs out.\n`javascript\nposthog.reset()`\nOpt in/out\nBy default, PostHog has tracking enabled unless it is forcefully disabled by default using the option `{ enable: false }`.\nYou can give your users the option to opt in or out by calling the relevant methods. Once these have been called they are persisted and will be respected until optIn/Out is called again or the `reset` function is called.\nTo Opt in/out of tracking, use the following calls.\n`javascript\nposthog.optIn() // opt in\nposthog.optOut() // opt out`\nIf you still wish capture these events but want to create a distinction between users and team in PostHog, you should look into Cohorts.\nFeature Flags\nFeature Flags can be loaded directly or via a helpful hook\n```js\nimport { useFeatureFlag } from 'posthog-react-native'\nconst MyComponent = () => {\n    const showFlaggedFeature = useFeatureFlag('my-flag-id')\n    const multiVariantFeature = useFeatureFlag('my-multivariant-feature-flag-id')\n\n\n```if (showFlaggedFeature === undefined) {\n    // the response is undefined if the flags are being loaded\n    return null\n}\n\nreturn showFlaggedFeature ? <Text>Testing feature \ud83d\ude04</Text> : <Text>Not Testing feature \ud83d\ude22</Text>\n```\n\n\n}\n```\nAlternatively you can call load the feature flags directly:\n`js\n// Defaults to undefined if not loaded yet or if there was a problem loading\nposthog.getFeatureFlag('my-flag')\n// Multi variant feature flags are returned as a string\nposthog.getFeatureFlag('my-multivariant-flag')`\nPostHog will load feature flags when instantiated and will refresh whenever other methods are called that could affect the flag such as `.group()`. If you have the need to forcefully trigger the refresh however you can use `reloadFeatureFlagsAsync`\n`js\nposthog.reloadFeatureFlagsAsync().then((refreshedFlags) => console.log(refreshedFlags))`\nBootstrapping Flags\nThere is a delay between loading the library and feature flags becoming available to use. For some cases, like redirecting users to a different page based on a feature flag, this is extremely detrimental, as the flags load after the redirect logic occurs, thus never working.\nIn cases like these, where you want flags to be immediately available on page load, you can use the bootstrap library option.\nThis allows you to pass in a distinctID and feature flags during library initialisation, like so:\n`js\nconst posthog = await Posthog.initAsync('<ph_project_api_key>', {\n    host: 'https://app.posthog.com',\n    bootstrap: {\n        distinctID: 'your-anonymous-id',\n        featureFlags: {\n            'flag-1': true,\n            'variant-flag': 'control',\n            'other-flag': false,\n        },\n    },\n})`\nTo compute these flag values, use the corresponding `getAllFlags` method in your server-side library. Note that bootstrapping flags requires server-side initialisation.\nIf the ID you're passing in is an identified ID (that is, an ID with which you've called `posthog.identify()` elsewhere), you can also pass in the `isIdentifiedID` bootstrap option, which ensures that this ID is treated as an identified ID in the library. This is helpful as it warns you when you try to do something wrong with this ID, like calling `identify` again.\n`js\nconst posthog = await Posthog.initAsync('<ph_project_api_key>', {\n    host: 'https://app.posthog.com',\n    bootstrap: {\n        distinctID: 'your-identified-id',\n        isIdentifiedID: true,\n        featureFlags: {\n            'flag-1': true,\n            'variant-flag': 'control',\n            'other-flag': false,\n        },\n    },\n})`\n\nNote: Passing in a distinctID to bootstrap replaces any existing IDs, which means you may fail to connect any old anonymous user events with the logged in person, if your logic calls identify in the frontend immediately on login. In this case, you can omit passing in the distinctID.\n\nGroup analytics\nGroup analytics allows you to associate the events for that person's session with a group (e.g. teams, organizations, etc.). Read the Group Analytics guide for more information.\n\nNote:  This is a paid feature and is not available on the open-source or free cloud plan. Learn more here.\n\n\nAssociate the events for this session with a group\n\n```js\nposthog.group('company', '42dlsfj23f')\nposthog.capture('upgraded plan') // this event is associated with company ID `42dlsfj23f`\n```\n\nAssociate the events for this session with a group AND update the properties of that group\n\n`js\nposthog.group('company', '42dlsfj23f', {\n    name: 'Awesome Inc.',\n    employees: 11,\n})`\nThe `name` is a special property which is used in the PostHog UI for the name of the Group. If you don't specify a `name` property, the group ID will be used instead.\nSending screen views\nWith the `PostHogProvider`, screen tracking is automatically captured if the `autocapture` property is used. Currently only @react-navigation/native is supported by autocapture and it is important that the `PostHogProvider` is configured as a child of the `NavigationContainer` as below:\nSee autocapture for configuration.\nIf you want to manually send a new screen capture event, use the `screen` function. This function requires a `name`. You may also pass in an optional `properties` object.\n`javascript\nposthog.screen('Dashboard', {\n    background: 'blue',\n    hero: 'superhog',\n})`\nUpgrading from V1 to V2\nV1 of this library utilised the underlying `posthog-ios` and `posthog-android` SDKs to do most of the work. Since the new version is written entirely in JS, using only Expo supported libraries, there are some changes to the way PostHog is configured as well as actually calling PostHog.\nFor iOS, the new React Native SDK will attempt to migrate the previously persisted data which should result in no unexpected changes to tracked data.\nFor Android, it is unfortunately not possible for persisted Android data to be loaded which means stored information such as the randomly generated `anonymousId` or the `distinctId` set by `posthog.identify` will not be present. For identified users, the simple workaround is to ensure that `identify` is called at least once when the app loads. For anonymous users there is unfortunately no straightforward workaround they will show up as new anonymous users in PostHog.\n```jsx\n// DEPRECATED V1 Setup\nimport PostHog from 'posthog-react-native'\nawait PostHog.setup('phc_ChmcdLbC770zgl23gp3Lax6SERzC2szOUxtp0Uy4nTf', {\n    host: 'https://app.posthog.com',\n    captureApplicationLifecycleEvents: false, // Replaced by `PostHogProvider`\n    captureDeepLinks: false, // No longer supported\n    recordScreenViews: false, // Replaced by `PostHogProvider` supporting @react-navigation/native\n    flushInterval: 30, // Stays the same\n    flushAt: 20, // Stays the same\n    android: {...}, // No longer needed\n    iOS: {...}, // No longer needed\n})\nPostHog.capture(\"foo\")\n// V2 Setup difference\nimport PostHog from 'posthog-react-native'\nconst posthog = await Posthog.initAsync('phc_ChmcdLbC770zgl23gp3Lax6SERzC2szOUxtp0Uy4nTf', {\n    host: 'https://app.posthog.com',\n    // Add any other options here.\n})\n// Use created instance rather than the PostHog class\nposthog.capture(\"foo\")",
    "tag": "posthog"
  },
  {
    "title": "identify.mdx",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/sdks/react-native/_snippets/identify.mdx",
    "content": "When you start tracking events with PostHog, each user gets an anonymous ID that is used to identify them in the system.\nIn order to link this anonymous user with someone from your database, use the `identify` call. \nIdentify lets you add metadata on your users so you can more easily identify who they are in PostHog, and even do things \nlike segment users by these properties.\nAn identify call requires:\n\n`distinctId` which uniquely identifies your user in your database\n`userProperties` with a dictionary with key: value pairs\n\n`javascript\nposthog.identify('distinctID', {\n    email: 'user@posthog.com',\n    name: 'My Name'\n})`\nThe most obvious place to make this call is whenever a user signs up, or when they update their information.",
    "tag": "posthog"
  },
  {
    "title": "capture.mdx",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/sdks/react-native/_snippets/capture.mdx",
    "content": "Capture allows you to capture anything a user does within your system, which you can later use in PostHog to find \npatterns in usage, work out which features to improve or where people are giving up.\nA `capture` call requires:\n\n`event` to specify the event name\nWe recommend naming events with \"[noun] [verb]\", such as `movie played` or `movie updated`, in order to easily identify what your events mean later on (we know this from experience).\n\nOptionally you can submit:\n\n`properties`, which is an object with any information you'd like to add\n\nFor example:\n```javascript\nposthog.capture('Button B Clicked', {\n    color: \"blue\",\n    icon: \"new2-final\"\n})",
    "tag": "posthog"
  },
  {
    "title": "configure.mdx",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/sdks/react-native/_snippets/configure.mdx",
    "content": "With the PosthogProvider\nThe recommended flow for setting up PostHog React Native is to use the `PostHogProvider` which utilises the Context API to pass the PostHog Client around as well as enabling autocapture and ensuring that the queue is flushed at the right time:\n```jsx\n// App.(js|ts)\nimport { usePostHog, PostHogProvider } from 'posthog-react-native'\n...\nexport function MyApp() {\n    return (\n        ',\n        }}>\n            \n\n    )\n}\n// Now you can simply access posthog elsewhere in the app like so\nconst MyComponent = () => {\n    const posthog = usePostHog()\n\n\n```useEffect(() => {\n    posthog.capture(\"MyComponent loaded\", { foo: \"bar\" })\n}, [])\n```\n\n\n}\n```\nWithout the PosthogProvider\nDue to the Async nature of React Native, PostHog needs to be initialised asynchronously in order for the persisted state to be loaded properly. The `PosthogProvider` takes care of this under-the-hood but you can alternatively create the instance yourself like so:\n```tsx\n// posthog.ts\nimport PostHog from 'posthog-react-native'\nexport let posthog: PostHog | undefined = undefined\nexport const posthogAsync: Promise = PostHog.initAsync('', {\n  // PostHog API host (https://app.posthog.com by default)\n  host: ''\n})\nposthogAsync.then(client => {\n  posthog = client\n})\n// app.ts\nimport { posthog, posthogAsync} from './posthog'\nexport function MyApp1() {\n    useEffect(async () => {\n        // Use posthog optionally with the possibility that it may still be loading\n        posthog?.capture('MyApp1 loaded')\n        // OR use posthog via the promise\n        (await posthogAsync).capture('MyApp1 loaded')\n    }, [])\n\n\n```return <View>Your app code</View>\n```\n\n\n}\n// You can even use this instance with the PostHogProvider\nexport function MyApp2() {\n  return {/ Your app code /}\n}\n```\nConfiguration options\nFor most people, the default configuration options will work great but if you need to you can further customise how PostHog will work.\n```ts\nexport const posthog = await PostHog.initAsync(\"<ph_project_api_key>\", {\n    // PostHog API host\n    host?: string = \"https://app.posthog.com\",\n    // The number of events to queue before sending to PostHog (flushing)\n    flushAt?: number = 20,\n    // The interval in milliseconds between periodic flushes\n    flushInterval?: number = 10000\n    // If set to false, tracking will be disabled until`optIn`is called\n    enable?: boolean = true,\n    // Whether to track that`getFeatureFlag` was called (used by Expriements)\n    sendFeatureFlagEvent?: boolean = true,\n    // Whether to load feature flags when initialised or not\n    preloadFeatureFlags?: boolean = true,\n    // How many times we will retry HTTP requests\n    fetchRetryCount?: number = 3,\n    // The delay between HTTP request retries\n    fetchRetryDelay?: number = 3000,\n    // For Session Analysis how long before we expire a session\n    sessionExpirationTimeSeconds?: number = 1800 // 30 mins\n    // Whether to post events to PostHog in JSON or compressed format\n    captureMode?: 'json' | 'form'\n})",
    "tag": "posthog"
  },
  {
    "title": "Substitutes posthog.api_key which still exists but has been deprecated",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/sdks/python/index.mdx",
    "content": "\ntitle: Python\nsidebarTitle: Python\nsidebar: Docs\nshowTitle: true\ngithub: https://github.com/PostHog/posthog-python\nicon: ../../../images/docs/integrate/python.svg\nfeatures:\n    eventCapture: true\n    userIdentification: true\n    autoCapture: false\n    sessionRecording: false\n    featureFlags: true\n    groupAnalytics: true\n\nimport CohortExpansionSnippet from '../_snippets/cohort-expansion'\nThis is an optional library you can install if you're working with Python. It uses an internal queue to make calls fast and non-blocking. It also batches requests and flushes asynchronously, making it perfect to use in any part of your web app or other server side application that needs performance.\nInstallation\n`bash\npip install posthog`\nIn your app, import the `posthog` library and set your api key and host before making any calls.\n```python\nimport posthog\nSubstitutes posthog.api_key which still exists but has been deprecated\nposthog.project_api_key = ''\nOnly necessary if you want to use feature flags\nposthog.personal_api_key = ''\nYou can remove this line if you're using app.posthog.com\nposthog.host = ''\n```\nYou can read more about the differences between the project and personal API keys in the dedicated API authentication section of the Docs.\n\nNote: As a general rule of thumb, we do not recommend having API keys in plaintext. Setting it as an environment variable would be best.\n\nYou can find your keys in the 'Project Settings' page in PostHog.\nTo debug, you can toggle debug mode on:\n`python\nposthog.debug = True`\nAnd to make sure no calls happen during your tests, you can disable them, like so:\n`python\nif settings.TEST:\n    posthog.disabled = True`\nMaking Calls\nCapture\nCapture allows you to capture anything a user does within your system, which you can later use in PostHog to find patterns in usage, work out which features to improve or where people are giving up.\nA `capture` call requires:\n\n`distinct id` which uniquely identifies your user\n\n`event name` to specify the event\n\n\nWe recommend naming events with \"[noun] [verb]\", such as `movie played` or `movie updated`, in order to easily identify what your events mean later on (we know this from experience).\n\n\nOptionally you can submit:\n\n`properties`, which is a dictionary with any information you'd like to add\n`timestamp`, a datetime object for when the event happened. If this isn't submitted, it'll be set to the current time\n`uuid`, a unique `uuid` for the event, leave blank to autogenerate\n`send_feature_flags`, a boolean that determines whether to send current known feature flags with this event. This is useful when running experiments which depends on this event. However, this makes things slow. Read this tutorial for manually computing this information and speeding things up\n\nFor example:\n`python\nposthog.capture('distinct id', 'movie played', {'movie_id': '123', 'category': 'romcom'})`\nor\n`python\nposthog.capture('distinct id', event='movie played', properties={'movie_id': '123', 'category': 'romcom'}, timestamp=datetime.utcnow().replace(tzinfo=tzutc()))`\nSetting user properties via an event\nTo set properties on your users via an event, you can leverage the event properties `$set` and `$set_once`.\n$set\nExample\n`python\nposthog.capture(\n    'distinct id',\n    event='movie played',\n    properties={ '$set': { 'userProperty': 'value' } }\n)`\nUsage\nWhen capturing an event, you can pass a property called `$set` as an event property, and specify its value to be an object with properties to be set on the user that will be associated with the user who triggered the event.\n$set_once\nExample\n`python\nposthog.capture(\n    'distinct id',\n    event='movie played',\n    properties={ '$set_once': { 'userProperty': 'value' } }\n)`\nUsage\n`$set_once` works just like `$set`, except that it will only set the property if the user doesn't already have that property set.\nIdentify\n\nWe highly recommend reading our section on Identifying users to better understand how to correctly use this method.\n\nIdentify lets you add metadata to your users so you can easily identify who they are in PostHog, as well as do things\nlike segment users by these properties.\nAn `identify` call requires:\n\n`distinct id` which uniquely identifies your user\n`properties` with a dict with any key:value pairs\n\nFor example:\n`python\nposthog.identify('distinct id', {\n    'email': 'dwayne@gmail.com',\n    'name': 'Dwayne Johnson'\n})`\nThe most obvious place to make this call is whenever a user signs up, or when they update their information.\nAlias\nTo connect whatever a user does before they sign up or log in with what they do after you need to make an alias call. This will allow you to answer questions like \"Which marketing channels leads to users churning after a month?\" or \"What do users do on our website before signing up?\"\nIn a purely back-end implementation, this means whenever an anonymous user does something, you'll want to send a session ID (Django, Flask) with the capture call. Then, when that users signs up, you want to do an alias call with the session ID and the newly created user ID.\nThe same concept applies for when a user logs in.\nIf you're using PostHog in the front-end and back-end, doing the `identify` call in the frontend will be enough.\nAn `alias` call requires:\n\n`distinct id` \u2013 the user id\n`alias` \u2013 the anonymous session distinct ID\n\nFor example:\n`python\nposthog.alias('user:123', 'session:12345');`\nFeature flags\nPostHog's Feature Flags allow you to safely deploy and roll back new features.\nWhen using them with one of libraries, you should check if a feature flag is enabled and use the result to toggle functionality on and off in you application.\nHow to check if a flag is enabled\n\nNote: Whenever we face an error computing the flag, the library returns `None`, instead of `true`, `false`, or a string variant value.\n\n```python\nposthog.feature_enabled('beta-feature', 'distinct id')\nreturns True or False or None\n```\nExample use case\nHere's how you might send different users a different version of your homepage, for example:\n`python\ndef homepage(request):\n    template = \"new.html\" if posthog.feature_enabled('new_ui', 'distinct id') else \"old.html\"\n    return render_template(template, request=request)`\n\nNote: Feature flags are persistent for users across sessions. Read more about feature flag persistence on our dedicated page.\n\nGet a flag value\nIf you're using multivariate feature flags, you can also get not just whether the flag is enabled, but what value its enabled to.\n\nNote: Whenever we face an error computing the flag, the library returns `None`, instead of `true`, `false`, or a string variant value.\n\n```python\nposthog.get_feature_flag('beta-feature', 'distinct id')\nreturns string or True or False or None\n```\nOverriding server properties\nSometimes, you might want to evaluate feature flags using properties that haven't been ingested yet, or were set incorrectly earlier. You can do so by setting properties the flag depends on with these calls.\nFor example, if the `beta-feature` depends on the `is_authorized` property, and you know the value of the property, you can tell PostHog to use this property, like so:\n```python\nposthog.get_feature_flag('beta-feature', 'distinct id', person_properties={'is_authorized': True})\nreturns string or None\n```\nThe same holds for groups. If you have a group name `organisation`, you can add properties like so:\n```python\nposthog.get_feature_flag('beta-feature', 'distinct id', groups={'organisation': 'google'}, group_properties={'organisation': {'is_authorized': True})\nreturns string or None\n```\nGetting all flag values\nYou can also get all known flag values as well. This is useful when you want to seed a frontend client with initial known flags. Like all methods above, this also takes optional person and group properties, if known.\n```python\nposthog.get_all_flags('distinct id', groups={}, person_properties={'is_authorized': True}, group_properties={})\nreturns dict of flag key and value pairs.\n```\nLocal Evaluation\n\nNote: To enable local evaluation of feature flags you must also set a `personal_api_key` when configuring the integration, as described in the Installation section.\nNote: This feature requires version 2.0 of the library, which in turn requires a minimum PostHog version of 1.38\n\nAll feature flag evaluation requires an API request to your PostHog servers to get a response. However, where latency matters, you can evaluate flags locally. This is much faster, and requires two things to work:\n\nThe library must be initialised with a personal API key\nYou must know all person or group properties the flag depends on.\n\nThen, the flag can be evaluated locally. The method signature looks exactly like above\n```python\nposthog.get_feature_flag('beta-feature', 'distinct id', person_properties={'is_authorized': True})\nreturns string or None\n```\n\nNote: New feature flag definitions are polled every 30 seconds by default, which means there will be up to a 30 second delay between you changing the flag definition, and it reflecting on your servers. You can change this default on the client by setting `posthog.poll_interval = <value in seconds>`.\n\nThis works for `get_all_flags` as well. It evaluates all flags locally if possible. If even one flag isn't locally evaluable, it falls back to decide.\n```python\nposthog.get_all_flags('distinct id', groups={}, person_properties={'is_authorized': True}, group_properties={})\nreturns dict of flag key and value pairs.\n```\nRestricting evaluation to local only\nSometimes, performance might matter to you so much that you never want an HTTP request roundtrip delay when computing flags. In this case, you can set the `only_evaluate_locally` parameter to true, which tries to compute flags only with the properties it has. If it fails to compute a flag, it returns `None`, instead of going to PostHog's servers to get the value.\n\nGroup analytics\nGroup analytics allows you to associate an event with a group (e.g. teams, organizations, etc.). Read the Group Analytics guide for more information.\n\nNote:  This is a paid feature and is not available on the open-source or free cloud plan. Learn more here.\n\n\nCapture an event and associate it with a group\n\n`python\nposthog.capture('[distinct id]', 'some event', groups={'company': '42dlsfj23f'})`\n\nUpdate properties on a group\n\n`python\nposthog.group_identify('company', '42dlsfj23f', {\n    'name': 'Awesome Inc.',\n    'employees': 11\n})`\nThe `name` is a special property which is used in the PostHog UI for the name of the Group. If you don't specify a `name` property, the group ID will be used instead.\nSending page views\nIf you're aiming for a full back-end implementation of PostHog, you can send `pageviews` from your backend\n`python\nposthog.capture('distinct id', '$pageview', {'$current_url': 'https://example.com'})`\nDjango\nFor Django, you can do the initialisation of the key in the AppConfig, so that it's available everywhere.\nin `yourapp/apps.py`\n```python\nfrom django.apps import AppConfig\nimport posthog\nclass YourAppConfig(AppConfig):\n    def ready(self):\n        posthog.api_key = ''\n        posthog.host = '' # You can remove this line if you're using app.posthog.com\n```\nThen, anywhere else in your app you can do:\n```python\nimport posthog\ndef purchase(request):\n    # example capture\n    posthog.capture(request.user.pk, 'purchase', ...)\n```\nIntegrations\nSentry\nWhen using Sentry in Python, you can connect to PostHog in order to link Sentry errors to PostHog user profiles.\nExample implementation\nSee the sentry_django_example project for a complete example.\n```python\nimport sentry_sdk\nfrom sentry_sdk.integrations.django import DjangoIntegration\nfrom posthog.sentry.posthog_integration import PostHogIntegration\nPostHogIntegration.organization = \"orgname\"\nsentry_sdk.init(\n    dsn=\"https://examplePublicKey@o0.ingest.sentry.io/0\",\n    integrations=[DjangoIntegration(), PostHogIntegration()],\n)\nAlso set `posthog_distinct_id` tag\nfrom sentry_sdk import configure_scope\nwith configure_scope() as scope:\n    scope.set_tag('posthog_distinct_id', 'some distinct id')\n```\nExample implementation with Django\nThis can be made automatic in Django, by adding the following middleware and settings to `settings.py`:\n```python\nMIDDLEWARE = [\n    \"posthog.sentry.django.PosthogDistinctIdMiddleware\"\n]\nPOSTHOG_DJANGO = {\n    \"distinct_id\": lambda request: request.user and request.user.distinct_id\n}\n```\nAlternative name\nAs our open source project PostHog shares the same module name, we created a special `posthoganalytics` package, mostly for internal use to avoid module collision. It is the exact same.\nThank you",
    "tag": "posthog"
  },
  {
    "title": "Installation",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/sdks/elixir/index.mdx",
    "content": "\ntitle: Elixir\nsidebarTitle: Elixir\nsidebar: Docs\nshowTitle: true\ntags:\n    - community\ngithub: https://github.com/whitepaperclip/posthog\nicon: ../../../images/docs/integrate/elixir.svg\nfeatures:\n    eventCapture: true\n    userIdentification: true\n    autoCapture: false\n    sessionRecording: false\n    featureFlags: false\n    groupAnalytics: false\n\n\nThis library was built by the community and is not maintained by the PostHog core team.\n\nThis library provides an Elixir HTTP client for PostHog. See the repository for more information.\nInstallation\nThe package can be installed by adding `posthog` to your list of dependencies in `mix.exs`:\n`elixir\ndef deps do\n  [\n    {:posthog, \"~> 0.1\"}\n  ]\nend`\nConfiguration\n`elixir\nconfig :posthog,\n  api_url: \"<ph_instance_address>\",\n  api_key: \"<ph_project_api_key>\"`\nOptionally, you can pass in a `:json_library` key. The default JSON parser is Jason.\nUsage\nCapturing events:\n`elixir\nPosthog.capture(\"login\", distinct_id: user.id)`\nCapturing multiple events:\n`elixir\nPosthog.batch([{\"login\", [distinct_id: user.id], nil}])`\nThanks",
    "tag": "posthog"
  },
  {
    "title": "Why does this exist?",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/sdks/js/index.mdx",
    "content": "\ntitle: JavaScript\nsidebarTitle: JavaScript\nsidebar: Docs\nshowTitle: true\ngithub: https://github.com/PostHog/posthog-js\nicon: ../../../images/docs/integrate/js.svg\nfeatures:\n    eventCapture: true\n    userIdentification: true\n    autoCapture: true\n    sessionRecording: true\n    featureFlags: true\n    groupAnalytics: true\n\n\nNote: You can just use our snippet to start capturing events with our JS.\n\nThis page of the Docs refers to our posthog-js library.\nWhy does this exist?\nThe reason this exists is that whilst the default snippet captures every click on certain elements (like `a`, `button`, `input` etc.) and page views, it's often worth sending more context whenever a user does something. This might also be useful if you have a one page app.\nInstallation\nimport JSInstall from './_snippets/install.mdx'\n\nUsage\nAutocapture\nimport JSAutoCapture from './_snippets/autocapture.mdx'\nimport JSCaptureIgnoreElements from './_snippets/capture-ignore-elements.mdx'\n\nTuning autocapture\nYou can configure autocapture using allowlists. If an array is passed for an allowlist, autocapture events will only be sent for elements matching at least one of the items in the array.\nFor example, to only capture clicks on buttons on the docs section of the website that contain the data attribute `ph-autocapture` you can do:\n`js\nposthog.init('<ph_project_api_key>', {\n    api_host: '<ph_instance_address>',\n    autocapture: {\n        event_allowlist: ['click'], // DOM events from this list ['click', 'change', 'submit']\n        url_allowlist: ['posthog.com./docs/.*'], // strings or RegExps\n        element_allowlist: ['button'], // DOM elements from this list ['a', 'button', 'form', 'input', 'select', 'textarea', 'label']\n        css_selector_allowlist: ['[ph-autocapture]'], // List of CSS selectors\n    },\n})`\nThe allowlists only filter autocapture events, they won't affect the data collected by session recordings.\n\nAlternatively you can disable autocapture completely by setting `autocapture: false` in the config.\nDisabling autocapture will not disable session recording. You can disable session recording using `disable_session_recording` or by turning it off in your project's settings.\nAutocapture on input elements\nFor security reasons, PostHog is very conservative regarding `input` tags to prevent passwords or other sensitive data from being collected.\nSpecifically, PostHog autocapture will grab only the `name`, `id`, and `class` attributes from `input` tags.\nSometimes less data might be collected than expected. If you need to collect more data from inputs, you should look into Custom events and Actions.\nTrack across marketing website & app\nWe recommend putting PostHog both on your homepage and your application if applicable. That means you'll be able to follow a user from the moment they come onto your website, all the way through signup and actually using your product.\n\nPostHog automatically sets a cross-domain cookie, so if your website is `yourapp.com` and your app is on `app.yourapp.com` users will be followed when they go from one to the other.\n\nPermitted domains\nYou can also configure \"permitted domains\" in your 'Project Settings'. These are domains where you'll be able to record user sessions and use the PostHog toolbar.\nSend custom events with `posthog.capture`\nimport JSCapture from './_snippets/capture.mdx'\n\nSetting user properties via an event\nTo set properties on a user, you can use the `posthog.people.set` and `posthog.people.set_once` methods.\nHowever, you can also leverage the event properties `$set` and `$set_once` to do this via an event.\n$set\nExample\n`js\nposthog.capture('some event', { $set: { userProperty: 'value' } })`\nUsage\nWhen capturing an event, you can pass a property called `$set` as an event property, and specify its value to be an object with properties to be set on the user that will be associated with the user who triggered the event.\nThis works the same way as `posthog.people.set`.\n$set_once\nExample\n`js\nposthog.capture('some event', { $set_once: { userProperty: 'value' } })`\nUsage\n`$set_once` works just like `$set`, except that it will only set the property if the user doesn't already have that property set.\nThis works the same way as `posthog.people.set_once`.\nIdentifying users\n\nWe highly recommend reading our section on Identifying users to better understand how to correctly use this method.\n\nimport JSIdentify from './_snippets/identify.mdx'\n\nMultiple IDs\nIf you use multiple distinct IDs for the same user (e.g. a logical ID and a UUID), you may want to set both for the same user. This way, if PostHog receives events for either ID, it will properly match them to the same person. This is also helpful if you want to associate anonymous IDs with an identified person (we actually do this automatically when you call `.identify`). To associate multiple IDs to the same person, you do an alias call, as shown below.\n`js\nposthog.alias('[distinct ID]', '[alias ID]')`\nAny events sent to either will be received under the same person.\nNote: that the `'[alias ID]'` cannot be associated with multiple distinct ids.\nReset after logout\nIf a user is logged out, you most likely want to call `reset` to unset any `distinct_ids`.\n\n    This is especially important if your users are sharing a computer, as otherwise all of those users will be grouped\n    together into a single user due to shared cookies between sessions.\n\nWe strongly recommend you do this on logout even if you don't expect users to share a computer. This can help make sure all your users are properly tracked in the odd case a user logs in with a different account.\nYou can do that like so:\n`js\nposthog.reset()`\nIf you also want to reset `device_id`, you can pass `true` as a parameter:\n`js\nposthog.reset(true)`\nSending user information\nAn ID alone might not be enough to work out which user is who within PostHog. That's why it's useful to send over more metadata of the user. At minimum, we recommend sending the `email` property, which is also what we use to display in PostHog.\nYou can make this call on every page view to make sure this information is up-to-date. Alternatively, you can also do this whenever a user first appears (after signup) or when they change their information.\n`js\nposthog.people.set({ email: 'john@gmail.com' })`\nOne-page apps and page views\nThis JS snippet automatically sends `pageview` events whenever it gets loaded. If you have a one-page app, that means it'll only send a pageview once, when your app loads.\nTo make sure any navigating a user does within your app gets captured, you can make a pageview call manually.\n`js\nposthog.capture('$pageview')`\nThis will automatically send the current URL.\nSignup example\nAs an example, here is how to put some of the above concepts together:\n```js\nfunction signup(email) {\n    // Your own internal logic for creating an account and getting a user_id\n    let userId = createAccount(email)\n\n\n```// Identify user with internal ID\nposthog.identify(userId)\n// Set email or any other data\nposthog.people.set({ email: email })\n```\n\n\n}\n```\nSuper Properties\nSuper Properties are properties associated with events that are set once and then sent with every `capture` call, be it a $pageview, an autocaptured button click, or anything else.\nThey are set using `posthog.register`, which takes a properties object as a parameter, and they persist across sessions.\nFor example, take a look at the following call:\n`js\nposthog.register({\n    'icecream pref': 'vanilla',\n    team_id: 22,\n})`\nThe call above ensures that every event sent by the user will include `\"icecream pref\": \"vanilla\"` and `\"team_id\": 22`. This way, if you filtered events by property using `icecream_pref = vanilla`, it would display all events captured on that user after the `posthog.register` call, since they all include the specified Super Property.\nHowever, please note that this does not store properties against the User, only against their events. To store properties against the User object, you should use `posthog.people.set`. More information on this can be found on the Sending User Information section.\nFurthermore, if you register the same property multiple times, the next event will use the new value of that property. If you want to register a property only once (e.g. for ad campaign properties) you can use `register_once`, like so:\n`js\nposthog.register_once({\n    'campaign source': 'twitter',\n})`\nUsing `register_once` will ensure that if a property is already set, it will not be set again. For example, if the user already has property `\"icecream pref\": \"vanilla\"`, calling `posthog.register_once({\"icecream pref\": \"chocolate\"})` will not update the property.\nRemoving stored Super Properties\nSetting Super Properties creates a cookie on the client with the respective properties and their values. In order to stop sending a Super Property with events and remove the cookie, you can use `posthog.unregister`, like so:\n`js\nposthog.unregister('icecream pref')`\nThis will remove the Super Property and subsequent events will not include it.\nOpt users out\nPostHog JS offers a function to opt users out based on your cookie settings definition (e.g. preferences set via a cookie banner).\nThis is also the suggested way to prevent capturing any data from the admin on the page, as well as from team members of your organization. A simple way to do this is to access the page as the admin (or any other user on your team you wish to stop capturing data on), and call `posthog.opt_out_capturing();` on the developer console. You can also add this logic in you app and call it directly after an admin/team member logs in.\nIf you still wish to capture these events but want to create a distinction between users and team in PostHog, you should look into Cohorts.\nWith PostHog, you can:\nOpt a user out:\n`js\nposthog.opt_out_capturing()`\nSee if a user has opted out:\n`js\nposthog.has_opted_out_capturing()`\nOpt a user back in:\n`js\nposthog.opt_in_capturing()`\nFeature Flags\nPostHog v1.10.0 introduced Feature Flags, which allow you to safely deploy and roll back new features.\nHere's how you can use them:\n\n\nDo something when the feature flags load:\nThe argument `callback(flags: string[])` will be called when the feature flags are loaded.\nIn case the flags are already loaded, it'll be called immediately. Additionally, it will also be called when the flags are re-loaded e.g. after calling `identify` or `reloadFeatureFlags`.\n\n\n`js\nposthog.onFeatureFlags(callback)`\n\nCheck if a feature is enabled:\n\n`js\nposthog.isFeatureEnabled('keyword')`\n\nTrigger a reload of the feature flags:\n\n`js\nposthog.reloadFeatureFlags()`\n\nBy default, this function will send a `$feature_flag_called` event to your instance every time it's called so you're able to do analytics. You can disable this by passing the send_event property:\n\n`js\nposthog.isFeatureEnabled('keyword', { send_event: false })`\nFeature Flag Payloads\nPayloads allow you to retrieve a value that is associated with the matched flag. The value can be a\nstring, boolean, number, dictionary, or array. This allows for custom configurations based on values defined in the posthog app.\n`js\nposthog.getFeatureFlagPayload('keyword')`\nBootstrapping Flags\nThere is a delay between loading the library and feature flags becoming available to use. For some cases, like redirecting users to a different page based on a feature flag, this is extremely detrimental, as the flags load after the redirect logic occurs, thus never working.\nIn cases like these, where you want flags to be immediately available on page load, you can use the bootstrap library option.\nThis allows you to pass in a distinctID and feature flags during library initialisation, like so:\n`js\nposthog.init('<ph_project_api_key>', {\n    api_host: '<ph_instance_address>',\n    bootstrap: {\n        distinctID: 'your-anonymous-id',\n        featureFlags: {\n            'flag-1': true,\n            'variant-flag': 'control',\n            'other-flag': false,\n        },\n    },\n})`\nTo compute these flag values, use the corresponding `getAllFlags` method in your server-side library. Note that bootstrapping flags requires server-side initialisation.\nIf the ID you're passing in is an identified ID (that is, an ID with which you've called `posthog.identify()` elsewhere), you can also pass in the `isIdentifiedID` bootstrap option, which ensures that this ID is treated as an identified ID in the library. This is helpful as it warns you when you try to do something wrong with this ID, like calling `identify` again.\n`js\nposthog.init('<ph_project_api_key>', {\n    api_host: '<ph_instance_address>',\n    bootstrap: {\n        distinctID: 'your-identified-id',\n        isIdentifiedID: true,\n        featureFlags: {\n            'flag-1': true,\n            'variant-flag': 'control',\n            'other-flag': false,\n        },\n    },\n})`\n\nNote: Passing in a distinctID to bootstrap replaces any existing IDs, which means you may fail to connect any old anonymous user events with the logged in person, if your logic calls identify in the frontend immediately on login. In this case, you can omit passing in the distinctID.\n\nGroup analytics\nGroup analytics allows you to associate the events for that person's session with a group (e.g. teams, organizations, etc.). Read the Group Analytics guide for more information.\n\nNote:  This is a paid feature and is not available on the open-source or free cloud plan. Learn more here.\n\n\nAssociate the events for this session with a group\n\n```js\nposthog.group('company', '42dlsfj23f')\nposthog.capture('upgraded plan') // this event is associated with company ID `42dlsfj23f`\n```\n\nAssociate the events for this session with a group AND update the properties of that group\n\n`js\nposthog.group('company', '42dlsfj23f', {\n    name: 'Awesome Inc.',\n    employees: 11,\n})`\nThe `name` is a special property which is used in the PostHog UI for the name of the Group. If you don't specify a `name` property, the group ID will be used instead.\nHandling logging out\nWhen the user logs out it's important to call `posthog.reset()` to avoid new events being registered under the previously active group.\nIntegrating groups with feature flags\nIf you have updated tracking, you can use group-based feature flags as normal.\n`js\nif (posthog.isFeatureEnabled('new-groups-feature')) {\n    // do something\n}`\nTo check flag status for a different group, first switch the active group by calling `posthog.group()`.\nPersistence\nIn order for PostHog to work optimally, we require storing a small amount of information about the user on the user's browser. This ensures that if the user navigates away, and comes back to your site at a later time, we will still identify them properly. We store the following information in the user's browser:\n\nUser's ID\nSession ID & Device ID\nActive & enabled feature flags\nAny super properties you have defined.\nSome PostHog configuration options (e.g. whether session recording is enabled)\n\nBy default we store all this information in a `cookie`, which means that PostHog will still be able to identify your users even across subdomains. By default, this cookie is set to expire after `365` days.\nIf you would like to change how PostHog stores this information, you can do so with the `persistence` parameter.\n\n`persistence: \"cookie\"` (default). Everything is stored in a cookie.\n`persistence: \"localStorage+cookie\"`. User's distinct ID is stored in a cookie and everything else is stored in the browser's localStorage.\n`persistence: \"localStorage\"`. Everything is stored in localStorage.\n`persistence: \"memory\"`. Stores in page memory, which means data is only persisted for the duration of the page view.\n\nAs a note, due to the size limitation of cookies you may run into `431 Request Header Fields Too Large` errors (e.g. if you have a lot of feature flags). In that case, use `localStorage+cookie`.\n\nNote:  Please be aware that localStorage can't be used across subdomains. If you have multiple sites on the same domain, you may want to consider the `cookie` option or make sure to set all super properties across each subdomain.\n\nIf you don't want PostHog to store anything on the user's browser (e.g. if you want to rely on your own identification mechanism only, or want completely anonymous users), you can set `disable_persistence: true` in PostHog's config.\nWarning: Remember to call `posthog.identify` every time your app loads or every page refresh will be treated as a different user.\nConfig\nWhen calling `posthog.init`, there are various configuration options you can set in addition to `loaded` and `api_host`.\nTo configure these options, pass them as an object to the `posthog.init` call, like so:\n`js\nposthog.init('<ph_project_api_key>', {\n    api_host: '<ph_instance_address>',\n    loaded: function (posthog) {\n        posthog.identify('[user unique id]')\n    },\n    autocapture: false,\n    // ... more options\n})`\nThere are multiple different configuration options, most of which you do not have to ever worry about. For brevity, only the most relevant ones are used here. However you can view all the configuration options in posthog-core.js.\nSome of the most relevant options are:\n| Attribute                                                                                                                                                                                 | Description                                                                                                                    |\n| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------ |\n| `api_host`Type: StringDefault: `https://app.posthog.com`                                                                                                           | URL of your PostHog instance.                                                                                                  |\n| `autocapture`Type: BooleanDefault: `true`                                                                                                                          | Determines if PostHog should autocapture events. This setting does not affect capturing pageview events (see `capture_pageview`).            |\n| `bootstrap`Type: ObjectDefault: `{}`                                                                                                                       | An object containing the `distinctID`, `isIdentifiedID`, and `featureFlags` keys, where `distinctID` is a string, and `featureFlags` is an object of key-value pairs |\n| `capture_pageview`Type: BooleanDefault: `true`                                                                                                                     | Determines if PostHog should automatically capture pageview events.                                                            |\n| `capture_pageleave`Type: BooleanDefault: `true`                                                                                                                     | Determines if PostHog should automatically capture pageleave events.                                                            |\n| `cross_subdomain_cookie`Type: BooleanDefault: `true`                                                                                                                     | Determines if cookie should be set on the top level domain (example.com). If PostHog-js is loaded on a subdomain (test.example.com), and `cross_subdomain_cookie` is set to false, it'll set the cookie on the subdomain only (test.example.com).                                                           |\n| `disable_persistence`Type: BooleanDefault: `false`                                                                                                                 | Disable persisting user data across pages. This will disable cookies, session storage and local storage.                       |\n| `disable_session_recording`Type: BooleanDefault: `false`                                                                                                           | Determines if users should be opted out of session recording.                                                                  |\n| `enable_recording_console_log`Type: BooleanDefault: `false`                                                                                                        | Determines if console logs should be recorded as part of the session recording. More information.                                               |\n| `loaded`Type: FunctionDefault: `function () {}`                                                                                                                    | A function to be called once the PostHog scripts have loaded successfully.                                                     |\n| `mask_all_text`Type: BooleanDefault: `false`                                                                                                                       | Prevent PostHog autocapture from capturing any text from your elements.                                                        |\n| `mask_all_element_attributes`Type: BooleanDefault: `false`                                                                                                         | Prevent PostHog autocapture from capturing any attributes from your elements.                                                  |\n| `opt_out_capturing_by_default`Type: BooleanDefault: `false`                                                                                                        | Determines if users should be opted out of PostHog tracking by default, requiring additional logic to opt them into capturing. |\n| `persistence`Type: `localStorage` or `cookie` or `memory` or `localStorage+cookie`Default: `cookie`                                                                | Determines how PostHog stores information about the user. See persistence for details.                         |\n| `property_blacklist`Type: ArrayDefault: `[]`                                                                                                                       | A list of properties that should never be sent with `capture` calls.                                                           |\n| `sanitize_properties`Type: FunctionDefault: `null`                                                                                                                 | A function that allows you to sanitize or modify the properties that get sent. Example: `sanitize_properties: function(properties, event) { if(properties['$ip']) { properties['$ip'] = null } return properties }` |\n| `session_recording`Type: ObjectDefault: See here. | Configuration options for recordings. More details found here         |\n| `xhr_headers`Type: ObjectDefault: `{}`                                                                                                                             | Any additional headers you wish to pass with the XHR requests to the PostHog API.                                              |\nAdvanced configuration\nIn this section we describe some additional details on advanced configuration available.\n| Attribute                                                                     | Description                                                                                                    |\n| ----------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------- |\n| `advanced_disable_decide`Type: BooleanDefault: `false` | Will completely disable the `/decide` endpoint request (and features that rely on it). More details below.     |\n| `secure_cookie` Type: BooleanDefault: `false`          | If this is `true`, PostHog cookies will be marked as secure, meaning they will only be transmitted over HTTPS. |\n\n    These are features for advanced users and may lead to unintended side effects if not reviewed carefully. If you are\n    unsure about something, just{' '}\n    \n        reach out\n    \n    .\n\nDisable `/decide` endpoint\n\n    This feature was introduced in posthog-js 1.10.0. Previously, disabling autocapture would inherently disable the\n    /decide endpoint altogether. This meant that disabling autocapture would inadvertenly turn off session recording,\n    feature flags, compression and the toolbar too.\n\nOne of the very first things the PostHog library does when `init()` is called is make a request to the `/decide` endpoint on PostHog's backend. This endpoint contains information on how to run the PostHog library so events are properly received in the backend. This endpoint is required to run most features of the library (detailed below). However, if you're not using any of the described features, you may wish to turn off the call completely to avoid an extra request and reduce resource usage on both the client and the server.\nThe `/decide` endpoint can be disabled by setting `advanced_disable_decide = true` in PostHog config.\nResources dependent on `/decide`\n\n    These are features/resources that will be fully disabled when the /decide endpoint is disabled.\n\n\nAutocapture. The `/decide` endpoint contains information on whether autocapture should be enabled or not (apart from local configuration).\nSession recording. The endpoint contains information on where to send relevant session recording events.\nCompression. The endpoint contains information on what compression methods are supported on the backend (e.g. LZ64, gzip) for event payloads.\nFeature flags. The endpoint contains the feature flags enabled for the current person.\nToolbar. The endpoint contains authentication information and other toolbar capabilities information required to run it.\n\nAny custom event capturing (`posthog.capture`), `$identify`, `$set`, `$set_once` and basically any other calls not detailed above will work as expected when `/decide` is disabled.\nDebugging\nIn your dev console you can run `posthog.debug()`. This will enable debugging, easily allowing you to see all data that is being sent to PostHog.\nDevelopment",
    "tag": "posthog"
  },
  {
    "title": "install.mdx",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/sdks/js/_snippets/install.mdx",
    "content": "import Snippet from \"../../../integrate/_snippets/snippet.mdx\"\nYou can either load the snippet as a script in your HTML:\n\nPlace the snippet in the `<head>` tags of your website, ideally just above the closing `</head>` tag. You will need to do this for all pages that you wish to track.\nOr you can include it using npm, by doing either:\n`bash\nyarn add posthog-js`\nor:\n`bash\nnpm install --save posthog-js`\nAnd then include it in your files:\n`js\nimport posthog from 'posthog-js'\nposthog.init('<ph_project_api_key>', { api_host: '<ph_instance_address>' })`\nIf you don't want to send a bunch of test data while you're developing, you could do the following:\n```js\nif (!window.location.host.includes('127.0.0.1') && !window.location.host.includes('localhost')) {\n    posthog.init('', { api_host: '' })\n}",
    "tag": "posthog"
  },
  {
    "title": "identify.mdx",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/sdks/js/_snippets/identify.mdx",
    "content": "To make sure you understand which user is performing actions within your app, you can identify users at any point. From the moment you make this call, all events will be identified with that distinct id.\nThe ID can by anything, but is usually the unique ID that you identify users by in the database.\nNormally, you would put this below `posthog.init` if you have the information there.\nIf a user was previously anonymous (because they hadn't signed up or logged in yet), we'll automatically alias their anonymous ID with their new unique ID. That means all their events from before and after they signed up will be shown under the same user.\n`js\nposthog.identify(\n    '[user unique id]', // distinct_id, required\n    { userProperty: 'value1' }, // $set, optional\n    { anotherUserProperty: 'value2' } // $set_once, optional\n);`\nYou can also pass two more arguments to `posthog.identify`. Both take objects consisting of as many properties as you want to be set on that user's profile. The difference is that the second argument will use the `$set` mechanism, whereas the third argument will use `$set_once`. \nYou can read more about the difference between this in the setting properties section.\n\n    Warning! You can't call identify straight after an `.init` (as `.init` sends a pageview event, probably with the user's\n    anonymous ID).\n\nTo address the issue described above, you can call `.init` passing a `loaded` callback function, inside which you can then call identify, like so:\n```js\nposthog.init('', {\n    api_host: '',\n    loaded: function(posthog) { posthog.identify('[user unique id]'); }\n});",
    "tag": "posthog"
  },
  {
    "title": "Installation",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/sdks/rust/index.mdx",
    "content": "\ntitle: Rust\ngithub: https://github.com/PostHog/posthog-rs\nicon: ../../../images/docs/integrate/rust.svg\nfeatures:\n    eventCapture: true\n    userIdentification: false\n    autoCapture: false\n    sessionRecording: false\n    featureFlags: false\n    groupAnalytics: false\n\n\nWarning: This crate is still under development and is not recommended for production use yet.\n\nInstallation\nInstall the `posthog-rs` crate by adding it to your `Cargo.toml`.\n`toml file=Cargo.toml\n[dependencies]\nposthog-rs = \"0.2.0\"`\nUsage\nTo setup the client, all you need to do is pass your PostHog project key.\n`rust\nlet client = posthog_rs::client(env!(\"POSTHOG_API_KEY\"));`\n\nNote: Currently, there is no way to customize the host that events are sent to, and we default to `app.posthog.com`\n\nCapturing events\nCurrently, the only functionality this library supports is sending events, which can be done using the `capture` and `cature_batch` methods.\n```rust\nlet mut event = Event::new(\"\", \"\");\nevent.insert_prop(\"key1\", \"value1\").unwrap();\nevent.insert_prop(\"key2\", vec![\"a\", \"b\"]).unwrap();\nclient.capture(event).unwrap();\n```\n```rust\nlet event1 = posthog_rs::Event::new(\"event 1\", \"1234\");\nlet event2 = posthog_rs::Event::new(\"event 2\", \"1234\");\nclient.capture_batch(vec![event1, event2]).unwrap();",
    "tag": "posthog"
  },
  {
    "title": "cohort-expansion.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/sdks/_snippets/cohort-expansion.md",
    "content": "Cohort expansion\nTo support feature flags that depend on cohorts locally as well, we translate the cohort definition into person properties, so that the person properties you set can be used to evaluate cohorts as well.\nHowever, there are a few constraints here and we don't support doing this for arbitrary cohorts. Cohorts won't be evaluated locally if:\n\nThey have non-person properties\nThere's more than one cohort in the feature flag definition.\nThe cohort in the feature flag is in the same group as another condition.\nThe cohort has nested AND-OR filters. Only simple cohorts that have a top level OR group, and inner level ANDs will be evaluated locally.\n",
    "tag": "posthog"
  },
  {
    "title": "Installation",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/sdks/java/index.mdx",
    "content": "\ntitle: Java\nsidebarTitle: Java\nsidebar: Docs\nshowTitle: true\ngithub: https://github.com/PostHog/posthog-java\nicon: ../../../images/docs/integrate/java.svg\nfeatures:\n    eventCapture: true\n    userIdentification: true\n    autoCapture: false\n    sessionRecording: false\n    featureFlags: false\n    groupAnalytics: false\n\n\n    \u26a0\ufe0f  Warning - This is in beta and may break.\n\nThis is an optional library you can install if you're working with Java. It uses an internal queue to make calls fast and non-blocking. It also batches requests and flushes asynchronously, making it perfect to use in any part of your web app or other server side application that needs performance.\nInstallation\nThe best way to install the PostHog Android library is with a build system like\nGradle or Maven. This ensures you can easily upgrade to the latest versions.\nLookup the latest version in com.posthog.java.\nGradle\nAll you need to do is add the `posthog` module to your `build.gradle`:\n`bash\ndependencies {\n  implementation 'com.posthog.java:posthog:+'\n}`\nMaven\nAll you need to do is add the `posthog` module to your `pom.xml`:\n`xml file=pom.xml\n<dependency>\n  <groupId>com.posthog.java</groupId>\n  <artifactId>posthog</artifactId>\n  <version>LATEST</version>\n</dependency>`\nOther\nSee the com.posthog.java in the Maven Central Repository. Clicking on the latest version shows you options for adding dependencies for other build systems.\nUsage\n```java\nimport com.posthog.java.PostHog;\nclass Sample {\n  private static final String POSTHOG_API_KEY = \"\";\n  private static final String POSTHOG_HOST = \"\";\npublic static void main(String args[]) {\n    PostHog posthog = new PostHog.Builder(POSTHOG_API_KEY).host(POSTHOG_HOST).build();\n\n\n```// run commands\n\nposthog.shutdown();  // send the last events in queue\n```\n\n\n}\n}\n```\nMaking calls\nCapture\nCapture allows you to capture anything a user does within your system, which you can later use in PostHog to find patterns in usage, work out which features to improve or where people are giving up.\nA `capture` call requires:\n\n`distinct id` which uniquely identifies your user\n\n`event name` to specify the event\n\n\nWe recommend naming events with \"[noun] [verb]\", such as `movie played` or `movie updated`, in order to easily identify what your events mean later on (we know this from experience).\n\n\nOptionally you can submit:\n\n`properties`, which is a dictionary with any information you'd like to add\n\nFor example:\n`java\nposthog.capture(\"distinct id\", \"movie played\", new HashMap<String, Object>() {\n  {\n    put(\"movie_id\", 123);\n    put(\"category\", \"romcom\");\n  }\n});`\nIdentify\n\nWe highly recommend reading our section on Identifying users to better understand how to correctly use this method.\n\nIdentify lets you add metadata to your users so you can easily identify who they are in PostHog, as well as do things\nlike segment users by these properties.\nAn `identify` call requires:\n\n`distinct id` which uniquely identifies your user\n`properties` with a dict with any key:value pairs\n\nFor example:\n`java\nposthog.identify(\"distinct id\", new HashMap<String, Object>() {\n  {\n    put(\"email\", \"john@doe.com\");\n    put(\"proUser\", false);\n  }\n});`\nThe most obvious place to make this call is whenever a user signs up, or when they update their information.\nAlias\nTo connect whatever a user does before they sign up or login with what they do after, you need to make an alias call. This will allow you to answer questions like \"Which marketing channels lead to users churning after a month?\" or \"What do users do on our website before signing up?\"\nIn a purely back-end implementation, this means whenever an anonymous user does something, you'll want to send a session ID with the capture call. Then, when that users signs up, you want to do an alias call with the session ID and the newly created user ID.\nThe same concept applies for when a user logs in.\nIf you're using PostHog in the front-end and back-end, doing the `identify` call in the frontend will be enough.\nAn `alias` call requires:\n-   `distinct id` \u2013 the user id\n-   `alias` \u2013 the anonymous session distinct ID\nFor example:\n`java\nposthog.alias(\"user:123\", \"session:12345\");`\nSetting user properties\nNote: `set_once` works just like `set`, except that it will only set the property if the user doesn't already have that property set.\nThere are three options:\nWith the `capture` call\n`java\nposthog.capture(\"distinct id\", \"movie played\", new HashMap<String, Object>() {\n  {\n    put(\"eventProperty\", \"value1\");                    // event properties\n    put(\"$set\",  new HashMap<String, Object>() {       // user properties\n      {\n        put(\"email\", \"john@doe.com\");\n        put(\"proUser\", false);\n      }\n    });\n    put(\"$set_once\",  new HashMap<String, Object>() {  // user properties\n      {\n        put(\"user_first_location\", \"colorado\");\n      }\n    });\n  }\n});`\nWith the `identify` call\n`java\nposthog.identify(\"distinct id\", new HashMap<String, Object>() {  // set\n    {\n      put(\"email\", \"john@doe.com\");\n      put(\"proUser\", false);\n    }\n  }, new HashMap<String, Object>() {                             // set_once\n    {\n      put(\"user_first_location\", \"colorado\");\n    }\n});`\nWith `set` or `set_once` calls\n`java\nposthog.set(\"distinct id\", new HashMap<String, Object>() {\n  {\n    put(\"email\", \"john@doe.com\");\n    put(\"proUser\", false);\n  }\n});`\n```java\nposthog.setOnce(\"distinct id\", new HashMap() {\n  {\n    put(\"user_first_location\", \"colorado\");\n  }\n});",
    "tag": "posthog"
  },
  {
    "title": "Installation",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/sdks/php/index.mdx",
    "content": "\ntitle: PHP\nsidebarTitle: PHP\nsidebar: Docs\nshowTitle: true\ngithub: https://github.com/PostHog/posthog-php\nicon: ../../../images/docs/integrate/php.svg\nfeatures:\n    eventCapture: true\n    userIdentification: true\n    autoCapture: false\n    sessionRecording: false\n    featureFlags: true\n    groupAnalytics: true\n\nimport CohortExpansionSnippet from '../_snippets/cohort-expansion'\nThis is an optional library you can install if you're working with PHP. It uses an internal queue to make calls fast and non-blocking. It also batches requests and flushes asynchronously, making it perfect to use in any part of your web app or other server-side application that needs performance.\nInstallation\nAdd the following to composer.json:\n`json\n{\n    \"require\": {\n        \"posthog/posthog-php\": \"2.1.*\"\n    }\n}`\nAnd then install the dependencies with the command: `php composer.phar install`\nIn your app, set your API key before making any calls.\n`php\nPostHog::init(\"<ph_project_api_key>\",\n  array('host' => '<ph_instance_address>') // You can remove this line if you're using app.posthog.com\n);`\n\nNote: As a general rule of thumb, we do not recommend having API keys in plaintext. Setting it as an environment variable would be best.\n\nYou can find your key in the 'Project Settings' page in PostHog.\nMaking calls\nCapture\nCapture allows you to capture anything a user does within your system, which you can later use in PostHog to find patterns in usage, work out which features to improve or where people are giving up.\nA `capture` call requires:\n - `distinct id` which uniquely identifies your user\n - `event name` to specify the event\n  * We recommend naming events with \"[noun] [verb]\", such as `movie played` or `movie updated`, in order to easily identify what your events mean later on (we know this from experience).\nOptionally you can submit:\n- `properties`, which is an array with any information you'd like to add\nFor example:\n`php\nPostHog::capture(array(\n  'distinctId' => 'user:123',\n  'event' => 'movie played',\n  'properties' => array(\n    'movieId' => '123',\n    'category' => 'romcom'\n  )\n));`\nSetting user properties via an event\nTo set properties on your users via an event, you can leverage the event properties `$set` and `$set_once`.\n$set\nExample\n`php\nPostHog::capture(array(\n  'distinctId' => 'user:123',\n  'event' => 'movie played',\n  'properties' => array(\n    '$set' => array(\n      'userProperty' => 'value'\n    )\n  )\n));`\nUsage\nWhen capturing an event, you can pass a property called `$set` as an event property, and specify its value to be an object with properties to be set on the user that will be associated with the user who triggered the event.\n$set_once\nExample\n`php\nPostHog::capture(array(\n  'distinctId' => 'user:123',\n  'event' => 'movie played',\n  'properties' => array(\n    '$set_once' => array(\n      'userProperty' => 'value'\n    )\n  )\n));`\nUsage\n`$set_once` works just like `$set`, except that it will only set the property if the user doesn't already have that property set.\nIdentify\n\nWe highly recommend reading our section on Identifying users to better understand how to correctly use this method.\n\nIdentify lets you add metadata to your users so you can easily identify who they are in PostHog, as well as do things \nlike segment users by these properties.\nAn `identify` call requires:\n- `distinct id` which uniquely identifies your user\n- `properties` with a dict with any key: value pairs \nFor example:\n`php\nPostHog::identify(array(\n  'distinctId' => 'user:123',\n  'properties' => array(\n    'email' => 'john@doe.com',\n    'proUser' => false\n  )\n));`\nThe most obvious place to make this call is whenever a user signs up, or when they update their information.\nAlias\nTo connect whatever a user does before they sign up or log in with what they do after you need to make an alias call. This will allow you to answer questions like \"Which marketing channels leads to users churning after a month?\" or \"What do users do on our website before signing up?\"\nIn a purely back-end implementation, this means whenever an anonymous user does something, you'll want to send a session ID with the capture call. Then, when that users signs up, you want to do an alias call with the session ID and the newly created user ID.\nThe same concept applies for when a user logs in.\nIf you're using PostHog in the front-end and back-end, doing the identify call in the frontend will be enough.\nAn alias call requires:\n-   `distinctId` \u2013 the user id\n-   `alias` \u2013 the anonymous session distinct ID\nFor example:\n`php\nPostHog::alias(array(\n  'distinctId' => 'user:123',\n  'alias' => 'session:12345'\n));`\nSending page views\nIf you're aiming for a full back-end implementation of PostHog, you can send pageviews from your backend\n`php\nPostHog::capture(array(\n  'distinctId' => 'user:123',\n  'event' => '$pageview',\n  'properties' => array(\n    '$current_url' => 'https://example.com'\n  )\n));`\nFeature flags\nHow to check if a flag is enabled\nTo check if a feature flag is enabled for a given user, use `isFeatureEnabled`, like so:\n`php\nif (PostHog::isFeatureEnabled('my-amazing-flag', 'some distinct id')) {\n    // do something here\n}`\nGet a flag value\nif you're using multivariate feature flags, you can also get not just whether the flag is enabled, but what value its enabled to.\n`php\nPostHog::getFeatureFlag('multivariate-flag', 'some distinct id');\n// returns string or true or false or null`\nOverriding server properties\nSometimes, you might want to evaluate feature flags using properties that haven't been ingested yet, or were set incorrectly earlier. You can do so by setting properties the flag depends on with these calls.\nFOr example, if the `beta-feature` depends on the `is_authorized` property, and you know the value of the property, you can tell PostHog to use this property, like so:\n`php\nPostHog::getFeatureFlag('beta-feature', 'some distinct id', [], [\"is_authorized\" => true])\n// the third argument is for groups`\nThe same holds for groups. If you have a group name `organisation`, you can add properties like so:\n`php\nPostHog::getFeatureFlag('beta-feature', 'some distinct id', [\"organisation\" => \"some-company\"], [], [\"organisation\" => [\"is_authorized\" => true]])`\nGetting all flag values\nYou can also get all known flag values as well. This is useful when you want to seed a frontend client with initial known flags. Like all methods above, this also takes optional person and group properties, if known.\n`php\nPostHog::getAllFlags('distinct id')`\nLocal Evaluation\n\nNote: To enable local evaluation of feature flags you must also set a `personal_api_key` when configuring the integration, as described in the Installation section.\nNote: This feature requires version 3.0 of the library, which in turn requires a minimum PostHog version of 1.38\n\nAll feature flag evaluation requires an API request to your PostHog servers to get a response. However, where latency matters, you can evaluate flags locally. This is much faster, and requires two things to work:\n1. The library must be initialised with a personal API key\n2. You must know all person or group properties the flag depends on.\nThen, the flag can be evaluated locally. The method signature looks exactly like above.\n`php\nPostHog::getFeatureFlag('beta-feature', 'some distinct id', [], [\"is_authorized\" => true])\n// the third argument is for groups`\nThis works for `getAllFlags` as well. It evaluates all flags locally if possible. If even one flag isn't locally evaluable, it falls back to decide.\n`js\nPostHog::getAllFlags('distinct id', [\"organisation\" => \"some-company\"], [], [\"organisation\" => [\"is_authorized\" => true]])`\nRestricting evaluation to local only\nSometimes, performance might matter to you so much that you never want an HTTP request roundtrip delay when computing flags. In this case, you can set the `onlyEvaluateLocally` parameter to true, which tries to compute flags only with the properties it has. If it fails to compute a flag, it returns `None`, instead of going to PostHog's servers to get the value.\n\nGroup analytics\nGroup analytics allows you to associate an event with a group (e.g. teams, organizations, etc.). This feature requires a posthog-php version of `2.1.0` or above. Read the Group Analytics guide for more information.\n\nNote:  This is a paid feature and is not available on the open-source or free cloud plan. Learn more here.\n\n\nCapture an event and associate it with a group\n\n`php\nPostHog::capture(array(\n    'distinctId' => '[distinct id]',\n    'event' => 'some event',\n    '$groups' => array(\"company\" => \"42dlsfj23f\")\n));`\n\nUpdate properties on a group\n\n`php\nPostHog::groupIdentify(array(\n    'groupType' => 'company',\n    'groupKey' => '42dlsfj23f',\n    'properties' => array(\"name\" => \"Awesome Inc.\", \"employees\" => 11)\n));`\nThe `name` is a special property which is used in the PostHog UI for the name of the Group. If you don't specify a `name` property, the group ID will be used instead.\nThank you",
    "tag": "posthog"
  },
  {
    "title": "Installation",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/sdks/ios/index.mdx",
    "content": "\ntitle: iOS\nsidebarTitle: iOS\nsidebar: Docs\nshowTitle: true\ngithub: https://github.com/PostHog/posthog-ios\nicon: ../../../images/docs/integrate/ios.svg\nfeatures:\n    eventCapture: true\n    userIdentification: true\n    autoCapture: false\n    sessionRecording: false\n    featureFlags: true\n    groupAnalytics: true\n\nThis library uses an internal queue to make calls fast and non-blocking. It also batches requests and flushes asynchronously, making it perfect to use in any part of your mobile app.\nInstallation\nimport IOSInstall from './_snippets/install.mdx'\nimport IOSConfigure from './_snippets/configure.mdx'\nimport IOSIdentify from './_snippets/identify.mdx'\nimport IOSCapture from './_snippets/capture.mdx'\n\nConfiguration\n\nMaking calls\nIdentify\n\nWe highly recommend reading our section on Identifying users to better understand how to correctly use this method.\n\n\nCapture\n\nSetting user properties via an event\nTo set properties on your users via an event, you can leverage the event properties `$set` and `$set_once`.\n$set\nExample\n`objectivec\n[[PHGPostHog sharedPostHog] capture:@\"Signed Up\" properties:@{ @\"plan\": @\"Pro++\", @\"$set\":@{ @\"userProp\": \"value1\" } }];`\n`swift\nposthog.capture(\"Signed Up\", properties: [\"plan\": \"Pro++\", \"$set\": [\"userProp\": \"value1\"] ])`\nUsage\nWhen capturing an event, you can pass a property called `$set` as an event property, and specify its value to be an object with properties to be set on the user that will be associated with the user who triggered the event.\n$set_once\nExample\n`objectivec\n[[PHGPostHog sharedPostHog] capture:@\"Signed Up\" properties:@{ @\"plan\": @\"Pro++\", @\"$set_once\":@{ @\"userProp\": \"value1\" } }];`\n`swift\nposthog.capture(\"Signed Up\", properties: [\"plan\": \"Pro++\", \"$set_once\": [\"userProp\": \"value1\"] ])`\nUsage\n`$set_once` works just like `$set`, except that it will only set the property if the user doesn't already have that property set.\nFlush\nYou can set the number of events in the configuration that should queue before flushing.\nSetting this to `1` will send events immediately and will use more battery. This is set to `20` by default.\n`objectivec\nconfiguration.flushAt = 1;`\nYou can also manually flush the queue:\n`objectivec\n[[PHGPostHog sharedPostHog] capture:@\"Logged Out\"];\n[[PHGPostHog sharedPostHog] flush]`\n`swift\nposthog.capture('Logged Out')\nposthog.flush()`\nReset\nTo reset the user's ID and anonymous ID, call `reset`. Usually you would do this right after the user logs out.\n`objectivec\n[[PHGPostHog sharedPostHog] reset]`\n`swift\nposthog.reset()`\nSending screen views\nWith `configuration.recordScreenViews` set as `YES`, PostHog will try to record all screen changes automatically.\nIf you want to manually send a new screen capture event, use the `screen` function.\n`objectivec\n[[PHGPostHog sharedPostHog] screen:@\"Dashboard\" properties:@{ @\"fromIcon\": @\"bottom\" }];`\n`swift\nposthog.screen(\"Dashboard\", properties: [\"fromIcon\": \"bottom\"])`\nA note about IDFA (identifier for advertisers) collection in iOS 14\nStarting with iOS 14, Apple will further restrict apps that track users. Any references to Apple's AdSupport framework, even in strings, will trip the App Store's static analysis.\nHence starting with posthog-ios version 1.2.0 we have removed all references to Apple's AdSupport framework.\nGroup analytics\nGroup analytics allows you to associate the events for that person's session with a group (e.g. teams, organizations, etc.). Read the Group Analytics guide for more information.\n\nNote:  This is a paid feature and is not available on the open-source or free cloud plan. Learn more here.\n\n\nAssociate the events for this session with a group\n\n`objectivec\n[[PHGPostHog sharedPostHog] group:@\"organization\" groupKey:@\"42dlsfj23f\"];`\n`swift\nposthog.group( \"some-type\", groupKey: \"42dlsfj23f\")`\n\nAssociate the events for this session with a group AND update the properties of that group\n\n`objectivec\n[[PHGPostHog sharedPostHog] group:@\"organization\" groupKey:@\"42dlsfj23f\" properties:@{ @\"name\": @\"Awesome Inc.\" }];`\n`swift\nposthog.group( \"organization\", groupKey: \"42dlsfj23f\", properties: [\n    \"name\": \"ACME Corp\"\n])`\nThe `name` is a special property which is used in the PostHog UI for the name of the Group. If you don't specify a `name` property, the group ID will be used instead.\nFeature Flags\nFeature Flags allow you to safely deploy and roll back new features.\n\nNote: This requires minimum library version 2.0.0\n\nHere's how you can use them:\n\nCheck if a feature is enabled:\n\n`objectivec\n[[PHGPosthog sharedPostHog] isFeatureEnabled:@\"keyword\"];`\n`swift\nposthog.isFeatureEnabled('keyword')`\n\nTrigger a reload of the feature flags:\n\n`objectivec\n[[PHGPosthog sharedPostHog] reloadFeatureFlags];`\n`swift\nposthog.reloadFeatureFlags()`\n\nBy default, this function will send a `$feature_flag_called` event to your instance every time it's called so you're able to do analytics.\n\nGet a flag value\nIf you're using multivariate feature flags, you can also get the value of the flag, as well as whether or not it is enabled.\n`objectivec\n[[PHGPosthog sharedPostHog] getFeatureFlag:@\"some-flag\"];`\n`swift\nposthog.getFeatureFlag(\"some-flag\")`\nAll configuration options\nThe `configuration` element contains several other settings you can toggle:\n```objectivec\n/**\n * Whether the posthog client should use location services.\n * If`YES`and the host app hasn't asked for permission to use location services then the user will be\n * presented with an alert view asking to do so.`NO`by default. If`YES`, please make sure to add a\n * description for`NSLocationAlwaysUsageDescription`in your`Info.plist` explaining why your app is\n * accessing Location APIs.\n */\nconfiguration.shouldUseLocationServices = NO;\n/*\n * The number of queued events that the posthog client should flush at. Setting this to `1` will not queue\n * any events and will use more battery. `20` by default.\n /\nconfiguration.flushAt = 20;\n/*\n * The amount of time to wait before each tick of the flush timer.\n * Smaller values will make events delivered in a more real-time manner and also use more battery.\n * A value smaller than 10 seconds will seriously degrade overall performance.\n * 30 seconds by default.\n /\nconfiguration.flushInterval = 30;\n/*\n * The maximum number of items to queue before starting to drop old ones. This should be a value greater\n * than zero, the behaviour is undefined otherwise. `1000` by default.\n /\nconfiguration.maxQueueSize = 1000;\n/*\n * Whether the posthog client should automatically make a capture call for application lifecycle events,\n * such as \"Application Installed\", \"Application Updated\" and \"Application Opened\".\n /\nconfiguration.captureApplicationLifecycleEvents = NO;\n/*\n * Whether the posthog client should record bluetooth information. If `YES`, please make sure to add a\n * description for `NSBluetoothPeripheralUsageDescription` in your `Info.plist` explaining explaining why\n * your app is accessing Bluetooth APIs. `NO` by default.\n /\nconfiguration.shouldUseBluetooth = NO;\n/*\n * Whether the posthog client should automatically make a screen call when a view controller is added to\n * a view hierarchy. Because the underlying implementation uses method swizzling, we recommend initializing\n * the posthog client as early as possible (before any screens are displayed), ideally during the\n * Application delegate's applicationDidFinishLaunching method.\n /\nconfiguration.recordScreenViews = NO;\n/*\n * Whether the posthog client should automatically capture in-app purchases from the App Store.\n /\nconfiguration.captureInAppPurchases = NO;\n/*\n * Whether the posthog client should automatically capture push notifications.\n /\nconfiguration.capturePushNotifications = NO;\n/*\n * Whether the posthog client should automatically capture deep links. You'll still need to call the\n * continueUserActivity and openURL methods on the posthog client.\n /\nconfiguration.captureDeepLinks = NO;\n/*\n * Whether the posthog client should include the `$device_id` property when sending events.\n * When enabled, `UIDevice`'s `identifierForVendor` property is used. Changing the value of\n * of this property after initializing the client will have no effect.\n * The default value is `YES`.\n /\nconfiguration.shouldSendDeviceID = YES;\n```\nThank you",
    "tag": "posthog"
  },
  {
    "title": "install.mdx",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/sdks/ios/_snippets/install.mdx",
    "content": "PostHog is available through CocoaPods and Carthage or you can add it as a Swift Package Manager based dependency.\nCocoaPods\n`ruby file=Podfile\npod \"PostHog\", \"~> 1.1\"`\nCarthage\n`ruby file=Cartfile\ngithub \"posthog/posthog-ios\"`\nSwift Package Manager\nAdd PostHog as a dependency in your Xcode project \"Package Dependencies\" and select the project target for your app, as appropriate.\nFor a Swift Package Manager based project, add PostHog as a dependency in your \"Package.swift\" file's Package dependencies section:\n`swift file=Package.swift\ndependencies: [\n  .package(url: \"https://github.com/PostHog/posthog-ios.git\", from: \"2.0.0\")\n],`\nand then as a dependency for the Package target utilizing PostHog:\n```swift\n  .target(\n    name: \"myApp\",\n    dependencies: [.product(name: \"PostHog\", package: \"posthog-ios\")]),",
    "tag": "posthog"
  },
  {
    "title": "identify.mdx",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/sdks/ios/_snippets/identify.mdx",
    "content": "When you start tracking events with PostHog, each user gets an anonymous ID that is used to identify them in the system.\nIn order to link this anonymous user with someone from your database, use the `identify` call. \nIdentify lets you add metadata to your users so you can easily identify who they are in PostHog, as well as do things \nlike segment users by these properties.\nAn identify call requires:\n\n`distinct_id` which uniquely identifies your user in your database\n`properties` with a dictionary of key:value pairs\n\nFor example:\n`objectivec\n// in objective-c\n[[PHGPostHog sharedPostHog] identify:@\"distinct_id_from_your_database\"\n                          properties:@{ @\"name\": @\"Peter Griffin\",\n                                       @\"email\": @\"peter@familyguy.com\" }];`\n`swift\n// in swift\nposthog.identify(\"user_id_from_your_database\", \n          properties: [\"name\": \"Peter Griffin\", \"email\": \"peter@familyguy.com\"])`\nThe most obvious place to make this call is whenever a user signs up, or when they update their information.",
    "tag": "posthog"
  },
  {
    "title": "capture.mdx",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/sdks/ios/_snippets/capture.mdx",
    "content": "Capture allows you to send events related to anything a user does within your system, which you can later use in PostHog to find \npatterns in usage, work out which features to improve, or find out where people are giving up.\nA `capture` call requires:\n\n`event name` to specify the event\nWe recommend using [noun] [verb], like `movie played` or `movie updated` to easily identify what your events mean later on.\n\nOptionally you can submit:\n\n`properties`, which can be an array with any information you'd like to add\n\nFor example:\n`objectivec\n// in objective-c\n[[PHGPostHog sharedPostHog] capture:@\"Signed Up\" properties:@{ @\"plan\": @\"Pro++\" }];`\n```swift\n// in swift\nposthog.capture(\"Signed Up\", properties: [\"plan\": \"Pro++\"])",
    "tag": "posthog"
  },
  {
    "title": "configure.mdx",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/sdks/ios/_snippets/configure.mdx",
    "content": "With Objective-C\n```objectivec\nimport \nimport \n// on posthog.com\nPHGPostHogConfiguration *configuration = [PHGPostHogConfiguration configurationWithApiKey:@\"YOUR_API_KEY\"];\n// self-hosted\nPHGPostHogConfiguration *configuration = [PHGPostHogConfiguration configurationWithApiKey:@\"YOUR_API_KEY\" \n                                                                          host:@\"https://app.posthog.com\"];\nconfiguration.captureApplicationLifecycleEvents = YES; // Record certain application events automatically!\nconfiguration.recordScreenViews = YES; // Record screen views automatically!\n[PHGPostHog setupWithConfiguration:configuration];\n```\nWith Swift\n```swift\nimport PostHog\n// `host` is optional if you use PostHog Cloud (app.posthog.com)\nlet configuration = PHGPostHogConfiguration(apiKey: \"\", host: \"\")\nconfiguration.captureApplicationLifecycleEvents = true; // Record certain application events automatically!\nconfiguration.recordScreenViews = true; // Record screen views automatically!\nPHGPostHog.setup(with: configuration)\nlet posthog = PHGPostHog.shared()",
    "tag": "posthog"
  },
  {
    "title": "Package",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/sdks/flutter/index.mdx",
    "content": "\ntitle: Flutter\nsidebarTitle: Flutter\nsidebar: Docs\nshowTitle: true\ngithub: https://github.com/PostHog/posthog-flutter\nicon: ../../../images/docs/integrate/flutter.svg\nfeatures:\n    eventCapture: true\n    userIdentification: true\n    autoCapture: false\n    sessionRecording: false\n    featureFlags: false\n    groupAnalytics: false\n\nThis is an optional library you can install if you're working with Flutter. It uses an internal queue to make calls fast and non-blocking. It also batches requests and flushes asynchronously, making it perfect to use in any part of your mobile app.\nPackage\nimport FlutterPackage from './_snippets/package.mdx'\n\nUsage\nTo use this, add `posthog_flutter` as a dependency in your pubspec.yaml file.\nSupported methods\n| Method           | Android | iOS | Web |\n| ---------------- | ------- | --- | --- |\n| `identify`       | X       | X   | X   |\n| `capture`        | X       | X   | X   |\n| `screen`         | X       | X   | X   |\n| `alias`          | X       | X   | X   |\n| `getAnonymousId` | X       | X   | X   |\n| `reset`          | X       | X   | X   |\n| `disable`        | X       | X   |     |\n| `enable`         | X       | X   |     |\n| `debug`          | X*     | X   | X   |\n| `setContext`     | X       | X   |     |\n\nDebugging must be set as a configuration parameter in `AndroidManifest.xml` (see below). The Official PostHog Library does not offer the debug method for Android.\n\nExample\n```dart\nimport 'package:flutter/material.dart';\nimport 'package:posthog_flutter/posthog_flutter.dart';\nvoid main() => runApp(MyApp());\nclass MyApp extends StatelessWidget {\n  @override\n  Widget build(BuildContext context) {\n    Posthog.screen(\n      screenName: 'Example Screen',\n    );\n    return MaterialApp(\n      home: Scaffold(\n        appBar: AppBar(\n          title: Text('PostHog example app'),\n        ),\n        body: Center(\n          child: FlatButton(\n            child: Text('TRACK ACTION WITH POSTHOG'),\n            onPressed: () {\n              Posthog.capture(\n                eventName: 'ButtonClicked',\n                properties: {\n                  'foo': 'bar',\n                  'number': 1337,\n                  'clicked': true,\n                  '$set': {\n                    'userProp': 'value1'\n                  },\n                  '$set_once': {\n                    'userProp': 'value2'\n                  }\n                },\n              );\n            },\n          ),\n        ),\n      ),\n      navigatorObservers: [\n        PosthogObserver(),\n      ],\n    );\n  }\n}\n```\nInstallation\nimport FlutterInstall from './_snippets/install.mdx'\n\nIssues\nPlease file any issues, bugs, or feature requests in our GitHub repository.\nContributing",
    "tag": "posthog"
  },
  {
    "title": "install.mdx",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/sdks/flutter/_snippets/install.mdx",
    "content": "Setup your Android, iOS, and/or web sources as described at PostHog.com and generate your API keys.\nSet your PostHog API key and change the automatic event tracking (only for Android and iOS) on if you wish the library to take care of it for you.\nRemember that the application lifecycle events won't have any special context set for you by the time it is initialized. If you are using a self-hosted instance of PostHog you will need to have the public hostname or IP for your instance as well.\nAndroid\n`xml file=AndroidManifest.xml\n<manifest xmlns:android=\"http://schemas.android.com/apk/res/android\" package=\"com.posthog.posthog_flutter_example\">\n    <application>\n        <activity>\n            [...]\n        </activity>\n        <meta-data android:name=\"com.posthog.posthog.API_KEY\" android:value=\"<ph_project_api_key>\" />\n        <meta-data android:name=\"com.posthog.posthog.POSTHOG_HOST\" android:value=\"<ph_instance_address>\" />\n        <meta-data android:name=\"com.posthog.posthog.TRACK_APPLICATION_LIFECYCLE_EVENTS\" android:value=\"false\" />\n        <meta-data android:name=\"com.posthog.posthog.DEBUG\" android:value=\"false\" />\n    </application>\n</manifest>`\niOS\n```xml file=Info.plist\n\n\n\n\n    [...]\n    com.posthog.posthog.API_KEY\n\ncom.posthog.posthog.POSTHOG_HOST\n\ncom.posthog.posthog.TRACK_APPLICATION_LIFECYCLE_EVENTS\n\n\n    [...]\n\n\n```\nWeb\n```html\n\n\n\n\nexample\n\n\n\n\n\n\n```",
    "tag": "posthog"
  },
  {
    "title": "Installation",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/sdks/android/index.mdx",
    "content": "\ntitle: Android\nsidebarTitle: Android\nsidebar: Docs\nshowTitle: true\ngithub: https://github.com/PostHog/posthog-android\nicon: ../../../images/docs/integrate/android.svg\nfeatures:\n    eventCapture: true\n    userIdentification: true\n    autoCapture: false\n    sessionRecording: false\n    featureFlags: true\n    groupAnalytics: true\n\nIt uses an internal queue to make calls fast and non-blocking. It also batches requests and flushes asynchronously, making it perfect to use in any part of your mobile app.\nimport AndroidInstall from './_snippets/install.mdx'\nimport AndroidConfigure from './_snippets/configure.mdx'\nimport AndroidIdentify from './_snippets/identify.mdx'\nimport AndroidCapture from './_snippets/capture.mdx'\nInstallation\n\nConfiguration\n\nMaking calls\nIdentify\n\nWe highly recommend reading our section on Identifying users to better understand how to correctly use this method.\n\n\nCapture\n\nSetting user properties via an event\nTo set properties on your users via an event, you can leverage the event properties `$set` and `$set_once`.\n$set\nExample\n```java\n// import java.util.HashMap;\nHashMap userProps = new HashMap();\nuserProps.put(\"string\", \"value1\");\nuserProps.put(\"integer\", 2);\nPostHog.with(this)\n       .capture(\"Button B Clicked\", new Properties()\n                                        .putValue(\"color\", \"blue\")\n                                        .putValue(\"$set\", userProps));\n```\nUsage\nWhen capturing an event, you can pass a property called `$set` as an event property, and specify its value to be an object with properties to be set on the user that will be associated with the user who triggered the event.\n$set_once\nExample\n```java\n// import java.util.HashMap;\nHashMap userProps = new HashMap();\nuserProps.put(\"string\", \"value1\");\nuserProps.put(\"integer\", 2);\nPostHog.with(this)\n       .capture(\"Button B Clicked\", new Properties()\n                                        .putValue(\"color\", \"blue\")\n                                        .putValue(\"$set_once\", userProps));\n```\nUsage\n`$set_once` works just like `$set`, except that it will only set the property if the user doesn't already have that property set.\nFlush\nYou can set the number of events in the configuration that should queue before flushing.\nSetting this to `1` will send events immediately and will use more battery. The default value for this is `20`.\nYou can also configure the flush interval. By default we flush all events after `30` seconds,\nno matter how many events have been gathered.\n```java\nPostHog posthog = new PostHog.Builder(this, POSTHOG_API_KEY, POSTHOG_HOST)\n  .flushQueueSize(20)\n  .flushInterval(30, TimeUnit.SECONDS)\n  .build();\n```\nYou can also manually flush the queue:\n`java\nPostHog.with(this)\n       .flush();`\nReset\nTo reset the user's ID and anonymous ID, call `reset`. Usually you would do this right after the user logs out.\n`java\nPostHog.with(this)\n       .reset();`\nSending screen views\nWith `recordScreenViews()`, PostHog will try to record all screen changes automatically.\nIf you want to manually send a new screen capture event, use the `screen` function.\nThis function requires a `name`. You may also pass in an optional `properties` object.\n`java\nPostHog.with(this)\n       .screen(\"Dashboard\", new Properties()\n                                  .putValue(\"background\", \"blue\")\n                                  .putValue(\"hero\", \"superhog\"));`\nFeature Flags\nPostHog v1.10.0 introduced Feature Flags, which allow you to safely deploy and roll back new features. This feature requires posthog-android version of `2.0.0` or above.\nHere's how you can use them:\n\nCheck if a feature is enabled:\n\n`java\nPostHog.with(this).isFeatureEnabled(\"enabled-flag\")`\n\nTrigger a reload of the feature flags:\n\n`java\nPostHog.with(this).reloadFeatureFlags()`\nGet a flag value\nIf you're using multivariate feature flags, you can also get the value of the flag, as well as whether or not it is enabled.\n`java\nPostHog.with(this).getFeatureFlag(\"enabled-flag\")`\n\nBy default, this function will send a `$feature_flag_called` event to your instance every time it's called so you're able to do analytics.\n\nGroups\nGroup analytics allows you to associate the events for that person's session with a group (e.g. teams, organizations, etc.). Read the Group Analytics guide for more information.\n\nNote:  This is a paid feature and is not available on the open-source or free cloud plan. Learn more here.\n\n\nAssociate the events for this session with a group\n\n`java\nPostHog.with(this).group(\"organization\", \"42dlsfj23f\") // organization is the group type, 42dlsfj23f is the group ID`\n\nAssociate the events for this session with a group AND update the properties of that group\n\n`java\nPostHog.with(this).group(\"organization\", \"42dlsfj23f\", new Properties().putValue(\"name\", \"Awesome Inc.\"));`\nThe `name` is a special property which is used in the PostHog UI for the name of the Group. If you don't specify a `name` property, the group ID will be used instead.\nAll configuration options\nWhen creating the PostHog client, there are many options you can set:\n```java\nPostHog posthog = new PostHog.Builder(this, POSTHOG_API_KEY, POSTHOG_HOST)\n    // Record certain application events automatically! (off/false by default)\n    .captureApplicationLifecycleEvents()\n\n\n``` // Record screen views automatically! (off/false by default)\n.recordScreenViews()\n\n// Capture deep links as part of the screen call. (off by default)\n.captureDeepLinks()\n\n // Maximum number of events to keep in queue before flushing (20)\n.flushQueueSize(int flushQueueSize)\n\n// Max delay before flushing the queue (30 seconds)\n.flushInterval(long flushInterval, TimeUnit timeUnit)\n\n// Enable or disable collection of ANDROID_ID (true)\n.collectDeviceId(boolean collect)\n\n.build();\n```\n\n\n```\nThank you",
    "tag": "posthog"
  },
  {
    "title": "identify.mdx",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/sdks/android/_snippets/identify.mdx",
    "content": "When you start tracking events with PostHog, each user gets an anonymous ID that is used to identify them in the system.\nIn order to link this anonymous user with someone from your database, use the `identify` call. \nIdentify lets you add metadata to your users so you can easily identify who they are in PostHog, as well as do things \nlike segment users by these properties.\nAn identify call requires:\n\n`distinctId` which uniquely identifies your user in your database\n`userProperties` with a dictionary of key:value pairs\n\n`java\nPostHog.with(this)\n       .identify(distinctID, new Properties()\n                                .putValue(\"name\", \"My Name\")\n                                .putValue(\"email\", \"user@posthog.com\"));`\nThe most obvious place to make this call is whenever a user signs up, or when they update their information.",
    "tag": "posthog"
  },
  {
    "title": "capture.mdx",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/sdks/android/_snippets/capture.mdx",
    "content": "Capture allows you to capture anything a user does within your system, which you can later use in PostHog to find \npatterns in usage, work out which features to improve, or find out where people are giving up.\nA `capture` call requires:\n\n`event` to specify the event name\nWe recommend naming events with \"[noun] [verb]\", such as `movie played` or `movie updated`, in order to easily identify what your events mean later on (we know this from experience).\n\nOptionally you can submit:\n\n`properties`, which can be an array with any information you'd like to add\n\nFor example:\n```java\nPostHog.with(this)\n       .capture(\"Button B Clicked\", new Properties()\n                                        .putValue(\"color\", \"blue\")\n                                        .putValue(\"icon\", \"new2-final\"));",
    "tag": "posthog"
  },
  {
    "title": "configure.mdx",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/sdks/android/_snippets/configure.mdx",
    "content": "The best place to initialize the client is in your `Application` subclass.\n```java\npublic class SampleApp extends Application {\n  private static final String POSTHOG_API_KEY = \"\";\n  private static final String POSTHOG_HOST = \"\";\n@Override\n  public void onCreate() {\n    // Create a PostHog client with the given context, API key and host.\n    PostHog posthog = new PostHog.Builder(this, POSTHOG_API_KEY, POSTHOG_HOST)\n      .captureApplicationLifecycleEvents() // Record certain application events automatically!\n      .recordScreenViews() // Record screen views automatically!\n      .build();\n\n\n```// Set the initialized instance as a globally accessible instance.\nPostHog.setSingletonInstance(posthog);\n\n// Now anytime you call PostHog.with, the custom instance will be returned.\nPostHog posthog = PostHog.with(this);\n```\n\n\n}\n}",
    "tag": "posthog"
  },
  {
    "title": "or",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/sdks/node/index.mdx",
    "content": "\ntitle: Node.js\nsidebarTitle: Node.js\nsidebar: Docs\nshowTitle: true\ngithub: https://github.com/PostHog/posthog-node\nicon: ../../../images/docs/integrate/nodejs.svg\nfeatures:\n    eventCapture: true\n    userIdentification: true\n    autoCapture: false\n    sessionRecording: false\n    featureFlags: true\n    groupAnalytics: true\n\nimport CohortExpansionSnippet from '../_snippets/cohort-expansion'\nIf you're working with Node.js, the official `posthog-node` library is the simple way to integrate your software with PostHog. This library uses an internal queue to make calls fast and non-blocking. It also batches requests and flushes asynchronously, making it perfect to use in any part of your web app or other server-side application that needs performance. And in addition to event capture, feature flags are supported as well.\nInstallation\nRun either `npm` or `yarn` in terminal to add it to your project:\n```bash\nnpm install posthog-node --save\nor\nyarn add posthog-node\n```\nIn your app, set your API key before making any calls.\n```node\nimport { PostHog } from 'posthog-node'\nconst client = new PostHog(\n    '',\n    { host: '' } // You can omit this line if using PostHog Cloud\n)\n// On program exit, call shutdown to stop pending pollers and flush any remaining events\nawait client.shutdownAsync()\n```\nYou can find your key in the 'Project Settings' page in PostHog.\n\nNote: As a rule of thumb, we do not recommend hardcoding API keys. Setting it as an environment variable would be best.\n\nOptions\n| Variable                      | Description                                                                                                           | Default value              |\n| :---------------------------- | :-------------------------------------------------------------------------------------------------------------------- | :------------------------- |\n| `host`                        | Your PostHog host                                                                                                     | `https://app.posthog.com/` |\n| `flushAt`                     | After how many capture calls we should flush the queue (in one batch)                                                 | `20`                       |\n| `flushInterval`               | After how many ms we should flush the queue                                                                           | `10000`                    |\n| `personalApiKey`              | An optional personal API key for evaluating feature flags locally | `null`                     |\n| `featureFlagsPollingInterval` | Interval in milliseconds specifying how often feature flags should be fetched from the PostHog API                    | `300000`                   |\n| `requestTimeout`              | Timeout in milliseconds for any calls                                                                                 | `10000`                    |\n| `maxCacheSize`                | Maximum size of cache that deduplicates $feature_flag_called calls per user.                                          | `50000`                    |\n\nNote: When using PostHog in an AWS Lambda function or a similar serverless function environment, make sure you set `flushAt` to `1` and `flushInterval` to `0`.\n\nMaking calls\nCapture\nCapture allows you to capture anything a user does within your system, which you can later use in PostHog to find patterns in usage, work out which features to improve or where people are giving up.\nA `capture` call requires:\n\n`distinct id` which uniquely identifies your user\n\n`event name` to identify the event\n\n\nWe recommend naming events with \"[noun] [verb]\", such as `movie played` or `movie updated`, in order to easily identify what your events mean later on (we know this from experience).\n\n\nOptionally you can submit:\n\n`properties`, which is an object with any information you'd like to add\n`sendFeatureFlags`, a boolean that determines whether to send current known feature flags with this event. This is useful when running experiments which depends on this event. However, this makes things slow. Read this tutorial for manually computing this information and speeding things up\n\nFor example:\n`node\nclient.capture({\n    distinctId: 'distinct id',\n    event: 'movie played',\n    properties: {\n        movieId: '123',\n        category: 'romcom',\n    },\n})`\nSetting user properties via an event\nTo set properties on your users via an event, you can leverage the event properties `$set` and `$set_once`.\n$set\n`node\nclient.capture({\n    distinctId: 'distinct id',\n    event: 'movie played',\n    properties: {\n        $set: { userProperty: 'value' },\n    },\n})`\nWhen capturing an event, you can pass a property called `$set` as an event property, and specify its value to be an object with properties to be set on the user that will be associated with the user who triggered the event.\n$set_once\n`node\nclient.capture({\n    distinctId: 'distinct id',\n    event: 'movie played',\n    properties: {\n        $set_once: { userProperty: 'value' },\n    },\n})`\n`$set_once` works just like `$set`, except that it will only set the property if the user doesn't already have that property set.\nIdentify\n\nWe highly recommend reading our section on Identifying users to better understand how to correctly use this method.\n\nIdentify lets you add metadata to your users so you can easily identify who they are in PostHog, as well as do things\nlike segment users by these properties.\nAn `identify` call requires:\n\n`distinctId` \u2013 a distinct ID belonging to the user\n`properties` \u2013 a user properties object\n\nFor example:\n`node\nclient.identify({\n    distinctId: 'user:123',\n    properties: {\n        email: 'john@doe.com',\n        proUser: false,\n    },\n})`\nThe most obvious place to make this call is whenever a user signs up, or when they update their information.\nAlias\nTo connect whatever a user does before they sign up or log in with what they do after you need to make an alias call. This will allow you to answer questions like \"Which marketing channels leads to users churning after a month?\" or \"What do users do on our website before signing up?\"\nIn a purely back-end implementation, this means whenever an anonymous user does something, you'll want to send a session ID with the capture call. Then, when that users signs up, you want to do an alias call with the session ID and the newly created user ID.\nThe same concept applies for when a user logs in.\nIf you're using PostHog in the front-end and back-end, doing the `identify` call in the frontend will be enough.\nAn `alias` call requires:\n\n`distinctId` \u2013 the user id\n`alias` \u2013 the anonymous session distinct ID\n\nFor example:\n`node\nclient.alias({\n    distinctId: 'user:123',\n    alias: 'session:12345',\n})`\nSending page views\nIf you're aiming for a full back-end implementation of PostHog, you can send `pageviews` from your backend, like so:\n`node\nclient.capture({\n    distinctId: 'distinct id',\n    event: '$pageview',\n    properties: {\n        $current_url: 'https://example.com',\n    },\n})`\nFeature flags\nPostHog's feature flags enable you to safely deploy and roll back new features.\nWhen using them with one of libraries, you should check if a feature flag is enabled and use the result to toggle functionality on and off in you application.\nHow to check if a flag is enabled\n\nNote: Whenever we face an error computing the flag, the library returns `undefined`, instead of `true`, `false`, or a string variant value.\n\n```node\n// isFeatureEnabled(key: string, distinctId: string, options: {}): Promise\nconst isMyFlagEnabledForUser = await client.isFeatureEnabled('flag-key', 'user distinct id')\nif (isMyFlagEnabledForUser) {\n    // Do something differently for this user\n}\n```\nGet a flag value\nIf you're using multivariate feature flags, you can also get the value of the flag, as well as whether or not it is enabled.\n\nNote: Whenever we face an error computing the flag, the library returns `None`, instead of `true` or `false` or a string variant value.\n\n`node\n// getFeatureFlag(key: string, distinctId: string, options: {}): Promise<string | boolean | undefined>\nconst flagValue = await client.getFeatureFlag('flag-key', 'user distinct id')`\nGet a flag payload\nPosthog Node v2.3.0 introduces feature flag payloads. Feature flags can be returned with matching payloads which are JSONType (string, number, boolean, dictionary, array) values.\nThis allows for custom configurations based on values defined in the posthog app.\n\nNote: `getFeatureFlag` does not need to be called prior to `getFeatureFlagPayload`. `getFeatureFlagPayload` will implicitly perform\ngetFeatureFlag to determine the matching flag and return the corresponding payload.\n\n`node\n// getFeatureFlagPayload(key: string, distinctId: string, matchValue?: string | boolean, options: {}): Promise<JsonType | undefined>\nconst flagPayload = await client.getFeatureFlagPayload('flag-key', 'user distinct id')`\nOverriding server properties\nSometimes, you might want to evaluate feature flags using properties that haven't been ingested yet, or were set incorrectly earlier. You can do so by setting properties the flag depends on with these calls.\nFor example, if the `beta-feature` depends on the `is_authorized` property, and you know the value of the property, you can tell PostHog to use this property, like so:\n`node\n// getFeatureFlag(\n//    key: string,\n//    distinctId: string,\n//    options?: {\n//      groups?: Record<string, string>\n//      personProperties?: Record<string, string>\n//      groupProperties?: Record<string, Record<string, string>>\n//      onlyEvaluateLocally?: boolean\n//      sendFeatureFlagEvents?: boolean\n//    }\n//  ): Promise<string | boolean | undefined>\nconst flagValue = await client.getFeatureFlag('flag-key', 'user distinct id', {\n    personProperties: { is_authorized: true },\n})`\nThe same holds for groups. If you have a group named `organisation`, you can add properties like so:\n`node\nconst flagValue = await client.getFeatureFlag('flag-key', 'user distinct id', {groups:{'organisation': 'google'}, groupProperties:{'organisation': {'is_authorized': True}})`\nGetting all flag values\nYou can also get all known flag values as well. This is useful when you want to seed a frontend client with initial known flags. Like all methods above, this also takes optional person and group properties, if known.\n`node\nawait client.getAllFlags('distinct id', { groups: {}, personProperties: { is_authorized: True }, groupProperties: {} })\n// returns dict of flag key and value pairs.`\nLocal Evaluation\n\nNote: To enable local evaluation of feature flags you must also set a `personal_api_key` when configuring the integration, as described in the Installation section.\nNote: This feature requires version 2.0 of the library, which in turn requires a minimum PostHog version of 1.38\n\nAll feature flag evaluation requires an API request to your PostHog servers to get a response. However, where latency matters, you can evaluate flags locally. This is much faster, and requires two things to work:\n\nThe library must be initialised with a personal API key\nYou must know all person or group properties the flag depends on.\n\nThen, the flag can be evaluated locally. The method signature looks exactly like above.\n`node\nawait client.getFeatureFlag('beta-feature', 'distinct id', { personProperties: { is_authorized: True } })\n// returns string or None`\n\nNote: New feature flag definitions are polled every 30 seconds by default, which means there will be up to a 30 second delay between you changing the flag definition, and it reflecting on your servers. You can change this default on the client by setting `featureFlagsPollingInterval` during client initialisation.\n\nThis works for `getAllFlags` as well. It evaluates all flags locally if possible. If even one flag isn't locally evaluable, it falls back to decide.\n`node\nawait client.getAllFlags('distinct id', { groups: {}, personProperties: { is_authorized: True }, groupProperties: {} })\n// returns dict of flag key and value pairs.`\nRestricting evaluation to local only\nSometimes, performance might matter to you so much that you never want an HTTP request roundtrip delay when computing flags. In this case, you can set the `only_evaluate_locally` parameter to true, which tries to compute flags only with the properties it has. If it fails to compute a flag, it returns `None`, instead of going to PostHog's servers to get the value.\n\nReloading feature flags\nWhen initializing PostHog, you can configure the interval at which feature flags are polled (fetched from the server). However, if you need to force a reload, you can use `reloadFeatureFlags`:\n```node\nawait client.reloadFeatureFlags()\n// Do something with feature flags here\n```\nGroup analytics\nGroup analytics allows you to associate an event with a group (e.g. teams, organizations, etc.). Read the Group Analytics guide for more information.\n\nNote:  This is a paid feature and is not available on the open-source or free cloud plan. Learn more here.\n\n\nCapture an event and associate it with a group\n\n`node\nclient.capture({\n    event: 'some event',\n    distinctId: '[distinct id]',\n    groups: { company: '42dlsfj23f' },\n})`\n\nUpdate properties on a group\n\n`node\nclient.groupIdentify({\n    groupType: 'company',\n    groupKey: '42dlsfj23f',\n    properties: {\n        name: 'Awesome Inc',\n        employees: 11,\n    },\n})`\nShutdown\nYou should call `shutdown` on your program's exit to exit cleanly:\n`node\n// Stop pending pollers and flush any remaining events\nclient.shutdown()\n// or\nawait client.shutdownAsync()`\nUsing in a short-lived process like AWS Lambda\nPostHogs's client SDKs are all designed to queue and batch requests in the background to optimise API calls and network time. For lambda environments (or also when shutting down a standard Node.js app) we provide a method .shutdownAsync which can be awaited and ensures the queued events are all flushed to the API.\nFor example:\n```node\nexport const handler() {\n  posthog.capture({\n    distinctId: 'distinct id',\n    event: 'thing happened'\n  })\nposthog.capture({\n    distinctId: 'distinct id',\n    event: 'other thing happened'\n  })\n// So far 2 events are queued but not sent\n// Calling shutdown, flushed the queue but batched into 1 API call for maximum efficiency\n  await posthog.shutdownAsync()\n}\n```\nUpgrading from V1 to V2\nV2.x.x of the Node.js library is completely rewritten in Typescript and is based on a new JS core shared with other JavaScript based libraries with the goal of ensuring new features and fixes reach the different libraries at the same pace.\nWith the release of V2, the API was kept mostly the same but with some small changes and deprecations:\n\nThe minimum PostHog version requirement is 1.38\nThe `callback` parameter passed as an optional last argument to most of the methods is no longer supported\nThe method signature for `isFeatureEnabled` and `getFeatureFlag` is slightly modified. See the above documentation for each method for more details.\n",
    "tag": "posthog"
  },
  {
    "title": "returns True or False or Nil",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/sdks/ruby/index.mdx",
    "content": "\ntitle: Ruby\nsidebarTitle: Ruby\nsidebar: Docs\nshowTitle: true\ngithub: https://github.com/PostHog/posthog-ruby\nicon: ../../../images/docs/integrate/ruby.svg\nfeatures:\n    eventCapture: true\n    userIdentification: true\n    autoCapture: false\n    sessionRecording: false\n    featureFlags: true\n    groupAnalytics: true\n\nimport CohortExpansionSnippet from '../_snippets/cohort-expansion'\nThe `posthog-ruby` library provides tracking functionality on the server-side for applications built in Ruby.\nIt uses an internal queue to make calls fast and non-blocking. It also batches requests and flushes asynchronously, making it perfect to use in any part of your web app or other server-side application that needs performance.\nInstallation\nAdd this to your `Gemfile`:\n`bash\ngem \"posthog-ruby\"`\nIn your app, set your API key before making any calls. If setting a custom `host`, make sure to include the protocol (e.g. `https://`).\n`ruby\nposthog = PostHog::Client.new({\n  api_key: \"<ph_project_api_key>\",\n  host: \"<ph_instance_address>\", # You can remove this line if you're using https://app.posthog.com\n  on_error: Proc.new { |status, msg| print msg }\n})`\nYou can find your key in the 'Project Settings' page in PostHog.\nDebug logging\nThe log level by default is set to WARN. You can change it to DEBUG if you want to debug the client by running `posthog.logger.level = Logger::DEBUG`, where `posthog` is your initialized `PostHog::Client` instance.\nMaking calls\nCapture\nCapture allows you to capture anything a user does within your system, which you can later use in PostHog to find patterns in usage, work out which features to improve or where people are giving up.\nA `capture` call requires:\n\n`distinct id` which uniquely identifies your user\n\n`event name` to specify the event\n\n\nWe recommend naming events with \"[noun] [verb]\", such as `movie played` or `movie updated`, in order to easily identify what your events mean later on (we know this from experience).\n\n\nOptionally you can submit:\n\n`properties`, which is a dictionary with any information you'd like to add\n`timestamp`, a datetime object for when the event happened. If this isn't submitted, it'll be set to the current time\n`send_feature_flags`, a boolean that determines whether to send current known feature flags with this event. This is useful when running experiments which depends on this event. However, this makes things slow. Read this tutorial for manually computing this information and speeding things up\n\nFor example:\n`ruby\nposthog.capture({\n    distinct_id: 'distinct id',\n    event: 'movie played',\n    properties: {\n        movie_id: '123',\n        category: 'romcom'\n    }\n})`\nSetting user properties via an event\nTo set properties on your users via an event, you can leverage the event properties `$set` and `$set_once`.\n$set\nExample\n`ruby\nposthog.capture({\n    distinct_id: 'distinct id',\n    event: 'movie played',\n    properties: {\n        $set: { userProperty: 'value' }\n    }\n})`\nUsage\nWhen capturing an event, you can pass a property called `$set` as an event property, and specify its value to be an object with properties to be set on the user that will be associated with the user who triggered the event.\n$set_once\nExample\n`ruby\nposthog.capture({\n    distinct_id: 'distinct id',\n    event: 'movie played',\n    properties: {\n        $set_once: { userProperty: 'value' }\n    }\n})`\nUsage\n`$set_once` works just like `$set`, except that it will only set the property if the user doesn't already have that property set.\nIdentify\n\nWe highly recommend reading our section on Identifying users to better understand how to correctly use this method.\n\nIdentify lets you add metadata to your users so you can easily identify who they are in PostHog, as well as do things\nlike segment users by these properties.\nAn `identify` call requires:\n\n`distinct id` which uniquely identifies your user\n`properties` with a dict with any key:value pairs\n\nFor example:\n`ruby\nposthog.identify({\n  distinct_id: \"user:123\",\n  properties: {\n    email: 'john@doe.com',\n    pro_user: false\n  }\n})`\nThe most obvious place to make this call is whenever a user signs up, or when they update their information.\nAlias\nTo connect whatever a user does before they sign up or log in with what they do after you need to make an alias call. This will allow you to answer questions like \"Which marketing channels leads to users churning after a month?\" or \"What do users do on our website before signing up?\"\nIn a purely back-end implementation, this means whenever an anonymous user does something, you'll want to send a session ID with the capture call. Then, when that users signs up, you want to do an alias call with the session ID and the newly created user ID.\nThe same concept applies for when a user logs in.\nIf you're using PostHog in the front-end and back-end, doing the `identify` call in the frontend will be enough.\nAn `alias` call requires:\n\n`distinct_id` \u2013 the logged in user id\n`alias` \u2013 the anonymous session distinct ID\n\nFor example:\n`ruby\nposthog.alias({\n  distinct_id: \"user:123\",\n  alias: \"session:12345\",\n})`\nSending page views\nIf you're aiming for a full back-end implementation of PostHog, you can send pageviews from your backend\n`ruby\nposthog.capture({\n    distinct_id: 'distinct id',\n    event: '$pageview',\n    properties: {\n        '$current_url': 'https://example.com'\n    }\n})`\nFeature flags\nPostHog's feature flags enable you to safely deploy and roll back new features.\nWhen using them with one of libraries, you should check if a feature flag is enabled and use the result to toggle functionality on and off in you application.\nHow to check if a flag is enabled\n\nNote: Whenever we face an error computing the flag, the library returns `Nil`, instead of `true` or `false`.\n\n```ruby\nposthog.is_feature_enabled('beta-feature', 'distinct id')\nreturns True or False or Nil\n```\nExample use case\nHere's how you might send different users a different version of your homepage, for example:\n`ruby\ndef homepage(request):\n    template = \"new.html\" if posthog.is_feature_enabled('new_ui', 'distinct id') else \"old.html\"\n    return render_template(template, request=request)`\n\nNote: Feature flags are persistent for users across sessions. Read more about feature flag persistence on our dedicated page.\n\nGet a flag value\nIf you're using multivariate feature flags, you can also get not just whether the flag is enabled, but what value its enabled to.\n\nNote: Whenever we face an error computing the flag, the library returns `Nil`, instead of `true` or `false` or a string variant value.\n\n```ruby\nposthog.get_feature_flag('beta-feature', 'distinct id')\nreturns string or True or False or Nil\n```\nOverriding server properties\nSometimes, you might want to evaluate feature flags using properties that haven't been ingested yet, or were set incorrectly earlier. You can do so by setting properties the flag depends on with these calls.\nFor example, if the `beta-feature` depends on the `is_authorized` property, and you know the value of the property, you can tell PostHog to use this property, like so:\n`ruby\nposthog.get_feature_flag('beta-feature', 'distinct id', person_properties: {'is_authorized': True})`\nYou can pass symbols or strings, the library treats them the same.\nThe same holds for groups. If you have a group name `organisation`, you can add properties like so:\n```ruby\nposthog.get_feature_flag('beta-feature', 'distinct id', groups: {'organisation': 'google'}, group_properties: {'organisation': {'is_authorized': True})\nreturns string or Nil\n```\nGetting all flag values\nYou can also get all known flag values as well. This is useful when you want to seed a frontend client with initial known flags. Like all methods above, this also takes optional person and group properties, if known.\n```ruby\nposthog.get_all_flags('distinct id', groups: {}, person_properties: {'is_authorized': True}, group_properties: {})\nreturns hash of flag key and value pairs.\n```\nLocal Evaluation\n\nNote: To enable local evaluation of feature flags you must also set a `personal_api_key` when configuring the integration, as described in the Installation section.\nNote: This feature requires version 2.0 of the library, which in turn requires a minimum PostHog version of 1.38\n\nAll feature flag evaluation requires an API request to your PostHog servers to get a response. However, where latency matters, you can evaluate flags locally. This is much faster, and requires two things to work:\n\nThe library must be initialised with a personal API key\nYou must know all person or group properties the flag depends on.\n\nThen, the flag can be evaluated locally. The method signature looks exactly like above\n```ruby\nposthog.get_feature_flag('beta-feature', 'distinct id', person_properties: {'is_authorized': True})\nreturns string or Nil\n```\n\nNote: New feature flag definitions are polled every 30 seconds by default, which means there will be up to a 30 second delay between you changing the flag definition, and it reflecting on your servers. You can change this default on the client by setting `feature_flags_polling_interval = <value in seconds>`.\n\nThis works for `getAllFlags` as well. It evaluates all flags locally if possible. If even one flag isn't locally evaluable, it falls back to decide.\n```ruby\nposthog.get_all_flags('distinct id', groups: {}, person_properties: {'is_authorized': True}, group_properties: {})\nreturns hash of flag key and value pairs.\n```\nRestricting evaluation to local only\nSometimes, performance might matter to you so much that you never want an HTTP request roundtrip delay when computing flags. In this case, you can set the `only_evaluate_locally` parameter to true, which tries to compute flags only with the properties it has. If it fails to compute a flag, it returns `None`, instead of going to PostHog's servers to get the value.\n\nReloading feature flags\nWhen initializing PostHog, you can configure the interval at which feature flags are polled (fetched from the server). However, if you need to force a reload, you can use `reloadFeatureFlags`:\n```ruby\nposthog.reload_feature_flags()\n// Do something with feature flags here\n```\nGroup analytics\nGroup analytics allows you to associate an event with a group (e.g. teams, organizations, etc.). Read the Group Analytics guide for more information.\n\nNote:  This is a paid feature and is not available on the open-source or free cloud plan. Learn more here.\n\n\nCapture an event and associate it with a group\n\n`ruby\nposthog.capture({\n    distinct_id: 'distinct id',\n    event: 'movie played',\n    properties: {\n        movie_id: '123',\n        category: 'romcom'\n    }\n    groups: {\n        'company': '42dlsfj23f'\n    }\n})`\n\nUpdate properties on a group\n\n`ruby\nposthog.group_identify(\n  {\n    group_type: \"organization\",\n    group_key: \"42dlsfj23f\",\n    properties: {\n      name: \"Awesome Inc.\"\n    }\n  }\n)`\nThe `name` is a special property which is used in the PostHog UI for the name of the Group. If you don't specify a `name` property, the group ID will be used instead.\nThank you",
    "tag": "posthog"
  },
  {
    "title": "Data ingestion process",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/integrate/ingest-historic-data.mdx",
    "content": "\ntitle: Ingest historical data\nsidebar: Docs\nshowTitle: true\n\nHistorical data ingestion (or importing data), opposed to live data ingestion, is the process of transporting data from external sources into PostHog so you can benefit from PostHog product analytics on historical data. It may be that you have historical data that you want to analyze along with new live data or that you have a requirement to periodically import data from third-party sources to augment your live data.\nWhatever the reason for the historical data ingestion, this guide covers what to consider during that process.\nThe three main factors to consider are:\n\nData ingestion process: how to get the event data from the third-party source into PostHog\nImporting events: Sending the events captured in the third-party data source into PostHog as custom events\nUser identification: How to identify users within PostHog and ties those users back to the user within original data source\n\nHistorical data is sent to PostHog using either a server library or the PostHog API. For more information see the importing events section.\nData ingestion process\nSince the third-party data source will offer an API you can use the power of software to import the data from one or more sources to PostHog using the PostHog API.\nThe following factors are important in the export and then import process:\n\nThe volume of data\nAPI rate limits of both the data source and PostHog\nEnsure that only the data required for events is exported\nHandle error scenarios allowing the process to resume form the last successful point\n\nWith the above factors in mind it's recommended that you break the process up into steps such as the following:\n\nSequentially export selective data from the data source that represent key events keeping track of where you are in the sequence so that you can restart the process from the last successful point if any problems occur\nStore the selected exported data to a new data storage for faster access in future steps\n\nTransform the data to the format you will use with PostHog and again save to a storage mechanism for faster access in later steps.\nThe data format should be:\n`json\n{\n    \"event\": \"[event name]\",\n    \"distinct_id\": \"[your users' distinct id]\",\n    \"properties\": {\n        \"key1\": \"value1\",\n        \"key2\": \"value2\"\n    },\n    \"timestamp\": \"[optional timestamp in ISO 8601 format]\"\n}`\nAt this stage it's also important to consider the following:\n\nUse the same `event` name that you're going to use with your live data ingestion so the historical and live events are seen as the same type within PostHog\nUse the same unique identifier within the `distinct_id` field as you are within your live data ingestion so historical events and live events are associated with the same user\nConvert the old event property names to the new event property names you are using within the events in your live data ingestion\nEnsure that the `timestamp` is a converted version of the original timestamp is ISO 8601 format so that PostHog correctly identifies when the original event occurred\nYou may want to set an additional property within `properties` that identifies the original event within the data source\n\n\n\nSequentially import the events into PostHog keeping track of the last successfully imported event so that you can restart the process from the last successful point if any problems occur\n\n\nImporting events\nOnce you are ready to import the data into PostHog you can use one of the following:\n\na PostHog server libraries\nthe PostHog API\n\nAs mentioned above, the data should be in the following format:\n`json\n{\n    \"event\": \"[event name]\",\n    \"distinct_id\": \"[your users' distinct id]\",\n    \"properties\": {\n        \"key1\": \"value1\",\n        \"key2\": \"value2\"\n    },\n    \"timestamp\": \"[optional timestamp in ISO 8601 format]\"\n}`\nimport NodeCapture from '../sdks/node/_snippets/capture.mdx'\nimport PythonCapture from '../sdks/python/_snippets/capture.mdx'\nimport PHPCapture from '../sdks/php/_snippets/capture.mdx'\nimport GoCapture from '../sdks/go/_snippets/capture.mdx'\nimport RubyCapture from '../sdks/ruby/_snippets/capture.mdx'\nimport CURLCapture from '../sdks/curl/_snippets/capture.mdx'\nimport Tab from \"components/Tab\"\n\nThe server libraries handle batching capture requests. If you decide to use the API directly you will need to manage this yourself.\n\n\n\nNode.js\nPython\nPHP\nGo\nRuby\ncURL\n\n\n\n\nFor more information see the Node.js docs.\n\n\n\nFor more information see the python docs.\n\n\n\nFor more information see the PHP docs.\n\n\n\nFor more information see the Go docs.\n\n\n\nFor more information see the Ruby docs.\n\n\n\nFor more information see the API docs.\n\n\n\nUser identification\nAs discussed within the data ingestion process section, a unique user identifier `distinct_id` should be set for each event. In addition to setting the user with each event you can enrich information about that user by adding more properties:\nimport NodeIdentify from '../sdks/node/_snippets/identify.mdx'\nimport PythonIdentify from '../sdks/python/_snippets/identify.mdx'\nimport PHPIdentify from '../sdks/php/_snippets/identify.mdx'\nimport GoIdentify from '../sdks/go/_snippets/identify.mdx'\nimport RubyIdentify from '../sdks/ruby/_snippets/identify.mdx'\nimport CURLIdentify from '../sdks/curl/_snippets/identify.mdx'\n\n\nNode.js\nPython\nPHP\nGo\nRuby\ncURL\n\n\n\n\nFor more information see the Node.js docs.\n\n\n\nFor more information see the Python docs.\n\n\n\nFor more information see the PHP docs.\n\n\n\nFor more information see the Go docs.\n\n\n\nFor more information see the Ruby docs.\n\n\n\nFor more information see the API docs.\n\n",
    "tag": "posthog"
  },
  {
    "title": "Considerations",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/integrate/identifying-users.mdx",
    "content": "\ntitle: Identifying users\nsidebarTitle: Identifying users\nsidebar: Docs\nshowTitle: true\n\nPostHog allows you to identify your users with an ID of your choice so you can track users across platforms, connect events from before and after users log in, and leverage your preferred type of ID for filtering through your users. \nIdentifying users is usually done via our libraries, by calling the `identify` or `alias` method. This method will associate an anonymous ID with a distinct ID of your choice. In client libraries, the anonymous ID is stored locally and inferred for you, but in server-side libraries you need to tell PostHog what that anonymous ID is too.\nFor example, if you identify an user in your website as follows:\n`js\n// posthog-js\nposthog.identify('my_user_12345')`\nPostHog will then pull their anonymous ID (e.g. `17b845b08de74-033c497ed2753c-35667c03-1fa400-17b845b08dfd55`) and associate it with the ID you passed in (`my_user_12345`).\nFrom now on, all events PostHog sees with ID `17b845b08de74-033c497ed2753c-35667c03-1fa400-17b845b08dfd55` will be attributed to the person with ID `my_user_12345`. This person now has 2 distinct IDs, and either of them can be used to reference the same person.\nConsiderations\nIdentifying users is a powerful feature, but it also has the potential to create problems if misused. \nAn important mistake to avoid is using non-unique distinct IDs to identify users. Two common ways in which this can happen are:\n\nYour logic for generating IDs does not generate sufficiently strong IDs and you can end up with a clash where 2 users have the same ID\nThere's a bug, typo, or mistake in your code leading to most or all users being identified with generic IDs like `null`, `true`, or `distinctId`\n\nAll of the above scenarios are highly problematic, as they will cause distinct users to be merged together in PostHog.\nWhile implementing analytics with PostHog, make sure you avoid above pitfalls to maintain data integrity.\nPostHog also has a few built-in protections stopping the most common threats to data integrity:\n\nWe do not allow identifying users with the following IDs (case insensitive):\n`anonymous`\n`guest`\n`distinctid`\n`distinct_id`\n`id`\n`not_authenticated`\n`email`\n`undefined`\n`true`\n`false`\n\n\nWe do not allow identifying users with the following IDs (case sensitive):\n`[object Object]` \n`NaN`\n`None`\n`none`\n`null`\n`0`\nWe do not allow identifying users with empty space strings of any length (`' '`, `'       '`, etc.)\nWe do not allow merging from an already identified user (`distinct_id` user can be previously identified, but `anon_distinct_id` and `alias` user cannot).\n\nIf we encounter an `$identify` or `$create_alias` event with one of the above problems, the following will happen:\n\nWe process the event normally (it will be ingested and show up in the UI)\nWe refuse to merge users and an ingestion warning will be logged (see ingestion warnings for more details).\nThe event will be only be tied to user behind the first passed `distinct_id`\n\nFiltering internal users\nIf you want to avoid tracking users within your organization, you can do this within your project's settings.\nSignup flow with frontend and backend\nTo use PostHog effectively we want all of the events tied to the same user to be tied to the same `person_id` (see consequences of merging users). \nFor when a user signs up to your service you may trigger some events on the frontend and the backend. The key is to make sure that both frontend and backend use the same distinctId at least once.\nExample login flow\nOn the backend (example with Node.JS) you receive the signup / login code and track the user\n`js\nconst user = await createUser();\nposthog.identify({\n  distinctId: user.id,\n  properties: {\n    email: user.email\n  }\n})`\nOn the frontend you need to have the same ID passed down in order to link the two users\n`js\nconst user = await fetch(\"/api/users/@me\")\nposthog.identify(user.id)`\nIf you use a different identifier or multiple identifiers, be sure to alias the two IDs together for example on the backend with `posthog-node`\n`js\nposthog.alias({\n    distinctId: user.id,\n    alias: user.alternativeId,\n})`\nThings to be aware of\nThere's a few things to keep in mind when using a sign-up flow that involves both the frontend and backend: \n1. We have an event buffer to delay creating persons from backend events (see all about the event buffer) that will help.\n2. The event buffer has a limited time window, so the (identify or alias) event that merges the frontend and backend user should come in within that window (60s)\n3. We don't buffer `$identify` events, so from the backend take care to not send those for setting properties before the users are merged. For setting user properties you can use any custom event, e.g. \n```python\nposthog.capture(\n  'distinct id',\n  event='movie played',\n  properties={ '$set': { 'userProperty': 'value' } }\n)",
    "tag": "posthog"
  },
  {
    "title": "index.mdx",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/integrate/index.mdx",
    "content": "\ntitle: Integrate PostHog\nsidebarTitle: Overview\nsidebar: Docs\nshowTitle: true\nhideAnchor: true\n\nOnce your PostHog instance is up and running, the next step is to start sending events.\nimport Tab from \"components/Tab\"\nimport SnippetInstructions from \"./_snippets/snippet.mdx\"\nimport SDKs from \"./_snippets/sdks.mdx\"\nimport Frameworks from \"./_snippets/frameworks.mdx\"\nimport API from \"./_snippets/api.mdx\"\nInstallation\n\n\n\nJS snippet\nSDKs\nFramework guides\nAPI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat to read next\nLearn how to take your PostHog integrations to the next level with these in-depth guides.\n\nIdentifying users\nAdding properties to users\nConnecting PostHog with your CDP\nDeploying a reverse proxy\n",
    "tag": "posthog"
  },
  {
    "title": "GDPR and your use of PostHog",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/integrate/gdpr.mdx",
    "content": "\ntitle: GDPR guidance\nsidebarTitle: GDPR guidance\nsidebar: Docs\nshowTitle: true\n\nThe General Data Protection Regulation (GDPR) is a privacy and security law, drafted and passed by the European Union (EU). It imposes obligations onto organizations anywhere, so long as they target or collect data related to people in the EU.\nWe recommend that you read the full text of the GDPR and seek independent legal advice regarding your obligations.\nGDPR and your use of PostHog\nGDPR requirements differ depending on how your business interacts with personal data. Companies can be data controllers, data processors, or both a controller and a processor. Data controllers collect their end users\u2019 data and decide why and how it is processed. Data processors are businesses instructed to process customer data on behalf of other businesses.\nYou will be using PostHog in one of two ways:\n\nYou are using PostHog Cloud\nYou are self-hosting and managing a PostHog instance\n\nIf you are using PostHog Cloud then PostHog is the Data Processor and you are the Data Controller.\nIf you are self-hosting PostHog then you are both the Data Processor and the Data Controller because you are responsible for your PostHog instance.\nIn both cases you are the Data Controller, so this guide identifies some ways in you can instruct PostHog, whether Cloud or self-hosted, to help you comply with GDPR requirements.\nChecklist\nThe following list is based on the GDPR key issues. However, it is not exhaustive and instead identifies some areas relevant to PostHog.\nRight to be informed (provide consent)\nSince PostHog automatically captures some data (for example, the IP address of a user's web browser) which can be considered\u2020 Personally Identifiable Information (PII), you must provide a mechanism for the consensual capturing of that data.\n\u2020 Court confirms that IP addresses are personal data in some cases\nWithin the consent you should identify the types of PII that are being processed and what tools are being used to process them. If you are using PostHog Cloud you should identify PostHog as a tool. If you are self-hosting you can either not list a tool or provide a generic description such as \"Product Analytics\".\nOnce the user has made a decision around their consent you can start or stop data capturing and processing accordingly. If the user has opted out there are a number of ways to ensure data is not captured by PostHog and how you do this will differ depending on your application setup. Here are some ways PostHog makes this possible:\n\nIf posthog-js has been initialized, call `posthog.opt_out_capturing()`. See the posthog-js docs.\nEnsure posthog-js is configured not to auto-capture and do not make capture calls using the installed PostHog SDK on any client.\nDo not load the posthog-js SDK. If you do this you should ensure your application logic always performs conditional checks for the availability of the PostHog SDK. This may not be possible in modern JavaScript applications.\nDo not initialize the posthog-js SDK via the call to `init`. If you do this you should ensure your application logic always performs conditional checks regarding the initialization state of the PostHog SDK.\n\nRight to be forgotten\nA user must be able to request that their data be removed from PostHog. How you facilitate that request is up to you. For example, you could accept requests via email or form submission.\nYou can remove a user from a PostHog instance via the PostHog user interface. To do this:\n\nSelect Persons from the left-hand menu.\nSearch for the person via their unique ID. For example, their email.\nClick view next to the person within the search results .\nClick Delete this person to remove them and all their associated data from the PostHog instance. You will be prompted to confirm this action.\n\nReference\n\nRight to be informed\n",
    "tag": "posthog"
  },
  {
    "title": "What is a CDP?",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/integrate/cdp.md",
    "content": "\ntitle: Using PostHog with a CDP\nsidebarTitle: CDPs\nsidebar: Docs\nshowTitle: true\n\nWhat is a CDP?\nA CDP is a Customer Data Platform. It is a platform that allows you to collect, and send customer data to other tools like product analytics (PostHog!), marketing automation tools, your CRM, data warehouses etc.\nThe most common CDPs are Segment and RudderStack, both of which work with PostHog.\nIf you already have a Customer Data Platforms (CDP) setup or are thinking of using one this guide will help you decide how to integrate with PostHog.\nWhich PostHog CDP setup should I use?\nPostHog has many of the imports and exports of common (CDPs) built-in. This means there's two options for how you could integrate PostHog with a 3rd party CDP:\n\nUse PostHog as a CDP (recommended if you don't have a 3rd party CDP set up and we have the exports you need)\nUse a 3rd party CDP and integrate PostHog\n\nWhich method you choose will depend on what your goals are, what you have already set up, and how much time and money you are willing to invest.\nHere's a decision tree that you might find handy:\n`mermaid\ngraph TD\nA[Are you extensively using a 3rd party CDP?] --> |Yes| B[Option 2<br/>Use a 3rd party CDP and integrate PostHog]\nA --> |No| C[Are there any exports you need immediately that PostHog doesn't have yet?<br/>e.g. paid marketing platforms] --> |Yes| D[Option 2]\nC --> |No| E[Option 1<br/>Use PostHog as a CDP]`\nOption 1: Use PostHog as a CDP\nIn general, we'd highly recommend starting with PostHog as your CDP using the variety of imports and exports apps that we have. This is the easiest and least expensive way to get started.\nIf you later need some extra destinations that we don't yet have you have several options: you can send your PostHog data to a 3rd party CDP (see extra info here), create your own app, or fully transition to a 3rd party CDP (option 2). Every month we are adding more destinations.\n`mermaid\ngraph LR\nA[Website - PostHog Javascript] --> B[PostHog]\nC[iOS App - PostHog Swift] --> B[PostHog]\nD[Android App - PostHog Java] --> B[PostHog]\nE[Backend - PostHog Python] --> B[PostHog]\nB[PostHog] --> H[Saleforce]\nB[PostHog] --> I[Intercom]\nB[PostHog] --> K[Customer.io]\nB[PostHog] --> F[Data warehouse e.g. Big Query / Snowflake]`\nPros:\n- Least expensive as you don't need a 3rd party CDP.\n- You don't have a 3rd party CDP before PostHog reducing risk of data being dropped.\n- Works with all the features of PostHog (analytics, autocapture, feature flags, session recording, etc.)\nCons:\n- We don't yet have as many 3rd party integrations as existing 3rd party CDPs - particularly for marketing platforms (for Facebook Ads, Google Ads, TikTok Ads etc.). If you need these extra exports immediately and you have the time and money for a 3rd party CDP you might want to go with option 2.\nInstructions:\n1. Setup PostHog\n2. Install the PostHog apps you need\nOption 2: Use a 3rd party CDP with PostHog as a destination\nIf you already have a CDP being used extensively, you'll likely want to integrate PostHog with the CDP. For your frontend sources you'll want to configure PostHog as a device mode destination to ensure you get the full functionality.\n`mermaid\ngraph LR\nA[Website - CDP Javascript] --> G\nC[iOS App - CDP Swift] --> G\nD[Android App - CDP Java] --> G\nE[Backend - CDP Python] --> G\nG[3rd party - CDP e.g. Segment/RudderStack] --> B[PostHog]\nG --> N[Google Ads]\nG --> O[Facebook Ads]\nG --> H[Saleforce]\nG --> I[Intercom]\nG --> K[Customer.io]\nG --> F[Data warehouse e.g. Big Query / Snowflake]`\nPros:\n- You can integrate PostHog with your existing CDP.\n- Can manage all your sources and destinations in one place (the 3rd party CDP).\n- The 3rd party CDP has more destinations available than PostHog alone\nCons:\n- You'll need to pay for a 3rd party CDP.\n- If you use PostHog's event autocapture, the other CDP destinations will not receive the autocapture events.\n- Enabling feature flags and session recordings requires extra setup (instructions are included for Segment integration instructions or RudderStack integration instructions) or manually installing the PostHog script.\n- Data to PostHog first passes through a CDP, adding risk that it's dropped.\n- Variable support by the CDP libraries for PostHog's features.\nInstructions: see the Segment integration instructions or RudderStack integration instructions depending on which CDP you are using.\nIf using feature flags and/or session recordings, the true flow of data will end up looking more like the following. With the custom events going through the 3rd party CDP, and the PostHog specific data (like autocapture events, session recordings, and feature flag calls) going directly to PostHog.\n`mermaid\ngraph LR\nA[Website] --CDP Javascript--> G\nC[iOS App] --CDP Swift--> G\nD[Android App] --CDP Java--> G\nE[Backend] --CDP Python--> G\nG[3rd party CDP event stream] --Product analytics--> B[PostHog]\nA --Session Recordings/Feature Flags--> B\nC --Feature Flags--> B\nD --Feature Flags--> B\nE --Feature Flags--> B\nG --> N[Google Ads]\nG --> O[Facebook Ads]\nG --> H[Saleforce]\nG --> I[Intercom]\nG --> K[Customer.io]\nG --> F[Data warehouse e.g. Big Query / Snowflake]`\nSending PostHog data to a 3rd party CDP for extra destinations\nIf there's key exports you are looking for that we don't currently have, you can use the data export app to send the PostHog data to the 3rd party CDP and then add the extra destinations you need.\nNote: this won't work for the CDP device-mode sources such as Facebook Ads and Google Ads (where the CDP injects the marketing script onto the page). If you need this we'd recommend integrating the marketing platforms directly, using Google Tag Manager or using a 3rd party CDP as your primary CDP (option 2).\n`mermaid\ngraph LR\nA[Website - PostHog Javascript] --> B[PostHog]\nC[iOS App - PostHog Swift] --> B[PostHog]\nD[Android App - PostHog Java] --> B[PostHog]\nE[Backend - PostHog Python] --> B[PostHog]\nB --> G\nG[3rd party CDP e.g. Segment/RudderStack] --> N[Extra destinations]\nB --> H[Saleforce]\nB --> I[Intercom]\nB --> K[Customer.io]\nB --> F[Data warehouse e.g. Big Query / Snowflake]`\nPros:\n- More destinations available than PostHog alone\n- You don't have a 3rd party CDP before PostHog reducing risk of data being dropped\n- Works with all the features of PostHog (analytics, autocapture, feature flags, session recording, etc.)\n- Can control within PostHog what data is sent onto the CDP and the extra destinations\nCons:\n- Can't use the CDP for device mode sources such as Facebook Ads and Google Ads\n- More expensive than Option 1 as you now need to pay for a 3rd party CDP",
    "tag": "posthog"
  },
  {
    "title": "Install a library",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/integrate/ingest-live-data.mdx",
    "content": "\ntitle: Ingest live data\nsidebar: Docs\nshowTitle: true\n\nPostHog enables you to analyze data in real-time, as events come in. Make full use of this power by ingesting live data with our analytics integrations: client libraries, server libraries, as well as third-party platforms.\nThe purpose of this guide is to help you understand some key concepts with a goal of ingesting live data into PostHog. For simplicity, we'll focus on client libraries as a means of data ingestion.\nThe guide covers the following:\n\nInstalling and initializing a PostHog library\nHow autocapture works with the JavaScript library\nHow to capture user events with PostHog\nHow to identify and associate users with events\n\nIf you prefer to learn by doing, you can get started on the web with the JavaScript snippet.\n\nNote that some events with a never-before-seen distinct ID are deliberately delayed by around a minute.\nMore on that in the \"Event ingestion nuances\" section.\n\nInstall a library\nInstall the library for the platform you are building your application for.\nimport JSInstall from '../sdks/js/_snippets/install.mdx'\nimport AndroidInstall from '../sdks/android/_snippets/install.mdx'\nimport AndroidConfigure from '../sdks/android/_snippets/configure.mdx'\nimport IOSInstall from '../sdks/ios/_snippets/install.mdx'\nimport IOSConfigure from '../sdks/ios/_snippets/configure.mdx'\nimport FlutterPackage from '../sdks/flutter/_snippets/package.mdx'\nimport FlutterInstall from '../sdks/flutter/_snippets/install.mdx'\nimport ReactNativeInstall from '../sdks/react-native/_snippets/install.mdx'\nimport ReactNativeConfigure from '../sdks/react-native/_snippets/configure.mdx'\nimport Tab from \"components/Tab\"\n\n\nJavaScript\nAndroid\niOS\nFlutter\nReact Native\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse autocapture\nimport JSAutoCapture from '../sdks/js/_snippets/autocapture.mdx'\nimport JSCaptureIgnoreElements from '../sdks/js/_snippets/capture-ignore-elements.mdx'\n\nAutocapture is presently only available in the web browser using the JavaScript library. For non-browser platforms and for situations where you wish to manually capture events, see the capture user events section.\n\n\n\nCapture user events\nThis allows you to send more context than the default event info that PostHog captures whenever a user does something. In that case, you can send an event with any metadata you may wish to add.\n\n`js\nposthog.capture('[event-name]', { property1: 'value', property2: 'another value' })`\n`android\nPostHog.with(this)\n       .capture(\"Button B Clicked\", new Properties()\n                                        .putValue(\"color\", \"blue\")\n                                        .putValue(\"icon\", \"new2-final\"));`\n```ios\n// In swift\nposthog.capture(\"Signed Up\", properties: [\"plan\": \"Pro++\"])\n// In objective-c\n[[PHGPostHog sharedPostHog] capture:@\"Signed Up\" properties:@{ @\"plan\": @\"Pro++\" }];\n```\n`react-native\nposthog.capture('Button B Clicked', {\n    color: \"blue\",\n    icon: \"new2-final\"\n})`\n\nA `capture` call requires:\n\n`event` to specify the event name\nWe recommend naming events with \"[noun] [verb]\", such as `movie played` or `movie updated`, in order to easily identify what your events mean later on (we know this from experience).\n\n\n\nOptionally you can submit:\n\n`properties`, which is an object with any information you'd like to add\n\nIdentify users\nimport JSIdentify from '../sdks/js/_snippets/identify.mdx'\nimport AndroidIdentify from '../sdks/android/_snippets/identify.mdx'\nimport IOSIdentify from '../sdks/ios/_snippets/identify.mdx'\nimport ReactNativeIdentify from '../sdks/react-native/_snippets/identify.mdx'\n\n\nJavaScript\nAndroid\niOS\nFlutter\nReact Native\n\n\n\n\n\n\n\n\n\n\n\n\n See the Flutter library docs for more information.\n\n\n\n\n\n\nEvent ingestion nuances\nIt's a priority for us that events are fully processed and saved as soon as possible. However, there is a class of events which we deliberately process with a slight delay. Specifically, an event is delayed by around a minute if it fits all of the following three conditions:\n\nisn't from an anonymous user (anonymous users are recognized by having the `distinct_id` the same as the `$device_id` property)\nisn't an `$identify` event (e.g. from `posthog.identify()`)\nits `distinct_id` cannot be matched to an existing person\n\nThis delay mechanism is called the event buffer, and it materially improves handling of an edge case which could otherwise inflate unique user counts.\n\nHow does the event buffer help?\n\nStarting with version 1.38.0, PostHog stores the person associated with an event inline with the event record. This greatly improves query performance, but because events are immutable, it also means that persons can't be merged retroactively. See this scenario where that's problematic:\n\n1. User visits signup page, in turn frontend captures anonymous `$pageview` for distinct ID `XYZ` (anonymous distinct ID = device ID).  \n   This event gets person ID `A`.\n2. User click signup button, initiating in a backend request, in turn frontend captures anonymous `$autocapture` (click) for distinct ID `XYZ`.  \n   This event gets person ID `A`.\n3. Signup request is processed in the backend, in turn backend captures identified signup for distinct ID `alice@example`.com.  \n   OOPS! We haven't seen `alice@example.com` before, so this event gets person ID `B`.\n4. Signup request finishes successfully, in turn frontend captures identified $identify aliasing distinct ID `XYZ` to `alice@example.com`.  \n   This event gets person ID `A`.\n\nHere, the event from step 3 got a new person ID `B`, impacting unique users counts. If it were delayed just a bit and processed after the event from step 4, all events would get the expected person ID `A`. This is exactly what the event buffer achieves.",
    "tag": "posthog"
  },
  {
    "title": "Setting properties",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/integrate/user-properties.mdx",
    "content": "\ntitle: User properties\nsidebarTitle: User properties\nsidebar: Docs\nshowTitle: true\n\nSetting properties\nThe easiest way to set properties is to add the properties to the `identify` function during the initial login.\n\nIdentify the user and set the user properties\n```js\nposthog.identify('user_id', {\n    name: 'James PostHog', // also sets the display name in the PostHog UI\n    email: 'hey@posthog.com',\n})\n\n// { name: 'James PostHog', email: 'hey@posthog.com' }\n```\nTo update the properties of a user you can use `set`, `set_once`, and `$unset`. Depending on the integration library the actual function calls look a bit different, but internally they all work the same way.\n\n`set`: Set the property even if the property exists on the user\n```js\nposthog.people.set( { location: 'London'  })\n\n// { location: 'London' }\n```\n\n`set_once`: Set the property if the property does not exist on the user\n```js\nposthog.people.set_once( { initial_location: 'Rome'})\nposthog.people.set_once( { initial_location: 'London'})\n\n// { initial_location: 'Rome' }\n```\nAll these methods can be used in properties with normal event capture:\n\nSet the properties through a capture\n`js\nposthog.capture(\n    'Set some user properties', \n    { \n        $set: { location: 'London'  },\n        $set_once: { initial_location: 'London' },\n        $unset: [ 'priority' ],\n    }`\n\nWhen to use `set` and `set_once`?\nUse `set` when we want the value to be the most recently sent value. For example, to update the email for a user.\nUse `set_once` when we want to set the first value and it to never be updated afterwards, e.g. the first URL that referred a user to the site.\nIn summary: `set` always overrides, `set_once` only writes when the property doesn't already exist on the user.\nFor example:\n```js\nposthog.people.set( { location: 'London'  })\nposthog.people.set( { location: 'Rome'  })\n// { location: 'Rome' }\nposthog.people.set_once( { initial_location: 'London'  })\nposthog.people.set_once( { initial_location: 'Rome'  })\n// { location: 'London' }\n```\nNote: that we ignore the event timestamps and just process everything at ingestion time.\nSometimes we might want to mix `set` and `set_once` usage. Imagine, when we have some heuristics that help us determine a value, but users can also specify it. In these cases it might be beneficial to use `set_once` for the heuristically computed value and `set` for user specified value. This way we can ensure that the user specified value is always used, but if the user doesn't specify it, we can still use the heuristically computed value.\nHow are properties managed when merging or aliasing users\nWhen an anonymous user is identified as user (A), all the properties of the anonymous user are added to user (A). If there is a conflict, the properties of user (A) are kept.\n```js\nclient.identify({\n    distinctId: 'user_A',\n    properties: {\n        name: 'User A',\n        location: 'London',\n        timezone: 'GMT',\n    },\n})\nclient.reset()\n// as an anonymous user\nposthog.people.set({ name: 'Anonymous User', phone: '0800-POSTHOG' })\nclient.identify({\n    distinctId: 'user_A',\n    properties: {\n        timezone: 'GMT+1', // daylight saving time\n    },\n})\n// for user 'user_A', { name: 'User A', location: 'London', phone: '0800-POSTHOG', timezone: 'GMT+1' }\n```\nWhen a user (B) is merged into another user (A), all the properties of the user (B) are added to user (A). If there is a conflict, the properties of user (A) are kept.\n```js\nclient.identify({\n    distinctId: 'user_A',\n    properties: {\n        location: 'London',\n        name: 'User A',\n    },\n})\nclient.identify({\n    distinctId: 'user_B',\n    properties: {\n        name: 'User B',\n        location: 'Rome',\n        phone: '0800-POSTHOG',\n    },\n})\nposthog.alias('user_A', 'user_B')\n// for user 'user_A', { name: 'User A', location: 'London', phone: '0800-POSTHOG' }\n```\nAny events sent to either `user_A` or `user_B` will be received under the same person.\nNote: that the `'[alias ID]'` cannot be associated with multiple distinct ids.\nPerson Display Name\nThe person display name is the name that is shown in the PostHog UI. The person properties that are used as the display name can be configured in Project Settings.\nDefault user properties\nIt's often useful to set the `email` property for a user. This is useful for many of the apps, filtering users in the PostHog UI and as the Person Display Name.\nThe following properties are automatically set for all users when using the `posthog-js` library:\n| Property | Property Name | Description | Example |\n| :--- | :--- | :--- | :--- |\n| `utm_campaign` | UTM Campaign | UTM campaign tag (last-touch). | feature launch, discount |\n| `utm_content` | UTM Content | UTM content tag (last-touch). | bottom link, second button |\n| `utm_medium` | UTM Medium | UTM medium tag (last-touch). | Social, Organic, Paid, Email |\n| `utm_source` | UTM Source | UTM source tag (last-touch). | Google, Bing, Twitter, Facebook |\n| `$browser` | Browser | Name of the browser the user has used. | Chrome, Firefox |\n| `$browser_version` | Browser Version | The version of the browser that was used. Used in combination with Browser. | 70, 79 |\n| `$initial_browser` | Initial Browser | Name of the browser the user first used (first-touch). | Chrome, Firefox |\n| `$initial_browser_version` | Initial Browser Version | The version of the browser that the user first used (first-touch). Used in combination with Browser. | 70, 79 |\n| `$initial_current_url` | Initial Current URL | The first URL the user visited, including all the trimings. | https://example.com/interesting-article?parameter=true |\n| `$initial_device_type` | Initial Device Type | The initial type of device that was used (first-touch). | Mobile, Tablet, Desktop |\n| `$initial_os` | Initial OS | The operating system that the user first used (first-touch). | Windows, Mac OS X |\n| `$initial_pathname` | Initial Path Name | The path of the Current URL, which means everything in the url after the domain. Data from the first time this user was seen. | /pricing, /about-us/team |\n| `$initial_referrer` | Initial Referrer URL | URL of where the user came from most recently (last-touch). Data from the first time this user was seen. | https://google.com/search?q=posthog&rlz=1C... |\n| `$initial_referring_domain` | Initial Referring Domain | Domain of where the user came from most recently (last-touch). Data from the first time this user was seen. | google.com, facebook.com |\n| `$initial_utm_campaign` | Initial UTM Campaign | UTM campaign tag (first-touch). | feature launch, discount |\n| `$initial_utm_content` | Initial UTM Content | UTM content tag (first-touch). | bottom link, second button |\n| `$initial_utm_medium` | Initial UTM Medium | UTM medium tag (first-touch). | Social, Organic, Paid, Email |\n| `$initial_utm_source` | Initial UTM Source | UTM source tag (first-touch). | Google, Bing, Twitter, Facebook |\n| `$os` | OSThe operating system of the user. | Windows, Mac OS X |\n| `$referrer` | Referrer URL | URL of where the user came from most recently (last-touch). | https://google.com/search?q=posthog&rlz=1C... |\n| `$referring_domain` | Referring Domain | Domain of where the user came from most recently (last-touch). | google.com, facebook.com |\nGeoIP Properties\nBy default, the GeoIP plugin is turned on which adds the following properties to all events.\n| Property | Property Name | Description | Example |\n| :--- | :--- | :--- | :--- |\n| `$geoip_city_name` | City Name | Name of the city matched to this event's IP address. | Sydney, Chennai, Brooklyn |\n| `$geoip_continent_code` | Continent Code | Code of the continent matched to this event's IP address. | OC, AS,  NA |\n| `$geoip_continent_name` | Continent Name | Name of the continent matched to this event's IP address. | Oceania, Asia, North America |\n| `$geoip_country_code` | Country Code | Code of the country matched to this event's IP address. | AU, IN, US |\n| `$geoip_country_name` | Country Name | Name of the country matched to this event's IP address. | Australia, India, United States |\n| `$geoip_latitude` | Latitude | Approximated latitude matched to this event's IP address. | -33.8591, 13.1337, 40.7 |\n| `$geoip_longitude` | Longitude | Approximated longitude matched to this event's IP address. | 151.2, 80.8008, -73.9 |\n| `$geoip_postal_code` | Postal Code | Approximated postal code matched to this event's IP address. | 2000, 600004, 11211 |\n| `$geoip_subdivision_1_code` | Subdivision 1 Code | Code of the subdivision matched to this event's IP address. | NSW, TN, NY |\n| `$geoip_subdivision_1_name` | Subdivision 1 Name | Name of the subdivision matched to this event's IP address. | New South Wales, Tamil Nadu, New York |\n| `$geoip_subdivision_2_code` | Subdivision 2 Code | Code of the second subdivision matched to this event's IP address. |\n| `$geoip_subdivision_2_name` | Subdivision 2 Name | Name of the second subdivision matched to this event's IP address. |\n| `$geoip_time_zone` | Timezone | Timezone matched to this event's IP address. | Australia/Sydney, Asia/Kolkata, America/New_York |\n| `$initial_geoip_city_name` | Initial City Name | Name of the city matched to this event's IP address. Data from the first time this user was seen. | Sydney, Chennai, Brooklyn |\n| `$initial_geoip_continent_code` | Initial Continent Code | Code of the continent matched to this event's IP address. Data from the first time this user was seen. | OC, AS,  NA |\n| `$initial_geoip_continent_name` | Initial Continent Name | Name of the continent matched to this event's IP address. Data from the first time this user was seen. | Oceania, Asia, North America |\n| `$initial_geoip_country_code` | Initial Country Code | Code of the country matched to this event's IP address. Data from the first time this user was seen. | AU, IN, US |\n| `$initial_geoip_country_name` | Initial Country Name | Name of the country matched to this event's IP address. Data from the first time this user was seen. | Australia, India, United States |\n| `$initial_geoip_latitude` | Initial Latitude | Approximated latitude matched to this event's IP address. Data from the first time this user was seen. | -33.8591, 13.1337, 40.7 |\n| `$initial_geoip_subdivision_1_code` | Initial Subdivision 1 Code | Code of the subdivision matched to this event's IP address. Data from the first time this user was seen. | NSW, TN, NY |\n| `$initial_geoip_longitude` | Initial Longitude | Approximated longitude matched to this event's IP address. Data from the first time this user was seen. | 151.2, 80.8008, -73.9 |\n| `$initial_geoip_postal_code` | Initial Postal Code | Approximated postal code matched to this event's IP address. Data from the first time this user was seen. | 2000, 600004, 11211 |\n| `$initial_geoip_subdivision_1_name` | Initial Subdivision 1 Name | Name of the subdivision matched to this event's IP address. Data from the first time this user was seen. | New South Wales, Tamil Nadu, New York |\n| `$initial_geoip_subdivision_2_code` | Initial Subdivision 2 Code | Code of the second subdivision matched to this event's IP address. Data from the first time this user was seen. |\n| `$initial_geoip_subdivision_2_name` | Initial Subdivision 2 Name | Name of the second subdivision matched to this event's IP address. Data from the first time this user was seen. |",
    "tag": "posthog"
  },
  {
    "title": "Deploying a reverse proxy",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/integrate/proxy.mdx",
    "content": "\ntitle: Deploying a reverse proxy to PostHog Cloud\nsidebar: Docs\nshowTitle: true\n\nA reverse proxy allows you to send events to PostHog Cloud using your own domain.\nThis means that events are sent from your own domain and are less likely to be intercepted by tracking blockers. You be able to capture more usage data without having to self-host PostHog.\nSetting up a reverse proxy means setting up a service to redirect requests from a subdomain you choose (like `e.yourdomain.com`) to PostHog. It is best practice to use a subdomain that does not include `posthog`, `analytics`, `tracking`, or other similar words. \nYou then use this subdomain as your instance host in the initialization of PostHog instead of `app.posthog.com` or `eu.posthog.com`.\n\nNote: PostHog Cloud requires that the proxy sets the `Host` header to `eu.posthog.com` for requests sent to `https://eu.posthog.com`\nand `app.posthog.com` for requests sent to `https://app.posthog.com`.\n\nDeploying a reverse proxy\nUsing Caddy\nWe like using Caddy because it makes setting up the reverse proxy and TLS a breeze.\n`bash\ndocker run -p 80:80 -p 443:443 caddy caddy reverse-proxy --to app.posthog.com:443 --from <YOUR_TRACKING_DOMAIN> --change-host-header`\nYou'll want to sub out `YOUR_TRACKING_DOMAIN` for whatever domain you use for proxying to PostHog. We'd suggest something like `e.yourdomain.com` or the like.\nMake sure your DNS records point to your machine and that ports 80 and 443 are open to the public and directed toward Caddy.\nUsing AWS CloudFront\nCloudFront can be used as a reverse proxy. Although there are multiple other options if you're using AWS\nCloudFront doesn't forward headers, cookies, or query parameters received from the origin that PostHog uses by default. To set these up, you need an \"origin request policy\" as in the instructions below.\nCreate a distribution\n\nCreate a CloudFront distribution\nSet the origin domain to your PostHog instance (`app.posthog.com` or `eu.posthog.com` for PostHog Cloud).\nAllow all HTTP methods\nCreate and attach to the distribution, an \"origin request policy\" that forwards all query parameters\nIn \"Cache key settings\" for the \"Cache policy\" set \"Query strings\" to \"All\".\nYou may also need to set any headers your application needs as part of the cache key. For example the `Authorization` or `Content-Type` headers.\n\n\n\nimport diagram from '../../images/docs/cloud/cloudfront-proxy/cache-policy.png'\n\n\nChoose the appropriate price class for your use\nOnce the distribution is deployed set its URL as the API host in your JS snippet or SDK config\nCreate a lambda@edge to set the Host header (example).\n\nYou can view AWS CDK code for creating a reverse proxy here (originally shared by CJ Enright in our Slack community, 100 \ud83e\udd94 points for them \ud83d\udc96!).\nYou can find out about CloudFront pricing on the AWS website\nCloudFront distribution setup video\n\nUsing Cloudflare\nIn Cloudflare, create a new CNAME record for your domain. It should point to `app.posthog.com` or `eu.posthog.com` depending on your region, and have proxy enabled (e.g. `CNAME, e, app.posthog.com, proxy enabled`). Finally, use Page Rules to change the Host header. \n\nCloudflare does require your domain to be hosted with them, and using them does more than just proxying requests, such as blocking traffic from bots.\n",
    "tag": "posthog"
  },
  {
    "title": "Setup for Firefox & Chrome plugins",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/integrate/browser-extension.md",
    "content": "\ntitle: Product analytics for browser extensions\nsidebar: Docs\nshowTitle: true\n\nDo you have a Firefox or Chrome browser plugin with a user interface and want to understand how it's being used? PostHog is the perfect way to do just that.\nSetup for Firefox & Chrome plugins\nInstalling PostHog inside your plugin\nOpen the HTML file used in your `default_popup` and add the PostHog `array.js` script. To do this you'll need to either:\n1. Copy the latest version of array.js from: https://app.posthog.com/static/array.js and import it locally using `<script src=\"array.js\" />` before the `</head>` tag\n2. If you're packaging your plugin automatically use the npm module for posthog.js\nAll you need to do now is initialize PostHog, add the following code to a new js file and import it into your `default_popup` html file to initiate PostHog.\n`javascript\nposthog.init('your_project_token',{api_host:'https://app.posthog.com',persistence:'localStorage'})`\n\n`your_project_token` - This is the `Project API key` which can be found on PostHog under Project Settings\n`api_host` - This is the URL to your PostHog instance (if you're not using PostHog Cloud)\n`persistence` - This indicates we should store user data in localStorage rather than cookies - there are issues with cookie persistence on Firefox plugins\n\nTracking events\nOne of the best things about using PostHog is, all the interactions like clicks will automatically generate events in PostHog, so you don't need to do anything else to start analyzing.\nIf you'd like to instrument your own custom events, all you need to do is:\n`javascript\nposthog.capture('custom_event_name', {})`",
    "tag": "posthog"
  },
  {
    "title": "Apps",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/integrate/_snippets/apps.mdx",
    "content": "Apps\nApps extend PostHog's functionality and are an incredibly powerful part of the PostHog ecosystem. If you need to bring in data from a 3rd party service or perform some operation on all events coming into PostHog, apps are generally the best and most reliable option.\nApps can help you:\n\nImport data from 3rd party services\nEnrich events in your pipeline by adding extra data\nFilter events to keep your data clean\n\nInstallation\nBelow is a list of all our apps for bringing data and events into PostHog. Apps can be installed from the app section in the dashboard.\nFor information on how to set up each app, the list below contains links to each app's documentation page.\nIngestion apps (25)\nFor a list of all our apps, see here\n",
    "tag": "posthog"
  },
  {
    "title": "snippet.mdx",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/integrate/_snippets/snippet.mdx",
    "content": "import Snippet from \"../snippet.mdx\"\n\n    JavaScript snippet Recommended\n\n\nThis is the simplest way to get PostHog up and running on your website, and only takes a few minutes to set-up.\n\nAdd to your website & app\nPaste this snippet within the `<head>` tags of your website - ideally just inside the closing `</head>` tag - on all pages that you wish to track.\n\nBe sure to replace `<ph_project_api_key>` and `<ph_instance_address>` with your project's values. (You can find the snippet pre-filled with this data in the PostHog app under Project / Settings. (Quick links if you use PostHog Cloud US or PostHog Cloud EU)\n\nIntegrated the snippet? Here are some ideas for what to do after you've installed PostHog, from setting up event capture to building your first dashboard.\n\nWhat this code does\nAfter adding the snippet to your website, it will automatically start to:\n\nCapture `$pageview` events when a user visits a page\nTrack when users click on links or buttons\nRecord videos of user sessions that you can play back (if you've enabled the Session recordings feature, which we highly recommend!)\n\nTips\n\nAdd the snippet on your (marketing) website and your application using the same PostHog project. That means you'll be able to follow a user from the moment they come onto your website, all the way through sign up and their actual usage of your product.\nTrack users across multiple domains by including the same project snippet on all sites you wish to track.\nDisable tracking sensitive information by adding the `ph-no-capture` class to elements you don't want to track. (We do this automatically for common fields like passwords and credit cards.)\n\nNext steps\n\nAdd permitted domains in your PostHog project settings so you can create actions using the PostHog toolbar directly on your site - a convenient low-code option.\nSend custom events from your codebase for things like signups, purchases, and more KPIs.\n",
    "tag": "posthog"
  },
  {
    "title": "Sending events",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/integrate/_snippets/api.mdx",
    "content": "Events can be ingested directly using our API and the /capture endpoint, which is the same endpoint that all of our libraries use behind the scenes.\nGenerally, this isn't something you'll need to use when integrating PostHog, but if you're working with a language or framework that PostHog doesn't support yet, this will allow you to still send events.\n\nNote: For this API, you should use your 'Project API Key' from the 'Project' page in PostHog. This is the same key used in your frontend snippet.\n\nSending events\nEvents can be sent either one at a time, or together in a batch. There is no limit on the number of events you can send in a batch, but the entire request body must be less than `20MB` by default.\n\n`shell label=Single\nPOST https://[your-instance].com/capture/\nContent-Type: application/json\nBody:\n{\n    \"api_key\": \"<ph_project_api_key>\",\n    \"event\": \"[event name]\",\n    \"properties\": {\n        \"distinct_id\": \"[your users' distinct id]\",\n        \"key1\": \"value1\",\n        \"key2\": \"value2\"\n    },\n    \"timestamp\": \"[optional timestamp in ISO 8601 format]\"\n}`\n`shell label=Batch\nPOST https://[your-instance].com/batch/\nContent-Type: application/json\nBody:\n{\n    \"api_key\": \"<ph_project_api_key>\",\n    \"batch\": [\n        {\n            \"event\": \"[event name]\",\n            \"properties\": {\n                \"distinct_id\": \"[your users' distinct id]\",\n                \"key1\": \"value1\",\n                \"key2\": \"value2\"\n            },\n            \"timestamp\": \"[optional timestamp in ISO 8601 format]\"\n        },\n        ...\n    ]\n}`\n\n\nNote: Timestamp is optional. If not set, it'll automatically be set to the current time.\n",
    "tag": "posthog"
  },
  {
    "title": "Install",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/integrate/third-party/gatsby.mdx",
    "content": "\ntitle: Gatsby\nicon: ../../../images/docs/integrate/frameworks/gatsby.svg\n\nThanks to Ritesh Kadmawala for building this!\nGitHub: posthog/gatsby-plugin-posthog\nGatsby behaves like a single-page app which means to track `$pageview` events special care is needed. This integration takes care of that.\nInstall\n`bash\nyarn add gatsby-plugin-posthog`\nor\n`bash\nnpm install --save gatsby-plugin-posthog`\nHow to use\n`js\n// In your gatsby-config.js\nmodule.exports = {\n  plugins: [\n    {\n      resolve: `gatsby-plugin-posthog`,\n      options: {\n        // Specify the API key for your PostHog Project (required)\n        apiKey: \"<ph_project_api_key>\",\n        // Specify the app host if self-hosting (optional, default: https://app.posthog.com)\n        apiHost: \"<ph_instance_address>\",\n        // Puts tracking script in the head instead of the body (optional, default: true)\n        head: true,\n        // Enable posthog analytics tracking during development (optional, default: false)\n        isEnabledDevMode: true\n      },\n    },\n  ],\n}`\nThis will automatically start tracking pageviews, clicks and more.\nIn your code you can access posthog via `window.posthog`.",
    "tag": "posthog"
  },
  {
    "title": "nuxt-js.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/integrate/third-party/nuxt-js.md",
    "content": "\ntitle: Nuxt.js\nicon: ../../../images/docs/integrate/frameworks/nuxt.svg\n\nIf you are using Nuxt.js and want to track your application using PostHog this tutorial might help you out. \nIt will guide you through an example integration of PostHog using Nuxt.js. \nIs this tutorial for me?\nThis tutorial is aimed at Nuxt.js users which run Nuxt in `spa` or `universal` mode. \nWe are going to look at some minimal example code which should get you started quickly and provide a base for further customization.\nPrerequisites\nTo follow this tutorial along, you need to:\n\nHave deployed PostHog.\nHave a running Nuxt.js application\n\nMinimal example\nWe are going to implement PostHog as a Nuxt.js integration which gives us the possibility to inject\nthe posthog object and make it available across our application.\nThe first thing you want to do is to install the posthog-js library in your project - so add it using your package manager:\n`shell\nyarn add posthog-js`\nor\n`shell\nnpm install --save posthog-js`\nAfter that we want to create a app in `plugins/posthog/index.js`\n```javascript\nimport posthog from 'posthog-js'\nimport Vue from 'vue'\nexport default function({ app: { router } }, inject) {\n  // Init PostHog\n  posthog.init('', {\n    api_host: '',\n    capture_pageview: false,\n    loaded: () => posthog.identify('unique_id') // If you can already identify your user\n  })\n// Inject PostHog into the application and make it available via this.$posthog (or app.$posthog)\n  inject('posthog', posthog)\n// Make sure that pageviews are captured with each route change\n  router.afterEach(to => {\n    Vue.nextTick(() => {\n      / Note: this might also be a good place to call posthog.register(...) in order to update your properties\n      on each page view\n      /\n      posthog.capture('$pageview', {\n        $current_url: to.fullPath\n      })\n    })\n  })\n}\n```\nFinally, we need to activate it on the client side in our `nuxt.config.js`\n`javascript\nplugins: [\n    ...\n    { src: './plugins/posthog', mode: 'client' }\n  ],`\nUsage\nBy using the example code above you can now use PostHog across your app with `this.$posthog` or `app.$posthog` - depending on the context. \nCompare with the Nuxt.js docs on further details when to use `app.$posthog` or `this.$posthog`.\nLet's say for example the user makes a purchase you could track an event like that:\n```javascript\n\n",
    "tag": "posthog"
  },
  {
    "title": "Setting up Segment",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/integrate/third-party/segment.md",
    "content": "\ntitle: Segment\nicon: ../../../images/docs/integrate/frameworks/segment.svg\n\nSegment is a Customer Data Platform (CDP) that allows you to easily manage data and integrations with services across your growth, product, and marketing stack. By tracking events and users via Segment\u2019s API and libraries, you can send your product\u2019s data to all of your analytics/marketing platforms, with minimal instrumentation code. They offer support for most platforms, including iOS, Android, JavaScript, Node.js, PHP, and more.\n\nBefore integrating with Segment, we recommend you read our CDP integration guide to understand the different options for integrating with PostHog.\n\nSetting up Segment\nMake sure you have a Segment account and a PostHog account, using PostHog Cloud or self-hosting.\n\nFor each source that you want to send data to PostHog:\nIf it's a website or web app - follow Setting up Segment with your website\nFor all other sources, follow the instructions for Adding PostHog as a simple Segment destination\n\nSetting up Segment with your website (all features supported)\nIn order to use the full feature set of PostHog (autocapture, session recording, feature flags, heatmaps or the toolbar) we need to load our own Javascript snippet directly.\n\nIn addition to Segment, install your PostHog JS snippet\n\nModify the initialization as documented below to pass the segment `analytics` through for PostHog JS to sync with:\n```js\n// Load PostHog JS\n!function(t,e){var o,n,p,r;e.__SV||(window.posthog=e,e._i=[],e.init=function(i,s,a){function g(t,e){var o=e.split(\".\");2==o.length&&(t=t[o[0]],e=o[1]),t[e]=function(){t.push([e].concat(Array.prototype.slice.call(arguments,0)))}}(p=t.createElement(\"script\")).type=\"text/javascript\",p.async=!0,p.src=s.api_host+\"/static/array.js\",(r=t.getElementsByTagName(\"script\")[0]).parentNode.insertBefore(p,r);var u=e;for(void 0!==a?u=e[a]=[]:a=\"posthog\",u.people=u.people||[],u.toString=function(t){var e=\"posthog\";return\"posthog\"!==a&&(e+=\".\"+a),t||(e+=\" (stub)\"),e},u.people.toString=function(){return u.toString(1)+\".people (stub)\"},o=\"capture identify alias people.set people.set_once set_config register register_once unregister opt_out_capturing has_opted_out_capturing opt_in_capturing reset isFeatureEnabled onFeatureFlags\".split(\" \"),n=0;n<o.length;n++)g(u,o[n]);e._i.push([i,s,a])},e.__SV=1)}(document,window.posthog||[]);\n// Segment script\nvar analytics = \"\"; \nanalytics.load(\"\");\nanalytics.ready(() => {\n    window.posthog.init(\"\", {\n        api_host: 'https://app.posthog.com', // Use eu.posthog.com for EU instances\n        segment: window.analytics, // Pass window.analytics here - NOTE: `window.` is important\n        capture_pageview: false, // You want this false if you are going to use segment's `analytics.page()` for pageviews\n    });\n    // Make sure to send the first pageview after posthog is initialised to get all the correct properties linked\n    window.analytics.page();\n})\n```\n\n\nNote: It's possible to use PostHog as a simple Segment destination as mentioned below for your website, but you will lose out on the full feature set of PostHog.\nAdding PostHog as a simple Segment destination (minimal feature support)\nThe simple Segment destination only supports tracking of pageviews, custom events and identifying users - it does not support autocapture, session recording, feature flags, heatmaps or the toolbar. As such, we only recommend this method for basic tracking of events that are not using the client sidce javascript library e.g. sending your server-side events to PostHog.\n\nIn the Segment workspace, create a new project and enable PostHog as an integration. We are listed as a 'Destination' on Segment\nGrab the PostHog API key from the 'Project Settings' page in PostHog\nUse one of Segment's libraries to send events\nSee the events coming into PostHog\n\nSending events to PostHog\nOnce you have set up Segment and the destination properly configured, can use it to send events to PostHog. You can do this through the Segment API, or one of the Segment libraries. For example, with javascript you can use the `analytics.track('Event Name')` function to send events to Segment, which will then be sent to PostHog.\nFor the full list of functions see the relevant SDK docs e.g. the Javascript SDK.\nUsing group analytics",
    "tag": "posthog"
  },
  {
    "title": "Javascript integration",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/integrate/third-party/sentry.md",
    "content": "\ntitle: Sentry\nicon: ../../../images/docs/integrate/frameworks/sentry.svg\n\nWhy does this exist?\nOur Sentry integration is a two-way integration which works on Javascript & Python. Once installed, it will:\n- Add a direct link in Sentry to the profile of the person affected in PostHog.\n- Send an `$exception` event to PostHog with a direct link to Sentry.\nThis way, debugging issues becomes a lot easier, and you can also correlate error data with your product metrics.\nIf you're looking for the PostHog + Sentry integration for Python, please check out the Python docs.\nJavascript integration\nInstallation\nMake sure you're using both PostHog and Sentry as JS modules. You'll need to replace `'your organization'` and `project-id` with the organization and project-id from Sentry.\n\n`'your organization'` will be in the URL when you go to your Sentry instance, like so: `https://sentry.io/organizations/your-organization/projects/`\n`project-id` will be the last few digits in your Sentry DSN, such as `https://adf90sdc09asfd3@9ads0fue.ingest.sentry.io/project-id`\n\n```js\nimport posthog from 'posthog-js'\nimport * as Sentry from '@sentry/browser'\nposthog.init('')\nSentry.init({\n    dsn: '',\n    integrations: [new posthog.SentryIntegration(posthog, 'your organization', project-id)],\n})\n```\nUsage\nIn PostHog, you'll now have `$exception` events, which have a \"Sentry URL\" link to take you to the exception:\n\nFrom Sentry you will now be able to go directly to the affected person in PostHog and watch the session recording for when the exception happened, see what else the user has done, and find their details. Don't forget to click the little icon to the side of the URL, not the URL itself.",
    "tag": "posthog"
  },
  {
    "title": "Setting up the RudderStack Import",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/integrate/third-party/rudderstack.md",
    "content": "\ntitle: RudderStack\nicon: ../../../images/docs/integrate/frameworks/rudderstack.svg\n\nRudderStack is an open-source, customer data platform for developers. It allows you to collect and deliver customer event data to a variety of destinations across your growth, product, and marketing stack.\n\nBefore integrating with RudderStack, we recommend you read our CDP integration guide to understand the different options for integrating with PostHog.\n\nSetting up the RudderStack Import\nMake sure you have a RudderStack account and a PostHog account, using PostHog Cloud or self-hosting.\n\nFrom your RudderStack dashboard, add each source and select PostHog from the list of destinations.\nAssign a name to your destination (e.g. PostHog production) and click Continue.\nAdd your PostHog 'Project API Key' as the Team API key (Do not use a Personal API key) and your host url as `Your-Instance` (`https://app.posthog.com` if you're on PostHog Cloud):\n    \nIf it's a website or web app:\nIn the rudderstack console set `Use device-mode to send events` to `true` so that the events originate from the client side. Additionally, this will enable the toolbar and heatmaps.\nSet `Enable autocapture with PostHog` to `true`. This will automatically capture events from your website or web app\n\nIf it's not a website or web app (e.g. a mobile app or server), to use the full set of features such as feature flags and session recordings manually install the PostHog SDK docs in addition to adding PostHog as the event destination in RudderStack.\n\n\n\nFor more information see the RudderStack guide for setting up PostHog\nSending events to PostHog\nOnce you have set up RudderStack and PostHog, you can use RudderStack to send events to PostHog. You can send events through the RudderStack API, or one of the RudderStack libraries. For example, with javascript you can use the `analytics.track('Event Name')` function which will send the events to your RudderStack destinations including PostHog.",
    "tag": "posthog"
  },
  {
    "title": "TL;DR: (the short version)",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/integrate/third-party/wordpress.md",
    "content": "\ntitle: WordPress\nicon: ../../../images/docs/integrate/frameworks/wordpress.svg\n\nGetting analytics about your WordPress site is simple with PostHog. Get data about traffic, usage, and user behavior with our free, open-source analytics platform. Once you have that data, you can discover insights and build dashboards with our suite of analytics tools.\nIn this guide, we'll walk you through how to set up PostHog on your WordPress site. You can do this by adding our JavaScript snippet to the header of your WordPress site directly, or by using a third-party plugin to insert the snippet into your site's header.\nTL;DR: (the short version)\n\n\nDeploy PostHog or signup for PostHog Cloud.\n\n\nGet your PostHog snippet from your 'Project Settings' or the initial PostHog setup.\n\n\nAdd the PostHog snippet before the closing `</head>` tag in your `header.php` template file. This can be done in two ways:\n\nAccess WordPress admin, navigate to 'Appearance' -> 'Theme Editor', select your theme, and select 'Theme Header'.\nManually update the header template file. This is usually `wp-content/themes/your-theme/header.php`.\n\n\n\nRead on for additional information.\nPrerequisites\nTo follow this tutorial, you should:\n\nHave deployed PostHog or be using PostHog Cloud.\n\nStep-by-step instructions\nThe instructions below detail how to use the WordPress built-in functionality for editing templates via the admin interface.\n\nUsing the Theme Editor is very convenient, but you have to consider the potential draw-backs of having template files writable, which many prefer to disable for security purposes. Also, wrongfully editing a file may cause problems so be sure to perform appropriate backups before attempting this.\n\n\nGet your PostHog snippet from your 'Project Settings' or the initial PostHog setup.\nLogin to your WordPress admin dashboard.\nGo to 'Appearance' -> 'Theme Editor'.\n\nSelect your theme in the editor drop-down menu to the right and click the `header.php` file in the file column to the right (see image below).\n\n\n\n5. You should now see the contents of the `header.php` template file in the code editing view. It is recommended that you copy all the text/code and save it somewhere as a back-up.\n6. Find the closing `</head>` in the code editor and paste the PostHog snippet before it (see above image).\n7. Click the 'Update File' button at the bottom. You're good to go - PostHog should now be working on your WordPress website!\n\n\nTo confirm PostHog is configured correctly, visit your website and then check if the events from your session appear in PostHog. Note that this may take a few minutes.\nAlternative ways to install\nThere are alternative ways to installing the PostHog analytics snippet on WordPress:\n\n\nUse a WordPress header script insertion plugin. Easy, but there are no official WordPress plugins for editing the header. Reviewing the quality of third-party plugins, including after they get updated by providers, is an additional responsibility. The benefit is that plugins can be easily turned on and off, and updated with a few clicks through the admin interface. \n\n\nSome WordPress themes include additional functionality for inserting custom code into headers and footers. This is a convenient approach, so make sure to check if your theme has that option before installing a plug-in or editing `header.php`.\n\n\nManually edit the header template file in WordPress. This is usually `header.php`. This gives you the most control, but requires backend access and re-editing in case of updates. If your theme auto-updates, you may lose your settings. Making a Child Theme is the recommended approach.\n\n\nFurther reading\n\nComplete guide to event tracking\n",
    "tag": "posthog"
  },
  {
    "title": "Objective",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/integrate/third-party/google-tag-manager.md",
    "content": "\ntitle: Google Tag Manager\nicon: ../../../images/docs/integrate/frameworks/gtm.svg\n\nObjective\nIntegrating PostHog into your website using Google Tag Manager.\nWhy is this useful?\nGoogle Tag Manager helps you add tags into your website in a codeless way, for services such as marketing and analytics tools. \nIt is an easy way to integrate PostHog into your website without having to update your codebase. \nPrerequisites\nTo follow this tutorial along, you should:\n\nHave deployed PostHog or be using PostHog Cloud.\n\nStep-by-step instructions\n\nGet your PostHog snippet from your 'Project Settings' or the initial PostHog setup\nAccess your Google Tag Manager dashboard and navigate to the desired account/container that is integrated with the website you want to add PostHog tracking to\n\nClick to add a new tag:\n\n\n\nOn the page to configure a new tag, add your PostHog snippet as a 'Custom HTML Tag' under 'Tag Configuration'\n\nFor the trigger, select the default \"All Pages - Page View\" trigger and then click 'Save' on the top right corner of the drawer\nBack on the main dashboard, click 'Submit' to update your website with the new PostHog tag\nYou're done! PostHog is now configured for your website.\n",
    "tag": "posthog"
  },
  {
    "title": "Objective",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/integrate/third-party/shopify.md",
    "content": "\ntitle: Shopify\nicon: ../../../images/docs/integrate/frameworks/shopify.svg\n\nObjective\nIntegrating PostHog with your Shopify store\nWhy is this useful?\nTracking how users use your Shopify store can help you improve the user experience and increase conversion rates on your e-shop.  \nPrerequisites\nTo follow this tutorial along, you should:\n\nHave deployed PostHog or be using PostHog Cloud.\n\nStep-by-step instructions\n\nGet your PostHog snippet from your 'Project Settings' or the initial PostHog setup\nLogin to your Shopify dashboard\nGo to 'Online Store' -> 'Themes' (see image below)\n\nOn your theme, click 'Actions' -> 'Edit code' (see image below)\n\n\n\n\n\nYou should now be seeing a code editor. Click on `theme.liquid` under 'Layout' on the left sidebar (see image below)\n\n\nNavigate until you see the closing `</head>` tag. Paste your snippet there, before that tag, like in the image below:\n\n\n\n\n\nClick the green save button on the top right and you're good to go - PostHog should now be working on your Shopify store!\n\n",
    "tag": "posthog"
  },
  {
    "title": "Prerequisites",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/integrate/third-party/next-js.md",
    "content": "\ntitle: Next.js\nicon: ../../../images/docs/integrate/frameworks/nextjs.svg\n\nPostHog makes it easy to get data about traffic and usage of your Next.js app. Integrating PostHog into your site enables analytics about user behavior, custom events capture, session recordings, feature flags, and more.\nThis guide will walk you through an example integration of PostHog using Next.js and the posthog-js library. \nPrerequisites\nTo follow this tutorial along, you need:\n\na self-hosted instance of PostHog or use PostHog Cloud.\na running Next.js application\n\nSetup and tracking page views (automatically)\nThe first thing you want to do is to install the posthog-js in your project - so add it using your package manager:\n`shell\nyarn add posthog-js`\nor\n`shell\nnpm install --save posthog-js`\nAfter that, we want to initialize the PostHog instance in `pages/_app.js`\n```jsx\nimport { useRouter } from 'next/router';\nimport { useEffect } from 'react';\nimport posthog from 'posthog-js';\nif (typeof window !== \"undefined\") {\n  // This ensures that as long as we are client-side, posthog is always ready\n  // NOTE: If set as an environment variable be sure to prefix with `NEXT_PUBLIC_`\n  // For more info see https://nextjs.org/docs/basic-features/environment-variables#exposing-environment-variables-to-the-browser\nposthog.init('', { api_host: '' });\n}\nfunction MyApp({ Component, pageProps }) {\n  const router = useRouter();\nuseEffect(() => {\n    // Track page views\n    const handleRouteChange = () => posthog.capture('$pageview');\n    router.events.on('routeChangeComplete', handleRouteChange);\n\n\n```return () => {\n  router.events.off('routeChangeComplete', handleRouteChange);\n};\n```\n\n\n}, []);\nreturn ;\n}\nexport default MyApp;\n```\nDisable in development\n```tsx\nimport posthog from 'posthog-js';\nif (typeof window !== \"undefined\") {\n  // This ensures that as long as we are client-side, posthog is always ready\n  posthog.init('', { \n    api_host: '', \n    loaded: (posthog) => {\n      if (process.env.NODE_ENV === 'development') posthog.opt_out_capturing()\n    },\n  });\n}\n```\nTracking custom events\nNow that PostHog is setup and initialized PostHog, you can use it to capture events where you want to track user behavior. For example, if you want to track when a user clicks a button, you can do it like this:\n```jsx\nconst handleOnBuy = () => {\n  posthog.capture('purchase', { price: 5900, currency: 'USD' });\n};\nreturn (\n\n\nStore\nBuy\n\n);\n```\nFurther reading\n\nComplete guide to event tracking\nTracking pageviews in single page apps (SPA)\n",
    "tag": "posthog"
  },
  {
    "title": "Objective",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/integrate/third-party/retool.md",
    "content": "\ntitle: Retool\nicon: ../../../images/docs/integrate/frameworks/retool.svg\n\nObjective\nIntegrating PostHog with Retool.\nWhy is this useful?\nRetool is a platform you can use to quickly build internal tools that leverage your data from different sources with little to no-code.\nPrerequisites\nTo follow this tutorial along, you should:\n\nHave deployed PostHog. \nHave a Retool account\n\nStep-by-step instructions\nRetool app setup\nFirst, create a new app from the Retool dashboard:\n\nYou will then need to create a new resource i.e. set up the configuration for where you'll pull the date from. If you have not created any resources before, Retool will prompt you to create one. \nOtherwise, you can add a new resource on the queries tab at the bottom of the page, by creating a new query and selecting '+ Add a new resource' on the query tab.\nThe resource you create will depend on how you use PostHog. If you're self-hosting our open-source version you can connect Retool directly to your PostgreSQL database. However, if you're using PostHog Cloud or our enterprise offering, you should use our API. Regarding the enterprise offering, this is the case because Retool does not yet integrate with ClickHouse.\n\nIf you're on EE, we still use Postgres for certain types of data which you could use with Retool, but you'll have no access to event data, for example.\n\nAPI access is significantly slower, especially since we need to setup a way for Retool to handle pagination on our endpoints.\nTo see the difference in speed, check out our demo Retool app:\n\n\n\nIntegrating directly with PostgreSQL\nIntegrating Retool with PostgreSQL directly is rather simple. \n\nWhen creating a new resource, select the PostgreSQL integration. \nThis will prompt you to enter your database details, which are the same authentication parameters you use to connect the database to PostHog. \nWith the connection complete, you are now able to run SQL queries on your PostHog database and use the result on Retool tables, charts, and any other component available. Retool also makes the database tables and their respective schemas available to you, making the process of writing queries easier. \n\n\nNote: Where your Database credentials will be available to you is dependent on the deployment method you used. If you deployed PostHog on Heroku, you can find them on the settings for the 'Heroku Postgres' add-on. In the case of AWS CloudFormation, these are available on the RDS settings. And, most importantly, if you deployed PostHog using Docker, some additional setup is required to allow Retool to connect to the database.\n\nIntegrating via API\n\nWhen creating a new resource, select \"REST API\". \n\nThis will open a configuration page for the API resource:\n\n\n\n\n\nOn the configuration page, use `https://app.posthog.com/api/` for the 'Base URL' if you're using PostHog Cloud. Otherwise use the address of your PostHog instance, followed by `/api/`. Then, on the 'Headers' section, configure a header called `Authorization` with value `Bearer <YOUR_PERSONAL_API_KEY>`. For more information on API authentication, see our dedicated page for this. \n\nClick 'Create Resource' and you should now be able to connect to PostHog endpoints through Retool queries. For information on our endpoints, see our API Documentation.\nFor some of our endpoints, this configuration is enough. However, endpoints like `/event` and `/person` have pagination, which Retool does not support out of the box. As such, follow the next steps for instructions on how to handle PostHog's pagination with Retool.\n\nHandling pagination with Retool\nTo handle pagination in Retool and show results beyond the first \"page\", we need to do some Retool magic.\n\nNote: This example will teach you how to handle pagination on our endpoints that use cursor-based pagination, such as `/person`. Trying this with the events endpoint will not work. We are working on a tutorial for that, but Retool requires extensive workarounds to make it work.\n\n\nFirst, create a new query that uses the PostHog API resource and input the endpoint you want to use. \n\nThen, add a 'URL parameter' called `cursor` and set its value to `{{cursor}}`. Here's what the config will look like:\n\n\n\n\n\nRetool will complain about `cursor` not being defined, but that's OK. This is the recommended way to handle paginated API endpoints while that isn't supported natively. \"Save and run\" your query. It will fail as expected.\n\n\nNow, create a new query, using the 'Run JS Code (javascript)' resource. On the text editor, paste the following:\n```js\nconst fetchMany = (cursor, results, n) => {\n// Return results if limit reached or all records fetched\n  if (!cursor || n === Math.ceil(Number(textinput1.value) / 100)) {\n    return results\n  }\nreturn new Promise((resolve) => {\n    return query1.trigger({\n      additionalScope: {\n        cursor // Set the cursor URL param \n      },\n      onSuccess: (queryResult) => {\n        // Join with results from the previous request\n        const newResults = results.concat(queryResult.results)\n\n\n```    let nextCursor = \"\" \n    if (queryResult.next) {\n      // Decode cursor value so Retool can re-encode it\n      nextCursor = decodeURIComponent(\n        queryResult.next.split(\"?\")[1].slice(7)\n      )\n    }\n    return resolve(fetchMany(nextCursor, newResults, n + 1))\n  }\n})\n```\n\n\n})\n}\nreturn fetchMany(\" \", [], 0)\n```\nThis JavaScript query will inject the `cursor` value as the undefined URL param from the previous step and run until the API has returned all the results or a limit specified in a text input has been reached. In this case, we are reading the number of records to fetch from `textinput1`. You can see how this works in our demo Retool app, and can remove the logic associated with the text input if you prefer.\n\n\nLastly, set a component to read the data from your query (by setting 'Data' to `{{your_js_query.data}}`) and determine the triggers for running it (e.g. on page load). \n\n\nAnd that's it, enjoy having your PostHog data in Retool!",
    "tag": "posthog"
  },
  {
    "title": "Install",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/integrate/third-party/docusaurus.mdx",
    "content": "\ntitle: Docusaurus\nicon: ../../../images/docs/integrate/frameworks/docusaurus.svg\ntags:\n  - community\n\n\nThis is a community integration that is not maintained by the PostHog core team.\n\nInstall\n`bash\nyarn add posthog-docusaurus`\nor\n`bash\nnpm install --save posthog-docusaurus`\nHow to use\n`js\n// in docusaurus.config.js\nmodule.exports = {\n  plugins: [\n    [\n      \"posthog-docusaurus\",\n      {\n        apiKey: \"<ph_project_api_key>'\",\n        appUrl: \"<ph_instance_address>\", // optional\n        enableInDevelopment: false, // optional\n        // other options are passed to posthog-js init as is\n      },\n    ],\n  ],\n};`\nThis will automatically start tracking pageviews, clicks and more.",
    "tag": "posthog"
  },
  {
    "title": "1. Create app",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/integrate/webhooks/slack.md",
    "content": "\ntitle: Slack\nsidebar: Docs\nshowTitle: true\n\n\nFor message formatting instructions, see this dedicated page.\n\n1. Create app\nGo to the Slack page to create apps and create a new app. Call it \"PostHog\" and connect it to the workspace of your choice.\nFeel free to use an image from here as the app's logo.\n\n2. Create webhook\nGo to the 'Incoming Webhooks' page for your newly-created app and toggle 'Activate Incoming Webhooks' to turn it on. Then click on 'Add New Webhook to Workspace' and select the channel that the notifications will be posted to:\n\n3. Setup webhook in PostHog\nCopy the Webhook URL into the PostHog Setup page:\n\n4. Add to action\nFor each action that should be posted to Slack, select \"Post to webhook when this action is triggered\":\n\n5. Celebrate!\n",
    "tag": "posthog"
  },
  {
    "title": "Default",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/integrate/webhooks/message-formatting.md",
    "content": "\ntitle: Message formatting\nsidebar: Docs\nshowTitle: true\n\nMessage formatting works the same way whether you are using Slack, Microsoft Teams, or Discord.\nTokens are in the format `[type.property]`.\nMessage format can be customised when editing/creating an Action.\nDefault\nBy default, the message format is\n`[action.name] was triggered by [user.name].`\nTypes\nAction\nAllowed properties\n\n`name`: name of triggered action with link.\n\nEvent\nAllowed properties\n\n`name`: name of the event which triggered the Action with link.\n\nAccessing event properties\nYou can also access any property on the event that triggered the webhook by using `event.properties.your_desired_prop`.\nThis can be a property set by PostHog, such as `event.properties.ip`, and it can also use any custom properties you set, like `event.properties.username`. \nIf a property does not exist on the event, the webhook message will say `undefined` for the property.\nFor example, the following message format:\n`[user.name] triggered [action.name] from [event.properties.country]`\nWould yield the a message like the one below, if the property `country` is not set on the event:\n`John Doe triggered Pageview from undefined`\nUser\nAllowed properties\n\n`name`: user's username, email, or distinct ID depending on availability.\n`ip`: IP address used by user when the action was triggered.\n`os`: user's Operating System.\n`browser`: user's Web Browser.\n`browser_version`: version of user's Web Browser.\n`host`: URL of PostHog instance user connected via.\n`time`: timestamp of Event.\n`pathname`: HREF path Action was triggered on.\n`device_id`: ID of user's device.\n`screen_width`: width of user's screen.\n`screen_height`: height of user's screen.\n",
    "tag": "posthog"
  },
  {
    "title": "1. Create an incoming webhook in Teams",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/integrate/webhooks/microsoft-teams.md",
    "content": "\ntitle: Microsoft Teams\nsidebar: Docs\nshowTitle: true\n\n\nFor message formatting instructions, see this dedicated page.\n\n1. Create an incoming webhook in Teams\n\nNavigate to the channel where you want to add the webhook and select (\u2022\u2022\u2022) More Options from the top navigation bar.\nChoose Connectors from the drop-down menu and search for Incoming Webhook.\nSelect the Configure button, provide a name, and, optionally, upload an image avatar for your webhook.\nThe dialog window will present a unique URL that will map to the channel. Make sure that you copy and save the URL\u2014you will need to provide it to the outside service.\nSelect the Done button. The webhook will be available in the team channel.\n\nSee the Microsoft Teams documentation for more info.\n2. Setup webhook in PostHog\nCopy the Webhook URL into the PostHog Setup page:\n\n3. Add to action\nFor each Action that should be posted to Teams, select \"Post to webhook when this action is triggered\":\n\n4. Celebrate!",
    "tag": "posthog"
  },
  {
    "title": "1. Create an incoming webhook in Discord ",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/integrate/webhooks/discord.md",
    "content": "\ntitle: Discord\nsidebar: Docs\nshowTitle: true\n\n\nFor message formatting instructions, see this dedicated page.\n\n1. Create an incoming webhook in Discord\n\nNavigate to the channel where you want to add the webhook and select the 'Edit Channel' option from left navigation pane. \nSelect the 'Integrations' option from the left navigation pane. \nIf creating a webhook for the first time, click the \"Create Webhook\" button. \nIf you have other webhooks, click \"View Webhooks\" and now click \"New Webhook\". \nGive the webhook any name you prefer (say PostHog). \nSelect the channel where the message should be posted from the drop-down.\nClick the \"Copy Webhook URL\" button to copy the webhook URL into the clipboard.  \n\nIt would look something like this. \n\nFor more information see the Discord webhook docs.\n2. Setup webhook in PostHog\n\n\nCopy the Webhook URL into the PostHog Setup page:\n\n\n\nClick \"Test & Save\" and you should receive a message on Discord. \n\n\n3. Add to action\nFor each Action that should be posted to Discord, select \"Post to webhook when this action is triggered\":\n\n4. Celebrate\n",
    "tag": "posthog"
  },
  {
    "title": "upgrade-notes.mdx",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/runbook/upgrade-notes.mdx",
    "content": "\ntitle: Upgrade notes\nsidebar: Docs\nshowTitle: true\n\nimport Sunset from \"../self-host/_snippets/sunset-disclaimer.mdx\"\n\n30.0.0\nSelf-hosted users must run async migrations 0005-0007 before updating to PostHog version 1.42.0.\nIf you haven't run async migrations 0005-0007 yet, or if they aren't available on your current version, we recommend upgrading to PostHog version 1.41.4 first, then running the async migrations, then upgrading to 1.42.0.\nNote: for async migration 0007 disable the post checks in the advanced options when kicking off the async migration (otherwise you might see them fail and need to re-run the migration with them disabled).\nIf you are not upgrading forward and staying on PostHog version 1.42 (not recommended), then turn off persons-on-events in the instance settings page in the app, in version 1.43 it's already disabled by default.\n29.0.0\nThis version upgrades PostHog version to 1.41, which comes with some breaking changes. The upgrade guide is here.\n28.0.0\nThis version changes the supported Kubernetes version to >=1.23 <= 1.25. Kubernetes 1.22 support has been dropped as it has reached end of life on 2022-10-28.\n27.0.0\nThis version upgrades the bundled `grafana/loki` Helm chart from `2.10.2` to `3.0.6`. The new 3.x Helm Chart\nis based on the old `grafana/loki-simple-scalable` and as such the configuration options are not compatible.\nIf you have `loki.enabled=false` (the default) then you do not need to do anything special with this upgrade.\nOtherwise, before performing the upgrade please ensure that you have read the upgrade documentation\non the Loki Chart repo.\n27.1.0\nIf you're upgrading the chart directly to 29.0.0 (recommended) ignore this (and run the 0005 migration on top of 1.41 if not ran already).\nThis version upgrades PostHog version to 1.40, which requires async migration 0006 to be completed (if you're on version <1.38 then upgrade to 1.39.1 and run 0005 and 0006 async migrations before upgrading to 1.40).\n26.0.0\nThis version upgrades the Prometheus service from version `2.31.1` to `2.36.2`. As part of this upgrade, we've also changed some default values in the `prometheus` stanza:\n\n`prometheus.alertmanager`\n`prometheus.kubeStateMetrics`\n`prometheus.nodeExporter`\n`prometheus.alertmanagerFiles`\n\nnow defaults as the upstream Helm chart.\nAdditionally `prometheus.serverFiles.alerting_rules.yml` has now new defaults that from now on we will consider UNSTABLE. Alerting is an important part of any production system. With this Helm chart, we aim to provide a good collection of default rules that can be used to successfully alert an operator if a PostHog installation is not working as expected. As those rules will probably evolve over time and as we don\u2019t want to cut a new major release every time it happens, please consider the default values of this input variable as UNSTABLE. Please consider to explicitly override this input in your `values.yaml` if you need to keep it stable.\n25.0.0\nThis version upgrades the PgBouncer service from version `1.12.0` to `1.17.0`. As part of this upgrade, we've migrated from the container image `edoburu/pgbouncer:1.12.0` to `bitnami/pgbouncer:1.17.0`. If you are not overriding `pgbouncer.env` values, there's nothing you need to do. Otherwise, please remember to verify if those are working with the new container image.\n24.0.0\nThis version changes the supported Kubernetes version to >=1.22 <= 1.24 by dropping the support for Kubernetes 1.21 as it has reached end of life on 2022-06-28.\n23.4.0\nUpdated the app version to 1.37.0 which requires the async migration 0004 to be completed, head over to `/instance/async_migrations` and make sure you run that before updating.\n23.0.0\nThis version changes the default ClickHouse service type from `NodePort` to `ClusterIP`. This is to remove the possibility of exposing the service in environments where the Kubernetes nodes are not deployed in private subnets or when they are deployed in public subnets but without any network restriction in place.\nIf you are not overriding the `clickhouse.serviceType` there's nothing you need to do. If you are overriding it using either `LoadBalancer` or `NodePort` please remember to keep your installation secure by:\n\ndeploying your Kubernetes nodes in private subnet(s) or restrict access to those via firewall rules\nrestrict network access to the load balancer\nadd authentication to the load balancer\nconfigure TLS for ClickHouse\nprovide a unique ClickHouse login by overriding the `clickhouse.user` and `clickhouse.password` values\nrestrict access to the ClickHouse cluster, ClickHouse offers settings for\n    restricting the ips/hosts that can access the cluster. See the\n    user_name/networks setting for details. We expose this setting\n    via the Helm Chart as `clickhouse.allowedNetworkIps`\n\nFor other suggestions and best practices take a look at our docs:\n\nPostHog chart configuration\nSecuring PostHog\n\nWe'd like to thank Alexander Nicholson and the team at TableCheck for sharing their POC with us, which allowed us to quickly reproduce and address this issue.\n22.0.0\nThis version upgrades ClickHouse from version `21.6.5` to `22.3.6.5`. This update brings several improvements to the overall service. For more info, you can look at the upstream changelog.\nNote: the ClickHouse pod(s) will be reprovisioned as part of this upgrade. We expect no downtime for the ingestion pipeline.\n21.0.0\nThis version changes the supported Kubernetes version to >=1.21 <= 1.24:\n\ndrops support for Kubernetes 1.20 as it has reached end of life on 2022-02-28\nadds support for Kubernetes 1.24 released on 2022-05-24\n\n20.0.0\nThis version upgrades the altinity/clickhouse-operator from version `0.16.1` to `0.18.4`. This brings some updates to the custom resource definition (CRD). In order to keep everything in sync, please run the following steps before updating your Helm release:\n\n\nDownload and extract the Helm chart release source code\n`mkdir -p posthog-crd-upgrade\nwget -qO- https://github.com/PostHog/charts-clickhouse/archive/refs/tags/posthog-20.0.0.tar.gz | tar xvz - --strip 1 -C posthog-crd-upgrade`\n\n\nUpdate the `altinity/clickhouse-operator` CRDs by running:\n`kubectl apply \\\n    -f posthog-crd-upgrade/charts/posthog/crds/clickhouseinstallations.clickhouse.altinity.com.yaml \\\n    -f posthog-crd-upgrade/charts/posthog/crds/clickhouseinstallationtemplates.clickhouse.altinity.com.yaml \\\n    -f posthog-crd-upgrade/charts/posthog/crds/clickhouseoperatorconfigurations.clickhouse.altinity.com.yaml`\nNote: you'll likely see a warning like:\n```\nWarning: resource customresourcedefinitions/clickhouseinstallations.clickhouse.altinity.com is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.\ncustomresourcedefinition.apiextensions.k8s.io/clickhouseinstallations.clickhouse.altinity.com configured\nWarning: resource customresourcedefinitions/clickhouseinstallationtemplates.clickhouse.altinity.com is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.\ncustomresourcedefinition.apiextensions.k8s.io/clickhouseinstallationtemplates.clickhouse.altinity.com configured\nWarning: resource customresourcedefinitions/clickhouseoperatorconfigurations.clickhouse.altinity.com is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.\ncustomresourcedefinition.apiextensions.k8s.io/clickhouseoperatorconfigurations.clickhouse.altinity.com configured\n```\nbut this is safe to be ignored.\n\n\nYou can now move on with the Helm update as usual:\n    `helm repo update posthog\n    helm upgrade --install -f ...`\n    Note: the ClickHouse pod will not be restarted but the `clickhouse-operator` will, no downtime is expected as part of this release.\n\n\n19.0.0\nThis version upgrades the Redis dependency chart from version `14.6.2` to `16.8.9` and upgrades Redis from version `6.2.4` to `6.2.7`.\nThe upstream changelog includes changes that shouldn't be relevant to the majority of our users but if you are overriding any of the values listed in the changelog, please make the corresponding changes before upgrading. Otherwise there's nothing you need to do.\n18.0.0\nThis version requires 3 async migrations to be completed.\nIf you're on PostHog app version 1.33 head over to `/instance/async_migrations` and run first the three required migrations.\nIf you're on a PostHog app version < 1.33:\n\nupgrade to chart version 16.x.x first (example: use `--version 16.1.0` in the `helm upgrade` command)\nrun the async migrations at `/instance/async_migrations`\ncontinue the upgrade process as usual\n\n17.0.0\nThis version upgrades the Kafka dependency chart from version `12.6.0` to `14.9.3` and upgrades Kafka to version `2.8.1`.\nThe upstream changelog includes changes that shouldn't be relevant to the majority of our users but if you are overriding `kafka.image` or `kafka.provisioning`, please make the corresponding changes before upgrading. Otherwise there's nothing you need to do.\nNote: the Kafka pod will be reprovisioned as part of this upgrade and the ingestion pipeline will experience a brief downtime.\n16.0.0\nThis version improves the configuration of Kafka.\nThe built-in Kafka service type default is now `ClusterIP` from the previous `NodePort`. If you were relying on this setting you can override the new default by setting `kafka.service.type` to `NodePort`.\nAs part of this work, we have also renamed a few chart inputs in order to reduce confusion and align our naming convention to the industry standards:\n\n`kafka.url`, `kafka.host`, `kafka.port` have been consolidated into the `externalKafka.brokers` variable.\n\nIf you are overriding any of those values, please make the corresponding changes before upgrading. Otherwise there's nothing you need to do.\n15.0.0\nThis version renamed the `prometheus-statsd-exporter` subchart alias from `statsd` to the default (`prometheus-statsd-exporter`).\nAs part of this work, we've also:\n\ndeprecated all the resources in the `metrics` namespace\nadded the possibility to use an external `statsd` service by using the `externalStatsd` variable\n\nIf you are using the `statsd` or the `metrics` variables, please make the corresponding changes before upgrading. Otherwise there's nothing you need to do.\n14.0.0\nThis version fixes customizing PostgreSQL installation, previously many `values.yaml` values did not work as expected.\nAs part of this work, the following chart inputs have changed:\n\n`postgresql.postgresqlHost` -> `externalPostgresql.postgresqlHost`\n`postgresql.postgresqlPort` -> `externalPostgresql.postgresqlPort`\n`postgresql.postgresqlUsername` -> `externalPostgresql.postgresqlUsername`\nExisting `postgresql.postgresqlUsername` was removed as PostHog requires admin access to the postgresql cluster to install extensions.\n`postgresql.existingSecret` will now work after 14.0.0\n`postgresql.existingSecretKey` was removed. This did not previously work.\n\nIf you are overriding any of those values, please make the corresponding changes before upgrading. Otherwise there's nothing you need to do.\n13.0.0\nThis version fixes connecting to an external ClickHouse cluster. You can now also specify a secret containing the external ClickHouse service password.\nAs part of this work, the following chart inputs have changed:\n\n`clickhouse.host` -> `externalClickhouse.host`\n`clickhouse.enabled` now toggles the internal ClickHouse cluster on/off. If this is off, you will need to specify an external clickhouse cluster.\n`clickhouse.database` was previously used as the cluster name as well. Now `clickhouse.cluster` has been introduced.\n`clickhouse.user` is now usable\n`clickhouse.replication` was removed\n\nIf you are overriding any of those values, please make the corresponding changes before upgrading. Otherwise there's nothing you need to do.\n12.0.0\nThis version fixes a couple of long standing bugs related to the Redis service setup. You can now provide a Redis password (or a pointer to a Kubernetes secret containing it) directly in your `values.yaml` file.\nAs part of this work, we have also renamed a few chart inputs in order to reduce confusion and align our naming convention to the industry standards:\n\n`redis.host` -> `externalRedis.host`\n`redis.port` -> `externalRedis.port`\n`redis.password` -> `externalRedis.password`\n\nIf you are overriding any of those values, please make the corresponding changes before upgrading. Otherwise there's nothing you need to do.\n11.0.0\nThis version removes some legacy Helm annotations not needed anymore. By removing those and upgrading your installation, all future upgrades to stateless components should now happen without downtime (see #179 for more info).\nBefore running the Helm upgrade command, please run the following script first (replace the `RELEASE_NAME` and `RELEASE_NAMESPACE` accordingly if you are using a custom release name/namespace, which can be found via `helm list`):\n```\n!/usr/bin/env sh\nRELEASE_NAME=\"posthog\"\nRELEASE_NAMESPACE=\"posthog\"\nfor deployment in $(kubectl -n \"$RELEASE_NAMESPACE\" get deployments --no-headers -o custom-columns=NAME:.metadata.name | grep \"posthog\")\ndo\n  kubectl -n \"$RELEASE_NAMESPACE\" label deployment \"$deployment\" \"app.kubernetes.io/managed-by=Helm\"\n  kubectl -n \"$RELEASE_NAMESPACE\" annotate deployment \"$deployment\" \"meta.helm.sh/release-name=$RELEASE_NAME\"\n  kubectl -n \"$RELEASE_NAMESPACE\" annotate deployment \"$deployment\" \"meta.helm.sh/release-namespace=$RELEASE_NAMESPACE\"\ndone\n```\nNote: you can safely ignore errors like `error: --overwrite is false but found the following declared annotation(s): 'meta.helm.sh/release-name' already has a value (posthog)`\nAfter that you continue the Helm upgrade process as usual.\n10.0.0\nThis version introduces two major changes:\n\n\nas of today we've been including additional `StorageClass` definition into our Helm chart when installing it on AWS or GCP platforms. Starting from this release, we will not do that anymore and we will rely on the cluster default storage class. If you still want to install those additional storage classes, simply set `installCustomStorageClass: true` in your `values.yaml`. If you are planning to use the default storage class, make sure you are running with our requirement settings (`allowVolumeExpansion` set to `true` and `reclaimPolicy` set to `Retain`).\n\n\nwe have renamed few chart inputs in order to reduce confusion and align our naming convention to the industry standards:\n\n`clickhouseOperator.enabled` -> `clickhouse.enabled`\n`clickhouseOperator.namespace` -> `clickhouse.namespace`\n`clickhouseOperator.storage` -> `clickhouse.persistence.size`\n`clickhouseOperator.useNodeSelector` -> `clickhouse.useNodeSelector`\n`clickhouseOperator.serviceType` -> `clickhouse.serviceType`\n`clickhouse.persistentVolumeClaim` -> `clickhouse.persistence.existingClaim`\n\nIf you are overriding any of those, please make the corresponding changes before upgrading. Depending on your settings and setup, during this upgrade the ClickHouse pod might get recreated.\n\n\n9.0.0\nThis version changes the supported Kubernetes version to >=1.20 <= 1.23:\n\ndrops support for Kubernetes 1.19 as it has reached end of life on 2021-10-28\nadds support for Kubernetes 1.23 released on 2021-12-07\n\n8.0.0\nThis version deprecates the `beat` deployment (#184) as its functionalities are now executed by the `workers` deployment.\nAs result, we have deprecated the following Helm values:\n\n`beat.replicacount`\n`beat.resources`\n`beat.nodeSelector`\n`beat.tolerations`\n`beat.affinity`\n\nIf you didn\u2019t make any customization to those, there\u2019s nothing you need to do. Otherwise, please rename your customized values to be in the `workers.` scope.\n7.0.0\nThis version upgrades the Helm dependency chart jetstack/cert-manager from version `1.2.0` to `1.6.1`. This brings some updates to the custom resource definition (CRD).\nIf you are not overriding `cert-manager.installCRDs` by setting it to `false` there\u2019s nothing you need to do. You can go on updating your Helm release as usual and enjoy your day!\nIf otherwise you are manually managing the `cert-manager`\u2018s CRDs, please remember to update the definitions in order to keep everything in sync.\n6.0.0\nThis version upgrades the altinity/clickhouse-operator version to `0.16.1`. This brings some updates to the custom resource definition (CRD). In order to keep everything in sync, please run the following steps before updating your Helm release:\n\n\nDownload and extract the Helm chart release source code\n`mkdir -p posthog-crd-upgrade\nwget -qO- https://github.com/PostHog/charts-clickhouse/archive/refs/tags/posthog-6.0.0.tar.gz | tar xvz - --strip 1 -C posthog-crd-upgrade`\n\n\nUpdate the `altinity/clickhouse-operator` CRDs by running:\n`kubectl apply \\\n    -f posthog-crd-upgrade/charts/posthog/crds/clickhouseinstallations.clickhouse.altinity.com.yaml \\\n    -f posthog-crd-upgrade/charts/posthog/crds/clickhouseinstallationtemplates.clickhouse.altinity.com.yaml \\\n    -f posthog-crd-upgrade/charts/posthog/crds/clickhouseoperatorconfigurations.clickhouse.altinity.com.yaml`\nNote: you'll likely see a warning like:\n```\nWarning: resource customresourcedefinitions/clickhouseinstallations.clickhouse.altinity.com is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.\ncustomresourcedefinition.apiextensions.k8s.io/clickhouseinstallations.clickhouse.altinity.com configured\nWarning: resource customresourcedefinitions/clickhouseinstallationtemplates.clickhouse.altinity.com is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.\ncustomresourcedefinition.apiextensions.k8s.io/clickhouseinstallationtemplates.clickhouse.altinity.com configured\nWarning: resource customresourcedefinitions/clickhouseoperatorconfigurations.clickhouse.altinity.com is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.\ncustomresourcedefinition.apiextensions.k8s.io/clickhouseoperatorconfigurations.clickhouse.altinity.com configured\n```\nbut this is safe to be ignored.\n\n\nYou can now move on with the Helm update as usual:\n    `helm repo update posthog\n    helm upgrade --install -f ...`\n    Note: the ClickHouse pod will not be restarted but the `clickhouse-operator` will, no downtime is expected as part of this release.\n\n\n5.0.0\nThis version changes defaults for Kafka PVC size and log retention policy. If your upgrade fails with the following:\n`Error: UPGRADE FAILED: cannot patch \"posthog-posthog-kafka\" with kind StatefulSet: StatefulSet.apps \"posthog-posthog-kafka\" is invalid: spec: Forbidden: updates to statefulset spec for fields other than 'replicas', 'template', and 'updateStrategy' are forbidden`\nThere are two options:\n\nChange your values to match to what the Kafka values were before (`size: 5Gi` and `logRetentionBytes: _4_000_000_000`)\nResize the PVC (Persistent Volume Claim) used by Kafka following our runbook kafka-resize-disk\n\n4.0.0\nThis version introduces a breaking change related to how `cert-manager` CRDs resources are deployed.\n\nwe renamed the `certManager.enabled` variable to `cert-manager.enabled`\n\nwe introduced a new variable called `cert-manager.installCRDs`. `cert-manager` requires a number of CRD resources to work.\n\nto automatically install and manage them as part of your Helm release, simply leave the new `cert-manager.installCRDs` variable set to `true`\nto manually install and manage them, simply set the new `cert-manager.installCRDs` variable to `false`\n\n\n\n3.0.0\nThis version changes the way ZK is run in the chart. ZK has been spun out and is now a cluster being used for Kafka and ClickHouse. An unfortunate side effect of that is that Kafka StatefulSet must be deleted. The reason for this is because Kafka records the cluster ID in ZooKeeper and when you swap it out it complains that the cluster ID has changed and refuses to start. Note that when you delete the Kafka StatefulSet and pod you might lose some events that were in Kafka, but not yet in ClickHouse.\nThe error you will see from Kafka pod while upgrading:\n`[2021-07-23 14:24:27,143] ERROR Fatal error during KafkaServer startup. Prepare to shutdown (kafka.server.KafkaServer)\nkafka.common.InconsistentClusterIdException: The Cluster ID TYP8xsIWRFWkzSYmO_YWEA doesn't match stored clusterId Some(CizxEcefTou4Ehu65rmpuA) in meta.properties. The broker is trying to join the wrong cluster. Configured zookeeper.connect may be wrong.`\nHow to fix?\n\nDelete Kafka and Zookeeper StatefulSet `kubectl -n posthog delete sts posthog-posthog-kafka posthog-zookeeper`\nDelete kafka persistent volume claim `kubectl -n posthog delete pvc data-posthog-posthog-kafka-0`\nWait for Kafka and Zookeeper pods to spin down (deleting sts in step 1 will also trigger the pods deletion)\nUpgrade helm chart `helm upgrade -f values.yaml --timeout 30m --namespace posthog posthog posthog/posthog`\n\n2.0.0\nThis version updated Redis chart in an incompatible way. If your upgrade fails with the following:\n`Upgrade \"posthog\" failed: cannot patch \"posthog-posthog-redis-master\" with kind StatefulSet: StatefulSet.apps \"posthog-posthog-redis-master\" is invalid: spec: Forbidden: updates to statefulset spec for fields other than 'replicas', 'template', and 'updateStrategy' are forbidden`",
    "tag": "posthog"
  },
  {
    "title": "index.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/runbook/index.md",
    "content": "\ntitle: Runbook\nsidebar: Docs\nshowTitle: true\n\nThis section contains routine procedures and operations to manage your self-hosted PostHog installations. The runbooks contain procedures to begin, stop, supervise, and debug a PostHog infrastructure.\n\nIf you're using PostHog Cloud, there's no need to worry about any of this as we handle updating, scaling, and monitoring for you!\n\nUpgrading\n\nOverview\nUpgrade notes\n\nMigrating\n\nBetween PostHog instances\nTo PostHog from Amplitude\n\nServices\n\nClickHouse\nPlugin server\nKafka\nMinIO\nPostgreSQL\nRedis\n",
    "tag": "posthog"
  },
  {
    "title": "1. Get and update the Helm repo",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/runbook/upgrading-posthog.mdx",
    "content": "\ntitle: Upgrading PostHog\nsidebar: Docs\nshowTitle: true\n\nimport Sunset from '../self-host/_snippets/sunset-disclaimer.mdx'\nimport CommandHelmGetRepoSnippet from '../self-host/_snippets/command-helm-get-repo.mdx'\nimport CommandHelmUpgradeSnippet from '../self-host/_snippets/command-helm-upgrade.mdx'\n\nWe want to make sure upgrading PostHog is as smooth as possible.\nWe use a synchronous and an asynchronous migration framework to make sure database migrations are easy to be deployed, rolled back and safe to run.\nBecause some data migrations require touching tables that can get very large for some PostHog installation, some migrations are a little more involved.\nFor more information on any extra steps needed when updating to specific versions, check out our upgrade notes.\nIf you need help, feel free to join our community Slack for further assistance.\n1. Get and update the Helm repo\nTo upgrade the Helm release `posthog` in the `posthog` namespace:\n\n2. Follow the upgrade notes\n`shell\nhelm list -n posthog\nhelm search repo posthog`\nTake note of the numbers of the Helm chart version (in the format `posthog-{major}.{minor}.{patch}` - for example, `posthog-19.15.1`) when running the commands above.\nMake sure to check the upgrade notes for the new version, along with all the prior versions before moving forward**.\n3. Run the upgrade\n",
    "tag": "posthog"
  },
  {
    "title": "Data considerations",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/runbook/disaster-recovery.mdx",
    "content": "\ntitle: Disaster recovery considerations\nsidebarTitle: Disaster recovery\nsidebar: Docs\nshowTitle: true\n\nIt's important to make sure you understand how to recovery from scenarios before\nthey happen.\nData considerations\nData is stored in PostgreSQL, ClickHouse and Kafka. At a minimum you should have\nbackups of PostgreSQL and ClickHouse, these are crucial to PostHog. Kafka stores\nthe incoming analytics events which are then persisted into ClickHouse.\nNote: in order to work, ClickHouse and Kafka use Zookeeper as distributed key value\nstore and locking service. While Zookeeper doesn't host any data related to PostHog\nit's a core dependency of the overall stack.\nPostgreSQL\nPostgreSQL stores your PostHog instance configuration and authentication/authorization.\nWe recommend that you use a provider that supports backups. For\nexample, AWS RDS will handle backups and restores for you.\nOther cloud providers offer similar services.\nClickHouse\nFor ClickHouse, see our ClickHouse backup runbook.\nClickHouse consumes from Kafka using a static consumer group. Make sure that you\nonly ever have one cluster consuming at a time. When bringing up a cluster from backup\nit is a good idea to ensure that the consumer cursors are reset such that the cluster\nconsumes any events in Kafka that may be missing from your ClickHouse the backup.\nKafka\nKafka is the first place events are persisted before being consumed by other services,\nand ultimately ending up in ClickHouse. Solutions exist for streaming this data to\nlong term storage. For example, with AWS you can use either the\nLenses.io S3 Kafka Connector\nor AWS Kinesis Firehose.\nIf PostHog is consuming events to ClickHouse in good time, you should not lose much\nin case of a Kafka failure, so it may be a risk that you want to accept.\nZookeeper\nZooKeeper is a centralized service providing distributed synchronization. It is a core",
    "tag": "posthog"
  },
  {
    "title": "debug-hanging-freezing-process.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/runbook/services/clickhouse/debug-hanging-freezing-process.md",
    "content": "\ntitle: Debug hanging / freezing process\nsidebar: Docs\nshowTitle: true\n\nIf a ClickHouse node is unresponsive and you don't know what is happening, you can easily check the stacktraces of all the working threads by running:\n`sql\nSELECT\n arrayStringConcat(arrayMap(x -> demangle(addressToSymbol(x)), trace), '\\n') AS trace_functions,\n count()\nFROM system.stack_trace\nGROUP BY trace_functions\nORDER BY count()\nDESC\nSETTINGS allow_introspection_functions=1\nFORMAT Vertical;`\nIf you can't run any query but you have access to the node running the process, you can execute the following command to send a `TSTP` signal to every thread of the process:\n`shell\nfor i in $(ls -1 /proc/$(pidof clickhouse-server)/task/); do kill -TSTP $i; done`",
    "tag": "posthog"
  },
  {
    "title": "vertical-scaling.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/runbook/services/clickhouse/vertical-scaling.md",
    "content": "\ntitle: Scaling ClickHouse Vertically\nsidebar: Docs\nshowTitle: true\n\nimport Sunset from \"../../../self-host/_snippets/sunset-disclaimer.mdx\"\n\nHow to scale ClickHouse vertically\nCurrently the easiest way to scale up a ClickHouse environment hosted by our helm chart config is to set the affinity for which node ClickHouse is deployed to and scale that node up in terms of the resources it has available to it. This is very easy to do in practice. Let's get down to the nuts and bolts of how to get this done!\n\nCreate a node instance or group with more CPU and memory in your K8s cluster with a label of `clickhouse:true` set on it (this will be used to target that node for ClickHouse deployment). There are a few ways to create a node group and most are implementation specific to your kubernetes platform. A few references for how to create an manage node groups can be found for GKE, EKS, and DigitalOcean.\nEssentially if you know the node that you want ClickHouse to be installed on you can run `kubectl label nodes <desired-clickhouse-node-name> clickhouse=true`\nTo restrict other pods from not using that node we can add a taint via `kubectl taint nodes <desired-clickhouse-node-name> dedicated=clickhouse:NoSchedule`\n\n\nUpdate your `values.yaml`:\n\n`clickhouse:\n  nodeSelector:\n    clickhouse: \"true\"\n  tolerations:\n    - key: \"dedicated\"\n      value: \"clickhouse\"\n      operator: \"Equal\"\n      effect: \"NoSchedule\"`\n\nYou might need to trigger the reallocation for the clickhouse pod, e.g. run `kubectl delete pod chi-posthog-posthog-0-0-0`\n",
    "tag": "posthog"
  },
  {
    "title": "restore.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/runbook/services/clickhouse/restore.md",
    "content": "\ntitle: Restore\nsidebar: Docs\nshowTitle: true\n\nA backup is worthless if the restoration process hasn\u2019t been tested. If you backed something up and never tried to restore it, chances are that restore will not work properly when you actually need it or it will take longer than business can tolerate. Perform regular test restores to ensure your data will be there when you need it!\nAutomated using `clickhouse-backup`\nThe tool provides an automatic restore operation that you can invoke by running: `clickhouse-backup restore <BACKUP NAME>`.\nFor more information please look at the official documentation.\nManual\nIn this specific example, we will restore the table `events`.\n\n\nOptional: create the table you would like to restore (if it doesn\u2019t exist yet) from its backed up metadata file\n    `shell\n    cat events.sql | clickhouse-client --database posthog`\n\n\nCopy the backup data from the `data/posthog/events/` directory inside the backup you want to restore to the `/var/lib/clickhouse/data/posthog/events/detached/` directory\n    `shell\n    cp -r $SOURCE_BACKUP_LOCATION/data/posthog/events/* /var/lib/clickhouse/data/posthog/events/detached/`\n\n\nAdd the data to the table from the detached directory. It is possible to add data for an entire partition or for a separate part (take a look here for more info)\n    `shell\n    clickhouse-client --database posthog --query \"ALTER TABLE events ATTACH PARTITION 202111\"`\n\n\nYour data should be now available\n    ```shell\n    clickhouse-client --database posthog --query \"SELECT COUNT(1) from events\"\n\n",
    "tag": "posthog"
  },
  {
    "title": "kafka-engine.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/runbook/services/clickhouse/kafka-engine.md",
    "content": "\ntitle: Kafka Engine\nsidebar: Docs\nshowTitle: true\n\nClickHouse provides a table engine called Kafka Engine to consume Kafka messages, convert the messages into table rows, and persist the table rows into the destination table. Although a Kafka engine can be configured with multiple topics, a Kafka engine can only have one table schema defined.\nPostHog uses the `Kafka` engine type for several tables:\n`shell\nclickhouse-client --query \"SELECT name FROM system.tables WHERE database = 'posthog' AND engine = 'Kafka'\"\nkafka_events\nkafka_events_dead_letter_queue\nkafka_person\nkafka_person_distinct_id\nkafka_plugin_log_entries\nkafka_session_recording_events`",
    "tag": "posthog"
  },
  {
    "title": "How to set up sharding and replication",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/runbook/services/clickhouse/sharding-and-replication.md",
    "content": "\ntitle: Horizontal scaling (sharding & replication)\nsidebar: Docs\nshowTitle: true\n\nSharding is a horizontal cluster scaling strategy that puts parts of one ClickHouse database on different shards. This can help you to:\n\n\nImprove fault tolerance.\nSharding lets you isolate individual host or replica set malfunctions. If you don't use sharding, then when one host or a set of replicas fails, the entire data they contain may become inaccessible. But if 1 shard out of 5 fails, for example, then 80% of the table data is still available.\n\n\nImprove the query performance.\nRequests compete with each other for the computing resources of cluster hosts, which can reduce the rate of request processing. This drop in the rate usually becomes obvious as the number of read operations or CPU time per query grows. In a sharded cluster where queries to the same table can be executed in parallel, competition for shared resources is eliminated and query processing time is reduced.\n\n\nHow to set up sharding and replication\nSharding PostHog ClickHouse is a new experimental feature only supported from PostHog 1.34.0.\nTo use sharding, first upgrade to version >= 1.34.0 and run the `0004_replicated_schema` async migration\nUsing PostHog Helm charts\nUpdate `values.yaml` with the appropriate sharding settings.\nExample:\n`yaml\nclickhouse:\n  layout:\n    shardsCount: 3\n    replicasCount: 2`\nPostHog helm charts implement sharding by leveraging clickhouse-operator configuration. Full documentation for this can be found in clickhouse-operator documentation\nWith external ClickHouse\nIf you're using an external ClickHouse provider like Altinity.Cloud, you can change\nsharding and replication settings within that platform.\nNote that to propagate all the schemas to the new ClickHouse nodes, you should also do an `helm upgrade` which creates the right schema\non the new nodes.\nRebalancing data\nWhen adding new shards to an existing cluster, data between shards is not automatically rebalanced. This rebalancing is the job of the PostHog operator to do.",
    "tag": "posthog"
  },
  {
    "title": "Show me the queries:",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/runbook/services/clickhouse/debugging-load.md",
    "content": "\ntitle: Diagnosing ClickHouse load using ClickHouse\nsidebar: Docs\nshowTitle: true\n\nOne of the more powerful features of ClickHouse is its introspective capabilities. This can be easily leveraged to understand\nwhere load on our multi-tenant clickhouse servers is coming from.\nShow me the queries:\nThe following query gives an at-a-glance overview of what is generating load on the cluster:\n`sql\nwith\n    toDateTime(now()) as target_time,\n    toIntervalDay(3) as interval,\n    (\n        SELECT sum(read_bytes)\n        from clusterAllReplicas('posthog', system,query_log)\n        where event_time >= target_time - interval and event_time <= target_time and type > 1 and is_initial_query\n    ) AS total_bytes_read\nselect\n    multiIf(\n        http_user_agent = 'Apache-HttpClient/4.5.13 (Java/11.0.17)', 'Metabase?',\n        http_user_agent = 'Go-http-client/1.1', 'Grafana?',\n        http_user_agent = 'python-requests/2.28.1', 'infi_orm',\n        http_user_agent != '', http_user_agent,\n        query = 'SELECT version()', 'version_query',\n        JSONExtractString(log_comment, 'kind') = 'request' and JSONExtractString(log_comment, 'route_id') IN ('api/event/?$', 'api/projects/(?P<parent_lookup_team_id>[^/.]+)/events/?$'), '/api/event',\n        log_comment != '', JSONExtractString(log_comment, 'kind'),\n        query_kind = 'Insert',\n        query LIKE '%FORMAT JSON%' or (not is_initial_query and query like '%`elements_chain` FROM `posthog`.`sharded_events%') or (query like '%min(`_timestamp`) AS `min`, max(`_timestamp`)%'), 'historical-exports',\n        'unknown'\n    ) as query_type,\n    formatReadableSize(sum(read_bytes)) AS read_bytes_,\n    sum(read_bytes) / total_bytes_read AS read_bytes_percentage,\n    count(),\n    sum(query_duration_ms),\n    formatReadableSize(sum(result_bytes)) AS result_bytes_,\n    sum(read_rows),\n    arraySort(arrayDistinct(groupArray(getMacro('replica')))) as hosts\nfrom clusterAllReplicas('posthog', system, query_log)\nwhere event_time >= target_time - interval and event_time <= target_time and type > 1 and is_initial_query\ngroup by query_type\norder by sum(query_duration_ms) DESC`\nAdvanced\nTo diagnose further, it's important to understand ClickHouse operations.\nUseful dimensions to slice the data on:\n- `query_duration_ms` - How long the query took\n- `formatReadableSize(read_bytes)` - Total number of bytes read from all tables and table functions participated in query.\n- `formatReadableSize(memory_usage)` - Memory usage of this query.\n- `formatReadableSize(result_bytes)` - How big was the response for this query. Useful for `not is_initial_query` to determine if too much data streaming is going on.\n- `ProfileEvents['OSCPUVirtualTimeMicroseconds'])` - How much time was spent by the CPU\n- `ProfileEvents['ReadCompressedBytes'])` - How much data was read from disk (compressed)\n- `ProfileEvents['NetworkSendElapsedMicroseconds'])` - How much time was spent sending data over network\n- `ProfileEvents['NetworkReceiveBytes'])` - How much time was spent reading data over network\n- `ProfileEvents['NetworkSendBytes'])` - How much data was sent over the network\nOther useful expressions:\n- `is_initial_query` - indicates whether this was a main query or pushed down from coordinator. Note `log_comment` is also forwarded.\n- `any(log_comment)` - shows the structure of the log comment\n- `getMacro('replica')` - What replica was this on?\n- `getMacro('shard')` - What shard was this query made on?\n- `getMacro('hostClusterType')` - What cluster was this on? Online or offline?\nlog_comment\nWe make use of the log_comment column quite extensively to add metadata to queries in order to make analysis simpler.\n`log_comment` is set by specifying the `log_comment` setting when running a query. It then populates in the column with the same name on the query log table.\nSome useful things you'll find in `log_comment` include:\n\n`JSONExtractString(log_comment, 'kind')` - What is the query from? Either `celery` or `request`\n`JSONExtractString(log_comment, 'query_type')` - Type of query e.g. `trends_total_volume` or `funnel_conversion`\n`JSONExtractString(log_comment, 'id')` - Request path or task name (depending on kind)\n`JSONExtractString(log_comment, 'route_id')` - what geberic route id was responsible for the query (only set for kind=request)\n`JSONExtractInt(log_comment, 'user_id')` - user_id\n`JSONExtractInt(log_comment, 'team_id')` - team_id\n`JSONExtractString(log_comment, 'access_method')` - What access method was used? Blank indicates it's normal web traffic (only set for kind=request)\n`JSONExtractString(log_comment, 'http_referer')` - HTTP referer (only set for kind=request)\n`JSONExtractString(log_comment, 'http_user_agent')` - HTTP user agent (only set for kind=request)\n`JSONExtractString(log_comment, 'container_hostname')` - Kubernetes pod where the query was initiated from\n`JSONExtractString(log_comment, 'workload')` - Either `WORKLOAD.ONLINE` or `WORKLOAD.OFFLINE`. This determines which node on the shard we prefer to send the query to\n`JSONExtractString(JSONExtractString(log_comment, 'query_settings'), '<setting_name>')` - Any settings passed along with the query can be found within the `query_settings` object. To get the value for a specific setting, you need to call `JSONExtractX` twice.\n",
    "tag": "posthog"
  },
  {
    "title": "Metrics",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/runbook/services/clickhouse/index.md",
    "content": "\ntitle: ClickHouse\nsidebar: Docs\nshowTitle: true\n\nClickHouse\u00ae is an open-source, high performance columnar OLAP database management system for real-time analytics using SQL.\nWe use it to store information like:\n- event\n- person\n- person distinct id / session\nand to power all our analytics queries.\nThis is a guide for how to operate ClickHouse with respect to our stack.\nMetrics\nAs with any database it is important to keep an eye on metrics to make sure everything is in ship shape. Most of these metrics shouldn't be a surprise.\nThe metrics you should keep an eye on with ClickHouse are:\n- Latency of Events from ingestion -> visibility in ClickHouse <- This is an indication of ClickHouse falling behind.\n- Disk IOPS\n- Disk throughput (read and write)\n- CPU I/O wait (iowait) time\n- CPU boundedness (high CPU utilization)\n- MergeTree Parts Count (by table)\n- Replication lag (by table and if enabled)\nCollecting metrics\nThere are a few handy ways of collecting and accessing metrics about ClickHouse.\n\nStraight from ClickHouse via system tables\nClickHouse exposes a Prometheus endpoint for scraping metrics by Prometheus.\nYou can configure ClickHouse to export metrics to your Graphite server\nA few other direct and indirect methods documented in the official docs\n\nIf you use DataDog you can even use their examples to collect and report on metrics found here.\nSematext has a great post on important metrics for ClickHouse and how to collect them on their blog. It's a great read and quite detailed.\nCollecting, reporting, and monitoring metrics is similar for all installations, but unique to every organization depending on the tools you are using elsewhere within your organization. We will be firming up on our own opinions in the future, but for now it may make the most sense to integrate these metrics into systems that you are using for monitoring other production workloads.\nZookeeper\nWe have shipped zookeeper with our stack in preparation of sharding, but we don't broadly support sharding or replication on all installations yet. We will update this section soon.\nWhat is Zookeeper used for with regard to ClickHouse?\n- Replication between nodes for `ReplicatingMergeTree` family of tables\nZookeeper is currently the source of truth for what instance has what parts and what parts exist. For replicating ClickHouse is a master-master system and the instances do their best with Zookeeper's help finding out what data is available and where to go to get it within the cluster.\nClickHouse Operating Tips\nClickHouse has a great list of tips for operating a ClickHouse setup and troubleshooting issues.\nOperating Tips Summary\n\nUse `performance` CPU Scaling Governor\nUse a ton of RAM for your instance. ClickHouse loves to use the page cache to speed up queries and prevent hitting disk. If you can fit your entire dataset in memory, get a node that has that much in RAM. You will get the best performance.\nDo not disable overcommit. The value `cat /proc/sys/vm/overcommit_memory` should be 0 or 1.\nIf you can, use SSD over spinning disk. If you use spinning disk use 7200RPM disks.\nSoftware RAID on linux is preferred. Specifically use RAID 10 in `far` layout\nCalculate the exact number from the number of devices and the block size, using the formula: `2 * num_devices * chunk_size_in_bytes / 4096`.\nUse `ext4`\nIf you are using IPv6, increase the size of the route cache.\n\nFailure modes\nClickhouse has a few failure modes depending on configuration, utilization, disk performance, and whether replication and sharding is used (which brings Zookeeper into the picture)\nDisk Throughput\nThe primary failure mode most should be aware of is around ClickHouse instance resources. ClickHouse is very resources intensive on disk, especially during heavy writes such as backfills or large migrations. What generally will happen is ClickHouse will saturate throughput to disks as the MergeTree backed tables are constantly being merged and resorted. (A correctly sorted table's parts is critical to ClickHouse's ability to query quickly).\nWhen this happens you will notice a few things:\n - That queries will take longer and longer to execute.\n - The count of Parts per table will increase slowly and then quite quickly.\n - As the Parts increase the queries will become even slower.\n - CPU IO Wait will saturate the CPU.\n - The latency between an event getting ingestion and it being visible in PostHog will grow.\nAn easy fix for this is to dramatically increase memory for the instance so that page cache can be hit more frequently reducing ClickHouse's dependency on Disk for queries and to increase the throughput and iops available for ClickHouse's data volume if you can.\nCPU Saturation\nAnother Failure mode is CPU saturation. Generally this will either be a result of IO Wait (see above disk saturation) or simply because of under provisioned resources on the CPU side for the query volume the instance is handling. We are constantly working to improve our query performance to reduce load on ClickHouse so the two easiest ways to mitigate this failure mode is to upgrade PostHog and to simply increase the number of CPUs that are available to ClickHouse. If you are operating on Kubernetes you will need to increase the size of the nodes in the node group that is running the ClickHouse pod(s) and make sure that the affinity of the ClickHouse Operator keeps the pod(s) pinned to that node group.\nOut of Disk Capacity\nRunning out of Disk is another common failure case for ClickHouse. What you will typically notice is ClickHouse restarting repeatedly. The easiest way to mitigate this is to simply increase the volume size that ClickHouse is using for data. This parameter can be found in the `values.yaml` file in our chart if you are using our helm chart for your deployment. Otherwise adjust the volume size however it makes sense for your infrastructure.\nMigrations\nOn this note though, it is important that your instance never exceeds 50% capacity of disk utilization. This is because we routinely include large data migrations in our updates to PostHog as we include new features and iterate how we store data on disk within ClickHouse. Frequently we adapt new features within ClickHouse that require a migration of this type. If you do not plan on upgrading PostHog (not suggested) you can use more than 50% of disk capacity on ClickHouse, but you will not be able to benefit from our future improvement.\nMaintenance Tasks\nThere are no maintenance tasks that you should be planning for with ClickHouse outside of the Migrations mentioned above. Luckily those are all managed without PostHog and require very little input from you, the operator. Other than that, there are no Vacuums or Vacuum fulls that you need to run as if you were operating a Postgres instance. If there is any maintenance that needs to be done it will come as an 'Async Migration' or as a command that we will reach out to you to run. You should still do basic operations like backing up your instance and testing those backups in a restore (see backup and restore sections in this runbook).\nScaling up\nCheck out the configuration section of our self-deploy docs!\nDictionary\n\nsharding: is a method for splitting data up across multiple ClickHouse instances 'horizontally'\nshard: holds a subset of the dataset that is stored on a single ClickHouse instance\nreplica: is a node that is responsible for a part of a shard. If you have 3 replicas of a shard that 'shard' of data is replicated across 3 ClickHouse instance 'replicas'\ndistributed table: is a wrapper around a multiple 'shards' of data hosted on multiple ClickHouse instances. It is a virtual table the represents the data of multiple instances.\n\nUseful links",
    "tag": "posthog"
  },
  {
    "title": "backup.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/runbook/services/clickhouse/backup.md",
    "content": "\ntitle: Backup\nsidebar: Docs\nshowTitle: true\n\nA critical requirement in order to generate a consistent backup is to \"freeze\" the dataset while the operation is in process. As with all database systems, consistent backups depend on the system to be in a \"quiesced\" state.\nClickHouse offers native \"table freeze\" support for backup and migration operations without having to halt the database entirely. This is a no-downtime operation.\nSpecifically to PostHog, we should back up all the tables using the MergeTree family type engine. To identify which tables you should back up, run the following command:\n`shell\nclickhouse-client --query \"SELECT name FROM system.tables WHERE database = 'posthog' AND engine LIKE '%MergeTree'\"`\nAs of today, you should get an list of tables like:\n`cohortpeople\nevents\nevents_dead_letter_queue\ninfi_clickhouse_orm_migrations\nperson\nperson_distinct_id\nperson_distinct_id2\nperson_distinct_id_backup\nperson_static_cohort\nplugin_log_entries\nsession_recording_events`\nAutomated using `clickhouse-backup`\nThe clickhouse-backup tool helps you to automate the manual steps above. It also offers native support for multiple backends and object stores like: local storage, FTP, SFTP, Azure Blob Storage, AWS S3, Google Cloud Storage, ...\nOnce configured, the tool provides a variety of sub-commands for managing backups. Creating a backup is as easy as running: `clickhouse-backup create`.\nFor more information please look at the official documentation.\nManual\nTo perform a manual backup, we will ask ClickHouse to freeze our tables, creating hard links to the table data. Hard links are placed in the directory `/var/lib/clickhouse/shadow/N/` where `N` is the incremental number of the backup. The query creates the backup almost instantly, but first it will wait for the current queries to the corresponding table to finish running. After we created the backup, we can copy the data from `/var/lib/clickhouse/shadow/` to a remote server or object store service and delete it from the local server.\nIn this specific example, we will backup the table `events`.\n\n\nVerify if the ClickHouse `shadow` directory is empty or does not exist (yet):\n `shell\n ls -lah /var/lib/clickhouse/shadow/`\n\n\nAsk ClickHouse to freeze the table `events` and backup its dataset:\n `shell\n clickhouse-client --database \"posthog\" --query \"ALTER TABLE events FREEZE\"`\n\n\nThe `ALTER TABLE events FREEZE` query copies only the table data but not the table metadata. If you want to back that up, you can run:\n `shell\n clickhouse-client --database \"posthog\" --query=\"SHOW CREATE TABLE events\" --format=\"TabSeparatedRaw\" | tee $TARGET_BACKUP_LOCATION/events.sql`\n\n\nWe can now copy our table data and metadata to a remote server or object store service. Example:\n `shell\n cp -r /var/lib/clickhouse/shadow/ $TARGET_BACKUP_LOCATION`\n\n\nFinally, let\u2019s clear the content of the `shadow` directory:\n `shell\n rm -rf /var/lib/clickhouse/shadow/*`\n\n\n\nNote: ClickHouse uses filesystem hard links to achieve instantaneous backups with no downtime (or locking). We can further leverage these hard links for efficient backup storage. On filesystems that support hard links (such as local filesystems or NFS), use `cp` with the `-l` flag (or rsync with the `\u2013hard-links` and `\u2013numeric-ids` flags) to avoid copying data.\nWhen hard links are used, storage on disk is much more efficient. Because they rely on hard links, each backup is effectively a \"full\" backup, even though we avoided the duplicate use of disk space.\n",
    "tag": "posthog"
  },
  {
    "title": "resize-disk.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/runbook/services/clickhouse/resize-disk.md",
    "content": "\ntitle: Resize disk\nsidebar: Docs\nshowTitle: true\n\nimport Sunset from \"../../../self-host/_snippets/sunset-disclaimer.mdx\"\n\nimport ResizeDiskRequirementsSnippet from '../../snippets/resize-disk-requirements.mdx'\n\nHow-to\n\n\nConnect to the Clickhouse container to verify the data directory filesystem size (in this example 20G)\n`shell\nkubectl -n posthog exec -it chi-posthog-posthog-0-0-0 -- /bin/bash\nclickhouse@chi-posthog-posthog-0-0-0:/$ df -h /var/lib/clickhouse/\nFilesystem                                                                Size  Used Avail Use% Mounted on\n/dev/disk/by-id/scsi-0DO_Volume_pvc-f39035c1-c68c-4572-81f2-273de6eb088c   20G   50M   19G   1% /var/lib/clickhouse`\n\n\nIn your Helm chart configuration, update the `clickhouse.persistence.size` value in `value.yaml` to the target size (40G in this example)\n\n\nRun a `helm` upgrade\n\n\nConnect to the ClickHouse container to verify the new filesystem size\n    `shell\n    kubectl -n posthog exec -it chi-posthog-posthog-0-0-0 -- /bin/bash\n    clickhouse@chi-posthog-posthog-0-0-0:/$ df -h /var/lib/clickhouse/\n    Filesystem                                                                Size  Used Avail Use% Mounted on\n    /dev/disk/by-id/scsi-0DO_Volume_pvc-f39035c1-c68c-4572-81f2-273de6eb088c   40G  186M   38G   1% /var/lib/clickhouse`\n\n\nTroubleshooting\nIf the resize didn't work check for errors in the ClickHouse operator pod.\nThis procedure doesn't work to decrease a volume. If you try, the disk won't be resized and the following errors can be seen in the ClickHouse operator pod\n`posthog/posthog:ERROR unable to reconcile PVC(posthog/data-volumeclaim-template-chi-posthog-posthog-0-0-0) err: PersistentVolumeClaim \"data-volumeclaim-template-chi-posthog-posthog-0-0-0\" is in valid: spec.resources.requests.storage: Forbidden: field can not be less than previous value`\nFor increases double check what the storage class is that's used for ClickHouse and make sure it's expandable as mentioned in requirements above.\n`shell\nkubectl get pvc data-volumeclaim-template-chi-posthog-posthog-0-0-0 -o json | jq '.spec.storageClassName'`",
    "tag": "posthog"
  },
  {
    "title": "How to run:",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/runbook/services/postgresql/long-migrations.mdx",
    "content": "\ntitle: Debugging long-running migrations\nsidebar: Docs\nshowTitle: true\n\nWhen trying to upgrade a self-hosted Postgres-backed PostHog instance, we have seen some users have issues with long-running migrations on tables in the hot path.\nFor example, when trying to migrate the persons table, some users have reported that a lock is never acquired, making the migration hang.\nThis usually happens because some analytics queries are taking too long to complete.\nTo get rid of these queries and run the migration, you can use the script below on the node where the PostHog Django server (web) is running.\n\n\u26a0\ufe0f Warning! Proceed with caution. The following script has potentially destructive behavior.\n\n\nPython script\n\n\n```python\n# How to run:\n# Add this code to a delete_queries.py file\n# Run python manage.py shell < delete_queries.py\n# Set DRY_RUN=0 if you're sure you know what you're doing\n\nimport os\nfrom time import sleep\n\nfrom django.db import connection\n\nQUERY_TIME_LIMIT = os.environ.get(\"QUERY_TIME_LIMIT\", \"5 minutes\")\nTARGET_TABLE = os.environ.get(\"TARGET_TABLE\", \"posthog_person\")\nDRY_RUN = os.environ.get(\"DRY_RUN\", \"1\") == \"1\"\n\nMAX_TRIES = 10\n\n\ndef fetch_open_queries():\n    res = []\n    with connection.cursor() as cursor:\n        cursor.execute(\n            f\"\"\"\n            SELECT\n                pid,\n                now() - pg_stat_activity.query_start AS duration,\n                query\n            FROM pg_stat_activity\n            WHERE (now() - pg_stat_activity.query_start) > interval '{QUERY_TIME_LIMIT}'\n            AND state = 'active'\n            \"\"\"\n        )\n        res = cursor.fetchall()\n    return res\n\n\nopen_queries = fetch_open_queries()\ntries = 0\n\nwhile open_queries and len(open_queries) > 0 and tries < MAX_TRIES:\n    for query_details in open_queries:\n        if TARGET_TABLE in query_details[2]:\n            print(f\"Preparing to kill query {query_details[0]}:\")\n            print(query_details[2])\n            print(f\"It has been active for {query_details[1].seconds}s\")\n            if not DRY_RUN:\n                with connection.cursor() as cursor:\n                    cursor.execute(f\"SELECT pg_terminate_backend({query_details[0]})\")\n                print(f\"Killed query {query_details[0]}\")\n            print(\"\")\n    open_queries = fetch_open_queries()\n    tries += 1\n    sleep(1)\n```\n\n\n\nI'd rather run the SQL myself\n\n\nInstead of the script, you can also look for long running queries in Postgres yourself, like so:\n\n```sql\nSELECT\n    pid,\n    now() - pg_stat_activity.query_start AS duration,\n    query\nFROM pg_stat_activity\nWHERE (now() - pg_stat_activity.query_start) > interval '5 minutes'\nAND state = 'active'\n```\n\nUsing the PIDs from the query above, you can then kill them like so:\n\n```sql\nSELECT pg_terminate_backend(pid)\n```",
    "tag": "posthog"
  },
  {
    "title": "resize-disk.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/runbook/services/postgresql/resize-disk.md",
    "content": "\ntitle: Resize disk\nsidebar: Docs\nshowTitle: true\n\nimport Sunset from \"../../../self-host/_snippets/sunset-disclaimer.mdx\"\n\nimport ResizeDiskRequirementsSnippet from '../../snippets/resize-disk-requirements'\n\nHow-to\n\n\nConnect to the Postgresql container to verify the data directory filesystem size (in this example 10GB)\n`shell\nkubectl -n posthog exec -it posthog-posthog-postgresql-0 -- /bin/bash\nI have no name!@posthog-posthog-postgresql-0:/$ df -h /bitnami/postgresql\nFilesystem                                                                Size  Used Avail Use% Mounted on\n/dev/disk/by-id/scsi-0DO_Volume_pvc-966716a8-cac6-407a-afb4-8cab52b0ad9b  9.8G  145M  9.2G   2% /bitnami/postgresql`\n\n\nResize the underlying PVC (in this example we are resizing it to 20G)\n`shell\nkubectl -n posthog patch pvc data-posthog-posthog-postgresql-0 -p '{ \"spec\": { \"resources\": { \"requests\": { \"storage\": \"20Gi\" }}}}'\npersistentvolumeclaim/data-posthog-posthog-postgresql-0 patched`\nNote: while resizing the PVC you might get an error `disk resize is only supported on Unattached disk, current disk state: Attached` (see below for more details).\n\nIn this specific case you need to temporary scale down the `StatefulSet` replica value to zero. This will briefly disrupt the Postgresql service availability and make the PostHog UI inaccessible. On newer versions of PostHog events will be queued and ingestion won't be impacted\nYou can do that by running: `kubectl -n posthog patch statefulset posthog-posthog-postgresql -p '{ \"spec\": { \"replicas\": 0 }}'`\nAfter you successfully resized the PVC, you can restore the initial replica definition with: `kubectl -n posthog patch statefulset posthog-posthog-postgresql -p '{ \"spec\": { \"replicas\": 1 }}'`\n\n\n\nDelete the `StatefulSet` definition but leave its `pod`s online (this is to avoid an impact to using PostHog): `kubectl -n posthog delete sts --cascade=orphan posthog-posthog-postgresql`\n\n\nIn your Helm chart configuration, update the `postgresql.persistence` value in `value.yaml` to the target size (20G in this example)\n\n\nRun a `helm` upgrade to recycle all the pods and re-deploy the `StatefulSet` definition\n\n\nConnect to the Postgresql container to verify the new filesystem size\n    ```shell\n    kubectl -n posthog exec -it posthog-posthog-postgresql-0 -- /bin/bash\n    I have no name!@posthog-posthog-postgresql-0:/$ df -h /bitnami/postgresql\n    Filesystem                                                                Size  Used Avail Use% Mounted on\n    /dev/disk/by-id/scsi-0DO_Volume_pvc-966716a8-cac6-407a-afb4-8cab52b0ad9b  20G   153M   19G   1% /bitnami/postgresql\n\n",
    "tag": "posthog"
  },
  {
    "title": "What does the plugin server do?",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/runbook/services/plugin-server/index.mdx",
    "content": "\ntitle: Making sense of the plugin server\nsidebar: Docs\nshowTitle: true\n\nimport EventFlowDiagram from './diagrams/event-flow'\nimport PluginServerDiagram from './diagrams/plugin-server'\nimport IngestionServerDiagram from './diagrams/ingestion-server'\nimport AsyncServerDiagram from './diagrams/async-server'\nThe plugin server is the core of event ingestion and the plugins platform at PostHog.\nDespite being called the \"plugin server\", it is a core service that PostHog cannot function without.\nThis doc should give you a sense of what the plugin server is, how it operates, why it works this way, as well as a better sense of how to operate it in production. \nSo buckle up, the journey is about to start.\nWhat does the plugin server do?\nBefore getting into the hows and the whys, it's important to understand what the plugin server is responsible for.\nIn short, its main responsibilities are:\n\nValidating, parsing, and ingesting events\nHandling all updates to the various models events can trigger changes to (e.g. persons, groups)\nRunning all plugins \n\nNote that these are the responsibilities covered by the plugin server codebase, but not necessarily by the same service. More on this later.\nA brief history\nThe plugin server was born out of a hackathon in Italy during the first ever PostHog offsite.\nBack then, the project had one goal: transforming events in flight. \nMuch like CDPs offer \"transformations\", the goal was to be able to provide a way for PostHog users to enrich events with additional data from other sources.\nFrom this hackathon, our most widely used plugin to date was born - GeoIP. \nAn initial step towards the ultimate goal of displaying a \"world map\" in PostHog (solved 2 offsites later in Iceland) - the GeoIP plugin took the IP from an event and used that to enrich the event properties with location data (country, city, etc).\nNow, while this may sound like irrelevant context - it set the stage for many architectural decisions to come. \nThe first implementation of plugins was built directly into our core Django server, but we found the JavaScript ecosystem to be more mature when it came to running arbitrary code in a sandboxed environment.\nAs a result, a new service was born: the plugin server.\nFast forward a bit, and we decided that it seemed overly complex and inefficient to do a round trip while ingesting an event (Django -> Plugin Server -> Django -> Database), so we kept the endpoint for sending events in Django, and moved all the other logic pertaining to ingestion over to the plugin server.\nOver a year passes, and now PostHog plugins are more powerful than ever. They can access storage, trigger jobs, run scheduled tasks, process events in flight, access events after ingestion, and a lot more. \nYet, because of that first use case (processing events in flight), ingestion and all this extra processing are tightly coupled together. This is a poor design from an architectural perspective - plugins, meant to \"extend\" the platform, can cause problems for event ingestion, a core aspect of the PostHog product.\nThe plugin server has various mechanisms in place to prevent things like plugins running for too long, but ultimately the number of plugins that can run is unbounded, and they can trigger a great deal of tasks and jobs that can end up hogging resources meant for ingestion. \nWe could keep on adding checks - but ultimately the best solution here was a fundamental refactor. It was time to split the service into two.\nAnd that's where we are today. \nThe plugin server can now be run in three modes - \"default\", \"ingestion\", and \"async\". \nThe \"default\" mode follows the old architecture - one server doing everything. It might eventually disappear, but for now it greatly simplifies dev setups, as well as the setup for small/medium self-hosted instances.\nThe \"ingestion\" mode handles ingestion-related processing almost exclusively. The one bit of processing it does for plugins is handling the `processEvent` function - the function allowing events to be modified pre-ingestion. However, any additional processing triggered through running that function is not processed by the \"ingestion\" server.\nLastly, the \"async\" server handles everything the \"ingestion\" server doesn't. The three key areas it covers are plugin jobs, scheduled tasks (e.g. `runEveryMinute`), and \"async handlers\" (webhooks, `onEvent`).\nPlugins\nJust before we dig into the architecture, it's important to get a briefing on key plugin concepts as well as plugin capabilities in PostHog.\nWhat can plugins do?\nPlugins can:\n\nProcess events before they're ingested (via `processEvent`)\nAccess events and actions after the event has been ingested* (via `onEvent` and `exportEvents`)\nSchedule periodic tasks (via `runEveryMinute`, `runEveryHour`, and `runEveryDay`)\nTrigger custom jobs to be run at any given point in the future (via the jobs API)\nAccess APIs for key-value storage (using Postgres) and cache (using Redis)\nCreate new events (via the `posthog` API inside plugin VMs)\n\nBeyond this, they can also use some Node APIs and third-party libraries. \nTo read about everything plugins can do, take a look at our Developer Reference.\nPlugins vs. plugin configs\nThere are two core models that plugins are built on top of - plugins (stored in the Postgres table `posthog_plugin` ) and plugin configs (stored in the Postgres table `posthog_pluginconfig`).\nPlugins represent plugin \"definitions\" - most importantly they contain the plugin's config schema, and the plugin code, as well as metadata such as name, description, and logo.\nFrom an installed plugin, teams in PostHog can set up their own instance of a plugin, called a \"plugin config\". \nFor instance, a plugin may depend on a configuration option called `eventToExport`. Then, when a team decides to enable that plugin, they will set `eventToExport` to the desired event name that suits their purpose, like `my_special_event`. \nPlugin configs are what we use to setup plugin VMs. We load the plugin's code and inject the config's context.\nFor example, all storage and cache usage will be indexed by config ID, and a given VM will only receive events and actions for the team associated with the plugin config.\n\nInitially, for every plugin config, we would setup a plugin VM on every thread on every server.\nSince then, two things have changed:\n\nStateless plugins: Plugins marked as \"stateless\" use the same VM for all plugin configs. In other words, the same VM is used for all teams that have the plugin enabled. This is because they don't rely on any team-specific context. The GeoIP plugin, for example, takes an IP and enriches the event with some location data, without depending on any additional configuration. The same operation is done for all events across all teams, so there's no need to run a VM per config. Read more here.\nCapabilities: A nice feature that came about as a result of plugin server \"modes\" is that we no longer need to run all plugins on all servers. For instance, if a plugin only has scheduled tasks, it will never be used on the ingestion server, so we don't need a VM for it on that server. Read more here.\n\n\nArchitecture\nEvent flow\nHere's a simplified high level overview of how an event flows through the PostHog ingestion pipeline:\nDiagram 1: Event flow\n\nThe event is received over in Django, where some basic validation is done on the payload (does it have a distinct ID, does the project token exist, etc).\nThis event is then put in Kafka to be consumed by the ingestion server. The diagram is meant to only show the flow of events, but at this step we will also create and update any persons and groups based on the event, both on Postgres and ClickHouse.\nThe processed event is then put into another Kafka topic, which two consumer groups consume from. \nOne consumer group is managed by ClickHouse, via its Kafka table engine, which consumes messages from Kafka into rows in the database.\nThe other consumer group corresponds to the async server, which pass the event to plugins running functions like `onEvent`.\nIf your server is running in \"default\" mode, this step is skipped, and instead async handlers are processed after the `createEvent` step on the same server.\n\nNote that high load production deployments run many instances of the Django server, the ingestion server, and the async server. This is handled correctly by all services, and covered later.\n\nPlugin server\nNow that we understand how events flow through the system, let's go even deeper into how the plugin server works.\nDiagram 2: Plugin server\n\nMain thread\nStarting with the main thread, the first thing to note is that not all \"services\" run with every mode. \nServices in gray are \"optional\" from an individual server's perspective, but at least one server in the fleet should run them in order to cover all plugin capabilities.\nAll the services listed here run concurrently. \nKafka Consumer\nThe Kafka consumer consumes from a specified Kafka topic and triggers a run of the event pipeline in the worker thread. The pipeline may be run partially or in full, and all that it does will depend on the mode the server is running in.\nPubSub\nThe PubSub service connects to Redis Pub/Sub channel and is used to trigger plugin reloads (e.g. if the code or config has changed) as well as action reloads (e.g. if an action definition has changed).\nGraphile Queue\nWe use the Graphile Worker for our plugin jobs implementation. This queue is a consumer that will periodically check the jobs listed in Postgres and trigger job runs when the jobs target time is reached.\nSchedule\nA node-schedule scheduler that will trigger scheduled plugin tasks (`runEveryMinute`, `runEveryDay`, etc). \nHowever, tasks are not necessarily triggered in a given server when the scheduled time hits. We use Redlock to ensure that only one plugin server across the fleet processes scheduled tasks at a given point in time.\nMMDB\nThe MMDB service uses a MaxMind database we download from mmdb.posthog.net (managed by this service) and keep in memory in the main thread to allow for geolocation data lookups by IP.\nThe worker thread communicates with the main thread via TCP to fetch the necessary data.\nHTTP Server\nA simple HTTP server that's currently only used for healthchecks.\nPiscina\nWe use the Piscina thread pool to dispatch tasks from the main thread to the worker threads in the plugin server.\nBy default, Piscina will spawn as many worker threads as the machine's CPU cores, and each worker can process 10 tasks concurrently by default. \nWhen a task is triggered, Piscina will select an available worker to process it. We've also forked Piscina and added a feature that allows us to run a task on all workers. We use this when we need to reload a plugin that has changed, for instance.\nWorker thread\nMost often, a task triggered from the main thread will either immediately trigger a plugin function, or run the event pipeline.\nThe former happens in the case of \"standalone\" functions, like scheduled tasks and jobs. The task payload contains everything the VM needs to execute the function, so we call it immediately.\nOn the other hand, the event pipeline handles the entire lifecycle of an event. The steps currently in the pipeline are the following:\n\n`pluginsProcessEventStep`\n`prepareEventStep`\n`emitToBufferStep`\n`createEventStep`\n`runAsyncHandlersStep`\n\nAt certain steps, the pipeline may trigger plugin functions in the VMs, such as `processEvent` at step 1 and `onEvent` at step 5.\nMost importantly, the pipeline need not be run in full each time. When both the \"ingestion\" and \"async\" servers are running, the ingestion server covers steps 1-4 and the async server covers step 5.\nWhen running the event pipeline, we also update persons and groups in Postgres and ClickHouse (via Kafka).\nFinally, the diagram also shows everything plugin VMs can connect to, directly or indirectly. For instance, they can create events that get put into Kafka, as well as trigger jobs that go to Postgres via the Graphile Producer.\nIngestion server\nDiagram 3: Ingestion server\n\nAsync server\nDiagram 4: Async server\n\nPlugin server management\nFrom all this context and all these diagrams, we can derive useful lessons about managing this stack in production.\nHorizontal scaling\nIn order to cope with increased load, one can scale the plugin server horizontally. \nHowever, the marginal benefit of horizontal scaling will largely depend on the number of partitions for the `clickhouse_events_json` and `events_plugin_ingestion` topics.\nAny additional ingestion server instance beyond the number of partitions for `clickhouse_events_json` will do absolutely nothing.\nAdditional async server instances beyond the number of partitions for `events_plugin_ingestion` will be useful, but their benefit will be limited. They will not process async handlers, but will still be able to process both jobs and scheduled tasks.\nQuirks\n\nIn order to work correctly, you must either have a fleet composed of both async servers and ingestion servers, or all servers should be running in \"default\" mode. Currently all non-cloud installations use \"default\" mode, but we will provide options for separate scaling in the chart soon.\nWe run a fork of Piscina with two key changes: a feature called `broadcastTask`, and a change to its internal use of the Atomics API\nBoth our Cloud environment and self-hosted installations need to connect to a microservice we manage if they wish to use the GeoIP plugin. GeoIP capabilities are built into the plugin server.\nPlugin VMs are not fully isolated from the environment they work on. Thus, one should always verify a plugin's source code carefully before installing it!\n\nFailure modes\n\nNote that we often mention how the plugin server can pick back up and ingest events that have been sitting in Kafka because some service is down. However, messages will be deleted from Kafka according to the topic's retention policy, so during long outages we should backup messages from the relevant Kafka topics. \n\n______ is down\nClickHouse is down\nThis is fine. Historical exports will stop working but that's all. We will also not see any new events in the app, but these will be safe in Kafka waiting for ClickHouse to come back up.\nRedis is down\nBoth plugins and the plugin server itself use Redis for caching. However, the plugin server does not, and plugins should not need Redis to operate.\nThus, Redis being down is also fine. Actions and plugins may get out of date, but these can also be updated by triggering a server restart.\nIngestion server is down\nThis is pretty self-explanatory. If the ingestion server goes down, PostHog event ingestion comes to a halt. \nHowever, if Django is working fine, events should be waiting in Kafka to be processed when the server comes back up.\nFrom this, we can also derive that it is fine to restart the ingestion server if necessary. Consequences should be minimal.\nAsync server is down\nIf the async server is down, ingestion will continue to work, but scheduled tasks, plugin jobs, and async handlers will stop working. \nJobs are kept in Postgres, so these are safe and will pick back up when the server starts.\nAsync handlers run on events from Kafka, so those will also \"pick up from where they left off\".\nScheduled tasks meant to trigger during the time the server is down will of course not run again, but they run on intervals, so the impact here is minimal as they will run on the next iteration if the server is back up.\nPostgres is down\nIf Postgres is down, PostHog will have a difficult time doing anything, and the same applies to both the async and ingestion servers.\nDuring this time, Django will send events to the dead letter queue, so the plugin server will not process events sent during this time, even when Postgres is back up.\nKafka is down\nA key takeaway from all this is that Kafka is an essential piece of our infrastructure.",
    "tag": "posthog"
  },
  {
    "title": "Background",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/runbook/services/plugin-server/ingestion-lag.mdx",
    "content": "\ntitle: Ingestion lag\nsidebar: Docs\nshowTitle: true\n\n\nImportant: Please ensure you have access to production so that you are able to handle incidents!\n\nBackground\nTeam Pipeline has committed to processing \"standard\" events end-to-end within 30 seconds (p95) and \"buffered\" events within 1m30s (p95). \nIt's important for customers that their data is queryable soon after being sent so they can debug their implementation and get the feeling that PostHog is working correctly.\nTo calculate how we're doing with respect to our commitment, we have a Celery task that every 60 seconds calculates lag based on an event that we emit every minute from a plugin. \nAs a result of this, our metric includes 60 second error bars, meaning we actually target 1m30s for \"standard\" events and 2m30s for \"buffered\" events.\nAlso, beyond measuring how fast we are, the ingestion lag metric being high might also be an indication that we're not processing events at all, so it's important that we treat it seriously.\nDebugging\nThere are many things that can lead to ingestion lag being high, so it's very important to rule out parts of the system completely in order to determine the root cause.\nThe first thing to establish is if we're processing events with a delay or not processing events at all. A third option to consider later on is if we're processing some events and dropping others.\nYou can do this in many ways:\n\nLook at the live events tab\nSend an event and see if it appears on live events\n\nCheck ClickHouse for the latest event `_timestamp`, and ingestion rate per minute\n`sql\nSELECT\n    toStartOfMinute(timestamp) ts,\n    count(*)\nFROM events\nWHERE timestamp > now() - INTERVAL 2 HOUR AND timestamp < now()\nGROUP BY ts\nORDER BY ts`\n\nPlease note that `timestamp` reflects the time the event happened, not when the event was ingested. Thus it is possible that this metric is off if users are sending us events in the past or the future. However, it generally is a great representation of things. You can use `_timestamp` to get ingestion time timestamps, but note that it is a column that's not in our sorting key and thus filtering on it leads to expensive (and thus slow) queries.\n\n\n\nIf we're not processing events, then things are very serious. However, even then there are levels to the issue.\nThe worst case scenario is if events are being dropped by the capture endpoint (`posthog/api/capture.py`). This is because these events are forever lost.\nIt's a smaller but still serious issue if the plugin server is unable to process events or ClickHouse unable to consume them. These events will remain in Kafka until the messages older than the specified retention for the topic.\nTo determine this, there are a few useful resources:\n\nGraph: Kafka Production Rate (US Cloud link)\nGraph: Events received per minute (US Cloud link)\nGraph: HTTP Response Codes (US Cloud link)\nSentry errors for `/e`, `/batch` etc endpoints (US Cloud link)\nPlugin server logs (US Cloud link)\n\nActions\nRestarting the ingestion pods\nA reasonably safe operation to perform is restarting the plugin server ingestion pods. This doesn't cause event loss and can be done by triggering a deploy or running a command like the following:\n`shell\nkubectl rollout restart deployment/plugins-ingestion -n posthog`\nIt's ok to do this when you're not fully sure what's wrong with the ingestion pipeline.\nScaling the ingestion pods\nIf you suspect that we're struggling due to high load, then you can scale vertically or horizontally.\nTo scale vertically (i.e. give the pods more resources), you should update the requests and limits for CPU and memory on the relevant section of the posthog-cloud-infra repo.\nTo scale horizontally (i.e. add more pods), you should update the min and max `hpa` values on the relevant section of the posthog-cloud-infra repo.\nDealing with hot partitions\nIf you see from the Kafka consumer lag graph (US Cloud link) that our lag seems to stem from only one or a few partitions, that's likely because we have a hot partition. \nWe partition the `events_plugin_ingestion` topic by `<team_id>:<distinct_id>`, meaning we're likely getting a burst of events for a given person in a short period of time, causing a hot partition. \nThere are a few possible mitigation steps for this:\n\nWait. If the burst is temporary we should recover in time.\nDiscover the problematic IDs and force events being sent to them to be randomly partitioned.\n\nIn order to do #2, you should first try to discover what IDs could be causing problems. Here's a useful query to run on ClickHouse:\n`sql\nSELECT\n    team_id,\n    distinct_id,\n    count(*) AS c\nFROM events\nWHERE \n    timestamp > now() - INTERVAL 1 HOUR AND\n    timestamp < now()\nGROUP BY\n    team_id,\n    distinct_id\nORDER BY c DESC\nLIMIT 20`\nIf an ID stands out, you can do some further digging with:\n`sql\nSELECT \n    toStartOfMinute(timestamp) ts,\n    count(*)\nFROM events \nWHERE \n    team_id=<team_id> AND \n    distinct_id=<distinct_id> AND \n    timestamp > now() - INTERVAL 1 DAY AND\n    timestamp < now() \nGROUP BY ts`\nRunning the above query on Metabase gives you a very useful visualization, where you can see if events for this ID have increased significantly recently or if the volume is always high.\nOften times this graph will give you a clear indication of if an ID caused the hot partition or not.\nIf you've established that there's likely an ID causing a hot partition, it's worth peeking into the properties for the person it's associated to:\n`sql\nWITH p AS ( \n    SELECT \n        id, \n        properties \n    FROM person\n    WHERE \n        team_id=<team_id>\n),\npdi AS (\n    SELECT person_id \n    FROM person_distinct_id2\n    WHERE\n        team_id=<team_id> AND \n        distinct_id=<distinct_id>\n)\nSELECT p.properties \nFROM p JOIN pdi\nON p.id = pdi.person_id`\nIf it looks like the properties are all / mostly generic (e.g. initial referrers, OS, properties PostHog sets by default) or the hot partition problem is very serious (a lot of lag), you can then copy the approach from this PR to force the events for the ID to be randomly partitioned.\nWe check the person properties because if a lot of custom properties are being used random partitioning may cause properties to get inconsistent values.\nDropping and recreating ClickHouse \"ingestion tables\"\nIf you've established that ClickHouse is not ingesting data correctly (via Kafka consumer lag graphs, ClickHouse logs, etc), then it could be worth recreating the relevant Kafka table + materialized view pairs that are responsible for ingesting data.\nIn order to do this, first get the schema for each table:\n`sql\n-- save the results somewhere\nSHOW CREATE TABLE kafka_events_json;\nSHOW CREATE TABLE events_json_mv;`\nThen you should first drop the materialized view (remove the comment to run on all nodes):\n`sql\nDROP VIEW events_json_mv -- ON CLUSTER 'posthog'`\nAnd later the Kafka table:\n`sql\nDROP TABLE kafka_events_json -- ON CLUSTER 'posthog'`\nAnd recreate them in reverse order using the schemas you copied earlier. i.e. Kafka table first, then materialized view. Make sure to add the `ON CLUSTER` clause if you dropped the tables on the whole cluster.\nRestarting the ClickHouse server on a node\n\nImportant: You should never do this if you're not comfortable doing so from experience operating ClickHouse! Please loop in someone to do this in case you feel like ClickHouse has gotten into a bad state (from looking at logs, metrics, etc).\n\nIn extreme situations a ClickHouse node may have gotten into a bad state. In that case, restarting the ClickHouse server might help. To do this, you should SSH into the node and run:\n`shell\nsudo systemctl restart clickhouse-server`\nQuick reference\nA list of relevant resources / information that is useful for debugging ingestion lag.\nKafka topics\n\n`events_plugin_ingestion`: Events sent from the Django events service to the plugin server. Consumed by a group poorly named `clickhouse-ingestion`.\n`clickhouse_events_json`: Events sent from the plugin server to ClickHouse. Consumed by a group called `group1` (and also by a group called `clickhouse-plugin-server-async` for the purpose of exports, but that's irrelevant here).\n`conversion_events_buffer`: Events that should wait (60s by default) to be processed. Produced to from the plugin server and consumed by the plugin server using a group called `ingester`.\n\nTools\n\nhttps://metrics.posthog.net (Grafana)\nhttps://grafana.us-east-1.prod.posthog.dev (Grafana, requires VPN)\nhttps://grafana.eu-central-1.prod.posthog.dev (Grafana, requires VPN access)\nhttps://gregor.posthog.net (Kafka manager)\nhttps://metabase.posthog.net (Metabase)\nSentry\nAWS console / CLI\n`kubectl` \n",
    "tag": "posthog"
  },
  {
    "title": "Background",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/runbook/services/plugin-server/scheduled-tasks.mdx",
    "content": "\ntitle: Scheduled tasks not executing\nsidebar: Docs\nshowTitle: true\n\n\nImportant: Please ensure you have access to production so that you are able to handle incidents!\n\nBackground\nThe term \"scheduled tasks\" refers to the plugin functions `runEveryMinute`, `runEveryHour`, and `runEveryDay`. \nThese functions are used for a variety of use cases that are essential to the plugins that use them. For instance, the Snowflake Export plugin uses a scheduled task to insert batches of data from the external stage into Snowflake. Another very common use case for them is data importing, where plugins hit an external service to pull some data based on the schedule.\nWe use Graphile Worker (the same tool that's used for jobs) to run scheduled tasks. It runs a \"cron\" service at runtime, but keeps track of tasks in the database. When it's time for a task to execute, Graphile Worker looks up and updates the `graphile_worker.known_crontabs` table and then enqueues a job to execute the task.\nCurrently, our approach to scheduled tasks is not optimal, as we have one overarching task per cadence that executes all the tasks from plugins that use the function i.e. one task that runs every minute and executes in sequence all the `runEveryMinute` functions from enables plugins. Ideally, we'd have one task per `runEveryX:pluginConfigId`, so that these could be better distributed across the fleet, but a past attempt to do so put way too much load in the database. This is something we should revisit. It's worth being aware of this as a potential cause for issues is a given server being overloaded. \nIt's important to note that it's not very important that a given plugin's `runEveryMinute` function runs every minute (despite the name). Given another task is bound to run soon enough, it's ok for these to be delayed by a couple of minutes. This is why at the moment you'll see fluctuations in the number of tasks we actually run per minute if you look at the metrics. It is more important that `runEveryHour` and `runEveryDay` don't actually skip slots.\nScheduled tasks only run in plugin servers with the `scheduledTasks` capability.\nDebugging\nIf scheduled tasks aren't working, there are a few places to look in for issues:\n\nThe Graphile Worker database: The database could be down or under a lot of load. A solution may be to scale it up.\nThe `plugins-scheduler` pods: The pods could be under a lot of load and should be scaled up, we may have shipped a bug affecting scheduled task execution, or we may have misconfigured the Graphile Worker.\nIndividual plugins: Ideally an individual plugin with a bug would never affect other plugins, but it's possible we may not have covered all ways in which plugins can interfere. One example is that since we currently run tasks in sequence, a few tasks reaching the 30s timeout would cause delays for the subsequent tasks\n\nActions\nRestarting scheduler pods\nWith scheduled taks in particular, it's unlikely that a restart would fix things, but it's a safe operation that might be worth trying.\n`shell\nkubectl rollout restart deployment/plugins-scheduler -n posthog`\nScaling up the Graphile Worker database\nIf you've determined that the Graphile Worker database is the bottleneck, you should be able to safely scale it up by changing the instance type in the posthog-cloud-infra repo.\nDisabling a plugin\nIf you need to disable a given plugin config after you've established it's problematic, you can do so by:\na) Logging in as the user and disabling it\nb) Updating the `posthog_pluginconfig` table and setting `enabled=false` for the appropriate column",
    "tag": "posthog"
  },
  {
    "title": "Background",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/runbook/services/plugin-server/jobs.mdx",
    "content": "\ntitle: Jobs not executing\nsidebar: Docs\nshowTitle: true\n\n\nImportant: Please ensure you have access to production so that you are able to handle incidents!\n\nBackground\nJobs are an important functionality of PostHog apps / plugins. Among other things, they power much of our exports functionality.\nIn order to debug jobs not working properly, it's important to understand the following:\n\nJobs can be triggered from plugin servers with any capability\nJobs can only be processed from plugin servers with the jobs capability\n\nIn our Cloud environments, plugin server capabilities can be inferred from deployment names. To debug the jobs processing pipeline, you'll be looking at the `plugins-jobs-xxxxx` pods.\nIt's also important to know that in our Cloud environments, jobs are not stored in our main Postgres database. Rather, we store them in a separate RDS instance that is used only for jobs.\nThe jobs pipeline works as follows:\n\nEnqueue job into `jobs` Kafka topic from any plugin server instance\nPlugin server with jobs capability consumes from Kafka and persists the job in the jobs database (via Graphile Worker)\nThe Graphile Worker in a plugin server instance with the jobs capability pulls the jobs from the jobs database when it's time, runs them, and deletes them from the database\n\nDebugging\nThere are a few potential services that can cause our jobs processing pipeline to have issues:\n\nKafka: We may be failing to add jobs to the `jobs` Kafka topic. Potential reasons: Kafka is down, jobs messages have gotten larger than the default Kafka limit of 1mb, we've shipped a bug causing jobs messages to be malformed.\nJobs database: The jobs database may be oversaturated or unreacheable. \nPlugin server: The plugin server could have stopped enqueueing / processing jobs because it is oversaturated or we've shipped a bug.\n\nActions\nThe most straightforward and safe operation to perform is to trigger a \"restart\" of the jobs pods. This can be done with a redeploy or using `kubectl` to spin up an entire new set of pods.\nExample:\n`shell\nkubectl rollout restart deployment/plugins-jobs -n posthog`\nIf this doesn't fix the issue, you should try to establish the health of both Kafka and the jobs database. Provided they look healthy, we've likely shipped a bug and should look at Git history and revert any suspicious changes.",
    "tag": "posthog"
  },
  {
    "title": "plugin-server.mdx",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/runbook/services/plugin-server/diagrams/plugin-server.mdx",
    "content": "```mermaid\nflowchart LR\n    subgraph Wrapper [ ]\n        direction LR\n\n\n```    Redis[(Redis)]\n    ClickHouse[(ClickHouse)]\n    Postgres[(Postgres)]\n\n    Kafka{Kafka}\n\n    subgraph PluginServer [ ]\n\n        direction LR\n\n        KafkaConsumer(Kafka Consumer)\n        Graphile(Graphile queue)\n        HttpServer(HTTP Server)\n        PubSub(PubSub)\n        Piscina(Piscina)\n        Schedule(Schedule)\n        WorkerTasks(Worker tasks)\n        VM1{VM}\n        VM2{VM}\n\n        EventPipeline(eventPipeline)\n        GraphileProducer(Graphile producer)\n        KafkaProducer(Kafka producer)\n        MMDB(MMDB)\n\n        subgraph MainThread [Main thread]\n            KafkaConsumer\n            Graphile\n            Schedule\n            PubSub\n            HttpServer\n            MMDB\n        end\n\n        subgraph WorkerThread [Worker thread]\n            WorkerTasks\n\n            EventPipeline\n            GraphileProducer\n            KafkaProducer\n\n            subgraph VMs [Plugin VMs]\n                VM1\n                VM2\n            end\n\n        end\n\n    end\n\n    KafkaConsumer --> Piscina\n    PubSub --> Piscina\n    Graphile --> Piscina\n    Schedule --> Piscina\n\n    Piscina --> WorkerTasks\n\n    WorkerTasks --> EventPipeline\n\n    EventPipeline <-->|processEvent,onEvent,...| VMs\n    VMs -->|Create event| KafkaProducer\n    EventPipeline -->|create/update<BR>events,persons,groups,...| KafkaProducer\n    EventPipeline <-->|updates to persons,groups,...| Postgres\n\n    WorkerTasks -->|runJob,runEveryMinute,...| VMs\n\n    VMs --> |Enqueue job| GraphileProducer\n    VMs <---> |Cache extension| Redis\n    VMs <--->|Storage extension| Postgres\n    VMs <--->|Historical exports| ClickHouse\n\n    KafkaProducer --> Kafka\n    Kafka -->|Kafka engine| ClickHouse\n    GraphileProducer --> Postgres\n\n\n    MMDB <-->|GeoIP lookups via TCP| VMs\n\n\n    %% Styles\n\n    class MainThread,WorkerThread sgraph;\n    class Graphile,Schedule OptionalServices;\n\n    classDef sgraph fill:#fff,stroke-width:3px,stroke-dasharray:5,stroke:#000;\n    classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;\n    classDef OptionalServices fill:#848383;\n\nend\n```\n\n",
    "tag": "posthog"
  },
  {
    "title": "event-flow.mdx",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/runbook/services/plugin-server/diagrams/event-flow.mdx",
    "content": "```mermaid\nflowchart LR\n   subgraph Main [ ]\n        direction LR\n\n\n```    ClickHouse[(ClickHouse)]\n\n    PluginServerIngestion(Plugin server<BR>mode=ingestion)\n    PluginServerAsync(Plugin server<BR>mode=async)\n\n    KafkaPlugins{Kafka}\n    KafkaCH{Kafka}\n\n    Django[Capture endpoint]\n\n    subgraph PluginServerIngestion [Ingestion server]\n        direction LR\n\n        KafkaConsumerIngestion(Kafka Consumer)\n\n        ProcessEvent(Plugins processEvent)\n        CreateEvent(Ingest event)\n\n\n        subgraph MainThreadIngestion [Main thread]\n            KafkaConsumerIngestion\n        end\n\n        subgraph WorkerThreadIngestion [Worker thread]\n            ProcessEvent(pluginsProcessEvent)\n            PrepareEvent(prepareEvent)\n            EmitToBuffer(emitToBuffer)\n            CreateEvent(createEvent)\n        end\n\n    PiscinaIngestion[Piscina]\n\n    end\n\n    subgraph PluginServerAsync [Async server]\n\n        direction LR\n\n        KafkaConsumer(Kafka Consumer)\n        AsyncHandlers(runAsyncHandlers<BR>onEvent,webhooks)\n        Piscina(Piscina)\n\n        subgraph MainThread [Main thread]\n            KafkaConsumer\n        end\n\n        subgraph WorkerThread [Worker thread]\n            AsyncHandlers\n        end\n\n\n    end\n\n    Django --> KafkaPlugins\n    KafkaPlugins -->|events_plugin_ingestion| KafkaConsumerIngestion\n\n\n    KafkaConsumerIngestion -.-> PiscinaIngestion\n    KafkaConsumerIngestion -.-> PiscinaIngestion\n    KafkaConsumerIngestion -.-> PiscinaIngestion\n\n\n    PiscinaIngestion --> ProcessEvent\n    ProcessEvent --> PrepareEvent\n    PrepareEvent --> EmitToBuffer\n    EmitToBuffer --> CreateEvent\n\n    CreateEvent -->|clickhouse_events_json| KafkaCH\n\n    KafkaCH --> ClickHouse\n    KafkaCH --> KafkaConsumer\n\n\n    KafkaConsumer -.-> Piscina\n    KafkaConsumer -.-> Piscina\n    KafkaConsumer -.-> Piscina\n\n    Piscina --> AsyncHandlers\n\n    %% Styling\n\n    class PluginServerAsync plugingraph;\n    class Main,MainThread,WorkerThread,MainThreadIngestion,WorkerThreadIngestion sgraph;\n\n    classDef plugingraph fill:#fff,stroke-width:3px,stroke:#000;\n    classDef sgraph fill:#fff,stroke-width:3px,stroke-dasharray:5,stroke:#000;\n    classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;\nend\n```\n\n",
    "tag": "posthog"
  },
  {
    "title": "ingestion-server.mdx",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/runbook/services/plugin-server/diagrams/ingestion-server.mdx",
    "content": "```mermaid\nflowchart LR\n    subgraph Wrapper [ ]\n        direction LR\n        Redis[(Redis)]\n        ClickHouse[(ClickHouse)]\n\n\n```    Kafka{Kafka}\n    Kafka2{Kafka}\n\n    Django(Django)\n\n\n    subgraph PluginServerIngestion [ ]\n\n        KafkaConsumer(Kafka Consumer)\n        HttpServer(HTTP Server)\n        PubSub(PubSub)\n        Piscina[Piscina]\n\n\n        subgraph MainThread [Main thread]\n            KafkaConsumer\n            PubSub\n            HttpServer\n        end\n\n        subgraph WorkerThread [Worker thread]\n\n            ProcessEvent(pluginsProcessEvent)\n            PrepareEvent(prepareEvent)\n            EmitToBuffer(emitToBuffer)\n            CreateEvent(createEvent)\n            ReloadPlugins(Reload plugins)\n\n        end\n\n    end\n\n    Django --> Kafka\n    Django --> Redis\n\n    Redis --> PubSub\n    PubSub --> Piscina\n    Piscina --> ReloadPlugins\n\n    Kafka --> |events_plugin_ingestion| KafkaConsumer\n    KafkaConsumer -.-> Piscina\n    KafkaConsumer -.-> Piscina\n    KafkaConsumer -.-> Piscina\n\n    Piscina --> ProcessEvent\n    ProcessEvent --> PrepareEvent\n    PrepareEvent --> EmitToBuffer\n    EmitToBuffer --> CreateEvent\n    CreateEvent --> |clickhouse_events_json| Kafka2\n    Kafka2 --> ClickHouse\n\n\n    %% Styles\n\n    classDef sgraph fill:#fff,stroke-width:3px,stroke-dasharray:5,stroke:#000;\n\n    class MainThread,WorkerThread sgraph;\n    classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;\n    classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;\n    classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;\nend\n```\n\n",
    "tag": "posthog"
  },
  {
    "title": "async-server.mdx",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/runbook/services/plugin-server/diagrams/async-server.mdx",
    "content": "```mermaid\nflowchart LR\n\n\n```subgraph Wrapper [ ]\n    direction LR\n    Redis[(Redis)]\n    Postgres[(Postgres)]\n    Kafka{Kafka}\n    Django(Django)\n    IngestionServer(Ingestion server)\n\n\n    subgraph PluginServerAsync [ ]\n\n        KafkaConsumer(Kafka Consumer)\n\n        Graphile(Graphile job queue)\n        Piscina[Piscina]\n        Schedule(Schedule)\n        HttpServer(HTTP Server)\n        PubSub(PubSub)\n\n\n        subgraph MainThread [Main thread]\n            KafkaConsumer\n            PubSub\n            Graphile\n            HttpServer\n            Schedule\n        end\n\n        subgraph WorkerThread [Worker thread]\n            AsyncHandlers(runAsyncHandlers<BR>onEvent,webhooks)\n            Scheduled(Scheduled tasks)\n            Jobs(Plugin jobs)\n            ReloadPlugins(Reload plugins<BR>Reload actions)\n        end\n\n    end\n\n\n    Kafka --> |clickhouse_events_json| KafkaConsumer\n    KafkaConsumer -.-> Piscina\n    KafkaConsumer -.-> Piscina\n    KafkaConsumer -.-> Piscina\n\n    Piscina --> AsyncHandlers\n    Schedule --> Piscina\n    Piscina --> Scheduled\n\n    Postgres --> Graphile\n\n    Graphile --> Piscina\n    Piscina --> Jobs\n    IngestionServer --> Kafka\n    Django --> Redis\n    Redis --> PubSub\n    Redis --> PubSub\n\n\n    PubSub --> Piscina \n    Piscina --> ReloadPlugins\n\n    %% Styles\n\n    classDef sgraph fill:#fff,stroke-width:3px,stroke-dasharray:5,stroke:#000;\n\n    class MainThread,WorkerThread sgraph;\n    classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;\n    classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;\n    classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;\n```\n\n\nend",
    "tag": "posthog"
  },
  {
    "title": "Metrics",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/runbook/services/minio/index.md",
    "content": "\ntitle: MinIO\nsidebar: Docs\nshowTitle: true\n\nMinIO offers high-performance, S3 compatible object storage. We use it to store files like:\n- file exports\nThis is a guide for how to operate MinIO with respect to our stack.\nMetrics\nAs with any system it is important to keep an eye on metrics to make sure everything is in ship shape. Most of these metrics shouldn't be a surprise.\nThe metrics you should keep an eye on with MinIO are:\n- CPU usage\n- Disk usage (IOPS/throughput/usage)\nCollecting metrics\n\nMinIO exposes a Prometheus endpoint for scraping metrics by Prometheus.\n\nFailure modes\nMinIO has a few failure modes depending on configuration, utilization, disk performance and more.\nOut of Disk Capacity\nRunning out of Disk is another common failure case. What you will typically notice is MinIO restarting repeatedly. The easiest way to mitigate this is to simply increase the volume size that MinIO is using for data. This parameter can be found in the `values.yaml` file in our chart. Please adjust the volume size however it makes sense for your infrastructure and your retention settings.\nUseful links",
    "tag": "posthog"
  },
  {
    "title": "Failure modes",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/runbook/services/zookeeper/index.md",
    "content": "\ntitle: Zookeeper\nsidebar: Docs\nshowTitle: true\n\nimport Sunset from \"../../../self-host/_snippets/sunset-disclaimer.mdx\"\n\nZooKeeper is a centralized service for maintaining configuration information, naming,\nproviding distributed synchronization, and providing group services.\nAt PostHog we use it to store metadata information for ClickHouse\nand Kafka.\nFailure modes\nDisk space usage increases rapidly\nIt has been observed that Zookeepers can suddenly increase it's disk usage, after\nbeing in a stable state for some time. This can sometimes be resolved by ensuring\nthat old Zookeeper snapshots are cleared. If you experience this issue you can\nvalidate this solution by running zkCleanup.sh:\n`bash\nkubectl exec -it -n posthog posthog-posthog-zookeeper -- df -h /bitnami/zookeeper\nkubectl exec -it -n posthog posthog-posthog-zookeeper -- /opt/bitnami/zookeeper/bin/zkCleanup.sh -n 3\nkubectl exec -it -n posthog posthog-posthog-zookeeper -- df -h /bitnami/zookeeper`\nThis will remove all snapshots aside from the last three, printing out the disk\nusage before and after.\nIn newer versions of our Helm chart we\nrun snapshot cleanups periodically every hour. If you experience Zookeeper space issues\nand are on chart 18.2.0 or below, you can update to a later version to enable this.\nAlternatively you can specify the Helm value `zookeeper.autopurge.purgeInterval=1` which\nwill cause the clean up job to run every hour.\nIf you wish to further debug what is being added to your cluster, you can inspect\na snapshot diff by running zhSnapshotComparer.sh e.g.:\n`bash\nkubectl exec -it -n posthog posthog-posthog-zookeeper -- /opt/bitnami/zookeeper/bin/zkSnapshotComparer.sh -l /bitnami/zookeeper/data/version-2/snapshot.fe376 -r /bitnami/zookeeper/data/version-2/snapshot.ff8c0 -b 2 -n 1`\nThis will give you a breakdown of the number of nodes in each snapshot, as well as\nthe exact node difference between the two. For example:\n```bash\nDeserialized snapshot in snapshot.fe376 in 0.045252 seconds\nProcessed data tree in 0.038782 seconds\nDeserialized snapshot in snapshot.ff8c0 in 0.018605 seconds\nProcessed data tree in 0.006101 seconds\nNode count: 1312\nTotal size: 115110\nMax depth: 10\nCount of nodes at depth 0: 1\nCount of nodes at depth 1: 2\nCount of nodes at depth 2: 5\nCount of nodes at depth 3: 4\nCount of nodes at depth 4: 12\nCount of nodes at depth 5: 262\nCount of nodes at depth 6: 546\nCount of nodes at depth 7: 317\nCount of nodes at depth 8: 162\nCount of nodes at depth 9: 1\nNode count: 1312\nTotal size: 115112\nMax depth: 10\nCount of nodes at depth 0: 1\nCount of nodes at depth 1: 2\nCount of nodes at depth 2: 5\nCount of nodes at depth 3: 4\nCount of nodes at depth 4: 12\nCount of nodes at depth 5: 262\nCount of nodes at depth 6: 546\nCount of nodes at depth 7: 317\nCount of nodes at depth 8: 162\nCount of nodes at depth 9: 1\nPrinting analysis for nodes difference larger than 2 bytes or node count difference larger than 1.\nAnalysis for depth 0\nAnalysis for depth 1\nAnalysis for depth 2\nAnalysis for depth 3\nAnalysis for depth 4\nAnalysis for depth 5\nAnalysis for depth 6\nNode /clickhouse/tables/0/posthog.events/blocks/202203_10072597193275699042_5516746108958885708 found only in right tree. Descendant size: 20. Descendant count: 0\n...\n```\nUseful links",
    "tag": "posthog"
  },
  {
    "title": "log-retention.mdx",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/runbook/services/kafka/log-retention.mdx",
    "content": "\ntitle: Log retention\nsidebar: Docs\nshowTitle: true\n\nWhen Kafka's disk gets full, the service can get stuck, leading us to drop all incoming events. To mitigate the issue, we can edit Kafka's log retention policies to free up some space. There are two configs we can set (both set minimum values and data can't be deleted beforehand):\n\ntime - kafka docs\nbytes - kafka docs\n\nNote that the retention check loop by default is ran every 5min retention check interval, we can change it to be more frequent, but probably don't need to.",
    "tag": "posthog"
  },
  {
    "title": "index.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/runbook/services/kafka/index.md",
    "content": "\ntitle: Kafka\nsidebar: Docs\nshowTitle: true\n\nApache Kafka is an open-source distributed event streaming platform used by thousands of companies for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications.\nAt PostHog we mainly use it to stream events from our ingestion pipeline to ClickHouse.\nDictionary\n\n`broker`: a cluster is built by one or more servers. The servers forming the storage layer are called brokers\n`event`: an event records the fact that \"something happened\" in the world or in your business. It is also called record or message in the documentation. When you read or write data to Kafka, you do this in the form of events. Conceptually, an event has a key, value, timestamp, and optional metadata headers\n`producers`: client applications that publish (write) events to Kafka\n`consumer`: client application subscribed to (read and process) events from Kafka\n`topic`: group of events\n`partition`: topics are partitioned, meaning a topic is spread over a number of \"buckets\" located on different Kafka brokers\n`replication`: to make your data fault-tolerant and highly-available, every topic can be replicated, so that there are always multiple brokers that have a copy of the data just in case things go wrong\n\nUseful links",
    "tag": "posthog"
  },
  {
    "title": "resize-disk.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/runbook/services/kafka/resize-disk.md",
    "content": "\ntitle: Resize disk\nsidebar: Docs\nshowTitle: true\n\nimport Sunset from \"../../../self-host/_snippets/sunset-disclaimer.mdx\"\n\nimport ResizeDiskRequirementsSnippet from '../../snippets/resize-disk-requirements'\n\nHow-to\n\n\nList your pods\n`shell\nkubectl get pods -n posthog\nNAME                          READY   STATUS    RESTARTS   AGE\nposthog-posthog-kafka-0       1/1     Running   0          5m15s`\n\n\nConnect to the Kafka container to verify the data directory filesystem size (in this example 15GB)\n`shell\nkubectl -n posthog exec -it posthog-posthog-kafka-0 -- /bin/bash\nposthog-posthog-kafka-0:/$ df -h /bitnami/kafka\nFilesystem                                                                Size  Used Avail Use% Mounted on\n/dev/disk/by-id/scsi-0DO_Volume_pvc-97776a5e-9cdc-4fac-8dad-199f1728b857   15G   40M   14G   1% /bitnami/kafka`\n\n\nResize the underlying PVC (in this example we are resizing it to 20G)\n`shell\nkubectl -n posthog patch pvc data-posthog-posthog-kafka-0 -p '{ \"spec\": { \"resources\": { \"requests\": { \"storage\": \"20Gi\" }}}}'\npersistentvolumeclaim/data-posthog-posthog-kafka-0 patched`\nNote: while resizing the PVC you might get an error `disk resize is only supported on Unattached disk, current disk state: Attached` (see below for more details).\n\nIn this specific case you need to temporary scale down the `StatefulSet` replica value to zero. This will briefly disrupt the Kafka service availability and all the events after this point will be dropped as event ingestion will stop working\nYou can do that by running: `kubectl -n posthog patch statefulset posthog-posthog-kafka -p '{ \"spec\": { \"replicas\": 0 }}'`\nAfter you successfully resized the PVC, you can restore the initial replica definition with: `kubectl -n posthog patch statefulset posthog-posthog-kafka -p '{ \"spec\": { \"replicas\": 1 }}'`\n\n\n\nDelete the `StatefulSet` definition but leave its `pod`s online (this is to avoid an impact on the ingestion pipeline availability): `kubectl -n posthog delete sts --cascade=orphan posthog-posthog-kafka`\n\n\nIn your Helm chart configuration, update the `kafka.persistence` value in `value.yaml` to the target size (20G in this example). You might want to update the retention policy too, more info here\n\n\nRun a `helm` upgrade to recycle all the pods and re-deploy the `StatefulSet` definition\n\n\nConnect to the Kafka container to verify the new filesystem size\n    ```shell\n    kubectl -n posthog exec -it posthog-posthog-kafka-0 -- /bin/bash\n    posthog-posthog-kafka-0:/$ df -h /bitnami/kafka\n    Filesystem                                                                Size  Used Avail Use% Mounted on\n    /dev/disk/by-id/scsi-0DO_Volume_pvc-97776a5e-9cdc-4fac-8dad-199f1728b857   20G   40M   19G   1% /bitnami/kafka\n\n",
    "tag": "posthog"
  },
  {
    "title": "FAQ",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/runbook/async-migrations/0003-fill-person-distinct-id2.mdx",
    "content": "\ntitle: Migration guide - 0002_fill_person_distinct_id2\nsidebar: Docs\nshowTitle: true\n\n0002_fill_distinct_id2 is an async migration added to migrate the data from the old `person_distinct_id` table to the new `person_distinct_id2` table.\nThis is needed for faster `person_distinct_id` queries as the old schema worked off of (`distinct_id`, `person_id`) pairs, making it expensive for our analytics queries, which need to map from `distinct_id` to the latest `person_id`. \nThe new schema works off of `distinct_id` columns, leveraging `ReplacingMergeTrees` with a version column we store in postgres.\nWe migrate teams one-by-one to avoid running out of memory.\nThe migration strategy:\n    1. Write any new updates to both tables\n    2. Insert all non-deleted (`team_id`, `distinct_id`, `person_id`) rows from `person_distinct_id` into `person_distinct_id2` (this migration)\n    3. Once migration has run, we only read/write from/to pdi2.\nFAQ\nIs it dangerous for this migration to be in an errored state?\nNo, the migration copies data to the new table, but that new table is not used until the migration has successfully completed.",
    "tag": "posthog"
  },
  {
    "title": "What are async migrations?",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/runbook/async-migrations/index.md",
    "content": "\ntitle: Async Migrations\nsidebar: Docs\nshowTitle: true\n\nWhat are async migrations?\nAsync migrations are data migrations that do not run synchronously on an update to a PostHog instance. Rather, they execute on the background of a running PostHog instance, and should be completed within a range of PostHog versions.\nYou can check the PostHog blog for more information about how and why we enable async migrations on PostHog.\nFurther internal information about async migrations can be found in our handbook.\nWhy are async migrations necessary?\nMigrations are inevitable, and sometimes it may be necessary to execute non-trivial schema changes that can take a long time to complete.\nFor example, ClickHouse does not support changing the primary key of a table, which is a change we were forced to make in anticipation of upgrading ClickHouse beyond version 21.6. As a result, the way to change the schema of the table was to create a new table and insert all the data from the old table into it, which took us an entire week to run on PostHog Cloud.\nNow, while we at PostHog can execute such changes to our Cloud instance \"manually\", we felt compelled to provide a better approach for our users to do so.\nAs a result, we created a system capable of safely and efficiently managing migrations that need to happen asynchronously.\nWorking with async migrations\nManaging async migrations is a job for self-hosted PostHog instance admins. These migrations require some level of supervision as they affect how data is stored and may run for long periods of time.\nHowever, worry not! We've built a system to make managing these as easy as possible.\nPrerequisite\nMake sure you're on PostHog App version 1.33 or later.\nTo manage async migrations, you must be a staff user. PostHog deployments from version 1.31.0 onwards will automatically give the instance's first user \"staff status\", but those who have deployed PostHog before 1.31.0 will have to manually update Postgres.\nTo do so, follow our guide for connecting to Postgres and then run the following query:\n`sql\nUPDATE posthog_user\nSET is_staff=true\nWHERE email=<your_email_here>`\nTo confirm that everything worked as expected, visit `/instance/async_migrations` in your instance. If you're able to see the migrations info, you're good to go!\nAsync migrations page\nWe've added a page where you can manage async migrations at `/instance/async_migrations`.\nOn this page you can trigger runs, stop running migrations, perform migration rollbacks, check errors, and gather useful debugging information.\nHere's a quick summary of the different columns you see on the async migrations table:\n| Column | Description |\n| :----- | :-------- |\n| Name and Description | The migration's name. This corresponds to the migration file name in posthog/async_migrations/migrations followed by an overview of what this migration does |\n| Status | The current status of this migration. One of: 'Not started','Running','Completed successfully','Errored','Rolled back','Starting'. |\n| Progress | How far along this migration is (0-100) |\n| Current operation index | The index of the operation currently being executed. Useful for cross-referencing with the migration file |\n| Current query ID | The ID of the last query ran (or currently running). Useful for checking and/or killing queries in the database if necessary. |\n| Started at | When the migration started. |\n| Finished at | When the migration ended. |\nThe settings tab allows you to change the configuration, e.g. whether async migrations should run automatically.\nHow can I stop the migration?\nIn the async migrations page at `/instance/async_migrations` you can choose to `stop` or `stop and rollback` the migration from the `...` button on the right most column.\n\nThe migration is in an Error state - what should I do?\nTry to rollback the migration to make sure we're in a safe state. You can do so from the async migrations page at `/instance/async_migrations` from `...` button on the right most column. If you're unable to rollback reachout to us in slack.\n\nCelery scaling considerations\nTo run async migrations, we occupy one Celery worker process to run the task. Celery runs `n` processes (per pod) where `n == number of CPU cores on the posthog-worker pod`. As such, we recommend scaling the `posthog-worker` pod in anticipation of running an async migration.\nYou can scale in two ways:\n\nHorizontally by increasing the desired number of replicas of `posthog-worker`\nVertically by increasing the CPU request of a `posthog-worker` pod\n\nOnce the migration has run, you can scale the pod back down.\nError Upgrading: Async migrations are not completed\nYou might have ran into a message like this:\n```\nList of async migrations to be applied:\n- 0123_migration_name_1 - Available on Posthog versions 1.35.0 - 1.40.9\n- 0124_migration_name_2 - Available on Posthog versions 1.37.0 - 1.40.9\nAsync migrations are not completed. See more info https://posthog.com/docs/self-host/configure/async-migrations/overview\n```\nThis means you were trying to update to a version that requires these async migrations to be completed.\n1. If you're on a version that has these migrations available you can head over to the async migrations page (at `/instance/async_migrations`). After completing the required migrations, re-run the upgrade. Note: we recommend a minimum version of 1.33.0 for running async migrations for a smoother experience.\n1. If you're not on a version that has the migration available you'll first need to upgrade to that version. Then head over to the async migrations page (at `/instance/async_migrations`). After completing the required migrations you can continue upgrading forward.\nThe table below lists out recommended PostHog app and chart versions to use for updating to if there's a need for a multi step upgrade.\n| Async Migration | PostHog Version | Chart Version | Notes  |\n| --------------- | --------------- | --------------| ------ |\n| 0001            | 1.33.0          | 16.1.0        |        |\n| 0002            | 1.33.0          | 16.1.0        |        |\n| 0003            | 1.33.0          | 16.1.0        |        |\n| 0004            | 1.36.1          | 26.0.0        | This NOT the default PostHog version for v26 Chart version, see upgrade instructions below. Run the async migration right after upgrading as there could be problems with ingestion otherwise | \n| 0005            | 1.41.4          | 29.0.11       |        |\n| 0006            | 1.41.4          | 29.0.11       |        |\n| 0007            | 1.41.4          | 29.0.11       | Completing this migration enables person on events. Further information: https://posthog.com/blog/persons-on-events |\nUpgrading hobby deployment to a specific version\nBefore following the normal upgrading procedure update the `.env` file to have `POSTHOG_APP_TAG` match `release-<desired version>`. For example run\n`echo \"POSTHOG_APP_TAG=release-1.33.0\" >>.env`\nUpgrading helm chart to a specific version\nTo upgrade to a specific chart version you can use `--version <desired version>` flag, e.g.\n`helm upgrade -f values.yaml --timeout 30m --namespace posthog posthog posthog/posthog --atomic --wait --wait-for-jobs --debug --version 16.1.0`",
    "tag": "posthog"
  },
  {
    "title": "Preparation",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/runbook/async-migrations/0002-events-sample-by.md",
    "content": "\ntitle: Migration guide - 0002_events_sample_by\nsidebar: Docs\nshowTitle: true\n\n0002_events_sample_by is an async migration added to change the `SAMPLE BY` and `ORDER BY` clauses of our events table in ClickHouse.\nThere were 2 important reasons for doing this:\n\nPerformance: The new schema leads to a performance (speed) improvement for various PostHog queries\nUpgrading ClickHouse: Changing the schema was necessary to unblock the upgrading of ClickHouse, which is something we aim to complete in PostHog version 1.33.0 and can also bring massive performance improvements.\n\n\nNote: During the migration event ingestion from Kafka to ClickHouse will be paused for a brief period. There won't be any data loss as we'll be consuming all the events from Kafka later. However during that brief period you might not see new events appear in the PostHog UI.\n\nPreparation\n\nMake sure you have enough free space in your ClickHouse instance. We certify this via a preflight check before running the migration, but it is good that you're also aware of the requirement.\nMake sure we have a long enough retention policy in Kafka (ClickHouse event ingestion will be paused during the migration, and to make sure we don't lose any data we'll want to make sure events won't expire too fast from Kafka).\n\n\n\nHow can I verify the migration was successful?\n\n\n\nFor ClickHouse check the events table size from the `/instance/status` page in the app. You can find it under \"ClickHouse table sizes\". We need that to be smaller than \"ClickHouse disk free space\" as we'll be duplicating the events table. If you need to increase your ClickHouse storage check out our [ClickHouse resize disk docs](/docs/runbook/services/clickhouse/resize-disk).\n\nFor Kafka by default we have `logRetentionHours=24`, but you could have overridden it in your `values.yaml`, which guarantees the minimal amount of time we'll keep events. Note, that there's also `logRetentionBytes` to better use the disk available, which might mean your retention in reality can be a lot longer than 24h. You can check what the oldest message is by running in your kafka pod shell:\n\n```\nkafka-console-consumer.sh --bootstrap-server localhost:9092 --topic clickhouse_events_proto --from-beginning --max-messages 1\n```\n\nRecall that we'll be pausing the event ingestion during this migration (likely for less than 30min), if the ingestion is paused for longer than we have retained in Kafka we would lose events/data. We suggest making sure you have at least 3 days worth of data. See the docs for info about [resizing kafka](/docs/runbook/services/kafka/resize-disk) and [kafka log retention](/docs/runbook/services/kafka/log-retention).\n\n\nOperations\n\nCreate a new table with the updated schema: `SAMPLE BY cityHash64(distinct_id)` + `ORDER BY (team_id, toDate(timestamp), event, cityHash64(distinct_id), cityHash64(uuid))`\nStart backfilling the new table (online) with data from older partitions\nDetach the `events_mv` materialized view so we stop ingesting events from Kafka to ClickHouse. This makes sure we don't lose any events while switching the tables and all the events will be waiting in Kafka. From this point until step 7 we might not see new events in the PostHog UI.\nInsert the remaining events into the new table\nRename the current table to `events_backup_0002_events_sample_by` and rename the new table to `events` (the table we use for querying)\nAttach the materialized view so we start ingestion again from where we left off before\nOptimize the table to remove duplicates\n\nChecks\n\n`is_required`: only run this migration on instances with the old schema (new deploys get the new schema by default)\n`precheck`: make sure there's enough free disk space in ClickHouse to run the migration\n`healthcheck`: prevent ClickHouse from blowing up for lack of disk space\n\nAfter completion\nTo be extra safe we don't delete `events_backup_0002_events_sample_by` table, but it could take up a significant amount of disk space. We suggest deleting it manually after a few hours/days if things look good. To do that connect to ClickHouse and run `DROP TABLE events_backup_0002_events_sample_by`.\nFAQ\nWill this migration cause any data loss?\nNo. During the migration event ingestion from Kafka to ClickHouse will be paused for a brief period. There won't be any data loss as we'll be consuming all the events from Kafka later. Furthermore this migration duplicates the events table and keeps the old table as a backup so we can always restore it.\nWill this migration stop ingestion?\nYes, but please note that:\n\nThere will not be any data loss as we'll be consuming all the events from Kafka later.\nIngestion is stopped only for a brief period of time, when we are processing the last partition of the old events table and renaming the tables.\nIngestion will only be stopped from Kafka to ClickHouse, which is the last step in the ingestion pipeline (see architecture).\n\nWill I see inconsistent data during the migration?\nYes, for the brief period of time. When event ingestion from Kafka to ClickHouse is stopped we will still process person data. For example we might see a person property that was just set, but we wouldn't see the event that set it yet. Once the migration (or rollback) has finished and we caught up everything will be consistent again.\nThe migration errored with \"EOF: Unexpected end of line when reading bytes\" - what should I do?\nYour ClickHouse instance may have run out of memory.\nTo check this, run:\n`kubectl describe pod chi-posthog-posthog-0-0-0 -n posthog`\nIf the output of the above showed the pod was once terminated with the reason `OOMKilled`, then you will have confirmed the diagnosis.\nTo scale ClickHouse vertically (add more memory), follow this scaling guide.\nKafka is crash looping (disk full) - what should I do?",
    "tag": "posthog"
  },
  {
    "title": "Steps",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/migrate/migrate-from-amplitude.mdx",
    "content": "\ntitle: Migrate to PostHog from Amplitude\nsidebar: Docs\nshowTitle: true\n\nIf you're running close to Amplitude's 10 million free events limit, migrating to PostHog could be a good idea, especially since the PostHog event schema is quite similar to Amplitude's.\nSteps\n\nExport your data from Amplitude using the Amplitude Export API\nImport data into PostHog using any of PostHog's SDKs or capture API.\n\nTranslating the data schema\nRoughly, for every Amplitude event, you'll need to:\n\nCapture the event: this involves calling the Capture events with the event name, timestamp, event properties, and user properties\nAlias the user: if this is a logged-in user, you'll want to Alias the device ID to the user\n\nCapture the event\nFor reference, as of writing, Amplitude's event structure is (according to their Export API documentation:\n```\n{\n  \"event_time\": UTC ISO-8601 formatted timestamp,\n  \"event_name\": string,\n  \"device_id\": string,\n  \"user_id\": string | null,\n  \"event_properties\": dict,\n  \"group_properties\": dict,\n  \"user_properties\": dict,\n// A bunch of other fields that include things like\n  // device_carrier, city, etc.\n  ...other_fields\n}\n```\nWhen capturing the event, roughly, the schema you'll want to use is:\n`js\nconst distinctId = user_id || device_id;\nconst eventMessage = {\n  properties: {\n    ...event_properties,\n    ...other_fields,\n    $set: { ...user_properties, ...group_properties },\n    $geoip_disable: true,\n  },\n  event: event_name,\n  distinctId: distinctId,\n  timestamp: event_time,\n};`\nIn short:\n\nWe want to track the event with the same:\nEvent name\nTimestamp\nFor the distinct ID, we either want to use the User ID if present, or the Device ID (a UUID string) if not\nWe want to track both User and Group properties as User Properties using `properties: {$set}`\nWe want to track Event properties using `properties`\n\nAliasing device IDs to user IDs\nIn addition to tracking the events themselves, we want to tie users' events both before and after login. For Amplitude, events before and after login look a bit like this:\n| Event | User ID | Device ID |\n|-------|---------|-----------|\n| Application installed | null | 551dc114-7604-430c-a42f-cf81a3059d2b |\n| Login | 123 | 551dc114-7604-430c-a42f-cf81a3059d2b |\n| Purchase | 123 | 551dc114-7604-430c-a42f-cf81a3059d2b |\nWe'd ideally like to be able to attribute \"Application installed\" to the user with ID 123, so we need to also call alias with:\n`js\nconst aliasMessage = {\n  distinctId: user_id, // 123\n  aliasId: device_id, // 551dc114-7604-430c-a42f-cf81a3059d2b\n};`",
    "tag": "posthog"
  },
  {
    "title": "Not sure which export method to use?",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/migrate/export-events.md",
    "content": "\ntitle: Export events\nsidebar: Docs\nshowTitle: true\n\nAt PostHog we believe in your right to own your data - this means being able to export the raw events whenever you want.\nThere are several ways to export your events depending on your use case:\nMethod | When? | Limitations\n--- | --- | ---\nPostHog UI - click \"Export\" on the events table | You need to export a small number of events | 3,500 events\nEvents API | Great for one-off smaller exports | 1 day date range and 3,500 events\nData export app | You need to export a large number of events | No limits\nIf you're looking to migrate to a new PostHog instance follow the migrating between PostHog instances guide.\nNot sure which export method to use?\nHere's a decision tree you might find useful:\n`mermaid\ngraph TD\n    A[Do you need to export a large number of events? Above 3,500] --> |Yes| B[Use a data export app]\n    A --> |No| C[Do you need to run regular exports e.g. once a day or once a week?]\n    C --> |Yes| B\n    C --> |No| E[Use the PostHog UI or the Events API]`\nUsing Data Export Apps\nPopular apps for data export include the S3, Google Cloud Storage, BigQuery, PostgreSQL, Redshift, Snowflake. See the full list of data export apps, or find out how to build your own app.\nTo export your historical data you'll want to connect the app and then run a historical export.\nAdditionally, these apps stream all the new events to the destination. This keeps your destination up to date with new events as they come in.\nWhy are there limitations on the PostHog UI export and the events API?",
    "tag": "posthog"
  },
  {
    "title": "Requirements",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/migrate/migrate-to-cloud.mdx",
    "content": "\ntitle: Migrate from self-hosted to PostHog Cloud\nsidebar: Docs\nshowTitle: true\n\nimport MigratingEvents from \"./snippets/migrating-events.mdx\"\n\nIf you're attempting this migration, feel free to ask questions and provide feedback via the PostHog Communty Slack workspace or a GitHub issue.\n\nRequirements\n\nAn existing PostHog instance running at least `1.30.0` (For upgrade instructions, take a look at this guide)\nA new project on PostHog Cloud\n\nApproach\nThis migration has two parts:\n\nMigrate your events, this will also create the necessary person, person distinct ID, and related records.\nMigrate your metadata (projects, dashboards, insights, actions, cohorts, feature flags, experiments, annotations).\n\nMigrate your events\nTo migrate your events, we'll be using the PostHog Replicator app,\nwhich allows us to stream both incoming and historical events to PostHog Cloud\n\nNote: Before starting, make sure to deactivate the GeoIP plugin on Cloud\n\nInstalling the Replicator\nStart by logging in to your existing self-hosted instance and navigating to the 'Apps' tab. Next, search for the 'Replicator' app and install it if it isn't already.\n\nFor the configuration:\n- Host: either app.posthog.com for US or eu.posthog.com for EU\n- Project API Key: the API key for the project in Cloud\nAfter clicking save and activating it (toggle on the left side), the Replicator will begin forwarding incoming events to Cloud. Note that it may take around 15-20 minutes for the first events to arrive.\nExporting historical events\nNow that the Replicator is sending new events, we'll set up a batch job to export past events.\nFirst, go back to the configuration menu and click the gear next to \"Export historical events\" under \"Jobs\", which will open the following popup.\n\nSelect a start and end date on the calendar for the range you would like to export.\nOnce you've chosen a date range, click \"Run job now\" to start the export.\nDepending on the amount of data you have stored, this may take a fair amount of time. You can monitor the progress by clicking the \"App metrics\" button (graph) and going to the \"Historical exports\" tab.\nSwitching tracking in you product\nNow that our export is running, the next step is to switch over tracking within your product to direct any new events to Cloud. First, ensure that the Replicator app is still running on your self-hosted instance. Next, begin swapping out the API key and host within all the areas of your product that you track. Since we still have the replicator app installed, any events sent to the old instance will be forwarded, while any events using the new API key will go directly to our Cloud instance.\nOnce this is complete and you've double-checked that no more events are coming to the old instance and that all historical exports have finished, you can deactivate the Replicator app.\nMigrate your metadata\nFollow the instructions on the PostHog migrate metadata repo:\n\nNote\nThis process has the following caveats:\n1. Project API Key. You'll need to replace the API key in your code with the new API key.\n2. \"created by\" information. Every object will appear as if it was created by you.\n3. \"created at\" information. Every object will appear as if it was created on the time you ran this script.\n\n\nClone the repo and cd into it\n    `bash\n    git clone https://github.com/PostHog/posthog-migrate-meta\n    cd posthog-migrate-meta`\nInstall the dependencies by running `yarn`\nRun the script\n    `bash\n    ts-node --source [posthog instance you want to migrate from] --sourcekey [personal api key for that instance] --destination [posthog instance you want to migrate to.] --destinationkey [personal api key for destination instance]`\n\nFor more information on the options see the repo's readme\nMigrating your custom apps\nIf the app was previously extracting events using the scheduled tasks api:\n\n(fastest option) you could move this logic from the app into scheduled tasks on your server potentially using a cron job\nIf you can make your app generalizable enough that others can benefit then submit your app to the store.\nTo make it generalizable you'll want to convert anything specific to your configuration into a plugin.json config value\n\n\n\nIf the app was transforming events before ingestion:\n\n(fastest option) you could move this logic from the app into your client before you send the event\nIf you can make your app generalizable enough that others can benefit then submit your app to the store.\nTo make it generalizable you'll want to convert anything specific to your configuration into a plugin.json config value\n\n\n\nIf the app was used to send events to a custom destination:\n\n(fastest option) Convert your app to work as a webhook. We are soon releasing a webhook destination. You can subscribe for updates on the roadmap\n\nIf you can make your app generalizable enough that others can benefit then submit your app to the store.\n\nTo make it generalizable you'll want to convert anything specific to your configuration into a plugin.json config value\n\n\n",
    "tag": "posthog"
  },
  {
    "title": "tip: use the flag -U to specify a username if necessary",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/migrate/migrate-broken-self-hosted.mdx",
    "content": "\ntitle: Migrating from a broken PostHog self-hosted instance\nsidebar: Docs\nshowTitle: true\n\n\nIf you're attempting this migration, feel free to ask questions and provide feedback via the PostHog Communty Slack workspace or a GitHub issue.\n\nIf your PostHog instance is running we highly recommend the migrating between PostHog instances guide\nRequirements\n\nExisting PostHog self-hosted instance that you can no longer login too\nA new PostHog instance\n\nApproach\nThis migration has two parts:\n1. Migrate your events, this will also create the necessary person, person distinct ID, and related records.\n2. Migrate your meta data (projects, dashboards, insights, actions, cohorts, feature flags, experiments, annotations).\n   - Note! It won't move over the following:\nMigrate your events\n\nAccess your Clickhouse database and export all your events to disk (including uuid, timestamp, distinctid, event, properties, $group0, $group[1-3])\nDisable the GeoIP plugin of your new PostHog instance\nWrite a script to send them as new requests (likely easiest with our node or python library). Here's an example of how we do it with the Replicator app.\nRe-enable the GeoIP plugin\n\nMigrate your meta data\nTo migrate the non-event data (such as user information, feature flags, dashboard configurations, etc), we'll need to create a data dump and load that into the new instance. This non-event data is stored in Postgres.\n\nThis is only officially supported for importing events into a new cloud instance of PostHog or an enterprise self-hosted instance using Kubernetes. If you're using a different deployment method, you can still try this approach but we can't guarantee it will work.\n\n\nYou should have a clean PostHog cloud or self-hosted PostHog instance up and running. Your new PostHog instance should have no ingested events. We recommend using a fresh and unused installation.\nYour old and new instances should both be running the exact same version of PostHog (minimum `1.30.0`).\n\n1. Create a data dump from your old instance\n\nPlease note that this guide assumes your PostHog pods are running in the `posthog` kubernetes namespace, thus appending `-n posthog` to `kubectl` commands. Substitute `posthog` for the namespace applicable to you, or omit the `-n` flag if you're using the `default` namespace.\n\nAccess your old PostHog instance and run the following command:\n```shell\ntip: use the flag -U to specify a username if necessary\npg_dump -d posthog -f tmp/export.sql --no-owner --data-only --disable-triggers \\\n-t posthog_action \\\n-t posthog_actionstep \\\n-t posthog_annotation \\\n-t posthog_cohort \\\n-t posthog_dashboard \\\n-t posthog_dashboarditem \\\n-t posthog_featureflag \\\n-t posthog_featureflagoverride \\\n-t posthog_messagingrecord \\\n-t posthog_organization \\\n-t posthog_organizationinvite \\\n-t posthog_organizationmembership \\\n-t posthog_personalapikey \\\n-t posthog_plugin \\\n-t posthog_pluginattachment \\\n-t posthog_pluginconfig \\\n-t posthog_pluginstorage \\\n-t posthog_team \\\n-t posthog_user \\\n-t posthog_user_groups \\\n-t posthog_user_user_permissions\n```\n\nNote: You may see a warning regarding circular dependencies between tables, which shouldn't be an issue as we are using `--disable-triggers`. If you do run into issues when restoring, try doing a full dump instead of `--data-only`.\n\nIf you do not access your Postgres database via a port-forwarding mechanism to your local machine, you will then need to copy the file from the server to your local system (or the remote server from where you access your new instance). You can do this with `scp`:\n\nNote: If your old PostHog instance is hosted on Heroku, you should follow this guide to export all of your data, load into a separate database (we recommend a local Postgres instance), and then run the command above.\n\n`shell\nscp <your_username>@<database_hostname>:/tmp/export.sql .`\nIf your old instance was deployed using the Helm chart, you can use:\n`shell\nkubectl cp <postgres-pod-name>:/tmp/export.sql ./export.sql # -n posthog`\n2. Send events to your new instance\nYou'll need to write a script to process this data and create the following resources on PostHog cloud. The PostHog migrate code which we use to migrate between two cloud instances will likely be useful.\nMoving a license key",
    "tag": "posthog"
  },
  {
    "title": "Requirements",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/migrate/migrate-between-posthog-instances.mdx",
    "content": "\ntitle: Migrate between PostHog instances\nsidebar: Docs\nshowTitle: true\n\nimport MigratingEvents from \"./snippets/migrating-events.mdx\"\n\nIf you're attempting this migration, feel free to ask questions and provide feedback via the PostHog Communty Slack workspace or a GitHub issue.\n\nRequirements\n\nExisting PostHog instance that you can login to (Self-hosted versions need a minimum version of `1.30.0`, if you version is lower you'll need to upgrade to this version first)\nA new PostHog instance\n\nApproach\nThis migration has two parts:\n1. Migrate your events, this will also create the necessary person, person distinct ID, and related records.\n2. Migrate your meta data (projects, dashboards, insights, actions, cohorts, feature flags, experiments, annotations).\n   - Note! It won't move over the following:\n     1. Project API Key. You'll need to replace the API key in your code with the new API key.\n     2. \"created by\" information. Every object will appear as if it was created by you.\n     3. \"created at\" information. Every object will appear as if it was created on the time you ran this script.\nMigrate your events\n\nMigrate your meta data\nFollow the instructions on the PostHog migrate metadata repo:\n1. Clone the repo and cd into it\n   `bash\n   git clone https://github.com/PostHog/posthog-migrate-meta\n   cd posthog-migrate-meta`\n2. Install the dependencies by running `yarn`\n3. Run the script\n    `bash\n    ts-node --source [posthog instance you want to migrate from] --sourcekey [personal api key for that instance] --destination [posthog instance you want to migrate to.] --destinationkey [personal api key for destination instance]`",
    "tag": "posthog"
  },
  {
    "title": "migrating-events.mdx",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/migrate/snippets/migrating-events.mdx",
    "content": "To migrate your events, we'll be using the PostHog Replicator app:\n\nDisable the GeoIP plugin of your new PostHog instance\nLog in to your old PostHog instance\nClick 'Apps' on the left-hand tool bar\nSearch for 'Replicator'\nIf self-hosted, select the app, press 'Install' (or make sure it's the latest version)\nFor the configuration details using the following:\n`Host`: the hostname of your new PostHog instance e.g. `app.posthog.com` if you are using US cloud, `eu.posthog.com` if you were are EU cloud, or the domain you'll be sending events to if you are self-hosting\n`Project API Key`: the API key for the new project that you want to send events to - found in the project settings page.\nAfter clicking save and activating it (toggle on the left side), the Replicator will start to run. Any events added to your old instance will be replicated in your new instance. It can take a little while (15-20 minutes) for the first events to show up.\nOnce the Replicator is activated, go back to the configuration menu and click the gear next to \"Export historical events\" under \"Jobs.\" On the calendar, select a start and end date, then click \"Run job now.\" This will begin the process of historical events being import (and might take a while). You can see the progress in logs or by clicking the \"App metrics\" button (graph) and going to the \"Historical exports\" tab.\nTo track progress of the export you can click the graph icon next to the Replicator app and then click on the \"Historical Exports\" tab\n",
    "tag": "posthog"
  },
  {
    "title": "Introduction",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/getting-started/cloud.md",
    "content": "\ntitle: Quickstart with PostHog Cloud\nIntroduction\nThis guide walks through setting up PostHog Cloud and sending your first events.\nPostHog Cloud is our hosted and managed version of the PostHog open-source platform.\nIt always runs on the latest release with updates and migrations managed for you.\nWe recommend PostHog Cloud for almost all our users, as it's the easiest way to get PostHog running and is able to scale to large events volumes.\nIt also comes with 1m events free each month.\n\nNeed to keep full-control of your data? Check out our docs for self-hosting PostHog.\n\n1. Create an account\nSet up a PostHog Cloud account (hosted in the US), or if you're looking for a GDPR-ready environment, try PostHog Cloud EU.\n2. Set up your project\nAfter signing up, you'll see a screen that asks you how you'd like to send events.\nSelect whichever option best matches your use case. If you're not ready to set up your integration yet, you can explore the demo project or invite a team member to help you start ingesting events.\n\nFor web-based projects, the easiest way to get started is by adding the JavaScript snippet to your website and app.\n\n\n3. Send your first events\nAfter you've selected an option, you'll be shown some instructions to get PostHog integrated with your platform of choice.\nIf you've chosen the 'Just Exploring' option, you'll instead be shown the PostHog bookmarklet, which can be used to temporarily send events from a webpage you're viewing to test your integration.\nAfter you've followed these steps, you'll then be sent to a waiting screen where PostHog will listen for events to ensure your integration is set up correctly.\n\nOnce PostHog receives its first event, you should then see a success screen with a 'Complete' button that will take you to your dashboard.\n\nIf you are having trouble getting events to show up, you can click the back arrow to go back to instructions for each specific platform, or you can checkout this page for more detailed information on ingesting events into PostHog.\nWe'd recommend you don't skip this step unless you've already set-up another PostHog account before or if you've used other product analytics platforms.\n4. Next steps\nNow that you have your PostHog Cloud account all set up, here are a bunch of cool things you can go and try out next:\n\nNext steps after installing PostHog - Our recommendations for what to do next after you've installed PostHog.\nProduct manual - Information on using specific features within PostHog.\nTutorials - In-depth guides on popular use cases and integrations.\nSet up a proxy - Capture more events by using your own domain - even on Cloud!\nToolbar - View heatmaps and customize your integration from your website.\nApps - PostHog provides over 50+ apps to connect with your favorite services.\n",
    "tag": "posthog"
  },
  {
    "title": "Our pledge",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/contribute/code-of-conduct.md",
    "content": "\ntitle: Code of conduct\nsidebar: Docs\nshowTitle: true\n\nOur pledge\nWe as members, contributors, and leaders pledge to make participation in our\ncommunity a harassment-free experience for everyone, regardless of age, body\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\nidentity and expression, level of experience, education, socio-economic status,\nnationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming,\ndiverse, inclusive, and healthy community.\nOur standards\nExamples of behavior that contributes to a positive environment for our\ncommunity include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes,\n  and learning from the experience\nFocusing on what is best not just for us as individuals, but for the\n  overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or\n  advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others' private information, such as a physical or email\n  address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a\n  professional setting\n\nNo solicitation\nThe following are not allowed within any community space without prior permission \nfrom the community moderators:\n\nPitching of a business, product, or service\nMarket or competitor research\n\nContact the community moderators via hey@posthog.com.\nEnforcement responsibilities\nCommunity leaders are responsible for clarifying and enforcing our standards of\nacceptable behavior and will take appropriate and fair corrective action in\nresponse to any behavior that they deem inappropriate, threatening, offensive,\nor harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject\ncomments, commits, code, wiki edits, issues, and other contributions that are\nnot aligned to this Code of conduct, and will communicate reasons for moderation\ndecisions when appropriate.\nScope\nThis Code of conduct applies within all community spaces, and also applies when\nan individual is officially representing the community in public spaces.\nExamples of representing our community include using an official e-mail address,\nposting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event.\nEnforcement\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported to the community leaders responsible for enforcement at\nhey@posthog.com.\nAll complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the\nreporter of any incident.\nEnforcement guidelines\nCommunity leaders will follow these community impact guidelines in determining\nthe consequences for any action they deem in violation of this Code of conduct:\n1. Correction\nCommunity Impact: Use of inappropriate language or other behavior deemed\nunprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing\nclarity around the nature of the violation and an explanation of why the\nbehavior was inappropriate. A public apology may be requested.\n2. Warning\nCommunity Impact: A violation through a single incident or series\nof actions.\nConsequence: A warning with consequences for continued behavior. No\ninteraction with the people involved, including unsolicited interaction with\nthose enforcing the Code of conduct, for a specified period of time. This\nincludes avoiding interactions in community spaces as well as external channels\nlike social media. Violating these terms may lead to a temporary or\npermanent ban.\n3. Temporary ban\nCommunity Impact: A serious violation of community standards, including\nsustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public\ncommunication with the community for a specified period of time. No public or\nprivate interaction with the people involved, including unsolicited interaction\nwith those enforcing the Code of conduct, is allowed during this period.\nViolating these terms may lead to a permanent ban.\n4. Permanent ban\nCommunity Impact: Demonstrating a pattern of violation of community\nstandards, including sustained inappropriate behavior,  harassment of an\nindividual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within\nthe community\nAttribution\nThis Code of conduct is adapted from the Contributor Covenant,\nversion 2.0.\nCommunity impact guidelines were inspired by \nMozilla's code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the\nhttps://www.contributor-covenant.org/faq. Also available are translations.",
    "tag": "posthog"
  },
  {
    "title": "Pull requests",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/contribute/recognizing-contributions.md",
    "content": "\ntitle: Recognizing contributions\nsidebar: Docs\nshowTitle: true\n\nAt PostHog we aim to recognize all contributions made to our open source codebases. We do this largely via automated processes that ensure our contributors are recognized. \nPull requests\nIf you submit a Pull Request to a repository under the `PostHog` organization, a bot will automatically send you an email to get in touch with us for a merch code. \nWe usually give out $100 for a product or app-related PR, and $25 for something on posthog.com (but we have gone over and above this for particularly large community PRs before!)\nIf, after a few days of having had your PR merged, you still didn't get a merch code, you can email us at hey@posthog.com and we'll sort it out! Someone on the PostHog team will send you a code manually. \nApps\nIf you build a app for PostHog that is accepted into our official repository, we will list you as a contributor in the categories `code` and `plugin` or `app`, as well as send you some merch. \nNon-PR contributions\nWe follow the All Contributors spec for recognizing contributions. This means that if you are actively engaged in discussions, open bug reports, or contribute in other ways, a PostHog team member is able to add you to our contributors list for any of the contribution types listed in the link above.\nAt the moment we only provide merch for `code` (PR merged) and `plugin` or `app` (app accepted into official repo) contributions, however, as a contributor in a category other than those two, we'll still list you on our README and create a contributor card for you on our contributors page.\nContributor cards\nAll accepted contributors get a digital contributor card from us, which you can find in our contributors page.\nHere's an explanation of the contents of that card:\nCommunity MVPs\nEvery time we do a release, starting with version 1.22.0, we have nominated a 'Community MVP'. This is a contributor that we have chosen to give special recognition to for one or many awesome contributions to PostHog over a given release cycle.\nIf you ever win one of these awards from us, they will appear as trophies on your contributor card. The number of trophies is equal to the number of times you've been named community MVP.\nLevel\nYour contributor level is determined by how many pull requests you have had merged.\n\n\nWhile we have created cards for all past contributors, the we have only started tracking levels from 26/03/2021, which is why your card might say 'lvl 0' even if you had a PR merged before.\n\n\nPowers\n'Powers' refer to the types of contributions you've made to PostHog. The types of contributions available can be seen on the All Contributors spec.\nContributions of type `code` are automatically provided to you for merged pull requests. All other contribution types must be manually requested by a member of the PostHog team.\nSending out merch",
    "tag": "posthog"
  },
  {
    "title": "Being a part of the community",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/contribute/index.md",
    "content": "\ntitle: Contribute to PostHog\nsidebarTitle: Overview\nsidebar: Docs\nshowTitle: true\n\nWe love contributions to PostHog, big or small. Here's what you need to know about contributing to our product:\nBeing a part of the community\nWe have an awesome, diverse, and inclusive community. In order to maintain and grow this, all community members must adhere to our Code of conduct.\nReporting bugs or issues\nBug reports help us make PostHog better for everyone. When you report a bug, the description will automatically be filled with a template that makes is very clear what we'd like you to add.\nBefore raising a new issue, please search within existing ones to make sure you're not creating a duplicate.\n\nImportant: If your issue is related to security, please email us directly at hey@posthog.com instead of creating a GitHub issue.\n\nDeciding what to work on\nWe maintain a list of good first issues that are a great way to get started contributing to the PostHog open-source product analytics platform. You can also pick up any other open tickets, though they may be more complicated to work with. If in doubt, just leave a comment for the author of the issue! Outside of tickets, if there are small improvements to layout, text, or functionality, feel free to create a pull request directly.\nYou can also update PostHog's documentation, handbook, or write a blog post. We maintain a list of good first issues here too. Take a look at our style guide before getting started.\nIf you're planning to work on a bigger feature that is not on the list of issues, please raise an issue first so we can check whether that feature makes sense for PostHog as a whole.\nWriting and submitting code\nAnyone can contribute code to PostHog, including you! To get started, follow our local development guide. Then, make your change and submit a pull request to the posthog repository. We'll be delighted to review your change.\nLicensing\nMost of PostHog's code is under the MIT license, as included in the PostHog repository on GitHub. Code of paid features, however, is covered by a proprietary license.\nAny third party components incorporated into our code are licensed under the original license provided by the owner of the applicable component.",
    "tag": "posthog"
  },
  {
    "title": "Show your support",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/contribute/badge.md",
    "content": "\ntitle: PostHog badges (optional)\nShow your support\nIf you want to shout from the rooftops about how much you love PostHog, we be honored if you'd add a badge to your website's footer.\nLight background\nPreview\n\n\n\nCode\n`html\n<a href=\"https://posthog.com\" title=\"Open source product analytics powered by PostHog\">\n    <svg width=\"108\" height=\"44\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n        <rect x=\".5\" y=\".5\" width=\"107\" height=\"43\" rx=\"2.5\" stroke=\"#000\" stroke-opacity=\".3\" />\n        <g clip-path=\"url(#a)\" fill=\"#000\" fill-opacity=\".3\">\n            <path\n                d=\"M6 34.01c0-.89 1.077-1.337 1.707-.706l1.384 1.384c.63.63.184 1.707-.707 1.707H7a1 1 0 0 1-1-1V34.01Zm0-4.027a1 1 0 0 0 .293.707l5.412 5.412a1 1 0 0 0 .707.293h1.97c.892 0 1.338-1.077.708-1.707l-7.383-7.383c-.63-.63-1.707-.183-1.707.707v1.97Zm0-5.999a1 1 0 0 0 .293.707l11.41 11.411a1 1 0 0 0 .708.293h1.97c.89 0 1.337-1.077.707-1.707l-13.38-13.38c-.63-.63-1.708-.185-1.708.706v1.97Zm5.998 0a1 1 0 0 0 .293.707l9.997 9.997c.63.63 1.707.184 1.707-.707v-1.97a1 1 0 0 0-.293-.708l-9.997-9.996c-.63-.63-1.707-.184-1.707.707v1.97Zm7.705-2.677c-.63-.63-1.707-.184-1.707.707v1.97a1 1 0 0 0 .293.707l3.998 3.999c.63.63 1.708.183 1.708-.707v-1.97a1 1 0 0 0-.293-.708l-3.999-3.998ZM31.515 33.118l-4.965-4.964c-.63-.63-1.707-.184-1.707.707v6.534a1 1 0 0 0 1 1h7.989c.53 0 .96-.43.96-.96s-.435-.95-.953-1.059a4.634 4.634 0 0 1-2.324-1.258Zm-3.793 1.357a.96.96 0 1 1 0-1.92.96.96 0 0 1 0 1.92ZM41.79 34h2.465v-4.087h2.059c2.262 0 3.713-1.342 3.713-3.417 0-2.074-1.451-3.416-3.713-3.416H41.79V34Zm2.465-6.193v-2.621h1.81c.935 0 1.497.5 1.497 1.31 0 .812-.562 1.31-1.498 1.31h-1.81ZM54.486 34.125c2.402 0 4.15-1.716 4.15-4.056s-1.748-4.056-4.15-4.056c-2.434 0-4.15 1.716-4.15 4.056s1.716 4.056 4.15 4.056Zm-1.872-4.056c0-1.248.749-2.106 1.872-2.106 1.107 0 1.856.858 1.856 2.106 0 1.248-.749 2.106-1.856 2.106-1.123 0-1.872-.858-1.872-2.106ZM62.58 34.125c1.84 0 3.088-1.155 3.088-2.512 0-3.182-4.227-2.153-4.227-3.4 0-.344.359-.562.873-.562.53 0 1.17.327 1.373 1.045l1.841-.764c-.359-1.14-1.685-1.92-3.292-1.92-1.731 0-2.807 1.03-2.807 2.263 0 2.98 4.165 2.153 4.165 3.385 0 .437-.406.733-1.014.733-.874 0-1.482-.608-1.67-1.341l-1.84.717c.405 1.186 1.59 2.356 3.51 2.356ZM71.598 33.922l-.156-1.981c-.265.14-.608.187-.89.187-.56 0-.935-.406-.935-1.108v-2.98h1.903v-1.902h-1.903V23.86h-2.293v2.278h-1.248v1.903h1.248v3.276c0 1.903 1.294 2.808 2.995 2.808.468 0 .905-.078 1.28-.203ZM79.594 23.08v4.243h-4.337V23.08h-2.465V34h2.465v-4.57h4.337V34h2.48V23.08h-2.48ZM87.545 34.125c2.402 0 4.15-1.716 4.15-4.056s-1.748-4.056-4.15-4.056c-2.434 0-4.15 1.716-4.15 4.056s1.716 4.056 4.15 4.056Zm-1.872-4.056c0-1.248.749-2.106 1.872-2.106 1.108 0 1.856.858 1.856 2.106 0 1.248-.748 2.106-1.856 2.106-1.123 0-1.872-.858-1.872-2.106ZM96.12 33.75c.859 0 1.623-.296 2.029-.81v.748c0 .874-.702 1.482-1.779 1.482-.764 0-1.435-.374-1.544-1.014l-2.09.328c.28 1.482 1.793 2.48 3.634 2.48 2.418 0 4.025-1.42 4.025-3.526v-7.3h-2.262v.67c-.421-.483-1.139-.795-2.06-.795-2.183 0-3.556 1.497-3.556 3.869 0 2.37 1.373 3.868 3.604 3.868Zm-1.372-3.868c0-1.17.686-1.92 1.747-1.92 1.076 0 1.763.75 1.763 1.92s-.686 1.918-1.763 1.918c-1.06 0-1.747-.748-1.747-1.918Z\"\n            />\n        </g>\n        <path\n            d=\"m20.462 10.07.77 2.17h-1.54l.77-2.17Zm-3.4 4.93h1.64l.51-1.44h2.5l.51 1.44h1.63l-2.65-7h-1.49l-2.65 7ZM24.968 15h1.47v-4.5l2.72 4.5h1.57V8h-1.47v4.5L26.538 8h-1.57v7ZM35.32 10.07l.77 2.17h-1.54l.77-2.17ZM31.92 15h1.64l.51-1.44h2.5l.51 1.44h1.63l-2.65-7h-1.49l-2.65 7ZM39.825 15h4.72v-1.36h-3.14V8h-1.58v7ZM46.92 15h1.58v-2.89L50.87 8H49.2l-1.49 2.63L46.23 8h-1.68l2.37 4.11V15ZM53.577 15h1.58V9.35h2.05V8h-5.69v1.35h2.06V15ZM58.318 15h1.58V8h-1.58v7ZM64.88 15.08c1.65 0 2.82-.83 3.15-2.22l-1.59-.36c-.16.75-.75 1.22-1.57 1.22-1.07 0-1.92-.76-1.92-2.22 0-1.46.86-2.23 1.92-2.23.81 0 1.45.53 1.61 1.37l1.57-.33c-.2-1.45-1.42-2.39-3.17-2.39-1.87 0-3.52 1.22-3.52 3.58 0 2.36 1.62 3.58 3.52 3.58ZM71.872 15.08c1.61 0 2.69-.85 2.69-2.12 0-2.86-3.6-1.97-3.6-3.08 0-.39.34-.62.89-.62.66 0 1.13.41 1.27 1.04l1.41-.59c-.24-1.07-1.33-1.79-2.68-1.79-1.37 0-2.47.82-2.47 1.97 0 2.57 3.59 1.79 3.59 3.11 0 .44-.41.74-1.04.74-.69 0-1.3-.52-1.39-1.3l-1.38.41c.13 1.28 1.31 2.23 2.71 2.23ZM78.872 15h2.92c1.48 0 2.52-.76 2.52-2.09 0-.91-.54-1.48-1.17-1.73.4-.22.81-.72.81-1.38 0-1.17-.97-1.8-2.23-1.8h-2.85v7Zm1.58-1.36v-1.67h1.21c.81 0 1.16.3 1.16.85 0 .54-.35.82-1.16.82h-1.21Zm0-2.99V9.24h1.11c.66 0 .94.27.94.71 0 .43-.28.7-.91.7h-1.14ZM87.04 15h1.58v-2.89L90.99 8h-1.67l-1.49 2.63L86.35 8h-1.68l2.37 4.11V15Z\"\n            fill=\"#000\"\n            fill-opacity=\".3\"\n        />\n        <defs>\n            <clipPath id=\"a\"><path fill=\"#fff\" transform=\"translate(6 19)\" d=\"M0 0h94.2v18H0z\" /></clipPath>\n        </defs>\n    </svg>\n</a>`\nDark background\nPreview\n\n\n\nCode\n```html\n\n\n\n\n\n\n\n\n\n\n\n",
    "tag": "posthog"
  },
  {
    "title": "Sending events",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/api/post-only-endpoints.mdx",
    "content": "\ntitle: POST-only public endpoints\nsidebar: Docs\nshowTitle: true\n\n\nUpdate: These endpoints can now be accessed with either your Team API key or your personal API key.\n\nAs explained in our API overview page, PostHog provides two different APIs.\nThis page refers to our public endpoints, which use the same API key as the PostHog snippet. The endpoints documented here are used solely with `POST` requests, and will not return any sensitive data from your PostHog instance.\n\nNote: For this API, you should use your 'Project API Key' from the 'Project' page in PostHog. This is the same key used in your frontend snippet.\n\nSending events\nTo send events to PostHog, you can use any of our libraries or any Mixpanel library by changing the `api_host` setting to the address of your instance.\nIf you'd prefer to do the requests yourself, you can send events in the following format:\nSingle event\n\nNote: Timestamp is optional. If not set, it'll automatically be set to the current time.\n\n`shell\nPOST https://[your-instance].com/capture/\nContent-Type: application/json\nBody:\n{\n    \"api_key\": \"<ph_project_api_key>\",\n    \"event\": \"[event name]\",\n    \"properties\": {\n        \"distinct_id\": \"[your users' distinct id]\",\n        \"key1\": \"value1\",\n        \"key2\": \"value2\"\n    },\n    \"timestamp\": \"[optional timestamp in ISO 8601 format]\"\n}`\nBatch events\nYou can send multiple events in one go with the Batch API.\nThere is no limit on the number of events you can send in a batch, but the entire request body must be less than 20MB by default (see API overview).\n\nNote: Timestamp is optional. If not set, it'll automatically be set to the current time.\n\n`bash\nPOST https://[your-instance].com/batch/\nContent-Type: application/json\nBody:\n{\n    \"api_key\": \"<ph_project_api_key>\",\n    \"batch\": [\n        {\n            \"event\": \"[event name]\",\n            \"properties\": {\n                \"distinct_id\": \"[your users' distinct id]\",\n                \"key1\": \"value1\",\n                \"key2\": \"value2\"\n            },\n            \"timestamp\": \"[optional timestamp in ISO 8601 format]\"\n        },\n        ...\n    ]\n}`\nSample requests\nHere are some sample `curl` queries for each event type. Do note that you need to insert your API key into the `api_key` field.\nAdditionally, if you're self-hosting, you'll have to substitute `https://app.posthog.com/` for the URL of your instance.\nAlias\n`bash\ncurl -v -L --header \"Content-Type: application/json\" -d '{\n    \"api_key\": \"<ph_project_api_key>\",\n    \"properties\": {\n        \"distinct_id\": \"123\",\n        \"alias\": \"456\"\n    },\n    \"timestamp\": \"2020-08-16 09:03:11.913767\",\n    \"context\": \"{}\",\n    \"type\": \"alias\",\n    \"event\": \"$create_alias\"\n}' https://app.posthog.com/capture/`\nCapture\n`bash\ncurl -v -L --header \"Content-Type: application/json\" -d '{\n    \"api_key\": \"<ph_project_api_key>\",\n    \"properties\": {},\n    \"timestamp\": \"2020-08-16 09:03:11.913767\",\n    \"context\": {},\n    \"distinct_id\": \"1234\",\n    \"type\": \"capture\",\n    \"event\": \"$event\",\n    \"messageId\": \"1234\"\n}' https://app.posthog.com/capture/`\nIdentify\n`bash\ncurl -v -L --header \"Content-Type: application/json\" -d '{\n    \"api_key\": \"<ph_project_api_key>\",\n    \"timestamp\": \"2020-08-16 09:03:11.913767\",\n    \"context\": {},\n    \"type\": \"screen\",\n    \"distinct_id\": \"1234\",\n    \"$set\": {},\n    \"event\": \"$identify\",\n    \"messageId\": \"123\"\n}' https://app.posthog.com/capture/`\nPage\n`bash\ncurl -v -L --header \"Content-Type: application/json\" -d '{\n    \"api_key\": \"<ph_project_api_key>\",\n    \"properties\": {},\n    \"timestamp\": \"2020-08-16 09:03:11.913767\",\n    \"category\": \"some category\",\n    \"context\": {},\n    \"distinct_id\": \"1234\",\n    \"type\": \"page\",\n    \"event\": \"$page\",\n    \"name\": \"a page\",\n    \"messageId\": \"123\"\n}' https://app.posthog.com/capture/`\nScreen\n`bash\ncurl -v -L --header \"Content-Type: application/json\" -d '{\n    \"api_key\": \"<ph_project_api_key>\",\n    \"properties\": {},\n    \"timestamp\": \"2020-08-16 09:03:11.913767\",\n    \"category\": \"some category\",\n    \"context\": {},\n    \"distinct_id\": \"1234\",\n    \"type\": \"screen\",\n    \"event\": \"$screen\",\n    \"name\": \"a page\",\n    \"messageId\": \"123\"\n}' https://app.posthog.com/capture/`\nResponses\nStatus code: 200\nResponses\n`js\n{\n    status: 1\n}`\nMeaning: A `200: OK` response means we have successfully received the payload, it is in the correct format, and the project API key (token) is valid. It does not imply that events are valid and will be ingested. As mentioned under Invalid events, certain event validation errors may cause an event not to be ingested.\nStatus code: 400\nResponses\n`js\n{\n    type: 'validation_error',\n    code: 'invalid_project',\n    detail: 'Invalid Project ID.',\n    attr: 'project_id'\n}`\nMeaning: We were unable to determine the project to associate the events with.\nStatus code: 401\nResponses\n`js\n{\n    type: 'authentication_error',\n    code: 'invalid_api_key',\n    detail: 'Project API key invalid. You can find your project API key in PostHog project settings.',\n}`\nMeaning: The token/API key you provided is invalid.\n\n`js\n{\n    type: 'authentication_error',\n    code: 'invalid_personal_api_key',\n    detail: 'Invalid Personal API key.',\n}`\nMeaning: The personal API key you used for authentication is invalid.\nStatus code: 503 (Deprecated)\nResponses\n`js\n{\n    type: 'server_error',\n    code: 'fetch_team_fail',\n    detail: 'Unable to fetch team from database.'\n}`\nMeaning: (Deprecated) This error will only occur in self-hosted Postgres instances if the database becomes unavailable. On ClickHouse-backed instances database failures cause events to be added to a dead letter queue, from which they can be recovered.\nInvalid events\nWe perform basic validation on the payload and project API key (token), returning a failure response if an error is encountered.\nHowever, we will not return an error to the client when the following happens:\n\nAn event does not have a name\nAn event does not have the `distinct_id` field set\nThe `distinct_id` field of an event has an empty value\n\nThe three cases above will cause the event to not be ingested, but you will still receive a `200: OK` response from us.\nThis approach allows us to process events asynchronously if necessary, ensuring reliability and low latency for our event ingestion endpoints.\nFeature flags\nPostHog offers support for feature flags, and you can use our APIs to create and make use of feature flags. However, it is important to note that while creating a feature flag is a private action that only your team should be able to perform, checking if a feature flag is active is not.\nAs such, to create feature flags, you will need to use another endpoint, which we're currently still documenting. However, to check if a feature flag is enabled, you can use the following endpoint:\n/decide\n`/decide` is the endpoint used to determine if a given flag is enabled for a certain user or not. This endpoint is used by our JavaScript Library's methods for feature flags.\nTo get the feature flags that are enabled for a given user, you will need to perform the following request:\n`shell\nPOST <ph_instance_address>/decide/ # e.g. https://posthog.yourcompany.com for self-hosted users\nContent-Type: application/json\nBody:\n{\n    \"api_key\": \"<ph_project_api_key>\",\n    \"distinct_id\": \"[user's distinct id]\"\n}`\nExample request & response: /decide v2\n`/decide` version 2 introduces support for multivariate feature flags and has a slightly different schema for the response. `posthog-js` version 1.13 and up will use version 2 of the decide endpoint by default.\nRequest\n`shell\ncurl -v -L --header \"Content-Type: application/json\" -d '  {\n    \"api_key\": \"<ph_project_api_key>\",\n    \"distinct_id\": \"1234\",\n    \"groups\" : { \n        \"<groupType>\": \"<groupKey>\"\n    }\n}' https://app.posthog.com/decide?v=2`\nResponse\n`shell\n{\n  \"config\": {\n    \"enable_collect_everything\": true\n  },\n  \"editorParams\": {},\n  \"isAuthenticated\": false,\n  \"supportedCompression\": [\n    \"gzip\",\n    \"lz64\"\n  ],\n  \"featureFlags\": {\n    \"my-awesome-flag\": true,\n    \"my-awesome-flag-2\": true,\n    \"my-multivariate-flag\": \"some-string-value\"\n  }\n}`\nExample request & response: /decide v1 (legacy)\n`/decide` version 1 is still the default if the query parameter `v` is not specified, although the latest `posthog-js` library no longer uses version 1 of the decide endpoint, and we recommend using version 2 as described above.\nRequest\n`shell\ncurl -v -L --header \"Content-Type: application/json\" -d '  {\n    \"api_key\": \"<ph_project_api_key>\",\n    \"distinct_id\": \"1234\"\n}' https://app.posthog.com/decide?v=1`\nResponse\n`shell\n{\n  \"config\": {\n    \"enable_collect_everything\": true\n  },\n  \"editorParams\": {},\n  \"isAuthenticated\": false,\n  \"supportedCompression\": [\n    \"gzip\",\n    \"lz64\"\n  ],\n  \"featureFlags\": [\n    \"my-awesome-flag-1\",\n    \"my-awesome-flag-2\",\n    \"my-multivariate-flag\"\n  ]\n}`\nFrom this response, if you are looking to use feature flags in your backend, you will most likely need only the values for the `featureFlags` key, which indicate what flags are on for the user with the distinct ID you provided. These flags persist for users (unless you change your flag settings), so you can cache them rather than send a request to the endpoint each time if you so wish.\nNote that if a multivariate feature flag is enabled for a given user, it will still show up in `featureFlags` under decide version 1, but its value will only be accessible when using decide version 2.\nReading data from PostHog\nWe have another set of APIs to read/modify anything in PostHog. See our API documentation for more information.",
    "tag": "posthog"
  },
  {
    "title": "Authentication",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/api/index.mdx",
    "content": "\ntitle: API\nsidebarTitle: Overview\nsidebar: Docs\nshowTitle: true\n\nThis section of our Docs explains how to pull or push data from/to our API. PostHog has an API available on all tiers of PostHog cloud pricing, including the free tier, and for every self-hosted version.\nPlease note that PostHog makes use of two different APIs, serving different purposes and using different mechanisms for authentication.\nOne API is used for pushing data into PostHog. This uses the 'Team API Key' that is included in the frontend snippet. This API Key is public, and is what we use in our frontend integration to push events into PostHog, as well as to check for feature flags, for instance.\nThe other API is more powerful and allows you to perform any action as if you were an authenticated user utilizing the PostHog UI. It is mostly used for getting data out of PostHog, as well as other private actions such as creating a feature flag. This uses a 'Personal API Key' which you need to create manually (instructions below). This API Key is private and you should not make it public nor share it with anyone. It gives you access to all the data held by your PostHog instance, which includes sensitive information.\nThese API Docs refer mostly to the private API, performing authentication as outlined below. The only exception is the POST-only public endpoints section. This section explicitly informs you on how to perform authentication. For endpoints in all other sections, authentication is done as described below.\nAuthentication\nPersonal API keys (Recommended)\nPersonal API keys allow full access to your account, just like e-mail address and password, but you can create any number of them and each one can invalidated individually at any moment. This makes for greater control for you and improved security of stored data.\nHow to obtain a personal API key\n\nClick on your name/avatar on the top right.\nClick the gear next to your name to access 'Account settings'.\nNavigate to the 'Personal API Keys' section.\nClick \"+ Create a Personal API Key\".\nGive your new key a label \u2013 it's just for you, usually to describe the key's purpose.\nClick 'Create Key'.\nThere you go! At the top of the list you should now be seeing your brand new key. Immediately copy its value, as you'll never see it again after refreshing the page. But don't worry if you forget to copy it \u2013 you can delete and create keys as much as you want.\n\nHow to use a personal API key\nThere are three options:\n\nUse the `Authorization` header and `Bearer` authentication, like so:\n    `js\n    const headers = {\n        Authorization: `Bearer ${POSTHOG_PERSONAL_API_KEY}`\n    }`\nPut the key in request body, like so:\n    `js\n    const body = {\n        personal_api_key: POSTHOG_PERSONAL_API_KEY\n    }`\nPut the key in query string, like so:\n    `js\n    const url = `https://posthog.example.com/api/event/?personal_api_key=${POSTHOG_PERSONAL_API_KEY}``\n\nAny one of these methods works, but only the value encountered first (in the order above) will be used for authentication!\nFor PostHog Cloud, use `app.posthog.com` as the host address.\nSpecifying a project when using the API\nBy default, if you're accessing the API, PostHog will return results from the last project you visited in the UI. To override this behavior, you can pass in your Project API Key (public token) as a query parameter in the request. This ensures you will get data from the project associated with that token.\nExample\n`api/event/?token=my_project_api_key`\ncURL example for self-hosted PostHog\n`bash\nPOSTHOG_PERSONAL_API_KEY=qTjsppKJqYLr2YskbsLXmu46eW1oH0r3jZkmKaERlf0\ncurl \\\n--header \"Authorization: Bearer $POSTHOG_PERSONAL_API_KEY\" \\\nhttps://posthog.example.com/api/person/`\ncURL example for PostHog Cloud\n`bash\nPOSTHOG_PERSONAL_API_KEY=qTjsppKJqYLr2YskbsLXmu46eW1oH0r3jZkmKaERlf0\ncurl \\\n--header \"Authorization: Bearer $POSTHOG_PERSONAL_API_KEY\" \\\nhttps://app.posthog.com/api/person/`\nTips\n\nThe /users/@me/ endpoint gives you useful information about the current user.\nThe `/api/event_definition/` and `/api/property_definition` endpoints provide the possible event names and properties you can use throughout the rest of the API.\nThe maximum size of a POST request body is governed by `settings.DATA_UPLOAD_MAX_MEMORY_SIZE`, and is 20MB by default.\n\nPagination\nSometimes requests are paginated. If that's the case, it'll be in the following format:\n`json\n{\n    \"next\": \"https://posthog.example.com/api/person/?cursor=cD0yMjgxOTA2\",\n    \"previous\": null,\n    \"results\": [\n        ...\n    ]\n}`\nYou can then just call the `\"next\"` URL to get the next set of results.\nRate limiting\nAll requests to PostHog, except for evaluating feature flags and sending events (i.e. the POST-only public endpoints), are rate limited.\nThere's separate limits for different kinds of resources.\nFor all CRUD endpoints, the rate limits are `480/minute` and `4800/hour`. \nFor all analytics endpoints (such as calculating insights, retrieving persons, retrieving session recordings), the rate limits are `240/minute` and `1200/hour`.\nNote that these limits apply to the entire team, so if you have a feature-flag requesting script that hits the rate limit, and another user, using a different API key, makes just a single request to the persons API, this would get rate limited as well.\nFor large or regular exports of events we would recommend using export apps.\nIf want to use the PostHog API beyond these limits (for example by using PostHog to power your user-facing dashboards) then please email us at customers@posthog.com.\nContributing to API docs\nAPI docs are generated using drf-spectacular. It looks at the Django models and djangorestframework serializers.\nNote: We don't automatically add new API endpoints to the sidebar, so you'll need to add those to sidebar.json\nYou can add a `help_text=\"Field that does x\"` attribute to any Model or Serializer field to help users understand what a specific field is used for:\n```python\nclass Insight(models.Model):\n    last_refresh: models.DateTimeField = models.DateTimeField(blank=True, null=True, help_text=\"When the cache for the result of this insight was last refreshed.\")\nclass InsightSerializer(TaggedItemSerializerMixin, InsightBasicSerializer):\n    filters_hash = serializers.CharField(\n        read_only=True,\n        help_text=\"A hash of the filters that generate this insight.\",\n    )\n```\nTo add a description to the top of a page, add a comment to the viewset class:\n`python\nclass InsightViewSet(TaggedItemViewSetMixin, StructuredViewSetMixin, viewsets.ModelViewSet):\n    \"\"\"\n    Stores saved insights along with their entire configuration options. Saved insights can be stored as standalone\n    reports or part of a dashboard.\n    \"\"\"`\nYou can do the same thing for specific endpoints.\n`python\n@action(methods=[\"GET\", \"POST\"], detail=False)\ndef trend(self, request: request.Request, *args: Any, **kwargs: Any) -> Response:\n    \"\"\"\n    Test comment, which [even supports markdown](https://example.com)\n    \"\"\"`\nTo check what any changes will roughly look like locally, you can go to http://127.0.0.1:8000/api/schema/redoc/.\nInsights serializer\nThe serializer for insight lives here. Each time an insight gets created we check it against these serializers, and we'll send an error to Sentry (but not the user) if it doesn't match, to ensure the API docs are up to date.\nDocumenting custom endpoints\nIf you have an `@action` endpoint or a custom endpoint (that doesn't use DRF) you can still document by providing a serializer for the request and response.\n`python\nfrom drf_spectacular.utils import OpenApiResponse\nfrom posthog.api.documentation import extend_schema\n@extend_schema(\n    request=FunnelSerializer,\n    responses=OpenApiResponse(\n        response=FunnelStepsResultsSerializer,\n        description=\"Note, if funnel_viz_type is set the response will be different.\",\n     ),\n    methods=[\"POST\"],\n    tags=[\"funnel\"],\n    operation_id=\"Funnels\",\n)\n@action(methods=[\"GET\", \"POST\"], detail=False)\ndef funnel(self, request: request.Request, *args: Any, **kwargs: Any) -> Response:`\nTesting API docs locally\nTo test or develop the API docs locally, you need to create a personal API key (see top of this page) and then export it before running gatsby, in the same terminal window:\n```bash\nexport POSTHOG_PERSONAL_API_KEY=yourkey",
    "tag": "posthog"
  },
  {
    "title": "filter-out.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/filter-out.md",
    "content": "\ntitle: Filter Out\ngithub: https://github.com/PostHog/posthog-filter-out-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=Filter%Out%20Plugin\nthumbnail: ../../apps/thumbnails/filter-out.png\ntags:\n    - filter-out\n\nWhat does the Filter Out app do?\nThis app enables you to create filters which prevent PostHog from ingesting data from your product unless it passes those filters. Any events which do not match the filter requirements are skipped over and are not ingested. \nWhat are the requirements for this app?\nUsing the Filter Out app requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nHow do I install the Filter Out app?\n\nVisit the 'Apps' page in your instance of PostHog.\nSearch for 'Filter Out' and select the app, press Install.\nFollow the on-screen steps to configure the app.\n\nThis app will only work on events ingested after the app was enabled.\nHow do I configure the Filter Out app?\nThe app can be configured via a JSON file which specifies the properties you would like to filter.\nIn the example config below, the app will only keep events where all of the following conditions are met:\n\nEmail does not contain yourcompany.com\nHost is not localhost:8000\nBrowser version greater than 100\n\n`[\n  {\n    \"property\": \"email\",\n    \"type\": \"string\",\n    \"operator\": \"not_contains\",\n    \"value\": \"yourcompany.com\"\n  },\n  {\n    \"property\": \"host\",\n    \"type\": \"string\",\n    \"operator\": \"is_not\",\n    \"value\": \"localhost:8000\"\n  },\n  {\n    \"property\": \"browser_version\",\n    \"type\": \"number\",\n    \"operator\": \"gt\",\n    \"value\": 100\n  }\n]`\nThe followed types and operators are allowed:\n| Type    | Operators                                            |\n| ------- | ---------------------------------------------------- |\n| number  | gt, gte, lt, lte, eq, neq                            |\n| string  | is, is_not, contains, not_contains, regex, not_regex |\n| boolean | is, is_not                                           |\nWho maintains this app?\nThis app was created by @plibither8 and is maintained by the community.  If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "hubspot-connector.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/hubspot-connector.md",
    "content": "\ntitle: Hubspot Connector\ngithub: https://github.com/PostHog/hubspot-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=Hubspot\nthumbnail: ../../apps/thumbnails/hubspot.svg\ntags:\n    - hubspot\n\nWhat is Hubspot?\nHubspot is a full-featured marketing and CRM platform which includes tools for everything from managing inbound leads to building landing pages. As one of the world\u2019s most popular CRM platforms, Hubspot is an essential PostHog integration for many organizations \u2014 and is especially popular with marketing teams.\nWhat does the Hubspot Connector do?\nThe Hubspot Connector for PostHog sends data from PostHog to Hubspot whenever an `$identify` event occurs. That is, whenever PostHog successfully identifies a user. This is useful for syncing customer information between both PostHog and Hubspot.\nWhat are the requirements for this app?\nThe Hubspot Connector requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nYou'll also need a Hubspot account to connect to.\nWhat information can I push to Hubspot from PostHog?\nCurrently, this integration supports sending the following data to Hubspot:\n\nEmail addresses\nFirst names\nLast names\nPhone numbers\nCompany names\nCompany website URLs\n\nThis information can be sent whenever an `$identify` event occurs in PostHog.\nNo other information can currently be sent to PostHog using this app. If this app exists in a chain where the above information would be filtered out (for example, by using the Property Filter app) then filtered information cannot be sent to Hubspot.\nConfiguration\n\nHow do I install the Hubspot app on PostHog?\n\nLog in to your PostHog instance\nClick 'Apps' on the left-hand tool bar\nSearch for 'Hubspot'\nSelect the app, press 'Install' and follow the on-screen instructions\n\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the Hubspot Connector is available on GitHub.\nWho created this app?\nA lot of people contributed to this app! We'd like to thank...\n\nKunal\nYakko Majuri\nMarcus Hyett\nMarius Andra\nJoe Martin\nPaul D'Ambra and\nOneshotEngineering\n\nFor creating the Hubspot Connector for PostHog. Thanks, all!\nWho maintains this app?\nThis app is maintained by PostHog. If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "user-interview.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/user-interview.md",
    "content": "\ntitle: User Interviewer\ngithub: https://github.com/PostHog/user-interview-app\ninstallUrl: https://app.posthog.com/project/apps?name=user-interview\nthumbnail: ../../apps/thumbnails/user-interview.png\ntags:\n    - user-interview\n\nWhat does the User Interviewer app do?\nThis app enables you to send a popup notification to targeted users that invites them to schedule an interview to provide feedback. This app was created by PostHog's product team and is something we use regularly to collect community feedback. \nWhat are the requirements for this app?\nThis app requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nHow do I install this app?\n\nMake sure you have enabled `opt_in_site_apps: true` in your posthog-js config.\nInstall the app from the PostHog App Repository\nCustomize the text and theme using the app config\nConfigure the app by creating a feature flag (see below)\n\nThat's it!\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for this app is available on GitHub.\nHow do I configure the app?\n\nCreate a feature flag in your PostHog account to control who sees the interview request pop-up. For this example, we'll use the name `user-interview`\nSet up the filters for the people that you want to speak to. You can create a filter based on properties, such as location or email, but not events. If you want to invite users based on them doing certain actions, you'll need to update the users' property once they've done that action.\nAdditionally, if you want to invite users based on historical actions, create an insight for that action and then use that to create a static cohort.\nSet the filter `Seen User Interview Invitation - {FEATURE_FLAG_NAME}` to `is not set` so that it doesn't show to users who have seen the user interview already. This property is added once the user has interacted with the popup - either to close it or to book in a time.\nAdd an autorollback based on the pageview where `Current URL` contains `bookedUserInterviewEvent={FEATURE_FLAG_NAME}`. This is where you'll redirect them after they've booked in. Set the average over last 7 days to be 1 for up to 7 interviews to be booked and 2 for up to 14 interviews to be booked.\nThis requires the `auto-rollback-feature-flags` feature flag to be turned on. Please be aware this is currently in beta. \nCreate your Calendly event (or other booking link) and\nset the redirect after booking to be `{Your app}?bookedUserInterviewEvent={FEATURE_FLAG_NAME}`, e.g. `https://app.posthog.com/home?bookedUserInterviewEvent=user-interview`\nAdd the feature flag and booking link to the app config `interviewConfigs` (you can have multiple feature flags with corresponding booking links by separating them with commas e.g. `interview-high-icp=https://calendly.com/user1/book-high-icp,user-interview=https://calendly.com/user1/user-interview`).\nRollout out the feature flag\n\nBy default the flags won't be shown to users who have seen a user interview popup within the last 90 days. You can override this with with `minDaysSinceLastSeenPopUp`\nWhat new properties and events does this app add?\nPlease check the Readme in the GitHub repo for a full list. \nWho created this app?\nWe'd like to thank PostHog team member Luke Harries for creating this app.\nWho maintains this app?\nThis app is maintained by PostHog. If you have issues with the app not functioning as intended, please raise a bug report.\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "pagerduty-connector.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/pagerduty-connector.md",
    "content": "\ntitle: PagerDuty Connector\ngithub: https://github.com/PostHog/posthog-pagerduty-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=PagerDuty\nthumbnail: ../../apps/thumbnails/pagerduty.svg\ntags:\n    - pagerduty connector\n\nWhat does the PagerDuty Connector do?\nThis app alerts PagerDuty when a PostHog insights/trends graph goes below or above a threshold.\nExample use cases:\n\nAlert when there is no $pageviews captured on my site the past hour,\nAlert when the rate of $billing_error events crosses a threshold.\n\nWhat are the requirements for this app?\nThe PagerDuty Connector app requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.26.0 or later.\nNot running 1.26.0? Find out how to update your self-hosted PostHog deployment!\nYou'll also need access to a PagerDuty account.\nHow do I install the PagerDuty Connector for PostHog?\nThis app requires PostHog 1.26.0 or above, or PostHog Cloud.\n\nLog in to your PostHog instance\nClick 'Apps' on the left-hand tool bar\nSearch for 'PagerDuty'\nSelect the app, press 'Install' and follow the on-screen instructions\n\nHow do I configure the PagerDuty Connector?\n\nGet the trends URL.\nGo to Insights.\nConstruct the Trends query you want to alert on. It must be a single-line trend graph.\nCopy the URL.\nChoose threshold and operator (less than or equal, greater than or equal).\nEnter PagerDuty service integration key (for Events API v2).\n\nConfiguration\n\nAre there any limitations?\nThis PagerDuty Connector only works when reading from single-line trend graphs.\nIt also requires PostHog 1.26.0 or above, or PostHog Cloud.\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the PagerDuty connector is available on GitHub.\nWho created this app?\nWe'd like to thank PostHog team members Yakko Majuri and Michael Matloka and Karl-Aksel Puulmann for creating the Currency Normalizer. Thank you, all!\nWho maintains this app?\nThis app is maintained by PostHog. If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "sendgrid-connector.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/sendgrid-connector.md",
    "content": "\ntitle: Sendgrid Connector\ngithub: https://github.com/PostHog/sendgrid-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=Sendgrid\nthumbnail: ../../apps/thumbnails/sendgrid-connector.png\ntags:\n    - sendgrid-connector\n\nWhat does the Sendgrid Connector app do?\nThe Sendgrid Connector sends event and emails data from PostHog into Sendgrid whenever a user is identified in PostHog.\nWhat are the requirements for this app?\nThe Sendgrid Connector requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nYou'll also need Sendgrid access, obviously.\nHow do I install the Sendgrid Connector app?\n\nVisit the 'Apps' page in your instance of PostHog.\nSearch for 'Sendgrid' and select the app, press Install.\nAdd your Sendgrid API key at the configuration step.\nEnable the app and watch your contacts list get populated in Sendgrid!\n\nConfiguration\n\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the Sendgrid Connector is available on GitHub.\nWho created this app?\nWe'd like to thank PostHog team members Yakko Majuri and Marius Andra, as well as and community member Jose Fuentes Castillo for creating the Sendgrid Connector. Thank you, all!\nWho maintains this app?\nThis app is maintained by PostHog. If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "stripe-connector.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/stripe-connector.md",
    "content": "\ntitle: Stripe Connector\ngithub: https://github.com/posthog/stripe-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=stripe\nthumbnail: ../../apps/thumbnails/stripe-connector.png\ntags:\n    - stripe-connector\n\nWhat does the Stripe Connector do?\nStripe is a payments platform used by millions of companies. The Stripe Connector enables you to get information about revenue and payments from Stripe, into PostHog. \nOnce revenue information is in PostHog, you can analyze it just like any other PostHog event using insights and dashboards. \nHow do I install the Stripe Connector?\n\nLog in to your PostHog instance\nClick 'Apps' on the left-hand tool bar\nSearch for 'Stripe'\nSelect the app, press 'Install' and follow the configuration instructions accessed by pressing the cog button.\n\nWhat are the requirements for this app?\nThe Stripe Connector requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the Stripe Connector is available on GitHub.\nWho created this app?\nWe'd like to thank PostHog team members Yakko Majuri, Tim Glaser, Kunal Pathak and Paul Hultgren for creating this app.\nWho maintains this app?\nThis app is maintained by PostHog. If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "bitbucket-release-tracker.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/bitbucket-release-tracker.md",
    "content": "\ntitle: BitBucket Release Tracker\ngithub: https://github.com/PostHog/bitbucket-release-tracker\ninstallUrl: https://app.posthog.com/project/apps?name=Bitbucket+Release+Tracker\nthumbnail: ../../apps/thumbnails/bitbucket-release-tracker.png\ntags:\n    - bitbucket-release-tracker\n\nHow do I install the BitBucket Release Tracker PostHog app?\nTo install this app, follow these simple steps...\n\nLog in to your PostHog instance\nClick 'Apps' on the left-hand tool bar\nSearch for 'BitBucket'\nSelect the BitBucket Release Tracker app, press 'Install'\n\nAnd follow the on-screen instructions to complete configuration.\nWhat are the requirements for this app?\nThe BitBucket Release Tracker requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nConfiguration\n\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the Bitbucket Release Tracker is available on GitHub.\nWho created this app?\nWe'd like to thank PostHog team members Yakko Majuri and Michael Matloka for creating the Bitbucket Release Tracker.\nWho maintains this app?\nThis app is maintained by PostHog. If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "What does the Amazon Kinesis Import app do?",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/amazon-kinesis.md",
    "content": "\ntitle: Amazon Kinesis Import\ngithub: https://github.com/posthog/posthog-kinesis-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=Kinesis\nthumbnail: ../../apps/thumbnails/kinesis.png\ntags:\n    - amazon-kinesis\n\nWhat does the Amazon Kinesis Import app do?\nThis app imports event data into PostHog from an Amazon Kinesis stream. Kinesis Records must be delivered in a JSON schema in order to be imported.\nWhat are the requirements for this app?\nUsing the Amazon Kinesis Import app requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nHow should I configure the Kinesis Record schema?\nKinesis Records must be delivered in a JSON schema.\nYou need to configure an `eventKey` that maps to the event name in PostHog. The `eventKey` can refer to a nested key.\nYou can optionally configure a comma-separated list of `additionalPropertyMappings`, that will map Kinesis Record keys to PostHog Event properties. The Kinesis Record keys can be nested keys, while the corresponding PostHog mapped keys cannot be nested.\nFor example, take the following Kinesis Record\n`// Kinesis Record\n{\n    ...\n    \"properties: {\n        \"eventName\": \"my posthog event\",\n        \"userId\": \"$userId\",\n        \"foo\": \"bar\"\n    }\n}`\nAnd the following configuration:\n`eventKey = properties.eventName\nadditionalPropertyMappings = properties.userId:distinct_id,properties.foo:foo`\nWill be parsed as:\n`// PostHog Event\n{\n    \"event\": \"my posthog event\",\n    \"properties: {\n        \"distinct_id\": \"$userId\",\n        \"foo\": \"bar\"\n    }\n}`\nWhat is the correct IAM policy?\nYou need to provide an AccessKeyID and a SecretAccessKey for a AWS IAM user with at least the following Kinesis Action rights:\n`DescribeStream\nGetShardIterator\nGetRecords`\nWhat app parameters are available?\n\nHow do I install the Amazon Kinesis Import app for PostHog?\n\nVisit the 'Apps' page in your instance of PostHog.\nSearch for 'AWS Kinesis' and select the app, press Install.\nFollow the steps to configure the app.\nWatch events roll in to PostHog.\n\nFurther information\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the Amazon Kinesis Import app is available on GitHub.\nWhere can I find out more?\nCheck Amazon's Kinesis documentation for more information on using Amazon Kinesis.\nWho created this app?\nWe'd like to thank PostHog team member Emanuele Capparelli for his work creating this app. Thank you, Emanuele!\nWho maintains this app?\nThis app is maintained by the community. If you have issues with the app not functioning as intended, please raise a bug report on the repo to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "How do I configure the Schema Enforcer?",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/schema-enforcer.md",
    "content": "\ntitle: Schema Enforcer\ngithub: https://github.com/PostHog/schema-enforcer-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=Schema%20Enforcer\nthumbnail: ../../apps/thumbnails/schema-enforcer.png\ntags:\n    - schema-enforcer\n\nHow does this app work?\nThe app enables you to enforce a schema on events in PostHog as they are ingested. It does this by preventing ingestion if the event:\n\nIs missing a required property\nHas a property with the wrong type\nIs not included in the file and `onlyIngestEventsFromFile` is true\n\nIt also:\n\nRemoves all other properties from an event except selected ones if acceptOnlySchemaProps is true\nConfiguration is done via a JSON file uploaded as an attachment\n\nWhat are the requirements for this app?\nThe Schema Enforcer app requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nHow do I install the Schema Enforcer for PostHog?\n\nLog in to your PostHog instance\nClick 'Apps' on the left-hand tool bar\nSearch for 'Schema Enforcer'\nSelect the app, press 'Install' and follow the on-screen instructions\n\nHow do I configure the Schema Enforcer?\nBelow is an example configuration file:\n`{\n    \"onlyIngestEventsFromFile\": true,\n    \"eventSchemas\": {\n        \"testEvent\": {\n            \"acceptOnlySchemaProps\": true,\n            \"schema\": {\n                \"foo\": {\n                    \"type\": \"string\",\n                    \"required\": false\n                },\n                \"bar\": {\n                    \"type\": \"number\",\n                    \"required\": true\n                },\n                \"baz\": {\n                    \"type\": \"boolean\",\n                    \"required\": false\n                }\n            }\n        }\n    }\n}`\nConfiguration\n\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the Schema Enforcer is available on NPM.\nWho created this app?\nA lot of people worked on this app! We'd like to thank the following PostHog team members...\n\nYakko Majuri\nMarius Andra\nJames Greenhill\nMichael Matloka\nAlex Kim\nKarl Aksel-Puulmann and\nTim Glaser\n\nFor creating the Schema Enforcer. Thank you, all!\nWho maintains this app?\nThis app is maintained by PostHog. If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "intercom.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/intercom.md",
    "content": "\ntitle: Intercom Connector\ngithub: https://github.com/posthog/posthog-intercom-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=Intercom\nthumbnail: ../../apps/thumbnails/intercom.png\ntags:\n    - intercom\n\nWith the Intercom Connector, you can send event data from PostHog to Intercom whenever an event matches a user who has been identified by their email address.\nWhat are the requirements for this app?\nUsing this app requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nInstallation\n\nVisit the 'Apps' page in your instance of PostHog.\nSearch for 'Intercom' and select the app, press 'Install'.\nFollow the steps below to configure the app.\n\n\nImportant: Only events that have an `email` property will be sent to Intercom. For more information on how to configure this, take a look at this section.\n\nConfiguration\nAfter you've pressed 'Install', you need to add your Intercom API key at the configuration step, as well as add triggering events you want to send to Intercom.\n\nIntercom API Key (required): you can get this one from the Intercom Developer Hub, by creating a new app and receiving an API Key\nTriggering events (required): A comma-separated list of PostHog events you want to send to Intercom (e.g.: `$identify,mycustomevent` ).\n\nAdditional configuration\n\nSetting up tracking\nIn order for your events to show up in Intercom, they need to have an `email` property so we know which user to connect them to.\nThe easiest way to do this is with Super Properties, which will add an `email` property on every event.\nAdd the following code wherever you make an `identify` call with the current user's email address.\n`js\nposthog.register({\n    email: 'hello@posthog.com'\n})`\nThis will then send this as a property on all future events, including autocaptured events.\n\nNote: Make sure to call `posthog.unregister('email')` whenever a user logs out to clear this property\n\nCurrently, Super Properties are only available in the `posthog-js` library or when using the PostHog snippet.\nIf you are using a different SDK, you'll need to manually the `email` property for every event that you want to send to Intercom.\nFurther information\nWho created this app?\nWe'd like to thank PostHog team member Emanuele Capparelli for his work creating this app. Thank you, Emanuele!\nWho maintains this app?\nThis app is maintained by the community. If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "twilio.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/twilio.md",
    "content": "\ntitle: Twilio Connector\ngithub: https://github.com/PostHog/posthog-twilio-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=posthog-twilio-plugin\nthumbnail: ../../apps/thumbnails/twilio.png\ntags:\n    - twilio\n\nWhat does the Twilio Connector app do?\nThis app triggers SMS messages in Twilio when specified events or actions are detected in PostHog.\nYou can set a timeout period of between 1 second and 31536000 seconds (1 calendar year) to avoid accidentally spamming users with too many messages.\nWhat are the requirements for this app?\nThis app requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nHow do I install this app for PostHog?\n\nVisit the 'Apps' page in your instance of PostHog.\nSearch for 'Twilio' and select the app, press 'Install'.\nFollow the steps below to configure the app.\n\nHow do I configure the Twilio Connector app for PostHog?\nTo configure this app, you will need your Account SID and Auth Token from Twilio. You can find these in Twilio in your account menu under `Account > Keys & Credentials > API Keys and tokens > Live credentials`.\nAdditionally, you will need to know your Twilio Phone Number. Follow Twilio's documentation for buying or finding your Twilio phone numbers.\nConfiguration\n\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the Twilio Connector app is available on GitHub.\nWho created this app?\nWe'd like to thank community members Sandeep Guptan and Himanshu Garg for their work creating this app. Thank you, both!\nWhere can I find out more?\nCheck Twilio's documentation for more information on connecting Twilio with other services.\nWho maintains this app?\nThis app is maintained by the community. If you have issues with the app not functioning as intended, please raise a bug report on the repo to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "timestamp-parser.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/timestamp-parser.md",
    "content": "\ntitle: Timestamp Parser\ngithub: https://github.com/PostHog/timestamp-parser-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=Timestamp%20Parser\nthumbnail: ../../apps/thumbnails/timestamp-parser.png\ntags:\n    - timestamp-parser\n\nThis app parses the timestamp of each event that comes in to PostHog and adds the following time-based properties:\n| Property          | Description                                       | Example    |\n| ----------------- | ------------------------------------------------- | ---------- |\n| `day_of_the_week` | Plain text value for the day of the week          | Monday     |\n| `day`             | Numeric value for the day within a month          | 7          |\n| `month`           | Numeric value corresponding to the current month  | 6 (June) |\n| `year`            | Numeric value of the year                         | 2022       |\n| `hour`            | Numeric value for the hour in UTC (24-hour clock) | 21         |\n| `minute`          | Numeric value for the minute                      | 37         |\nExample event\nHere is an example of what these properties look like after they have been added to an event.\n\nInstallation\nPostHog Cloud\nPostHog Cloud users can find the app here and click on the toggle to enable the app.\nOnce the app has been enabled, it will automatically start parsing all new events which come in to PostHog.\nPostHog Self-hosted\n\nThe Timestamp Parser requires a PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update.\n\n\nVisit the 'Apps' page in your instance of PostHog.\nSearch for 'Timestamp Parser'.\nClick on the toggle to enable the app.\n\nAny new events that come in to PostHog will now be automatically parsed!\nUsing the Timestamp Parser\nThe timestamp parser is a great tool for answering time-based questions that are sometimes very challenging to tackle with PostHog alone.\nBy filtering and breaking down events, we can now easily answer questions such as:\n\nDo we get more purchases on weekdays or weekends?\nWhy does our traffic spike on Tuesdays?\nHow do users use our platform differently during the holiday season?\nHow does retention compare for users who join on a weekend versus a weekday?\n\n\nNote:  This app only works on new events sent to PostHog, and as a result you won't be able to filter events that were sent before it was enabled.\n\nExamples\nHere's an example of creating a filter in a trends insight to only show events that were send on a Saturday or Sunday.\n\nWe can also break down an insight by `month` to get an idea of how it varies over the course of a year.\n\nOverall, the timestamp parser is a simple yet incredibly powerful app that these examples only scratch the surface on.\nHave a question?\nWe love answering questions. Ask us anything via our Support page or using the Q&A widget at the bottom of this page.\nYou can also join the PostHog Community Slack group to collaborate with others and get advice on developing your own PostHog apps.\nFurther information\nWho created this app?\nWe'd like to thank PostHog team member Yakko Majuri and community member Victor Campuzano for creating the Timestamp Parser. Thank you, both!\nWhat if I have feedback on this app?",
    "tag": "posthog"
  },
  {
    "title": "zendesk-connector.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/zendesk-connector.md",
    "content": "\ntitle: Zendesk Connector\ngithub: https://github.com/PostHog/posthog-zendesk-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=posthog-zendesk-plugin\nthumbnail: ../../apps/thumbnails/zendesk.svg\ntags:\n    - zendesk-connector\n\nWhat does the Zendesk Connector app do?\nThe Zendesk Connector app can import new and historic ticket events to PostHog. However, only the Date Type User Field is currently supported.\nWhat are the requirements for this app?\nThe Zendesk Connector requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nYou'll also need a Zendesk account, with admin access.\nAre there any limitations?\nThe ZenDesk API has a limit of 400hits/min. If you have higher ingestion than that, please contact Zendesk.\nHow do I install the Zendesk Connector app?\nMake sure to use your Zendesk Admin Account to perform the below activities.\n\nHead Over to Zendesk's Admin section -> Settings -> Account.\nIn the Branding section, scroll down to Subdomain and find your subdomain there.\nHead to the Admin Section -> Channels -> API.\nIn Settings, follow the below steps:\nTurn On Token Access.\nClick on Add API Token.\nGive it some name like PostHog.\nCopy the Token(You won't be able to see it later).\nSave the Token.\n\n\n\nNext, Head to the Admin section -> Manage -> User Fields. Click Add Fields and follow the steps below.\n\nGive Name\nSelect Type `Date`.\nAdd field key, (you will be required to share this key in PostHog while setting up)\nClick Save.\n\nConfiguration\n\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the Zendesk Connector is available on GitHub.\nWho created this app?\nThis app was created by the community. We'd like to thank Sandeep Guptan and Himanshu Garg for creating the Zendesk connector, as well as for all the other support and feedback. Thank you, both!\nWho maintains this app?\nThis app is maintained by the community. If you have issues with the app not functioning as intended, please raise an issue on the repo.\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "github-star-sync.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/github-star-sync.md",
    "content": "\ntitle: GitHub Star Sync\ngithub: https://github.com/PostHog/github-star-sync-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=Github+Star+Sync\nthumbnail: ../../apps/thumbnails/github.png\ntags:\n    - star-sync\n\nWhat does the GitHub Star Sync app do?\nThe GitHub Star Sync app enables you to track your GitHub stars as an event in PostHog. Because Stars are important!\nWhat are the requirements for this app?\nThe GitHub Star Sync app requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nHow do I install the Redshift Import app?\n\nLog in to your PostHog instance\nClick 'Apps' on the left-hand tool bar\nSearch for 'GitHub Star Sync' press 'Install'\nConfigure the by specifying a GitHub user/repo to sync.\nOptionally, add a personal API key to help with rate limits\n\nConfiguration\n\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the GitHub Star Sync app is available on GitHub.\nWho created this app?\nWe'd like to thank PostHog team members Yakko Majuri, and Marius Andra and Tim Glaser for creating the GitHub Star Sync app.\nWho maintains this app?\nThis app is maintained by PostHog. If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "Installation",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/bigquery-export.md",
    "content": "\ntitle: BigQuery Export\ngithub: https://github.com/PostHog/bigquery-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=BigQuery+Export\nthumbnail: ../../apps/thumbnails/bigquery.svg\ntags:\n    - bigquery-export\n\nThis app streams events from PostHog into BigQuery as they are ingested.\nInstallation\nThe BigQuery Export app requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nEnabling the app\n\nVisit the 'Apps' page from PostHog.\nSearch for 'BigQuery' and select the 'BigQuery Export' app.\nClick on the blue settings icon and follow the configuration steps:\nUpload your Google Cloud key `.json` file. (See below for instructions on how to retrieve this.)\nEnter your Dataset ID\nEnter your Table ID\n\n\nWatch events roll into BigQuery\n\nSetting up BigQuery access\nTo set the right permissions up for the BigQuery plugin, you'll need:\n\nA service account.\nA dataset which has permissions allowing the service account to access it.\n\nHere's how to set these up so that the app has access only to the table it needs:\n\n\nCreate a service account. Keep hold of the JSON file at the end of these steps for setting up the app, and remember the name too.\n\n\nCreate a role which has only the specific permissions the PostHog BigQuery app requires (listed below), or use the built in `BigQuery DataOwner` permission. If you create a custom role, you will need:\n\n`bigquery.datasets.get`\n`bigquery.tables.create`\n`bigquery.tables.get`\n`bigquery.tables.list`\n`bigquery.tables.updateData`\n\n\n\nCreate a dataset within a BigQuery project (ours is called `posthog`, but any name will do).\n\n\nFollow the instructions on granting access to a dataset in BigQuery to ensure your new service account has been granted either the role you created or the \"BigQuery Data Owner\" permission.\n\n\n\nUse the Share Dataset button to share your dataset with your new service account and either the `BigQuery DataOwner` role, or your custom role created above. In the below, we've used a custom role `PostHog Ingest`.\n\nThat's it! Once you've done the steps above, your data should start flowing from PostHog to BigQuery.\nEvent schema\nHere is a summary of all the fields that are exported to BigQuery.\n| Field                 | Type        | Description                                                                             |\n| --------------------- | ----------- | --------------------------------------------------------------------------------------- |\n| uuid                  | `STRING`    | The unique ID of the event within PostHog                                               |\n| event                 | `STRING`    | The name of the event that was sent                                                     |\n| properties            | `STRING`    | A JSON object with all the properties sent along with an event                          |\n| elements              | `STRING`    | Elements surrounding an autocaptured event |\n| set                   | `STRING`    | A JSON object with any person properties sent with the `$set` field                     |\n| set_once              | `STRING`    | A JSON object with any person properties sent with the `$set_once` field                |\n| distinct_id           | `STRING`    | The `distinct_id` of the user who sent the event                                        |\n| team_id               | `STRING`    | The `team_id` for the event                                                             |\n| ip                    | `STRING`    | The IP address that was sent with the event                                             |\n| site_url              | `STRING`    | This is always set as an empty string for backwards compatibility                       |\n| timestamp             | `TIMESTAMP` | The timestamp when the event was ingested into PostHog                                  |\n| bq_ingested_timestamp | `TIMESTAMP` | The timestamp when the event was sent to BigQuery                                       |\nConfiguration\n\nTroubleshooting\nWhat should I do if events aren't showing up?\nThe best way to debug events not showing up is by viewing the logs, which can be accessed by clicking the 'Logs' icon just to the left of the blue settings button.\nThis will bring up a new panel with a list of all the most recent logs from our app.\nTake a look back through the log and see if there are any `ERROR` messages that can help provide more information on why the export is failing.\n\nTip: You can filter down and only view `ERROR` or `WARN` messages using the toggles at the top of the panel next to 'Show logs of type'\n\nWhy am I seeing duplicate PostHog events in BigQuery?\nThere's a very rare case when duplicate events appear in BigQuery. This happens due to network errors, where the export seems to have failed, yet it actually reaches BigQuery.\nWhile this shouldn't happen, if you find duplicate events in BigQuery, follow these Google Cloud docs to manually remove them.\nHere is an example query based on the Google Cloud docs that would remove duplicates:\n`sql\nWITH\n-- first add a row number, one for each uuid\nraw_data AS\n(\n       SELECT *,\n              Row_number() OVER (partition BY uuid) AS row_number\n       FROM   `<project_id>.<dataset>.<TABLE>`\n       WHERE  date(timestamp) = '<YYYY-MM-DD>' ),\n-- now just filter for one row per uuid\nraw_data_deduplicated AS\n(\n       SELECT *\n       EXCEPT (row_number)\n       FROM   raw_data\n       WHERE  row_number = 1 )\nSELECT *\nFROM   raw_data_deduplicated ;`\nFurther information\nWho created this app?\nWe'd like to thank PostHog team members Yakko Majuri, Marius Andra, Neil Kakkar, Michael Matloka and community member Edward Hartwell Goose for creating this BigQuery Export app.\nWho maintains this app?\nThis app is maintained by PostHog. If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "engage-connector.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/engage-connector.md",
    "content": "\ntitle: Engage Connector\ngithub: https://github.com/PostHog/posthog-engage-so-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=Engage\nthumbnail: ../../apps/thumbnails/engage_logo.png\ntags:\n    - engage\n\nWhat does the Engage Connector do?\nThis app connects PostHog to Engage, a marketing automation platform, and sends data to Engage for use in segmentation, targeting and automation.\nSpecifically, this app only exports your `Custom` and `$identify` events to Engage.\nExtra event properties and metadata are also processed and sent to Engage.\n`posthog.identify(\n    '[user unique id]', // distinct_id, required\n    { userProperty: 'value1' }, // $set, optional\n    { anotherUserProperty: 'value2' } // $set_once, optional\n);`\nThe example above, using the PostHog JS SDK, appends extra properties to the identify event. These extra properties are also sent to Engage.\nWhat are the requirements for this app?\nThe Engage Connector requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nYou'll also need access to a Engage account, obviously.\nHow do I install the Engage Connector?\nTo install this app, you will need your Engage secret key and public key to send data to PostHog. These are available on the 'Settings' page of your Engage dashboard, under the 'Account' > 'API' Keys section.\nOnce you have made a note of your keys, log in to your PostHog instance and follow the steps below.\n\nVisit the 'Apps' page in your instance of PostHog.\nSearch for 'Engage' and select the app, press Install.\nEnter configuration by selecting the blue gear icon.\nEnter the API information as requested, select save.\n\nNow, as soon as the app is enabled, PostHog will start sending your events to Engage in real-time.\nConfiguration\n\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the Engage Connector is available on GitHub.\nWho created this app?\nWe'd like to thank PostHog community members Francis Onyishi and Opeyemi Obembe for creating the Engage Connector. Thank you!\nWho maintains this app?\nThis app is maintained by the community. If you have issues with the app not functioning as intended, please raise a bug report on the repo to let us know!\nWhat if I have feedback on this app?\nWe love feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions too! Ask us anything via our FAQ page.",
    "tag": "posthog"
  },
  {
    "title": "ingestion-alert.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/ingestion-alert.md",
    "content": "\ntitle: Ingestion Alert\ngithub: https://github.com/PostHog/ingestion-alert-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=Posthog+Ingestion+Alert+Plugin\nthumbnail: ../../apps/thumbnails/ingestion-alert.png\ntags:\n    - ingestion-alert\n\nWhat does the Ingestion Alert app do?\nThis app triggers a webhook when no events have been ingested for a specified period of time. It can be used to alert you when ingestion for your project / instance is not working correctly.\nWhat are the requirements for this app?\nThe Ingestion Alert app requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nHow do I install the Ingestion Alert app?\n\nLog in to your PostHog instance\nClick 'Apps' on the left-hand tool bar\nSearch for 'Ingestion Alert'\nSelect the app, press 'Install' and follow the on-screen instructions\n\nConfiguration\n\nWhy am I not getting ingestion alert notifications?\nIf you do not have a lot of users, or they are all based in the same timezone you may legitimately have 'dead periods' where no events are generated. To prevent such dead periods causing alerts you can increase the threshold. You can also use the heartbeat app to trigger events during dead periods if you wish to only monitor the ingestion pipeline.\nIf an alert has already been triggered and ingestion has not recovered for an extended period, you will not receive another reminder that it is down.\nThis is helpful to monitor if there are any ingestion issues within your posthog instance and within your setup (e.g. using the wrong project key).\nIf the app server itself is down, this app will not be able to alert you that ingestion has stopped.\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the Ingestion Alert app is available on GitHub.\nWho created this app?\nWe'd like to thank PostHog team member Marcus Hyett and former PostHog team member Kunal for creating the Ingestion Alert app.\nWho maintains this app?\nThis app is maintained by PostHog. If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "laudspeaker-connector.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/laudspeaker-connector.md",
    "content": "\ntitle: Laudspeaker Connector\ngithub: https://github.com/PostHog/posthog-laudspeaker-app\ninstallUrl: https://app.posthog.com/project/apps?name=Laudspeaker\nthumbnail: ../../apps/thumbnails/laudspeaker-connector.png\ntags:\n    - laudspeaker-connector\n\nWhat does the Laudspeaker Connector app do?\nThe Laudspeaker Connector sends event data from PostHog to Laudspeaker, so it can be used to trigger immediate or time-delayed customer communications across multiple channels, including Slack and email. \nLaudspeaker is an open source customer messaging service and an alternative to tools such as Braze or Customer.io. \nWhat are the requirements for this app?\nThe Laudspeaker Connector requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nHow do I install the Lauspeaker Connector?\nIt is recommended to start by importing PostHog users into Laudspeaker via the event integration on Laudspeaker. More information on this is available in Laudspeaker's docs.\nAfter configuring PostHog within Laudspeaker, you can install the Laudspeaker Connector in PostHog by following these steps...\n\nLog in to your PostHog instance\nClick 'Apps' on the left-hand tool bar\nSearch for 'Laudspeaker'\nSelect the app, press 'Install'\n\nTo configure the app, you must provide the app with your API for `write-key`. This can be found in the 'Settings' section of Laudspeaker and should be copied into the Laudspeaker Connector configuration in PostHog. \nFinally, you must provide (in PostHog) `[your_server's_url]/events/posthog` for `Lauspeaker URL` (or, if using a Laudspeaker hosted plan, `app.laudspeaker.com/events/posthog`). When this is entered, enable the app in PostHog. \nOther fields (email, phone number, custom) are optional and can be specified if you want to be able to message people from PostHog through those channels with Laudspeaker\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the Laudspeaker Connector is available on GitHub.\nWho maintains this app?\nThis app is maintained by Laudspeaker. If you have issues with the app not functioning as intended, please raise a bug report to let us know - or contact Laudspeaker directly.\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think. You can also share feedback with Laudspeaker\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "Can I make my own site apps?",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/pineapple-mode.md",
    "content": "\ntitle: Pineapple Mode\ngithub: https://github.com/PostHog/pineapple-mode-app\ninstallUrl: https://app.posthog.com/project/apps?name=pineapple-mode\nthumbnail: ../../apps/thumbnails/pineapple-mode.png\ntags:\n    - pineapple-mode\n\nWhat does the Pineapple Mode app do?\nActivating Pineapple Mode makes it rain pineapples all over your product or website. Yes, really.\nOK, but why does Pineapple Mode do that?\nPineapple Mode is an example site app. Site apps are a new, big and currently beta feature which enable you to inject code from PostHog into your website via `posthog-js`.\nWe think site apps are a potentially useful feature for things such as displaying forms, notifications or surveys in your product or website. They're also useful for making it rain pineapples!\nWhat are the requirements for this app?\nPineapple Mode requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.41.0 or later.\nNot running 1.41.0? Find out how to update your self-hosted PostHog deployment!\nYou'll also need to manually opt in to the site apps feature, as it's currently in beta.\nHow do I install Pineapple Mode?\nFirst, you need to manually opt in to the site apps beta. You can do this by configuring your `posthog-js` initialization to include `opt_in_site_apps: true`. Please be aware you do this at your own risk and, if you get hit on the head by a pineapple, it's not PostHog's fault.\nOnce you've opted in, simply visit the 'Apps' page in your PostHog instance, search for 'Pineapple Mode' and press install. You may then need to refresh your page. Here's what you should expect:\nCan I make my own site apps?\nYou certainly can. Check our tutorial about how to build a site app in PostHog to get started. \nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for Pineapple Mode is available on GitHub.\nWho created this app?\nYou can blame PostHog team member Marius Andra for this one.\nWho maintains this app?\nThis app is maintained by PostHog. If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "segment.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/segment.md",
    "content": "\ntitle: Segment Connector\ngithub: https://github.com/PostHog/posthog-segment\nthumbnail: ../../apps/thumbnails/segment.png\ntags:\n    - segment\n\nWhat does the Segment Connector app do?\nThe Segment Connector app enables you to send events to PostHog, via Segment.\nSegment allows you to easily manage data and integrations with services across your Growth, Product, and Marketing stack. By tracking events and users via Segment\u2019s API and libraries, you can send your product\u2019s data to all of your analytics/marketing platforms, with minimal instrumentation code. They offer support for most platforms, including iOS, Android, JavaScript, Node.js, PHP, and more.\nWhat are the requirements for this app?\nUsing the Segment Connector requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nYou'll also need access to a Segment workspace.\nHow do I get started with the Segment Connector app?\n\nIn your Segment workspace, create a new project and enable PostHog as an integration. We are listed as a 'Destination' on Segment.\nGrab the PostHog API key from the 'Project Settings' page in PostHog.\nUse one of Segment's libraries to send events.\nSee the events coming into PostHog.\n\nCan PostHog with Segment do everything PostHog does by itself?\nWe are big fans of Segment, and many people in our team use it now or have used it in the past. However, it comes with some limitations for PostHog.\nThe Segment app gives you access to some things our JS library can do, but using Segment alone means you can't have autocapture, feature flags, session recording, heatmaps or the toolbar. Segment is also more easily blocked by ad-blockers.\nTo get around these limitations, you can install the PostHog snippet or posthog-js alongside your Segment integration. You can then use Segment for any custom events (for example `segment.track('user sign up')`), and posthog-js will automatically give you access to all the extra features.\nWhere can I find out more?\nFurther information about PostHog's Segment Connector is available in Segment's integration catalog.\nWho maintains this app?\nThis app is maintained by Segment. For more information, please check Segment's integration catalog.\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "property-filter.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/property-filter.md",
    "content": "\ntitle: Property Filter\ngithub: https://github.com/witty-works/posthog-property-filter-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=Property%20Filter\nthumbnail: ../../apps/thumbnails/property-filter.png\ntags:\n    - property-filter\n\nWhat does the Property Filter app do?\nThis app sets all specified properties on ingested events to `null`, effectively preventing PostHog from collecting information you do not want it to use.\nIt is used by teams such as WittyWorks to protect user privacy by removing unneeded geographic data.\nWhat are the requirements for this app?\nThe Property Filter app requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nHow do I install the Property Filter app?\n\nLog in to your PostHog instance\nClick 'Apps' on the left-hand tool bar\nSearch for 'Property Filter' press 'Install'\nConfigure the by app by following the onscreen instructions.\n\nIt's important to note that this app effectively removes information from PostHog events by setting properties to `null`. Apps on PostHog run in sequence, so it usually makes sense to place this app at the end of a sequence.\nNote: If you are filtering `$ip`, `event.ip` will also be set to null.\nDoes this filter properties for retrospective events?\nNo. The Property Filter app will only work on events ingested after it was enabled.\nConfiguration\n\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the Property Filter app is available on GitHub.\nWho created this app?\nThis app was created by community members at WittyWorks to protect the privacy of their users. We'd like to thank Christian and Lukas Kahwe Smnith for creating the Property Filter.\nWho maintains this app?\nThis app is maintained by the community. If you have issues with the app not functioning as intended, please raise a bug report on the repo to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "heartbeat.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/heartbeat.md",
    "content": "\ntitle: Heartbeat\ngithub: https://github.com/PostHog/posthog-heartbeat-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=Heartbeat\nthumbnail: ../../apps/thumbnails/heartbeat.png\ntags:\n    - heartbeat\n\nWhat does the Heartbeat app do?\nThe Heartbeat app simply sends one event to your project every minute for as long as it is enabled. It's mainly useful for testing and works well in conjunction with the Ingestion Alert app.\nWhat are the requirements for this app?\nThe Heartbeat app doesn't require a living heart, but it does require either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nHow do I install the Heartbeat app?\n\nLog in to your PostHog instance\nClick 'Apps' on the left-hand tool bar\nSearch for 'Heartbeat' press 'Install'\nConfigure the by app by following the instructions below.\n\nConfiguration\n\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the Heartbeat app is available on GitHub.\nWho created this app?\nWe'd like to thank Marcus Hyett for creating the Heartbeat app. Thanks, Marcus!\nWho maintains this app?\nThis app is maintained by PostHog. If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "databricks.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/databricks.md",
    "content": "\ntitle: Databricks Export\ngithub: https://github.com/posthog/posthog-databricks-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=posthog+databricks+plugin\nthumbnail: ../../apps/thumbnails/databricks.png\ntags:\n    - databricks\n\nWhat does the Databricks Export app do?\nThe Databricks Export app for PostHog will push data from PostHog to Databricks, once every minute. The app creates a table and migrates data from DBFS to a database.\nWhat are the requirements for this app?\nUsing this app requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nHow do I install the Databricks Export app for PostHog?\n\nVisit the 'Apps' page in your instance of PostHog.\nSearch for 'Databricks' and select the app, press Install.\nFollow the steps below to configure the app.\n\nConfiguration\nYou will need the following, in order to full configure this app:\n\nDomain name of the cluster, provided by Databricks\nAn API key, generated by following this documentation\nYour cluster ID, found in the system documentation\n\nYou will also need to give a temporary filename path for saving raw data, and a database name for where you want to store the data. Enter events in comma ( , ) separated way in order to ignore the data.\n\nWhat are the limitations for this app?\nThe Databricks Export app cannot currently sync historic data, or change the frequency with which it pushes data to PostHog.\nInteresting in contributing to the app to remove these limitations? Check the GitHub repo!\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the Databricks Export app is available on GitHub.\nWho created this app?\nWe'd like to thank community members Sandeep Guptan and Himanshu Garg for their work creating this app. Thank you, both!\nWho maintains this app?\nThis app is maintained by the community. If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhere can I find out more?\nCheck Databricks' API documentation for more information on pulling and pushing data from/to Databricks.\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "automatic-cohort-creator.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/automatic-cohort-creator.md",
    "content": "\ntitle: Automatic Cohort Creator\ngithub: https://github.com/PostHog/posthog-automatic-cohorts-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=Automatic%20Cohort%20Creator\nthumbnail: ../../apps/thumbnails/automatic-cohort-creator.png\ntags:\n    - auto-cohort\n\n\nWarning: We are currently in the process of deprecating this app in favor of group analytics, and do not recommend installing it at this time.\n\nWhat does the Automatic Cohort Creator app do?\nThe Automatic Cohort Creator app enables you to specify a list of user properties which will be used to automatically assign new users to a cohort.\nWhat are the requirements for this app?\nThe Automatic Cohort Creator app requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nHow do I install the Automatic Cohort Creator app?\n\nVisit the 'Apps' page in your instance of PostHog.\nSearch for 'Automatic Cohort Creator' and select the app, press Install.\nFollow the on-screen steps to configure the app.\u00df\n\nConfiguration\n\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the Automatic Cohort Creator is available on GitHub.\nWho created this app?\nThis app was created by PostHog team member Yakko Majuri. Thanks, Yakko!\nWho maintains this app?\nThis app is maintained by PostHog. If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "event-sequence-timer.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/event-sequence-timer.md",
    "content": "\ntitle: Event Sequence Timer\ngithub: https://github.com/PostHog/event-sequence-timer-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=Event%20Sequence%20Timer%20Plugin\nthumbnail: ../../apps/thumbnails/event-sequence-timer-plugin.png\ntags:\n    - event-timer\n\nWhat does the Event Sequence Timer app do?\nThis app measures the time it takes for a user to perform one event (`EventB`), after an earlier event (`EventA`).\nWhat are the requirements for this app?\nThe Event Sequence Timer requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nHow do I install the Event Sequence Timer app?\n\nVisit the 'Apps' page in your instance of PostHog.\nSearch for 'Event Sequence Timer' and select the app, press Install.\nFollow the on-screen steps to configure the app.\n\nHow to configure the Event Sequence Timer app?\nFirst, you must configure the list of events to track time differences on. This list is specified as follows:\n`(eventA,eventB),(eventC,eventD),(eventA,eventD)`\nWhere the first event in a tuple is the event that \"starts the timer\" and the second event being the one that \"finishes it\". In other words, the first event happens before the second.\nYou can further configure the app using the 'Update timestamp on every new first event?' setting. The default behaviour is 'Yes'.\nIf you select 'Yes', the stored timestamp for the first event will always be updated when a new event with the same name comes in (for the same user). This means your second event will always contain the difference between its time and the last time the user triggered the first event.\nIf you select No, the stored timestamp will only be set once and never updated. This means you will get the difference between the time of the second event and the first time the user triggered the first event.\nHow does the Event Sequence Timer show elapsed time?\nThe Event Sequence Timer app measures time between two events (`EventA` and `EventB`) in milliseconds.\nWhen a sequence is completed, the Event Sequence Timer adds a new property called `time_since_eventA` to `EventB`. You can then use this property in analysis with other PostHog apps.\nConfiguration\n\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the Event Sequence Timer is available on GitHub.\nWho created this app?\nWe'd like to thank PostHog team member Yakko Majuri for creating the Event Sequence Timer. Thanks Yakko!\nWho maintains this app?\nThis app is maintained by PostHog. If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "twitter-followers.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/twitter-followers.md",
    "content": "\ntitle: Twitter Followers Tracker\ngithub: https://github.com/PostHog/twitter-followers-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=Twitter+Followers\nthumbnail: ../../apps/thumbnails/twitter-followers.png\ntags:\n    - twitter-followers\n\nWhat are the requirements for this app?\nThe Twitter Followers Tracker requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nYou'll also need a Twitter account and, ideally, some followers.\nHow do I install the Twitter Followers Tracker for PostHog?\n\nLog in to your PostHog instance\nClick 'Apps' on the left-hand tool bar\nSearch for 'Twitter'\nSelect the app, press 'Install' and follow the on-screen instructions\n\nHow do I view my follower count in PostHog?\nTo view your follower count as a PostHog Insight, do the following:\n\nIn 'Trends', select the event `twitter_followers`\nWhere it says 'Total Volume' next to the event name, change this to 'Maximum'\nA new dropdown will appear, titled 'Select property'. Select `follower_count`\nVisualize your follower count over time!\n\nConfiguration\n\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the Twitter Followers Tracker is available on GitHub.\nWho created this app?\nWe'd like to thank PostHog team members Yakko Majuri and Marius Andra for creating the Twitter Followers Tracker. Thank you, both!\nWho maintains this app?\nThis app is maintained by PostHog. If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "email-scoring.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/email-scoring.md",
    "content": "\ntitle: Email Scoring\ngithub: https://github.com/PostHog/mailboxlayer-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=Posthog+Ingestion+Alert+Plugin\nthumbnail: ../../apps/thumbnails/email-scoring.png\ntags:\n    - email-scoring\n\nWhat does the Email Scoring app do?\nThe Email Scoring app adds email scores to Persons in PostHog, giving you more context on each user. Email scores are added using the Mailboxlayer API.\nWhat are the requirements for this app?\nThe Email Scoring app requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nYou'll also need Mailboxlayer access and an API key.\nHow do I install the Email Scoring app?\n\nVisit the 'Apps' page in your instance of PostHog.\nSearch for 'Email Scoring' and select the app, press Install.\nGet an API key from Mailboxlayer.\nClick '+ Install new app' and use this URL to install: `https://github.com/PostHog/mailboxlayer-plugin`\nAdd the API key to the config.\nEnable the app and watch the email scores come in!\n\nConfiguration\n\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the Email Scoring app is available on GitHub.\nWho created this app?\nWe'd like to thank PostHog team members Yakko Majuri and Marius Andra for creating the Email Scoring app.\nWho maintains this app?\nThis app is maintained by PostHog. If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "gitlab-release-tracker.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/gitlab-release-tracker.md",
    "content": "\ntitle: GitLab Release Tracker\ngithub: https://github.com/PostHog/gitlab-release-tracking-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=Gitlab+Release+Tracker\nthumbnail: ../../apps/thumbnails/gitlab.png\ntags:\n    - gitlab-release-tracker\n\nWhat does the GitHub Release Tracker app do?\nThe GitHub Release Tracker adds an Annotation to PostHog whenever a specified repo launches a new GitHub release. This is useful for correlating the impact of new releases on other metrics, such as sign-ups, as well as other performance-related issues.\nHow do I install the GitHub Release Tracker for PostHog?\n\nLog in to your PostHog instance\nClick 'Apps' on the left-hand tool bar\nSearch for 'GitLab Release Tracker'\nSelect the app, press 'Install' and follow the on-screen instructions\n\nWhat are the requirements for this app?\nThe GitHub Release Tracker requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nConfiguration\n\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the GitLab Release Tracker is available on GitHub.\nWho created this app?\nWe'd like to thank PostHog team members Yakko Majuri, Tim Glaser, Michael Matloka and Marius Andra for creating the GitLab Release Tracker.\nWho maintains this app?\nThis app is maintained by PostHog. If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "shopify.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/shopify.md",
    "content": "\ntitle: Shopify Connector\ngithub: https://github.com/posthog/posthog-shopify-sync-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=Posthog+Shopify\nthumbnail: ../../apps/thumbnails/shopify.png\ntags:\n    - shopify\n\nWhat does the Shopify Connector app do?\nThe Shopify Connector for PostHog enables you to sync customer and order data from Shopify, into PostHog.\nThis app will:\n\nAssociate your Shopify customers with PostHog users\nCreate a PostHog user from a Shopify customer if it doesn't exist\nCreate events for every new order\n\nIf there is an error while fetching orders, the next run of `runEveryMinute()` will try to re-read information from where it was previously interrupted.\nWhat are the requirements for this app?\nUsing this app requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nHow do I install this app for PostHog?\n\nVisit the 'Apps' page in your instance of PostHog.\nSearch for 'Shopify' and select the app, press 'Install'.\nFollow the steps below to configure the app.\n\nHow do I configure the Shopify Connector for PostHog?\nTo configure the Shopify Connector you will need to set the store name from your Shopify account.\nAdditionally, you will need to create a Shopify Access Token, which the Shopify Connector app will call to fetch orders into PostHog.\nTo create a Shopify Access Token, create an app on the admin page of your Shopify Account and generate `Admin API access token` in the API Credentials tab or your newly created Shopify app.\nConfiguration\n\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the Shopify Connector app is available on GitHub.\nWho created this app?\nWe'd like to thank community member Sreeraj Rajan for his work creating this app. Thank you, Sreeraj!\nWho maintains this app?\nThis app is maintained by the community. If you have issues with the app not functioning as intended, please raise an issue in the repo.\nWhere can I find out more?\nCheck the Shopify API reference docs for more information about connecting services to Shopify.\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "variance-connector.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/variance-connector.md",
    "content": "\ntitle: Variance Connector\ngithub: https://github.com/PostHog/posthog-variance-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=Variance\nthumbnail: ../../apps/thumbnails/variance.png\ntags:\n    - variance-connector\n\nWhat does the Variance Connector app do?\nThis app exports PostHog data to Variance in real-time and formats it for use by revenue teams. This includes extracting accounts and contacts, making it easy to see customers by sales stage, and more.\nWhat are the requirements for this app?\nThe Variance Connector requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.31.0 or later. The app supports `capture`, `page`, `identify`, and `alias` calls.\nNot running 1.31.0? Find out how to update your self-hosted PostHog deployment!\nHow do I install the Variance Connector app?\nTo install the Variance app you'll need a Variance account. In your Variance account go to Variance > Integrations > Create a new PostHog connection. This will give you a Webhook URL and Authorization header value. You will then use those two values when installing the app in your PostHog instance. Don't forget to hit enable after you've added the configuration details.\nConfiguration\n\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the Variance Connector is available on GitHub.\nWho created this app?\nThis app was created by Variance. We'd like to thank everyone at Variance for creating the Variance connector. Thanks!\nWho maintains this app?\nThis app is maintained by Variance. For more information or to report an issues, please check Variance's documentation.\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions! You can ask anything via our FAQ page.",
    "tag": "posthog"
  },
  {
    "title": "orbit.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/orbit.md",
    "content": "\ntitle: Orbit Connector\ngithub: https://github.com/PostHog/posthog-orbit-love-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=Orbit+Love+Report+Sync\nthumbnail: ../../apps/thumbnails/orbit-stats-sync.png\ntags:\n    - orbit\n\nWhat does the Orbit Connector app do?\nThe Orbit app for PostHog pulls data from the Orbit.love workspace API into PostHog, enabling you to track your community as a product metric.\nWhat are the requirements for this app?\nThe Orbit Connector requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nYou'll also need an Orbit workspace you can connect to.\nHow do I install the Orbit Connector app?\n\nVisit the 'Apps' page in your instance of PostHog.\nSearch for 'Orbit' and select the app, press Install.\nFollow the on-screen steps to configure the app.\n\nHow do I use the Orbit Connector app?\nWorkspace stats are sent into your PostHog instance as an `orbit love report` event, which you can filter and explore using PostHog. Supported report types include: overview, members, and activities.\nRefer to Orbit's API documentation for a full list of properties available in each report.\nConfiguration\n\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the Orbit Connector is available on GitHub.\nWho created this app?\nWe'd like to thank PostHog co-founder Tim Glaser and former PostHog team member Kunal for creating the Orbit Connector. Thank you, both!\nWho maintains this app?\nThis app is maintained by PostHog. If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "pace-integration.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/pace-integration.md",
    "content": "\ntitle: Pace Integration\ngithub: https://github.com/PostHog/pace-posthog-integration\ninstallUrl: https://app.posthog.com/project/apps?name=Pace\nthumbnail: ../../apps/thumbnails/pace-integration.png\ntags:\n    - pace-integration\n\nWhat does the Pace Integration app do?\nPace is a tool that equips sellers with relevant insights at the right time so they can spend time growing revenue. It allows them to convert, retain, and grow customers by prioritizing time and effort on the users who need it most.\nPostHog's Pace integration simply forwards any events that PostHog receives to Pace's internal ingestion endpoint\nWhat are the requirements for this app?\nThe Pace Integrations requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nYou'll also need access to the relevant Pace account.\nHow do I install the Pace Integration?\n\nVisit the 'Apps' page in your instance of PostHog.\nSearch for 'Pace' and select the app, press Install.\nEnable the app enter your Pace API key to authenticate with Pace.\n\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the Pace Integration is available on GitHub.\nWho created this app?\nThis app was created by Saimon Alam at Pace. \nWho maintains this app?\nThis app is maintained by the community. If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "Enabling apps",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/index.md",
    "content": "\ntitle: Apps\nsidebarTitle: Overview\nsidebar: Docs\nshowTitle: true\n\nApps extend PostHog's functionality by either pulling data into or sending data out of PostHog. They allow anyone to extend and customize PostHog in order to better fit their needs.\n\nWe have a comprehensive library of apps, including data warehouses (Snowflake, BigQuery, Redshift) and marketing tools (HubSpot, Sendgrid, Customer.io, Salesforce).\n\nApps can be used for a wide variety of use cases, such as:\n\nSending the event data to a data warehouse.\n    If you have a data lake or data warehouse, you can use apps to send PostHog event data there, while ensuring you still have that data in PostHog to perform your analytics processes.\nPulling data from a third-party API to enrich the event. Apps can pull in information like exchange rates, GeoIP location data, online reviews, and anything else you can think of and add it to your PostHog events, enriching the data to improve your analytics processes.\nAdding your own data from other sources to PostHog. In addition to pulling data from third-parties, you might also want to bring in data from your own sources, such as other tools and platforms you use.\nLabeling events. In order to facilitate sorting through your events, apps can be used to determine arbitrary logic that can label an event (e.g. by setting a `label` property). This can help you tailor your metrics in PostHog, as well as facilitate data ordering if you ever use PostHog data somewhere else.\nEnforcing event schemas. By default, PostHog does not enforce schemas on events it receives. However, an app could do so, preventing ingestion of events that do not match the specified schema in order to keep your data clean and following specific guidelines you need it to follow.\n\nEnabling apps\nHead to 'Project' -> 'Apps' on the left sidebar in the PostHog app. Here you can install official PostHog apps, install a custom app by pasting a link to its public repository, or write your own app directly in PostHog.\nSelf-host app troubleshooting\nIf you're having issues getting apps to work on your self-hosted instance of PostHog, check out our troubleshooting guide.\nNext steps",
    "tag": "posthog"
  },
  {
    "title": "geoip-enrichment.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/geoip-enrichment.md",
    "content": "\ntitle: GeoIP Enricher\ngithub: https://github.com/PostHog/posthog-plugin-geoip\ninstallUrl: https://app.posthog.com/project/apps?name=GeoIP\nthumbnail: ../../apps/thumbnails/geoip.png\ntags:\n    - geoip\n\nWhat does the GeoIP Enricher app do?\nThis app enriches PostHog events and persons with IP location data. Simply enable this app and from that point on, your new events will have GeoIP data added, allowing you to locate users and run queries based on geographic data.\nHow does the GeoIP Enricher app work?\nThis app prefers to use event property `$ip` (which should be of type `string`), but if that is not provided, it uses the IP address of the client that sent the event.\nThis way the app can, in most cases, infer the IP address without any work on your side.\nWhat are the requirements for this app?\nThe GeoIP Enricher requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nHow do I install the GeoIP Enrichment app for PostHog?\n\nLog in to your PostHog instance\nClick 'Apps' on the left-hand tool bar\nSearch for 'GeoIP'\nSelect the GeoIP app, press 'Install' and follow the on-screen instructions\n\nHow do I add properties?\nThe following properties can be added to the event if its IP address can be matched to a GeoLite2 City location:\n`TypeScript\n$geoip_city_name?: string\n$geoip_country_name?: string\n$geoip_country_code?: string\n$geoip_continent_name?: string\n$geoip_continent_code?: string\n$geoip_latitude?: number\n$geoip_longitude?: number\n$geoip_time_zone?: string\n$geoip_subdivision_1_code?: string\n$geoip_subdivision_1_name?: string\n$geoip_subdivision_2_code?: string\n$geoip_subdivision_2_name?: string\n$geoip_subdivision_3_code?: string\n$geoip_subdivision_3_name?: string`\nThey are also set on the associated person same as above, plus set*once in `$initial_geoip*...` form, to record where the user was when they were first seen.\nHow do I skip events without applying GeoIP enrichment?\nA case to be aware of is sending events from a server \u2013 such events, if not provided with custom property `$ip`, will be detected as sent from the location of the data center, instead of the related user.\nIf you'd like this app to skip over an event and not add the above properties,\nset property `$geoip_disable` to `true` on that event.\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the GeoIP Enricher is available on GitHub.\nWho created this app?\nWe'd like to thank PostHog team members Yakko Majuri, Tim Glaser, Michael Matloka and former team member Paolo D'Amico for creating the GeoIP Enricher. We miss you, Paolo!\nWho maintains this app?\nThis app is maintained by PostHog. If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "advanced-geoip.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/advanced-geoip.md",
    "content": "\ntitle: Advanced GeoIP\ngithub: https://github.com/paolodamico/posthog-app-advanced-geoip\ninstallUrl: https://app.posthog.com/project/apps?name=advanced-geo-ip\nthumbnail: ../../apps/thumbnails/advanced-geoip.png\ntags:\n    - advanced-geoip\n\nWhat does the Advanced GeoIP app do?\nThis app extends functionality for the GeoIP Entricher app, offering the ability to automatically discard unwanted information in a way which is helpful users with privacy or compliance needs.  \nWhat are the requirements for this app?\nThis app requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nYou'll also need to have the GeoIP Entricher app installed. \nHow do I install the this app?\n\nVisit the 'Apps' page in your instance of PostHog.\nSearch for 'Advanced GeoIP' and select the app, press Install.\n\nThat's it!\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for this app is available on GitHub.\nWho created this app?\nWe'd like to thank former PostHog team member Paolo D'Amico for creating this app. We miss you, Paolo!\nWho maintains this app?\nThis app is maintained by the community. If you have issues with the app not functioning as intended, please raise a bug report to let the maintainer of this app know.\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "downsampling.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/downsampling.md",
    "content": "\ntitle: Downsampler\ngithub: https://github.com/PostHog/downsampling-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=Downsampling\nthumbnail: ../../apps/thumbnails/downsampling.png\ntags:\n    - downsampler\n\nWhat does the Downsampler app do?\nThis app enables you to reduce how many events a deployment of PostHog will ingest. This is especially useful for users who have huge volumes of data, but don't need to analyze it all and want to avoid large bills.\nWhat are the requirements for this app?\nThe Downsampler app requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nHow do I install the Downsampler app?\n\nVisit the 'Apps' page in your instance of PostHog.\nSearch for 'Downsampling' and select the app, press Install.\nFollow the on-screen steps to configure the app.\n\nCan I use the Downsampler app to control my PostHog bill?\nYes. Paid-for versions of PostHog are priced per event and the Downsampling app enables you to reduce the number of events which your instance ingests, thereby giving you greater control over billing.\nConfiguration\n\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the Downsampler app is available on GitHub.\nWho created this app?\nWe'd like to thank PostHog team members Michael Matloka and Marius Andra and Neil Kakkar for creating the Downsampler.\nWho maintains this app?\nThis app is maintained by PostHog. If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "redshift-import.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/redshift-import.md",
    "content": "\ntitle: Redshift Import\ngithub: https://github.com/PostHog/posthog-redshift-import-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=Redshift+Import\nthumbnail: ../../apps/thumbnails/redshift.svg\ntags:\n    - redshift-import\n\nWhat does the Redshift Import app do?\nThe Redshift Import app for PostHog enables you, predictably, to import data from a Redshift table into PostHog. Data appears in PostHog as a stream of events.\nWhat are the requirements for this app?\nUsing the Redshift Import app requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nYou'll also need access to a Redshift table to import from.\nHow do I install the Redshift Import app?\nFirst, create and select a Redshift table to use. You will also need to create a new user with sufficient privileges to access data in your selected table.\nNext, create a new table to store events and execute `INSERT` queries. You can and should block PostHog from doing anything else on any other tables. Giving PostHog table creation permissions should be enough to ensure this:\n`CREATE USER posthog WITH PASSWORD '123456yZ';\nGRANT CREATE ON DATABASE your_database TO posthog;`\nNext, visit the \"Apps\" page in your instance of PostHog and search for 'Redshift Import'. Select the app, press Install. Follow the on-screen steps to configure the app.\nFinally, you must determine what transformation to apply to your Redshift data. This app receives data from your table and transforms it into a PostHog event. You can check the available transformations below.\nIMPORTANT: Make sure your Redshift table has a sort key and use the sort key column in the \"Order by column\" field of the app config.\nWhat transformations are available?\nThis app receives data from your table and transforms it into a PostHog event.\nThe default transformation looks for the following columns in your table: event, timestamp, distinct_id, and properties, and maps them to the equivalent PostHog event fields of the same name.\n`async function transform (row, _) {\n    const { timestamp, distinct_id, event, properties } = row\n    const eventToIngest = {\n        event,\n        properties: {\n            timestamp,\n            distinct_id,\n            ...JSON.parse(properties),\n            source: 'redshift_import',\n        }\n    }\n    return eventToIngest\n}`\nAnother available transformation is the JSON Map. This transformation asks the user for a JSON file containing a map between their columns and fields of a PostHog event. For example:\n`{\n    \"event_name\": \"event\",\n    \"some_row\": \"timestamp\",\n    \"some_other_row\": \"distinct_id\"\n}`\nA version of the code is below, with error handling and type definitions removed for the sake of brevity.\n```\nasync function transform (row, { attachments }) {\n    let rowToEventMap = JSON.parse(attachments.rowToEventMap.contents.toString())\n\n\n```const eventToIngest = {\n    event: '',\n    properties: {}\n}\n\nfor (const [colName, colValue] of Object.entries(row)) {\n    if (!rowToEventMap[colName]) {\n        continue\n    }\n    if (rowToEventMap[colName] === 'event') {\n        eventToIngest.event = colValue\n    } else {\n        eventToIngest.properties[rowToEventMap[colName]] = colValue\n    }\n}\n\nreturn eventToIngest\n```\n\n\n}\n```\nHow can I contribute a transformation?\nIf none of the transformations listed above suits your use case, you're more than welcome to contribute your own transformation!\nTo do so, just add your transformation to the `transformations` object in the index.ts file of the repo and list it in the plugin.json choices list for the field `transformationName`.\nA transformation entry looks like this:\n```\n'': {\n    author: '',\n    transform: async (row, meta) => {\n        /*\n\n\n```    Fill in your transformation here and\n    make sure to return an event according to\n    the TransformedPluginEvent interface:\n\n    interface TransformedPluginEvent {\n        event: string,\n        properties?: PluginEvent['properties']\n    }\n\n    */\n}\n```\n\n\n}\n```\nYour GitHub username is important so that we only allow changes to transformations by the authors themselves.\nConfiguration\n\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the Redshift Import app is available on GitHub.\nWho created this app?\nWe'd like to thank PostHog team member Yakko Majuri and community member Utsavkumar Lal for creating the Redshift Import app. Thank you, both!\nWho maintains this app?\nThis app is maintained by PostHog. If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "taxonomy-standardizer.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/taxonomy-standardizer.md",
    "content": "\ntitle: Taxonomy Standardizer\ninstallUrl: https://app.posthog.com/project/apps?name=Taxonomy+Plugin\ngithub: https://github.com/PostHog/taxonomy-plugin\nthumbnail: ../../apps/thumbnails/taxonomy-standardizer.png\ntags:\n    - taxonomy-standardizer\n\nWhat does the Taxonomy Standardizer app do?\nThis app standardizes all your event names into a single pattern, so that data becomes more consistent and marketing teams aren't driven wild.\nWhat taxonomies are supported?\nThis app can convert from any of these taxonomies, to any other.\n\nCamel Case: `helloThereHedgehog`\nPascal Case: `HelloThereHedgehog`\nSnake Case: `hello_there_hedgehog`\nKebab Case: `hello-there-hedgehog`\nSpaces: `hello there hedgehog`\n\nWhat are the requirements for this app?\nThe Taxonomy Standardizer requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nHow do I install the Taxonomy Standardizer app?\n\nVisit the 'Apps' page in your instance of PostHog.\nSearch for 'Taxonomy Standardizer' and select the app, press Install.\nFollow the on-screen steps to configure the app.\n\nConfiguration\n\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the Taxonomy Standardizer is available on GitHub.\nWho created this app?\nWe'd like to thank PostHog team member Yakko Majuri for creating the Taxonomy Standardizer. Thank you, Yakko!\nWho maintains this app?\nThis app is maintained by PostHog. If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "unduplicator.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/unduplicator.md",
    "content": "\ntitle: Unduplicator\ngithub: https://github.com/paolodamico/posthog-app-unduplicates\ninstallUrl: https://app.posthog.com/project/apps?name=Unduplicates\nthumbnail: ../../apps/thumbnails/unduplicator.png\ntags:\n    - unduplicator\n\nWhat does the Unduplicator do?\nThis app prevents duplicate events from being ingested into PostHog. It's particularly helpful if you're backfilling information while you're already ingesting ongoing events.\nHow does the Unduplicator work?\nThe Unduplicator crafts an event UUID based on key properties for the event, so if the event is the same (see below for definition) it'll end with the same UUID.\nWhen events are processed by ClickHouse, the database will automatically dedupe events which have the same `toDate(timestamp)`, `event`, `distinct_id` and `uuid` combo, effectively making sure duplicates are not stored.\nThe app has two modes that define what's considered a duplicate event. Either mode will prevent duplicates within a project, though duplicates across projects are still permitted.\n\nEvent and Timestamp. An event will be treated as duplicate if the timestamp, event name and user's distinct ID matches exactly, regardless of what internal properties are included.\nAll Properties. An event will be treated as duplicate only if all properties match exactly, as well as the timestamp, event name and user's distinct ID.\n\nWhat are the requirements for this app?\nThe Unduplicator requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nHow do I install the Unduplicator app for PostHog?\n\nLog in to your PostHog instance\nClick 'Apps' on the left-hand tool bar\nSearch for 'Unduplicates'\nSelect the Unduplicator app, press 'Install'.\nOnce the app is installed, press the blue button to configure the app and select which of the de-duplication methods you want to use (described above).\n\nConfiguration\n\nIs the source code for this app available?\nPostHog is open source and so are all apps on the platform. The source code for the Unduplicator app is available on GitHub.\nWho created this app?\nWe'd like to thank former PostHog team member Paolo D'Amico for creating the Unduplicator. We miss you, Paolo!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our FAQ page.",
    "tag": "posthog"
  },
  {
    "title": "google-cloud-export.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/google-cloud-export.md",
    "content": "\ntitle: Google Cloud Export\ngithub: https://github.com/PostHog/posthog-gcs-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=GCS+Export\nthumbnail: ../../apps/thumbnails/gcs-export.png\ntags:\n    - gcs-export\n\nWhat does the Google Cloud Storage Export app do?\nThe Google Cloud Storage Export app enables you to send events from PostHog to a Google Cloud Storage bucket upon ingestion.\nWhat are the requirements for this app?\nThe Google Cloud Storage Export app requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nYou'll also need access to the Google Cloud Storage bucket you want to export to.\nHow do I install the PostgreSQL Export app?\nBefore installing the Google Cloud Storage Export app, you will need your Google Cloud .json file. Find out how to get this in Google's BigQuery API documentation.\n\nVisit the 'Apps' page in your instance of PostHog.\nSearch for 'GCS' and select the app, press Install and proceed to Configuration.\nUpload your Google Cloud key .json file.\nEnter your Project ID.\nEnter your bucket name.\n\nConfiguration\n\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the Google Cloud Storage Export app is available on GitHub.\nWho created this app?\nWe'd like to thank PostHog team member Yakko Majuri for creating the Google Cloud Storage Export app. Thanks, Yakko!\nWho maintains this app?\nThis app is maintained by PostHog. If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "replicator.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/replicator.md",
    "content": "\ntitle: Event Replicator\ngithub: https://github.com/posthog/posthog-plugin-replicator\ninstallUrl: https://app.posthog.com/project/apps?name=Replicator\nthumbnail: ../../apps/thumbnails/replicator.png\ntags:\n    - replicator\n\nimport MigratingEvents from \"../migrate/snippets/migrating-events.mdx\"\nWhat does the Replicator app do?\nThe Replicator app copies events from one PostHog instance to another, at the moment they are ingested. No changes are made to the events by this app if it runs in isolation.\nIf this app is deployed in a chain then any changes made to the event data before the Replicator runs will also be copied to the new instance. Otherwise, no changes will be copied.\nWhat are the requirements for this app?\nThe Replicator app requires two instances of PostHog running either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nBoth versions of PostHog should ideally be running the same version.\nHow do I use the Replicator app?\n\n\nLog in to your PostHog instance\nClick 'Apps' on the left-hand tool bar\nSearch for 'Replicator'\nSelect the app, press 'Install' and follow the on-screen instructions\nSee events come into another PostHog instance, identically to the originals\n\nConfiguration\n\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the Replicator is available on GitHub.\nWho created this app?\nWe'd like to thank PostHog team members Yakko Majuri and Michael Matloka for creating the Replicator app.\nWho maintains this app?\nThis app is maintained by PostHog. If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "rudderstack-export.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/rudderstack-export.md",
    "content": "\ntitle: RudderStack Export\ngithub: https://github.com/rudderlabs/rudderstack-posthog-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=RudderStack\nthumbnail: ../../apps/thumbnails/rudderstack-export.png\ntags:\n    - rudderstack\n\nWhat does the RudderStack Export app do?\nThe Rudderstack Export app enables you to send events from PostHog, to RudderStack. RudderStack will recognize PostHog as a data source, so you can use RudderStack's data pipeline features to send PostHog event data to other platforms.\nWhat are the requirements for this app?\nThe Rudderstack Export app requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nYou'll also need access to the Rudderstack instance you want to export to.\nHow do I install the RudderStack Export app?\nFirst, create a PostHog source in your RudderStack dashboard. Need help? Check RudderStack's documentation for more information.\nOnce you've added PostHog as a source, make a note of the `write-key` field and your RudderStack server URL (also called the Data Planer URL).\n\nVisit the 'Apps' page in your instance of PostHog.\nSearch for 'Rudderstack' and select the app, press Install.\nFollow the on-screen steps to configure the app, entering your `write-key` and server URL information.\n\nConfiguration\n\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the RudderStack Export app is available on GitHub.\nWho created this app?\nThe Rudderstack Export app was created by community members and the team at RudderStack. We'd like to thank Sayan-Mitra, Gavin, Amey Varangaonkar, Utsab Chowdhury and Arnab Pal for creating it. Thank you, all!\nWho maintains this app?\nThis app is maintained by Rudderstack. If you have issues with the app not functioning as intended, please raise a bug report on the repo.\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "postgres-export.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/postgres-export.md",
    "content": "\ntitle: PostgreSQL Export\ngithub: https://github.com/PostHog/postgres-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=PostgreSQL\nthumbnail: ../../apps/thumbnails/postgresql-export.png\ntags:\n    - postgres-export\n\nWhat does the PostgreSQL Export app do?\nThe PostgreSQL Export app enables you to export events from PostHog to a PostgreSQL instance on ingestion.\nWhat are the requirements for this app?\nUsing the PostgreSQL Export app requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nYou'll also need access to the PostgreSQL instance you want to export to.\nHow do I install the PostgreSQL Export app?\nFirstly, make sure that PostHog can access your PostgreSQL instance. Wherever your instance is hosted, make sure it is set to accept incoming connections so that PostHog can connect to the database and insert events. If this is not possible in your case, consider using the S3 Export app and then setting up your own system for getting data into your Postgres instance.\nYou will also need to create a user with table creation privileges in your PostgreSQL instance, as well as a new table to store events and execute INSERT queries. You can and should block PostHog from doing anything else on any other tables. Giving PostHog table creation permissions should be enough to ensure this:\n`CREATE USER posthog WITH PASSWORD '123456yZ';\nGRANT CREATE ON DATABASE your_database TO posthog;`\nFinally, follow the steps below.\n\nVisit the 'Apps' page in your instance of PostHog.\nSearch for 'PostgreSQL' and select the app, press Install.\nAdd the connection details at the plugin configuration step in PostHog.\n\nConfiguration\n\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the Postgres Export app is available on GitHub.\nWho created this app?\nWe'd like to thank PostHog team members Yakko Majuri and community member Michael Shanks for creating the Postgres Export app. Thanks, both!\nWho maintains this app?\nThis app is maintained by PostHog. If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "enabling.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/enabling.md",
    "content": "\ntitle: Troubleshooting\nsidebar: Docs\nshowTitle: true\n\nApp server is offline\nIf the server is offline, you need to update your PostHog deployment and/or manually start the server.\nHeroku\nWhen you update to the latest version of PostHog, the app server should start automatically.\nIf it doesn't you need to make sure that a `worker` dyno is running. You have the choice\nto enable any of the following dyno types:\n\n`celeryworker` - runs celery, the default background worker\n`pluginworker` - runs posthog-plugin-server \n`worker` - contains both `celeryworker` and `pluginworker` inside one dyno\n\nYou must have both the celery and app workers running for PostHog to function properly.\nYou can either launch and scale them in separate dynos\nor run the default `worker` dyno, which includes both of them.\nAWS CloudFormation\nPlease upgrade to the latest CloudFormation config \nthat combines all background workers into one task and activates the server.\nPreviously we had separate tasks for `worker-beat` and `worker-celery`. In the latest version there is just one `worker`\nthat starts all three services: celery, celery-beat and apps.\nHelm/Kubernetes\nPlease upgrade to at least version `1.4.0` of the PostHog helm chart to add the `apps` deployment.\nManual/other\nYou must run the `bin/plugin-server` script to start the plugin server.",
    "tag": "posthog"
  },
  {
    "title": "airbyte-export.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/airbyte-export.md",
    "content": "\ntitle: Airbyte Exporter\ninstallUrl: https://docs.airbyte.com/integrations/sources/posthog/\nthumbnail: ../../apps/thumbnails/airbyte.png\ntags:\n    - airbyte\n\nWhat does the Airbyte Export app do?\nThis Airbyte Export app sends data from PostHog, to Airbyte. It supports both Full Refresh and Incremental syncs. You can choose if this app will copy only the new or updated event data, or all rows in the tables and columns you set up for replication, every time a sync is run.\nWhat are the requirements for this app?\nUsing the Airbyte Export app requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nHow do I get started with the Airbyte Export app?\nThe Airbyte app is an API integration. You will need to get a PostHog Personal API key in order to connect Airbyte as a data destination.\nWhat output schema is available?\nThis app is capable of syncing the following streams:\n\nAnnotations\nCohorts\nEvents\nFeatureFlags\nInsights\nInsightsPath\nInsightsSessions\nPersons\nTrends\n\nFor more info, please check Airbyte's integration documentation.\nWhere can I find out more?\nCheck PostHog's API documentation for more information on pulling and pushing data from/to our API. Further information about Airbyte's connector is available in Airbyte's integration documentation.\nWho maintains this app?\nThis app is maintained by Airbyte. For more information, check Airbyte's integration documentation.\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "salesforce-connector.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/salesforce-connector.md",
    "content": "\ntitle: Salesforce Connector\ngithub: https://github.com/Vinovest/posthog-salesforce\ninstallUrl: https://app.posthog.com/project/apps?name=Salesforce\nthumbnail: ../../apps/thumbnails/salesforce.svg\ntags:\n    - salesforce\n\nWhat does the Salesforce Connector do?\nThis app connects to your Salesforce instance, sending events from PostHog to Salesforce as they are ingested.\nWhat are the requirements for this app?\nThe Salesforce Connector app requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nYou'll also need a Salesforce account to connect to, as well as the relevant levels of access to install and configure this app.\nHow do I install the Salesforce Connector app for PostHog?\n\nLog in to your PostHog instance\nClick 'Apps' on the left-hand tool bar\nSearch for 'Salesforce'\nSelect the app, press 'Install' and follow the on-screen instructions\n\nConfiguration\n\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the Salesforce Connector is available on GitHub.\nWho created this app?\nWe'd like to thank PostHog team member Yakko Majuri and community members Angela Purcell and Conrad Kurth for creating the Salesforce Connector. Thank you, all!\nWho maintains this app?\nThis app is maintained by the community. If you have issues with the app not functioning as intended, please raise an issue on the repo.\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "google-pub-sub-connector.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/google-pub-sub-connector.md",
    "content": "\ntitle: GCP Pub/Sub Connector\ngithub: https://github.com/vendasta/pubsub-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=Pub%2FSub%20Export\nthumbnail: ../../apps/thumbnails/pub-sub-export.png\ntags:\n    - pub-sub\n\nWhat does the Google Cloud Pub/Sub Connector do?\nThis app sends events from PostHog to a Google Cloud Pub/Sub topic when they are ingested. It's used by teams such as Vendasta.\nWhat are the requirements for this app?\nThe Pub/Sub Connector requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nYou'll also need a Google Cloud Pub/Sub account to connect to.\nHow do I install the Google Cloud Pub/Sub Connector?\n\nVisit the 'Apps' page in your instance of PostHog.\nSearch for 'Pub/Sub' and select the app, press Install.\nUpload your Google Cloud key .json file. (How to get the file.)\nEnter your Topic ID.\nWatch events publish to Topic.\n\nWhere is my Google Cloud key .json file?\nYou'll need this file to configure the Pub/Sub app for PostHog. You can find out where to find it in Google's Pub/Sub client libraries documentation.\nConfiguration\n\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the Google Cloud Pub/Sub Connector is available on GitHub.\nWho created this app?\nWe'd like to thank PostHog community member Jesse Redl from Vendasta for creating the Pub/Sub Connector. Thanks, Jesse!\nWho maintains this app?\nThis app is maintained by the community. If you have issues with the app not functioning as intended, please raise a bug report on the repo to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "property-flattener.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/property-flattener.md",
    "content": "\ntitle: Property Flattener\ngithub: https://github.com/PostHog/flatten-properties-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=Property%20Flattener%20Plugin\nthumbnail: ../../apps/thumbnails/property-flattener.png\ntags:\n    - property-flattener\n\nWhat does the Property Flattener app do?\nThis app flattens nested properties in PostHog events, making it easier to access them through filters if needed.\nThis is useful if, for example, you're an online retailer and have purchase events with the following property structure:\n`{\n    \"event\": \"purchase\",\n    \"properties\": {\n        \"product\": {\n            \"name\": \"AZ12 shoes\",\n            \"type\": \"footwear\",\n            \"size\": {\n                \"number\": 12,\n                \"gender\": \"M\"\n            }\n        }\n    }\n}`\nThis app will keep the nested properties unchanged, but also add any nested properties at the first depth, like so:\n`{\n    \"event\": \"purchase\",\n    \"properties\": {\n        \"product\": {\n            \"name\": \"AZ12 shoes\",\n            \"type\": \"footwear\",\n            \"size\": {\n                \"number\": 12,\n                \"gender\": \"M\"\n            }\n        },\n        \"product__name\": \"AZ12 shoes\",\n        \"product__type\": \"footwear\",\n        \"product__size__number\": 12,\n        \"product__size__gender\": \"M\"\n    }\n}`\nAs such, you can now filter your purchase events based on productsizenumber for example.\nWhat are the requirements for this app?\nThe Property Flattener requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nHow do I install the Property Flattener?\n\nVisit the 'Apps' page in your instance of PostHog.\nSearch for 'Property Flattener' and select the app, press Install.\nFollow the on-screen steps to configure the app.\n\nHow do I separate nested properties?\nThe default separator for nested properties is two subsequent underscores (__), but you can also change this to:\n\n`.`\n`>`\n`/`\n\nWhen picking your separator, make sure it will not clash with your property naming patterns! There be dragons.\nConfiguration\n\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the Property Flattener is available on GitHub.\nWho created this app?\nWe'd like to thank PostHog team members Yakko Majuri, Tim Glaser and Marius Andra and community member Chasovskiy for creating the Property Flattener.\nWho maintains this app?\nThis app is maintained by PostHog. If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "first-time-event-tracker.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/first-time-event-tracker.md",
    "content": "\ntitle: First Time Event Tracker\ngithub: https://github.com/PostHog/first-time-event-tracker\ninstallUrl: https://app.posthog.com/project/apps?name=First%20Time%20Event%20Tracker\nthumbnail: ../../apps/thumbnails/first-time-event-tracker.png\ntags:\n    - first-time\n\nWhat does the First Time Event Tracker do?\nThis app adds two new properties to events which you specify:\n\n`is_event_first_ever`\n`is_event_first_for_user`\n\nUsing these events, you can track if each event is the first time that event has run for a individual user, the first time it has run ever, both of these, or neither.\n\nNote: For Pageview and Identify events, use the event names `$pageview` and `$identify` respectively.\n\nWhat are the requirements for this app?\nThe First Time Event Tracker requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nHow do I install the First Time Event Tracker app?\n\nVisit the 'Apps' page in your instance of PostHog.\nSearch for 'First Time Event Tracker' and select the app, press Install.\nFollow the on-screen steps to configure the app.\n\nThis app will only work on events ingested after the app was enabled.\nDoes this app work retroactively?\nNo. This app will only work on events ingested after the app was enabled.\nThis means it will register events as being the first if there were events that occurred before it was enabled. To mitigate this, you could consider renaming the relevant events and creating an action that matches both the old event name and the new one.\nConfiguration\n\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the First Time Event Tracker is available on GitHub.\nWho created this app?\nWe'd like to thank PostHog team member Yakko Majuri for creating the First Time Event Tracker. Thanks Yakko!\nWho maintains this app?\nThis app is maintained by PostHog. If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "rudderstack-import.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/rudderstack-import.md",
    "content": "\ntitle: RudderStack Import\nthumbnail: ../../apps/thumbnails/rudderstack-export.png\ntopics:\n    - rudderstack\n\nWhat does the Ruddestack Importer do?\nThe Ruddestack Importer app enables you to send events to PostHog, via RudderStack.\nRudderStack is an open-source, customer data platform for developers. It allows you to collect and deliver customer event data to a variety of destinations across your growth, product, and marketing stack.\nWhat are the requirements for this app?\nUsing the RudderStack Import requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nYou'll also need access to a RudderStack workspace.\nHow do I get started with the RudderStack Import?\nBefore integrating with RudderStack, we recommend you read our CDP integration guide to understand the different options for integrating your CDP with PostHog.\nWhen you are ready to integrate, read the PostHog integration guide with RudderStack.\nWho maintains this app?\nThis app is maintained by RudderStack.\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "semver-flattener.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/semver-flattener.md",
    "content": "\ntitle: SemVer Flattener\ngithub: https://github.com/PostHog/semver-flattener-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=SemVer\nthumbnail: ../../apps/thumbnails/semver-flattener.png\ntags:\n    - semver-flattener\n\nWhat does the SemVer Flattener do?\nThe SemVer Flattener splits a valid SemVer version into the following \n`export interface VersionParts {\n    major: number\n    minor: number\n    patch?: number\n    preRelease?: string\n    build?: string\n}`\nAnd then flattens them onto an event. So, this...\n`{\n    properties: {\n        app_version: '22.7.11'\n    }\n}`\nBecomes this...\n`{\n    properties: {\n        app_version: '22.7.11'\n        app_version__major: 22,\n        app_version__minor: 7,\n        app_version__patch: 11\n    }\n}`\nThis is especially helpful for comparing versions when filtering in PostHog insights, as it isn't possible to correctly use string comparison in all situations. \nWhat are the requirements for this app?\nThis app requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nHow do I install the SemVer Flattener app?\n\nVisit the 'Apps' page in your instance of PostHog.\nSearch for 'SemVer Flattener' and select the app, press Install.\n\nThat's it!\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the SemVer Flattener is available on GitHub.\nWho created this app?\nWe'd like to thank PostHog team member Paul D'Ambra for creating this app. Cheers, Paul!\nWho maintains this app?\nThis app is maintained by PostHog. If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "currency-normalization.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/currency-normalization.md",
    "content": "\ntitle: Currency Normalizer\ngithub: https://github.com/PostHog/currency-normalization-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=Currency%20Normalization\nthumbnail: ../../apps/thumbnails/currency-normalization.png\ntags:\n    - currency-normalizer\n\nWhat does the Currency Normalizer app do?\nThis app normalizes currencies in events. E.g. amounts in EUR, USD and GBP will all be converted to EUR.\nWhat are the requirements for this app?\nThe Currency Normalizer requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nHow do I install the Currency Normalizer app for PostHog?\n\nVisit the 'Apps' page in your instance of PostHog.\nSearch for 'Currency Normalization' and select the app, press Install.\nUpdate the required settings (get the API key here) and enable the plugin.\n\nConfiguration\n\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the Currency Normalizer is available on GitHub.\nWho created this app?\nWe'd like to thank PostHog team members Yakko Majuri and Marius Andra, as well as community member Leo Mehlig for creating the Currency Normalizer.\nWho maintains this app?\nThis app is maintained by PostHog. If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "avo-inspector.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/avo-inspector.md",
    "content": "\ntitle: Avo Inspector\ngithub: https://github.com/PostHog/posthog-avo-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=avo-inspector\nthumbnail: ../../apps/thumbnails/avo-logo.png\ntags:\n    - avo-inspector\n\nWhat does the Avo Inspector do?\nAvo is a data governance platform which helps teams plan, implement and verify analytics at any scale. The Avo Inspector app sends event schema - but not events themselves - to Avo. This enables you to, for example, avoid losing data or events due to naming issues in your analytics. \nYou can read more about the Avo Inspector in the official announcement. \nWhat are the requirements for this app?\nUsing the Avo Inspector app requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nYou'll also need to have an Avo account, obviously. \nHow do I get started with the Avo Inspector?\nFirst, you need to set PostHog as a data source in Avo. We recommend checking Avo's documentation for setting PostHog as a source in Avo.\nOnce PostHog is set as a source in Avo, simply install and enable the app in your PostHog instance by heading to the Apps section. You'll need to enter your Avo API key to complete the setup. \nWhere can I find out more?\nAvo maintains robust documentation about integrating PostHog and Avo.\nWho maintains this app?\nThis app is maintained by the PostHog community. \nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "snowflake-export.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/snowflake-export.md",
    "content": "\ntitle: Snowflake Export\ngithub: https://github.com/PostHog/snowflake-export-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=Snowflake\nthumbnail: ../../apps/thumbnails/snowflake.svg\nofficial: true\ntags:\n    - snowflake-export\n\nThis app allows you to export both live and historical events from PostHog into Snowflake.\nThis is useful when you want to run custom SQL queries on your data in PostHog using Snowflake's high-performance infrastructure.\nThis app utilizes a Snowflake external stage to stage events in object storage - Amazon S3 or Google Cloud Storage.\nStaged events (stored in object storage as files containing event batches) are then copied into the final destination \u2013 your Snowflake table \u2013 once every 10 minutes by default.\nInstall\nPostHog Cloud\nPostHog Cloud users can find the app here on the apps page.\nBefore you can enable the app, you will need to configure it by clicking on the settings icon.\nOnce the app has been configured, you can enable it by flipping the toggle and it will start exporting newly ingested events to Snowflake.\nPostHog Self-hosted\n\nThe Snowflake Export app requires a PostHog instance running version 1.24.0 or later.\nNot running 1.24.0? Find out how to update!\n\n\nLog in to your PostHog instance\nClick 'Apps' on the left-hand navigation\nSearch for 'Snowflake'\nSelect the 'Snowflake Export' app and press 'Install'\nConfigure the app by entering both your AWS & Snowflake credentials\nEnable the app and watch events roll into Snowflake!\n\nConfigure\nSet-up Snowflake\nTo get started, you'll need to create a Snowflake account. Once you've set up your account, login to your Snowflake instance.\nCreating a virtual warehouse\nFirst, we'll need to set up a Snowflake Virtual Warehouse.\nIf you already have a Snowflake Warehouse that you would like to use you can skip this step.\nTo create a new warehouse, login to Snowflake and navigate to \"Admin\" > \"Warehouses\" on the left panel and click on the \"+ Warehouse\" button.\nGive your new warehouse a name and choose a size for it.\nFor more information on choosing a size for your warehouse, checkout the Snowflake docs.\n\nCreating a new database\nNow that we have a warehouse set up, we'll create a new database to store our exported data. To do this we'll use a Worksheet to execute SQL statements with our new Warehouse.\nWe can create a new Worksheet by navigating to the Worksheets tab and clicking the \"+ Worksheet\" button. Then in the code editor, paste the following SQL:\n`sql\nCREATE DATABASE IF NOT EXISTS POSTHOG_DB;`\nThis will create a new database called `POSTHOG_DB` where we can store our exports.\nSet-up roles and permissions\nNow that we have our warehouse and database set up, we're going to start to get our permissions configured so PostHog can access these resources.\nWe'll do this by creating a new role `POSTHOG_EXPORT` which we'll grant only the necessary permissions that PostHog needs with the following SQL:\n```sql\nCREATE ROLE IF NOT EXISTS POSTHOG_EXPORT;\nGRANT USAGE ON WAREHOUSE POSTHOG_WH TO ROLE POSTHOG_EXPORT;\nGRANT USAGE ON DATABASE POSTHOG_DB TO ROLE POSTHOG_EXPORT;\nGRANT USAGE,CREATE TABLE,CREATE STAGE ON SCHEMA POSTHOG_DB.PUBLIC TO ROLE POSTHOG_EXPORT;\nGRANT INSERT,UPDATE ON ALL TABLES IN SCHEMA POSTHOG_DB.PUBLIC TO ROLE POSTHOG_EXPORT;\n-- This will ensure that our admin user still has access to the table PostHog creates for our export data\nGRANT ALL ON FUTURE TABLES IN SCHEMA POSTHOG_DB.PUBLIC TO ROLE ACCOUNTADMIN;\n```\n\nNote:  For all the SQL snippets, make sure to execute every line! This can be done by selecting the entire block of commands within the editor and pressing the blue play button in the top right.\n\nThis will create a new role called `POSTHOG_EXPORT` with permission to use our newly created Warehouse, as well as permission to `INSERT` and `UPDATE` on tables in our database `POSTHOG_DB`.\nFinally, we'll create a new user and grant the `POSTHOG_EXPORT` role to it. Make sure to keep track of the password you use for this user as we'll need it later!\n```sql\nCREATE USER IF NOT EXISTS POSTHOG\n    PASSWORD = ''\n    DEFAULT_ROLE = POSTHOG_EXPORT\n    MUST_CHANGE_PASSWORD = false;\nGRANT ROLE POSTHOG_EXPORT TO USER POSTHOG;\n```\nSet-up external staging\nThis app uses an External stage to store exported data, which can then be copied into Snowflake for processing.\nCurrently, we support both Amazon S3 and Google Cloud Storage for staging files.\nConnect Amazon S3 to Snowflake\n\nCreate a new S3 bucket, preferably in the same AWS region as your Snowflake instance.\nFollow this Snowflake guide on S3 to configure AWS IAM User Credentials to access Amazon S3. However, instead of doing step 3 yourself, input the AWS Key ID and Secret Key in the appropriate app configuration options. We'll take care of creating the stage for you.\n\nConnect Google Cloud Storage to Snowflake\n\nCreate a new GCS bucket.\nFollow this Snowflake guide on GCS to create a storage integration and generate a user for Snowflake to use when accessing your bucket. Make sure not to skip over any part of the guide!\nThere should now be a service account created by Snowflake in Google Cloud. Download the JSON file with credentials for that account by referring to this Google Cloud guide. We'll upload this file into PostHog in the app configuration step.\n\nMake sure the user available to the plugin has permissions on the storage integration you just created. You can do this like so:\n`sql\nGRANT USAGE ON INTEGRATION <your_gcs_integration_name> TO ROLE POSTHOG_EXPORT;`\nConfigure the app within PostHog\nNow that Snowflake is set up, the last step is to configure the app. Below are the required options that you'll need to fill-in to get the app running.\nThere are also additional options available for advanced users who need to customize their integration further.\n`Snowflake account`\nThis can be found in your Snowflake URL. For example, if your URL when viewing Snowflake begins with: `https://app.snowflake.com/us-east-2.aws/xxx1111/...`, then your account ID would be `xxx1111.us-east-2.aws`.\n`Snowflake username`\nThe name of the user in Snowflake that has access to our database. If you followed the tutorial above, this will be `POSTHOG`\n`Snowflake password`\nThe password for the user we created.\n`Snowflake database name`\nThe name of the database where we would like PostHog to store our info. If you followed the tutorial above, set this as `POSTHOG_DB`\n`Snowflake database schema`\nThe schema within our database that we would like to use. If you followed the tutorial above, set this as `PUBLIC`\n`Snowflake table name`\nThe table within our database that we would like to copy our exported data into. If a table with the provided name does not exist yet, it will be created\n`Virtual warehouse to use for copying files`\nThe Warehouse we would like to use for executing our operations. If you followed, the tutorial above, set this as `POSTHOG_WH`\n`Stage type`\nWhether you set up external staging with `S3` or `GCS`. Depending on what you set up, you will need to provide different config information.\n`Name to use for external stage`\nThe name of the stage within Snowflake for sourcing data from.\nAmazon S3\n`Bucket name`\nThe name of the S3 bucket to use for exporting data. This should be without the path or region.\n`AWS access key ID`\nThe access key ID for the IAM user with access to your bucket.\n`AWS secret access key`\nThe secret access key for the IAM user with access to your bucket.\n`S3 Bucket region`\nThe region where your bucket is located. We recommend using the same region as your Snowflake instance.\nGoogle Cloud Storage\n`Bucket name`\nThe name of the GCS bucket to use for exporting data. This should be without the path or region.\n`GCS storage integration name`\nThe name of the integration within Snowflake you created in this step\n`GCS credentials file`\nA JSON file with your credentials for accessing GCS. Instructions on how to get this file can be found in this Google Cloud tutorial\nYour export is now set up!\nNow that the app is configured, all you'll need to do is \"Enable\" it and PostHog will start exporting events to Snowflake!\nTroubleshooting\nWhy am I seeing connection issues with the Snowflake Export app?\nIf you're running into connection issues please verify your login credentials, make sure all the permissions listed above have been granted to your user.\nIf you're exporting from PostHog Cloud, do NOT set any IP whitelist/blacklist or other network policies. PostHog Cloud operates on a decentralized network of computing resources and therefore the IPs could change at any time.\nFurther information\nWho created this app?\nA lot of people worked on this app! We'd like to thank the following contributors for creating the Snowflake Export app. Thank you, all!\n\nYakko Majuri\nMarius Andra\nMichael Matloka\nPaolo D'Amico (We miss you Paolo!)\n\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page or using the Q&A widget at the bottom of the page.",
    "tag": "posthog"
  },
  {
    "title": "patterns-connector.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/patterns-connector.md",
    "content": "\ntitle: Patterns Connector\nlayout: app\ngithub: https://github.com/PostHog/posthog-patterns-app\ninstallUrl: https://app.posthog.com/project/apps?name=Patterns\nthumbnail: ../../apps/thumbnails/patterns-logo.svg\ntags:\n    - patterns\n\nWhat is Patterns?\nPatterns is a data science platform for building and deploying data pipelines, machine learning models, and complex automations. It\u2019s built for data engineers, scientists, and analysts and abstracts away the overhead associated with setting up data infrastructure and having to configure many different tools. At the core is the Patterns protocol, a functional reactive declarative data pipelining framework that makes it easy to chain together Python and SQL scripts.\nUse Patterns with PostHog to calculate metrics such as cohort churn, retention and customer LTV. Or, integrate your PostHog data with other sources such as your marketing data, email marketing tools, payment and billing systems.\nWhat does the Patterns Connector do?\nThe Patterns Connector enables you to send event data from PostHog, to Patterns. This is useful for a number of reasons, including centralizing data into a CDP, or using it to generate customer models.\nWhat are the requirements for this app?\nUsing the Patterns Connector requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nYou'll also need an account with Patterns.\nHow do I install the Patterns Connector on PostHog?\nThis app requires PostHog 1.30.0 or above, or PostHog Cloud. You also need a Patterns account. \n\nLog in to your Patterns account and create a graph \nAdd a webhook node to your graph\nCopy the webhook URL from the sidebar\nLog in to your PostHog instance\nClick 'Apps' on the left-hand tool bar\nSearch for 'Patterns'\nSelect the app, press 'Install', then select the blue gear icon to begin configuration\nPaste the URL in \"Patterns Webhook URL\" during app configuration.\n\nYou can install the connector via the GitHub repo. \nOnce you\u2019ve installed the app and configured the webhook, data will begin streaming into Patterns in real-time. Here is an example app you can clone that can be used to enrich, score, and prioritize new leads to your website \u2014- this is common for optimizing a customer conversion funnel.\nAre there other ways to connect PostHog and Patterns?\nYes. You can also use the PostHog data extractor within Patterns to extract a full historical and incremental load of events. Check this tutorial about connecting PostHog and Patterns for more information.\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the Patterns Connector is available on GitHub.\nWho created this app?\nWe'd like to thank the team at Patterns for creating this app. Thank you!\nWho maintains this app?\nThis app is maintained by the community. For more information, please contact Patterns. \nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "Is the source code for this app available?",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/redshift-export.md",
    "content": "\ntitle: Redshift Export\ngithub: https://github.com/PostHog/redshift-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=Redshift+Export\nthumbnail: ../../apps/thumbnails/redshift.svg\ntags:\n    - redshift-export\n\nWhat are the requirements for this app?\nUsing the Redshift Export app requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nYou'll also need access to a Redshift Cluster to export to.\nHow do I install the Redshift Export app?\n\nCreate a Redshift Cluster\nMake sure PostHog can access your cluster\n\nThis might require a few things:\n\nAllowing public access to the cluster\n\nEnsuring your VPC security group allows traffic to and from the Redshift cluster - If this is not possible in your case, you should consider using our S3 Export app and then setting up your own system for getting data into your Redshift cluster\n\n\nCreate a user with table creation privileges\n\n\nWe need to create a new table to store events and execute `INSERT` queries. You can and should block us from doing anything else on any other tables. Giving us table creation permissions should be enough to ensure this:\n`sql\nCREATE USER posthog WITH PASSWORD '123456yZ';\nGRANT CREATE ON DATABASE your_database TO posthog;`\n\nAdd the connection details at the configuration step in PostHog\n\nConfiguration\n\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the Redshift Export app is available on GitHub.\nWho created this app?\nWe'd like to thank PostHog team members Yakko Majuri and Marius Andra, as well as community member Jean Roman for creating the Redshift Export app.\nWho maintains this app?\nThis app is maintained by PostHog. If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "Substitutes posthog.api_key which still exists but has been deprecated",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/sentry-connector.md",
    "content": "\ntitle: Sentry Connector\ntags:\n    - sentry-connector\n\nWhat does the Sentry Connector do?\nOur Sentry Connector integration is a two-way integration which works on Javascript & Python.\nOnce installed, it will:\n\nAdd a direct link in Sentry to the profile of the person affected in PostHog.\nSend an `$exception` event to PostHog with a direct link to Sentry.\n\nThis way, debugging issues becomes a lot easier, and you can also correlate error data with your product metrics.\nWhat are the requirements for this app?\nThe Sentry Connector requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nYou'll also need an account with Sentry.\nHow do I install the Sentry Connector with Javascript?\nMake sure you're using both PostHog and Sentry as JS modules. You'll need to replace `'your organization'` and `project-id` with the organization and project-id from Sentry.\n\n`'your organization'` will be in the URL when you go to your Sentry instance, like so: `https://sentry.io/organizations/your-organization/projects/`\n`project-id` will be the last few digits in your Sentry DSN, such as `https://adf90sdc09asfd3@9ads0fue.ingest.sentry.io/project-id`\n\n```js\nimport posthog from 'posthog-js'\nimport * as Sentry from '@sentry/browser'\nposthog.init('')\nSentry.init({\n    dsn: '',\n    integrations: [new posthog.SentryIntegration(posthog, 'your organization', project - id)],\n})\n```\nHow do I install the Sentry Connector with Python?\n`bash\npip install posthog`\nIn your app, import the `posthog` library and set your api key and host before making any calls.\n```python\nimport posthog\nSubstitutes posthog.api_key which still exists but has been deprecated\nposthog.project_api_key = ''\nOnly necessary if you want to use feature flags\nposthog.personal_api_key = ''\nYou can remove this line if you're using app.posthog.com\nposthog.host = ''\n```\nYou can read more about the differences between the project and personal API keys in the dedicated API authentication section of the Docs.\n\nNote: As a general rule of thumb, we do not recommend having API keys in plaintext. Setting it as an environment variable would be best.\n\nYou can find your keys in the 'Project Settings' page in PostHog.\nTo debug, you can toggle debug mode on:\n`python\nposthog.debug = True`\nAnd to make sure no calls happen during your tests, you can disable them, like so:\n`python\nif settings.TEST:\n    posthog.disabled = True`\nHow do I use the Sentry Connector?\nOnce installed you'll now have `$exception` events in PostHog, which have a \"Sentry URL\" link to take you to the exception:\nFrom Sentry you will now be able to go directly to the affected person in PostHog and watch the session recording for when the exception happened, see what else the user has done, and find their details. Don't forget to click the little icon to the side of the URL, not the URL itself.\nWho maintains this app?\nThis app is maintained by PostHog. If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "How can I connect the feedback to Slack?",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/feedback-widget.md",
    "content": "\ntitle: Feedback Widget\ngithub: https://github.com/PostHog/feedback-app\ninstallUrl: https://app.posthog.com/project/apps?name=Feedback%20Widget\nthumbnail: ../../apps/thumbnails/feedback-widget.png\ntags:\n    - feedback-widget\n\nWhat does the Feedback Widget app do?\nThis app enables you to gather feedback from your users and ingest it as an event into PostHog. The app accomplishes this by injecting code into your website, such that a small widget appears to prompt users to share feedback.\nSending feedback looks like this...\n\nYou can see the feedback in PostHog as a custom event called `Feedback Sent` with the feedback in the `$feedback` property or through the Feedback Widget app's page.\n\nYou can also use Zapier to send feedback to other tools like Slack, Zendesk, and more.\n\nWhat are the requirements for this app?\nFirst, you need to opt in to the site apps beta. You'll need to be on PostHog cloud or if self-hosting using PostHog 1.41.0 or later.\nYou can opt in to the beta by configuring your `posthog-js` initialization to include `opt_in_site_apps: true`. Please be aware you do this at your own risk.\nSelf-hosted and not running 1.41.0? Find out how to update your self-hosted PostHog deployment!\nHow do I install the Feedback Widget?\n\nOpt into the site apps beta by setting your `posthog-js` initialization to include `opt_in_site_apps: true`.  \nVisit the 'Apps' page in your instance of PostHog.\nSearch for 'Feedback Widget' and select the app, press Install.\nCustomise the text, and enable the plugin\nEither select \"Show feedback button on the page\" or add a button with a corresponding data attribute e.g. `data-attr='posthog-feedback-button'` which when clicked will open the feedback widget\nEach feedback from your users is now captured as a custom `Feedback Sent` event with the feedback in the `$feedback` property\nYou can now use this event to trigger PostHog actions, or create a funnel to see how many users are giving feedback\n\nHow can I connect the feedback to Slack?\n\nInstall the Feedback Widget app as described above\nCreate an action called `Feedback Sent Action` that matches the custom event `Feedback Sent`.\nCheck the box \"Post to webhook when this action is triggered\" with the following message: `[person] sent feedback on [event.properties.$current_url]: [event.properties.$feedback]`\n\nCreate the slack app\nCopy the Webhook URL into the PostHog Setup page\n\nCan I make my own site apps?\nYou certainly can. Check our tutorial about how to build a site app in PostHog to get started.\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the Feedback Widget is available on GitHub.\nWho created this app?\nWe'd like to thank PostHog team members Luke Harries, Marius Andra, and user Ankit Ghosh. Thanks, all!\nWho maintains this app?\nThis app is maintained by PostHog. If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nOr, if you see the feedback widget enabled, use that!\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "user-agent-populator.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/user-agent-populator.md",
    "content": "\ntitle: User Agent Populator\ngithub: https://github.com/weyert/useragentplugin\ninstallUrl: https://app.posthog.com/project/apps?name=User%20Agent%20Populator\nthumbnail: ../../apps/thumbnails/user-agent-enhancer.png\ntags:\n    - user-agent\n\nThis app enhances incoming events by including browser & OS details from the `$useragent` property.\n\nNote:  This app is generally only needed when using clients that don't already set these properties, or when sending events directly from the server.\n\nThis app extracts the following properties from the provided `$useragent`:\n| Property | Description | Example|\n| --- | --- | --- |\n| `$browser` | Name of the browser for the user | Chrome, Firefox |\n| `$browser_version` | The version of the browser that was used | 70, 79 |\n| `$os` | The operating system of the user | Windows, Mac OS X |\n| `$browser_type` | The type of client that made the request | bot, browser |\nInstallation\nPostHog Cloud\nPostHog Cloud users can find the app here in their dashboard.\nBefore you can enable the app, you will need to configure it by clicking on the settings icon.\nOnce the app has been configured, you can enable it by flipping the toggle and it will start transforming all new events.\nPostHog Self-hosted\n\nThe User Agent Populator requires a PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update!\n\n\nVisit the 'Apps' page in your instance of PostHog.\nSearch for 'User Agent Populator'.\nConfigure the app by clicking on the settings icon.\nClick the toggle to enable the app.\n\nOnce the app has been configured and enabled, it will begin to transform all new events which come into PostHog.\nConfigure\nBefore an app can be enabled in PostHog, it has to be configured.\n\nUsing the User Agent Populator\nThis app works by parsing the `$useragent` property on events as they are ingested.\nAs a result, if an event is ingested without the `$useragent` (or `$user-agent`) property set, this app will do nothing.\nThis property can be set using any of our client or server libraries.\n\nNote:  Most of our client libraries will already automatically extract the `$browser`, `$browser_version`, and `$os` properties, so there is no need to set the `$useragent` property when using these libraries.\n\nOne common use-case for this app is populating client information when sending events from the server-side. Typically, a `UserAgent` header will be set when a client sends a request to your server, which your server can then forward to PostHog with the `$useragent` property.\nThis gives you an idea of what types of clients are using your service and allows you to create insights that filter based on these properties.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Questions page or using the Q&A widget at the bottom of this page.\nYou can also join the PostHog Community Slack group to collaborate with others and get advice on developing your own PostHog apps.\nFurther information\nWho created this app?\nThis app was created by the community. We'd like to thank Weyert for creating the User Agent Populator, as well as for all the other support and feedback. Thank you, Weyert!\nWho maintains this app?\nThis app is maintained by the community. If you have issues with the app not functioning as intended, please raise an issue on the repo.\nWhat if I have feedback on this app?",
    "tag": "posthog"
  },
  {
    "title": "url-normalizer.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/url-normalizer.md",
    "content": "\ntitle: URL Normalizer\ngithub: https://github.com/PostHog/posthog-url-normalizer-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=URL%20Normalizer\nthumbnail: ../../apps/thumbnails/url_normalizer.png\ntags:\n    - url-normalizer\n\nWhat does the URL Normalizer app do?\nThis app normalizes the format of URLs so you can more easily compare them in insights.\nBy default, the URL Normalizer converts all URLs to lowercase and strips trailing /s, overriding the old `current_url` property.\nWhat are the requirements for this app?\nThe URL Normalizer app requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nHow do I install the URL Normalizer?\n\nVisit the 'Apps' page in your instance of PostHog.\nSearch for 'URL Normalizer' and select the app, press Install.\n\nHow do I configure the URL Normalizer?\nNo configuration is needed. The URL Normalizer will automatically convert all URLs to lowercase and remove any trailing slashes.\nIf you'd like to normalize URLs into a different format, please consider contributing a PR to the repo.\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the URL Normalizer is available on GitHub.\nWho created this app?\nWe'd like to thank PostHog user and community member Mark Bennett for creating this app\nWho maintains this app?\nThis app is maintained by the PostHog Community. If you have issues with the app not functioning as intended, please raise a bug report.\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "customer-io.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/customer-io.md",
    "content": "\ntitle: Customer.io Connector\ngithub: https://github.com/PostHog/customerio-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=Customer.io\nthumbnail: ../../apps/thumbnails/customerio-connector.png\ntags:\n    - customer.io-connector\n\nWhat does the Customer.io Connector app do?\nThe Customer.io Connector sends event data from PostHog into Customer.io. User emails will also be sent if available and customers will be created in Customer.io.\nWhat are the requirements for this app?\nThe Customer.io Connector requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nYou'll also need access to the relevant Customer.io account.\nHow do I install the Customer.io Connector app?\n\nVisit the 'Apps' page in your instance of PostHog.\nSearch for 'Customer.io' and select the app, press Install.\nAdd your Customer.io site ID and token at the configuration step.\nEnable the app and watch your 'People' list get populated in Customer.io!\n\nConfiguration\n\nHow do I match persons in PostHog with customers in Customer.io?\nWe assume that you use the same ID to identify users in Customer.io as you use as distinct_id or in `posthog.identify()`.\nHow do I set properties on a Customer.io customer via PostHog?\nPostHog will send any property inside the `$set: {}` property to customer.io. In the example below, `email` and `userProperty` will be set on the customer\n`js\nposthog.capture(\n  'some event',\n  {\n    event_property: 'this will not get sent',\n    $set: {\n      email: 'test@example.com',\n      userProperty: 'value'\n    }\n  }\n)`\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the Customer.io Connector is available on GitHub.\nWho created this app?\nWe'd like to thank PostHog team members Yakko Majuri and Marius Andra, Michael Matloka and community members Angela Purcell, Conrad Kurth and Alberto S for creating the Customer.io Connector.\nWho maintains this app?\nThis app is maintained by PostHog. If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "n8n.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/n8n.md",
    "content": "\ntitle: n8n Connector\nthumbnail: ../../apps/thumbnails/n8n.png\ntags:\n    - n8n\n\nWhat does the n8n Connector app do?\nThis n8n app enables you to automate PostHog workflows and create workflows using other services, using n8n.\nHow do I get started with the n8n Connector app?\nThe n8n app is an API integration. You will need a PostHog Personal API key, which must be authenticated with n8n in order to connect n8n and get started.\nFor more information, please check n8n's integration documentation.\nWhat are the requirements for this app?\nUsing the n8n Connector requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nWhere can I find out more?\nCheck PostHog's API documentation for more information on pulling and pushing data from/to our API. Further information about n8n's connector is available in n8n's connector documentation.\nWho maintains this app?\nThis app is maintained by n8n. For more information, check n8n's connector documentation.\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "github-release-tracker.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/github-release-tracker.md",
    "content": "\ntitle: GitHub Release Tracker\ngithub: https://github.com/PostHog/github-release-tracking-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=Github+Release+Tracker\nthumbnail: ../../apps/thumbnails/github.png\ntags:\n    - github-release-tracker\n\nWhat does the GitHub Release Tracker do?\nThis app enables you to tracker releases in GitHub as events in PostHog. This is useful for correlating releases with user behaviour or performance issues, for example.\nWhat are the requirements for this app?\nThe GitHub Release Tracker requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nIf you want to track releases from a private GitHub repo, you'll also need access to it.\nHow do I install the GitHub Release Tracker for PostHog?\n\nLog in to your PostHog instance\nClick 'Apps' on the left-hand tool bar\nSearch for 'GitHub Release Tracker'\nSelect the app, press 'Install' and follow the on-screen instructions\n\nConfiguration\n\nIs the source code for this app available?\nPostHog is open-source and so are all apps on the platform. The source code for the GitHub Release Tracker is available on GitHub.\nWho created this app?\nWe'd like to thank PostHog team members Yakko Majuri, Marius Andra and Michael Matloka and community member Mack Etherington for creating the GitHub Release Tracker.\nWho maintains this app?\nThis app is maintained by PostHog. If you have issues with the app not functioning as intended, please raise a bug report to let us know!\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "zapier-connector.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/zapier-connector.md",
    "content": "\ntitle: Zapier Connector\ngithub: https://github.com/PostHog/posthog-zapier/\ninstallUrl: https://zapier.com/apps/posthog/integrations\nthumbnail: ../../apps/thumbnails/zapier.svg\ntags:\n    - zapier-connector\n\nWhat does the Zapier Connector app do?\nThe Zapier app for PostHog enables you to connect PostHog with thousands of services through Zapier.\nThe following steps are at your disposal:\n| Type    | Name               | Plan Required       |\n| :------ | :----------------- | :------------------ |\n| Action  | Capture Event      | Any, including free |\n| Trigger | Action Performed   | Any paid plan   |\n| Trigger | Action Defined     | Any paid plan   |\n| Trigger | Annotation Created | Any paid plan   |\nWhat are the requirements for this app?\nConnecting Zapier requires either PostHog Cloud, or a self-hosted PostHog instance running version 1.30.0 or later.\nNot running 1.30.0? Find out how to update your self-hosted PostHog deployment!\nYou'll also need a Zapier account which can connect to the other services you want to use via this app.\nHow do I connect PostHog to other services with Zapier?\nOur official Zapier app is compatible with PostHog Cloud, as well as with self-hosted instances. No additional setup needed.\nHow can I create a private PostHog connection with Zapier?\nCreate a private Zapier app for PostHog easily with a few simple steps:\n\nClone or download this repository.\nEnter its directory with `cd`.\nInstall Node modules.\n    `bash\n    npm install`\nUpdate `DEFAULT_API_HOST` value in `src/utils.ts` (e.g. for PostHog Cloud it's `app.posthog.com` and for your self-hosted instance it may be `posthog.example.com`).\nGlobally install Zapier CLI.\n    `bash\n    npm install -g zapier-platform-cli`\nLog into Zapier from the command line.\n    `bash\n    zapier login`\nEither register a new integration on Zapier.\n    `bash\n    zapier register \"PostHog @ $YOUR_ORG\"`\n    Or link to an existing one.\n    `bash\n    zapier link`\nPush to Zapier.\n    `bash\n    npm run push`\nFinish by filling in integration details in the Zapier Platform dashboard. And don't forget to invite users!\n\nWhere can I find out more?\nFurther information about the Zapier Connector is available in Zapier's integration documentation.\nWhat if I have feedback on this app?\nWe love feature requests and feedback! Please create an issue to tell us what you think.\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page.",
    "tag": "posthog"
  },
  {
    "title": "s3-export.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/s3-export.md",
    "content": "\ntitle: Amazon S3 Export\ngithub: https://github.com/PostHog/s3-export-plugin\ninstallUrl: https://app.posthog.com/project/apps?name=S3+Export+Plugin\nthumbnail: ../../apps/thumbnails/s3.svg\nofficial: true\ntags:\n    - s3 export\n\nThis app enables you to export events to Amazon S3 on ingestion. Archive your data, or simply free it up for other kinds of analysis, by integrating export right into your event processing pipeline.\n\nNote:  Currently, this plugin does not support exporting of historical data and will only export data that is ingested while the app is enabled. We're actively looking into adding support for historical exports.\n\nInstall\nPostHog Cloud\nPostHog Cloud users can find the app here on the apps page.\nBefore you can enable the app, you will need to configure it by clicking on the settings icon.\nOnce the app has been configured, you can enable it by flipping the toggle and it will start exporting newly ingested events to S3.\nPostHog Self-hosted\n\nThe S3 Export Plugin requires a PostHog instance running version 1.24.0 or later.\nNot running 1.24.0? Find out how to update your self-hosted PostHog deployment!\n\n\nLog in to your PostHog instance\nClick 'Apps' on the left-hand navigation\nSearch for 'S3'\nSelect the 'S3 Export Plugin' and press 'Install'\nConfigure the app by entering your AWS credentials and S3 bucket details\nEnable the app and Watch events roll into S3!\n\nConfigure\nBefore we can get things set up in PostHog, we will need to work on creating an S3 Bucket in AWS which we will use as a destination for our events.\nAlready have a bucket set up? Skip here to start configuring access to your bucket.\nCreating a bucket\n\nLog in to AWS.\nOpen S3 in the AWS console and create a new bucket in your chosen region.\n\n\n\nMake sure to take note of both the name of your bucket and the region that it's in, we'll need these later.\n\nSet up access\nNext, we'll need to create a new user in our AWS console with programmatic access to our newly created bucket.\n\nOpen IAM and create a new policy to allow access to this bucket.\nOn the left under \"Access management\" select \"Policies\" and click \"Create policy\"\nUnder the service, choose \"S3\"\nUnder \"Actions\" select\n\"Write\" -> \"PutObject\"\n\"Permission Management\" -> \"PutObjectAcl\"\n\n\nUnder \"Resources\" select \"Specific\" and click \"object\" -> \"Add ARN\"\nSpecify your bucket name and choose \"any\" for the object name. In the example below, replace `posthog-s3-export` with the bucket name you chose in the previous section\n\n\n\nYour config should now look like the following\n\n\n\nClick \"Next\" until you end up on the \"Review Policy\" page\nGive your policy a name and click \"Create policy\"\n\nThe final step is now to create a new user and give them access to our Bucket by attaching our newly created Policy.\n\nOpen IAM and navigate to \"Users\" on the left\nClick \"Add Users\"\nSpecify a name and make sure to choose \"Access key - Programmatic access\"\n\n\n\nClick \"Next\"\nAt the top, select \"Attach existing policies directly\"\nSearch for the policy you just created and click the checkbox on the far left to attach it to this user\n\n\n\nClick \"Next\" until you reach the \"Create user\" button. Click that as well.\nMake sure to copy your \"Access key\" and \"Secret access key\". The latter will not be shown again.\n\n\nConfigure the app\nNow that we have our Bucket set up, we have all the information we need to set up the S3 Export app!\n\nStart by navigating back to your PostHog dashboard and opening the \"Apps\" page.\nOpen the configuration for the S3 Export plugin by clicking the settings icon.\nFirst, we'll fill in the information for our S3 Bucket\n\n\n\nNote:  Make sure to replace the `S3 Bucket name` and `AWS Region` with the values for your own bucket!\n\nFinally, we'll configure the rest of the settings. Below is a list of a few important options, with a more detailed list of all available options in the next section.\n\n`S3 filename prefix`\nThis option is used if you need to prefix filenames with a specific identifier, or if you want to store exports in a specific directory within your Bucket. The latter option can be done using a prefix which ends in a trailing slash (e.g. /posthog/exports)\n\n\n`Upload at most every X minutes`\nDefault: Every minute\nThis option sets how frequently the app will check for new events. This plugin buffers events in-memory before uploading them, and this timeout determines how frequently it will check this buffer for new events.\n\n\n`Maximum upload size`\nDefault: 1 MB\nEach time an upload is ingested into PostHog, this plugin will check whether its internal buffer is under this limit. If at any point it exceeds this limit, it will immediately flush events to S3. As a result, this plugin may upload more frequently than specified in the `Upload at most every X minutes` settings if the buffer frequently reaches this limit.\n\n\n\nBatch uploads\nTo vastly increase export throughput, this app batches events in memory before uploading them to S3.\nUpload frequency (every minute by default) and maximum upload size (1 MB by default) can be configured when the app is installed.\nYou should make sure to keep these numbers reasonable to avoid running out of memory on your server. Note that the values apply to each concurrent app server thread.\nConfiguration\nBasic Options\n| Option                                                                                                                         | Description                                                                                                                                                                                      |\n| ------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| `AWS Access Key`Type: StringRequired: Yes                     | The Access Key ID for the User you created in AWS.                                                                                                                                               |\n| `AWS Access Secret Key`Type: StringRequired: Yes              | The secret Access Key for the User you created in AWS.                                                                                                                                           |\n| `AWS Region`Type: StringRequired: Yes                         | The AWS region where you created the destination bucket.                                                                                                                                         |\n| `S3 Bucket name`Type: StringRequired: Yes                     | The name of the bucket where you would like to send exports.                                                                                                                                     |\n| `S3 filename prefix`Type: StringRequired: No                  | This will be prepended to every file sent to S3. Allows you to specify a directory to store exports in (backups/posthog/ with a trailing slash).                                                 |\n| `Upload at most every X minutes`Type: NumberRequired: Yes     | This option sets the interval at which the plugin will check if there are new events to export. The value must be between 1 and 60 (minutes).                                                    |\n| `Maximum upload size (megabytes)`Type: NumberRequired: Yes    | The maximum single file size that this plugin will send to S3. When the buffer reaches this limit, the app will automatically send these events to S3. The value must be between 1 and 100 MB. |\n| `Events to ignore`Type: StringRequired: No                    | Comma separated list of events to ignore                                                                                                                                                         |\n| `Format of uploaded files`Options: `jsonl`Default: `jsonl`    | Currently, we only support exports in `jsonl`, which serializes events in JSON, one event per line.                                                                                              |\n| `Compression`Options: `gzip`, `brotli`, `none`Default: `gzip` | The method used to compress events before sending. Upload size limits are applied before compression.                                                                                          |\nAdvanced Options\n| Option                                                                                                                                             | Description                                                                                                                                                                                    |\n| -------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `S3 Bucket endpoint`Type: StringRequired: No                                      | Used when exporting to S3-compatible storage other than AWS. For example: `http://127.0.0.1:9000`, which is a minio or localstack instance that is API compatible with Amazon S3               |\n| `Signature version`Options: `V4`                                                            | AWS Signature version used when signing uploads.                                                     |\n| `Server-side encryption`Options: `disabled`, `AES256`, `aws:kms`Default: `AES256` | Specifies server-side encryption of the object in S3.                                                                                                                                          |\n| `SSE KMS key ID`Type: StringRequired: No                                          | The customer-managed AWS Key Management Service (KMS) key ID that should be used to server-side encrypt the object in S3. Only used if the `Server-side encryption` option is set to `aws:kms` |\nFurther information\nWho created this app?\nA lot of people worked on this app! We'd like to thank the following contributors for creating the S3 Export app. Thank you, all!\n\nYakko Majuri\nMarius Andra\nMichael Matloka\nMaximilian Ferdinand M\u00fcller\nSam Winslow\nStackoverFrog\n\nWhat if my question isn't answered above?\nWe love answering questions. Ask us anything via our Support page or using the Q&A widget at the bottom of the page.",
    "tag": "posthog"
  },
  {
    "title": "`plugin.json` file",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/build/reference.md",
    "content": "\ntitle: Apps developer reference\n\nNote: It's worth reading the Building apps overview for a quick introduction to how to build your own app.\n\n`plugin.json` file\nA `plugin.json` file is structured as follows:\n`json\n{\n  \"name\": \"<plugin_display_name>\",\n  \"url\": \"<repo_url>\",\n  \"description\": \"<description>\",\n  \"main\": \"<entry_point>\",\n  \"config\": [\n    {\n      \"markdown\": \"A Markdown block.\\n[Use links](http://example.com) and other goodies!\"\n    },\n    {\n      \"key\": \"param1\",\n      \"name\": \"<param1_name>\",\n      \"type\": \"<param1_type>\",\n      \"default\": \"<param1_default_value>\",\n      \"hint\": \"<param1_hint_value>\",\n      \"required\": true,\n      \"secret\": true\n    },\n    {\n      \"key\": \"param2\",\n      \"name\": \"<param2_name>\",\n      \"type\": \"<param2_type>\",\n      \"default\": \"<param2_default_value>\",\n      \"required\": false\n    }\n  ]\n}`\nHere's an example `plugin.json` file from our 'Hello world app':\n`json\n{\n  \"name\": \"Hello World\",\n  \"url\": \"https://github.com/PostHog/helloworldplugin\",\n  \"description\": \"Greet the World and Foo a Bar, JS edition!\",\n  \"main\": \"index.js\",\n  \"config\": [\n    {\n      \"markdown\": \"This is a sample app!\"\n    },\n    {\n      \"key\": \"bar\",\n      \"name\": \"What's in the bar?\",\n      \"type\": \"string\",\n      \"default\": \"baz\",\n      \"hint\": \"This will be sent in a **property**\",\n      \"required\": false\n    }\n  ]\n}`\nMost options in this file are self-explanatory, but there are a few worth exploring further:\n`main`\n`main` determines the entry point for your app, where your `setupPlugin` and `processEvent` functions are. More on these later.\n`config`\n`config` consists of an array of objects that each pertain to a specific configuration field or markdown explanation for your plugin.\nEach object in a config can have the following properties:\n|   Key    |                    Type                    |                                                                           Description                                                                           |\n| :------ | :---------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n|   type   | `\"string\"` or `\"attachment\"` or `\"choice\"` | Determines the type of the field - \"attachment\" asks the user for an upload, and \"choice\" requires the config object to have a `choices` array, explained below |\n|   key    |                  `string`                  |                                     The key of the app config field, used to reference the value from inside the app                                      |\n|   name   |                  `string`                  |                                          Displayable name of the field - appears on the app setup in the PostHog UI                                          |\n| default  |                  `string`                  |                                                                   Default value of the field                                                                    |\n|   hint   |                  `string`                  |                                             More information about the field, displayed under the in the PostHog UI                                             |\n| markdown |                  `string`                  |                                                             Markdown to be displayed with the field                                                             |\n|  order   |                  `number`                  |                                                                           Deprecated                                                                            |\n| required |                 `boolean`                  |                                               Specifies if the user needs to provide a value for the field or not                                               |\n|  secret  |                 `boolean`                  |                     Secret values are write-only and never shown to the user again - useful for apps that ask for API Keys, for example                      |\n| choices  |                  `string[]`                   |                           Only accepted on configs with `type` equal to `\"choice\"` - an array of choices (of type `string`) to be presented to the user                            |\n\nNote: You can have a config field that only contains `markdown`. This won't be used to configure your app but can be placed anywhere in the `config` array and is useful for customizing the content of your app's configuration step in the PostHog UI.\n\n`PluginMeta`\n\nCheck out App Types for a full spec of types for app authors.\n\nEvery plugin server function is called by the server with an object of type `PluginMeta` that will always contain the object `cache`, and can also include `global`, `attachments`, and `config`, which you can use in your logic. \nHere's what they do:\n`config`\nGives you access to the app config values as described in `plugin.json` and configured via the PostHog interface.\nExample:\n`js\nexport async function processEvent(event, { config }) {\n    event.properties['greeting'] = config.greeting\n    return event\n}`\n`cache`\nA way to cache values globally across plugin reloads. The values are stored in Redis, an in-memory store. This storage is not persistent, so values can be dropped by the system.\nThe `cache` type is defined as follows:\n`js\ninterface CacheExtension {\n    set: (key: string, value: unknown, ttlSeconds?: number, options?: CacheOptions) => Promise<void>\n    get: (key: string, defaultValue: unknown, options?: CacheOptions) => Promise<unknown>\n    incr: (key: string) => Promise<number>\n    expire: (key: string, ttlSeconds: number) => Promise<boolean>\n    lpush: (key: string, elementOrArray: unknown[]) => Promise<number>\n    lrange: (key: string, startIndex: number, endIndex: number) => Promise<string[]>\n    llen: (key: string) => Promise<number>\n}`\nStoring values is done via `cache.set`, which takes a key and a value, as well as an optional value in seconds after which the key will expire.\nRetrieving values uses `cache.get`, which takes the key of the value to be retrieved, as well as a default value in case the key does not exist.\nYou can also use `cache.incr` to increment numerical values by 1, and `cache.expire` to make keys volatile, meaning they will expire after the specified number of seconds.\nMethods `cache.lpush`, `cache.lrange`, and `cache.llen` enable operations on Redis lists.\nAll the above methods represent their equivalent Redis commands \u2013 see Redis documentation:\n\nSET\nGET\nINCR\nEXPIRE\nLPUSH\nLRANGE\nLLEN\n\nExample:\n`js\nexport function processEvent(event, { config, cache }) {\n    const counterValue = (await cache.get('greeting_counter', 0))\n    await cache.set('greeting_counter', counterValue + 1)\n    if (!event.properties) event.properties = {}\n    event.properties['greeting_counter'] = counterValue\n    return event\n}`\n`global`\nThe `global` object is used for sharing functionality between `setupPlugin` and the rest of the special functions, like `processEvent`, `onEvent`, or `runEveryMinute`, since global scope does not work in the context of PostHog apps. `global` is not shared across worker threads\nExample:\n```js\nexport function setupPlugin({ global, config }) {\n    global.eventsToTrack = (config.eventsToTrack || '').split(',') \n}\nexport function processEvent(event, { global, config }) {\n    if(global.eventsToTrack.includes(event.event)) {\n        // Do something\n    }\n}\n```\n`attachments`\n`attachments` gives access to files uploaded by the user for config parameters of type `attachment`. An `attachment` has the following type definition:\n`js\ninterface PluginAttachment {\n    content_type: string\n    file_name: string\n    contents: any\n}`\nAs such, accessing the contents of an uploaded file can be done with `attachments.attachmentName.contents`.\nExample:\n`js\nexport function setupPlugin({ attachments, global }: Meta) {\n    if (attachments.maxmindMmdb) {\n        global.ipLookup = new Reader(attachments.maxmindMmdb.contents)\n    }\n}`\n`jobs`\nThe `jobs` object gives you access to the jobs you specified in your app. See Jobs for more information.\n`geoip`\n`geoip` provides a way to interface with a MaxMind database running in the app server to get location data for an IP address. It is primarily used for the PostHog GeoIP plugin.\nIt has a `locate` method that takes an IP address and returns an object possibly containing `city`, `location`, `postal`, and `subdivisions`.\nRead more about the response from `geoip.locate` here.\nMaximizing reliability with `RetryError`\nSince plugins generally handle data in some way, it's crucial for data integrity that each plugin is as reliable as possible. One system-level mechanism you can leverage to improve reliability is function retries.\nWhile normally a plugin function simply fails without ceremony the moment it throws an error, select functions can be retried by throwing a special error type: `RetryError` \u2013 which is included in the `@posthog/plugin-scaffold` package.\nAs an example, it's safe to assume that a connection to an external service will fail eventually. Due to security considerations, `setTimeout` cannot be used in a plugin to wait until the network problem has passed, but with function retries the solution is even simpler! Just `catch` the connection error and `throw new RetryError` \u2013 the system will re-run the function for you:\n```js\nimport { RetryError } from '@posthog/plugin-scaffold'\nexport function setupPlugin() {\n    try {\n        // Some network connection\n    } catch {\n        throw new RetryError('Service is unavailable, but it might be back up in a moment')\n    }\n}\n```\nAt the same time, make sure NOT to use `RetryError` when the problem cannot be intermittent \u2013 perhaps an invalid config, an unhandled edge case, or just a random bug in the code of the plugin. Retrying such a case would just put extra load on the system, without any benefit.\n```js\nimport { RetryError } from '@posthog/plugin-scaffold'\nexport function setupPlugin({ config }) {\n    let eventsToTrack\n    try {\n        eventsToTrack = config.nonExistentKey.split(',')\n    } catch {\n        throw new RetryError('Retrying this will never help')\n    }\n}\n```\nThe maximum number of retries is documented with each function, as it might differ across them. However, the mechanism is constant in its use of exponential backoff, that is: the wait time between retries is doubled with each attempt. For instance, if the 1st retry takes place 1 s after the initial failure, the gap between the 5th and the 6th will be 32 s (`2^5`).\nAs of PostHog 1.37+, the following functions are retriable:\n- `setupPlugin`\n- `onEvent`\n- `exportEvents`\n`setupPlugin` function\n`setupPlugin` is a function you can use to dynamically set app configuration based on the user's inputs at the configuration step. \nYou could, for example, check if an API Key inputted by the user is valid and throw an error if it isn't, prompting PostHog to ask for a new key.\nIt takes only an object of type `PluginMeta` as a parameter and does not return anything.\nExample (from the PostHog MaxMind app):\n`js\nexport function setupPlugin({ attachments, global }) {\n    if (attachments.maxmindMmdb) {\n        global.ipLookup = new Reader(attachments.maxmindMmdb.contents)\n    }\n}`\n`setupPlugin` can be retried up to 5 times (first retry after 5 s, then 10 s after that, 20 s, 40 s, lastly 80 s) by throwing RetryError. Attempting to retry more than 5 times disables the plugin. The plugin is disabled immediately if any error other than `RetryError` is thrown in `setupPlugin`.\nOn PostHog Cloud and email-enabled instances of PostHog, project members are notified by email of the plugin being disabled automatically. This is to ensure that action is taken if the plugin is important for data integrity.\n`teardownPlugin` function\n`teardownPlugin` is ran when an app VM is destroyed, because of, for example, a app server shutdown or an update to the app. It can be used to flush/complete any operations that may still be pending, like exporting events to a third-party service.\n`js\nasync function teardownPlugin({ global }) {\n  await global.buffer.flush()\n}`\n`processEvent` function\n\nIf you were using `processEventBatch` before, you should now use `processEvent`. `processEventBatch` has been deprecated.\n\n`processEvent` is the juice of your app. \nIn essence, it takes an event as a parameter and returns an event as a result. In the process, this event can be:\n\nModified\nSent somewhere else\nNot returned (preventing ingestion)\n\nIt takes an event and an object of type `PluginMeta` as parameters and returns an event.\nHere's an example (from the 'Hello World App'):\n```js\nasync function processEvent(event, { config, cache }) {\n    const counter = await cache.get('counter', 0)\n    cache.set('counter', counter + 1)\n\n\n```if (event.properties) {\n    event.properties['hello'] = 'world'\n    event.properties['bar'] = config.bar\n    event.properties['$counter'] = counter\n}\n\nreturn event\n```\n\n\n}\n```\nAs you can see, the function receives the event before it is ingested by PostHog, adds properties to it (or modifies them), and returns the enriched event, which will then be ingested by PostHog (after all apps run).\n`onEvent` function\n\nMinimum PostHog version: 1.25.0 \n\n`onEvent` works similarly to `processEvent`, except any returned value is ignored by the app server. In other words, `onEvent` can read an event but not modify it. \nIn addition, `onEvent` functions will run after all enabled apps have run `processEvent`. This ensures you will be receiving an event following all possible modifications to it.\nThis was originally built for and is particularly useful for export apps. These apps need to receive the \"final form\" of an event and send it out of PostHog, without having to modify it.\nHere's a quick example:\n```js\nasync function onEvent(event) {\n    // do something to the event\n    sendEventToSalesforce(event)\n\n\n```// no need to return anything\n```\n\n\n}\n```\n`onEvent` can be retried up to 5 times (first retry after 5 s, then 10 s after that, 20 s, 40 s, lastly 80 s) by throwing RetryError. Attempting to retry more than 5 times is ignored.\nScheduled tasks\nApps can also run scheduled tasks through the functions:\n\n`runEveryMinute`\n`runEveryHour`\n`runEveryDay`\n\nThese functions only take an object of type `PluginMeta` as a parameter and do not return anything.\nExample usage:\n```js\nasync function runEveryMinute({ config }) {\n    const url =`https://api.github.com/repos/PostHog/posthog`\n    const response = await fetch(url)\n    const metrics = await response.json()\n// posthog.capture is also available in apps by default\n    posthog.capture('github metrics', { \n        stars: metrics.stargazers_count,\n        open_issues: metrics.open_issues_count,\n        forks: metrics.forks_count,\n        subscribers: metrics.subscribers_count\n    })\n}\n```\nIt's worth noting that scheduled tasks are debounced, meaning that only a single run of a given task can be in progress at any given time. For example, if a `runEveryMinute` run takes more than a minute, it will make the system skip each following run until that current one has finished \u2013 then, the schedule will resume normally.\n`exportEvents`\n`exportEvents` was built to make exporting PostHog events to third-party services (like data warehouses) extremely easy. \nExample:\n`js\nasync function exportEvents(events, meta) {\n  try {\n    // send events somewhere\n  } catch {\n    throw new RetryError('Service is down')\n  }\n}`\nIn the background, `exportEvents` sets up asynchronous processing of batches and ensures the events in the batch have already been processed by all enabled apps. `exportEvents` can be retried up to 3 times (first retry after 6 s, then 12 s after that, 24 s) by throwing RetryError. Attempting to retry more than 3 times is ignored.\nUsing the PostHog API\nAll apps have access to the PostHog API which can be used to read and create almost anything within PostHog, as well as send additional events.\nFor more information on using the API, take a look at this guide.\nAvailable packages and imports\nApps have access to some special objects in the global scope, as well as a variety of libraries for importing. Scheduling functions (`setInterval`, `setTimeout` and `setImmediate`) are not available. Use jobs instead.\nGlobal\n`fetch`\n\n\n\u26a0\ufe0f Be very careful when using `fetch` to send events to a PostHog instance from `processEvent` or `onEvent`! The event captured will also be run through all the installed apps and could potentially lead to an infinite loop of event generation.\n\n\nEquivalent to node-fetch.\nAvailable imports\n| Import name | Description |\n| :--------- | :--------- | \n| `crypto`    | Node.js standard lib's crypto module |\n| `url`    | Node.js standard lib's url module |\n| `zlib`    | Node.js standard lib's zlib module |\n| `generic-pool`    | npm package generic-pool |\n| `pg`    | npm package node-postgres |\n| `snowflake-sdk`    | npm package snowflake-sdk |\n| `aws-sdk`    | npm package aws-sdk |\n| `@google-cloud/bigquery`    | npm package @google-cloud/bigquery |\n| `@google-cloud/storage`    | npm package @google-cloud/storage |\n| `@google-cloud/pubsub`    | npm package @google-cloud/pubsub |\n| `node-fetch`    | npm package node-fetch |\n| `@posthog/plugin-scaffold`    | Types for PostHog plugins. npm package @posthog/plugin-scaffold |\n| `@posthog/plugin-contrib`    | Helpers for plugin devs maintained by PostHog. npm package @posthog/plugin-contrib |\nExample\nHere's an example of the use of imports from the BigQuery plugin:\n`js\nimport { createBuffer } from '@posthog/plugin-contrib'\nimport { Plugin, PluginMeta, PluginEvent, RetryError } from '@posthog/plugin-scaffold'\nimport { BigQuery, Table, TableField, TableMetadata } from '@google-cloud/bigquery'`\nYou can also use `require` for imports.\nJobs\nApps can schedule tasks in PostHog that run asynchronously on a given schedule. These can be used to import or export data, as well as creating other resources in PostHog such as annotations and cohorts.\nFor more information on jobs, check out this guide.\nTesting\nIn order to ensure apps are stable and work as expected for all their users, we highly recommend writing tests for every app you build.\nAdding testing capabilities to your app\nYou will need to add jest and our app testing scaffold to your project in your `package.json` file:\n`json\n\"jest\": {\n    \"testEnvironment\": \"node\"\n},\n\"scripts\": {\n    \"test\": \"jest .\"\n},\n\"devDependencies\": {\n    \"@posthog/plugin-scaffold\": \"*\",\n    \"jest\": \"^27.0.4\"\n}`\nCreate your test files e.g. `index.test.js` or `index.test.ts` for testing your `index.js` or `index.ts` file\nWriting tests\nWrite tests in jest, you can learn more about the syntax and best practices in the jest documentation. We recommend writing tests to cover the primary functions of your app (e.g. does it create events in the expected format) and also for edge cases (e.g. does it crash if no data is sent).\nFor more information on how to setup testing, take a look at this guide.\nLogs\nApps can make use of the `console` for logging and debugging. `console.log`, `console.warn`, `console.error`, `console.debug`, `console.info` are all supported.",
    "tag": "posthog"
  },
  {
    "title": "jobs.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/build/jobs.md",
    "content": "\ntitle: Jobs\n\nMinimum PostHog version: 1.25.0\n\nJobs are a way for app developers to schedule and run tasks asynchronously using a powerful scheduling API.\nJobs make possible use cases such as retrying failed requests, a key component of apps that export data out of PostHog.\nSpecifying jobs\nTo specify jobs, you should export a `jobs` object mapping string keys (job names) to functions (jobs), like so:\n`js\nexport const jobs = {\n    retryRequest: (request, meta) => {\n        fetch(request.url, request.options)\n    }\n}`\nJob functions can optionally take a payload as their first argument, which can be of any type. They can also access the `meta` object, which is appended as an argument to all app functions, meaning that it will be the second argument in the presence of a payload, and the first (and only) argument in the absence of one.\nTriggering a job\nJobs are accessed as `jobs` via the `meta` object. Triggering a job works as follows:\n`js\nawait jobs.retryRequest(request).runIn(30, 'seconds')\nawait jobs.retryRequest(request).runNow()\nawait jobs.retryRequest(request).runAt(new Date())`\nHaving gotten a job function via its key from the `jobs` object, calling the function with the desired payload will return another object with 3 functions that can be used to schedule your job. They are:\n\n`runNow`: Runs the job now, but does so asynchronously\n`runAt`: Takes a JavaScript `Date` object that specifies when the job should run\n`runIn`: Takes a duration as a `number` and a unit as a `string` specifying in how many units of time to run this job (e.g. 1 hour)\n\n\nAccepted values for the unit argument of `runIn` are: 'milliseconds', 'seconds', 'minutes', 'hours', 'days', 'weeks', 'months', 'quarters', and 'years'. The function will accept these in both singular (e.g. 'second') or plural (e.g. 'seconds') form.\n\nAll jobs return a promise that does not resolve to any value. \nFull example\n```js\nexport const jobs = {\n    continueSearchingForTheTeapot: async (request, meta) => {\n        await lookForTheTeapot(request)\n    }\n}\nasync function lookForTheTeapot (request) {\n    const res = await fetch(request.url)\n    if (res.status !== 418) {\n        await jobs.continueSearchingForTheTeapot(request).runIn(30, 'seconds')\n        return\n    }\n    console.log('found the teapot!')\n}\nexport async function processEvent (event, { jobs }) {\n\n\n```const request = { url: 'https://www.google.com/teapot' }\nawait lookForTheTeapot(request)\n\nreturn event\n```\n\n\n}",
    "tag": "posthog"
  },
  {
    "title": "testing.md",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/build/testing.md",
    "content": "\ntitle: Testing\nIn order to ensure apps are stable and work as expected for all their users, we highly recommend writing tests for every app you build.\nAdding testing capabilities to your app\nYou will need to add jest and our app testing scaffold to your project in your `package.json` file:\n`json\n\"jest\": {\n    \"testEnvironment\": \"node\"\n},\n\"scripts\": {\n    \"test\": \"jest .\"\n},\n\"devDependencies\": {\n    \"@posthog/plugin-scaffold\": \"0.10.0\",\n    \"jest\": \"^27.0.4\"\n}`\nCreate your test files e.g. `index.test.js` or `index.test.ts` for testing your `index.js` or `index.ts` file\nWriting tests\nWrite tests in jest, you can learn more about the syntax and best practices in the jest documentation. We recommend writing tests to cover the primary functions of your app (e.g. does it create events in the expected format) and also for edge cases (e.g. does it crash if no data is sent).\nUsing the app scaffold\nSince most PostHog apps are likely to rely on PostHog specific features like \"processEvent\" we have a number of helper functions to mock these.\n\nCreateEvent - This will mock an event being created in PostHog e.g. `createEvent({ event: \"booking completed\", properties: { amount: \"20\", currency: \"USD\" } })`\nCreateIdentify - This will mock an identify event e.g. `createIdentify()`\n\nMore detail on other helper functions and how to use them can be found in our hello world example and in the utils library\nThese helper functions can be added to your test script using the following line:\n`js\nconst { createEvent, createIdentify} = require(\"@posthog/plugin-scaffold/test/utils\");`\nFor testing cron activities (e.g. run every minute), we recommend testing the functions that are called from this cron in your test - rather than trying to mock the cron event.\nRunning tests\nIf you have configured your package.json file as above you should be able to run\n`bash\nnpm run test`",
    "tag": "posthog"
  },
  {
    "title": "Prerequisites",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/build/tutorial.md",
    "content": "\ntitle: How to build PostHog apps\nsidebar: Docs\nshowTitle: true\n\nThis tutorial explains the development workflow and best practices, using an example 'Hello World' app. We go from zero to publishing your app in the official PostHog repository.\nPrerequisites\n\nA self-hosted PostHog instance (or a local development environment)\nSome knowledge of JavaScript (or TypeScript)\n\nThe app\nEvery app begins with either the PostHog app source editor, or a new GitHub repository. In both cases, our app source code will look like this:\n`js\n/* Runs on every event */\nexport function processEvent(event, meta) {\n    // Some events (like $identify) don't have properties\n    if (event.properties) {\n        event.properties['hello'] = `Hello ${meta.config.name || 'world'}`\n    }\n    // Return the event to ingest, return nothing to discard  \n    return event\n}`\nAnd our config would look like:\n`js\n[\n  {\n    \"key\": \"name\", // name of key to be accessed using meta. Check value using `meta.config.name`\n    \"name\": \"Person to greet\",\n    \"type\": \"string\",\n    \"hint\": \"Used to personalise the property `hello`\",\n    \"default\": \"\",\n    \"required\": false\n  }\n]`\nFor information on what code to write and what special functions to use, check out the overview and the developer reference.\nUsing the app source editor\nGo to Apps -> Advanced tab -> App editor -> Start coding.\n\nThen, click on \"Edit Source\", and you're good to go. Copy your code and config into the editor, and you're ready to test the app.\nUsing a GitHub repository\nWe have a GitHub template (GH login required) which helps you create a new repository with all the right files. There are only two files which make up the entire app: the `index.js` and `plugin.json`. Your code goes into `index.js`, and your configuration goes into `plugin.json`.\nOther than this, there's the `index.test.js` file for tests, and `package.json` for package dependencies and metadata.\nRemember to update `package.json` with the appropriate metadata, like name, description, and maintainer.\nOnce you've written the code in this new repository, you can run it by installing it locally in PostHog. See testing for more information.\nApp naming conventions\nWhen creating your repository, follow the naming convention of `posthog-<plugin-name>-plugin`. For example, the hello world  repository would be called `posthog-hello-world-plugin`.\nConverting a source app to a GitHub repository\nIf you wish to submit your app to the official repository, you need to convert it into a GitHub repository. The easiest way to do this is to start with the template and copy your source code into `index.js` and your config into the config field of `plugin.json`. Then update `package.json` with the appropriate metadata, like name, description, and maintainer.\nSee submission instructions for how to submit the app to the PostHog Repository.\nTesting\nFor now, the best way to test apps is to install them locally. \n\nIf you're writing a app in the App source editor, this is as easy as clicking \"Save\".\nIf you're writing a app in a GitHub repository, install it locally using the \"Install Local App\" option in the Advanced Tab.\n\n\nThis allows you to tweak your app and see that everything works fine.\nDebugging\nApps can make use of the JavaScript `console` for logging and debugging. \nThese logs can be seen on the 'Logs' page of each app, which can be accessed on the 'Apps' page of the PostHog UI.\nPublishing your app\nThere are four ways to use apps you build:\n\nPublish the app to `npm` and install it with the url from `npmjs.com` \nYou can add it via its repository URL (e.g. GitHub/GitLab)\n\nReference the location of the app on your local instance (e.g. /Users/yourname/path/to/app)  \nThis can be configured in 'Settings' -> 'Project Apps'.\n1. Submit it to the official repository. See below \n\n\nSubmitting your app\nIf you wish to, you can contribute back to the PostHog community by submitting to the official App Store. This means everyone else can use your app, too!\nIf you built a app inside the PostHog editor, first convert it to a GitHub repository\nTo submit, email your GitHub URL to hey@posthog.com",
    "tag": "posthog"
  },
  {
    "title": "Example of an app chain",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/build/index.md",
    "content": "\ntitle: Overview\nsidebar: Docs\nshowTitle: true\n\n\nNote: You can create apps for both self-hosted and PostHog Cloud, but releasing an app on PostHog Cloud requires a review process. Read more about the review process in the tutorial\n\nPostHog makes it possible to build your own apps and integrate with other platforms. So, if App Store is missing something you need then you may still be able to create it yourself.\nApps can add more information to an event, modify existing properties, import or export data, or trigger a range of other activities. There are also some apps that enqueue jobs to run in the future. Find out more about jobs in our developer reference docs.\nBefore building your first app it's important to understand how data flows through apps in the first place. There are two critical concepts to remember:\n\n\nApps usually act on single events coming in to PostHog.\n\n\nThe output of one app will go into the next app, creating a chain.\n\n\nBefore we get started, lets look at an examples of these principles in action. \nExample of an app chain\nThe GeoIP Enricher is an example of an app which adds information to events. Specifically, it adds geographical information based on the user IP address. It is triggered on each single event and adds additional informational to each event before it is stored.\nBy running a second app after the GeoIP app, we create a chain. Here's an example of how this can look for an individual event when a second app (which simply adds `Hello: \"world\"` to the event) runs after the GeoIP Enricher. \n\nApp chains are important because they control what the event looks like before it is stored. If you want to remove certain properties out of an event with the Property Filter app, for example, it is best to have it run at the end of the app chain so that all unwanted properties can be filtered out before storage.  \nExample of an app integrating with an external system\nThe GeoIP Enricher is an example of an app which modifies an event as it is ingested, but apps don't have to modify events at all. They can do all sorts of other things, such as integrating with or exporting to other systems.\nFor example, an app can send an event to AWS S3 whenever it is seen in PostHog. Indeed, the S3 Export app does exactly that. In this case, it doesn't matter if the S3 export succeeds or not, the event will always be stored.\nHere's how this can look:\n\nAs before, it is important to bear in mind how chains work. If you wanted the event stored on S3 to contain GeoIP information, for example, then the GeoIP Enricher must run before the S3 Exporter. \nBuilding your own app\nNow, how do you make all of this happen? Each app has two files: `index.js` and `plugin.json`. The index file has code for the entire app, and the JSON file has configuration for user inputs. This config is what you see in PostHog:\n\nWe have some special function names which enable you to process an event, like in the GeoIP Enricher, or to do something else entirely, like in the S3 Exporter. We expect `index.js` to export these special functions.\nTwo notable functions to be aware of are `processEvent` and `onEvent`. Both of these take in a single event and the meta object. You can find out more about meta objects in our developer reference docs, but one key property is `meta.config`. This property enables your code to read the configuration values set by users via `plugin.json`.\nIf you want to add new properties to your event, like the GeoIP Enricher does, use the `processEvent` function. Here's an example app that adds the `hello` property to events.\n`js\n/* Runs on every event */\nexport function processEvent(event, meta) {\n    // Some events (like $identify) don't have properties\n    if (event.properties) {\n        event.properties['hello'] = `Hello ${meta.config.name || 'world'}`\n    }\n    // Return the event to ingest, return nothing to discard  \n    return event\n}`\nNote how you need to return the event to ensure the chain continues. If you return `null` or `undefined`, you're telling PostHog to discard this event. For example, the Schema Enforcer app does precisely this for events that don't adhere to a schema.\nTo do something elese, like exporting to S3, use the `onEvent` function. For example, the below app logs the current URL on $pageview type events:\n```js\n/ Runs on every event /\nexport function onEvent(event, meta) {\n\n\n```if (event.event === \"$pageview\") {\n    // these logs appear in the UI\n    console.log(event.$current_url)\n}\n\n// Don't need to return event, any return value is discarded, and the event is not modified\n```\n\n\n}\n```\nThis app is admittedly useless since PostHog can already show you this information, but it serves to explain how things work. Note how you can choose what kind of events you want to operate on by using the existing event properties.\nNext steps\nThat's all for the crash course. There's a lot you can do with apps, such as running specific jobs every hour, sending events elsewhere via HTTP endpoints or modifying events before they're stored. Here are some additional resources to help you get started in building your own app for PostHog:\n\nFor in-depth information on all the special functions, check out the developer reference docs.\nFor building your own app from start to finish, check out our tutorial.\n",
    "tag": "posthog"
  },
  {
    "title": "`capture`",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/build/api.mdx",
    "content": "\ntitle: Using the PostHog API\nAll apps have access to a global helper object `posthog`, which provides easy access to the PostHog API.\nThe global `posthog` object contains the following interfaces:\n- capture - Send additional events from an app\n- api - Access the PostHog API from an app\n`capture`\n\nWarning: Be very careful when calling `posthog.capture` from `processEvent` or `onEvent`! The event captured will also be run through all the installed apps and could potentially lead to an infinite loop of event generation.\n\nCalling `posthog.capture` allows you to capture a new event under the same project the app is running in.\n`ts\ncapture(event: string, properties?: Record<string, any>) => void`\nIt takes 2 arguments, the first one being an event name (required), and the second one being an object with properties to set on the event (optional).\nApps can pass `distinct_id` on the event properties to specify what user the event should be attributed to. If `distinct_id` is not specified, the event will be attributed to a generic \"App Server\" person.\nApps can optionally pass a `timestamp`, which needs to be in ISO 8601 format. If not set, the timestamp will default to current time.\nExample usage\n`ts\nposthog.capture('Stripe Customer Subscribed', {\n  distinct_id: 'a user id',\n  timestamp: new Date(subscription.created*1000).toISOString()\n})`\n`api`\nThe `posthog.api` object contains a number of methods that make interacting with the PostHog API much easier.\n`posthog.api` gives you access to the following 4 methods, which each correspond to the 4 HTTP methods used to access the API.\n`ts\nget(path: string, options?: ApiMethodOptions): Promise<Response>\npost(path: string, options?: ApiMethodOptions): Promise<Response>\nput(path: string, options?: ApiMethodOptions): Promise<Response>\ndelete(path: string, options?: ApiMethodOptions): Promise<Response>`\nBy default, all these methods will create a temporary access token that the app uses to access the API.\n\nWarning: Be very careful when using `posthog.api.post` to send events from `processEvent` or `onEvent`! The event captured will also be run through all the installed apps and could potentially lead to an infinite loop of event generation.\n\nSending an API request\nIt's best to look at an example to see how these methods work in practice. Here, we're looking to create a new annotation from within our app.\n`ts\nconst res = await posthog.api.post(\n  '/api/projects/@current/annotations',\n  {\n    data: {\n      content: 'New Annotation',\n      scope: 'organization',\n      date_marker: '2022-09-22T07:30:00Z',\n    },\n    host: 'https://posthog.mydomain.com' // defaults to https://app.posthog.com if not specified\n  }\n)`\nAs you can see, when specifying the path of our API route, we can use the `@current` identifier in place of our project ID in order to let the API know we want to access the project the app is currently running in.\nWe've also specified the host of our PostHog instance. This is only necessary when running an app on a self-hosted instance, and when using PostHog Cloud this can be omitted.\n`js\nconst body = await res.json()`\nKeep in mind that these functions will return the raw response object in a Promise, so in order to access the body, you can simply call the `.json()` method.\nCustomizing options\nYou can override these defaults to interact with another PostHog project or instance.\nThe options available are:\n```ts\ninterface ApiMethodOptions {\n    // any data to send with the request, GET and DELETE will set these as URL params\n    data?: Record\n\n\n```// posthog host, defaults to https://app.posthog.com\nhost?: string\n\n// specifies the project to interact with\nprojectApiKey?: string\n\n// authenticates the user\npersonalApiKey?: string\n```\n\n\n}\n```",
    "tag": "posthog"
  },
  {
    "title": "if using yarn",
    "source": "https://github.com/PostHog/posthog.com/tree/master/contents/docs/apps/build/types.md",
    "content": "\ntitle: TypeScript types \nsidebar: Docs\nshowTitle: true\n\nPostHog supports TypeScript apps natively, without you having to compile the TypeScript yourself (although you can also do that).\nTo build a TypeScript app, you'll probably need some types, so read on.\nInstallation\nTo use the types in your app, you can install them as follows:\n```bash\nif using yarn\nyarn add --dev @posthog/plugin-scaffold\nif using npm\nnpm install --save-dev @posthog/plugin-scaffold\n``` \nThen, in your apps, you can use them like so:\n```typescript\nimport { PluginEvent, PluginMeta } from '@posthog/plugin-scaffold'\nexport function processEvent(event: PluginEvent, meta: PluginMeta) {\n    if (event.properties) {\n        event.properties['hello'] = 'world'\n    }\n    return event\n}",
    "tag": "posthog"
  }
]