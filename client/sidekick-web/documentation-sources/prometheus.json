[
  {
    "title": "Storage",
    "source": "https://github.com/prometheus/prometheus/tree/main/docs/storage.md",
    "content": "\ntitle: Storage\nsort_rank: 5\n\nStorage\nPrometheus includes a local on-disk time series database, but also optionally integrates with remote storage systems.\nLocal storage\nPrometheus's local time series database stores data in a custom, highly efficient format on local storage.\nOn-disk layout\nIngested samples are grouped into blocks of two hours. Each two-hour block consists\nof a directory containing a chunks subdirectory containing all the time series samples\nfor that window of time, a metadata file, and an index file (which indexes metric names\nand labels to time series in the chunks directory). The samples in the chunks directory\nare grouped together into one or more segment files of up to 512MB each by default. When series are\ndeleted via the API, deletion records are stored in separate tombstone files (instead\nof deleting the data immediately from the chunk segments).\nThe current block for incoming samples is kept in memory and is not fully\npersisted. It is secured against crashes by a write-ahead log (WAL) that can be\nreplayed when the Prometheus server restarts. Write-ahead log files are stored\nin the `wal` directory in 128MB segments. These files contain raw data that\nhas not yet been compacted; thus they are significantly larger than regular block\nfiles. Prometheus will retain a minimum of three write-ahead log files.\nHigh-traffic servers may retain more than three WAL files in order to keep at\nleast two hours of raw data.\nA Prometheus server's data directory looks something like this:\n`./data\n\u251c\u2500\u2500 01BKGV7JBM69T2G1BGBGM6KB12\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 meta.json\n\u251c\u2500\u2500 01BKGTZQ1SYQJTR4PB43C8PD98\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 chunks\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 000001\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tombstones\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 index\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 meta.json\n\u251c\u2500\u2500 01BKGTZQ1HHWHV8FBJXW1Y3W0K\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 meta.json\n\u251c\u2500\u2500 01BKGV7JC0RY8A6MACW02A2PJD\n\u2502\u00a0  \u251c\u2500\u2500 chunks\n\u2502\u00a0  \u2502   \u2514\u2500\u2500 000001\n\u2502\u00a0  \u251c\u2500\u2500 tombstones\n\u2502\u00a0  \u251c\u2500\u2500 index\n\u2502\u00a0  \u2514\u2500\u2500 meta.json\n\u251c\u2500\u2500 chunks_head\n\u2502\u00a0  \u2514\u2500\u2500 000001\n\u2514\u2500\u2500 wal\n\u00a0   \u251c\u2500\u2500 000000002\n\u00a0   \u2514\u2500\u2500 checkpoint.00000001\n\u00a0\u00a0      \u2514\u2500\u2500 00000000`\nNote that a limitation of local storage is that it is not clustered or\nreplicated. Thus, it is not arbitrarily scalable or durable in the face of\ndrive or node outages and should be managed like any other single node\ndatabase. The use of RAID is suggested for storage availability, and snapshots\nare recommended for backups. With proper\narchitecture, it is possible to retain years of data in local storage.\nAlternatively, external storage may be used via the remote read/write APIs. Careful evaluation is required for these systems as they vary greatly in durability, performance, and efficiency.\nFor further details on file format, see TSDB format.\nCompaction\nThe initial two-hour blocks are eventually compacted into longer blocks in the background.\nCompaction will create larger blocks containing data spanning up to 10% of the retention time, or 31 days, whichever is smaller.\nOperational aspects\nPrometheus has several flags that configure local storage. The most important are:\n\n`--storage.tsdb.path`: Where Prometheus writes its database. Defaults to `data/`.\n`--storage.tsdb.retention.time`: When to remove old data. Defaults to `15d`. Overrides `storage.tsdb.retention` if this flag is set to anything other than default.\n`--storage.tsdb.retention.size`: The maximum number of bytes of storage blocks to retain. The oldest data will be removed first. Defaults to `0` or disabled. Units supported: B, KB, MB, GB, TB, PB, EB. Ex: \"512MB\". Based on powers-of-2, so 1KB is 1024B. Only the persistent blocks are deleted to honor this retention although WAL and m-mapped chunks are counted in the total size. So the minimum requirement for the disk is the peak space taken by the `wal` (the WAL and Checkpoint) and `chunks_head` (m-mapped Head chunks) directory combined (peaks every 2 hours).\n`--storage.tsdb.retention`: Deprecated in favor of `storage.tsdb.retention.time`.\n`--storage.tsdb.wal-compression`: Enables compression of the write-ahead log (WAL). Depending on your data, you can expect the WAL size to be halved with little extra cpu load. This flag was introduced in 2.11.0 and enabled by default in 2.20.0. Note that once enabled, downgrading Prometheus to a version below 2.11.0 will require deleting the WAL.\n\nPrometheus stores an average of only 1-2 bytes per sample. Thus, to plan the capacity of a Prometheus server, you can use the rough formula:\n`needed_disk_space = retention_time_seconds * ingested_samples_per_second * bytes_per_sample`\nTo lower the rate of ingested samples, you can either reduce the number of time series you scrape (fewer targets or fewer series per target), or you can increase the scrape interval. However, reducing the number of series is likely more effective, due to compression of samples within a series.\nIf your local storage becomes corrupted for whatever reason, the best\nstrategy to address the problem is to shut down Prometheus then remove the\nentire storage directory. You can also try removing individual block directories,\nor the WAL directory to resolve the problem.  Note that this means losing\napproximately two hours data per block directory. Again, Prometheus's local\nstorage is not intended to be durable long-term storage; external solutions\noffer extended retention and data durability.\nCAUTION: Non-POSIX compliant filesystems are not supported for Prometheus' local storage as unrecoverable corruptions may happen. NFS filesystems (including AWS's EFS) are not supported. NFS could be POSIX-compliant, but most implementations are not. It is strongly recommended to use a local filesystem for reliability.\nIf both time and size retention policies are specified, whichever triggers first\nwill be used.\nExpired block cleanup happens in the background. It may take up to two hours to remove expired blocks. Blocks must be fully expired before they are removed.\nRemote storage integrations\nPrometheus's local storage is limited to a single node's scalability and durability.\nInstead of trying to solve clustered storage in Prometheus itself, Prometheus offers\na set of interfaces that allow integrating with remote storage systems.\nOverview\nPrometheus integrates with remote storage systems in three ways:\n\nPrometheus can write samples that it ingests to a remote URL in a standardized format.\nPrometheus can receive samples from other Prometheus servers in a standardized format.\nPrometheus can read (back) sample data from a remote URL in a standardized format.\n\n\nThe read and write protocols both use a snappy-compressed protocol buffer encoding over HTTP. The protocols are not considered as stable APIs yet and may change to use gRPC over HTTP/2 in the future, when all hops between Prometheus and the remote storage can safely be assumed to support HTTP/2.\nFor details on configuring remote storage integrations in Prometheus, see the remote write and remote read sections of the Prometheus configuration documentation.\nThe built-in remote write receiver can be enabled by setting the `--web.enable-remote-write-receiver` command line flag. When enabled, the remote write receiver endpoint is `/api/v1/write`.\nFor details on the request and response messages, see the remote storage protocol buffer definitions.\nNote that on the read path, Prometheus only fetches raw series data for a set of label selectors and time ranges from the remote end. All PromQL evaluation on the raw data still happens in Prometheus itself. This means that remote read queries have some scalability limit, since all necessary data needs to be loaded into the querying Prometheus server first and then processed there. However, supporting fully distributed evaluation of PromQL was deemed infeasible for the time being.\nExisting integrations\nTo learn more about existing integrations with remote storage systems, see the Integrations documentation.\nBackfilling from OpenMetrics format\nOverview\nIf a user wants to create blocks into the TSDB from data that is in OpenMetrics format, they can do so using backfilling. However, they should be careful and note that it is not safe to backfill data from the last 3 hours (the current head block) as this time range may overlap with the current head block Prometheus is still mutating. Backfilling will create new TSDB blocks, each containing two hours of metrics data. This limits the memory requirements of block creation. Compacting the two hour blocks into larger blocks is later done by the Prometheus server itself.\nA typical use case is to migrate metrics data from a different monitoring system or time-series database to Prometheus. To do so, the user must first convert the source data into OpenMetrics  format, which is the input format for the backfilling as described below.\nUsage\nBackfilling can be used via the Promtool command line. Promtool will write the blocks to a directory. By default this output directory is ./data/, you can change it by using the name of the desired output directory as an optional argument in the sub-command.\n`promtool tsdb create-blocks-from openmetrics <input file> [<output directory>]`\nAfter the creation of the blocks, move it to the data directory of Prometheus. If there is an overlap with the existing blocks in Prometheus, the flag `--storage.tsdb.allow-overlapping-blocks` needs to be set for Prometheus versions v2.38 and below. Note that any backfilled data is subject to the retention configured for your Prometheus server (by time or size).\nLonger Block Durations\nBy default, the promtool will use the default block duration (2h) for the blocks; this behavior is the most generally applicable and correct. However, when backfilling data over a long range of times, it may be advantageous to use a larger value for the block duration to backfill faster and prevent additional compactions by TSDB later.\nThe `--max-block-duration` flag allows the user to configure a maximum duration of blocks. The backfilling tool will pick a suitable block duration no larger than this.\nWhile larger blocks may improve the performance of backfilling large datasets, drawbacks exist as well. Time-based retention policies must keep the entire block around if even one sample of the (potentially large) block is still within the retention policy. Conversely, size-based retention policies will remove the entire block even if the TSDB only goes over the size limit in a minor way.\nTherefore, backfilling with few blocks, thereby choosing a larger block duration, must be done with care and is not recommended for any production instances.\nBackfilling for Recording Rules\nOverview\nWhen a new recording rule is created, there is no historical data for it. Recording rule data only exists from the creation time on. `promtool` makes it possible to create historical recording rule data.\nUsage\nTo see all options, use: `$ promtool tsdb create-blocks-from rules --help`.\nExample usage:\n`$ promtool tsdb create-blocks-from rules \\\n    --start 1617079873 \\\n    --end 1617097873 \\\n    --url http://mypromserver.com:9090 \\\n    rules.yaml rules2.yaml`\nThe recording rule files provided should be a normal Prometheus rules file.\nThe output of `promtool tsdb create-blocks-from rules` command is a directory that contains blocks with the historical rule data for all rules in the recording rule files. By default, the output directory is `data/`. In order to make use of this new block data, the blocks must be moved to a running Prometheus instance data dir `storage.tsdb.path` (for Prometheus versions v2.38 and below, the flag `--storage.tsdb.allow-overlapping-blocks` must be enabled). Once moved, the new blocks will merge with existing blocks when the next compaction runs.\nLimitations\n\nIf you run the rule backfiller multiple times with the overlapping start/end times, blocks containing the same data will be created each time the rule backfiller is run.\nAll rules in the recording rule files will be evaluated.\nIf the `interval` is set in the recording rule file that will take priority over the `eval-interval` flag in the rule backfill command.\nAlerts are currently ignored if they are in the recording rule file.\n",
    "tag": "prometheus"
  },
  {
    "title": "Management API",
    "source": "https://github.com/prometheus/prometheus/tree/main/docs/management_api.md",
    "content": "\ntitle: Management API\nsort_rank: 8\n\nManagement API\nPrometheus provides a set of management APIs to facilitate automation and integration.\nHealth check\n`GET /-/healthy\nHEAD /-/healthy`\nThis endpoint always returns 200 and should be used to check Prometheus health.\nReadiness check\n`GET /-/ready\nHEAD /-/ready`\nThis endpoint returns 200 when Prometheus is ready to serve traffic (i.e. respond to queries).\nReload\n`PUT  /-/reload\nPOST /-/reload`\nThis endpoint triggers a reload of the Prometheus configuration and rule files. It's disabled by default and can be enabled via the `--web.enable-lifecycle` flag.\nAlternatively, a configuration reload can be triggered by sending a `SIGHUP` to the Prometheus process.\nQuit\n`PUT  /-/quit\nPOST /-/quit`\nThis endpoint triggers a graceful shutdown of Prometheus. It's disabled by default and can be enabled via the `--web.enable-lifecycle` flag.",
    "tag": "prometheus"
  },
  {
    "title": "Federation",
    "source": "https://github.com/prometheus/prometheus/tree/main/docs/federation.md",
    "content": "\ntitle: Federation\nsort_rank: 6\n\nFederation\nFederation allows a Prometheus server to scrape selected time series from\nanother Prometheus server.\nNote about native histograms (experimental feature): To scrape native histograms\nvia federation, the scraping Prometheus server needs to run with native histograms\nenabled (via the command line flag `--enable-feature=native-histograms`), implying\nthat the protobuf format is used for scraping. Should the federated metrics contain\na mix of different sample types (float64, counter histogram, gauge histogram) for\nthe same metric name, the federation payload will contain multiple metric families\nwith the same name (but different types). Technically, this violates the rules of\nthe protobuf exposition format, but Prometheus is nevertheless able to ingest all\nmetrics correctly.\nUse cases\nThere are different use cases for federation. Commonly, it is used to either\nachieve scalable Prometheus monitoring setups or to pull related metrics from\none service's Prometheus into another.\nHierarchical federation\nHierarchical federation allows Prometheus to scale to environments with tens of\ndata centers and millions of nodes. In this use case, the federation topology\nresembles a tree, with higher-level Prometheus servers collecting aggregated\ntime series data from a larger number of subordinated servers.\nFor example, a setup might consist of many per-datacenter Prometheus servers\nthat collect data in high detail (instance-level drill-down), and a set of\nglobal Prometheus servers which collect and store only aggregated data\n(job-level drill-down) from those local servers. This provides an aggregate\nglobal view and detailed local views.\nCross-service federation\nIn cross-service federation, a Prometheus server of one service is configured\nto scrape selected data from another service's Prometheus server to enable\nalerting and queries against both datasets within a single server.\nFor example, a cluster scheduler running multiple services might expose\nresource usage information (like memory and CPU usage) about service instances\nrunning on the cluster. On the other hand, a service running on that cluster\nwill only expose application-specific service metrics. Often, these two sets of\nmetrics are scraped by separate Prometheus servers. Using federation, the\nPrometheus server containing service-level metrics may pull in the cluster\nresource usage metrics about its specific service from the cluster Prometheus,\nso that both sets of metrics can be used within that server.\nConfiguring federation\nOn any given Prometheus server, the `/federate` endpoint allows retrieving the\ncurrent value for a selected set of time series in that server. At least one\n`match[]` URL parameter must be specified to select the series to expose. Each\n`match[]` argument needs to specify an\ninstant vector selector like\n`up` or `{job=\"api-server\"}`. If multiple `match[]` parameters are provided,\nthe union of all matched series is selected.\nTo federate metrics from one server to another, configure your destination\nPrometheus server to scrape from the `/federate` endpoint of a source server,\nwhile also enabling the `honor_labels` scrape option (to not overwrite any\nlabels exposed by the source server) and passing in the desired `match[]`\nparameters. For example, the following `scrape_configs` federates any series\nwith the label `job=\"prometheus\"` or a metric name starting with `job:` from\nthe Prometheus servers at `source-prometheus-{1,2,3}:9090` into the scraping\nPrometheus:\n```yaml\nscrape_configs:\n  - job_name: 'federate'\n    scrape_interval: 15s\n\n\n```honor_labels: true\nmetrics_path: '/federate'\n\nparams:\n  'match[]':\n    - '{job=\"prometheus\"}'\n    - '{__name__=~\"job:.*\"}'\n\nstatic_configs:\n  - targets:\n    - 'source-prometheus-1:9090'\n    - 'source-prometheus-2:9090'\n    - 'source-prometheus-3:9090'\n```\n\n",
    "tag": "prometheus"
  },
  {
    "title": "Prometheus 2.0 migration guide",
    "source": "https://github.com/prometheus/prometheus/tree/main/docs/migration.md",
    "content": "\ntitle: Migration\nsort_rank: 9\n\nPrometheus 2.0 migration guide\nIn line with our stability promise,\nthe Prometheus 2.0 release contains a number of backwards incompatible changes.\nThis document offers guidance on migrating from Prometheus 1.8 to Prometheus 2.0 and newer versions.\nFlags\nThe format of Prometheus command line flags has changed. Instead of a\nsingle dash, all flags now use a double dash. Common flags (`--config.file`,\n`--web.listen-address` and `--web.external-url`) remain but\nalmost all storage-related flags have been removed.\nSome notable flags which have been removed:\n\n\n`-alertmanager.url` In Prometheus 2.0, the command line flags for configuring\n  a static Alertmanager URL have been removed. Alertmanager must now be\n  discovered via service discovery, see Alertmanager service discovery.\n\n\n`-log.format` In Prometheus 2.0 logs can only be streamed to standard error.\n\n\n`-query.staleness-delta` has been renamed to `--query.lookback-delta`; Prometheus\n  2.0 introduces a new mechanism for handling staleness, see staleness.\n\n\n`-storage.local.*` Prometheus 2.0 introduces a new storage engine; as such all\n  flags relating to the old engine have been removed.  For information on the\n  new engine, see Storage.\n\n\n`-storage.remote.*` Prometheus 2.0 has removed the deprecated remote\n  storage flags, and will fail to start if they are supplied. To write to\n  InfluxDB, Graphite, or OpenTSDB use the relevant storage adapter.\n\n\nAlertmanager service discovery\nAlertmanager service discovery was introduced in Prometheus 1.4, allowing Prometheus\nto dynamically discover Alertmanager replicas using the same mechanism as scrape\ntargets. In Prometheus 2.0, the command line flags for static Alertmanager config\nhave been removed, so the following command line flag:\n`./prometheus -alertmanager.url=http://alertmanager:9093/`\nWould be replaced with the following in the `prometheus.yml` config file:\n`yaml\nalerting:\n  alertmanagers:\n  - static_configs:\n    - targets:\n      - alertmanager:9093`\nYou can also use all the usual Prometheus service discovery integrations and\nrelabeling in your Alertmanager configuration. This snippet instructs\nPrometheus to search for Kubernetes pods, in the `default` namespace, with the\nlabel `name: alertmanager` and with a non-empty port.\n`yaml\nalerting:\n  alertmanagers:\n  - kubernetes_sd_configs:\n      - role: pod\n    tls_config:\n      ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n    bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n    relabel_configs:\n    - source_labels: [__meta_kubernetes_pod_label_name]\n      regex: alertmanager\n      action: keep\n    - source_labels: [__meta_kubernetes_namespace]\n      regex: default\n      action: keep\n    - source_labels: [__meta_kubernetes_pod_container_port_number]\n      regex:\n      action: drop`\nRecording rules and alerts\nThe format for configuring alerting and recording rules has been changed to YAML.\nAn example of a recording rule and alert in the old format:\n```\njob:request_duration_seconds:histogram_quantile99 =\n  histogram_quantile(0.99, sum by (le, job) (rate(request_duration_seconds_bucket[1m])))\nALERT FrontendRequestLatency\n  IF job:request_duration_seconds:histogram_quantile99{job=\"frontend\"} > 0.1\n  FOR 5m\n  ANNOTATIONS {\n    summary = \"High frontend request latency\",\n  }\n```\nWould look like this:\n`yaml\ngroups:\n- name: example.rules\n  rules:\n  - record: job:request_duration_seconds:histogram_quantile99\n    expr: histogram_quantile(0.99, sum by (le, job) (rate(request_duration_seconds_bucket[1m])))\n  - alert: FrontendRequestLatency\n    expr: job:request_duration_seconds:histogram_quantile99{job=\"frontend\"} > 0.1\n    for: 5m\n    annotations:\n      summary: High frontend request latency`\nTo help with the change, the `promtool` tool has a mode to automate the rules conversion.  Given a `.rules` file, it will output a `.rules.yml` file in the\nnew format. For example:\n`$ promtool update rules example.rules`\nYou will need to use `promtool` from Prometheus 2.5 as later versions no longer contain the above subcommand.\nStorage\nThe data format in Prometheus 2.0 has completely changed and is not backwards\ncompatible with 1.8 and older versions. To retain access to your historic monitoring data we\nrecommend you run a non-scraping Prometheus instance running at least version\n1.8.1 in parallel with your Prometheus 2.0 instance, and have the new server\nread existing data from the old one via the remote read protocol.\nYour Prometheus 1.8 instance should be started with the following flags and an\nconfig file containing only the `external_labels` setting (if any):\n`$ ./prometheus-1.8.1.linux-amd64/prometheus -web.listen-address \":9094\" -config.file old.yml`\nPrometheus 2.0 can then be started (on the same machine) with the following flags:\n`$ ./prometheus-2.0.0.linux-amd64/prometheus --config.file prometheus.yml`\nWhere `prometheus.yml` contains in addition to your full existing configuration, the stanza:\n`yaml\nremote_read:\n  - url: \"http://localhost:9094/api/v1/read\"`\nPromQL\nThe following features have been removed from PromQL:\n\n`drop_common_labels` function - the `without` aggregation modifier should be used\n  instead.\n`keep_common` aggregation modifier - the `by` modifier should be used instead.\n`count_scalar` function - use cases are better handled by `absent()` or correct\n  propagation of labels in operations.\n\nSee issue #3060 for more\ndetails.\nMiscellaneous\nPrometheus non-root user\nThe Prometheus Docker image is now built to run Prometheus\nas a non-root user. If you\nwant the Prometheus UI/API to listen on a low port number (say, port 80), you'll\nneed to override it. For Kubernetes, you would use the following YAML:\n`yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: security-context-demo-2\nspec:\n  securityContext:\n    runAsUser: 0\n...`\nSee Configure a Security Context for a Pod or Container\nfor more details.\nIf you're using Docker, then the following snippet would be used:\n`docker run -p 9090:9090 prom/prometheus:latest`\nPrometheus lifecycle\nIf you use the Prometheus `/-/reload` HTTP endpoint to automatically reload your\nPrometheus config when it changes,\nthese endpoints are disabled by default for security reasons in Prometheus 2.0.",
    "tag": "prometheus"
  },
  {
    "title": "Getting started",
    "source": "https://github.com/prometheus/prometheus/tree/main/docs/getting_started.md",
    "content": "\ntitle: Getting started\nsort_rank: 1\n\nGetting started\nThis guide is a \"Hello World\"-style tutorial which shows how to install,\nconfigure, and use a simple Prometheus instance. You will download and run\nPrometheus locally, configure it to scrape itself and an example application,\nthen work with queries, rules, and graphs to use collected time\nseries data.\nDownloading and running Prometheus\nDownload the latest release of Prometheus for\nyour platform, then extract and run it:\n`bash\ntar xvfz prometheus-*.tar.gz\ncd prometheus-*`\nBefore starting Prometheus, let's configure it.\nConfiguring Prometheus to monitor itself\nPrometheus collects metrics from targets by scraping metrics HTTP\nendpoints. Since Prometheus exposes data in the same\nmanner about itself, it can also scrape and monitor its own health.\nWhile a Prometheus server that collects only data about itself is not very\nuseful, it is a good starting example. Save the following basic\nPrometheus configuration as a file named `prometheus.yml`:\n```yaml\nglobal:\n  scrape_interval:     15s # By default, scrape targets every 15 seconds.\n# Attach these labels to any time series or alerts when communicating with\n  # external systems (federation, remote storage, Alertmanager).\n  external_labels:\n    monitor: 'codelab-monitor'\nA scrape configuration containing exactly one endpoint to scrape:\nHere it's Prometheus itself.\nscrape_configs:\n  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.\n  - job_name: 'prometheus'\n\n\n```# Override the global default and scrape targets from this job every 5 seconds.\nscrape_interval: 5s\n\nstatic_configs:\n  - targets: ['localhost:9090']\n```\n\n\n```\nFor a complete specification of configuration options, see the\nconfiguration documentation.\nStarting Prometheus\nTo start Prometheus with your newly created configuration file, change to the\ndirectory containing the Prometheus binary and run:\n```bash\nStart Prometheus.\nBy default, Prometheus stores its database in ./data (flag --storage.tsdb.path).\n./prometheus --config.file=prometheus.yml\n```\nPrometheus should start up. You should also be able to browse to a status page\nabout itself at localhost:9090. Give it a couple of\nseconds to collect data about itself from its own HTTP metrics endpoint.\nYou can also verify that Prometheus is serving metrics about itself by\nnavigating to its metrics endpoint:\nlocalhost:9090/metrics\nUsing the expression browser\nLet us explore data that Prometheus has collected about itself. To\nuse Prometheus's built-in expression browser, navigate to\nhttp://localhost:9090/graph and choose the \"Table\" view within the \"Graph\" tab.\nAs you can gather from localhost:9090/metrics,\none metric that Prometheus exports about itself is named\n`prometheus_target_interval_length_seconds` (the actual amount of time between\ntarget scrapes). Enter the below into the expression console and then click \"Execute\":\n`prometheus_target_interval_length_seconds`\nThis should return a number of different time series (along with the latest value\nrecorded for each), each with the metric name\n`prometheus_target_interval_length_seconds`, but with different labels. These\nlabels designate different latency percentiles and target group intervals.\nIf we are interested only in 99th percentile latencies, we could use this\nquery:\n`prometheus_target_interval_length_seconds{quantile=\"0.99\"}`\nTo count the number of returned time series, you could write:\n`count(prometheus_target_interval_length_seconds)`\nFor more about the expression language, see the\nexpression language documentation.\nUsing the graphing interface\nTo graph expressions, navigate to http://localhost:9090/graph and use the \"Graph\"\ntab.\nFor example, enter the following expression to graph the per-second rate of chunks\nbeing created in the self-scraped Prometheus:\n`rate(prometheus_tsdb_head_chunks_created_total[1m])`\nExperiment with the graph range parameters and other settings.\nStarting up some sample targets\nLet's add additional targets for Prometheus to scrape.\nThe Node Exporter is used as an example target, for more information on using it\nsee these instructions.\n```bash\ntar -xzvf node_exporter-..tar.gz\ncd node_exporter-.\nStart 3 example targets in separate terminals:\n./node_exporter --web.listen-address 127.0.0.1:8080\n./node_exporter --web.listen-address 127.0.0.1:8081\n./node_exporter --web.listen-address 127.0.0.1:8082\n```\nYou should now have example targets listening on http://localhost:8080/metrics,\nhttp://localhost:8081/metrics, and http://localhost:8082/metrics.\nConfigure Prometheus to monitor the sample targets\nNow we will configure Prometheus to scrape these new targets. Let's group all\nthree endpoints into one job called `node`. We will imagine that the\nfirst two endpoints are production targets, while the third one represents a\ncanary instance. To model this in Prometheus, we can add several groups of\nendpoints to a single job, adding extra labels to each group of targets. In\nthis example, we will add the `group=\"production\"` label to the first group of\ntargets, while adding `group=\"canary\"` to the second.\nTo achieve this, add the following job definition to the `scrape_configs`\nsection in your `prometheus.yml` and restart your Prometheus instance:\n```yaml\nscrape_configs:\n  - job_name:       'node'\n\n\n```# Override the global default and scrape targets from this job every 5 seconds.\nscrape_interval: 5s\n\nstatic_configs:\n  - targets: ['localhost:8080', 'localhost:8081']\n    labels:\n      group: 'production'\n\n  - targets: ['localhost:8082']\n    labels:\n      group: 'canary'\n```\n\n\n```\nGo to the expression browser and verify that Prometheus now has information\nabout time series that these example endpoints expose, such as `node_cpu_seconds_total`.\nConfigure rules for aggregating scraped data into new time series\nThough not a problem in our example, queries that aggregate over thousands of\ntime series can get slow when computed ad-hoc. To make this more efficient,\nPrometheus can prerecord expressions into new persisted\ntime series via configured recording rules. Let's say we are interested in\nrecording the per-second rate of cpu time (`node_cpu_seconds_total`) averaged\nover all cpus per instance (but preserving the `job`, `instance` and `mode`\ndimensions) as measured over a window of 5 minutes. We could write this as:\n`avg by (job, instance, mode) (rate(node_cpu_seconds_total[5m]))`\nTry graphing this expression.\nTo record the time series resulting from this expression into a new metric\ncalled `job_instance_mode:node_cpu_seconds:avg_rate5m`, create a file\nwith the following recording rule and save it as `prometheus.rules.yml`:\n`groups:\n- name: cpu-node\n  rules:\n  - record: job_instance_mode:node_cpu_seconds:avg_rate5m\n    expr: avg by (job, instance, mode) (rate(node_cpu_seconds_total[5m]))`\nTo make Prometheus pick up this new rule, add a `rule_files` statement in your `prometheus.yml`. The config should now\nlook like this:\n```yaml\nglobal:\n  scrape_interval:     15s # By default, scrape targets every 15 seconds.\n  evaluation_interval: 15s # Evaluate rules every 15 seconds.\n# Attach these extra labels to all timeseries collected by this Prometheus instance.\n  external_labels:\n    monitor: 'codelab-monitor'\nrule_files:\n  - 'prometheus.rules.yml'\nscrape_configs:\n  - job_name: 'prometheus'\n\n\n```# Override the global default and scrape targets from this job every 5 seconds.\nscrape_interval: 5s\n\nstatic_configs:\n  - targets: ['localhost:9090']\n```\n\n\n\n\njob_name:       'node'\nOverride the global default and scrape targets from this job every 5 seconds.\nscrape_interval: 5s\nstatic_configs:\n  - targets: ['localhost:8080', 'localhost:8081']\n    labels:\n      group: 'production'\n\ntargets: ['localhost:8082']\n    labels:\n      group: 'canary'\n```\n\n\n\nRestart Prometheus with the new configuration and verify that a new time series\nwith the metric name `job_instance_mode:node_cpu_seconds:avg_rate5m`",
    "tag": "prometheus"
  },
  {
    "title": "API Stability Guarantees",
    "source": "https://github.com/prometheus/prometheus/tree/main/docs/stability.md",
    "content": "\ntitle: API Stability\nsort_rank: 10\n\nAPI Stability Guarantees\nPrometheus promises API stability within a major version, and strives to avoid\nbreaking changes for key features. Some features, which are cosmetic, still\nunder development, or depend on 3rd party services, are not covered by this.\nThings considered stable for 2.x:\n\nThe query language and data model\nAlerting and recording rules\nThe ingestion exposition format\nv1 HTTP API (used by dashboards and UIs)\nConfiguration file format (minus the service discovery remote read/write, see below)\nRule/alert file format\nConsole template syntax and semantics\n\nThings considered unstable for 2.x:\n\nAny feature listed as experimental or subject to change, including:\nThe holt_winters PromQL function\nRemote read, remote write and the remote read endpoint\nServer-side HTTPS and basic authentication\nService discovery integrations, with the exception of `static_configs` and `file_sd_configs`\nGo APIs of packages that are part of the server\nHTML generated by the web UI\nThe metrics in the /metrics endpoint of Prometheus itself\nExact on-disk format. Potential changes however, will be forward compatible and transparently handled by Prometheus\nThe format of the logs\n\nAs long as you are not using any features marked as experimental/unstable, an\nupgrade within a major version can usually be performed without any operational\nadjustments and very little risk that anything will break. Any breaking changes\nwill be marked as `CHANGE` in release notes.",
    "tag": "prometheus"
  },
  {
    "title": "Feature flags",
    "source": "https://github.com/prometheus/prometheus/tree/main/docs/feature_flags.md",
    "content": "\ntitle: Feature flags\nsort_rank: 11\n\nFeature flags\nHere is a list of features that are disabled by default since they are breaking changes or are considered experimental.\nTheir behaviour can change in future releases which will be communicated via the release changelog.\nYou can enable them using the `--enable-feature` flag with a comma separated list of features.\nThey may be enabled by default in future versions.\nExpand environment variables in external labels\n`--enable-feature=expand-external-labels`\nReplace `${var}` or `$var` in the external_labels\nvalues according to the values of the current environment variables. References\nto undefined variables are replaced by the empty string.\nThe `$` character can be escaped by using `$$`.\nRemote Write Receiver\n`--enable-feature=remote-write-receiver`\nThe remote write receiver allows Prometheus to accept remote write requests from other Prometheus servers. More details can be found here.\nActivating the remote write receiver via a feature flag is deprecated. Use `--web.enable-remote-write-receiver` instead. This feature flag will be ignored in future versions of Prometheus.\nExemplars storage\n`--enable-feature=exemplar-storage`\nOpenMetrics introduces the ability for scrape targets to add exemplars to certain metrics. Exemplars are references to data outside of the MetricSet. A common use case are IDs of program traces.\nExemplar storage is implemented as a fixed size circular buffer that stores exemplars in memory for all series. Enabling this feature will enable the storage of exemplars scraped by Prometheus. The config file block storage/exemplars can be used to control the size of circular buffer by # of exemplars. An exemplar with just a `traceID=<jaeger-trace-id>` uses roughly 100 bytes of memory via the in-memory exemplar storage. If the exemplar storage is enabled, we will also append the exemplars to WAL for local persistence (for WAL duration).\nMemory snapshot on shutdown\n`--enable-feature=memory-snapshot-on-shutdown`\nThis takes the snapshot of the chunks that are in memory along with the series information when shutting down and stores\nit on disk. This will reduce the startup time since the memory state can be restored with this snapshot and m-mapped\nchunks without the need of WAL replay.\nExtra scrape metrics\n`--enable-feature=extra-scrape-metrics`\nWhen enabled, for each instance scrape, Prometheus stores a sample in the following additional time series:\n\n`scrape_timeout_seconds`. The configured `scrape_timeout` for a target. This allows you to measure each target to find out how close they are to timing out with `scrape_duration_seconds / scrape_timeout_seconds`.\n`scrape_sample_limit`. The configured `sample_limit` for a target. This allows you to measure each target\n  to find out how close they are to reaching the limit with `scrape_samples_post_metric_relabeling / scrape_sample_limit`. Note that `scrape_sample_limit` can be zero if there is no limit configured, which means that the query above can return `+Inf` for targets with no limit (as we divide by zero). If you want to query only for targets that do have a sample limit use this query: `scrape_samples_post_metric_relabeling / (scrape_sample_limit > 0)`.\n`scrape_body_size_bytes`. The uncompressed size of the most recent scrape response, if successful. Scrapes failing because `body_size_limit` is exceeded report `-1`, other scrape failures report `0`.\n\nNew service discovery manager\n`--enable-feature=new-service-discovery-manager`\nWhen enabled, Prometheus uses a new service discovery manager that does not\nrestart unchanged discoveries upon reloading. This makes reloads faster and reduces\npressure on service discoveries' sources.\nUsers are encouraged to test the new service discovery manager and report any\nissues upstream.\nIn future releases, this new service discovery manager will become the default and\nthis feature flag will be ignored.\nPrometheus agent\n`--enable-feature=agent`\nWhen enabled, Prometheus runs in agent mode. The agent mode is limited to\ndiscovery, scrape and remote write.\nThis is useful when you do not need to query the Prometheus data locally, but\nonly from a central remote endpoint.\nPer-step stats\n`--enable-feature=promql-per-step-stats`\nWhen enabled, passing `stats=all` in a query request returns per-step\nstatistics. Currently this is limited to totalQueryableSamples.\nWhen disabled in either the engine or the query, per-step statistics are not\ncomputed at all.\nAuto GOMAXPROCS\n`--enable-feature=auto-gomaxprocs`\nWhen enabled, GOMAXPROCS variable is automatically set to match Linux container CPU quota.\nNo default scrape port\n`--enable-feature=no-default-scrape-port`\nWhen enabled, the default ports for HTTP (`:80`) or HTTPS (`:443`) will not be added to\nthe address used to scrape a target (the value of the `__address_` label), contrary to the default behavior.\nIn addition, if a default HTTP or HTTPS port has already been added either in a static configuration or\nby a service discovery mechanism and the respective scheme is specified (`http` or `https`), that port will be removed.\nNative Histograms\n`--enable-feature=native-histograms`\nWhen enabled, Prometheus will ingest native histograms (formerly also known as\nsparse histograms or high-res histograms). Native histograms are still highly\nexperimental. Expect breaking changes to happen (including those rendering the\nTSDB unreadable).\nNative histograms are currently only supported in the traditional Prometheus\nprotobuf exposition format. This feature flag therefore also enables a new (and\nalso experimental) protobuf parser, through which all metrics are ingested\n(i.e. not only native histograms). Prometheus will try to negotiate the\nprotobuf format first. The instrumented target needs to support the protobuf\nformat, too, and it needs to expose native histograms. The protobuf format\nallows to expose conventional and native histograms side by side. With this\nfeature flag disabled, Prometheus will continue to parse the conventional\nhistogram (albeit via the text format). With this flag enabled, Prometheus will\nstill ingest those conventional histograms that do not come with a\ncorresponding native histogram. However, if a native histogram is present,\nPrometheus will ignore the corresponding conventional histogram, with the",
    "tag": "prometheus"
  },
  {
    "title": "Installation",
    "source": "https://github.com/prometheus/prometheus/tree/main/docs/installation.md",
    "content": "\ntitle: Installation\nsort_rank: 2\n\nInstallation\nUsing pre-compiled binaries\nWe provide precompiled binaries for most official Prometheus components. Check\nout the download section for a list of all\navailable versions.\nFrom source\nFor building Prometheus components from source, see the `Makefile` targets in\nthe respective repository.\nUsing Docker\nAll Prometheus services are available as Docker images on\nQuay.io or\nDocker Hub.\nRunning Prometheus on Docker is as simple as `docker run -p 9090:9090\nprom/prometheus`. This starts Prometheus with a sample\nconfiguration and exposes it on port 9090.\nThe Prometheus image uses a volume to store the actual metrics. For\nproduction deployments it is highly recommended to use a\nnamed volume\nto ease managing the data on Prometheus upgrades.\nTo provide your own configuration, there are several options. Here are\ntwo examples.\nVolumes & bind-mount\nBind-mount your `prometheus.yml` from the host by running:\n`bash\ndocker run \\\n    -p 9090:9090 \\\n    -v /path/to/prometheus.yml:/etc/prometheus/prometheus.yml \\\n    prom/prometheus`\nOr bind-mount the directory containing `prometheus.yml` onto\n`/etc/prometheus` by running:\n`bash\ndocker run \\\n    -p 9090:9090 \\\n    -v /path/to/config:/etc/prometheus \\\n    prom/prometheus`\nCustom image\nTo avoid managing a file on the host and bind-mount it, the\nconfiguration can be baked into the image. This works well if the\nconfiguration itself is rather static and the same across all\nenvironments.\nFor this, create a new directory with a Prometheus configuration and a\n`Dockerfile` like this:\n`Dockerfile\nFROM prom/prometheus\nADD prometheus.yml /etc/prometheus/`\nNow build and run it:\n`bash\ndocker build -t my-prometheus .\ndocker run -p 9090:9090 my-prometheus`\nA more advanced option is to render the configuration dynamically on start\nwith some tooling or even have a daemon update it periodically.\nUsing configuration management systems\nIf you prefer using configuration management systems you might be interested in\nthe following third-party contributions:\nAnsible\n\nCloud Alchemy/ansible-prometheus\n\nChef\n\nrayrod2030/chef-prometheus\n\nPuppet\n\npuppet/prometheus\n\nSaltStack",
    "tag": "prometheus"
  },
  {
    "title": "Writing HTTP Service Discovery",
    "source": "https://github.com/prometheus/prometheus/tree/main/docs/http_sd.md",
    "content": "\ntitle: HTTP SD\nsort_rank: 7\n\nWriting HTTP Service Discovery\nPrometheus provides a generic HTTP Service Discovery,\nthat enables it to discover targets over an HTTP endpoint.\nThe HTTP Service Discovery is complimentary to the supported service\ndiscovery mechanisms, and is an alternative to File-based Service Discovery.\nComparison between File-Based SD and HTTP SD\nHere is a table comparing our two generic Service Discovery implementations.\n| Item | File SD | HTTP SD |\n| ---- | ------- | ------- |\n| Event Based | Yes, via inotify | No |\n| Update frequency | Instant, thanks to inotify | Following refresh_interval |\n| Format | Yaml or JSON | JSON |\n| Transport | Local file | HTTP/HTTPS |\n| Security | File-Based security | TLS, Basic auth, Authorization header, OAuth2 |\nRequirements of HTTP SD endpoints\nIf you implement an HTTP SD endpoint, here are a few requirements you should be\naware of.\nThe response is consumed as is, unmodified. On each refresh interval (default: 1\nminute), Prometheus will perform a GET request to the HTTP SD endpoint. The GET\nrequest contains a `X-Prometheus-Refresh-Interval-Seconds` HTTP header with the\nrefresh interval.\nThe SD endpoint must answer with an HTTP 200 response, with the HTTP Header\n`Content-Type: application/json`. The answer must be UTF-8 formatted.\nIf no targets should be transmitted, HTTP 200 must also be emitted, with\nan empty list `[]`. Target lists are unordered.\nPrometheus caches target lists. If an error occurs while fetching an updated\ntargets list, Prometheus keeps using the current targets list. The targets list\nis not saved across restart. The `prometheus_sd_http_failures_total` counter \nmetric tracks the number of refresh failures.\nThe whole list of targets must be returned on every scrape. There is no support\nfor incremental updates. A Prometheus instance does not send its hostname and it\nis not possible for a SD endpoint to know if the SD requests is the first one\nafter a restart or not.\nThe URL to the HTTP SD is not considered secret. The authentication and any API\nkeys should be passed with the appropriate authentication mechanisms. Prometheus\nsupports TLS authentication, basic authentication, OAuth2, and authorization\nheaders.\nHTTP_SD format\n`json\n[\n  {\n    \"targets\": [ \"<host>\", ... ],\n    \"labels\": {\n      \"<labelname>\": \"<labelvalue>\", ...\n    }\n  },\n  ...\n]`\nExamples:\n```json\n[\n    {\n        \"targets\": [\"10.0.10.2:9100\", \"10.0.10.3:9100\", \"10.0.10.4:9100\", \"10.0.10.5:9100\"],\n        \"labels\": {\n            \"__meta_datacenter\": \"london\",\n            \"__meta_prometheus_job\": \"node\"\n        }\n    },\n    {\n        \"targets\": [\"10.0.40.2:9100\", \"10.0.40.3:9100\"],\n        \"labels\": {\n            \"__meta_datacenter\": \"london\",\n            \"__meta_prometheus_job\": \"alertmanager\"\n        }\n    },\n    {\n        \"targets\": [\"10.0.40.2:9093\", \"10.0.40.3:9093\"],\n        \"labels\": {\n            \"__meta_datacenter\": \"newyork\",\n            \"__meta_prometheus_job\": \"alertmanager\"\n        }\n    }\n]",
    "tag": "prometheus"
  },
  {
    "title": "Unit Testing for Rules",
    "source": "https://github.com/prometheus/prometheus/tree/main/docs/configuration/unit_testing_rules.md",
    "content": "\ntitle: Unit Testing for Rules\nsort_rank: 6\n\nUnit Testing for Rules\nYou can use `promtool` to test your rules.\n```shell\nFor a single test file.\n./promtool test rules test.yml\nIf you have multiple test files, say test1.yml,test2.yml,test2.yml\n./promtool test rules test1.yml test2.yml test3.yml\n```\nTest file format\n```yaml\nThis is a list of rule files to consider for testing. Globs are supported.\nrule_files:\n  [ -  ]\n[ evaluation_interval:  | default = 1m ]\nThe order in which group names are listed below will be the order of evaluation of\nrule groups (at a given evaluation time). The order is guaranteed only for the groups mentioned below.\nAll the groups need not be mentioned below.\ngroup_eval_order:\n  [ -  ]\nAll the tests are listed here.\ntests:\n  [ -  ]\n```\n`<test_group>`\n``` yaml\nSeries data\ninterval: \ninput_series:\n  [ -  ]\nName of the test group\n[ name:  ]\nUnit tests for the above data.\nUnit tests for alerting rules. We consider the alerting rules from the input file.\nalert_rule_test:\n  [ -  ]\nUnit tests for PromQL expressions.\npromql_expr_test:\n  [ -  ]\nExternal labels accessible to the alert template.\nexternal_labels:\n  [ :  ... ]\nExternal URL accessible to the alert template.\nUsually set using --web.external-url.\n[ external_url:  ]\n```\n`<series>`\n```yaml\nThis follows the usual series notation '{=, ...}'\nExamples:\nseries_name{label1=\"value1\", label2=\"value2\"}\ngo_goroutines{job=\"prometheus\", instance=\"localhost:9090\"}\nseries: \nThis uses expanding notation.\nExpanding notation:\n'a+bxc' becomes 'a a+b a+(2b) a+(3b) \u2026 a+(c*b)'\nRead this as series starts at a, then c further samples incrementing by b.\n'a-bxc' becomes 'a a-b a-(2b) a-(3b) \u2026 a-(c*b)'\nRead this as series starts at a, then c further samples decrementing by b (or incrementing by negative b).\nThere are special values to indicate missing and stale samples:\n'_' represents a missing sample from scrape\n'stale' indicates a stale sample\nExamples:\n1. '-2+4x3' becomes '-2 2 6 10' - series starts at -2, then 3 further samples incrementing by 4.\n2. ' 1-2x4' becomes '1 -1 -3 -5 -7' - series starts at 1, then 4 further samples decrementing by 2.\n3. ' 1x4' becomes '1 1 1 1 1' - shorthand for '1+0x4', series starts at 1, then 4 further samples incrementing by 0.\n4. ' 1 x3 stale' becomes '1 _  _ stale' - the missing sample cannot increment, so 3 missing samples are produced by the '_x3' expression.\nvalues: \n```\n`<alert_test_case>`\nPrometheus allows you to have same alertname for different alerting rules. Hence in this unit testing, you have to list the union of all the firing alerts for the alertname under a single `<alert_test_case>`.\n``` yaml\nThe time elapsed from time=0s when the alerts have to be checked.\neval_time: \nName of the alert to be tested.\nalertname: \nList of expected alerts which are firing under the given alertname at\ngiven evaluation time. If you want to test if an alerting rule should\nnot be firing, then you can mention the above fields and leave 'exp_alerts' empty.\nexp_alerts:\n  [ -  ]\n```\n`<alert>`\n``` yaml\nThese are the expanded labels and annotations of the expected alert.\nNote: labels also include the labels of the sample associated with the\nalert (same as what you see in `/alerts`, without series `__name__` and `alertname`)\nexp_labels:\n  [ :  ]\nexp_annotations:\n  [ :  ]\n```\n`<promql_test_case>`\n```yaml\nExpression to evaluate\nexpr: \nThe time elapsed from time=0s when the expression has to be evaluated.\neval_time: \nExpected samples at the given evaluation time.\nexp_samples:\n  [ -  ]\n```\n`<sample>`\n```yaml\nLabels of the sample in usual series notation '{=, ...}'\nExamples:\nseries_name{label1=\"value1\", label2=\"value2\"}\ngo_goroutines{job=\"prometheus\", instance=\"localhost:9090\"}\nlabels: \nThe expected value of the PromQL expression.\nvalue: \n```\nExample\nThis is an example input file for unit testing which passes the test. `test.yml` is the test file which follows the syntax above and `alerts.yml` contains the alerting rules.\nWith `alerts.yml` in the same directory, run `./promtool test rules test.yml`.\n`test.yml`\n```yaml\nThis is the main input for unit testing.\nOnly this file is passed as command line argument.\nrule_files:\n    - alerts.yml\nevaluation_interval: 1m\ntests:\n    # Test 1.\n    - interval: 1m\n      # Series data.\n      input_series:\n          - series: 'up{job=\"prometheus\", instance=\"localhost:9090\"}'\n            values: '0 0 0 0 0 0 0 0 0 0 0 0 0 0 0'\n          - series: 'up{job=\"node_exporter\", instance=\"localhost:9100\"}'\n            values: '1+0x6 0 0 0 0 0 0 0 0' # 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n          - series: 'go_goroutines{job=\"prometheus\", instance=\"localhost:9090\"}'\n            values: '10+10x2 30+20x5' # 10 20 30 30 50 70 90 110 130\n          - series: 'go_goroutines{job=\"node_exporter\", instance=\"localhost:9100\"}'\n            values: '10+10x7 10+30x4' # 10 20 30 40 50 60 70 80 10 40 70 100 130\n\n\n```  # Unit test for alerting rules.\n  alert_rule_test:\n      # Unit test 1.\n      - eval_time: 10m\n        alertname: InstanceDown\n        exp_alerts:\n            # Alert 1.\n            - exp_labels:\n                  severity: page\n                  instance: localhost:9090\n                  job: prometheus\n              exp_annotations:\n                  summary: \"Instance localhost:9090 down\"\n                  description: \"localhost:9090 of job prometheus has been down for more than 5 minutes.\"\n  # Unit tests for promql expressions.\n  promql_expr_test:\n      # Unit test 1.\n      - expr: go_goroutines > 5\n        eval_time: 4m\n        exp_samples:\n            # Sample 1.\n            - labels: 'go_goroutines{job=\"prometheus\",instance=\"localhost:9090\"}'\n              value: 50\n            # Sample 2.\n            - labels: 'go_goroutines{job=\"node_exporter\",instance=\"localhost:9100\"}'\n              value: 50\n```\n\n\n```\n`alerts.yml`\n```yaml\nThis is the rules file.\ngroups:\n- name: example\n  rules:\n\n\nalert: InstanceDown\n    expr: up == 0\n    for: 5m\n    labels:\n        severity: page\n    annotations:\n        summary: \"Instance {{ $labels.instance }} down\"\n        description: \"{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.\"\n\n\nalert: AnotherInstanceDown\n    expr: up == 0\n    for: 10m\n    labels:\n        severity: page\n    annotations:\n        summary: \"Instance {{ $labels.instance }} down\"\n        description: \"{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.\"\n\n",
    "tag": "prometheus"
  },
  {
    "title": "Template reference",
    "source": "https://github.com/prometheus/prometheus/tree/main/docs/configuration/template_reference.md",
    "content": "\ntitle: Template reference\nsort_rank: 5\n\nTemplate reference\nPrometheus supports templating in the annotations and labels of alerts,\nas well as in served console pages. Templates have the ability to run\nqueries against the local database, iterate over data, use conditionals,\nformat data, etc. The Prometheus templating language is based on the Go\ntemplating system.\nData Structures\nThe primary data structure for dealing with time series data is the sample, defined as:\n`go\ntype sample struct {\n        Labels map[string]string\n        Value  float64\n}`\nThe metric name of the sample is encoded in a special `__name__` label in the `Labels` map.\n`[]sample` means a list of samples.\n`interface{}` in Go is similar to a void pointer in C.\nFunctions\nIn addition to the default\nfunctions provided by Go\ntemplating, Prometheus provides functions for easier processing of query\nresults in templates.\nIf functions are used in a pipeline, the pipeline value is passed as the last argument.\nQueries\n| Name          | Arguments     | Returns  | Notes    |\n| ------------- | ------------- | -------- | -------- |\n| query         | query string  | []sample | Queries the database, does not support returning range vectors.  |\n| first         | []sample      | sample   | Equivalent to `index a 0`  |\n| label         | label, sample | string   | Equivalent to `index sample.Labels label`  |\n| value         | sample        | float64  | Equivalent to `sample.Value`  |\n| sortByLabel   | label, []samples | []sample | Sorts the samples by the given label. Is stable.  |\n`first`, `label` and `value` are intended to make query results easily usable in pipelines.\nNumbers\n| Name                | Arguments        | Returns |  Notes    |\n|---------------------| -----------------| --------| --------- |\n| humanize            | number or string | string  | Converts a number to a more readable format, using metric prefixes.\n| humanize1024        | number or string | string  | Like `humanize`, but uses 1024 as the base rather than 1000. |\n| humanizeDuration    | number or string | string  | Converts a duration in seconds to a more readable format. |\n| humanizePercentage  | number or string | string  | Converts a ratio value to a fraction of 100. |\n| humanizeTimestamp   | number or string | string     | Converts a Unix timestamp in seconds to a more readable format. |\n| toTime              | number or string | *time.Time | Converts a Unix timestamp in seconds to a time.Time.            |\nHumanizing functions are intended to produce reasonable output for consumption\nby humans, and are not guaranteed to return the same results between Prometheus\nversions.\nStrings\n| Name          | Arguments     | Returns |    Notes    |\n| ------------- | ------------- | ------- | ----------- |\n| title         | string        | string  | strings.Title, capitalises first character of each word.|\n| toUpper       | string        | string  | strings.ToUpper, converts all characters to upper case.|\n| toLower       | string        | string  | strings.ToLower, converts all characters to lower case.|\n| stripPort     | string        | string  | net.SplitHostPort, splits string into host and port, then returns only host.|\n| match         | pattern, text | boolean | regexp.MatchString Tests for a unanchored regexp match. |\n| reReplaceAll  | pattern, replacement, text | string | Regexp.ReplaceAllString Regexp substitution, unanchored. |\n| graphLink  | expr | string | Returns path to graph view in the expression browser for the expression. |\n| tableLink  | expr | string | Returns path to tabular (\"Table\") view in the expression browser for the expression. |\n| parseDuration | string | float | Parses a duration string such as \"1h\" into the number of seconds it represents. |\n| stripDomain | string | string | Removes the domain part of a FQDN. Leaves port untouched. |\nOthers\n| Name          | Arguments     | Returns |    Notes    |\n| ------------- | ------------- | ------- | ----------- |\n| args          | []interface{} | map[string]interface{} | This converts a list of objects to a map with keys arg0, arg1 etc. This is intended to allow multiple arguments to be passed to templates. |\n| tmpl          | string, []interface{} | nothing  | Like the built-in `template`, but allows non-literals as the template name. Note that the result is assumed to be safe, and will not be auto-escaped. Only available in consoles. |\n| safeHtml      | string        | string  | Marks string as HTML not requiring auto-escaping. |\n| pathPrefix    | none        | string  | The external URL path for use in console templates. |\nTemplate type differences\nEach of the types of templates provide different information that can be used to\nparameterize templates, and have a few other differences.\nAlert field templates\n`.Value`, `.Labels`, `.ExternalLabels`, and `.ExternalURL` contain the alert value, the alert\nlabels, the globally configured external labels, and the external URL (configured with `--web.external-url`) respectively. They are\nalso exposed as the `$value`, `$labels`, `$externalLabels`, and `$externalURL` variables for\nconvenience.\nConsole templates\nConsoles are exposed on `/consoles/`, and sourced from the directory pointed to\nby the `-web.console.templates` flag.\nConsole templates are rendered with\nhtml/template, which provides\nauto-escaping. To bypass the auto-escaping use the `safe*` functions.,\nURL parameters are available as a map in `.Params`. To access multiple URL\nparameters by the same name, `.RawParams` is a map of the list values for each\nparameter. The URL path is available in `.Path`, excluding the `/consoles/`\nprefix. The globally configured external labels are available as\n`.ExternalLabels`. There are also convenience variables for all four:\n`$rawParams`, `$params`, `$path`, and `$externalLabels`.\nConsoles also have access to all the templates defined with `{{define\n\"templateName\"}}...{{end}}` found in `*.lib` files in the directory pointed to\nby the `-web.console.libraries` flag. As this is a shared namespace, take care\nto avoid clashes with other users. Template names beginning with `prom`,\n`_prom`, and `__` are reserved for use by Prometheus, as are the functions",
    "tag": "prometheus"
  },
  {
    "title": "Defining recording rules",
    "source": "https://github.com/prometheus/prometheus/tree/main/docs/configuration/recording_rules.md",
    "content": "\ntitle: Recording rules\nsort_rank: 2\n\nDefining recording rules\nConfiguring rules\nPrometheus supports two types of rules which may be configured and then\nevaluated at regular intervals: recording rules and alerting\nrules. To include rules in Prometheus, create a file\ncontaining the necessary rule statements and have Prometheus load the file via\nthe `rule_files` field in the Prometheus configuration.\nRule files use YAML.\nThe rule files can be reloaded at runtime by sending `SIGHUP` to the Prometheus\nprocess. The changes are only applied if all rule files are well-formatted.\nNote about native histograms (experimental feature): Native histogram are always\nrecorded as gauge histograms (for now). Most cases will create gauge histograms\nnaturally, e.g. after `rate()`.\nSyntax-checking rules\nTo quickly check whether a rule file is syntactically correct without starting\na Prometheus server, you can use Prometheus's `promtool` command-line utility\ntool:\n`bash\npromtool check rules /path/to/example.rules.yml`\nThe `promtool` binary is part of the `prometheus` archive offered on the\nproject's download page.\nWhen the file is syntactically valid, the checker prints a textual\nrepresentation of the parsed rules to standard output and then exits with\na `0` return status.\nIf there are any syntax errors or invalid input arguments, it prints an error \nmessage to standard error and exits with a `1` return status.\nRecording rules\nRecording rules allow you to precompute frequently needed or computationally\nexpensive expressions and save their result as a new set of time series.\nQuerying the precomputed result will then often be much faster than executing\nthe original expression every time it is needed. This is especially useful for\ndashboards, which need to query the same expression repeatedly every time they\nrefresh.\nRecording and alerting rules exist in a rule group. Rules within a group are\nrun sequentially at a regular interval, with the same evaluation time.\nThe names of recording rules must be\nvalid metric names.\nThe names of alerting rules must be\nvalid label values.\nThe syntax of a rule file is:\n`yaml\ngroups:\n  [ - <rule_group> ]`\nA simple example rules file would be:\n`yaml\ngroups:\n  - name: example\n    rules:\n    - record: code:prometheus_http_requests_total:sum\n      expr: sum by (code) (prometheus_http_requests_total)`\n`<rule_group>`\n```\nThe name of the group. Must be unique within a file.\nname: \nHow often rules in the group are evaluated.\n[ interval:  | default = global.evaluation_interval ]\nLimit the number of alerts an alerting rule and series a recording\nrule can produce. 0 is no limit.\n[ limit:  | default = 0 ]\nrules:\n  [ -  ... ]\n```\n`<rule>`\nThe syntax for recording rules is:\n```\nThe name of the time series to output to. Must be a valid metric name.\nrecord: \nThe PromQL expression to evaluate. Every evaluation cycle this is\nevaluated at the current time, and the result recorded as a new set of\ntime series with the metric name as given by 'record'.\nexpr: \nLabels to add or overwrite before storing the result.\nlabels:\n  [ :  ]\n```\nThe syntax for alerting rules is:\n```\nThe name of the alert. Must be a valid label value.\nalert: \nThe PromQL expression to evaluate. Every evaluation cycle this is\nevaluated at the current time, and all resultant time series become\npending/firing alerts.\nexpr: \nAlerts are considered firing once they have been returned for this long.\nAlerts which have not yet fired for long enough are considered pending.\n[ for:  | default = 0s ]\nHow long an alert will continue firing after the condition that triggered it\nhas cleared.\n[ keep_firing_for:  | default = 0s ]\nLabels to add or overwrite for each alert.\nlabels:\n  [ :  ]\nAnnotations to add to each alert.\nannotations:\n  [ :  ]\n```\nSee also the\nbest practices for naming metrics created by recording rules.\nLimiting alerts and series\nA limit for alerts produced by alerting rules and series produced recording rules\ncan be configured per-group. When the limit is exceeded, all series produced\nby the rule are discarded, and if it's an alerting rule, all alerts for\nthe rule, active, pending, or inactive, are cleared as well. The event will be\nrecorded as an error in the evaluation, and as such no stale markers are",
    "tag": "prometheus"
  },
  {
    "title": "Alerting rules",
    "source": "https://github.com/prometheus/prometheus/tree/main/docs/configuration/alerting_rules.md",
    "content": "\ntitle: Alerting rules\nsort_rank: 3\n\nAlerting rules\nAlerting rules allow you to define alert conditions based on Prometheus\nexpression language expressions and to send notifications about firing alerts\nto an external service. Whenever the alert expression results in one or more\nvector elements at a given point in time, the alert counts as active for these\nelements' label sets.\nDefining alerting rules\nAlerting rules are configured in Prometheus in the same way as recording\nrules.\nAn example rules file with an alert would be:\n`yaml\ngroups:\n- name: example\n  rules:\n  - alert: HighRequestLatency\n    expr: job:request_latency_seconds:mean5m{job=\"myjob\"} > 0.5\n    for: 10m\n    labels:\n      severity: page\n    annotations:\n      summary: High request latency`\nThe optional `for` clause causes Prometheus to wait for a certain duration\nbetween first encountering a new expression output vector element and counting an alert as firing for this element. In this case, Prometheus will check that the alert continues to be active during each evaluation for 10 minutes before firing the alert. Elements that are active, but not firing yet, are in the pending state.\nThe `labels` clause allows specifying a set of additional labels to be attached\nto the alert. Any existing conflicting labels will be overwritten. The label\nvalues can be templated.\nThe `annotations` clause specifies a set of informational labels that can be used to store longer additional information such as alert descriptions or runbook links. The annotation values can be templated.\nTemplating\nLabel and annotation values can be templated using console\ntemplates.  The `$labels`\nvariable holds the label key/value pairs of an alert instance. The configured\nexternal labels can be accessed via the `$externalLabels` variable. The\n`$value` variable holds the evaluated value of an alert instance.\n\n\n```# To insert a firing element's label values:\n{{ $labels.<labelname> }}\n# To insert the numeric expression value of the firing element:\n{{ $value }}\n```\n\n\nExamples:\n```yaml\ngroups:\n- name: example\n  rules:\n# Alert for any instance that is unreachable for >5 minutes.\n  - alert: InstanceDown\n    expr: up == 0\n    for: 5m\n    labels:\n      severity: page\n    annotations:\n      summary: \"Instance {{ $labels.instance }} down\"\n      description: \"{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.\"\n# Alert for any instance that has a median request latency >1s.\n  - alert: APIHighRequestLatency\n    expr: api_http_request_latencies_second{quantile=\"0.5\"} > 1\n    for: 10m\n    annotations:\n      summary: \"High request latency on {{ $labels.instance }}\"\n      description: \"{{ $labels.instance }} has a median request latency above 1s (current value: {{ $value }}s)\"\n```\nInspecting alerts during runtime\nTo manually inspect which alerts are active (pending or firing), navigate to\nthe \"Alerts\" tab of your Prometheus instance. This will show you the exact\nlabel sets for which each defined alert is currently active.\nFor pending and firing alerts, Prometheus also stores synthetic time series of\nthe form `ALERTS{alertname=\"<alert name>\", alertstate=\"<pending or firing>\", <additional alert labels>}`.\nThe sample value is set to `1` as long as the alert is in the indicated active\n(pending or firing) state, and the series is marked stale when this is no\nlonger the case.\nSending alert notifications\nPrometheus's alerting rules are good at figuring what is broken right now, but\nthey are not a fully-fledged notification solution. Another layer is needed to\nadd summarization, notification rate limiting, silencing and alert dependencies\non top of the simple alert definitions. In Prometheus's ecosystem, the\nAlertmanager takes on this\nrole. Thus, Prometheus may be configured to periodically send information about\nalert states to an Alertmanager instance, which then takes care of dispatching\nthe right notifications.\nPrometheus can be configured to automatically discover available",
    "tag": "prometheus"
  },
  {
    "title": "HTTPS and authentication",
    "source": "https://github.com/prometheus/prometheus/tree/main/docs/configuration/https.md",
    "content": "\ntitle: HTTPS and authentication\nsort_rank: 7\n\nHTTPS and authentication\nPrometheus supports basic authentication and TLS.\nThis is experimental and might change in the future.\nTo specify which web configuration file to load, use the `--web.config.file` flag.\nThe file is written in YAML format,\ndefined by the scheme described below.\nBrackets indicate that a parameter is optional. For non-list parameters the\nvalue is set to the specified default.\nThe file is read upon every http request, such as any change in the\nconfiguration and the certificates is picked up immediately.\nGeneric placeholders are defined as follows:\n\n`<boolean>`: a boolean that can take the values `true` or `false`\n`<filename>`: a valid path in the current working directory\n`<secret>`: a regular string that is a secret, such as a password\n`<string>`: a regular string\n\nA valid example file can be found here.\n```\ntls_server_config:\n  # Certificate and key files for server to use to authenticate to client.\n  cert_file: \n  key_file: \n# Server policy for client authentication. Maps to ClientAuth Policies.\n  # For more detail on clientAuth options:\n  # https://golang.org/pkg/crypto/tls/#ClientAuthType\n  #\n  # NOTE: If you want to enable client authentication, you need to use\n  # RequireAndVerifyClientCert. Other values are insecure.\n  [ client_auth_type:  | default = \"NoClientCert\" ]\n# CA certificate for client certificate authentication to the server.\n  [ client_ca_file:  ]\n# Minimum TLS version that is acceptable.\n  [ min_version:  | default = \"TLS12\" ]\n# Maximum TLS version that is acceptable.\n  [ max_version:  | default = \"TLS13\" ]\n# List of supported cipher suites for TLS versions up to TLS 1.2. If empty,\n  # Go default cipher suites are used. Available cipher suites are documented\n  # in the go documentation:\n  # https://golang.org/pkg/crypto/tls/#pkg-constants\n  #\n  # Note that only the cipher returned by the following function are supported:\n  # https://pkg.go.dev/crypto/tls#CipherSuites\n  [ cipher_suites:\n    [ -  ] ]\n# prefer_server_cipher_suites controls whether the server selects the\n  # client's most preferred ciphersuite, or the server's most preferred\n  # ciphersuite. If true then the server's preference, as expressed in\n  # the order of elements in cipher_suites, is used.\n  [ prefer_server_cipher_suites:  | default = true ]\n# Elliptic curves that will be used in an ECDHE handshake, in preference\n  # order. Available curves are documented in the go documentation:\n  # https://golang.org/pkg/crypto/tls/#CurveID\n  [ curve_preferences:\n    [ -  ] ]\nhttp_server_config:\n  # Enable HTTP/2 support. Note that HTTP/2 is only supported with TLS.\n  # This can not be changed on the fly.\n  [ http2:  | default = true ]\n  # List of headers that can be added to HTTP responses.\n  [ headers:\n    # Set the Content-Security-Policy header to HTTP responses.\n    # Unset if blank.\n    [ Content-Security-Policy:  ]\n    # Set the X-Frame-Options header to HTTP responses.\n    # Unset if blank. Accepted values are deny and sameorigin.\n    # https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Frame-Options\n    [ X-Frame-Options:  ]\n    # Set the X-Content-Type-Options header to HTTP responses.\n    # Unset if blank. Accepted value is nosniff.\n    # https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Content-Type-Options\n    [ X-Content-Type-Options:  ]\n    # Set the X-XSS-Protection header to all responses.\n    # Unset if blank.\n    # https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-XSS-Protection\n    [ X-XSS-Protection:  ]\n    # Set the Strict-Transport-Security header to HTTP responses.\n    # Unset if blank.\n    # Please make sure that you use this with care as this header might force\n    # browsers to load Prometheus and the other applications hosted on the same\n    # domain and subdomains over HTTPS.\n    # https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Strict-Transport-Security\n    [ Strict-Transport-Security:  ] ]\nUsernames and hashed passwords that have full access to the web\nserver via basic authentication. If empty, no basic authentication is\nrequired. Passwords are hashed with bcrypt.\nbasic_auth_users:\n  [ :  ... ]\n```",
    "tag": "prometheus"
  },
  {
    "title": "Configuration",
    "source": "https://github.com/prometheus/prometheus/tree/main/docs/configuration/configuration.md",
    "content": "\ntitle: Configuration\nsort_rank: 1\n\nConfiguration\nPrometheus is configured via command-line flags and a configuration file. While\nthe command-line flags configure immutable system parameters (such as storage\nlocations, amount of data to keep on disk and in memory, etc.), the\nconfiguration file defines everything related to scraping jobs and their\ninstances, as well as\nwhich rule files to load.\nTo view all available command-line flags, run `./prometheus -h`.\nPrometheus can reload its configuration at runtime. If the new configuration\nis not well-formed, the changes will not be applied.\nA configuration reload is triggered by sending a `SIGHUP` to the Prometheus process or\nsending a HTTP POST request to the `/-/reload` endpoint (when the `--web.enable-lifecycle` flag is enabled).\nThis will also reload any configured rule files.\nConfiguration file\nTo specify which configuration file to load, use the `--config.file` flag.\nThe file is written in YAML format,\ndefined by the scheme described below.\nBrackets indicate that a parameter is optional. For non-list parameters the\nvalue is set to the specified default.\nGeneric placeholders are defined as follows:\n\n`<boolean>`: a boolean that can take the values `true` or `false`\n`<duration>`: a duration matching the regular expression `((([0-9]+)y)?(([0-9]+)w)?(([0-9]+)d)?(([0-9]+)h)?(([0-9]+)m)?(([0-9]+)s)?(([0-9]+)ms)?|0)`, e.g. `1d`, `1h30m`, `5m`, `10s`\n`<filename>`: a valid path in the current working directory\n`<float>`: a floating-point number\n`<host>`: a valid string consisting of a hostname or IP followed by an optional port number\n`<int>`: an integer value\n`<labelname>`: a string matching the regular expression `[a-zA-Z_][a-zA-Z0-9_]*`\n`<labelvalue>`: a string of unicode characters\n`<path>`: a valid URL path\n`<scheme>`: a string that can take the values `http` or `https`\n`<secret>`: a regular string that is a secret, such as a password\n`<string>`: a regular string\n`<size>`: a size in bytes, e.g. `512MB`. A unit is required. Supported units: B, KB, MB, GB, TB, PB, EB.\n`<tmpl_string>`: a string which is template-expanded before usage\n\nThe other placeholders are specified separately.\nA valid example file can be found here.\nThe global configuration specifies parameters that are valid in all other configuration\ncontexts. They also serve as defaults for other configuration sections.\n```yaml\nglobal:\n  # How frequently to scrape targets by default.\n  [ scrape_interval:  | default = 1m ]\n# How long until a scrape request times out.\n  [ scrape_timeout:  | default = 10s ]\n# How frequently to evaluate rules.\n  [ evaluation_interval:  | default = 1m ]\n# The labels to add to any time series or alerts when communicating with\n  # external systems (federation, remote storage, Alertmanager).\n  external_labels:\n    [ :  ... ]\n# File to which PromQL queries are logged.\n  # Reloading the configuration will reopen the file.\n  [ query_log_file:  ]\nRule files specifies a list of globs. Rules and alerts are read from\nall matching files.\nrule_files:\n  [ -  ... ]\nA list of scrape configurations.\nscrape_configs:\n  [ -  ... ]\nAlerting specifies settings related to the Alertmanager.\nalerting:\n  alert_relabel_configs:\n    [ -  ... ]\n  alertmanagers:\n    [ -  ... ]\nSettings related to the remote write feature.\nremote_write:\n  [ -  ... ]\nSettings related to the remote read feature.\nremote_read:\n  [ -  ... ]\nStorage related settings that are runtime reloadable.\nstorage:\n  [ tsdb:  ]\n  [ exemplars:  ]\nConfigures exporting traces.\ntracing:\n  [  ]\n```\n`<scrape_config>`\nA `scrape_config` section specifies a set of targets and parameters describing how\nto scrape them. In the general case, one scrape configuration specifies a single\njob. In advanced configurations, this may change.\nTargets may be statically configured via the `static_configs` parameter or\ndynamically discovered using one of the supported service-discovery mechanisms.\nAdditionally, `relabel_configs` allow advanced modifications to any\ntarget and its labels before scraping.\n```yaml\nThe job name assigned to scraped metrics by default.\njob_name: \nHow frequently to scrape targets from this job.\n[ scrape_interval:  | default =  ]\nPer-scrape timeout when scraping this job.\n[ scrape_timeout:  | default =  ]\nThe HTTP resource path on which to fetch metrics from targets.\n[ metrics_path:  | default = /metrics ]\nhonor_labels controls how Prometheus handles conflicts between labels that are\nalready present in scraped data and labels that Prometheus would attach\nserver-side (\"job\" and \"instance\" labels, manually configured target\nlabels, and labels generated by service discovery implementations).\n\nIf honor_labels is set to \"true\", label conflicts are resolved by keeping label\nvalues from the scraped data and ignoring the conflicting server-side labels.\n\nIf honor_labels is set to \"false\", label conflicts are resolved by renaming\nconflicting labels in the scraped data to \"exported_\" (for\nexample \"exported_instance\", \"exported_job\") and then attaching server-side\nlabels.\n\nSetting honor_labels to \"true\" is useful for use cases such as federation and\nscraping the Pushgateway, where all labels specified in the target should be\npreserved.\n\nNote that any globally configured \"external_labels\" are unaffected by this\nsetting. In communication with external systems, they are always applied only\nwhen a time series does not have a given label yet and are ignored otherwise.\n[ honor_labels:  | default = false ]\nhonor_timestamps controls whether Prometheus respects the timestamps present\nin scraped data.\n\nIf honor_timestamps is set to \"true\", the timestamps of the metrics exposed\nby the target will be used.\n\nIf honor_timestamps is set to \"false\", the timestamps of the metrics exposed\nby the target will be ignored.\n[ honor_timestamps:  | default = true ]\nConfigures the protocol scheme used for requests.\n[ scheme:  | default = http ]\nOptional HTTP URL parameters.\nparams:\n  [ : [, ...] ]\nSets the `Authorization` header on every scrape request with the\nconfigured username and password.\npassword and password_file are mutually exclusive.\nbasic_auth:\n  [ username:  ]\n  [ password:  ]\n  [ password_file:  ]\nSets the `Authorization` header on every scrape request with\nthe configured credentials.\nauthorization:\n  # Sets the authentication type of the request.\n  [ type:  | default: Bearer ]\n  # Sets the credentials of the request. It is mutually exclusive with\n  # `credentials_file`.\n  [ credentials:  ]\n  # Sets the credentials of the request with the credentials read from the\n  # configured file. It is mutually exclusive with `credentials`.\n  [ credentials_file:  ]\nOptional OAuth 2.0 configuration.\nCannot be used at the same time as basic_auth or authorization.\noauth2:\n  [  ]\nConfigure whether scrape requests follow HTTP 3xx redirects.\n[ follow_redirects:  | default = true ]\nWhether to enable HTTP2.\n[ enable_http2:  | default: true ]\nConfigures the scrape request's TLS settings.\ntls_config:\n  [  ]\nOptional proxy URL.\n[ proxy_url:  ]\nSpecifies headers to send to proxies during CONNECT requests.\n[ proxy_connect_header:\n  [ : [, ...] ] ]\nList of Azure service discovery configurations.\nazure_sd_configs:\n  [ -  ... ]\nList of Consul service discovery configurations.\nconsul_sd_configs:\n  [ -  ... ]\nList of DigitalOcean service discovery configurations.\ndigitalocean_sd_configs:\n  [ -  ... ]\nList of Docker service discovery configurations.\ndocker_sd_configs:\n  [ -  ... ]\nList of Docker Swarm service discovery configurations.\ndockerswarm_sd_configs:\n  [ -  ... ]\nList of DNS service discovery configurations.\ndns_sd_configs:\n  [ -  ... ]\nList of EC2 service discovery configurations.\nec2_sd_configs:\n  [ -  ... ]\nList of Eureka service discovery configurations.\neureka_sd_configs:\n  [ -  ... ]\nList of file service discovery configurations.\nfile_sd_configs:\n  [ -  ... ]\nList of GCE service discovery configurations.\ngce_sd_configs:\n  [ -  ... ]\nList of Hetzner service discovery configurations.\nhetzner_sd_configs:\n  [ -  ... ]\nList of HTTP service discovery configurations.\nhttp_sd_configs:\n  [ -  ... ]\nList of IONOS service discovery configurations.\nionos_sd_configs:\n  [ -  ... ]\nList of Kubernetes service discovery configurations.\nkubernetes_sd_configs:\n  [ -  ... ]\nList of Kuma service discovery configurations.\nkuma_sd_configs:\n  [ -  ... ]\nList of Lightsail service discovery configurations.\nlightsail_sd_configs:\n  [ -  ... ]\nList of Linode service discovery configurations.\nlinode_sd_configs:\n  [ -  ... ]\nList of Marathon service discovery configurations.\nmarathon_sd_configs:\n  [ -  ... ]\nList of AirBnB's Nerve service discovery configurations.\nnerve_sd_configs:\n  [ -  ... ]\nList of Nomad service discovery configurations.\nnomad_sd_configs:\n  [ -  ... ]\nList of OpenStack service discovery configurations.\nopenstack_sd_configs:\n  [ -  ... ]\nList of OVHcloud service discovery configurations.\novhcloud_sd_configs:\n  [ -  ... ]\nList of PuppetDB service discovery configurations.\npuppetdb_sd_configs:\n  [ -  ... ]\nList of Scaleway service discovery configurations.\nscaleway_sd_configs:\n  [ -  ... ]\nList of Zookeeper Serverset service discovery configurations.\nserverset_sd_configs:\n  [ -  ... ]\nList of Triton service discovery configurations.\ntriton_sd_configs:\n  [ -  ... ]\nList of Uyuni service discovery configurations.\nuyuni_sd_configs:\n  [ -  ... ]\nList of labeled statically configured targets for this job.\nstatic_configs:\n  [ -  ... ]\nList of target relabel configurations.\nrelabel_configs:\n  [ -  ... ]\nList of metric relabel configurations.\nmetric_relabel_configs:\n  [ -  ... ]\nAn uncompressed response body larger than this many bytes will cause the\nscrape to fail. 0 means no limit. Example: 100MB.\nThis is an experimental feature, this behaviour could\nchange or be removed in the future.\n[ body_size_limit:  | default = 0 ]\nPer-scrape limit on number of scraped samples that will be accepted.\nIf more than this number of samples are present after metric relabeling\nthe entire scrape will be treated as failed. 0 means no limit.\n[ sample_limit:  | default = 0 ]\nPer-scrape limit on number of labels that will be accepted for a sample. If\nmore than this number of labels are present post metric-relabeling, the\nentire scrape will be treated as failed. 0 means no limit.\n[ label_limit:  | default = 0 ]\nPer-scrape limit on length of labels name that will be accepted for a sample.\nIf a label name is longer than this number post metric-relabeling, the entire\nscrape will be treated as failed. 0 means no limit.\n[ label_name_length_limit:  | default = 0 ]\nPer-scrape limit on length of labels value that will be accepted for a sample.\nIf a label value is longer than this number post metric-relabeling, the\nentire scrape will be treated as failed. 0 means no limit.\n[ label_value_length_limit:  | default = 0 ]\nPer-scrape config limit on number of unique targets that will be\naccepted. If more than this number of targets are present after target\nrelabeling, Prometheus will mark the targets as failed without scraping them.\n0 means no limit. This is an experimental feature, this behaviour could\nchange in the future.\n[ target_limit:  | default = 0 ]\n```\nWhere `<job_name>` must be unique across all scrape configurations.\n`<tls_config>`\nA `tls_config` allows configuring TLS connections.\n```yaml\nCA certificate to validate API server certificate with.\n[ ca_file:  ]\nCertificate and key files for client cert authentication to the server.\n[ cert_file:  ]\n[ key_file:  ]\nServerName extension to indicate the name of the server.\nhttps://tools.ietf.org/html/rfc4366#section-3.1\n[ server_name:  ]\nDisable validation of the server certificate.\n[ insecure_skip_verify:  ]\nMinimum acceptable TLS version. Accepted values: TLS10 (TLS 1.0), TLS11 (TLS\n1.1), TLS12 (TLS 1.2), TLS13 (TLS 1.3).\nIf unset, Prometheus will use Go default minimum version, which is TLS 1.2.\nSee MinVersion in https://pkg.go.dev/crypto/tls#Config.\n[ min_version:  ]\nMaximum acceptable TLS version. Accepted values: TLS10 (TLS 1.0), TLS11 (TLS\n1.1), TLS12 (TLS 1.2), TLS13 (TLS 1.3).\nIf unset, Prometheus will use Go default maximum version, which is TLS 1.3.\nSee MaxVersion in https://pkg.go.dev/crypto/tls#Config.\n[ max_version:  ]\n```\n`<oauth2>`\nOAuth 2.0 authentication using the client credentials grant type.\nPrometheus fetches an access token from the specified endpoint with\nthe given client access and secret keys.\n```yaml\nclient_id: \n[ client_secret:  ]\nRead the client secret from a file.\nIt is mutually exclusive with `client_secret`.\n[ client_secret_file:  ]\nScopes for the token request.\nscopes:\n  [ -  ... ]\nThe URL to fetch the token from.\ntoken_url: \nOptional parameters to append to the token URL.\nendpoint_params:\n  [ :  ... ]\nConfigures the token request's TLS settings.\ntls_config:\n  [  ]\nOptional proxy URL.\n[ proxy_url:  ]\nSpecifies headers to send to proxies during CONNECT requests.\n[ proxy_connect_header:\n  [ : [, ...] ] ]\n```\n`<azure_sd_config>`\nAzure SD configurations allow retrieving scrape targets from Azure VMs.\nThe following meta labels are available on targets during relabeling:\n\n`__meta_azure_machine_id`: the machine ID\n`__meta_azure_machine_location`: the location the machine runs in\n`__meta_azure_machine_name`: the machine name\n`__meta_azure_machine_computer_name`: the machine computer name\n`__meta_azure_machine_os_type`: the machine operating system\n`__meta_azure_machine_private_ip`: the machine's private IP\n`__meta_azure_machine_public_ip`: the machine's public IP if it exists\n`__meta_azure_machine_resource_group`: the machine's resource group\n`__meta_azure_machine_tag_<tagname>`: each tag value of the machine\n`__meta_azure_machine_scale_set`: the name of the scale set which the vm is part of (this value is only set if you are using a scale set)\n`__meta_azure_machine_size`: the machine size\n`__meta_azure_subscription_id`: the subscription ID\n`__meta_azure_tenant_id`: the tenant ID\n\nSee below for the configuration options for Azure discovery:\n```yaml\nThe information to access the Azure API.\nThe Azure environment.\n[ environment:  | default = AzurePublicCloud ]\nThe authentication method, either OAuth or ManagedIdentity.\nSee https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview\n[ authentication_method:  | default = OAuth]\nThe subscription ID. Always required.\nsubscription_id: \nOptional tenant ID. Only required with authentication_method OAuth.\n[ tenant_id:  ]\nOptional client ID. Only required with authentication_method OAuth.\n[ client_id:  ]\nOptional client secret. Only required with authentication_method OAuth.\n[ client_secret:  ]\nOptional resource group name. Limits discovery to this resource group.\n[ resource_group:  ]\nRefresh interval to re-read the instance list.\n[ refresh_interval:  | default = 300s ]\nThe port to scrape metrics from. If using the public IP address, this must\ninstead be specified in the relabeling rule.\n[ port:  | default = 80 ]\nAuthentication information used to authenticate to the Azure API.\nNote that `basic_auth`, `authorization` and `oauth2` options are\nmutually exclusive.\n`password` and `password_file` are mutually exclusive.\nOptional HTTP basic authentication information, currently not support by Azure.\nbasic_auth:\n  [ username:  ]\n  [ password:  ]\n  [ password_file:  ]\nOptional `Authorization` header configuration, currently not supported by Azure.\nauthorization:\n  # Sets the authentication type.\n  [ type:  | default: Bearer ]\n  # Sets the credentials. It is mutually exclusive with\n  # `credentials_file`.\n  [ credentials:  ]\n  # Sets the credentials to the credentials read from the configured file.\n  # It is mutually exclusive with `credentials`.\n  [ credentials_file:  ]\nOptional OAuth 2.0 configuration, currently not supported by Azure.\noauth2:\n  [  ]\nOptional proxy URL.\n[ proxy_url:  ]\nSpecifies headers to send to proxies during CONNECT requests.\n[ proxy_connect_header:\n  [ : [, ...] ] ]\nConfigure whether HTTP requests follow HTTP 3xx redirects.\n[ follow_redirects:  | default = true ]\nWhether to enable HTTP2.\n[ enable_http2:  | default: true ]\nTLS configuration.\ntls_config:\n  [  ]\n```\n`<consul_sd_config>`\nConsul SD configurations allow retrieving scrape targets from Consul's\nCatalog API.\nThe following meta labels are available on targets during relabeling:\n\n`__meta_consul_address`: the address of the target\n`__meta_consul_dc`: the datacenter name for the target\n`__meta_consul_health`: the health status of the service\n`__meta_consul_partition`: the admin partition name where the service is registered \n`__meta_consul_metadata_<key>`: each node metadata key value of the target\n`__meta_consul_node`: the node name defined for the target\n`__meta_consul_service_address`: the service address of the target\n`__meta_consul_service_id`: the service ID of the target\n`__meta_consul_service_metadata_<key>`: each service metadata key value of the target\n`__meta_consul_service_port`: the service port of the target\n`__meta_consul_service`: the name of the service the target belongs to\n`__meta_consul_tagged_address_<key>`: each node tagged address key value of the target\n`__meta_consul_tags`: the list of tags of the target joined by the tag separator\n\n```yaml\nThe information to access the Consul API. It is to be defined\nas the Consul documentation requires.\n[ server:  | default = \"localhost:8500\" ]\n[ token:  ]\n[ datacenter:  ]\nNamespaces are only supported in Consul Enterprise.\n[ namespace:  ]\nAdmin Partitions are only supported in Consul Enterprise.\n[ partition:  ]\n[ scheme:  | default = \"http\" ]\nThe username and password fields are deprecated in favor of the basic_auth configuration.\n[ username:  ]\n[ password:  ]\nA list of services for which targets are retrieved. If omitted, all services\nare scraped.\nservices:\n  [ -  ]\nSee https://www.consul.io/api/catalog.html#list-nodes-for-service to know more\nabout the possible filters that can be used.\nAn optional list of tags used to filter nodes for a given service. Services must contain all tags in the list.\ntags:\n  [ -  ]\nNode metadata key/value pairs to filter nodes for a given service.\n[ node_meta:\n  [ :  ... ] ]\nThe string by which Consul tags are joined into the tag label.\n[ tag_separator:  | default = , ]\nAllow stale Consul results (see https://www.consul.io/api/features/consistency.html). Will reduce load on Consul.\n[ allow_stale:  | default = true ]\nThe time after which the provided names are refreshed.\nOn large setup it might be a good idea to increase this value because the catalog will change all the time.\n[ refresh_interval:  | default = 30s ]\nAuthentication information used to authenticate to the consul server.\nNote that `basic_auth`, `authorization` and `oauth2` options are\nmutually exclusive.\n`password` and `password_file` are mutually exclusive.\nOptional HTTP basic authentication information.\nbasic_auth:\n  [ username:  ]\n  [ password:  ]\n  [ password_file:  ]\nOptional `Authorization` header configuration.\nauthorization:\n  # Sets the authentication type.\n  [ type:  | default: Bearer ]\n  # Sets the credentials. It is mutually exclusive with\n  # `credentials_file`.\n  [ credentials:  ]\n  # Sets the credentials to the credentials read from the configured file.\n  # It is mutually exclusive with `credentials`.\n  [ credentials_file:  ]\nOptional OAuth 2.0 configuration.\noauth2:\n  [  ]\nOptional proxy URL.\n[ proxy_url:  ]\nSpecifies headers to send to proxies during CONNECT requests.\n[ proxy_connect_header:\n  [ : [, ...] ] ]\nConfigure whether HTTP requests follow HTTP 3xx redirects.\n[ follow_redirects:  | default = true ]\nWhether to enable HTTP2.\n[ enable_http2:  | default: true ]\nTLS configuration.\ntls_config:\n  [  ]\n```\nNote that the IP number and port used to scrape the targets is assembled as\n`<__meta_consul_address>:<__meta_consul_service_port>`. However, in some\nConsul setups, the relevant address is in `__meta_consul_service_address`.\nIn those cases, you can use the relabel\nfeature to replace the special `__address__` label.\nThe relabeling phase is the preferred and more powerful\nway to filter services or nodes for a service based on arbitrary labels. For\nusers with thousands of services it can be more efficient to use the Consul API\ndirectly which has basic support for filtering nodes (currently by node\nmetadata and a single tag).\n`<digitalocean_sd_config>`\nDigitalOcean SD configurations allow retrieving scrape targets from DigitalOcean's\nDroplets API.\nThis service discovery uses the public IPv4 address by default, by that can be\nchanged with relabeling, as demonstrated in the Prometheus digitalocean-sd\nconfiguration file.\nThe following meta labels are available on targets during relabeling:\n\n`__meta_digitalocean_droplet_id`: the id of the droplet\n`__meta_digitalocean_droplet_name`: the name of the droplet\n`__meta_digitalocean_image`: the slug of the droplet's image\n`__meta_digitalocean_image_name`: the display name of the droplet's image\n`__meta_digitalocean_private_ipv4`: the private IPv4 of the droplet\n`__meta_digitalocean_public_ipv4`: the public IPv4 of the droplet\n`__meta_digitalocean_public_ipv6`: the public IPv6 of the droplet\n`__meta_digitalocean_region`: the region of the droplet\n`__meta_digitalocean_size`: the size of the droplet\n`__meta_digitalocean_status`: the status of the droplet\n`__meta_digitalocean_features`: the comma-separated list of features of the droplet\n`__meta_digitalocean_tags`: the comma-separated list of tags of the droplet\n`__meta_digitalocean_vpc`: the id of the droplet's VPC\n\n```yaml\nAuthentication information used to authenticate to the API server.\nNote that `basic_auth` and `authorization` options are\nmutually exclusive.\npassword and password_file are mutually exclusive.\nOptional HTTP basic authentication information, not currently supported by DigitalOcean.\nbasic_auth:\n  [ username:  ]\n  [ password:  ]\n  [ password_file:  ]\nOptional `Authorization` header configuration.\nauthorization:\n  # Sets the authentication type.\n  [ type:  | default: Bearer ]\n  # Sets the credentials. It is mutually exclusive with\n  # `credentials_file`.\n  [ credentials:  ]\n  # Sets the credentials to the credentials read from the configured file.\n  # It is mutually exclusive with `credentials`.\n  [ credentials_file:  ]\nOptional OAuth 2.0 configuration.\nCannot be used at the same time as basic_auth or authorization.\noauth2:\n  [  ]\nOptional proxy URL.\n[ proxy_url:  ]\nSpecifies headers to send to proxies during CONNECT requests.\n[ proxy_connect_header:\n  [ : [, ...] ] ]\nConfigure whether HTTP requests follow HTTP 3xx redirects.\n[ follow_redirects:  | default = true ]\nWhether to enable HTTP2.\n[ enable_http2:  | default: true ]\nTLS configuration.\ntls_config:\n  [  ]\nThe port to scrape metrics from.\n[ port:  | default = 80 ]\nThe time after which the droplets are refreshed.\n[ refresh_interval:  | default = 60s ]\n```\n`<docker_sd_config>`\nDocker SD configurations allow retrieving scrape targets from Docker Engine hosts.\nThis SD discovers \"containers\" and will create a target for each network IP and port the container is configured to expose.\nAvailable meta labels:\n\n`__meta_docker_container_id`: the id of the container\n`__meta_docker_container_name`: the name of the container\n`__meta_docker_container_network_mode`: the network mode of the container\n`__meta_docker_container_label_<labelname>`: each label of the container\n`__meta_docker_network_id`: the ID of the network\n`__meta_docker_network_name`: the name of the network\n`__meta_docker_network_ingress`: whether the network is ingress\n`__meta_docker_network_internal`: whether the network is internal\n`__meta_docker_network_label_<labelname>`: each label of the network\n`__meta_docker_network_scope`: the scope of the network\n`__meta_docker_network_ip`: the IP of the container in this network\n`__meta_docker_port_private`: the port on the container\n`__meta_docker_port_public`: the external port if a port-mapping exists\n`__meta_docker_port_public_ip`: the public IP if a port-mapping exists\n\nSee below for the configuration options for Docker discovery:\n```yaml\nAddress of the Docker daemon.\nhost: \nOptional proxy URL.\n[ proxy_url:  ]\nSpecifies headers to send to proxies during CONNECT requests.\n[ proxy_connect_header:\n  [ : [, ...] ] ]\nTLS configuration.\ntls_config:\n  [  ]\nThe port to scrape metrics from, when `role` is nodes, and for discovered\ntasks and services that don't have published ports.\n[ port:  | default = 80 ]\nThe host to use if the container is in host networking mode.\n[ host_networking_host:  | default = \"localhost\" ]\nOptional filters to limit the discovery process to a subset of available\nresources.\nThe available filters are listed in the upstream documentation:\nhttps://docs.docker.com/engine/api/v1.40/#operation/ContainerList\n[ filters:\n  [ - name: \n      values: , [...] ]\nThe time after which the containers are refreshed.\n[ refresh_interval:  | default = 60s ]\nAuthentication information used to authenticate to the Docker daemon.\nNote that `basic_auth` and `authorization` options are\nmutually exclusive.\npassword and password_file are mutually exclusive.\nOptional HTTP basic authentication information.\nbasic_auth:\n  [ username:  ]\n  [ password:  ]\n  [ password_file:  ]\nOptional `Authorization` header configuration.\nauthorization:\n  # Sets the authentication type.\n  [ type:  | default: Bearer ]\n  # Sets the credentials. It is mutually exclusive with\n  # `credentials_file`.\n  [ credentials:  ]\n  # Sets the credentials to the credentials read from the configured file.\n  # It is mutually exclusive with `credentials`.\n  [ credentials_file:  ]\nOptional OAuth 2.0 configuration.\nCannot be used at the same time as basic_auth or authorization.\noauth2:\n  [  ]\nConfigure whether HTTP requests follow HTTP 3xx redirects.\n[ follow_redirects:  | default = true ]\nWhether to enable HTTP2.\n[ enable_http2:  | default: true ]\n```\nThe relabeling phase is the preferred and more powerful\nway to filter containers. For users with thousands of containers it\ncan be more efficient to use the Docker API directly which has basic support for\nfiltering containers (using `filters`).\nSee this example Prometheus configuration file\nfor a detailed example of configuring Prometheus for Docker Engine.\n`<dockerswarm_sd_config>`\nDocker Swarm SD configurations allow retrieving scrape targets from Docker Swarm\nengine.\nOne of the following roles can be configured to discover targets:\n`services`\nThe `services` role discovers all Swarm services\nand exposes their ports as targets. For each published port of a service, a\nsingle target is generated. If a service has no published ports, a target per\nservice is created using the `port` parameter defined in the SD configuration.\nAvailable meta labels:\n\n`__meta_dockerswarm_service_id`: the id of the service\n`__meta_dockerswarm_service_name`: the name of the service\n`__meta_dockerswarm_service_mode`: the mode of the service\n`__meta_dockerswarm_service_endpoint_port_name`: the name of the endpoint port, if available\n`__meta_dockerswarm_service_endpoint_port_publish_mode`: the publish mode of the endpoint port\n`__meta_dockerswarm_service_label_<labelname>`: each label of the service\n`__meta_dockerswarm_service_task_container_hostname`: the container hostname of the target, if available\n`__meta_dockerswarm_service_task_container_image`: the container image of the target\n`__meta_dockerswarm_service_updating_status`: the status of the service, if available\n`__meta_dockerswarm_network_id`: the ID of the network\n`__meta_dockerswarm_network_name`: the name of the network\n`__meta_dockerswarm_network_ingress`: whether the network is ingress\n`__meta_dockerswarm_network_internal`: whether the network is internal\n`__meta_dockerswarm_network_label_<labelname>`: each label of the network\n`__meta_dockerswarm_network_scope`: the scope of the network\n\n`tasks`\nThe `tasks` role discovers all Swarm tasks\nand exposes their ports as targets. For each published port of a task, a single\ntarget is generated. If a task has no published ports, a target per task is\ncreated using the `port` parameter defined in the SD configuration.\nAvailable meta labels:\n\n`__meta_dockerswarm_container_label_<labelname>`: each label of the container\n`__meta_dockerswarm_task_id`: the id of the task\n`__meta_dockerswarm_task_container_id`: the container id of the task\n`__meta_dockerswarm_task_desired_state`: the desired state of the task\n`__meta_dockerswarm_task_slot`: the slot of the task\n`__meta_dockerswarm_task_state`: the state of the task\n`__meta_dockerswarm_task_port_publish_mode`: the publish mode of the task port\n`__meta_dockerswarm_service_id`: the id of the service\n`__meta_dockerswarm_service_name`: the name of the service\n`__meta_dockerswarm_service_mode`: the mode of the service\n`__meta_dockerswarm_service_label_<labelname>`: each label of the service\n`__meta_dockerswarm_network_id`: the ID of the network\n`__meta_dockerswarm_network_name`: the name of the network\n`__meta_dockerswarm_network_ingress`: whether the network is ingress\n`__meta_dockerswarm_network_internal`: whether the network is internal\n`__meta_dockerswarm_network_label_<labelname>`: each label of the network\n`__meta_dockerswarm_network_label`: each label of the network\n`__meta_dockerswarm_network_scope`: the scope of the network\n`__meta_dockerswarm_node_id`: the ID of the node\n`__meta_dockerswarm_node_hostname`: the hostname of the node\n`__meta_dockerswarm_node_address`: the address of the node\n`__meta_dockerswarm_node_availability`: the availability of the node\n`__meta_dockerswarm_node_label_<labelname>`: each label of the node\n`__meta_dockerswarm_node_platform_architecture`: the architecture of the node\n`__meta_dockerswarm_node_platform_os`: the operating system of the node\n`__meta_dockerswarm_node_role`: the role of the node\n`__meta_dockerswarm_node_status`: the status of the node\n\nThe `__meta_dockerswarm_network_*` meta labels are not populated for ports which\nare published with `mode=host`.\n`nodes`\nThe `nodes` role is used to discover Swarm nodes.\nAvailable meta labels:\n\n`__meta_dockerswarm_node_address`: the address of the node\n`__meta_dockerswarm_node_availability`: the availability of the node\n`__meta_dockerswarm_node_engine_version`: the version of the node engine\n`__meta_dockerswarm_node_hostname`: the hostname of the node\n`__meta_dockerswarm_node_id`: the ID of the node\n`__meta_dockerswarm_node_label_<labelname>`: each label of the node\n`__meta_dockerswarm_node_manager_address`: the address of the manager component of the node\n`__meta_dockerswarm_node_manager_leader`: the leadership status of the manager component of the node (true or false)\n`__meta_dockerswarm_node_manager_reachability`: the reachability of the manager component of the node\n`__meta_dockerswarm_node_platform_architecture`: the architecture of the node\n`__meta_dockerswarm_node_platform_os`: the operating system of the node\n`__meta_dockerswarm_node_role`: the role of the node\n`__meta_dockerswarm_node_status`: the status of the node\n\nSee below for the configuration options for Docker Swarm discovery:\n```yaml\nAddress of the Docker daemon.\nhost: \nOptional proxy URL.\n[ proxy_url:  ]\nSpecifies headers to send to proxies during CONNECT requests.\n[ proxy_connect_header:\n  [ : [, ...] ] ]\nTLS configuration.\ntls_config:\n  [  ]\nRole of the targets to retrieve. Must be `services`, `tasks`, or `nodes`.\nrole: \nThe port to scrape metrics from, when `role` is nodes, and for discovered\ntasks and services that don't have published ports.\n[ port:  | default = 80 ]\nOptional filters to limit the discovery process to a subset of available\nresources.\nThe available filters are listed in the upstream documentation:\nServices: https://docs.docker.com/engine/api/v1.40/#operation/ServiceList\nTasks: https://docs.docker.com/engine/api/v1.40/#operation/TaskList\nNodes: https://docs.docker.com/engine/api/v1.40/#operation/NodeList\n[ filters:\n  [ - name: \n      values: , [...] ]\nThe time after which the service discovery data is refreshed.\n[ refresh_interval:  | default = 60s ]\nAuthentication information used to authenticate to the Docker daemon.\nNote that `basic_auth` and `authorization` options are\nmutually exclusive.\npassword and password_file are mutually exclusive.\nOptional HTTP basic authentication information.\nbasic_auth:\n  [ username:  ]\n  [ password:  ]\n  [ password_file:  ]\nOptional `Authorization` header configuration.\nauthorization:\n  # Sets the authentication type.\n  [ type:  | default: Bearer ]\n  # Sets the credentials. It is mutually exclusive with\n  # `credentials_file`.\n  [ credentials:  ]\n  # Sets the credentials to the credentials read from the configured file.\n  # It is mutually exclusive with `credentials`.\n  [ credentials_file:  ]\nOptional OAuth 2.0 configuration.\nCannot be used at the same time as basic_auth or authorization.\noauth2:\n  [  ]\nConfigure whether HTTP requests follow HTTP 3xx redirects.\n[ follow_redirects:  | default = true ]\nWhether to enable HTTP2.\n[ enable_http2:  | default: true ]\n```\nThe relabeling phase is the preferred and more powerful\nway to filter tasks, services or nodes. For users with thousands of tasks it\ncan be more efficient to use the Swarm API directly which has basic support for\nfiltering nodes (using `filters`).\nSee this example Prometheus configuration file\nfor a detailed example of configuring Prometheus for Docker Swarm.\n`<dns_sd_config>`\nA DNS-based service discovery configuration allows specifying a set of DNS\ndomain names which are periodically queried to discover a list of targets. The\nDNS servers to be contacted are read from `/etc/resolv.conf`.\nThis service discovery method only supports basic DNS A, AAAA, MX and SRV\nrecord queries, but not the advanced DNS-SD approach specified in\nRFC6763.\nThe following meta labels are available on targets during relabeling:\n\n`__meta_dns_name`: the record name that produced the discovered target.\n`__meta_dns_srv_record_target`: the target field of the SRV record\n`__meta_dns_srv_record_port`: the port field of the SRV record\n`__meta_dns_mx_record_target`: the target field of the MX record\n\n```yaml\nA list of DNS domain names to be queried.\nnames:\n  [ -  ]\nThe type of DNS query to perform. One of SRV, A, AAAA or MX.\n[ type:  | default = 'SRV' ]\nThe port number used if the query type is not SRV.\n[ port: ]\nThe time after which the provided names are refreshed.\n[ refresh_interval:  | default = 30s ]\n```\n`<ec2_sd_config>`\nEC2 SD configurations allow retrieving scrape targets from AWS EC2\ninstances. The private IP address is used by default, but may be changed to\nthe public IP address with relabeling.\nThe IAM credentials used must have the `ec2:DescribeInstances` permission to\ndiscover scrape targets, and may optionally have the\n`ec2:DescribeAvailabilityZones` permission if you want the availability zone ID\navailable as a label (see below).\nThe following meta labels are available on targets during relabeling:\n\n`__meta_ec2_ami`: the EC2 Amazon Machine Image\n`__meta_ec2_architecture`: the architecture of the instance\n`__meta_ec2_availability_zone`: the availability zone in which the instance is running\n`__meta_ec2_availability_zone_id`: the availability zone ID in which the instance is running (requires `ec2:DescribeAvailabilityZones`)\n`__meta_ec2_instance_id`: the EC2 instance ID\n`__meta_ec2_instance_lifecycle`: the lifecycle of the EC2 instance, set only for 'spot' or 'scheduled' instances, absent otherwise\n`__meta_ec2_instance_state`: the state of the EC2 instance\n`__meta_ec2_instance_type`: the type of the EC2 instance\n`__meta_ec2_ipv6_addresses`: comma separated list of IPv6 addresses assigned to the instance's network interfaces, if present\n`__meta_ec2_owner_id`: the ID of the AWS account that owns the EC2 instance\n`__meta_ec2_platform`: the Operating System platform, set to 'windows' on Windows servers, absent otherwise\n`__meta_ec2_primary_subnet_id`: the subnet ID of the primary network interface, if available\n`__meta_ec2_private_dns_name`: the private DNS name of the instance, if available\n`__meta_ec2_private_ip`: the private IP address of the instance, if present\n`__meta_ec2_public_dns_name`: the public DNS name of the instance, if available\n`__meta_ec2_public_ip`: the public IP address of the instance, if available\n`__meta_ec2_region`: the region of the instance\n`__meta_ec2_subnet_id`: comma separated list of subnets IDs in which the instance is running, if available\n`__meta_ec2_tag_<tagkey>`: each tag value of the instance\n`__meta_ec2_vpc_id`: the ID of the VPC in which the instance is running, if available\n\nSee below for the configuration options for EC2 discovery:\n```yaml\nThe information to access the EC2 API.\nThe AWS region. If blank, the region from the instance metadata is used.\n[ region:  ]\nCustom endpoint to be used.\n[ endpoint:  ]\nThe AWS API keys. If blank, the environment variables `AWS_ACCESS_KEY_ID`\nand `AWS_SECRET_ACCESS_KEY` are used.\n[ access_key:  ]\n[ secret_key:  ]\nNamed AWS profile used to connect to the API.\n[ profile:  ]\nAWS Role ARN, an alternative to using AWS API keys.\n[ role_arn:  ]\nRefresh interval to re-read the instance list.\n[ refresh_interval:  | default = 60s ]\nThe port to scrape metrics from. If using the public IP address, this must\ninstead be specified in the relabeling rule.\n[ port:  | default = 80 ]\nFilters can be used optionally to filter the instance list by other criteria.\nAvailable filter criteria can be found here:\nhttps://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeInstances.html\nFilter API documentation: https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_Filter.html\nfilters:\n  [ - name: \n      values: , [...] ]\nAuthentication information used to authenticate to the EC2 API.\nNote that `basic_auth`, `authorization` and `oauth2` options are\nmutually exclusive.\n`password` and `password_file` are mutually exclusive.\nOptional HTTP basic authentication information, currently not supported by AWS.\nbasic_auth:\n  [ username:  ]\n  [ password:  ]\n  [ password_file:  ]\nOptional `Authorization` header configuration, currently not supported by AWS.\nauthorization:\n  # Sets the authentication type.\n  [ type:  | default: Bearer ]\n  # Sets the credentials. It is mutually exclusive with\n  # `credentials_file`.\n  [ credentials:  ]\n  # Sets the credentials to the credentials read from the configured file.\n  # It is mutuall exclusive with `credentials`.\n  [ credentials_file:  ]\nOptional OAuth 2.0 configuration, currently not supported by AWS.\noauth2:\n  [  ]\nOptional proxy URL.\n[ proxy_url:  ]\nSpecifies headers to send to proxies during CONNECT requests.\n[ proxy_connect_header:\n  [ : [, ...] ] ]\nConfigure whether HTTP requests follow HTTP 3xx redirects.\n[ follow_redirects:  | default = true ]\nWhether to enable HTTP2.\n[ enable_http2:  | default: true ]\nTLS configuration.\ntls_config:\n  [  ]\n```\nThe relabeling phase is the preferred and more powerful\nway to filter targets based on arbitrary labels. For users with thousands of\ninstances it can be more efficient to use the EC2 API directly which has\nsupport for filtering instances.\n`<openstack_sd_config>`\nOpenStack SD configurations allow retrieving scrape targets from OpenStack Nova\ninstances.\nOne of the following `<openstack_role>` types can be configured to discover targets:\n`hypervisor`\nThe `hypervisor` role discovers one target per Nova hypervisor node. The target\naddress defaults to the `host_ip` attribute of the hypervisor.\nThe following meta labels are available on targets during relabeling:\n\n`__meta_openstack_hypervisor_host_ip`: the hypervisor node's IP address.\n`__meta_openstack_hypervisor_hostname`: the hypervisor node's name.\n`__meta_openstack_hypervisor_id`: the hypervisor node's ID.\n`__meta_openstack_hypervisor_state`: the hypervisor node's state.\n`__meta_openstack_hypervisor_status`: the hypervisor node's status.\n`__meta_openstack_hypervisor_type`: the hypervisor node's type.\n\n`instance`\nThe `instance` role discovers one target per network interface of Nova\ninstance. The target address defaults to the private IP address of the network\ninterface.\nThe following meta labels are available on targets during relabeling:\n\n`__meta_openstack_address_pool`: the pool of the private IP.\n`__meta_openstack_instance_flavor`: the flavor of the OpenStack instance.\n`__meta_openstack_instance_id`: the OpenStack instance ID.\n`__meta_openstack_instance_name`: the OpenStack instance name.\n`__meta_openstack_instance_status`: the status of the OpenStack instance.\n`__meta_openstack_private_ip`: the private IP of the OpenStack instance.\n`__meta_openstack_project_id`: the project (tenant) owning this instance.\n`__meta_openstack_public_ip`: the public IP of the OpenStack instance.\n`__meta_openstack_tag_<tagkey>`: each tag value of the instance.\n`__meta_openstack_user_id`: the user account owning the tenant.\n\nSee below for the configuration options for OpenStack discovery:\n```yaml\nThe information to access the OpenStack API.\nThe OpenStack role of entities that should be discovered.\nrole: \nThe OpenStack Region.\nregion: \nidentity_endpoint specifies the HTTP endpoint that is required to work with\nthe Identity API of the appropriate version. While it's ultimately needed by\nall of the identity services, it will often be populated by a provider-level\nfunction.\n[ identity_endpoint:  ]\nusername is required if using Identity V2 API. Consult with your provider's\ncontrol panel to discover your account's username. In Identity V3, either\nuserid or a combination of username and domain_id or domain_name are needed.\n[ username:  ]\n[ userid:  ]\npassword for the Identity V2 and V3 APIs. Consult with your provider's\ncontrol panel to discover your account's preferred method of authentication.\n[ password:  ]\nAt most one of domain_id and domain_name must be provided if using username\nwith Identity V3. Otherwise, either are optional.\n[ domain_name:  ]\n[ domain_id:  ]\nThe project_id and project_name fields are optional for the Identity V2 API.\nSome providers allow you to specify a project_name instead of the project_id.\nSome require both. Your provider's authentication policies will determine\nhow these fields influence authentication.\n[ project_name:  ]\n[ project_id:  ]\nThe application_credential_id or application_credential_name fields are\nrequired if using an application credential to authenticate. Some providers\nallow you to create an application credential to authenticate rather than a\npassword.\n[ application_credential_name:  ]\n[ application_credential_id:  ]\nThe application_credential_secret field is required if using an application\ncredential to authenticate.\n[ application_credential_secret:  ]\nWhether the service discovery should list all instances for all projects.\nIt is only relevant for the 'instance' role and usually requires admin permissions.\n[ all_tenants:  | default: false ]\nRefresh interval to re-read the instance list.\n[ refresh_interval:  | default = 60s ]\nThe port to scrape metrics from. If using the public IP address, this must\ninstead be specified in the relabeling rule.\n[ port:  | default = 80 ]\nThe availability of the endpoint to connect to. Must be one of public, admin or internal.\n[ availability:  | default = \"public\" ]\nTLS configuration.\ntls_config:\n  [  ]\n```\n`<ovhcloud_sd_config>`\nOVHcloud SD configurations allow retrieving scrape targets from OVHcloud's dedicated servers and VPS using\ntheir API.\nPrometheus will periodically check the REST endpoint and create a target for every discovered server.\nThe role will try to use the public IPv4 address as default address, if there's none it will try to use the IPv6 one. This may be changed with relabeling.\nFor OVHcloud's public cloud instances you can use the openstack_sd_config.\nVPS\n\n`__meta_ovhcloud_vps_cluster`: the cluster of the server\n`__meta_ovhcloud_vps_datacenter`: the datacenter of the server\n`__meta_ovhcloud_vps_disk`: the disk of the server\n`__meta_ovhcloud_vps_display_name`: the display name of the server\n`__meta_ovhcloud_vps_ipv4`: the IPv4 of the server\n`__meta_ovhcloud_vps_ipv6`: the IPv6 of the server\n`__meta_ovhcloud_vps_keymap`: the KVM keyboard layout of the server\n`__meta_ovhcloud_vps_maximum_additional_ip`: the maximum additional IPs of the server\n`__meta_ovhcloud_vps_memory_limit`: the memory limit of the server\n`__meta_ovhcloud_vps_memory`: the memory of the server\n`__meta_ovhcloud_vps_monitoring_ip_blocks`: the monitoring IP blocks of the server\n`__meta_ovhcloud_vps_name`: the name of the server\n`__meta_ovhcloud_vps_netboot_mode`: the netboot mode of the server\n`__meta_ovhcloud_vps_offer_type`: the offer type of the server\n`__meta_ovhcloud_vps_offer`: the offer of the server\n`__meta_ovhcloud_vps_state`: the state of the server\n`__meta_ovhcloud_vps_vcore`: the number of virtual cores of the server\n`__meta_ovhcloud_vps_version`: the version of the server\n`__meta_ovhcloud_vps_zone`: the zone of the server\n\nDedicated servers\n\n`__meta_ovhcloud_dedicated_server_commercial_range`: the commercial range of the server\n`__meta_ovhcloud_dedicated_server_datacenter`: the datacenter of the server\n`__meta_ovhcloud_dedicated_server_ipv4`: the IPv4 of the server\n`__meta_ovhcloud_dedicated_server_ipv6`: the IPv6 of the server\n`__meta_ovhcloud_dedicated_server_link_speed`: the link speed of the server\n`__meta_ovhcloud_dedicated_server_name`: the name of the server\n`__meta_ovhcloud_dedicated_server_os`: the operating system of the server\n`__meta_ovhcloud_dedicated_server_rack`: the rack of the server\n`__meta_ovhcloud_dedicated_server_reverse`: the reverse DNS name of the server\n`__meta_ovhcloud_dedicated_server_server_id`: the ID of the server\n`__meta_ovhcloud_dedicated_server_state`: the state of the server\n`__meta_ovhcloud_dedicated_server_support_level`: the support level of the server\n\nSee below for the configuration options for OVHcloud discovery:\n```yaml\nAccess key to use. https://api.ovh.com\napplication_key: \napplication_secret: \nconsumer_key: \nService of the targets to retrieve. Must be `vps` or `dedicated_server`.\nservice: \nAPI endpoint. https://github.com/ovh/go-ovh#supported-apis\n[ endpoint:  | default = \"ovh-eu\" ]\nRefresh interval to re-read the resources list.\n[ refresh_interval:  | default = 60s ]\n```\n`<puppetdb_sd_config>`\nPuppetDB SD configurations allow retrieving scrape targets from\nPuppetDB resources.\nThis SD discovers resources and will create a target for each resource returned\nby the API.\nThe resource address is the `certname` of the resource and can be changed during\nrelabeling.\nThe following meta labels are available on targets during relabeling:\n\n`__meta_puppetdb_query`: the Puppet Query Language (PQL) query\n`__meta_puppetdb_certname`: the name of the node associated with the resource\n`__meta_puppetdb_resource`: a SHA-1 hash of the resource\u2019s type, title, and parameters, for identification\n`__meta_puppetdb_type`: the resource type\n`__meta_puppetdb_title`: the resource title\n`__meta_puppetdb_exported`: whether the resource is exported (`\"true\"` or `\"false\"`)\n`__meta_puppetdb_tags`: comma separated list of resource tags\n`__meta_puppetdb_file`: the manifest file in which the resource was declared\n`__meta_puppetdb_environment`: the environment of the node associated with the resource\n`__meta_puppetdb_parameter_<parametername>`: the parameters of the resource\n\nSee below for the configuration options for PuppetDB discovery:\n```yaml\nThe URL of the PuppetDB root query endpoint.\nurl: \nPuppet Query Language (PQL) query. Only resources are supported.\nhttps://puppet.com/docs/puppetdb/latest/api/query/v4/pql.html\nquery: \nWhether to include the parameters as meta labels.\nDue to the differences between parameter types and Prometheus labels,\nsome parameters might not be rendered. The format of the parameters might\nalso change in future releases.\n\nNote: Enabling this exposes parameters in the Prometheus UI and API. Make sure\nthat you don't have secrets exposed as parameters if you enable this.\n[ include_parameters:  | default = false ]\nRefresh interval to re-read the resources list.\n[ refresh_interval:  | default = 60s ]\nThe port to scrape metrics from.\n[ port:  | default = 80 ]\nTLS configuration to connect to the PuppetDB.\ntls_config:\n  [  ]\nbasic_auth, authorization, and oauth2, are mutually exclusive.\nOptional HTTP basic authentication information.\nbasic_auth:\n  [ username:  ]\n  [ password:  ]\n  [ password_file:  ]\n`Authorization` HTTP header configuration.\nauthorization:\n  # Sets the authentication type.\n  [ type:  | default: Bearer ]\n  # Sets the credentials. It is mutually exclusive with\n  # `credentials_file`.\n  [ credentials:  ]\n  # Sets the credentials with the credentials read from the configured file.\n  # It is mutually exclusive with `credentials`.\n  [ credentials_file:  ]\nOptional OAuth 2.0 configuration.\nCannot be used at the same time as basic_auth or authorization.\noauth2:\n  [  ]\nOptional proxy URL.\n[ proxy_url:  ]\nSpecifies headers to send to proxies during CONNECT requests.\n[ proxy_connect_header:\n  [ : [, ...] ] ]\nConfigure whether HTTP requests follow HTTP 3xx redirects.\n[ follow_redirects:  | default = true ]\nWhether to enable HTTP2.\n[ enable_http2:  | default: true ]\n```\nSee this example Prometheus configuration file\nfor a detailed example of configuring Prometheus with PuppetDB.\n`<file_sd_config>`\nFile-based service discovery provides a more generic way to configure static targets\nand serves as an interface to plug in custom service discovery mechanisms.\nIt reads a set of files containing a list of zero or more\n`<static_config>`s. Changes to all defined files are detected via disk watches\nand applied immediately. Files may be provided in YAML or JSON format. Only\nchanges resulting in well-formed target groups are applied.\nFiles must contain a list of static configs, using these formats:\nJSON\n`json\n[\n  {\n    \"targets\": [ \"<host>\", ... ],\n    \"labels\": {\n      \"<labelname>\": \"<labelvalue>\", ...\n    }\n  },\n  ...\n]`\nYAML\n`yaml\n- targets:\n  [ - '<host>' ]\n  labels:\n    [ <labelname>: <labelvalue> ... ]`\nAs a fallback, the file contents are also re-read periodically at the specified\nrefresh interval.\nEach target has a meta label `__meta_filepath` during the\nrelabeling phase. Its value is set to the\nfilepath from which the target was extracted.\nThere is a list of\nintegrations with this\ndiscovery mechanism.\n```yaml\nPatterns for files from which target groups are extracted.\nfiles:\n  [ -  ... ]\nRefresh interval to re-read the files.\n[ refresh_interval:  | default = 5m ]\n```\nWhere `<filename_pattern>` may be a path ending in `.json`, `.yml` or `.yaml`. The last path segment\nmay contain a single `*` that matches any character sequence, e.g. `my/path/tg_*.json`.\n`<gce_sd_config>`\nGCE SD configurations allow retrieving scrape targets from GCP GCE instances.\nThe private IP address is used by default, but may be changed to the public IP\naddress with relabeling.\nThe following meta labels are available on targets during relabeling:\n\n`__meta_gce_instance_id`: the numeric id of the instance\n`__meta_gce_instance_name`: the name of the instance\n`__meta_gce_label_<labelname>`: each GCE label of the instance\n`__meta_gce_machine_type`: full or partial URL of the machine type of the instance\n`__meta_gce_metadata_<name>`: each metadata item of the instance\n`__meta_gce_network`: the network URL of the instance\n`__meta_gce_private_ip`: the private IP address of the instance\n`__meta_gce_interface_ipv4_<name>`: IPv4 address of each named interface\n`__meta_gce_project`: the GCP project in which the instance is running\n`__meta_gce_public_ip`: the public IP address of the instance, if present\n`__meta_gce_subnetwork`: the subnetwork URL of the instance\n`__meta_gce_tags`: comma separated list of instance tags\n`__meta_gce_zone`: the GCE zone URL in which the instance is running\n\nSee below for the configuration options for GCE discovery:\n```yaml\nThe information to access the GCE API.\nThe GCP Project\nproject: \nThe zone of the scrape targets. If you need multiple zones use multiple\ngce_sd_configs.\nzone: \nFilter can be used optionally to filter the instance list by other criteria\nSyntax of this filter string is described here in the filter query parameter section:\nhttps://cloud.google.com/compute/docs/reference/latest/instances/list\n[ filter:  ]\nRefresh interval to re-read the instance list\n[ refresh_interval:  | default = 60s ]\nThe port to scrape metrics from. If using the public IP address, this must\ninstead be specified in the relabeling rule.\n[ port:  | default = 80 ]\nThe tag separator is used to separate the tags on concatenation\n[ tag_separator:  | default = , ]\n```\nCredentials are discovered by the Google Cloud SDK default client by looking\nin the following places, preferring the first location found:\n\na JSON file specified by the `GOOGLE_APPLICATION_CREDENTIALS` environment variable\na JSON file in the well-known path `$HOME/.config/gcloud/application_default_credentials.json`\nfetched from the GCE metadata server\n\nIf Prometheus is running within GCE, the service account associated with the\ninstance it is running on should have at least read-only permissions to the\ncompute resources. If running outside of GCE make sure to create an appropriate\nservice account and place the credential file in one of the expected locations.\n`<hetzner_sd_config>`\nHetzner SD configurations allow retrieving scrape targets from\nHetzner Cloud API and\nRobot API.\nThis service discovery uses the public IPv4 address by default, but that can be\nchanged with relabeling, as demonstrated in the Prometheus hetzner-sd\nconfiguration file.\nThe following meta labels are available on all targets during relabeling:\n\n`__meta_hetzner_server_id`: the ID of the server\n`__meta_hetzner_server_name`: the name of the server\n`__meta_hetzner_server_status`: the status of the server\n`__meta_hetzner_public_ipv4`: the public ipv4 address of the server\n`__meta_hetzner_public_ipv6_network`: the public ipv6 network (/64) of the server\n`__meta_hetzner_datacenter`: the datacenter of the server\n\nThe labels below are only available for targets with `role` set to `hcloud`:\n\n`__meta_hetzner_hcloud_image_name`: the image name of the server\n`__meta_hetzner_hcloud_image_description`: the description of the server image\n`__meta_hetzner_hcloud_image_os_flavor`: the OS flavor of the server image\n`__meta_hetzner_hcloud_image_os_version`: the OS version of the server image\n`__meta_hetzner_hcloud_datacenter_location`: the location of the server\n`__meta_hetzner_hcloud_datacenter_location_network_zone`: the network zone of the server\n`__meta_hetzner_hcloud_server_type`: the type of the server\n`__meta_hetzner_hcloud_cpu_cores`: the CPU cores count of the server\n`__meta_hetzner_hcloud_cpu_type`: the CPU type of the server (shared or dedicated)\n`__meta_hetzner_hcloud_memory_size_gb`: the amount of memory of the server (in GB)\n`__meta_hetzner_hcloud_disk_size_gb`: the disk size of the server (in GB)\n`__meta_hetzner_hcloud_private_ipv4_<networkname>`: the private ipv4 address of the server within a given network\n`__meta_hetzner_hcloud_label_<labelname>`: each label of the server\n`__meta_hetzner_hcloud_labelpresent_<labelname>`: `true` for each label of the server\n\nThe labels below are only available for targets with `role` set to `robot`:\n\n`__meta_hetzner_robot_product`: the product of the server\n`__meta_hetzner_robot_cancelled`: the server cancellation status\n\n```yaml\nThe Hetzner role of entities that should be discovered.\nOne of robot or hcloud.\nrole: \nAuthentication information used to authenticate to the API server.\nNote that `basic_auth` and `authorization` options are\nmutually exclusive.\npassword and password_file are mutually exclusive.\nOptional HTTP basic authentication information, required when role is robot\nRole hcloud does not support basic auth.\nbasic_auth:\n  [ username:  ]\n  [ password:  ]\n  [ password_file:  ]\nOptional `Authorization` header configuration, required when role is\nhcloud. Role robot does not support bearer token authentication.\nauthorization:\n  # Sets the authentication type.\n  [ type:  | default: Bearer ]\n  # Sets the credentials. It is mutually exclusive with\n  # `credentials_file`.\n  [ credentials:  ]\n  # Sets the credentials to the credentials read from the configured file.\n  # It is mutually exclusive with `credentials`.\n  [ credentials_file:  ]\nOptional OAuth 2.0 configuration.\nCannot be used at the same time as basic_auth or authorization.\noauth2:\n  [  ]\nOptional proxy URL.\n[ proxy_url:  ]\nSpecifies headers to send to proxies during CONNECT requests.\n[ proxy_connect_header:\n  [ : [, ...] ] ]\nConfigure whether HTTP requests follow HTTP 3xx redirects.\n[ follow_redirects:  | default = true ]\nWhether to enable HTTP2.\n[ enable_http2:  | default: true ]\nTLS configuration.\ntls_config:\n  [  ]\nThe port to scrape metrics from.\n[ port:  | default = 80 ]\nThe time after which the servers are refreshed.\n[ refresh_interval:  | default = 60s ]\n```\n`<http_sd_config>`\nHTTP-based service discovery provides a more generic way to configure static targets\nand serves as an interface to plug in custom service discovery mechanisms.\nIt fetches targets from an HTTP endpoint containing a list of zero or more\n`<static_config>`s. The target must reply with an HTTP 200 response.\nThe HTTP header `Content-Type` must be `application/json`, and the body must be\nvalid JSON.\nExample response body:\n`json\n[\n  {\n    \"targets\": [ \"<host>\", ... ],\n    \"labels\": {\n      \"<labelname>\": \"<labelvalue>\", ...\n    }\n  },\n  ...\n]`\nThe endpoint is queried periodically at the specified refresh interval.\nThe `prometheus_sd_http_failures_total` counter metric tracks the number of\nrefresh failures.\nEach target has a meta label `__meta_url` during the\nrelabeling phase. Its value is set to the\nURL from which the target was extracted.\n```yaml\nURL from which the targets are fetched.\nurl: \nRefresh interval to re-query the endpoint.\n[ refresh_interval:  | default = 60s ]\nAuthentication information used to authenticate to the API server.\nNote that `basic_auth`, `authorization` and `oauth2` options are\nmutually exclusive.\n`password` and `password_file` are mutually exclusive.\nOptional HTTP basic authentication information.\nbasic_auth:\n  [ username:  ]\n  [ password:  ]\n  [ password_file:  ]\nOptional `Authorization` header configuration.\nauthorization:\n  # Sets the authentication type.\n  [ type:  | default: Bearer ]\n  # Sets the credentials. It is mutually exclusive with\n  # `credentials_file`.\n  [ credentials:  ]\n  # Sets the credentials to the credentials read from the configured file.\n  # It is mutually exclusive with `credentials`.\n  [ credentials_file:  ]\nOptional OAuth 2.0 configuration.\noauth2:\n  [  ]\nOptional proxy URL.\n[ proxy_url:  ]\nSpecifies headers to send to proxies during CONNECT requests.\n[ proxy_connect_header:\n  [ : [, ...] ] ]\nConfigure whether HTTP requests follow HTTP 3xx redirects.\n[ follow_redirects:  | default = true ]\nWhether to enable HTTP2.\n[ enable_http2:  | default: true ]\nTLS configuration.\ntls_config:\n  [  ]\n```\n`<ionos_sd_config>`\nIONOS SD configurations allows retrieving scrape targets from\nIONOS Cloud API. This service discovery uses the\nfirst NICs IP address by default, but that can be changed with relabeling. The\nfollowing meta labels are available on all targets during\nrelabeling:\n\n`__meta_ionos_server_availability_zone`: the availability zone of the server\n`__meta_ionos_server_boot_cdrom_id`: the ID of the CD-ROM the server is booted\n  from\n`__meta_ionos_server_boot_image_id`: the ID of the boot image or snapshot the\n  server is booted from\n`__meta_ionos_server_boot_volume_id`: the ID of the boot volume\n`__meta_ionos_server_cpu_family`: the CPU family of the server\n  to\n`__meta_ionos_server_id`: the ID of the server\n`__meta_ionos_server_ip`: comma separated list of all IPs assigned to the\n  server\n`__meta_ionos_server_lifecycle`: the lifecycle state of the server resource\n`__meta_ionos_server_name`: the name of the server\n`__meta_ionos_server_nic_ip_<nic_name>`: comma separated list of IPs, grouped\n  by the name of each NIC attached to the server\n`__meta_ionos_server_servers_id`: the ID of the servers the server belongs to\n`__meta_ionos_server_state`: the execution state of the server\n`__meta_ionos_server_type`: the type of the server\n\n```yaml\nThe unique ID of the data center.\ndatacenter_id: \nAuthentication information used to authenticate to the API server.\nNote that `basic_auth` and `authorization` options are\nmutually exclusive.\npassword and password_file are mutually exclusive.\nOptional HTTP basic authentication information, required when using IONOS\nCloud username and password as authentication method.\nbasic_auth:\n  [ username:  ]\n  [ password:  ]\n  [ password_file:  ]\nOptional `Authorization` header configuration, required when using IONOS\nCloud token as authentication method.\nauthorization:\n  # Sets the authentication type.\n  [ type:  | default: Bearer ]\n  # Sets the credentials. It is mutually exclusive with\n  # `credentials_file`.\n  [ credentials:  ]\n  # Sets the credentials to the credentials read from the configured file.\n  # It is mutually exclusive with `credentials`.\n  [ credentials_file:  ]\nOptional OAuth 2.0 configuration.\nCannot be used at the same time as basic_auth or authorization.\noauth2:\n  [  ]\nOptional proxy URL.\n[ proxy_url:  ]\nSpecifies headers to send to proxies during CONNECT requests.\n[ proxy_connect_header:\n  [ : [, ...] ] ]\nConfigure whether HTTP requests follow HTTP 3xx redirects.\n[ follow_redirects:  | default = true ]\nWhether to enable HTTP2.\n[ enable_http2:  | default: true ]\nTLS configuration.\ntls_config:\n  [  ]\nThe port to scrape metrics from.\n[ port:  | default = 80 ]\nThe time after which the servers are refreshed.\n[ refresh_interval:  | default = 60s ]\n```\n`<kubernetes_sd_config>`\nKubernetes SD configurations allow retrieving scrape targets from\nKubernetes' REST API and always staying synchronized with\nthe cluster state.\nOne of the following `role` types can be configured to discover targets:\n`node`\nThe `node` role discovers one target per cluster node with the address defaulting\nto the Kubelet's HTTP port.\nThe target address defaults to the first existing address of the Kubernetes\nnode object in the address type order of `NodeInternalIP`, `NodeExternalIP`,\n`NodeLegacyHostIP`, and `NodeHostName`.\nAvailable meta labels:\n\n`__meta_kubernetes_node_name`: The name of the node object.\n`__meta_kubernetes_node_provider_id`: The cloud provider's name for the node object.\n`__meta_kubernetes_node_label_<labelname>`: Each label from the node object.\n`__meta_kubernetes_node_labelpresent_<labelname>`: `true` for each label from the node object.\n`__meta_kubernetes_node_annotation_<annotationname>`: Each annotation from the node object.\n`__meta_kubernetes_node_annotationpresent_<annotationname>`: `true` for each annotation from the node object.\n`__meta_kubernetes_node_address_<address_type>`: The first address for each node address type, if it exists.\n\nIn addition, the `instance` label for the node will be set to the node name\nas retrieved from the API server.\n`service`\nThe `service` role discovers a target for each service port for each service.\nThis is generally useful for blackbox monitoring of a service.\nThe address will be set to the Kubernetes DNS name of the service and respective\nservice port.\nAvailable meta labels:\n\n`__meta_kubernetes_namespace`: The namespace of the service object.\n`__meta_kubernetes_service_annotation_<annotationname>`: Each annotation from the service object.\n`__meta_kubernetes_service_annotationpresent_<annotationname>`: \"true\" for each annotation of the service object.\n`__meta_kubernetes_service_cluster_ip`: The cluster IP address of the service. (Does not apply to services of type ExternalName)\n`__meta_kubernetes_service_loadbalancer_ip`: The IP address of the loadbalancer. (Applies to services of type LoadBalancer)\n`__meta_kubernetes_service_external_name`: The DNS name of the service. (Applies to services of type ExternalName)\n`__meta_kubernetes_service_label_<labelname>`: Each label from the service object.\n`__meta_kubernetes_service_labelpresent_<labelname>`: `true` for each label of the service object.\n`__meta_kubernetes_service_name`: The name of the service object.\n`__meta_kubernetes_service_port_name`: Name of the service port for the target.\n`__meta_kubernetes_service_port_number`: Number of the service port for the target.\n`__meta_kubernetes_service_port_protocol`: Protocol of the service port for the target.\n`__meta_kubernetes_service_type`: The type of the service.\n\n`pod`\nThe `pod` role discovers all pods and exposes their containers as targets. For each declared\nport of a container, a single target is generated. If a container has no specified ports,\na port-free target per container is created for manually adding a port via relabeling.\nAvailable meta labels:\n\n`__meta_kubernetes_namespace`: The namespace of the pod object.\n`__meta_kubernetes_pod_name`: The name of the pod object.\n`__meta_kubernetes_pod_ip`: The pod IP of the pod object.\n`__meta_kubernetes_pod_label_<labelname>`: Each label from the pod object.\n`__meta_kubernetes_pod_labelpresent_<labelname>`: `true` for each label from the pod object.\n`__meta_kubernetes_pod_annotation_<annotationname>`: Each annotation from the pod object.\n`__meta_kubernetes_pod_annotationpresent_<annotationname>`: `true` for each annotation from the pod object.\n`__meta_kubernetes_pod_container_init`: `true` if the container is an InitContainer\n`__meta_kubernetes_pod_container_name`: Name of the container the target address points to.\n`__meta_kubernetes_pod_container_id`: ID of the container the target address points to. The ID is in the form `<type>://<container_id>`.\n`__meta_kubernetes_pod_container_image`: The image the container is using.\n`__meta_kubernetes_pod_container_port_name`: Name of the container port.\n`__meta_kubernetes_pod_container_port_number`: Number of the container port.\n`__meta_kubernetes_pod_container_port_protocol`: Protocol of the container port.\n`__meta_kubernetes_pod_ready`: Set to `true` or `false` for the pod's ready state.\n`__meta_kubernetes_pod_phase`: Set to `Pending`, `Running`, `Succeeded`, `Failed` or `Unknown`\n  in the lifecycle.\n`__meta_kubernetes_pod_node_name`: The name of the node the pod is scheduled onto.\n`__meta_kubernetes_pod_host_ip`: The current host IP of the pod object.\n`__meta_kubernetes_pod_uid`: The UID of the pod object.\n`__meta_kubernetes_pod_controller_kind`: Object kind of the pod controller.\n`__meta_kubernetes_pod_controller_name`: Name of the pod controller.\n\n`endpoints`\nThe `endpoints` role discovers targets from listed endpoints of a service. For each endpoint\naddress one target is discovered per port. If the endpoint is backed by a pod, all\nadditional container ports of the pod, not bound to an endpoint port, are discovered as targets as well.\nAvailable meta labels:\n\n`__meta_kubernetes_namespace`: The namespace of the endpoints object.\n`__meta_kubernetes_endpoints_name`: The names of the endpoints object.\n`__meta_kubernetes_endpoints_label_<labelname>`: Each label from the endpoints object.\n`__meta_kubernetes_endpoints_labelpresent_<labelname>`: `true` for each label from the endpoints object.\nFor all targets discovered directly from the endpoints list (those not additionally inferred\n  from underlying pods), the following labels are attached:\n`__meta_kubernetes_endpoint_hostname`: Hostname of the endpoint.\n`__meta_kubernetes_endpoint_node_name`: Name of the node hosting the endpoint.\n`__meta_kubernetes_endpoint_ready`: Set to `true` or `false` for the endpoint's ready state.\n`__meta_kubernetes_endpoint_port_name`: Name of the endpoint port.\n`__meta_kubernetes_endpoint_port_protocol`: Protocol of the endpoint port.\n`__meta_kubernetes_endpoint_address_target_kind`: Kind of the endpoint address target.\n`__meta_kubernetes_endpoint_address_target_name`: Name of the endpoint address target.\nIf the endpoints belong to a service, all labels of the `role: service` discovery are attached.\nFor all targets backed by a pod, all labels of the `role: pod` discovery are attached.\n\n`endpointslice`\nThe `endpointslice` role discovers targets from existing endpointslices. For each endpoint\naddress referenced in the endpointslice object one target is discovered. If the endpoint is backed by a pod, all\nadditional container ports of the pod, not bound to an endpoint port, are discovered as targets as well.\nAvailable meta labels:\n\n`__meta_kubernetes_namespace`: The namespace of the endpoints object.\n`__meta_kubernetes_endpointslice_name`: The name of endpointslice object.\nFor all targets discovered directly from the endpointslice list (those not additionally inferred\n  from underlying pods), the following labels are attached:\n`__meta_kubernetes_endpointslice_address_target_kind`: Kind of the referenced object.\n`__meta_kubernetes_endpointslice_address_target_name`: Name of referenced object.\n`__meta_kubernetes_endpointslice_address_type`: The ip protocol family of the address of the target.\n`__meta_kubernetes_endpointslice_endpoint_conditions_ready`:  Set to `true` or `false` for the referenced endpoint's ready state.\n`__meta_kubernetes_endpointslice_endpoint_conditions_serving`:  Set to `true` or `false` for the referenced endpoint's serving state.\n`__meta_kubernetes_endpointslice_endpoint_conditions_terminating`:  Set to `true` or `false` for the referenced endpoint's terminating state.\n`__meta_kubernetes_endpointslice_endpoint_topology_kubernetes_io_hostname`:  Name of the node hosting the referenced endpoint.\n`__meta_kubernetes_endpointslice_endpoint_topology_present_kubernetes_io_hostname`: Flag that shows if the referenced object has a kubernetes.io/hostname annotation.\n`__meta_kubernetes_endpointslice_port`: Port of the referenced endpoint.\n`__meta_kubernetes_endpointslice_port_name`: Named port of the referenced endpoint.\n`__meta_kubernetes_endpointslice_port_protocol`: Protocol of the referenced endpoint.\nIf the endpoints belong to a service, all labels of the `role: service` discovery are attached.\nFor all targets backed by a pod, all labels of the `role: pod` discovery are attached.\n\n`ingress`\nThe `ingress` role discovers a target for each path of each ingress.\nThis is generally useful for blackbox monitoring of an ingress.\nThe address will be set to the host specified in the ingress spec.\nAvailable meta labels:\n\n`__meta_kubernetes_namespace`: The namespace of the ingress object.\n`__meta_kubernetes_ingress_name`: The name of the ingress object.\n`__meta_kubernetes_ingress_label_<labelname>`: Each label from the ingress object.\n`__meta_kubernetes_ingress_labelpresent_<labelname>`: `true` for each label from the ingress object.\n`__meta_kubernetes_ingress_annotation_<annotationname>`: Each annotation from the ingress object.\n`__meta_kubernetes_ingress_annotationpresent_<annotationname>`: `true` for each annotation from the ingress object.\n`__meta_kubernetes_ingress_class_name`: Class name from ingress spec, if present.\n`__meta_kubernetes_ingress_scheme`: Protocol scheme of ingress, `https` if TLS\n  config is set. Defaults to `http`.\n`__meta_kubernetes_ingress_path`: Path from ingress spec. Defaults to `/`.\n\nSee below for the configuration options for Kubernetes discovery:\n```yaml\nThe information to access the Kubernetes API.\nThe API server addresses. If left empty, Prometheus is assumed to run inside\nof the cluster and will discover API servers automatically and use the pod's\nCA certificate and bearer token file at /var/run/secrets/kubernetes.io/serviceaccount/.\n[ api_server:  ]\nThe Kubernetes role of entities that should be discovered.\nOne of endpoints, endpointslice, service, pod, node, or ingress.\nrole: \nOptional path to a kubeconfig file.\nNote that api_server and kube_config are mutually exclusive.\n[ kubeconfig_file:  ]\nOptional authentication information used to authenticate to the API server.\nNote that `basic_auth` and `authorization` options are mutually exclusive.\npassword and password_file are mutually exclusive.\nOptional HTTP basic authentication information.\nbasic_auth:\n  [ username:  ]\n  [ password:  ]\n  [ password_file:  ]\nOptional `Authorization` header configuration.\nauthorization:\n  # Sets the authentication type.\n  [ type:  | default: Bearer ]\n  # Sets the credentials. It is mutually exclusive with\n  # `credentials_file`.\n  [ credentials:  ]\n  # Sets the credentials to the credentials read from the configured file.\n  # It is mutually exclusive with `credentials`.\n  [ credentials_file:  ]\nOptional OAuth 2.0 configuration.\nCannot be used at the same time as basic_auth or authorization.\noauth2:\n  [  ]\nOptional proxy URL.\n[ proxy_url:  ]\nSpecifies headers to send to proxies during CONNECT requests.\n[ proxy_connect_header:\n  [ : [, ...] ] ]\nConfigure whether HTTP requests follow HTTP 3xx redirects.\n[ follow_redirects:  | default = true ]\nWhether to enable HTTP2.\n[ enable_http2:  | default: true ]\nTLS configuration.\ntls_config:\n  [  ]\nOptional namespace discovery. If omitted, all namespaces are used.\nnamespaces:\n  own_namespace: \n  names:\n    [ -  ]\nOptional label and field selectors to limit the discovery process to a subset of available resources.\nSee https://kubernetes.io/docs/concepts/overview/working-with-objects/field-selectors/\nand https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/ to learn more about the possible\nfilters that can be used. The endpoints role supports pod, service and endpoints selectors.\nThe pod role supports node selectors when configured with `attach_metadata: {node: true}`.\nOther roles only support selectors matching the role itself (e.g. node role can only contain node selectors).\nNote: When making decision about using field/label selector make sure that this\nis the best approach - it will prevent Prometheus from reusing single list/watch\nfor all scrape configs. This might result in a bigger load on the Kubernetes API,\nbecause per each selector combination there will be additional LIST/WATCH. On the other hand,\nif you just want to monitor small subset of pods in large cluster it's recommended to use selectors.\nDecision, if selectors should be used or not depends on the particular situation.\n[ selectors:\n  [ - role: \n    [ label:  ]\n    [ field:  ] ]]\nOptional metadata to attach to discovered targets. If omitted, no additional metadata is attached.\nattach_metadata:\nAttaches node metadata to discovered targets. Valid for roles: pod, endpoints, endpointslice.\nWhen set to true, Prometheus must have permissions to get Nodes.\n[ node:  | default = false ]\n```\nSee this example Prometheus configuration file\nfor a detailed example of configuring Prometheus for Kubernetes.\nYou may wish to check out the 3rd party Prometheus Operator,\nwhich automates the Prometheus setup on top of Kubernetes.\n`<kuma_sd_config>`\nKuma SD configurations allow retrieving scrape target from the Kuma control plane.\nThis SD discovers \"monitoring assignments\" based on Kuma Dataplane Proxies,\nvia the MADS v1 (Monitoring Assignment Discovery Service) xDS API, and will create a target for each proxy\ninside a Prometheus-enabled mesh.\nThe following meta labels are available for each target:\n\n`__meta_kuma_mesh`: the name of the proxy's Mesh\n`__meta_kuma_dataplane`: the name of the proxy\n`__meta_kuma_service`: the name of the proxy's associated Service\n`__meta_kuma_label_<tagname>`: each tag of the proxy\n\nSee below for the configuration options for Kuma MonitoringAssignment discovery:\n```yaml\nAddress of the Kuma Control Plane's MADS xDS server.\nserver: \nThe time to wait between polling update requests.\n[ refresh_interval:  | default = 30s ]\nThe time after which the monitoring assignments are refreshed.\n[ fetch_timeout:  | default = 2m ]\nOptional proxy URL.\n[ proxy_url:  ]\nSpecifies headers to send to proxies during CONNECT requests.\n[ proxy_connect_header:\n  [ : [, ...] ] ]\nTLS configuration.\ntls_config:\n  [  ]\nAuthentication information used to authenticate to the Docker daemon.\nNote that `basic_auth` and `authorization` options are\nmutually exclusive.\npassword and password_file are mutually exclusive.\nOptional HTTP basic authentication information.\nbasic_auth:\n  [ username:  ]\n  [ password:  ]\n  [ password_file:  ]\nOptional the `Authorization` header configuration.\nauthorization:\n  # Sets the authentication type.\n  [ type:  | default: Bearer ]\n  # Sets the credentials. It is mutually exclusive with\n  # `credentials_file`.\n  [ credentials:  ]\n  # Sets the credentials with the credentials read from the configured file.\n  # It is mutually exclusive with `credentials`.\n  [ credentials_file:  ]\nOptional OAuth 2.0 configuration.\nCannot be used at the same time as basic_auth or authorization.\noauth2:\n  [  ]\nConfigure whether HTTP requests follow HTTP 3xx redirects.\n[ follow_redirects:  | default = true ]\nWhether to enable HTTP2.\n[ enable_http2:  | default: true ]\n```\nThe relabeling phase is the preferred and more powerful way\nto filter proxies and user-defined tags.\n`<lightsail_sd_config>`\nLightsail SD configurations allow retrieving scrape targets from AWS Lightsail\ninstances. The private IP address is used by default, but may be changed to\nthe public IP address with relabeling.\nThe following meta labels are available on targets during relabeling:\n\n`__meta_lightsail_availability_zone`: the availability zone in which the instance is running\n`__meta_lightsail_blueprint_id`: the Lightsail blueprint ID\n`__meta_lightsail_bundle_id`: the Lightsail bundle ID\n`__meta_lightsail_instance_name`: the name of the Lightsail instance\n`__meta_lightsail_instance_state`: the state of the Lightsail instance\n`__meta_lightsail_instance_support_code`: the support code of the Lightsail instance\n`__meta_lightsail_ipv6_addresses`: comma separated list of IPv6 addresses assigned to the instance's network interfaces, if present\n`__meta_lightsail_private_ip`: the private IP address of the instance\n`__meta_lightsail_public_ip`: the public IP address of the instance, if available\n`__meta_lightsail_region`: the region of the instance\n`__meta_lightsail_tag_<tagkey>`: each tag value of the instance\n\nSee below for the configuration options for Lightsail discovery:\n```yaml\nThe information to access the Lightsail API.\nThe AWS region. If blank, the region from the instance metadata is used.\n[ region:  ]\nCustom endpoint to be used.\n[ endpoint:  ]\nThe AWS API keys. If blank, the environment variables `AWS_ACCESS_KEY_ID`\nand `AWS_SECRET_ACCESS_KEY` are used.\n[ access_key:  ]\n[ secret_key:  ]\nNamed AWS profile used to connect to the API.\n[ profile:  ]\nAWS Role ARN, an alternative to using AWS API keys.\n[ role_arn:  ]\nRefresh interval to re-read the instance list.\n[ refresh_interval:  | default = 60s ]\nThe port to scrape metrics from. If using the public IP address, this must\ninstead be specified in the relabeling rule.\n[ port:  | default = 80 ]\nAuthentication information used to authenticate to the Lightsail API.\nNote that `basic_auth`, `authorization` and `oauth2` options are\nmutually exclusive.\n`password` and `password_file` are mutually exclusive.\nOptional HTTP basic authentication information, currently not supported by AWS.\nbasic_auth:\n  [ username:  ]\n  [ password:  ]\n  [ password_file:  ]\nOptional `Authorization` header configuration, currently not supported by AWS.\nauthorization:\n  # Sets the authentication type.\n  [ type:  | default: Bearer ]\n  # Sets the credentials. It is mutually exclusive with\n  # `credentials_file`.\n  [ credentials:  ]\n  # Sets the credentials to the credentials read from the configured file.\n  # It is mutuall exclusive with `credentials`.\n  [ credentials_file:  ]\nOptional OAuth 2.0 configuration, currently not supported by AWS.\noauth2:\n  [  ]\nOptional proxy URL.\n[ proxy_url:  ]\nSpecifies headers to send to proxies during CONNECT requests.\n[ proxy_connect_header:\n  [ : [, ...] ] ]\nConfigure whether HTTP requests follow HTTP 3xx redirects.\n[ follow_redirects:  | default = true ]\nWhether to enable HTTP2.\n[ enable_http2:  | default: true ]\nTLS configuration.\ntls_config:\n  [  ]\n```\n`<linode_sd_config>`\nLinode SD configurations allow retrieving scrape targets from Linode's\nLinode APIv4.\nThis service discovery uses the public IPv4 address by default, by that can be\nchanged with relabeling, as demonstrated in the Prometheus linode-sd\nconfiguration file.\nThe following meta labels are available on targets during relabeling:\n\n`__meta_linode_instance_id`: the id of the linode instance\n`__meta_linode_instance_label`: the label of the linode instance\n`__meta_linode_image`: the slug of the linode instance's image\n`__meta_linode_private_ipv4`: the private IPv4 of the linode instance\n`__meta_linode_public_ipv4`: the public IPv4 of the linode instance\n`__meta_linode_public_ipv6`: the public IPv6 of the linode instance\n`__meta_linode_region`: the region of the linode instance\n`__meta_linode_type`: the type of the linode instance\n`__meta_linode_status`: the status of the linode instance\n`__meta_linode_tags`: a list of tags of the linode instance joined by the tag separator\n`__meta_linode_group`: the display group a linode instance is a member of\n`__meta_linode_hypervisor`: the virtualization software powering the linode instance\n`__meta_linode_backups`: the backup service status of the linode instance\n`__meta_linode_specs_disk_bytes`: the amount of storage space the linode instance has access to\n`__meta_linode_specs_memory_bytes`: the amount of RAM the linode instance has access to\n`__meta_linode_specs_vcpus`: the number of VCPUS this linode has access to\n`__meta_linode_specs_transfer_bytes`: the amount of network transfer the linode instance is allotted each month\n`__meta_linode_extra_ips`: a list of all extra IPv4 addresses assigned to the linode instance joined by the tag separator\n\n```yaml\nAuthentication information used to authenticate to the API server.\nNote that `basic_auth` and `authorization` options are\nmutually exclusive.\npassword and password_file are mutually exclusive.\nNote: Linode APIv4 Token must be created with scopes: 'linodes:read_only', 'ips:read_only', and 'events:read_only'\nOptional HTTP basic authentication information, not currently supported by Linode APIv4.\nbasic_auth:\n  [ username:  ]\n  [ password:  ]\n  [ password_file:  ]\nOptional the `Authorization` header configuration.\nauthorization:\n  # Sets the authentication type.\n  [ type:  | default: Bearer ]\n  # Sets the credentials. It is mutually exclusive with\n  # `credentials_file`.\n  [ credentials:  ]\n  # Sets the credentials with the credentials read from the configured file.\n  # It is mutually exclusive with `credentials`.\n  [ credentials_file:  ]\nOptional OAuth 2.0 configuration.\nCannot be used at the same time as basic_auth or authorization.\noauth2:\n  [  ]\nOptional proxy URL.\n[ proxy_url:  ]\nSpecifies headers to send to proxies during CONNECT requests.\n[ proxy_connect_header:\n  [ : [, ...] ] ]\nConfigure whether HTTP requests follow HTTP 3xx redirects.\n[ follow_redirects:  | default = true ]\nWhether to enable HTTP2.\n[ enable_http2:  | default: true ]\nTLS configuration.\ntls_config:\n  [  ]\nThe port to scrape metrics from.\n[ port:  | default = 80 ]\nThe string by which Linode Instance tags are joined into the tag label.\n[ tag_separator:  | default = , ]\nThe time after which the linode instances are refreshed.\n[ refresh_interval:  | default = 60s ]\n```\n`<marathon_sd_config>`\nMarathon SD configurations allow retrieving scrape targets using the\nMarathon REST API. Prometheus\nwill periodically check the REST endpoint for currently running tasks and\ncreate a target group for every app that has at least one healthy task.\nThe following meta labels are available on targets during relabeling:\n\n`__meta_marathon_app`: the name of the app (with slashes replaced by dashes)\n`__meta_marathon_image`: the name of the Docker image used (if available)\n`__meta_marathon_task`: the ID of the Mesos task\n`__meta_marathon_app_label_<labelname>`: any Marathon labels attached to the app\n`__meta_marathon_port_definition_label_<labelname>`: the port definition labels\n`__meta_marathon_port_mapping_label_<labelname>`: the port mapping labels\n`__meta_marathon_port_index`: the port index number (e.g. `1` for `PORT1`)\n\nSee below for the configuration options for Marathon discovery:\n```yaml\nList of URLs to be used to contact Marathon servers.\nYou need to provide at least one server URL.\nservers:\n  - \nPolling interval\n[ refresh_interval:  | default = 30s ]\nOptional authentication information for token-based authentication\nhttps://docs.mesosphere.com/1.11/security/ent/iam-api/#passing-an-authentication-token\nIt is mutually exclusive with `auth_token_file` and other authentication mechanisms.\n[ auth_token:  ]\nOptional authentication information for token-based authentication\nhttps://docs.mesosphere.com/1.11/security/ent/iam-api/#passing-an-authentication-token\nIt is mutually exclusive with `auth_token` and other authentication mechanisms.\n[ auth_token_file:  ]\nSets the `Authorization` header on every request with the\nconfigured username and password.\nThis is mutually exclusive with other authentication mechanisms.\npassword and password_file are mutually exclusive.\nbasic_auth:\n  [ username:  ]\n  [ password:  ]\n  [ password_file:  ]\nOptional `Authorization` header configuration.\nNOTE: The current version of DC/OS marathon (v1.11.0) does not support\nstandard `Authentication` header, use `auth_token` or `auth_token_file`\ninstead.\nauthorization:\n  # Sets the authentication type.\n  [ type:  | default: Bearer ]\n  # Sets the credentials. It is mutually exclusive with\n  # `credentials_file`.\n  [ credentials:  ]\n  # Sets the credentials to the credentials read from the configured file.\n  # It is mutually exclusive with `credentials`.\n  [ credentials_file:  ]\nOptional OAuth 2.0 configuration.\nCannot be used at the same time as basic_auth or authorization.\noauth2:\n  [  ]\nConfigure whether HTTP requests follow HTTP 3xx redirects.\n[ follow_redirects:  | default = true ]\nWhether to enable HTTP2.\n[ enable_http2:  | default: true ]\nTLS configuration for connecting to marathon servers\ntls_config:\n  [  ]\nOptional proxy URL.\n[ proxy_url:  ]\nSpecifies headers to send to proxies during CONNECT requests.\n[ proxy_connect_header:\n  [ : [, ...] ] ]\n```\nBy default every app listed in Marathon will be scraped by Prometheus. If not all\nof your services provide Prometheus metrics, you can use a Marathon label and\nPrometheus relabeling to control which instances will actually be scraped.\nSee the Prometheus marathon-sd configuration file\nfor a practical example on how to set up your Marathon app and your Prometheus\nconfiguration.\nBy default, all apps will show up as a single job in Prometheus (the one specified\nin the configuration file), which can also be changed using relabeling.\n`<nerve_sd_config>`\nNerve SD configurations allow retrieving scrape targets from [AirBnB's Nerve]\n(https://github.com/airbnb/nerve) which are stored in\nZookeeper.\nThe following meta labels are available on targets during relabeling:\n\n`__meta_nerve_path`: the full path to the endpoint node in Zookeeper\n`__meta_nerve_endpoint_host`: the host of the endpoint\n`__meta_nerve_endpoint_port`: the port of the endpoint\n`__meta_nerve_endpoint_name`: the name of the endpoint\n\n```yaml\nThe Zookeeper servers.\nservers:\n  - \nPaths can point to a single service, or the root of a tree of services.\npaths:\n  - \n[ timeout:  | default = 10s ]\n```\n`<nomad_sd_config>`\nNomad SD configurations allow retrieving scrape targets from Nomad's\nService API.\nThe following meta labels are available on targets during relabeling:\n\n`__meta_nomad_address`: the service address of the target\n`__meta_nomad_dc`: the datacenter name for the target\n`__meta_nomad_namespace`: the namespace of the target\n`__meta_nomad_node_id`: the node name defined for the target\n`__meta_nomad_service`: the name of the service the target belongs to\n`__meta_nomad_service_address`: the service address of the target\n`__meta_nomad_service_id`: the service ID of the target\n`__meta_nomad_service_port`: the service port of the target\n`__meta_nomad_tags`: the list of tags of the target joined by the tag separator\n\n```yaml\nThe information to access the Nomad API. It is to be defined\nas the Nomad documentation requires.\n[ allow_stale:  | default = true ]\n[ namespace:  | default = default ]\n[ refresh_interval:  | default = 60s ]\n[ region:  | default = global ]\n[ server:  ]\n[ tag_separator:  | default = ,]\nAuthentication information used to authenticate to the nomad server.\nNote that `basic_auth`, `authorization` and `oauth2` options are\nmutually exclusive.\n`password` and `password_file` are mutually exclusive.\nOptional HTTP basic authentication information.\nbasic_auth:\n  [ username:  ]\n  [ password:  ]\n  [ password_file:  ]\nOptional `Authorization` header configuration.\nauthorization:\n  # Sets the authentication type.\n  [ type:  | default: Bearer ]\n  # Sets the credentials. It is mutually exclusive with\n  # `credentials_file`.\n  [ credentials:  ]\n  # Sets the credentials to the credentials read from the configured file.\n  # It is mutually exclusive with `credentials`.\n  [ credentials_file:  ]\nOptional OAuth 2.0 configuration.\noauth2:\n  [  ]\nOptional proxy URL.\n[ proxy_url:  ]\nSpecifies headers to send to proxies during CONNECT requests.\n[ proxy_connect_header:\n  [ : [, ...] ] ]\nConfigure whether HTTP requests follow HTTP 3xx redirects.\n[ follow_redirects:  | default = true ]\nWhether to enable HTTP2.\n[ enable_http2:  | default: true ]\nTLS configuration.\ntls_config:\n  [  ]\n```\n`<serverset_sd_config>`\nServerset SD configurations allow retrieving scrape targets from [Serversets]\n(https://github.com/twitter/finagle/tree/develop/finagle-serversets) which are\nstored in Zookeeper. Serversets are commonly\nused by Finagle and\nAurora.\nThe following meta labels are available on targets during relabeling:\n\n`__meta_serverset_path`: the full path to the serverset member node in Zookeeper\n`__meta_serverset_endpoint_host`: the host of the default endpoint\n`__meta_serverset_endpoint_port`: the port of the default endpoint\n`__meta_serverset_endpoint_host_<endpoint>`: the host of the given endpoint\n`__meta_serverset_endpoint_port_<endpoint>`: the port of the given endpoint\n`__meta_serverset_shard`: the shard number of the member\n`__meta_serverset_status`: the status of the member\n\n```yaml\nThe Zookeeper servers.\nservers:\n  - \nPaths can point to a single serverset, or the root of a tree of serversets.\npaths:\n  - \n[ timeout:  | default = 10s ]\n```\nServerset data must be in the JSON format, the Thrift format is not currently supported.\n`<triton_sd_config>`\nTriton SD configurations allow retrieving\nscrape targets from Container Monitor\ndiscovery endpoints.\nOne of the following `<triton_role>` types can be configured to discover targets:\n`container`\nThe `container` role discovers one target per \"virtual machine\" owned by the `account`.\nThese are SmartOS zones or lx/KVM/bhyve branded zones.\nThe following meta labels are available on targets during relabeling:\n\n`__meta_triton_groups`: the list of groups belonging to the target joined by a comma separator\n`__meta_triton_machine_alias`: the alias of the target container\n`__meta_triton_machine_brand`: the brand of the target container\n`__meta_triton_machine_id`: the UUID of the target container\n`__meta_triton_machine_image`: the target container's image type\n`__meta_triton_server_id`: the server UUID the target container is running on\n\n`cn`\nThe `cn` role discovers one target for per compute node (also known as \"server\" or \"global zone\") making up the Triton infrastructure.\nThe `account` must be a Triton operator and is currently required to own at least one `container`.\nThe following meta labels are available on targets during relabeling:\n\n`__meta_triton_machine_alias`: the hostname of the target (requires triton-cmon 1.7.0 or newer)\n`__meta_triton_machine_id`: the UUID of the target\n\nSee below for the configuration options for Triton discovery:\n```yaml\nThe information to access the Triton discovery API.\nThe account to use for discovering new targets.\naccount: \nThe type of targets to discover, can be set to:\n* \"container\" to discover virtual machines (SmartOS zones, lx/KVM/bhyve branded zones) running on Triton\n* \"cn\" to discover compute nodes (servers/global zones) making up the Triton infrastructure\n[ role :  | default = \"container\" ]\nThe DNS suffix which should be applied to target.\ndns_suffix: \nThe Triton discovery endpoint (e.g. 'cmon.us-east-3b.triton.zone'). This is\noften the same value as dns_suffix.\nendpoint: \nA list of groups for which targets are retrieved, only supported when `role` == `container`.\nIf omitted all containers owned by the requesting account are scraped.\ngroups:\n  [ -  ... ]\nThe port to use for discovery and metric scraping.\n[ port:  | default = 9163 ]\nThe interval which should be used for refreshing targets.\n[ refresh_interval:  | default = 60s ]\nThe Triton discovery API version.\n[ version:  | default = 1 ]\nTLS configuration.\ntls_config:\n  [  ]\n```\n`<eureka_sd_config>`\nEureka SD configurations allow retrieving scrape targets using the\nEureka REST API. Prometheus\nwill periodically check the REST endpoint and\ncreate a target for every app instance.\nThe following meta labels are available on targets during relabeling:\n\n`__meta_eureka_app_name`: the name of the app\n`__meta_eureka_app_instance_id`: the ID of the app instance\n`__meta_eureka_app_instance_hostname`: the hostname of the instance\n`__meta_eureka_app_instance_homepage_url`: the homepage url of the app instance\n`__meta_eureka_app_instance_statuspage_url`: the status page url of the app instance\n`__meta_eureka_app_instance_healthcheck_url`: the health check url of the app instance\n`__meta_eureka_app_instance_ip_addr`: the IP address of the app instance\n`__meta_eureka_app_instance_vip_address`: the VIP address of the app instance\n`__meta_eureka_app_instance_secure_vip_address`: the secure VIP address of the app instance\n`__meta_eureka_app_instance_status`: the status of the app instance\n`__meta_eureka_app_instance_port`: the port of the app instance\n`__meta_eureka_app_instance_port_enabled`: the port enabled of the app instance\n`__meta_eureka_app_instance_secure_port`: the secure port address of the app instance\n`__meta_eureka_app_instance_secure_port_enabled`: the secure port of the app instance\n`__meta_eureka_app_instance_country_id`: the country ID of the app instance\n`__meta_eureka_app_instance_metadata_<metadataname>`: app instance metadata\n`__meta_eureka_app_instance_datacenterinfo_name`: the datacenter name of the app instance\n`__meta_eureka_app_instance_datacenterinfo_<metadataname>`: the datacenter metadata\n\nSee below for the configuration options for Eureka discovery:\n```yaml\nThe URL to connect to the Eureka server.\nserver: \nSets the `Authorization` header on every request with the\nconfigured username and password.\npassword and password_file are mutually exclusive.\nbasic_auth:\n  [ username:  ]\n  [ password:  ]\n  [ password_file:  ]\nOptional `Authorization` header configuration.\nauthorization:\n  # Sets the authentication type.\n  [ type:  | default: Bearer ]\n  # Sets the credentials. It is mutually exclusive with\n  # `credentials_file`.\n  [ credentials:  ]\n  # Sets the credentials to the credentials read from the configured file.\n  # It is mutually exclusive with `credentials`.\n  [ credentials_file:  ]\nOptional OAuth 2.0 configuration.\nCannot be used at the same time as basic_auth or authorization.\noauth2:\n  [  ]\nConfigures the scrape request's TLS settings.\ntls_config:\n  [  ]\nOptional proxy URL.\n[ proxy_url:  ]\nSpecifies headers to send to proxies during CONNECT requests.\n[ proxy_connect_header:\n  [ : [, ...] ] ]\nConfigure whether HTTP requests follow HTTP 3xx redirects.\n[ follow_redirects:  | default = true ]\nWhether to enable HTTP2.\n[ enable_http2:  | default: true ]\nRefresh interval to re-read the app instance list.\n[ refresh_interval:  | default = 30s ]\n```\nSee the Prometheus eureka-sd configuration file\nfor a practical example on how to set up your Eureka app and your Prometheus\nconfiguration.\n`<scaleway_sd_config>`\nScaleway SD configurations allow retrieving scrape targets from Scaleway instances and baremetal services.\nThe following meta labels are available on targets during relabeling:\nInstance role\n\n`__meta_scaleway_instance_boot_type`: the boot type of the server\n`__meta_scaleway_instance_hostname`: the hostname of the server\n`__meta_scaleway_instance_id`: the ID of the server\n`__meta_scaleway_instance_image_arch`: the arch of the server image\n`__meta_scaleway_instance_image_id`: the ID of the server image\n`__meta_scaleway_instance_image_name`: the name of the server image\n`__meta_scaleway_instance_location_cluster_id`: the cluster ID of the server location\n`__meta_scaleway_instance_location_hypervisor_id`: the hypervisor ID of the server location\n`__meta_scaleway_instance_location_node_id`: the node ID of the server location\n`__meta_scaleway_instance_name`: name of the server\n`__meta_scaleway_instance_organization_id`: the organization of the server\n`__meta_scaleway_instance_private_ipv4`: the private IPv4 address of the server\n`__meta_scaleway_instance_project_id`: project id of the server\n`__meta_scaleway_instance_public_ipv4`: the public IPv4 address of the server\n`__meta_scaleway_instance_public_ipv6`: the public IPv6 address of the server\n`__meta_scaleway_instance_region`: the region of the server\n`__meta_scaleway_instance_security_group_id`: the ID of the security group of the server\n`__meta_scaleway_instance_security_group_name`: the name of the security group of the server\n`__meta_scaleway_instance_status`: status of the server\n`__meta_scaleway_instance_tags`: the list of tags of the server joined by the tag separator\n`__meta_scaleway_instance_type`: commercial type of the server\n`__meta_scaleway_instance_zone`: the zone of the server (ex: `fr-par-1`, complete list here)\n\nThis role uses the private IPv4 address by default. This can be\nchanged with relabeling, as demonstrated in the Prometheus scaleway-sd\nconfiguration file.\nBaremetal role\n\n`__meta_scaleway_baremetal_id`: the ID of the server\n`__meta_scaleway_baremetal_public_ipv4`: the public IPv4 address of the server\n`__meta_scaleway_baremetal_public_ipv6`: the public IPv6 address of the server\n`__meta_scaleway_baremetal_name`: the name of the server\n`__meta_scaleway_baremetal_os_name`: the name of the operating system of the server\n`__meta_scaleway_baremetal_os_version`: the version of the operating system of the server\n`__meta_scaleway_baremetal_project_id`: the project ID of the server\n`__meta_scaleway_baremetal_status`: the status of the server\n`__meta_scaleway_baremetal_tags`: the list of tags of the server joined by the tag separator\n`__meta_scaleway_baremetal_type`: the commercial type of the server\n`__meta_scaleway_baremetal_zone`: the zone of the server (ex: `fr-par-1`, complete list here)\n\nThis role uses the public IPv4 address by default. This can be\nchanged with relabeling, as demonstrated in the Prometheus scaleway-sd\nconfiguration file.\nSee below for the configuration options for Scaleway discovery:\n```yaml\nAccess key to use. https://console.scaleway.com/project/credentials\naccess_key: \nSecret key to use when listing targets. https://console.scaleway.com/project/credentials\nIt is mutually exclusive with `secret_key_file`.\n[ secret_key:  ]\nSets the secret key with the credentials read from the configured file.\nIt is mutually exclusive with `secret_key`.\n[ secret_key_file:  ]\nProject ID of the targets.\nproject_id: \nRole of the targets to retrieve. Must be `instance` or `baremetal`.\nrole: \nThe port to scrape metrics from.\n[ port:  | default = 80 ]\nAPI URL to use when doing the server listing requests.\n[ api_url:  | default = \"https://api.scaleway.com\" ]\nZone is the availability zone of your targets (e.g. fr-par-1).\n[ zone:  | default = fr-par-1 ]\nNameFilter specify a name filter (works as a LIKE) to apply on the server listing request.\n[ name_filter:  ]\nTagsFilter specify a tag filter (a server needs to have all defined tags to be listed) to apply on the server listing request.\ntags_filter:\n[ -  ]\nRefresh interval to re-read the targets list.\n[ refresh_interval:  | default = 60s ]\nConfigure whether HTTP requests follow HTTP 3xx redirects.\n[ follow_redirects:  | default = true ]\nWhether to enable HTTP2.\n[ enable_http2:  | default: true ]\nOptional proxy URL.\n[ proxy_url:  ]\nSpecifies headers to send to proxies during CONNECT requests.\n[ proxy_connect_header:\n  [ : [, ...] ] ]\nTLS configuration.\ntls_config:\n  [  ]\n```\n`<uyuni_sd_config>`\nUyuni SD configurations allow retrieving scrape targets from managed systems\nvia Uyuni API.\nThe following meta labels are available on targets during relabeling:\n\n`__meta_uyuni_endpoint_name`: the name of the application endpoint\n`__meta_uyuni_exporter`: the exporter exposing metrics for the target\n`__meta_uyuni_groups`: the system groups of the target\n`__meta_uyuni_metrics_path`: metrics path for the target\n`__meta_uyuni_minion_hostname`: hostname of the Uyuni client\n`__meta_uyuni_primary_fqdn`: primary FQDN of the Uyuni client\n`__meta_uyuni_proxy_module`: the module name if Exporter Exporter proxy is\n  configured for the target\n`__meta_uyuni_scheme`:  the protocol scheme used for requests\n`__meta_uyuni_system_id`: the system ID of the client\n\nSee below for the configuration options for Uyuni discovery:\n```yaml\nThe URL to connect to the Uyuni server.\nserver: \nCredentials are used to authenticate the requests to Uyuni API.\nusername: \npassword: \nThe entitlement string to filter eligible systems.\n[ entitlement:  | default = monitoring_entitled ]\nThe string by which Uyuni group names are joined into the groups label.\n[ separator:  | default = , ]\nRefresh interval to re-read the managed targets list.\n[ refresh_interval:  | default = 60s ]\nOptional HTTP basic authentication information, currently not supported by Uyuni.\nbasic_auth:\n  [ username:  ]\n    [ password:  ]\n    [ password_file:  ]\nOptional `Authorization` header configuration, currently not supported by Uyuni.\nauthorization:\n  # Sets the authentication type.\n    [ type:  | default: Bearer ]\n    # Sets the credentials. It is mutually exclusive with\n    # `credentials_file`.\n    [ credentials:  ]\n    # Sets the credentials to the credentials read from the configured file.\n    # It is mutually exclusive with `credentials`.\n    [ credentials_file:  ]\nOptional OAuth 2.0 configuration, currently not supported by Uyuni.\nCannot be used at the same time as basic_auth or authorization.\noauth2:\n  [  ]\nOptional proxy URL.\n[ proxy_url:  ]\nSpecifies headers to send to proxies during CONNECT requests.\n[ proxy_connect_header:\n  [ : [, ...] ] ]\nConfigure whether HTTP requests follow HTTP 3xx redirects.\n[ follow_redirects:  | default = true ]\nWhether to enable HTTP2.\n[ enable_http2:  | default: true ]\nTLS configuration.\ntls_config:\n  [  ]\n```\nSee the Prometheus uyuni-sd configuration file\nfor a practical example on how to set up Uyuni Prometheus configuration.\n`<vultr_sd_config>`\nVultr SD configurations allow retrieving scrape targets from Vultr.\nThis service discovery uses the main IPv4 address by default, which that be\nchanged with relabeling, as demonstrated in the Prometheus vultr-sd\nconfiguration file.\nThe following meta labels are available on targets during relabeling:\n\n`__meta_vultr_instance_id` : A unique ID for the vultr Instance.\n`__meta_vultr_instance_label` : The user-supplied label for this instance.\n`__meta_vultr_instance_os` : The Operating System name.\n`__meta_vultr_instance_os_id` : The Operating System id used by this instance.\n`__meta_vultr_instance_region` : The Region id where the Instance is located.\n`__meta_vultr_instance_plan` : A unique ID for the Plan.\n`__meta_vultr_instance_main_ip` : The main IPv4 address.\n`__meta_vultr_instance_internal_ip` : The private IP address.\n`__meta_vultr_instance_main_ipv6` : The main IPv6 address.\n`__meta_vultr_instance_features` : List of features that are available to the instance.\n`__meta_vultr_instance_tags` : List of tags associated with the instance.\n`__meta_vultr_instance_hostname` : The hostname for this instance.\n`__meta_vultr_instance_server_status` : The server health status.\n`__meta_vultr_instance_vcpu_count` : Number of vCPUs.\n`__meta_vultr_instance_ram_mb` : The amount of RAM in MB.\n`__meta_vultr_instance_disk_gb` : The size of the disk in GB.\n`__meta_vultr_instance_allowed_bandwidth_gb` : Monthly bandwidth quota in GB.\n\n```yaml\nAuthentication information used to authenticate to the API server.\nNote that `basic_auth` and `authorization` options are\nmutually exclusive.\npassword and password_file are mutually exclusive.\nOptional HTTP basic authentication information, not currently supported by Vultr.\nbasic_auth:\n  [ username:  ]\n  [ password:  ]\n  [ password_file:  ]\nOptional `Authorization` header configuration.\nauthorization:\n  # Sets the authentication type.\n  [ type:  | default: Bearer ]\n  # Sets the credentials. It is mutually exclusive with\n  # `credentials_file`.\n  [ credentials:  ]\n  # Sets the credentials to the credentials read from the configured file.\n  # It is mutually exclusive with `credentials`.\n  [ credentials_file:  ]\nOptional OAuth 2.0 configuration.\nCannot be used at the same time as basic_auth or authorization.\noauth2:\n  [  ]\nOptional proxy URL.\n[ proxy_url:  ]\nSpecifies headers to send to proxies during CONNECT requests.\n[ proxy_connect_header:\n  [ : [, ...] ] ]\nConfigure whether HTTP requests follow HTTP 3xx redirects.\n[ follow_redirects:  | default = true ]\nWhether to enable HTTP2.\n[ enable_http2:  | default: true ]\nTLS configuration.\ntls_config:\n  [  ]\nThe port to scrape metrics from.\n[ port:  | default = 80 ]\nThe time after which the instances are refreshed.\n[ refresh_interval:  | default = 60s ]\n```\n`<static_config>`\nA `static_config` allows specifying a list of targets and a common label set\nfor them.  It is the canonical way to specify static targets in a scrape\nconfiguration.\n```yaml\nThe targets specified by the static config.\ntargets:\n  [ - '' ]\nLabels assigned to all metrics scraped from the targets.\nlabels:\n  [ :  ... ]\n```\n`<relabel_config>`\nRelabeling is a powerful tool to dynamically rewrite the label set of a target before\nit gets scraped. Multiple relabeling steps can be configured per scrape configuration.\nThey are applied to the label set of each target in order of their appearance\nin the configuration file.\nInitially, aside from the configured per-target labels, a target's `job`\nlabel is set to the `job_name` value of the respective scrape configuration.\nThe `__address__` label is set to the `<host>:<port>` address of the target.\nAfter relabeling, the `instance` label is set to the value of `__address__` by default if\nit was not set during relabeling. The `__scheme__` and `__metrics_path__` labels\nare set to the scheme and metrics path of the target respectively. The `__param_<name>`\nlabel is set to the value of the first passed URL parameter called `<name>`.\nThe `__scrape_interval__` and `__scrape_timeout__` labels are set to the target's\ninterval and timeout. This is experimental and could change in the future.\nAdditional labels prefixed with `__meta_` may be available during the\nrelabeling phase. They are set by the service discovery mechanism that provided\nthe target and vary between mechanisms.\nLabels starting with `__` will be removed from the label set after target\nrelabeling is completed.\nIf a relabeling step needs to store a label value only temporarily (as the\ninput to a subsequent relabeling step), use the `__tmp` label name prefix. This\nprefix is guaranteed to never be used by Prometheus itself.\n```yaml\nThe source labels select values from existing labels. Their content is concatenated\nusing the configured separator and matched against the configured regular expression\nfor the replace, keep, and drop actions.\n[ source_labels: '['  [, ...] ']' ]\nSeparator placed between concatenated source label values.\n[ separator:  | default = ; ]\nLabel to which the resulting value is written in a replace action.\nIt is mandatory for replace actions. Regex capture groups are available.\n[ target_label:  ]\nRegular expression against which the extracted value is matched.\n[ regex:  | default = (.*) ]\nModulus to take of the hash of the source label values.\n[ modulus:  ]\nReplacement value against which a regex replace is performed if the\nregular expression matches. Regex capture groups are available.\n[ replacement:  | default = $1 ]\nAction to perform based on regex matching.\n[ action:  | default = replace ]\n```\n`<regex>` is any valid\nRE2 regular expression. It is\nrequired for the `replace`, `keep`, `drop`, `labelmap`,`labeldrop` and `labelkeep` actions. The regex is\nanchored on both ends. To un-anchor the regex, use `.*<regex>.*`.\n`<relabel_action>` determines the relabeling action to take:\n\n`replace`: Match `regex` against the concatenated `source_labels`. Then, set\n  `target_label` to `replacement`, with match group references\n  (`${1}`, `${2}`, ...) in `replacement` substituted by their value. If `regex`\n  does not match, no replacement takes place.\n`lowercase`: Maps the concatenated `source_labels` to their lower case.\n`uppercase`: Maps the concatenated `source_labels` to their upper case.\n`keep`: Drop targets for which `regex` does not match the concatenated `source_labels`.\n`drop`: Drop targets for which `regex` matches the concatenated `source_labels`.\n`keepequal`: Drop targets for which the concatenated `source_labels` do not match `target_label`.\n`dropequal`: Drop targets for which the concatenated `source_labels` do match `target_label`.\n`hashmod`: Set `target_label` to the `modulus` of a hash of the concatenated `source_labels`.\n`labelmap`: Match `regex` against all source label names, not just those specified in `source_labels`. Then\n   copy the values of the matching labels  to label names given by `replacement` with match\n   group references (`${1}`, `${2}`, ...) in `replacement` substituted by their value.\n`labeldrop`: Match `regex` against all label names. Any label that matches will be\n  removed from the set of labels.\n`labelkeep`: Match `regex` against all label names. Any label that does not match will be\n  removed from the set of labels.\n\nCare must be taken with `labeldrop` and `labelkeep` to ensure that metrics are\nstill uniquely labeled once the labels are removed.\n`<metric_relabel_configs>`\nMetric relabeling is applied to samples as the last step before ingestion. It\nhas the same configuration format and actions as target relabeling. Metric\nrelabeling does not apply to automatically generated timeseries such as `up`.\nOne use for this is to exclude time series that are too expensive to ingest.\n`<alert_relabel_configs>`\nAlert relabeling is applied to alerts before they are sent to the Alertmanager.\nIt has the same configuration format and actions as target relabeling. Alert\nrelabeling is applied after external labels.\nOne use for this is ensuring a HA pair of Prometheus servers with different\nexternal labels send identical alerts.\n`<alertmanager_config>`\nAn `alertmanager_config` section specifies Alertmanager instances the Prometheus\nserver sends alerts to. It also provides parameters to configure how to\ncommunicate with these Alertmanagers.\nAlertmanagers may be statically configured via the `static_configs` parameter or\ndynamically discovered using one of the supported service-discovery mechanisms.\nAdditionally, `relabel_configs` allow selecting Alertmanagers from discovered\nentities and provide advanced modifications to the used API path, which is exposed\nthrough the `__alerts_path__` label.\n```yaml\nPer-target Alertmanager timeout when pushing alerts.\n[ timeout:  | default = 10s ]\nThe api version of Alertmanager.\n[ api_version:  | default = v2 ]\nPrefix for the HTTP path alerts are pushed to.\n[ path_prefix:  | default = / ]\nConfigures the protocol scheme used for requests.\n[ scheme:  | default = http ]\nSets the `Authorization` header on every request with the\nconfigured username and password.\npassword and password_file are mutually exclusive.\nbasic_auth:\n  [ username:  ]\n  [ password:  ]\n  [ password_file:  ]\nOptional `Authorization` header configuration.\nauthorization:\n  # Sets the authentication type.\n  [ type:  | default: Bearer ]\n  # Sets the credentials. It is mutually exclusive with\n  # `credentials_file`.\n  [ credentials:  ]\n  # Sets the credentials to the credentials read from the configured file.\n  # It is mutually exclusive with `credentials`.\n  [ credentials_file:  ]\nOptional OAuth 2.0 configuration.\nCannot be used at the same time as basic_auth or authorization.\noauth2:\n  [  ]\nConfigures the scrape request's TLS settings.\ntls_config:\n  [  ]\nOptional proxy URL.\n[ proxy_url:  ]\nSpecifies headers to send to proxies during CONNECT requests.\n[ proxy_connect_header:\n  [ : [, ...] ] ]\nConfigure whether HTTP requests follow HTTP 3xx redirects.\n[ follow_redirects:  | default = true ]\nWhether to enable HTTP2.\n[ enable_http2:  | default: true ]\nList of Azure service discovery configurations.\nazure_sd_configs:\n  [ -  ... ]\nList of Consul service discovery configurations.\nconsul_sd_configs:\n  [ -  ... ]\nList of DNS service discovery configurations.\ndns_sd_configs:\n  [ -  ... ]\nList of EC2 service discovery configurations.\nec2_sd_configs:\n  [ -  ... ]\nList of Eureka service discovery configurations.\neureka_sd_configs:\n  [ -  ... ]\nList of file service discovery configurations.\nfile_sd_configs:\n  [ -  ... ]\nList of DigitalOcean service discovery configurations.\ndigitalocean_sd_configs:\n  [ -  ... ]\nList of Docker service discovery configurations.\ndocker_sd_configs:\n  [ -  ... ]\nList of Docker Swarm service discovery configurations.\ndockerswarm_sd_configs:\n  [ -  ... ]\nList of GCE service discovery configurations.\ngce_sd_configs:\n  [ -  ... ]\nList of Hetzner service discovery configurations.\nhetzner_sd_configs:\n  [ -  ... ]\nList of HTTP service discovery configurations.\nhttp_sd_configs:\n  [ -  ... ]\n# List of IONOS service discovery configurations.\nionos_sd_configs:\n  [ -  ... ]\nList of Kubernetes service discovery configurations.\nkubernetes_sd_configs:\n  [ -  ... ]\nList of Lightsail service discovery configurations.\nlightsail_sd_configs:\n  [ -  ... ]\nList of Linode service discovery configurations.\nlinode_sd_configs:\n  [ -  ... ]\nList of Marathon service discovery configurations.\nmarathon_sd_configs:\n  [ -  ... ]\nList of AirBnB's Nerve service discovery configurations.\nnerve_sd_configs:\n  [ -  ... ]\nList of Nomad service discovery configurations.\nnomad_sd_configs:\n  [ -  ... ]\nList of OpenStack service discovery configurations.\nopenstack_sd_configs:\n  [ -  ... ]\nList of OVHcloud service discovery configurations.\novhcloud_sd_configs:\n  [ -  ... ]\nList of PuppetDB service discovery configurations.\npuppetdb_sd_configs:\n  [ -  ... ]\nList of Scaleway service discovery configurations.\nscaleway_sd_configs:\n  [ -  ... ]\nList of Zookeeper Serverset service discovery configurations.\nserverset_sd_configs:\n  [ -  ... ]\nList of Triton service discovery configurations.\ntriton_sd_configs:\n  [ -  ... ]\nList of Uyuni service discovery configurations.\nuyuni_sd_configs:\n  [ -  ... ]\nList of Vultr service discovery configurations.\nvultr_sd_configs:\n  [ -  ... ]\nList of labeled statically configured Alertmanagers.\nstatic_configs:\n  [ -  ... ]\nList of Alertmanager relabel configurations.\nrelabel_configs:\n  [ -  ... ]\n```\n`<remote_write>`\n`write_relabel_configs` is relabeling applied to samples before sending them\nto the remote endpoint. Write relabeling is applied after external labels. This\ncould be used to limit which samples are sent.\nThere is a small demo of how to use\nthis functionality.\n```yaml\nThe URL of the endpoint to send samples to.\nurl: \nTimeout for requests to the remote write endpoint.\n[ remote_timeout:  | default = 30s ]\nCustom HTTP headers to be sent along with each remote write request.\nBe aware that headers that are set by Prometheus itself can't be overwritten.\nheaders:\n  [ :  ... ]\nList of remote write relabel configurations.\nwrite_relabel_configs:\n  [ -  ... ]\nName of the remote write config, which if specified must be unique among remote write configs.\nThe name will be used in metrics and logging in place of a generated value to help users distinguish between\nremote write configs.\n[ name:  ]\nEnables sending of exemplars over remote write. Note that exemplar storage itself must be enabled for exemplars to be scraped in the first place.\n[ send_exemplars:  | default = false ]\nEnables sending of native histograms, also known as sparse histograms, over remote write.\n[ send_native_histograms:  | default = false ]\nSets the `Authorization` header on every remote write request with the\nconfigured username and password.\npassword and password_file are mutually exclusive.\nbasic_auth:\n  [ username:  ]\n  [ password:  ]\n  [ password_file:  ]\nOptional `Authorization` header configuration.\nauthorization:\n  # Sets the authentication type.\n  [ type:  | default: Bearer ]\n  # Sets the credentials. It is mutually exclusive with\n  # `credentials_file`.\n  [ credentials:  ]\n  # Sets the credentials to the credentials read from the configured file.\n  # It is mutually exclusive with `credentials`.\n  [ credentials_file:  ]\nOptionally configures AWS's Signature Verification 4 signing process to\nsign requests. Cannot be set at the same time as basic_auth, authorization, or oauth2.\nTo use the default credentials from the AWS SDK, use `sigv4: {}`.\nsigv4:\n  # The AWS region. If blank, the region from the default credentials chain\n  # is used.\n  [ region:  ]\n# The AWS API keys. If blank, the environment variables `AWS_ACCESS_KEY_ID`\n  # and `AWS_SECRET_ACCESS_KEY` are used.\n  [ access_key:  ]\n  [ secret_key:  ]\n# Named AWS profile used to authenticate.\n  [ profile:  ]\n# AWS Role ARN, an alternative to using AWS API keys.\n  [ role_arn:  ]\nOptional OAuth 2.0 configuration.\nCannot be used at the same time as basic_auth, authorization, or sigv4.\noauth2:\n  [  ]\nConfigures the remote write request's TLS settings.\ntls_config:\n  [  ]\nOptional proxy URL.\n[ proxy_url:  ]\nSpecifies headers to send to proxies during CONNECT requests.\n[ proxy_connect_header:\n  [ : [, ...] ] ]\nConfigure whether HTTP requests follow HTTP 3xx redirects.\n[ follow_redirects:  | default = true ]\nWhether to enable HTTP2.\n[ enable_http2:  | default: true ]\nConfigures the queue used to write to remote storage.\nqueue_config:\n  # Number of samples to buffer per shard before we block reading of more\n  # samples from the WAL. It is recommended to have enough capacity in each\n  # shard to buffer several requests to keep throughput up while processing\n  # occasional slow remote requests.\n  [ capacity:  | default = 2500 ]\n  # Maximum number of shards, i.e. amount of concurrency.\n  [ max_shards:  | default = 200 ]\n  # Minimum number of shards, i.e. amount of concurrency.\n  [ min_shards:  | default = 1 ]\n  # Maximum number of samples per send.\n  [ max_samples_per_send:  | default = 500]\n  # Maximum time a sample will wait in buffer.\n  [ batch_send_deadline:  | default = 5s ]\n  # Initial retry delay. Gets doubled for every retry.\n  [ min_backoff:  | default = 30ms ]\n  # Maximum retry delay.\n  [ max_backoff:  | default = 5s ]\n  # Retry upon receiving a 429 status code from the remote-write storage.\n  # This is experimental and might change in the future.\n  [ retry_on_http_429:  | default = false ]\nConfigures the sending of series metadata to remote storage.\nMetadata configuration is subject to change at any point\nor be removed in future releases.\nmetadata_config:\n  # Whether metric metadata is sent to remote storage or not.\n  [ send:  | default = true ]\n  # How frequently metric metadata is sent to remote storage.\n  [ send_interval:  | default = 1m ]\n  # Maximum number of samples per send.\n  [ max_samples_per_send:  | default = 500]\n```\nThere is a list of\nintegrations\nwith this feature.\n`<remote_read>`\n```yaml\nThe URL of the endpoint to query from.\nurl: \nName of the remote read config, which if specified must be unique among remote read configs.\nThe name will be used in metrics and logging in place of a generated value to help users distinguish between\nremote read configs.\n[ name:  ]\nAn optional list of equality matchers which have to be\npresent in a selector to query the remote read endpoint.\nrequired_matchers:\n  [ :  ... ]\nTimeout for requests to the remote read endpoint.\n[ remote_timeout:  | default = 1m ]\nCustom HTTP headers to be sent along with each remote read request.\nBe aware that headers that are set by Prometheus itself can't be overwritten.\nheaders:\n  [ :  ... ]\nWhether reads should be made for queries for time ranges that\nthe local storage should have complete data for.\n[ read_recent:  | default = false ]\nSets the `Authorization` header on every remote read request with the\nconfigured username and password.\npassword and password_file are mutually exclusive.\nbasic_auth:\n  [ username:  ]\n  [ password:  ]\n  [ password_file:  ]\nOptional `Authorization` header configuration.\nauthorization:\n  # Sets the authentication type.\n  [ type:  | default: Bearer ]\n  # Sets the credentials. It is mutually exclusive with\n  # `credentials_file`.\n  [ credentials:  ]\n  # Sets the credentials to the credentials read from the configured file.\n  # It is mutually exclusive with `credentials`.\n  [ credentials_file:  ]\nOptional OAuth 2.0 configuration.\nCannot be used at the same time as basic_auth or authorization.\noauth2:\n  [  ]\nConfigures the remote read request's TLS settings.\ntls_config:\n  [  ]\nOptional proxy URL.\n[ proxy_url:  ]\nSpecifies headers to send to proxies during CONNECT requests.\n[ proxy_connect_header:\n  [ : [, ...] ] ]\nConfigure whether HTTP requests follow HTTP 3xx redirects.\n[ follow_redirects:  | default = true ]\nWhether to enable HTTP2.\n[ enable_http2:  | default: true ]\nWhether to use the external labels as selectors for the remote read endpoint.\n[ filter_external_labels:  | default = true ]\n```\nThere is a list of\nintegrations\nwith this feature.\n`<tsdb>`\n`tsdb` lets you configure the runtime-reloadable configuration settings of the TSDB.\nNOTE: Out-of-order ingestion is an experimental feature, but you do not need any additional flag to enable it. Setting `out_of_order_time_window` to a positive duration enables it.\n```yaml\nConfigures how old an out-of-order/out-of-bounds sample can be w.r.t. the TSDB max time.\nAn out-of-order/out-of-bounds sample is ingested into the TSDB as long as the timestamp\nof the sample is >= TSDB.MaxTime-out_of_order_time_window.\n\nWhen out_of_order_time_window is >0, the errors out-of-order and out-of-bounds are\ncombined into a single error called 'too-old'; a sample is either (a) ingestible\ninto the TSDB, i.e. it is an in-order sample or an out-of-order/out-of-bounds sample\nthat is within the out-of-order window, or (b) too-old, i.e. not in-order\nand before the out-of-order window.\n[ out_of_order_time_window:  | default = 0s ]\n```\n`<exemplars>`\nNote that exemplar storage is still considered experimental and must be enabled via `--enable-feature=exemplar-storage`.\n```yaml\nConfigures the maximum size of the circular buffer used to store exemplars for all series. Resizable during runtime.\n[ max_exemplars:  | default = 100000 ]\n```\n`<tracing_config>`\n`tracing_config` configures exporting traces from Prometheus to a tracing backend via the OTLP protocol. Tracing is currently an experimental feature and could change in the future.\n```yaml\nClient used to export the traces. Options are 'http' or 'grpc'.\n[ client_type:  | default = grpc ]\nEndpoint to send the traces to. Should be provided in format :.\n[ endpoint:  ]\nSets the probability a given trace will be sampled. Must be a float from 0 through 1.\n[ sampling_fraction:  | default = 0 ]\nIf disabled, the client will use a secure connection.\n[ insecure:  | default = false ]\nKey-value pairs to be used as headers associated with gRPC or HTTP requests.\nheaders:\n  [ :  ... ]\nCompression key for supported compression types. Supported compression: gzip.\n[ compression:  ]\nMaximum time the exporter will wait for each batch export.\n[ timeout:  | default = 10s ]\nTLS configuration.\ntls_config:\n  [  ]",
    "tag": "prometheus"
  },
  {
    "title": "Template examples",
    "source": "https://github.com/prometheus/prometheus/tree/main/docs/configuration/template_examples.md",
    "content": "\ntitle: Template examples\nsort_rank: 4\n\nTemplate examples\nPrometheus supports templating in the annotations and labels of alerts,\nas well as in served console pages. Templates have the ability to run\nqueries against the local database, iterate over data, use conditionals,\nformat data, etc. The Prometheus templating language is based on the Go\ntemplating system.\nSimple alert field templates\n`alert: InstanceDown\nexpr: up == 0\nfor: 5m\nlabels:\n  severity: page\nannotations:\n  summary: \"Instance {{$labels.instance}} down\"\n  description: \"{{$labels.instance}} of job {{$labels.job}} has been down for more than 5 minutes.\"`\nAlert field templates will be executed during every rule iteration for each\nalert that fires, so keep any queries and templates lightweight. If you have a\nneed for more complicated templates for alerts, it is recommended to link to a\nconsole instead.\nSimple iteration\nThis displays a list of instances, and whether they are up:\n`go\n{{ range query \"up\" }}\n  {{ .Labels.instance }} {{ .Value }}\n{{ end }}`\nThe special `.` variable contains the value of the current sample for each loop iteration.\nDisplay one value\n`go\n{{ with query \"some_metric{instance='someinstance'}\" }}\n  {{ . | first | value | humanize }}\n{{ end }}`\nGo and Go's templating language are both strongly typed, so one must check that\nsamples were returned to avoid an execution error. For example this could\nhappen if a scrape or rule evaluation has not run yet, or a host was down.\nThe included `prom_query_drilldown` template handles this, allows for\nformatting of results, and linking to the expression browser.\nUsing console URL parameters\n`go\n{{ with printf \"node_memory_MemTotal{job='node',instance='%s'}\" .Params.instance | query }}\n  {{ . | first | value | humanize1024 }}B\n{{ end }}`\nIf accessed as `console.html?instance=hostname`, `.Params.instance` will evaluate to `hostname`.\nAdvanced iteration\n```html\n\n{{ range printf \"node_network_receive_bytes{job='node',instance='%s',device!='lo'}\" .Params.instance | query | sortByLabel \"device\"}}\n  {{ .Labels.device }}\n\nReceived\n{{ with printf \"rate(node_network_receive_bytes{job='node',instance='%s',device='%s'}[5m])\" .Labels.instance .Labels.device | query }}{{ . | first | value | humanize }}B/s{{end}}\n\n\nTransmitted\n{{ with printf \"rate(node_network_transmit_bytes{job='node',instance='%s',device='%s'}[5m])\" .Labels.instance .Labels.device | query }}{{ . | first | value | humanize }}B/s{{end}}\n{{ end }}\n\n```\nHere we iterate over all network devices and display the network traffic for each.\nAs the `range` action does not specify a variable, `.Params.instance` is not\navailable inside the loop as `.` is now the loop variable.\nDefining reusable templates\nPrometheus supports defining templates that can be reused. This is particularly\npowerful when combined with\nconsole library support, allowing\nsharing of templates across consoles.\n```go\n{{/ Define the template /}}\n{{define \"myTemplate\"}}\n  do something\n{{end}}\n{{/ Use the template /}}\n{{template \"myTemplate\"}}\n```\nTemplates are limited to one argument. The `args` function can be used to wrap multiple arguments.\n```go\n{{define \"myMultiArgTemplate\"}}\n  First argument: {{.arg0}}\n  Second argument: {{.arg1}}\n{{end}}\n{{template \"myMultiArgTemplate\" (args 1 2)}}",
    "tag": "prometheus"
  },
  {
    "title": "Query examples",
    "source": "https://github.com/prometheus/prometheus/tree/main/docs/querying/examples.md",
    "content": "\ntitle: Querying examples\nnav_title: Examples\nsort_rank: 4\n\nQuery examples\nSimple time series selection\nReturn all time series with the metric `http_requests_total`:\n\n\n```http_requests_total\n```\n\n\nReturn all time series with the metric `http_requests_total` and the given\n`job` and `handler` labels:\n\n\n```http_requests_total{job=\"apiserver\", handler=\"/api/comments\"}\n```\n\n\nReturn a whole range of time (in this case 5 minutes up to the query time)\nfor the same vector, making it a range vector:\n\n\n```http_requests_total{job=\"apiserver\", handler=\"/api/comments\"}[5m]\n```\n\n\nNote that an expression resulting in a range vector cannot be graphed directly,\nbut viewed in the tabular (\"Console\") view of the expression browser.\nUsing regular expressions, you could select time series only for jobs whose\nname match a certain pattern, in this case, all jobs that end with `server`:\n\n\n```http_requests_total{job=~\".*server\"}\n```\n\n\nAll regular expressions in Prometheus use RE2\nsyntax.\nTo select all HTTP status codes except 4xx ones, you could run:\n\n\n```http_requests_total{status!~\"4..\"}\n```\n\n\nSubquery\nReturn the 5-minute rate of the `http_requests_total` metric for the past 30 minutes, with a resolution of 1 minute.\n\n\n```rate(http_requests_total[5m])[30m:1m]\n```\n\n\nThis is an example of a nested subquery. The subquery for the `deriv` function uses the default resolution. Note that using subqueries unnecessarily is unwise.\n\n\n```max_over_time(deriv(rate(distance_covered_total[5s])[30s:5s])[10m:])\n```\n\n\nUsing functions, operators, etc.\nReturn the per-second rate for all time series with the `http_requests_total`\nmetric name, as measured over the last 5 minutes:\n\n\n```rate(http_requests_total[5m])\n```\n\n\nAssuming that the `http_requests_total` time series all have the labels `job`\n(fanout by job name) and `instance` (fanout by instance of the job), we might\nwant to sum over the rate of all instances, so we get fewer output time series,\nbut still preserve the `job` dimension:\n\n\n```sum by (job) (\n  rate(http_requests_total[5m])\n)\n```\n\n\nIf we have two different metrics with the same dimensional labels, we can apply\nbinary operators to them and elements on both sides with the same label set\nwill get matched and propagated to the output. For example, this expression\nreturns the unused memory in MiB for every instance (on a fictional cluster\nscheduler exposing these metrics about the instances it runs):\n\n\n```(instance_memory_limit_bytes - instance_memory_usage_bytes) / 1024 / 1024\n```\n\n\nThe same expression, but summed by application, could be written like this:\n\n\n```sum by (app, proc) (\n  instance_memory_limit_bytes - instance_memory_usage_bytes\n) / 1024 / 1024\n```\n\n\nIf the same fictional cluster scheduler exposed CPU usage metrics like the\nfollowing for every instance:\n\n\n```instance_cpu_time_ns{app=\"lion\", proc=\"web\", rev=\"34d0f99\", env=\"prod\", job=\"cluster-manager\"}\ninstance_cpu_time_ns{app=\"elephant\", proc=\"worker\", rev=\"34d0f99\", env=\"prod\", job=\"cluster-manager\"}\ninstance_cpu_time_ns{app=\"turtle\", proc=\"api\", rev=\"4d3a513\", env=\"prod\", job=\"cluster-manager\"}\ninstance_cpu_time_ns{app=\"fox\", proc=\"widget\", rev=\"4d3a513\", env=\"prod\", job=\"cluster-manager\"}\n...\n```\n\n\n...we could get the top 3 CPU users grouped by application (`app`) and process\ntype (`proc`) like this:\n\n\n```topk(3, sum by (app, proc) (rate(instance_cpu_time_ns[5m])))\n```\n\n\nAssuming this metric contains one time series per running instance, you could\ncount the number of running instances per application like this:",
    "tag": "prometheus"
  },
  {
    "title": "Querying Prometheus",
    "source": "https://github.com/prometheus/prometheus/tree/main/docs/querying/basics.md",
    "content": "\ntitle: Querying basics\nnav_title: Basics\nsort_rank: 1\n\nQuerying Prometheus\nPrometheus provides a functional query language called PromQL (Prometheus Query\nLanguage) that lets the user select and aggregate time series data in real\ntime. The result of an expression can either be shown as a graph, viewed as\ntabular data in Prometheus's expression browser, or consumed by external\nsystems via the HTTP API.\nExamples\nThis document is meant as a reference. For learning, it might be easier to\nstart with a couple of examples.\nExpression language data types\nIn Prometheus's expression language, an expression or sub-expression can\nevaluate to one of four types:\n\nInstant vector - a set of time series containing a single sample for each time series, all sharing the same timestamp\nRange vector - a set of time series containing a range of data points over time for each time series\nScalar - a simple numeric floating point value\nString - a simple string value; currently unused\n\nDepending on the use-case (e.g. when graphing vs. displaying the output of an\nexpression), only some of these types are legal as the result from a\nuser-specified expression. For example, an expression that returns an instant\nvector is the only type that can be directly graphed.\nNotes about the experimental native histograms:\n\nIngesting native histograms has to be enabled via a feature\n  flag.\nOnce native histograms have been ingested into the TSDB (and even after\n  disabling the feature flag again), both instant vectors and range vectors may\n  now contain samples that aren't simple floating point numbers (float samples)\n  but complete histograms (histogram samples). A vector may contain a mix of\n  float samples and histogram samples.\n\nLiterals\nString literals\nStrings may be specified as literals in single quotes, double quotes or\nbackticks.\nPromQL follows the same escaping rules as\nGo. In single or double quotes a\nbackslash begins an escape sequence, which may be followed by `a`, `b`, `f`,\n`n`, `r`, `t`, `v` or `\\`. Specific characters can be provided using octal\n(`\\nnn`) or hexadecimal (`\\xnn`, `\\unnnn` and `\\Unnnnnnnn`).\nNo escaping is processed inside backticks. Unlike Go, Prometheus does not discard newlines inside backticks.\nExample:\n\n\n```\"this is a string\"\n'these are unescaped: \\n \\\\ \\t'\n`these are not unescaped: \\n ' \" \\t`\n```\n\n\nFloat literals\nScalar float values can be written as literal integer or floating-point numbers in the format (whitespace only included for better readability):\n\n\n```[-+]?(\n      [0-9]*\\.?[0-9]+([eE][-+]?[0-9]+)?\n    | 0[xX][0-9a-fA-F]+\n    | [nN][aA][nN]\n    | [iI][nN][fF]\n)\n```\n\n\nExamples:\n\n\n```23\n-2.43\n3.4e-9\n0x8f\n-Inf\nNaN\n```\n\n\nTime series Selectors\nInstant vector selectors\nInstant vector selectors allow the selection of a set of time series and a\nsingle sample value for each at a given timestamp (instant): in the simplest\nform, only a metric name is specified. This results in an instant vector\ncontaining elements for all time series that have this metric name.\nThis example selects all time series that have the `http_requests_total` metric\nname:\n\n\n```http_requests_total\n```\n\n\nIt is possible to filter these time series further by appending a comma separated list of label\nmatchers in curly braces (`{}`).\nThis example selects only those time series with the `http_requests_total`\nmetric name that also have the `job` label set to `prometheus` and their\n`group` label set to `canary`:\n\n\n```http_requests_total{job=\"prometheus\",group=\"canary\"}\n```\n\n\nIt is also possible to negatively match a label value, or to match label values\nagainst regular expressions. The following label matching operators exist:\n\n`=`: Select labels that are exactly equal to the provided string.\n`!=`: Select labels that are not equal to the provided string.\n`=~`: Select labels that regex-match the provided string.\n`!~`: Select labels that do not regex-match the provided string.\n\nRegex matches are fully anchored. A match of `env=~\"foo\"` is treated as `env=~\"^foo$\"`.\nFor example, this selects all `http_requests_total` time series for `staging`,\n`testing`, and `development` environments and HTTP methods other than `GET`.\n\n\n```http_requests_total{environment=~\"staging|testing|development\",method!=\"GET\"}\n```\n\n\nLabel matchers that match empty label values also select all time series that\ndo not have the specific label set at all. It is possible to have multiple matchers for the same label name.\nVector selectors must either specify a name or at least one label matcher\nthat does not match the empty string. The following expression is illegal:\n\n\n```{job=~\".*\"} # Bad!\n```\n\n\nIn contrast, these expressions are valid as they both have a selector that does not\nmatch empty label values.\n\n\n```{job=~\".+\"}              # Good!\n{job=~\".*\",method=\"get\"} # Good!\n```\n\n\nLabel matchers can also be applied to metric names by matching against the internal\n`__name__` label. For example, the expression `http_requests_total` is equivalent to\n`{__name__=\"http_requests_total\"}`. Matchers other than `=` (`!=`, `=~`, `!~`) may also be used.\nThe following expression selects all metrics that have a name starting with `job:`:\n\n\n```{__name__=~\"job:.*\"}\n```\n\n\nThe metric name must not be one of the keywords `bool`, `on`, `ignoring`, `group_left` and `group_right`. The following expression is illegal:\n\n\n```on{} # Bad!\n```\n\n\nA workaround for this restriction is to use the `__name__` label:\n\n\n```{__name__=\"on\"} # Good!\n```\n\n\nAll regular expressions in Prometheus use RE2\nsyntax.\nRange Vector Selectors\nRange vector literals work like instant vector literals, except that they\nselect a range of samples back from the current instant. Syntactically, a time\nduration is appended in square brackets (`[]`) at the end of a\nvector selector to specify how far back in time values should be fetched for\neach resulting range vector element.\nIn this example, we select all the values we have recorded within the last 5\nminutes for all time series that have the metric name `http_requests_total` and\na `job` label set to `prometheus`:\n\n\n```http_requests_total{job=\"prometheus\"}[5m]\n```\n\n\nTime Durations\nTime durations are specified as a number, followed immediately by one of the\nfollowing units:\n\n`ms` - milliseconds\n`s` - seconds\n`m` - minutes\n`h` - hours\n`d` - days - assuming a day has always 24h\n`w` - weeks - assuming a week has always 7d\n`y` - years - assuming a year has always 365d\n\nTime durations can be combined, by concatenation. Units must be ordered from the\nlongest to the shortest. A given unit must only appear once in a time duration.\nHere are some examples of valid time durations:\n\n\n```5h\n1h30m\n5m\n10s\n```\n\n\nOffset modifier\nThe `offset` modifier allows changing the time offset for individual\ninstant and range vectors in a query.\nFor example, the following expression returns the value of\n`http_requests_total` 5 minutes in the past relative to the current\nquery evaluation time:\n\n\n```http_requests_total offset 5m\n```\n\n\nNote that the `offset` modifier always needs to follow the selector\nimmediately, i.e. the following would be correct:\n\n\n```sum(http_requests_total{method=\"GET\"} offset 5m) // GOOD.\n```\n\n\nWhile the following would be incorrect:\n\n\n```sum(http_requests_total{method=\"GET\"}) offset 5m // INVALID.\n```\n\n\nThe same works for range vectors. This returns the 5-minute rate that\n`http_requests_total` had a week ago:\n\n\n```rate(http_requests_total[5m] offset 1w)\n```\n\n\nFor comparisons with temporal shifts forward in time, a negative offset\ncan be specified:\n\n\n```rate(http_requests_total[5m] offset -1w)\n```\n\n\nNote that this allows a query to look ahead of its evaluation time.\n@ modifier\nThe `@` modifier allows changing the evaluation time for individual instant\nand range vectors in a query. The time supplied to the `@` modifier\nis a unix timestamp and described with a float literal. \nFor example, the following expression returns the value of\n`http_requests_total` at `2021-01-04T07:40:00+00:00`:\n\n\n```http_requests_total @ 1609746000\n```\n\n\nNote that the `@` modifier always needs to follow the selector\nimmediately, i.e. the following would be correct:\n\n\n```sum(http_requests_total{method=\"GET\"} @ 1609746000) // GOOD.\n```\n\n\nWhile the following would be incorrect:\n\n\n```sum(http_requests_total{method=\"GET\"}) @ 1609746000 // INVALID.\n```\n\n\nThe same works for range vectors. This returns the 5-minute rate that\n`http_requests_total` had at `2021-01-04T07:40:00+00:00`:\n\n\n```rate(http_requests_total[5m] @ 1609746000)\n```\n\n\nThe `@` modifier supports all representation of float literals described\nabove within the limits of `int64`. It can also be used along\nwith the `offset` modifier where the offset is applied relative to the `@`\nmodifier time irrespective of which modifier is written first.\nThese 2 queries will produce the same result.\n\n\n```# offset after @\nhttp_requests_total @ 1609746000 offset 5m\n# offset before @\nhttp_requests_total offset 5m @ 1609746000\n```\n\n\nAdditionally, `start()` and `end()` can also be used as values for the `@` modifier as special values.\nFor a range query, they resolve to the start and end of the range query respectively and remain the same for all steps.\nFor an instant query, `start()` and `end()` both resolve to the evaluation time.\n\n\n```http_requests_total @ start()\nrate(http_requests_total[5m] @ end())\n```\n\n\nNote that the `@` modifier allows a query to look ahead of its evaluation time.\nSubquery\nSubquery allows you to run an instant query for a given range and resolution. The result of a subquery is a range vector.\nSyntax: `<instant_query> '[' <range> ':' [<resolution>] ']' [ @ <float_literal> ] [ offset <duration> ]`\n\n`<resolution>` is optional. Default is the global evaluation interval.\n\nOperators\nPrometheus supports many binary and aggregation operators. These are described\nin detail in the expression language operators page.\nFunctions\nPrometheus supports several functions to operate on data. These are described\nin detail in the expression language functions page.\nComments\nPromQL supports line comments that start with `#`. Example:\n\n\n```    # This is a comment\n```\n\n\nGotchas\nStaleness\nWhen queries are run, timestamps at which to sample data are selected\nindependently of the actual present time series data. This is mainly to support\ncases like aggregation (`sum`, `avg`, and so on), where multiple aggregated\ntime series do not exactly align in time. Because of their independence,\nPrometheus needs to assign a value at those timestamps for each relevant time\nseries. It does so by simply taking the newest sample before this timestamp.\nIf a target scrape or rule evaluation no longer returns a sample for a time\nseries that was previously present, that time series will be marked as stale.\nIf a target is removed, its previously returned time series will be marked as\nstale soon afterwards.\nIf a query is evaluated at a sampling timestamp after a time series is marked\nstale, then no value is returned for that time series. If new samples are\nsubsequently ingested for that time series, they will be returned as normal.\nIf no sample is found (by default) 5 minutes before a sampling timestamp,\nno value is returned for that time series at this point in time. This\neffectively means that time series \"disappear\" from graphs at times where their\nlatest collected sample is older than 5 minutes or after they are marked stale.\nStaleness will not be marked for time series that have timestamps included in\ntheir scrapes. Only the 5 minute threshold will be applied in that case.\nAvoiding slow queries and overloads\nIf a query needs to operate on a very large amount of data, graphing it might\ntime out or overload the server or browser. Thus, when constructing queries\nover unknown data, always start building the query in the tabular view of\nPrometheus's expression browser until the result set seems reasonable\n(hundreds, not thousands, of time series at most).  Only when you have filtered\nor aggregated your data sufficiently, switch to graph mode. If the expression\nstill takes too long to graph ad-hoc, pre-record it via a recording\nrule.\nThis is especially relevant for Prometheus's query language, where a bare\nmetric name selector like `api_http_requests_total` could expand to thousands\nof time series with different labels. Also keep in mind that expressions which\naggregate over many time series will generate load on the server even if the\noutput is only a small number of time series. This is similar to how it would\nbe slow to sum all values of a column in a relational database, even if the",
    "tag": "prometheus"
  },
  {
    "title": "HTTP API",
    "source": "https://github.com/prometheus/prometheus/tree/main/docs/querying/api.md",
    "content": "\ntitle: HTTP API\nsort_rank: 7\n\nHTTP API\nThe current stable HTTP API is reachable under `/api/v1` on a Prometheus\nserver. Any non-breaking additions will be added under that endpoint.\nFormat overview\nThe API response format is JSON. Every successful API request returns a `2xx`\nstatus code.\nInvalid requests that reach the API handlers return a JSON error object\nand one of the following HTTP response codes:\n\n`400 Bad Request` when parameters are missing or incorrect.\n`422 Unprocessable Entity` when an expression can't be executed\n  (RFC4918).\n`503 Service Unavailable` when queries time out or abort.\n\nOther non-`2xx` codes may be returned for errors occurring before the API\nendpoint is reached.\nAn array of warnings may be returned if there are errors that do\nnot inhibit the request execution. All of the data that was successfully\ncollected will be returned in the data field.\nThe JSON response envelope format is as follows:\n```\n{\n  \"status\": \"success\" | \"error\",\n  \"data\": ,\n// Only set if status is \"error\". The data field may still hold\n  // additional data.\n  \"errorType\": \"\",\n  \"error\": \"\",\n// Only if there were warnings while executing the request.\n  // There will still be data in the data field.\n  \"warnings\": [\"\"]\n}\n```\nGeneric placeholders are defined as follows:\n\n`<rfc3339 | unix_timestamp>`: Input timestamps may be provided either in\nRFC3339 format or as a Unix timestamp\nin seconds, with optional decimal places for sub-second precision. Output\ntimestamps are always represented as Unix timestamps in seconds.\n`<series_selector>`: Prometheus time series\nselectors like `http_requests_total` or\n`http_requests_total{method=~\"(GET|POST)\"}` and need to be URL-encoded.\n`<duration>`: Prometheus duration strings.\nFor example, `5m` refers to a duration of 5 minutes.\n`<bool>`: boolean values (strings `true` and `false`).\n\nNote: Names of query parameters that may be repeated end with `[]`.\nExpression queries\nQuery language expressions may be evaluated at a single instant or over a range\nof time. The sections below describe the API endpoints for each type of\nexpression query.\nInstant queries\nThe following endpoint evaluates an instant query at a single point in time:\n`GET /api/v1/query\nPOST /api/v1/query`\nURL query parameters:\n\n`query=<string>`: Prometheus expression query string.\n`time=<rfc3339 | unix_timestamp>`: Evaluation timestamp. Optional.\n`timeout=<duration>`: Evaluation timeout. Optional. Defaults to and\n   is capped by the value of the `-query.timeout` flag.\n\nThe current server time is used if the `time` parameter is omitted.\nYou can URL-encode these parameters directly in the request body by using the `POST` method and\n`Content-Type: application/x-www-form-urlencoded` header. This is useful when specifying a large\nquery that may breach server-side URL character limits.\nThe `data` section of the query result has the following format:\n`{\n  \"resultType\": \"matrix\" | \"vector\" | \"scalar\" | \"string\",\n  \"result\": <value>\n}`\n`<value>` refers to the query result data, which has varying formats\ndepending on the `resultType`. See the expression query result\nformats.\nThe following example evaluates the expression `up` at the time\n`2015-07-01T20:10:51.781Z`:\n`json\n$ curl 'http://localhost:9090/api/v1/query?query=up&time=2015-07-01T20:10:51.781Z'\n{\n   \"status\" : \"success\",\n   \"data\" : {\n      \"resultType\" : \"vector\",\n      \"result\" : [\n         {\n            \"metric\" : {\n               \"__name__\" : \"up\",\n               \"job\" : \"prometheus\",\n               \"instance\" : \"localhost:9090\"\n            },\n            \"value\": [ 1435781451.781, \"1\" ]\n         },\n         {\n            \"metric\" : {\n               \"__name__\" : \"up\",\n               \"job\" : \"node\",\n               \"instance\" : \"localhost:9100\"\n            },\n            \"value\" : [ 1435781451.781, \"0\" ]\n         }\n      ]\n   }\n}`\nRange queries\nThe following endpoint evaluates an expression query over a range of time:\n`GET /api/v1/query_range\nPOST /api/v1/query_range`\nURL query parameters:\n\n`query=<string>`: Prometheus expression query string.\n`start=<rfc3339 | unix_timestamp>`: Start timestamp, inclusive.\n`end=<rfc3339 | unix_timestamp>`: End timestamp, inclusive.\n`step=<duration | float>`: Query resolution step width in `duration` format or float number of seconds.\n`timeout=<duration>`: Evaluation timeout. Optional. Defaults to and\n   is capped by the value of the `-query.timeout` flag.\n\nYou can URL-encode these parameters directly in the request body by using the `POST` method and\n`Content-Type: application/x-www-form-urlencoded` header. This is useful when specifying a large\nquery that may breach server-side URL character limits.\nThe `data` section of the query result has the following format:\n`{\n  \"resultType\": \"matrix\",\n  \"result\": <value>\n}`\nFor the format of the `<value>` placeholder, see the range-vector result\nformat.\nThe following example evaluates the expression `up` over a 30-second range with\na query resolution of 15 seconds.\n`json\n$ curl 'http://localhost:9090/api/v1/query_range?query=up&start=2015-07-01T20:10:30.781Z&end=2015-07-01T20:11:00.781Z&step=15s'\n{\n   \"status\" : \"success\",\n   \"data\" : {\n      \"resultType\" : \"matrix\",\n      \"result\" : [\n         {\n            \"metric\" : {\n               \"__name__\" : \"up\",\n               \"job\" : \"prometheus\",\n               \"instance\" : \"localhost:9090\"\n            },\n            \"values\" : [\n               [ 1435781430.781, \"1\" ],\n               [ 1435781445.781, \"1\" ],\n               [ 1435781460.781, \"1\" ]\n            ]\n         },\n         {\n            \"metric\" : {\n               \"__name__\" : \"up\",\n               \"job\" : \"node\",\n               \"instance\" : \"localhost:9091\"\n            },\n            \"values\" : [\n               [ 1435781430.781, \"0\" ],\n               [ 1435781445.781, \"0\" ],\n               [ 1435781460.781, \"1\" ]\n            ]\n         }\n      ]\n   }\n}`\nFormatting query expressions\nThe following endpoint formats a PromQL expression in a prettified way:\n`GET /api/v1/format_query\nPOST /api/v1/format_query`\nURL query parameters:\n\n`query=<string>`: Prometheus expression query string.\n\nYou can URL-encode these parameters directly in the request body by using the `POST` method and\n`Content-Type: application/x-www-form-urlencoded` header. This is useful when specifying a large\nquery that may breach server-side URL character limits.\nThe `data` section of the query result is a string containing the formatted query expression. Note that any comments are removed in the formatted string.\nThe following example formats the expression `foo/bar`:\n`json\n$ curl 'http://localhost:9090/api/v1/format_query?query=foo/bar'\n{\n   \"status\" : \"success\",\n   \"data\" : \"foo / bar\"\n}`\nQuerying metadata\nPrometheus offers a set of API endpoints to query metadata about series and their labels.\nNOTE: These API endpoints may return metadata for series for which there is no sample within the selected time range, and/or for series whose samples have been marked as deleted via the deletion API endpoint. The exact extent of additionally returned series metadata is an implementation detail that may change in the future.\nFinding series by label matchers\nThe following endpoint returns the list of time series that match a certain label set.\n`GET /api/v1/series\nPOST /api/v1/series`\nURL query parameters:\n\n`match[]=<series_selector>`: Repeated series selector argument that selects the\n  series to return. At least one `match[]` argument must be provided.\n`start=<rfc3339 | unix_timestamp>`: Start timestamp.\n`end=<rfc3339 | unix_timestamp>`: End timestamp.\n\nYou can URL-encode these parameters directly in the request body by using the `POST` method and\n`Content-Type: application/x-www-form-urlencoded` header. This is useful when specifying a large\nor dynamic number of series selectors that may breach server-side URL character limits.\nThe `data` section of the query result consists of a list of objects that\ncontain the label name/value pairs which identify each series.\nThe following example returns all series that match either of the selectors\n`up` or `process_start_time_seconds{job=\"prometheus\"}`:\n`json\n$ curl -g 'http://localhost:9090/api/v1/series?' --data-urlencode 'match[]=up' --data-urlencode 'match[]=process_start_time_seconds{job=\"prometheus\"}'\n{\n   \"status\" : \"success\",\n   \"data\" : [\n      {\n         \"__name__\" : \"up\",\n         \"job\" : \"prometheus\",\n         \"instance\" : \"localhost:9090\"\n      },\n      {\n         \"__name__\" : \"up\",\n         \"job\" : \"node\",\n         \"instance\" : \"localhost:9091\"\n      },\n      {\n         \"__name__\" : \"process_start_time_seconds\",\n         \"job\" : \"prometheus\",\n         \"instance\" : \"localhost:9090\"\n      }\n   ]\n}`\nGetting label names\nThe following endpoint returns a list of label names:\n`GET /api/v1/labels\nPOST /api/v1/labels`\nURL query parameters:\n\n`start=<rfc3339 | unix_timestamp>`: Start timestamp. Optional.\n`end=<rfc3339 | unix_timestamp>`: End timestamp. Optional.\n`match[]=<series_selector>`: Repeated series selector argument that selects the\n  series from which to read the label names. Optional.\n\nThe `data` section of the JSON response is a list of string label names.\nHere is an example.\n`json\n$ curl 'localhost:9090/api/v1/labels'\n{\n    \"status\": \"success\",\n    \"data\": [\n        \"__name__\",\n        \"call\",\n        \"code\",\n        \"config\",\n        \"dialer_name\",\n        \"endpoint\",\n        \"event\",\n        \"goversion\",\n        \"handler\",\n        \"instance\",\n        \"interval\",\n        \"job\",\n        \"le\",\n        \"listener_name\",\n        \"name\",\n        \"quantile\",\n        \"reason\",\n        \"role\",\n        \"scrape_job\",\n        \"slice\",\n        \"version\"\n    ]\n}`\nQuerying label values\nThe following endpoint returns a list of label values for a provided label name:\n`GET /api/v1/label/<label_name>/values`\nURL query parameters:\n\n`start=<rfc3339 | unix_timestamp>`: Start timestamp. Optional.\n`end=<rfc3339 | unix_timestamp>`: End timestamp. Optional.\n`match[]=<series_selector>`: Repeated series selector argument that selects the\n  series from which to read the label values. Optional.\n\nThe `data` section of the JSON response is a list of string label values.\nThis example queries for all label values for the `job` label:\n`json\n$ curl http://localhost:9090/api/v1/label/job/values\n{\n   \"status\" : \"success\",\n   \"data\" : [\n      \"node\",\n      \"prometheus\"\n   ]\n}`\nQuerying exemplars\nThis is experimental and might change in the future.\nThe following endpoint returns a list of exemplars for a valid PromQL query for a specific time range:\n`GET /api/v1/query_exemplars\nPOST /api/v1/query_exemplars`\nURL query parameters:\n\n`query=<string>`: Prometheus expression query string.\n`start=<rfc3339 | unix_timestamp>`: Start timestamp.\n`end=<rfc3339 | unix_timestamp>`: End timestamp.\n\n`json\n$ curl -g 'http://localhost:9090/api/v1/query_exemplars?query=test_exemplar_metric_total&start=2020-09-14T15:22:25.479Z&end=2020-09-14T15:23:25.479Z'\n{\n    \"status\": \"success\",\n    \"data\": [\n        {\n            \"seriesLabels\": {\n                \"__name__\": \"test_exemplar_metric_total\",\n                \"instance\": \"localhost:8090\",\n                \"job\": \"prometheus\",\n                \"service\": \"bar\"\n            },\n            \"exemplars\": [\n                {\n                    \"labels\": {\n                        \"traceID\": \"EpTxMJ40fUus7aGY\"\n                    },\n                    \"value\": \"6\",\n                    \"timestamp\": 1600096945.479,\n                }\n            ]\n        },\n        {\n            \"seriesLabels\": {\n                \"__name__\": \"test_exemplar_metric_total\",\n                \"instance\": \"localhost:8090\",\n                \"job\": \"prometheus\",\n                \"service\": \"foo\"\n            },\n            \"exemplars\": [\n                {\n                    \"labels\": {\n                        \"traceID\": \"Olp9XHlq763ccsfa\"\n                    },\n                    \"value\": \"19\",\n                    \"timestamp\": 1600096955.479,\n                },\n                {\n                    \"labels\": {\n                        \"traceID\": \"hCtjygkIHwAN9vs4\"\n                    },\n                    \"value\": \"20\",\n                    \"timestamp\": 1600096965.489,\n                },\n            ]\n        }\n    ]\n}`\nExpression query result formats\nExpression queries may return the following response values in the `result`\nproperty of the `data` section. `<sample_value>` placeholders are numeric\nsample values. JSON does not support special float values such as `NaN`, `Inf`,\nand `-Inf`, so sample values are transferred as quoted JSON strings rather than\nraw numbers.\nThe keys `\"histogram\"` and `\"histograms\"` only show up if the experimental\nnative histograms are present in the response. Their placeholder `<histogram>`\nis explained in detail in its own section below. \nRange vectors\nRange vectors are returned as result type `matrix`. The corresponding\n`result` property has the following format:\n`[\n  {\n    \"metric\": { \"<label_name>\": \"<label_value>\", ... },\n    \"values\": [ [ <unix_time>, \"<sample_value>\" ], ... ],\n    \"histograms\": [ [ <unix_time>, <histogram> ], ... ]\n  },\n  ...\n]`\nEach series could have the `\"values\"` key, or the `\"histograms\"` key, or both. \nFor a given timestamp, there will only be one sample of either float or histogram type.\nInstant vectors\nInstant vectors are returned as result type `vector`. The corresponding\n`result` property has the following format:\n`[\n  {\n    \"metric\": { \"<label_name>\": \"<label_value>\", ... },\n    \"value\": [ <unix_time>, \"<sample_value>\" ],\n    \"histogram\": [ <unix_time>, <histogram> ]\n  },\n  ...\n]`\nEach series could have the `\"value\"` key, or the `\"histogram\"` key, but not both.\nScalars\nScalar results are returned as result type `scalar`. The corresponding\n`result` property has the following format:\n`[ <unix_time>, \"<scalar_value>\" ]`\nStrings\nString results are returned as result type `string`. The corresponding\n`result` property has the following format:\n`[ <unix_time>, \"<string_value>\" ]`\nNative histograms\nThe `<histogram>` placeholder used above is formatted as follows.\nNote that native histograms are an experimental feature, and the format below\nmight still change.\n`{\n  \"count\": \"<count_of_observations>\",\n  \"sum\": \"<sum_of_observations>\",\n  \"buckets\": [ [ <boundary_rule>, \"<left_boundary>\", \"<right_boundary>\", \"<count_in_bucket>\" ], ... ]\n}`\nThe `<boundary_rule>` placeholder is an integer between 0 and 3 with the\nfollowing meaning:\n\n0: \u201copen left\u201d (left boundary is exclusive, right boundary in inclusive)\n1: \u201copen right\u201d (left boundary is inclusive, right boundary in exclusive)\n2: \u201copen both\u201d (both boundaries are exclusive)\n3: \u201cclosed both\u201d (both boundaries are inclusive)\n\nNote that with the currently implemented bucket schemas, positive buckets are\n\u201copen left\u201d, negative buckets are \u201copen right\u201d, and the zero bucket (with a\nnegative left boundary and a positive right boundary) is \u201cclosed both\u201d.\nTargets\nThe following endpoint returns an overview of the current state of the\nPrometheus target discovery:\n`GET /api/v1/targets`\nBoth the active and dropped targets are part of the response by default.\n`labels` represents the label set after relabeling has occurred.\n`discoveredLabels` represent the unmodified labels retrieved during service discovery before relabeling has occurred.\n`json\n$ curl http://localhost:9090/api/v1/targets\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"activeTargets\": [\n      {\n        \"discoveredLabels\": {\n          \"__address__\": \"127.0.0.1:9090\",\n          \"__metrics_path__\": \"/metrics\",\n          \"__scheme__\": \"http\",\n          \"job\": \"prometheus\"\n        },\n        \"labels\": {\n          \"instance\": \"127.0.0.1:9090\",\n          \"job\": \"prometheus\"\n        },\n        \"scrapePool\": \"prometheus\",\n        \"scrapeUrl\": \"http://127.0.0.1:9090/metrics\",\n        \"globalUrl\": \"http://example-prometheus:9090/metrics\",\n        \"lastError\": \"\",\n        \"lastScrape\": \"2017-01-17T15:07:44.723715405+01:00\",\n        \"lastScrapeDuration\": 0.050688943,\n        \"health\": \"up\",\n        \"scrapeInterval\": \"1m\",\n        \"scrapeTimeout\": \"10s\"\n      }\n    ],\n    \"droppedTargets\": [\n      {\n        \"discoveredLabels\": {\n          \"__address__\": \"127.0.0.1:9100\",\n          \"__metrics_path__\": \"/metrics\",\n          \"__scheme__\": \"http\",\n          \"__scrape_interval__\": \"1m\",\n          \"__scrape_timeout__\": \"10s\",\n          \"job\": \"node\"\n        },\n      }\n    ]\n  }\n}`\nThe `state` query parameter allows the caller to filter by active or dropped targets,\n(e.g., `state=active`, `state=dropped`, `state=any`).\nNote that an empty array is still returned for targets that are filtered out.\nOther values are ignored.\n`json\n$ curl 'http://localhost:9090/api/v1/targets?state=active'\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"activeTargets\": [\n      {\n        \"discoveredLabels\": {\n          \"__address__\": \"127.0.0.1:9090\",\n          \"__metrics_path__\": \"/metrics\",\n          \"__scheme__\": \"http\",\n          \"job\": \"prometheus\"\n        },\n        \"labels\": {\n          \"instance\": \"127.0.0.1:9090\",\n          \"job\": \"prometheus\"\n        },\n        \"scrapePool\": \"prometheus\",\n        \"scrapeUrl\": \"http://127.0.0.1:9090/metrics\",\n        \"globalUrl\": \"http://example-prometheus:9090/metrics\",\n        \"lastError\": \"\",\n        \"lastScrape\": \"2017-01-17T15:07:44.723715405+01:00\",\n        \"lastScrapeDuration\": 50688943,\n        \"health\": \"up\"\n      }\n    ],\n    \"droppedTargets\": []\n  }\n}`\nThe `scrapePool` query parameter allows the caller to filter by scrape pool name.\n`json\n$ curl 'http://localhost:9090/api/v1/targets?scrapePool=node_exporter'\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"activeTargets\": [\n      {\n        \"discoveredLabels\": {\n          \"__address__\": \"127.0.0.1:9091\",\n          \"__metrics_path__\": \"/metrics\",\n          \"__scheme__\": \"http\",\n          \"job\": \"node_exporter\"\n        },\n        \"labels\": {\n          \"instance\": \"127.0.0.1:9091\",\n          \"job\": \"node_exporter\"\n        },\n        \"scrapePool\": \"node_exporter\",\n        \"scrapeUrl\": \"http://127.0.0.1:9091/metrics\",\n        \"globalUrl\": \"http://example-prometheus:9091/metrics\",\n        \"lastError\": \"\",\n        \"lastScrape\": \"2017-01-17T15:07:44.723715405+01:00\",\n        \"lastScrapeDuration\": 50688943,\n        \"health\": \"up\"\n      }\n    ],\n    \"droppedTargets\": []\n  }\n}`\nRules\nThe `/rules` API endpoint returns a list of alerting and recording rules that\nare currently loaded. In addition it returns the currently active alerts fired\nby the Prometheus instance of each alerting rule.\nAs the `/rules` endpoint is fairly new, it does not have the same stability\nguarantees as the overarching API v1.\n`GET /api/v1/rules`\nURL query parameters:\n- `type=alert|record`: return only the alerting rules (e.g. `type=alert`) or the recording rules (e.g. `type=record`). When the parameter is absent or empty, no filtering is done.\n```json\n$ curl http://localhost:9090/api/v1/rules\n{\n    \"data\": {\n        \"groups\": [\n            {\n                \"rules\": [\n                    {\n                        \"alerts\": [\n                            {\n                                \"activeAt\": \"2018-07-04T20:27:12.60602144+02:00\",\n                                \"annotations\": {\n                                    \"summary\": \"High request latency\"\n                                },\n                                \"labels\": {\n                                    \"alertname\": \"HighRequestLatency\",\n                                    \"severity\": \"page\"\n                                },\n                                \"state\": \"firing\",\n                                \"value\": \"1e+00\"\n                            }\n                        ],\n                        \"annotations\": {\n                            \"summary\": \"High request latency\"\n                        },\n                        \"duration\": 600,\n                        \"health\": \"ok\",\n                        \"labels\": {\n                            \"severity\": \"page\"\n                        },\n                        \"name\": \"HighRequestLatency\",\n                        \"query\": \"job:request_latency_seconds:mean5m{job=\\\"myjob\\\"} > 0.5\",\n                        \"type\": \"alerting\"\n                    },\n                    {\n                        \"health\": \"ok\",\n                        \"name\": \"job:http_inprogress_requests:sum\",\n                        \"query\": \"sum by (job) (http_inprogress_requests)\",\n                        \"type\": \"recording\"\n                    }\n                ],\n                \"file\": \"/rules.yaml\",\n                \"interval\": 60,\n                \"limit\": 0,\n                \"name\": \"example\"\n            }\n        ]\n    },\n    \"status\": \"success\"\n}\n```\nAlerts\nThe `/alerts` endpoint returns a list of all active alerts.\nAs the `/alerts` endpoint is fairly new, it does not have the same stability\nguarantees as the overarching API v1.\n`GET /api/v1/alerts`\n```json\n$ curl http://localhost:9090/api/v1/alerts\n{\n    \"data\": {\n        \"alerts\": [\n            {\n                \"activeAt\": \"2018-07-04T20:27:12.60602144+02:00\",\n                \"annotations\": {},\n                \"labels\": {\n                    \"alertname\": \"my-alert\"\n                },\n                \"state\": \"firing\",\n                \"value\": \"1e+00\"\n            }\n        ]\n    },\n    \"status\": \"success\"\n}\n```\nQuerying target metadata\nThe following endpoint returns metadata about metrics currently scraped from targets.\nThis is experimental and might change in the future.\n`GET /api/v1/targets/metadata`\nURL query parameters:\n\n`match_target=<label_selectors>`: Label selectors that match targets by their label sets. All targets are selected if left empty.\n`metric=<string>`: A metric name to retrieve metadata for. All metric metadata is retrieved if left empty.\n`limit=<number>`: Maximum number of targets to match.\n\nThe `data` section of the query result consists of a list of objects that\ncontain metric metadata and the target label set.\nThe following example returns all metadata entries for the `go_goroutines` metric\nfrom the first two targets with label `job=\"prometheus\"`.\n`json\ncurl -G http://localhost:9091/api/v1/targets/metadata \\\n    --data-urlencode 'metric=go_goroutines' \\\n    --data-urlencode 'match_target={job=\"prometheus\"}' \\\n    --data-urlencode 'limit=2'\n{\n  \"status\": \"success\",\n  \"data\": [\n    {\n      \"target\": {\n        \"instance\": \"127.0.0.1:9090\",\n        \"job\": \"prometheus\"\n      },\n      \"type\": \"gauge\",\n      \"help\": \"Number of goroutines that currently exist.\",\n      \"unit\": \"\"\n    },\n    {\n      \"target\": {\n        \"instance\": \"127.0.0.1:9091\",\n        \"job\": \"prometheus\"\n      },\n      \"type\": \"gauge\",\n      \"help\": \"Number of goroutines that currently exist.\",\n      \"unit\": \"\"\n    }\n  ]\n}`\nThe following example returns metadata for all metrics for all targets with\nlabel `instance=\"127.0.0.1:9090`.\n`json\ncurl -G http://localhost:9091/api/v1/targets/metadata \\\n    --data-urlencode 'match_target={instance=\"127.0.0.1:9090\"}'\n{\n  \"status\": \"success\",\n  \"data\": [\n    // ...\n    {\n      \"target\": {\n        \"instance\": \"127.0.0.1:9090\",\n        \"job\": \"prometheus\"\n      },\n      \"metric\": \"prometheus_treecache_zookeeper_failures_total\",\n      \"type\": \"counter\",\n      \"help\": \"The total number of ZooKeeper failures.\",\n      \"unit\": \"\"\n    },\n    {\n      \"target\": {\n        \"instance\": \"127.0.0.1:9090\",\n        \"job\": \"prometheus\"\n      },\n      \"metric\": \"prometheus_tsdb_reloads_total\",\n      \"type\": \"counter\",\n      \"help\": \"Number of times the database reloaded block data from disk.\",\n      \"unit\": \"\"\n    },\n    // ...\n  ]\n}`\nQuerying metric metadata\nIt returns metadata about metrics currently scraped from targets. However, it does not provide any target information.\nThis is considered experimental and might change in the future.\n`GET /api/v1/metadata`\nURL query parameters:\n\n`limit=<number>`: Maximum number of metrics to return.\n`metric=<string>`: A metric name to filter metadata for. All metric metadata is retrieved if left empty.\n\nThe `data` section of the query result consists of an object where each key is a metric name and each value is a list of unique metadata objects, as exposed for that metric name across all targets.\nThe following example returns two metrics. Note that the metric `http_requests_total` has more than one object in the list. At least one target has a value for `HELP` that do not match with the rest.\n```json\ncurl -G http://localhost:9090/api/v1/metadata?limit=2\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"cortex_ring_tokens\": [\n      {\n        \"type\": \"gauge\",\n        \"help\": \"Number of tokens in the ring\",\n        \"unit\": \"\"\n      }\n    ],\n    \"http_requests_total\": [\n      {\n        \"type\": \"counter\",\n        \"help\": \"Number of HTTP requests\",\n        \"unit\": \"\"\n      },\n      {\n        \"type\": \"counter\",\n        \"help\": \"Amount of HTTP requests\",\n        \"unit\": \"\"\n      }\n    ]\n  }\n}\n```\nThe following example returns metadata only for the metric `http_requests_total`.\n```json\ncurl -G http://localhost:9090/api/v1/metadata?metric=http_requests_total\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"http_requests_total\": [\n      {\n        \"type\": \"counter\",\n        \"help\": \"Number of HTTP requests\",\n        \"unit\": \"\"\n      },\n      {\n        \"type\": \"counter\",\n        \"help\": \"Amount of HTTP requests\",\n        \"unit\": \"\"\n      }\n    ]\n  }\n}\n```\nAlertmanagers\nThe following endpoint returns an overview of the current state of the\nPrometheus alertmanager discovery:\n`GET /api/v1/alertmanagers`\nBoth the active and dropped Alertmanagers are part of the response.\n`json\n$ curl http://localhost:9090/api/v1/alertmanagers\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"activeAlertmanagers\": [\n      {\n        \"url\": \"http://127.0.0.1:9090/api/v1/alerts\"\n      }\n    ],\n    \"droppedAlertmanagers\": [\n      {\n        \"url\": \"http://127.0.0.1:9093/api/v1/alerts\"\n      }\n    ]\n  }\n}`\nStatus\nFollowing status endpoints expose current Prometheus configuration.\nConfig\nThe following endpoint returns currently loaded configuration file:\n`GET /api/v1/status/config`\nThe config is returned as dumped YAML file. Due to limitation of the YAML\nlibrary, YAML comments are not included.\n`json\n$ curl http://localhost:9090/api/v1/status/config\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"yaml\": \"<content of the loaded config file in YAML>\",\n  }\n}`\nFlags\nThe following endpoint returns flag values that Prometheus was configured with:\n`GET /api/v1/status/flags`\nAll values are of the result type `string`.\n`json\n$ curl http://localhost:9090/api/v1/status/flags\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"alertmanager.notification-queue-capacity\": \"10000\",\n    \"alertmanager.timeout\": \"10s\",\n    \"log.level\": \"info\",\n    \"query.lookback-delta\": \"5m\",\n    \"query.max-concurrency\": \"20\",\n    ...\n  }\n}`\nNew in v2.2\nRuntime Information\nThe following endpoint returns various runtime information properties about the Prometheus server:\n`GET /api/v1/status/runtimeinfo`\nThe returned values are of different types, depending on the nature of the runtime property.\n`json\n$ curl http://localhost:9090/api/v1/status/runtimeinfo\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"startTime\": \"2019-11-02T17:23:59.301361365+01:00\",\n    \"CWD\": \"/\",\n    \"reloadConfigSuccess\": true,\n    \"lastConfigTime\": \"2019-11-02T17:23:59+01:00\",\n    \"timeSeriesCount\": 873,\n    \"corruptionCount\": 0,\n    \"goroutineCount\": 48,\n    \"GOMAXPROCS\": 4,\n    \"GOGC\": \"\",\n    \"GODEBUG\": \"\",\n    \"storageRetention\": \"15d\"\n  }\n}`\nNOTE: The exact returned runtime properties may change without notice between Prometheus versions.\nNew in v2.14\nBuild Information\nThe following endpoint returns various build information properties about the Prometheus server:\n`GET /api/v1/status/buildinfo`\nAll values are of the result type `string`.\n`json\n$ curl http://localhost:9090/api/v1/status/buildinfo\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"version\": \"2.13.1\",\n    \"revision\": \"cb7cbad5f9a2823a622aaa668833ca04f50a0ea7\",\n    \"branch\": \"master\",\n    \"buildUser\": \"julius@desktop\",\n    \"buildDate\": \"20191102-16:19:59\",\n    \"goVersion\": \"go1.13.1\"\n  }\n}`\nNOTE: The exact returned build properties may change without notice between Prometheus versions.\nNew in v2.14\nTSDB Stats\nThe following endpoint returns various cardinality statistics about the Prometheus TSDB:\n`GET /api/v1/status/tsdb`\n- headStats: This provides the following data about the head block of the TSDB:\n  - numSeries: The number of series.\n  - chunkCount: The number of chunks.\n  - minTime: The current minimum timestamp in milliseconds.\n  - maxTime: The current maximum timestamp in milliseconds.\n- seriesCountByMetricName:  This will provide a list of metrics names and their series count.\n- labelValueCountByLabelName: This will provide a list of the label names and their value count.\n- memoryInBytesByLabelName This will provide a list of the label names and memory used in bytes. Memory usage is calculated by adding the length of all values for a given label name.\n- seriesCountByLabelPair This will provide a list of label value pairs and their series count.\n`json\n$ curl http://localhost:9090/api/v1/status/tsdb\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"headStats\": {\n      \"numSeries\": 508,\n      \"chunkCount\": 937,\n      \"minTime\": 1591516800000,\n      \"maxTime\": 1598896800143,\n    },\n    \"seriesCountByMetricName\": [\n      {\n        \"name\": \"net_conntrack_dialer_conn_failed_total\",\n        \"value\": 20\n      },\n      {\n        \"name\": \"prometheus_http_request_duration_seconds_bucket\",\n        \"value\": 20\n      }\n    ],\n    \"labelValueCountByLabelName\": [\n      {\n        \"name\": \"__name__\",\n        \"value\": 211\n      },\n      {\n        \"name\": \"event\",\n        \"value\": 3\n      }\n    ],\n    \"memoryInBytesByLabelName\": [\n      {\n        \"name\": \"__name__\",\n        \"value\": 8266\n      },\n      {\n        \"name\": \"instance\",\n        \"value\": 28\n      }\n    ],\n    \"seriesCountByLabelValuePair\": [\n      {\n        \"name\": \"job=prometheus\",\n        \"value\": 425\n      },\n      {\n        \"name\": \"instance=localhost:9090\",\n        \"value\": 425\n      }\n    ]\n  }\n}`\nNew in v2.15\nWAL Replay Stats\nThe following endpoint returns information about the WAL replay:\n`GET /api/v1/status/walreplay`\nread: The number of segments replayed so far.\ntotal: The total number segments needed to be replayed.\nprogress: The progress of the replay (0 - 100%).\nstate: The state of the replay. Possible states:\n- waiting: Waiting for the replay to start.\n- in progress: The replay is in progress.\n- done: The replay has finished.\n`json\n$ curl http://localhost:9090/api/v1/status/walreplay\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"min\": 2,\n    \"max\": 5,\n    \"current\": 40,\n    \"state\": \"in progress\"\n  }\n}`\nNOTE: This endpoint is available before the server has been marked ready and is updated in real time to facilitate monitoring the progress of the WAL replay.\nNew in v2.28\nTSDB Admin APIs\nThese are APIs that expose database functionalities for the advanced user. These APIs are not enabled unless the `--web.enable-admin-api` is set.\nSnapshot\nSnapshot creates a snapshot of all current data into `snapshots/<datetime>-<rand>` under the TSDB's data directory and returns the directory as response.\nIt will optionally skip snapshotting data that is only present in the head block, and which has not yet been compacted to disk.\n`POST /api/v1/admin/tsdb/snapshot\nPUT /api/v1/admin/tsdb/snapshot`\nURL query parameters:\n\n`skip_head=<bool>`: Skip data present in the head block. Optional.\n\n`json\n$ curl -XPOST http://localhost:9090/api/v1/admin/tsdb/snapshot\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"name\": \"20171210T211224Z-2be650b6d019eb54\"\n  }\n}`\nThe snapshot now exists at `<data-dir>/snapshots/20171210T211224Z-2be650b6d019eb54`\nNew in v2.1 and supports PUT from v2.9\nDelete Series\nDeleteSeries deletes data for a selection of series in a time range. The actual data still exists on disk and is cleaned up in future compactions or can be explicitly cleaned up by hitting the Clean Tombstones endpoint.\nIf successful, a `204` is returned.\n`POST /api/v1/admin/tsdb/delete_series\nPUT /api/v1/admin/tsdb/delete_series`\nURL query parameters:\n\n`match[]=<series_selector>`: Repeated label matcher argument that selects the series to delete. At least one `match[]` argument must be provided.\n`start=<rfc3339 | unix_timestamp>`: Start timestamp. Optional and defaults to minimum possible time.\n`end=<rfc3339 | unix_timestamp>`: End timestamp. Optional and defaults to maximum possible time.\n\nNot mentioning both start and end times would clear all the data for the matched series in the database.\nExample:\n`json\n$ curl -X POST \\\n  -g 'http://localhost:9090/api/v1/admin/tsdb/delete_series?match[]=up&match[]=process_start_time_seconds{job=\"prometheus\"}'`\nNOTE: This endpoint marks samples from series as deleted, but will not necessarily prevent associated series metadata from still being returned in metadata queries for the affected time range (even after cleaning tombstones). The exact extent of metadata deletion is an implementation detail that may change in the future.\nNew in v2.1 and supports PUT from v2.9\nClean Tombstones\nCleanTombstones removes the deleted data from disk and cleans up the existing tombstones. This can be used after deleting series to free up space.\nIf successful, a `204` is returned.\n`POST /api/v1/admin/tsdb/clean_tombstones\nPUT /api/v1/admin/tsdb/clean_tombstones`\nThis takes no parameters or body.\n`json\n$ curl -XPOST http://localhost:9090/api/v1/admin/tsdb/clean_tombstones`\nNew in v2.1 and supports PUT from v2.9\nRemote Write Receiver\nPrometheus can be configured as a receiver for the Prometheus remote write\nprotocol. This is not considered an efficient way of ingesting samples. Use it\nwith caution for specific low-volume use cases. It is not suitable for\nreplacing the ingestion via scraping and turning Prometheus into a push-based\nmetrics collection system.\nEnable the remote write receiver by setting\n`--web.enable-remote-write-receiver`. When enabled, the remote write receiver\nendpoint is `/api/v1/write`. Find more details here.",
    "tag": "prometheus"
  },
  {
    "title": "Remote Read API",
    "source": "https://github.com/prometheus/prometheus/tree/main/docs/querying/remote_read_api.md",
    "content": "\ntitle: Remote Read API\nsort_rank: 7\n\nRemote Read API\nThis is not currently considered part of the stable API and is subject to change\neven between non-major version releases of Prometheus.\nFormat overview\nThe API response format is JSON. Every successful API request returns a `2xx`\nstatus code.\nInvalid requests that reach the API handlers return a JSON error object\nand one of the following HTTP response codes:\n\n`400 Bad Request` when parameters are missing or incorrect.\n`422 Unprocessable Entity` when an expression can't be executed\n  (RFC4918).\n`503 Service Unavailable` when queries time out or abort.\n\nOther non-`2xx` codes may be returned for errors occurring before the API\nendpoint is reached.\nAn array of warnings may be returned if there are errors that do\nnot inhibit the request execution. All of the data that was successfully\ncollected will be returned in the data field.\nThe JSON response envelope format is as follows:\n```\n{\n  \"status\": \"success\" | \"error\",\n  \"data\": ,\n// Only set if status is \"error\". The data field may still hold\n  // additional data.\n  \"errorType\": \"\",\n  \"error\": \"\",\n// Only if there were warnings while executing the request.\n  // There will still be data in the data field.\n  \"warnings\": [\"\"]\n}\n```\nGeneric placeholders are defined as follows:\n\n`<rfc3339 | unix_timestamp>`: Input timestamps may be provided either in\nRFC3339 format or as a Unix timestamp\nin seconds, with optional decimal places for sub-second precision. Output\ntimestamps are always represented as Unix timestamps in seconds.\n`<series_selector>`: Prometheus time series\nselectors like `http_requests_total` or\n`http_requests_total{method=~\"(GET|POST)\"}` and need to be URL-encoded.\n`<duration>`: Prometheus duration strings.\nFor example, `5m` refers to a duration of 5 minutes.\n`<bool>`: boolean values (strings `true` and `false`).\n\nNote: Names of query parameters that may be repeated end with `[]`.\nRemote Read API\nThis API provides data read functionality from Prometheus. This interface expects snappy compression.\nThe API definition is located here.\nRequest are made to the following endpoint.\n`/api/v1/read`\nSamples\nThis returns a message that includes a list of raw samples.\nStreamed Chunks\nThese streamed chunks utilize an XOR algorithm inspired by the Gorilla\ncompression to encode the chunks. However, it provides resolution to the millisecond instead of to the second.",
    "tag": "prometheus"
  },
  {
    "title": "Operators",
    "source": "https://github.com/prometheus/prometheus/tree/main/docs/querying/operators.md",
    "content": "\ntitle: Operators\nsort_rank: 2\n\nOperators\nBinary operators\nPrometheus's query language supports basic logical and arithmetic operators.\nFor operations between two instant vectors, the matching behavior\ncan be modified.\nArithmetic binary operators\nThe following binary arithmetic operators exist in Prometheus:\n\n`+` (addition)\n`-` (subtraction)\n`*` (multiplication)\n`/` (division)\n`%` (modulo)\n`^` (power/exponentiation)\n\nBinary arithmetic operators are defined between scalar/scalar, vector/scalar,\nand vector/vector value pairs.\nBetween two scalars, the behavior is obvious: they evaluate to another\nscalar that is the result of the operator applied to both scalar operands.\nBetween an instant vector and a scalar, the operator is applied to the\nvalue of every data sample in the vector. E.g. if a time series instant vector\nis multiplied by 2, the result is another vector in which every sample value of\nthe original vector is multiplied by 2. The metric name is dropped.\nBetween two instant vectors, a binary arithmetic operator is applied to\neach entry in the left-hand side vector and its matching element\nin the right-hand vector. The result is propagated into the result vector with the\ngrouping labels becoming the output label set. The metric name is dropped. Entries\nfor which no matching entry in the right-hand vector can be found are not part of\nthe result.\nTrigonometric binary operators\nThe following trigonometric binary operators, which work in radians, exist in Prometheus:\n\n`atan2` (based on https://pkg.go.dev/math#Atan2)\n\nTrigonometric operators allow trigonometric functions to be executed on two vectors using\nvector matching, which isn't available with normal functions. They act in the same manner\nas arithmetic operators.\nComparison binary operators\nThe following binary comparison operators exist in Prometheus:\n\n`==` (equal)\n`!=` (not-equal)\n`>` (greater-than)\n`<` (less-than)\n`>=` (greater-or-equal)\n`<=` (less-or-equal)\n\nComparison operators are defined between scalar/scalar, vector/scalar,\nand vector/vector value pairs. By default they filter. Their behavior can be\nmodified by providing `bool` after the operator, which will return `0` or `1`\nfor the value rather than filtering.\nBetween two scalars, the `bool` modifier must be provided and these\noperators result in another scalar that is either `0` (`false`) or `1`\n(`true`), depending on the comparison result.\nBetween an instant vector and a scalar, these operators are applied to the\nvalue of every data sample in the vector, and vector elements between which the\ncomparison result is `false` get dropped from the result vector. If the `bool`\nmodifier is provided, vector elements that would be dropped instead have the value\n`0` and vector elements that would be kept have the value `1`. The metric name\nis dropped if the `bool` modifier is provided.\nBetween two instant vectors, these operators behave as a filter by default,\napplied to matching entries. Vector elements for which the expression is not\ntrue or which do not find a match on the other side of the expression get\ndropped from the result, while the others are propagated into a result vector\nwith the grouping labels becoming the output label set.\nIf the `bool` modifier is provided, vector elements that would have been\ndropped instead have the value `0` and vector elements that would be kept have\nthe value `1`, with the grouping labels again becoming the output label set.\nThe metric name is dropped if the `bool` modifier is provided.\nLogical/set binary operators\nThese logical/set binary operators are only defined between instant vectors:\n\n`and` (intersection)\n`or` (union)\n`unless` (complement)\n\n`vector1 and vector2` results in a vector consisting of the elements of\n`vector1` for which there are elements in `vector2` with exactly matching\nlabel sets. Other elements are dropped. The metric name and values are carried\nover from the left-hand side vector.\n`vector1 or vector2` results in a vector that contains all original elements\n(label sets + values) of `vector1` and additionally all elements of `vector2`\nwhich do not have matching label sets in `vector1`.\n`vector1 unless vector2` results in a vector consisting of the elements of\n`vector1` for which there are no elements in `vector2` with exactly matching\nlabel sets. All matching elements in both vectors are dropped.\nVector matching\nOperations between vectors attempt to find a matching element in the right-hand side\nvector for each entry in the left-hand side. There are two basic types of\nmatching behavior: One-to-one and many-to-one/one-to-many.\nVector matching keywords\nThese vector matching keywords allow for matching between series with different label sets\nproviding:\n\n`on`\n`ignoring`\n\nLabel lists provided to matching keywords will determine how vectors are combined. Examples\ncan be found in One-to-one vector matches and in\nMany-to-one and one-to-many vector matches\nGroup modifiers\nThese group modifiers enable many-to-one/one-to-many vector matching:\n\n`group_left`\n`group_right`\n\nLabel lists can be provided to the group modifier which contain labels from the \"one\"-side to\nbe included in the result metrics.\nMany-to-one and one-to-many matching are advanced use cases that should be carefully considered.\nOften a proper use of `ignoring(<labels>)` provides the desired outcome.\nGrouping modifiers can only be used for\ncomparison and\narithmetic. Operations as `and`, `unless` and\n`or` operations match with all possible entries in the right vector by\ndefault.\nOne-to-one vector matches\nOne-to-one finds a unique pair of entries from each side of the operation.\nIn the default case, that is an operation following the format `vector1 <operator> vector2`.\nTwo entries match if they have the exact same set of labels and corresponding values.\nThe `ignoring` keyword allows ignoring certain labels when matching, while the\n`on` keyword allows reducing the set of considered labels to a provided list:\n\n\n```<vector expr> <bin-op> ignoring(<label list>) <vector expr>\n<vector expr> <bin-op> on(<label list>) <vector expr>\n```\n\n\nExample input:\n\n\n```method_code:http_errors:rate5m{method=\"get\", code=\"500\"}  24\nmethod_code:http_errors:rate5m{method=\"get\", code=\"404\"}  30\nmethod_code:http_errors:rate5m{method=\"put\", code=\"501\"}  3\nmethod_code:http_errors:rate5m{method=\"post\", code=\"500\"} 6\nmethod_code:http_errors:rate5m{method=\"post\", code=\"404\"} 21\n\nmethod:http_requests:rate5m{method=\"get\"}  600\nmethod:http_requests:rate5m{method=\"del\"}  34\nmethod:http_requests:rate5m{method=\"post\"} 120\n```\n\n\nExample query:\n\n\n```method_code:http_errors:rate5m{code=\"500\"} / ignoring(code) method:http_requests:rate5m\n```\n\n\nThis returns a result vector containing the fraction of HTTP requests with status code\nof 500 for each method, as measured over the last 5 minutes. Without `ignoring(code)` there\nwould have been no match as the metrics do not share the same set of labels.\nThe entries with methods `put` and `del` have no match and will not show up in the result:\n\n\n```{method=\"get\"}  0.04            //  24 / 600\n{method=\"post\"} 0.05            //   6 / 120\n```\n\n\nMany-to-one and one-to-many vector matches\nMany-to-one and one-to-many matchings refer to the case where each vector element on\nthe \"one\"-side can match with multiple elements on the \"many\"-side. This has to\nbe explicitly requested using the `group_left` or `group_right` modifiers, where\nleft/right determines which vector has the higher cardinality.\n\n\n```<vector expr> <bin-op> ignoring(<label list>) group_left(<label list>) <vector expr>\n<vector expr> <bin-op> ignoring(<label list>) group_right(<label list>) <vector expr>\n<vector expr> <bin-op> on(<label list>) group_left(<label list>) <vector expr>\n<vector expr> <bin-op> on(<label list>) group_right(<label list>) <vector expr>\n```\n\n\nThe label list provided with the group modifier contains additional labels from\nthe \"one\"-side to be included in the result metrics. For `on` a label can only\nappear in one of the lists. Every time series of the result vector must be\nuniquely identifiable.\nExample query:\n\n\n```method_code:http_errors:rate5m / ignoring(code) group_left method:http_requests:rate5m\n```\n\n\nIn this case the left vector contains more than one entry per `method` label\nvalue. Thus, we indicate this using `group_left`. The elements from the right\nside are now matched with multiple elements with the same `method` label on the\nleft:\n\n\n```{method=\"get\", code=\"500\"}  0.04            //  24 / 600\n{method=\"get\", code=\"404\"}  0.05            //  30 / 600\n{method=\"post\", code=\"500\"} 0.05            //   6 / 120\n{method=\"post\", code=\"404\"} 0.175           //  21 / 120\n```\n\n\nAggregation operators\nPrometheus supports the following built-in aggregation operators that can be\nused to aggregate the elements of a single instant vector, resulting in a new\nvector of fewer elements with aggregated values:\n\n`sum` (calculate sum over dimensions)\n`min` (select minimum over dimensions)\n`max` (select maximum over dimensions)\n`avg` (calculate the average over dimensions)\n`group` (all values in the resulting vector are 1)\n`stddev` (calculate population standard deviation over dimensions)\n`stdvar` (calculate population standard variance over dimensions)\n`count` (count number of elements in the vector)\n`count_values` (count number of elements with the same value)\n`bottomk` (smallest k elements by sample value)\n`topk` (largest k elements by sample value)\n`quantile` (calculate \u03c6-quantile (0 \u2264 \u03c6 \u2264 1) over dimensions)\n\nThese operators can either be used to aggregate over all label dimensions\nor preserve distinct dimensions by including a `without` or `by` clause. These\nclauses may be used before or after the expression.\n\n\n```<aggr-op> [without|by (<label list>)] ([parameter,] <vector expression>)\n```\n\n\nor\n\n\n```<aggr-op>([parameter,] <vector expression>) [without|by (<label list>)]\n```\n\n\n`label list` is a list of unquoted labels that may include a trailing comma, i.e.\nboth `(label1, label2)` and `(label1, label2,)` are valid syntax.\n`without` removes the listed labels from the result vector, while\nall other labels are preserved in the output. `by` does the opposite and drops\nlabels that are not listed in the `by` clause, even if their label values are\nidentical between all elements of the vector.\n`parameter` is only required for `count_values`, `quantile`, `topk` and\n`bottomk`.\n`count_values` outputs one time series per unique sample value. Each series has\nan additional label. The name of that label is given by the aggregation\nparameter, and the label value is the unique sample value. The value of each\ntime series is the number of times that sample value was present.\n`topk` and `bottomk` are different from other aggregators in that a subset of\nthe input samples, including the original labels, are returned in the result\nvector. `by` and `without` are only used to bucket the input vector.\n`quantile` calculates the \u03c6-quantile, the value that ranks at number \u03c6*N among\nthe N metric values of the dimensions aggregated over. \u03c6 is provided as the\naggregation parameter. For example, `quantile(0.5, ...)` calculates the median,\n`quantile(0.95, ...)` the 95th percentile. For \u03c6 = `NaN`, `NaN` is returned. For \u03c6 < 0, `-Inf` is returned. For \u03c6 > 1, `+Inf` is returned.\nExample:\nIf the metric `http_requests_total` had time series that fan out by\n`application`, `instance`, and `group` labels, we could calculate the total\nnumber of seen HTTP requests per application and group over all instances via:\n\n\n```sum without (instance) (http_requests_total)\n```\n\n\nWhich is equivalent to:\n\n\n``` sum by (application, group) (http_requests_total)\n```\n\n\nIf we are just interested in the total of HTTP requests we have seen in all\napplications, we could simply write:\n\n\n```sum(http_requests_total)\n```\n\n\nTo count the number of binaries running each build version we could write:\n\n\n```count_values(\"version\", build_version)\n```\n\n\nTo get the 5 largest HTTP requests counts across all instances we could write:\n\n\n```topk(5, http_requests_total)\n```\n\n\nBinary operator precedence\nThe following list shows the precedence of binary operators in Prometheus, from\nhighest to lowest.\n\n`^`\n`*`, `/`, `%`, `atan2`\n`+`, `-`\n`==`, `!=`, `<=`, `<`, `>=`, `>`\n`and`, `unless`\n`or`\n\nOperators on the same precedence level are left-associative. For example,\n`2 * 3 % 2` is equivalent to `(2 * 3) % 2`. However `^` is right associative,\nso `2 ^ 3 ^ 2` is equivalent to `2 ^ (3 ^ 2)`.\nOperators for native histograms\nNative histograms are an experimental feature. Ingesting native histograms has\nto be enabled via a feature flag. Once\nnative histograms have been ingested, they can be queried (even after the\nfeature flag has been disabled again). However, the operator support for native\nhistograms is still very limited.\nLogical/set binary operators work as expected even if histogram samples are\ninvolved. They only check for the existence of a vector element and don't\nchange their behavior depending on the sample type of an element (float or\nhistogram).\nThe binary `+` operator between two native histograms and the `sum` aggregation\noperator to aggregate native histograms are fully supported. Even if the\nhistograms involved have different bucket layouts, the buckets are\nautomatically converted appropriately so that the operation can be\nperformed. (With the currently supported bucket schemas, that's always\npossible.) If either operator has to sum up a mix of histogram samples and\nfloat samples, the corresponding vector element is removed from the output\nvector entirely.\nAll other operators do not behave in a meaningful way. They either treat the\nhistogram sample as if it were a float sample of value 0, or (in case of\narithmetic operations between a scalar and a vector) they leave the histogram\nsample unchanged. This behavior will change to a meaningful one before native",
    "tag": "prometheus"
  },
  {
    "title": "Functions",
    "source": "https://github.com/prometheus/prometheus/tree/main/docs/querying/functions.md",
    "content": "\ntitle: Query functions\nnav_title: Functions\nsort_rank: 3\n\nFunctions\nSome functions have default arguments, e.g. `year(v=vector(time())\ninstant-vector)`. This means that there is one argument `v` which is an instant\nvector, which if not provided it will default to the value of the expression\n`vector(time())`.\nNotes about the experimental native histograms:\n\nIngesting native histograms has to be enabled via a feature\n  flag. As long as no native histograms\n  have been ingested into the TSDB, all functions will behave as usual.\nFunctions that do not explicitly mention native histograms in their\n  documentation (see below) effectively treat a native histogram as a float\n  sample of value 0. (This is confusing and will change before native\n  histograms become a stable feature.)\nFunctions that do already act on native histograms might still change their\n  behavior in the future.\nIf a function requires the same bucket layout between multiple native\n  histograms it acts on, it will automatically convert them\n  appropriately. (With the currently supported bucket schemas, that's always\n  possible.)\n\n`abs()`\n`abs(v instant-vector)` returns the input vector with all sample values converted to\ntheir absolute value.\n`absent()`\n`absent(v instant-vector)` returns an empty vector if the vector passed to it\nhas any elements (floats or native histograms) and a 1-element vector with the\nvalue 1 if the vector passed to it has no elements.\nThis is useful for alerting on when no time series exist for a given metric name\nand label combination.\n```\nabsent(nonexistent{job=\"myjob\"})\n=> {job=\"myjob\"}\nabsent(nonexistent{job=\"myjob\",instance=~\".*\"})\n=> {job=\"myjob\"}\nabsent(sum(nonexistent{job=\"myjob\"}))\n=> {}\n```\nIn the first two examples, `absent()` tries to be smart about deriving labels\nof the 1-element output vector from the input vector.\n`absent_over_time()`\n`absent_over_time(v range-vector)` returns an empty vector if the range vector\npassed to it has any elements (floats or native histograms) and a 1-element\nvector with the value 1 if the range vector passed to it has no elements.\nThis is useful for alerting on when no time series exist for a given metric name\nand label combination for a certain amount of time.\n```\nabsent_over_time(nonexistent{job=\"myjob\"}[1h])\n=> {job=\"myjob\"}\nabsent_over_time(nonexistent{job=\"myjob\",instance=~\".*\"}[1h])\n=> {job=\"myjob\"}\nabsent_over_time(sum(nonexistent{job=\"myjob\"})[1h:])\n=> {}\n```\nIn the first two examples, `absent_over_time()` tries to be smart about deriving\nlabels of the 1-element output vector from the input vector.\n`ceil()`\n`ceil(v instant-vector)` rounds the sample values of all elements in `v` up to\nthe nearest integer.\n`changes()`\nFor each input time series, `changes(v range-vector)` returns the number of\ntimes its value has changed within the provided time range as an instant\nvector.\n`clamp()`\n`clamp(v instant-vector, min scalar, max scalar)`\nclamps the sample values of all elements in `v` to have a lower limit of `min` and an upper limit of `max`.\nSpecial cases:\n- Return an empty vector if `min > max`\n- Return `NaN` if `min` or `max` is `NaN`\n`clamp_max()`\n`clamp_max(v instant-vector, max scalar)` clamps the sample values of all\nelements in `v` to have an upper limit of `max`.\n`clamp_min()`\n`clamp_min(v instant-vector, min scalar)` clamps the sample values of all\nelements in `v` to have a lower limit of `min`.\n`day_of_month()`\n`day_of_month(v=vector(time()) instant-vector)` returns the day of the month\nfor each of the given times in UTC. Returned values are from 1 to 31.\n`day_of_week()`\n`day_of_week(v=vector(time()) instant-vector)` returns the day of the week for\neach of the given times in UTC. Returned values are from 0 to 6, where 0 means\nSunday etc.\n`day_of_year()`\n`day_of_year(v=vector(time()) instant-vector)` returns the day of the year for\neach of the given times in UTC. Returned values are from 1 to 365 for non-leap years,\nand 1 to 366 in leap years.\n`days_in_month()`\n`days_in_month(v=vector(time()) instant-vector)` returns number of days in the\nmonth for each of the given times in UTC. Returned values are from 28 to 31.\n`delta()`\n`delta(v range-vector)` calculates the difference between the\nfirst and last value of each time series element in a range vector `v`,\nreturning an instant vector with the given deltas and equivalent labels.\nThe delta is extrapolated to cover the full time range as specified in\nthe range vector selector, so that it is possible to get a non-integer\nresult even if the sample values are all integers.\nThe following example expression returns the difference in CPU temperature\nbetween now and 2 hours ago:\n`delta(cpu_temp_celsius{host=\"zeus\"}[2h])`\n`delta` acts on native histograms by calculating a new histogram where each\ncompononent (sum and count of observations, buckets) is the difference between\nthe respective component in the first and last native histogram in\n`v`. However, each element in `v` that contains a mix of float and native\nhistogram samples within the range, will be missing from the result vector.\n`delta` should only be used with gauges and native histograms where the\ncomponents behave like gauges (so-called gauge histograms).\n`deriv()`\n`deriv(v range-vector)` calculates the per-second derivative of the time series in a range\nvector `v`, using simple linear regression.\nThe range vector must have at least two samples in order to perform the calculation. When `+Inf` or \n`-Inf` are found in the range vector, the slope and offset value calculated will be `NaN`.\n`deriv` should only be used with gauges.\n`exp()`\n`exp(v instant-vector)` calculates the exponential function for all elements in `v`.\nSpecial cases are:\n\n`Exp(+Inf) = +Inf`\n`Exp(NaN) = NaN`\n\n`floor()`\n`floor(v instant-vector)` rounds the sample values of all elements in `v` down\nto the nearest integer.\n`histogram_count()` and `histogram_sum()`\nBoth functions only act on native histograms, which are an experimental\nfeature. The behavior of these functions may change in future versions of\nPrometheus, including their removal from PromQL.\n`histogram_count(v instant-vector)` returns the count of observations stored in\na native histogram. Samples that are not native histograms are ignored and do\nnot show up in the returned vector.\nSimilarly, `histogram_sum(v instant-vector)` returns the sum of observations\nstored in a native histogram.\nUse `histogram_count` in the following way to calculate a rate of observations\n(in this case corresponding to \u201crequests per second\u201d) from a native histogram:\n\n\n```histogram_count(rate(http_request_duration_seconds[10m]))\n```\n\n\nThe additional use of `histogram_sum` enables the calculation of the average of\nobserved values (in this case corresponding to \u201caverage request duration\u201d):\n\n\n```  histogram_sum(rate(http_request_duration_seconds[10m]))\n/\n  histogram_count(rate(http_request_duration_seconds[10m]))\n```\n\n\n`histogram_fraction()`\nThis function only acts on native histograms, which are an experimental\nfeature. The behavior of this function may change in future versions of\nPrometheus, including its removal from PromQL.\nFor a native histogram, `histogram_fraction(lower scalar, upper scalar, v\ninstant-vector)` returns the estimated fraction of observations between the\nprovided lower and upper values. Samples that are not native histograms are\nignored and do not show up in the returned vector.\nFor example, the following expression calculates the fraction of HTTP requests\nover the last hour that took 200ms or less:\n\n\n```histogram_fraction(0, 0.2, rate(http_request_duration_seconds[1h]))\n```\n\n\nThe error of the estimation depends on the resolution of the underlying native\nhistogram and how closely the provided boundaries are aligned with the bucket\nboundaries in the histogram.\n`+Inf` and `-Inf` are valid boundary values. For example, if the histogram in\nthe expression above included negative observations (which shouldn't be the\ncase for request durations), the appropriate lower boundary to include all\nobservations less than or equal 0.2 would be `-Inf` rather than `0`.\nWhether the provided boundaries are inclusive or exclusive is only relevant if\nthe provided boundaries are precisely aligned with bucket boundaries in the\nunderlying native histogram. In this case, the behavior depends on the schema\ndefinition of the histogram. The currently supported schemas all feature\ninclusive upper boundaries and exclusive lower boundaries for positive values\n(and vice versa for negative values). Without a precise alignment of\nboundaries, the function uses linear interpolation to estimate the\nfraction. With the resulting uncertainty, it becomes irrelevant if the\nboundaries are inclusive or exclusive.\n`histogram_quantile()`\n`histogram_quantile(\u03c6 scalar, b instant-vector)` calculates the \u03c6-quantile (0 \u2264\n\u03c6 \u2264 1) from a conventional\nhistogram or from\na native histogram. (See histograms and\nsummaries for a detailed\nexplanation of \u03c6-quantiles and the usage of the (conventional) histogram metric\ntype in general.)\nNote that native histograms are an experimental feature. The behavior of this\nfunction when dealing with native histograms may change in future versions of\nPrometheus.\nThe conventional float samples in `b` are considered the counts of observations\nin each bucket of one or more conventional histograms. Each float sample must\nhave a label `le` where the label value denotes the inclusive upper bound of\nthe bucket. (Float samples without such a label are silently ignored.) The\nother labels and the metric name are used to identify the buckets belonging to\neach conventional histogram. The histogram metric\ntype\nautomatically provides time series with the `_bucket` suffix and the\nappropriate labels.\nThe native histogram samples in `b` are treated each individually as a separate\nhistogram to calculate the quantile from.\nAs long as no naming collisions arise, `b` may contain a mix of conventional\nand native histograms.\nUse the `rate()` function to specify the time window for the quantile\ncalculation.\nExample: A histogram metric is called `http_request_duration_seconds` (and\ntherefore the metric name for the buckets of a conventional histogram is\n`http_request_duration_seconds_bucket`). To calculate the 90th percentile of request\ndurations over the last 10m, use the following expression in case\n`http_request_duration_seconds` is a conventional histogram:\n\n\n```histogram_quantile(0.9, rate(http_request_duration_seconds_bucket[10m]))\n```\n\n\nFor a native histogram, use the following expression instead:\n\n\n```histogram_quantile(0.9, rate(http_request_duration_seconds[10m]))\n```\n\n\nThe quantile is calculated for each label combination in\n`http_request_duration_seconds`. To aggregate, use the `sum()` aggregator\naround the `rate()` function. Since the `le` label is required by\n`histogram_quantile()` to deal with conventional histograms, it has to be\nincluded in the `by` clause. The following expression aggregates the 90th\npercentile by `job` for conventional histograms:\n\n\n```histogram_quantile(0.9, sum by (job, le) (rate(http_request_duration_seconds_bucket[10m])))\n```\n\n\nWhen aggregating native histograms, the expression simplifies to:\n\n\n```histogram_quantile(0.9, sum by (job) (rate(http_request_duration_seconds[10m])))\n```\n\n\nTo aggregate all conventional histograms, specify only the `le` label:\n\n\n```histogram_quantile(0.9, sum by (le) (rate(http_request_duration_seconds_bucket[10m])))\n```\n\n\nWith native histograms, aggregating everything works as usual without any `by` clause:\n\n\n```histogram_quantile(0.9, sum(rate(http_request_duration_seconds[10m])))\n```\n\n\nThe `histogram_quantile()` function interpolates quantile values by\nassuming a linear distribution within a bucket. \nIf `b` has 0 observations, `NaN` is returned. For \u03c6 < 0, `-Inf` is\nreturned. For \u03c6 > 1, `+Inf` is returned. For \u03c6 = `NaN`, `NaN` is returned.\nThe following is only relevant for conventional histograms: If `b` contains\nfewer than two buckets, `NaN` is returned. The highest bucket must have an\nupper bound of `+Inf`. (Otherwise, `NaN` is returned.) If a quantile is located\nin the highest bucket, the upper bound of the second highest bucket is\nreturned. A lower limit of the lowest bucket is assumed to be 0 if the upper\nbound of that bucket is greater than\n0. In that case, the usual linear interpolation is applied within that\nbucket. Otherwise, the upper bound of the lowest bucket is returned for\nquantiles located in the lowest bucket. \n`holt_winters()`\n`holt_winters(v range-vector, sf scalar, tf scalar)` produces a smoothed value\nfor time series based on the range in `v`. The lower the smoothing factor `sf`,\nthe more importance is given to old data. The higher the trend factor `tf`, the\nmore trends in the data is considered. Both `sf` and `tf` must be between 0 and\n1.\n`holt_winters` should only be used with gauges.\n`hour()`\n`hour(v=vector(time()) instant-vector)` returns the hour of the day\nfor each of the given times in UTC. Returned values are from 0 to 23.\n`idelta()`\n`idelta(v range-vector)` calculates the difference between the last two samples\nin the range vector `v`, returning an instant vector with the given deltas and\nequivalent labels.\n`idelta` should only be used with gauges.\n`increase()`\n`increase(v range-vector)` calculates the increase in the\ntime series in the range vector. Breaks in monotonicity (such as counter\nresets due to target restarts) are automatically adjusted for. The\nincrease is extrapolated to cover the full time range as specified\nin the range vector selector, so that it is possible to get a\nnon-integer result even if a counter increases only by integer\nincrements.\nThe following example expression returns the number of HTTP requests as measured\nover the last 5 minutes, per time series in the range vector:\n`increase(http_requests_total{job=\"api-server\"}[5m])`\n`increase` acts on native histograms by calculating a new histogram where each\ncompononent (sum and count of observations, buckets) is the increase between\nthe respective component in the first and last native histogram in\n`v`. However, each element in `v` that contains a mix of float and native\nhistogram samples within the range, will be missing from the result vector.\n`increase` should only be used with counters and native histograms where the\ncomponents behave like counters. It is syntactic sugar for `rate(v)` multiplied\nby the number of seconds under the specified time range window, and should be\nused primarily for human readability.  Use `rate` in recording rules so that\nincreases are tracked consistently on a per-second basis.\n`irate()`\n`irate(v range-vector)` calculates the per-second instant rate of increase of\nthe time series in the range vector. This is based on the last two data points.\nBreaks in monotonicity (such as counter resets due to target restarts) are\nautomatically adjusted for.\nThe following example expression returns the per-second rate of HTTP requests\nlooking up to 5 minutes back for the two most recent data points, per time\nseries in the range vector:\n`irate(http_requests_total{job=\"api-server\"}[5m])`\n`irate` should only be used when graphing volatile, fast-moving counters.\nUse `rate` for alerts and slow-moving counters, as brief changes\nin the rate can reset the `FOR` clause and graphs consisting entirely of rare\nspikes are hard to read.\nNote that when combining `irate()` with an\naggregation operator (e.g. `sum()`)\nor a function aggregating over time (any function ending in `_over_time`),\nalways take a `irate()` first, then aggregate. Otherwise `irate()` cannot detect\ncounter resets when your target restarts.\n`label_join()`\nFor each timeseries in `v`, `label_join(v instant-vector, dst_label string, separator string, src_label_1 string, src_label_2 string, ...)` joins all the values of all the `src_labels`\nusing `separator` and returns the timeseries with the label `dst_label` containing the joined value.\nThere can be any number of `src_labels` in this function.\nThis example will return a vector with each time series having a `foo` label with the value `a,b,c` added to it:\n`label_join(up{job=\"api-server\",src1=\"a\",src2=\"b\",src3=\"c\"}, \"foo\", \",\", \"src1\", \"src2\", \"src3\")`\n`label_replace()`\nFor each timeseries in `v`, `label_replace(v instant-vector, dst_label string, replacement string, src_label string, regex string)`\nmatches the regular expression `regex` against the value of the label `src_label`. If it\nmatches, the value of the label `dst_label` in the returned timeseries will be the expansion\nof `replacement`, together with the original labels in the input. Capturing groups in the\nregular expression can be referenced with `$1`, `$2`, etc. If the regular expression doesn't\nmatch then the timeseries is returned unchanged.\nThis example will return timeseries with the values `a:c` at label `service` and `a` at label `foo`:\n`label_replace(up{job=\"api-server\",service=\"a:c\"}, \"foo\", \"$1\", \"service\", \"(.*):.*\")`\n`ln()`\n`ln(v instant-vector)` calculates the natural logarithm for all elements in `v`.\nSpecial cases are:\n\n`ln(+Inf) = +Inf`\n`ln(0) = -Inf`\n`ln(x < 0) = NaN`\n`ln(NaN) = NaN`\n\n`log2()`\n`log2(v instant-vector)` calculates the binary logarithm for all elements in `v`.\nThe special cases are equivalent to those in `ln`.\n`log10()`\n`log10(v instant-vector)` calculates the decimal logarithm for all elements in `v`.\nThe special cases are equivalent to those in `ln`.\n`minute()`\n`minute(v=vector(time()) instant-vector)` returns the minute of the hour for each\nof the given times in UTC. Returned values are from 0 to 59.\n`month()`\n`month(v=vector(time()) instant-vector)` returns the month of the year for each\nof the given times in UTC. Returned values are from 1 to 12, where 1 means\nJanuary etc.\n`predict_linear()`\n`predict_linear(v range-vector, t scalar)` predicts the value of time series\n`t` seconds from now, based on the range vector `v`, using simple linear\nregression.\nThe range vector must have at least two samples in order to perform the \ncalculation. When `+Inf` or `-Inf` are found in the range vector, \nthe slope and offset value calculated will be `NaN`.\n`predict_linear` should only be used with gauges.\n`rate()`\n`rate(v range-vector)` calculates the per-second average rate of increase of the\ntime series in the range vector. Breaks in monotonicity (such as counter\nresets due to target restarts) are automatically adjusted for. Also, the\ncalculation extrapolates to the ends of the time range, allowing for missed\nscrapes or imperfect alignment of scrape cycles with the range's time period.\nThe following example expression returns the per-second rate of HTTP requests as measured\nover the last 5 minutes, per time series in the range vector:\n`rate(http_requests_total{job=\"api-server\"}[5m])`\n`rate` acts on native histograms by calculating a new histogram where each\ncompononent (sum and count of observations, buckets) is the rate of increase\nbetween the respective component in the first and last native histogram in\n`v`. However, each element in `v` that contains a mix of float and native\nhistogram samples within the range, will be missing from the result vector.\n`rate` should only be used with counters and native histograms where the\ncomponents behave like counters. It is best suited for alerting, and for\ngraphing of slow-moving counters.\nNote that when combining `rate()` with an aggregation operator (e.g. `sum()`)\nor a function aggregating over time (any function ending in `_over_time`),\nalways take a `rate()` first, then aggregate. Otherwise `rate()` cannot detect\ncounter resets when your target restarts.\n`resets()`\nFor each input time series, `resets(v range-vector)` returns the number of\ncounter resets within the provided time range as an instant vector. Any\ndecrease in the value between two consecutive samples is interpreted as a\ncounter reset.\n`resets` should only be used with counters.\n`round()`\n`round(v instant-vector, to_nearest=1 scalar)` rounds the sample values of all\nelements in `v` to the nearest integer. Ties are resolved by rounding up. The\noptional `to_nearest` argument allows specifying the nearest multiple to which\nthe sample values should be rounded. This multiple may also be a fraction.\n`scalar()`\nGiven a single-element input vector, `scalar(v instant-vector)` returns the\nsample value of that single element as a scalar. If the input vector does not\nhave exactly one element, `scalar` will return `NaN`.\n`sgn()`\n`sgn(v instant-vector)` returns a vector with all sample values converted to their sign, defined as this: 1 if v is positive, -1 if v is negative and 0 if v is equal to zero.\n`sort()`\n`sort(v instant-vector)` returns vector elements sorted by their sample values,\nin ascending order.\n`sort_desc()`\nSame as `sort`, but sorts in descending order.\n`sqrt()`\n`sqrt(v instant-vector)` calculates the square root of all elements in `v`.\n`time()`\n`time()` returns the number of seconds since January 1, 1970 UTC. Note that\nthis does not actually return the current time, but the time at which the\nexpression is to be evaluated.\n`timestamp()`\n`timestamp(v instant-vector)` returns the timestamp of each of the samples of\nthe given vector as the number of seconds since January 1, 1970 UTC.\n`vector()`\n`vector(s scalar)` returns the scalar `s` as a vector with no labels.\n`year()`\n`year(v=vector(time()) instant-vector)` returns the year\nfor each of the given times in UTC.\n`<aggregation>_over_time()`\nThe following functions allow aggregating each series of a given range vector\nover time and return an instant vector with per-series aggregation results:\n\n`avg_over_time(range-vector)`: the average value of all points in the specified interval.\n`min_over_time(range-vector)`: the minimum value of all points in the specified interval.\n`max_over_time(range-vector)`: the maximum value of all points in the specified interval.\n`sum_over_time(range-vector)`: the sum of all values in the specified interval.\n`count_over_time(range-vector)`: the count of all values in the specified interval.\n`quantile_over_time(scalar, range-vector)`: the \u03c6-quantile (0 \u2264 \u03c6 \u2264 1) of the values in the specified interval.\n`stddev_over_time(range-vector)`: the population standard deviation of the values in the specified interval.\n`stdvar_over_time(range-vector)`: the population standard variance of the values in the specified interval.\n`last_over_time(range-vector)`: the most recent point value in specified interval.\n`present_over_time(range-vector)`: the value 1 for any series in the specified interval.\n\nNote that all values in the specified interval have the same weight in the\naggregation even if the values are not equally spaced throughout the interval.\nTrigonometric Functions\nThe trigonometric functions work in radians:\n\n`acos(v instant-vector)`: calculates the arccosine of all elements in `v` (special cases).\n`acosh(v instant-vector)`: calculates the inverse hyperbolic cosine of all elements in `v` (special cases).\n`asin(v instant-vector)`: calculates the arcsine of all elements in `v` (special cases).\n`asinh(v instant-vector)`: calculates the inverse hyperbolic sine of all elements in `v` (special cases).\n`atan(v instant-vector)`: calculates the arctangent of all elements in `v` (special cases).\n`atanh(v instant-vector)`: calculates the inverse hyperbolic tangent of all elements in `v` (special cases).\n`cos(v instant-vector)`: calculates the cosine of all elements in `v` (special cases).\n`cosh(v instant-vector)`: calculates the hyperbolic cosine of all elements in `v` (special cases).\n`sin(v instant-vector)`: calculates the sine of all elements in `v` (special cases).\n`sinh(v instant-vector)`: calculates the hyperbolic sine of all elements in `v` (special cases).\n`tan(v instant-vector)`: calculates the tangent of all elements in `v` (special cases).\n`tanh(v instant-vector)`: calculates the hyperbolic tangent of all elements in `v` (special cases).\n\nThe following are useful for converting between degrees and radians:\n\n`deg(v instant-vector)`: converts radians to degrees for all elements in `v`.\n`pi()`: returns pi.\n",
    "tag": "prometheus"
  }
]