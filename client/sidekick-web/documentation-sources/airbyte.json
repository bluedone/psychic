[
  {
    "title": "API documentation",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/api-documentation.md",
    "content": "API documentation\nOpen-Source API\nAirbyte Open Source doesn't require an API Token for Authentication.\nAll endpoints are acccessible without the token.\n:::caution\nOur Open-Source API (configuration API) is still in an alpha state and might change. You won\u2019t lose any functionality, but you may need to update your code to catch up to any backwards incompatible changes in the API.\n:::\nCheck out the Open-Source API documentation.\nContact us on Slack if you have any questions about it.\n\nAirbyte Cloud API\nAirbyte Cloud will support API access in 2023. See Airbyte roadmap for more details.",
    "tag": "airbyte"
  },
  {
    "title": "Welcome to Airbyte Docs",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/",
    "content": "Welcome to Airbyte Docs\nWhether you are an Airbyte user or contributor, we have docs for you! \nFor Airbyte Cloud users\nBrowse the connector catalog to find the connector you want. In case the connector is not yet supported on Airbyte Cloud, consider using Airbyte Open Source.\nNext, check out the step-by-step tutorial to sign up for Airbyte Cloud, understand Airbyte concepts, and run your first sync. Then learn how to manage your Airbyte Cloud account.\nFor Airbyte Open Source users\nBrowse the connector catalog to find the connector you want. If the connector is not yet supported on Airbyte Open Source, build your own connector.\nNext, check out the Airbyte Open Source QuickStart. Then learn how to deploy and manage Airbyte Open Source in your cloud infrastructure. \nTo get help with Airbyte deployments, check out the Troubleshooting & FAQ, chat with Support on Discourse, or join us on Community Slack.\nFor Airbyte contributors\nTo contribute to Airbyte code, connectors, and documentation, refer to our Contributing Guide. \n   ",
    "tag": "airbyte"
  },
  {
    "title": "CLI documentation",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/cli-documentation.md",
    "content": "CLI documentation\nDisclaimer\nThe project is in alpha version.\nReaders can refer to our opened GitHub issues to check the ongoing work on this project.\nWhat is `octavia` CLI?\nOctavia CLI is a tool to manage Airbyte configurations in YAML.\nIt has the following features:\n\nScaffolding of a readable directory architecture that will host the YAML configs (`octavia init`).\nAuto-generation of YAML config file that matches the resources' schemas (`octavia generate`).\nManage Airbyte resources with YAML config files.\nSafe resources update through diff display and validation (`octavia apply`).\nSimple secret management to avoid versioning credentials.\n\nWhy should I use `octavia` CLI?\nA CLI provides freedom to users to use the tool in whatever context and use case they have.\nThese are non-exhaustive use cases `octavia` can be convenient for:\n\nManaging Airbyte configurations with a CLI instead of a web UI.\nVersioning Airbyte configurations in Git.\nUpdating of Airbyte configurations in an automated deployment pipeline.\nIntegrating the Airbyte configuration deployment in a dev ops tooling stack: Helm, Ansible etc.\nStreamlining the deployment of Airbyte configurations to multiple Airbyte instance.\n\nFeel free to share your use cases with the community in #octavia-cli or on Discourse.\nTable of content\n\nWorkflow\nSecret management\nInstall\nCommands reference\nContributing\nTelemetry\nChangelog\n\nWorkflow\n1. Generate local YAML files for sources or destinations\n\nRetrieve the definition id of the connector you want to use using `octavia list` command.\nGenerate YAML configuration running `octavia generate source <DEFINITION_ID> <SOURCE_NAME>` or `octavia generate destination <DEFINITION_ID> <DESTINATION_NAME>`.\n\n2. Edit your local YAML configurations\n\nEdit the generated YAML configurations according to your need.\nUse the secret management feature feature to avoid storing credentials in the YAML files.\n\n3. Create the declared sources or destinations on your Airbyte instance\n\nRun `octavia apply` to create the sources and destinations\n\n4. Generate connections\n\nRun `octavia octavia generate connection --source <PATH_TO_SOURCE_CONFIG> --destination <PATH_TO_DESTINATION_CONFIG> <CONNECTION_NAME>` to create a YAML configuration for a new connection.\nEdit the created configuration file according to your need: change the scheduling or the replicated streams list.\n\n5. Create the declared connections\n\nRun `octavia apply` to create the newly declared connection on your Airbyte instance.\n\n6. Update your configurations\nChanges in your local configurations can be propagated to your Airbyte instance using `octavia apply`. You will be prompted for validation of changes. You can bypass the validation step using the `--force` flag.\nSecret management\nSources and destinations configurations have credential fields that you do not want to store as plain text in your VCS.\n`octavia` offers secret management through environment variables expansion:\n`yaml\nconfiguration:\n  password: ${MY_PASSWORD}`\nIf you have set a `MY_PASSWORD` environment variable, `octavia apply` will load its value into the `password` field.\nInstall\nRequirements\nWe decided to package the CLI in a docker image with portability in mind.\nPlease install and run Docker if you are not.\nAs a command available in your bash profile\n`bash\ncurl -s -o- https://raw.githubusercontent.com/airbytehq/airbyte/master/octavia-cli/install.sh | bash`\nThis script:\n\nPulls the octavia-cli image from our Docker registry.\nCreates an `octavia` alias in your profile.\nCreates a `~/.octavia` file whose values are mapped to the octavia container's environment variables.\n\nUsing `docker run`\n`bash\ntouch ~/.octavia # Create a file to store env variables that will be mapped the octavia-cli container\nmkdir my_octavia_project_directory # Create your octavia project directory where YAML configurations will be stored.\ndocker run --name octavia-cli -i --rm -v my_octavia_project_directory:/home/octavia-project --network host --user $(id -u):$(id -g) --env-file ~/.octavia airbyte/octavia-cli:0.40.32`\nUsing `docker-compose`\nUsing octavia in docker-compose could be convenient for automatic `apply` on start-up.\nAdd another entry in the services key of your Airbyte `docker-compose.yml`\n`yaml\nservices:\n  # . . .\n  octavia-cli:\n    image: airbyte/octavia-cli:latest\n    command: apply --force\n    env_file:\n      - ~/.octavia # Use a local env file to store variables that will be mapped the octavia-cli container\n    volumes:\n      - <path_to_your_local_octavia_project_directory>:/home/octavia-project\n    depends_on:\n      - webapp`\nOther commands besides `apply` can be run like so:\n`bash\ndocker compose run octavia-cli <command>``\nCommands reference\n`octavia` command flags\n| Flag                                 | Description                                                                   | Env Variable           | Default                                            |\n| ---------------------------------------- | --------------------------------------------------------------------------------- |----------------------------| ------------------------------------------------------ |\n| `--airbyte-url`                          | Airbyte instance URL.                                                             | `AIRBYTE_URL`              | `http://localhost:8000`                                |\n| `--airbyte-username`                     | Airbyte instance username (basic auth).                                           | `AIRBYTE_USERNAME`         | `airbyte`                                              |\n| `--airbyte-password`                     | Airbyte instance password (basic auth).                                           | `AIRBYTE_PASSWORD`         | `password`                                             |\n| `--workspace-id`                         | Airbyte workspace id.                                                             | `AIRBYTE_WORKSPACE_ID`     | The first workspace id found on your Airbyte instance. |\n| `--enable-telemetry/--disable-telemetry` | Enable or disable the sending of telemetry data.                                  | `OCTAVIA_ENABLE_TELEMETRY` | True                                                   |\n| `--api-http-header`                      | HTTP Header value pairs passed while calling Airbyte's API                        | None                       | None                                                   |\n| `--api-http-headers-file-path`           | Path to the YAML file that contains custom HTTP Headers to send to Airbyte's API. | None                       | None                                                   |\nUsing custom HTTP headers\nYou can set custom HTTP headers to send to Airbyte's API with options:\n`bash\noctavia --api-http-header Header-Name Header-Value --api-http-header Header-Name-2 Header-Value-2 list connectors sources`\nYou can also use a custom YAML file (one is already created on init in `api_http_headers.yaml`) to declare the HTTP headers to send to the API:\n`yaml\nheaders:\n  Authorization: Bearer my-secret-token\n  User-Agent: octavia-cli/0.0.0`\nEnvironment variable expansion is available in this Yaml file\n`yaml\nheaders:\n  Authorization: Bearer ${MY_API_TOKEN}`\nOptions based headers are overriding file based headers if an header is declared in both.\n`octavia` subcommands\n| Command                               | Usage                                                                                  |\n| ----------------------------------------- | ------------------------------------------------------------------------------------------ |\n| `octavia init`                        | Initialize required directories for the project.                                           |\n| `octavia list connectors sources`     | List all sources connectors available on the remote Airbyte instance.                      |\n| `octavia list connectors destination` | List all destinations connectors available on the remote Airbyte instance.                 |\n| `octavia list workspace sources`      | List existing sources in current the Airbyte workspace.                                    |\n| `octavia list workspace destinations` | List existing destinations in the current Airbyte workspace.                               |\n| `octavia list workspace connections`  | List existing connections in the current Airbyte workspace.                                |\n| `octavia get source`                  | Get the JSON representation of an existing source in current the Airbyte workspace.        |\n| `octavia get destination`             | Get the JSON representation of an existing destination in the current Airbyte workspace.   |\n| `octavia get connection`              | Get the JSON representation of an existing connection in the current Airbyte workspace.    |\n| `octavia import all`                  | Import all existing sources, destinations and connections to manage them with octavia-cli. |\n| `octavia import source`               | Import an existing source to manage it with octavia-cli.                                   |\n| `octavia import destination`          | Import an existing destination to manage it with octavia-cli.                              |\n| `octavia import connection`           | Import an existing connection to manage it with octavia-cli.                               |\n| `octavia generate source`             | Generate a local YAML configuration for a new source.                                      |\n| `octavia generate destination`        | Generate a local YAML configuration for a new destination.                                 |\n| `octavia generate connection`         | Generate a local YAML configuration for a new connection.                                  |\n| `octavia apply`                       | Create or update Airbyte remote resources according to local YAML configurations.          |\n`octavia init`\nThe `octavia init` commands scaffolds the required directory architecture for running `octavia generate` and `octavia apply` commands.\nExample:\n`bash\n$ mkdir my_octavia_project && cd my_octavia_project\n$ octavia init\n\ud83d\udc19 - Octavia is targetting your Airbyte instance running at http://localhost:8000 on workspace e1f46f7d-5354-4200-aed6-7816015ca54b.\n\ud83d\udc19 - Project is not yet initialized.\n\ud83d\udd28 - Initializing the project.\n\u2705 - Created the following directories: sources, destinations, connections.\n$ ls\nconnections  destinations sources`\n`octavia list connectors sources`\nList all the source connectors currently available on your Airbyte instance.\nExample:\n`bash\n$ octavia list connectors sources\nNAME                            DOCKER REPOSITORY                              DOCKER IMAGE TAG  SOURCE DEFINITION ID\nAirtable                        airbyte/source-airtable                        0.1.1             14c6e7ea-97ed-4f5e-a7b5-25e9a80b8212\nAWS CloudTrail                  airbyte/source-aws-cloudtrail                  0.1.4             6ff047c0-f5d5-4ce5-8c81-204a830fa7e1\nAmazon Ads                      airbyte/source-amazon-ads                      0.1.3             c6b0a29e-1da9-4512-9002-7bfd0cba2246\nAmazon Seller Partner           airbyte/source-amazon-seller-partner           0.2.16            e55879a8-0ef8-4557-abcf-ab34c53ec460`\n`octavia list connectors destinations`\nList all the destinations connectors currently available on your Airbyte instance.\nExample:\n`bash\n$ octavia list connectors destinations\nNAME                                  DOCKER REPOSITORY                                 DOCKER IMAGE TAG  DESTINATION DEFINITION ID\nAzure Blob Storage                    airbyte/destination-azure-blob-storage            0.1.3             b4c5d105-31fd-4817-96b6-cb923bfc04cb\nAmazon SQS                            airbyte/destination-amazon-sqs                    0.1.0             0eeee7fb-518f-4045-bacc-9619e31c43ea\nBigQuery                              airbyte/destination-bigquery                      0.6.11            22f6c74f-5699-40ff-833c-4a879ea40133\nBigQuery (denormalized typed struct)  airbyte/destination-bigquery-denormalized         0.2.10            079d5540-f236-4294-ba7c-ade8fd918496`\n`octavia list workspace sources`\nList all the sources existing on your targeted Airbyte instance.\nExample:\n`bash\n$ octavia list workspace sources\nNAME     SOURCE NAME  SOURCE ID\nweather  OpenWeather  c4aa8550-2122-4a33-9a21-adbfaa638544`\n`octavia list workspace destinations`\nList all the destinations existing on your targeted Airbyte instance.\nExample:\n`bash\n$ octavia list workspace destinations\nNAME   DESTINATION NAME  DESTINATION ID\nmy_db  Postgres          c0c977c2-48e7-46fe-9f57-576285c26d42`\n`octavia list workspace connections`\nList all the connections existing on your targeted Airbyte instance.\nExample:\n`bash\n$ octavia list workspace connections\nNAME           CONNECTION ID                         STATUS  SOURCE ID                             DESTINATION ID\nweather_to_pg  a4491317-153e-436f-b646-0b39338f9aab  active  c4aa8550-2122-4a33-9a21-adbfaa638544  c0c977c2-48e7-46fe-9f57-576285c26d42`\n`octavia get source <SOURCE_ID> or <SOURCE_NAME>`\nGet an existing source in current the Airbyte workspace. You can use a source ID or name.\n| Argument  | Description  |\n| ------------- | ---------------- |\n| `SOURCE_ID`   | The source id.   |\n| `SOURCE_NAME` | The source name. |\nExamples:\n`bash\n$ octavia get source c0c977c2-48e7-46fe-9f57-576285c26d42\n{'connection_configuration': {'key': '**********',\n                              'start_date': '2010-01-01T00:00:00.000Z',\n                              'token': '**********'},\n 'name': 'Pokemon',\n 'source_definition_id': 'b08e4776-d1de-4e80-ab5c-1e51dad934a2',\n 'source_id': 'c0c977c2-48e7-46fe-9f57-576285c26d42',\n 'source_name': 'My Poke',\n 'workspace_id': 'c4aa8550-2122-4a33-9a21-adbfaa638544'}`\n`bash\n$ octavia get source \"My Poke\"\n{'connection_configuration': {'key': '**********',\n                              'start_date': '2010-01-01T00:00:00.000Z',\n                              'token': '**********'},\n 'name': 'Pokemon',\n 'source_definition_id': 'b08e4776-d1de-4e80-ab5c-1e51dad934a2',\n 'source_id': 'c0c977c2-48e7-46fe-9f57-576285c26d42',\n 'source_name': 'My Poke',\n 'workspace_id': 'c4aa8550-2122-4a33-9a21-adbfaa638544'}`\n`octavia get destination <DESTINATION_ID> or <DESTINATION_NAME>`\nGet an existing destination in current the Airbyte workspace. You can use a destination ID or name.\n| Argument       | Description       |\n| ------------------ | --------------------- |\n| `DESTINATION_ID`   | The destination id.   |\n| `DESTINATION_NAME` | The destination name. |\nExamples:\n`bash\n$ octavia get destination c0c977c2-48e7-46fe-9f57-576285c26d42\n{\n  \"destinationDefinitionId\": \"c0c977c2-48e7-46fe-9f57-576285c26d42\",\n  \"destinationId\": \"18102e7c-5160-4000-841b-15e8ec48c301\",\n  \"workspaceId\": \"18102e7c-5160-4000-883a-30bc7cd65601\",\n  \"connectionConfiguration\": {\n    \"user\": \"charles\"\n  },\n  \"name\": \"pg\",\n  \"destinationName\": \"Postgres\"\n}`\n`bash\n$ octavia get destination pg\n{\n  \"destinationDefinitionId\": \"18102e7c-5160-4000-821f-4d7cfdf87201\",\n  \"destinationId\": \"18102e7c-5160-4000-841b-15e8ec48c301\",\n  \"workspaceId\": \"18102e7c-5160-4000-883a-30bc7cd65601\",\n  \"connectionConfiguration\": {\n    \"user\": \"charles\"\n  },\n  \"name\": \"string\",\n  \"destinationName\": \"string\"\n}`\n`octavia get connection <CONNECTION_ID> or <CONNECTION_NAME>`\nGet an existing connection in current the Airbyte workspace. You can use a connection ID or name.\n| Argument      | Description      |\n| ----------------- | -------------------- |\n| `CONNECTION_ID`   | The connection id.   |\n| `CONNECTION_NAME` | The connection name. |\nExample:\n`bash\n$ octavia get connection c0c977c2-48e7-46fe-9f57-576285c26d42\n{\n  \"connectionId\": \"c0c977c2-48e7-46fe-9f57-576285c26d42\",\n  \"name\": \"Poke To PG\",\n  \"namespaceDefinition\": \"source\",\n  \"namespaceFormat\": \"${SOURCE_NAMESPACE}\",\n  \"prefix\": \"string\",\n  \"sourceId\": \"18102e7c-5340-4000-8eaa-4a86f844b101\",\n  \"destinationId\": \"18102e7c-5340-4000-8e58-6bed49c24b01\",\n  \"operationIds\": [\n    \"18102e7c-5340-4000-8ef0-f35c05a49a01\"\n  ],\n  \"syncCatalog\": {\n    \"streams\": [\n      {\n        \"stream\": {\n          \"name\": \"string\",\n          \"jsonSchema\": {},\n          \"supportedSyncModes\": [\n            \"full_refresh\"\n          ],\n          \"sourceDefinedCursor\": false,\n          \"defaultCursorField\": [\n            \"string\"\n          ],\n          \"sourceDefinedPrimaryKey\": [\n            [\n              \"string\"\n            ]\n          ],\n          \"namespace\": \"string\"\n        },\n        \"config\": {\n          \"syncMode\": \"full_refresh\",\n          \"cursorField\": [\n            \"string\"\n          ],\n          \"destinationSyncMode\": \"append\",\n          \"primaryKey\": [\n            [\n              \"string\"\n            ]\n          ],\n          \"aliasName\": \"string\",\n          \"selected\": false\n        }\n      }\n    ]\n  },\n  \"schedule\": {\n    \"units\": 0,\n    \"timeUnit\": \"minutes\"\n  },\n  \"status\": \"active\",\n  \"resourceRequirements\": {\n    \"cpu_request\": \"string\",\n    \"cpu_limit\": \"string\",\n    \"memory_request\": \"string\",\n    \"memory_limit\": \"string\"\n  },\n  \"sourceCatalogId\": \"18102e7c-5340-4000-85f3-204ab7715801\"\n}`\n`bash\n$ octavia get connection \"Poke To PG\"\n{\n  \"connectionId\": \"c0c977c2-48e7-46fe-9f57-576285c26d42\",\n  \"name\": \"Poke To PG\",\n  \"namespaceDefinition\": \"source\",\n  \"namespaceFormat\": \"${SOURCE_NAMESPACE}\",\n  \"prefix\": \"string\",\n  \"sourceId\": \"18102e7c-5340-4000-8eaa-4a86f844b101\",\n  \"destinationId\": \"18102e7c-5340-4000-8e58-6bed49c24b01\",\n  \"operationIds\": [\n    \"18102e7c-5340-4000-8ef0-f35c05a49a01\"\n  ],\n  \"syncCatalog\": {\n    \"streams\": [\n      {\n        \"stream\": {\n          \"name\": \"string\",\n          \"jsonSchema\": {},\n          \"supportedSyncModes\": [\n            \"full_refresh\"\n          ],\n          \"sourceDefinedCursor\": false,\n          \"defaultCursorField\": [\n            \"string\"\n          ],\n          \"sourceDefinedPrimaryKey\": [\n            [\n              \"string\"\n            ]\n          ],\n          \"namespace\": \"string\"\n        },\n        \"config\": {\n          \"syncMode\": \"full_refresh\",\n          \"cursorField\": [\n            \"string\"\n          ],\n          \"destinationSyncMode\": \"append\",\n          \"primaryKey\": [\n            [\n              \"string\"\n            ]\n          ],\n          \"aliasName\": \"string\",\n          \"selected\": false\n        }\n      }\n    ]\n  },\n  \"schedule\": {\n    \"units\": 0,\n    \"timeUnit\": \"minutes\"\n  },\n  \"status\": \"active\",\n  \"resourceRequirements\": {\n    \"cpu_request\": \"string\",\n    \"cpu_limit\": \"string\",\n    \"memory_request\": \"string\",\n    \"memory_limit\": \"string\"\n  },\n  \"sourceCatalogId\": \"18102e7c-5340-4000-85f3-204ab7715801\"\n}`\n`octavia import all`\nImport all existing resources (sources, destinations, connections) on your Airbyte instance to manage them with octavia-cli.\nExamples:\n`bash\n$ octavia import all\n\ud83d\udc19 - Octavia is targetting your Airbyte instance running at http://localhost:8000 on workspace b06c6fbb-cadd-4c5c-bdbb-710add7dedb9.\n\u2705 - Imported source poke in sources/poke/configuration.yaml. State stored in sources/poke/state_b06c6fbb-cadd-4c5c-bdbb-710add7dedb9.yaml\n\u26a0\ufe0f  - Please update any secrets stored in sources/poke/configuration.yaml\n\u2705 - Imported destination Postgres in destinations/postgres/configuration.yaml. State stored in destinations/postgres/state_b06c6fbb-cadd-4c5c-bdbb-710add7dedb9.yaml\n\u26a0\ufe0f  - Please update any secrets stored in destinations/postgres/configuration.yaml\n\u2705 - Imported connection poke-to-pg in connections/poke_to_pg/configuration.yaml. State stored in connections/poke_to_pg/state_b06c6fbb-cadd-4c5c-bdbb-710add7dedb9.yaml`\nYou know have local configuration files for all Airbyte resources that were already existing.\nYou need to edit any secret values that exist in these configuration files as secrets are not imported.\nYou can edit the configuration files and run `octavia apply` to continue managing them with octavia-cli.\n`octavia import destination <DESTINATION_ID> or <DESTINATION_NAME>`\nImport an existing destination to manage it with octavia-cli. You can use a destination ID or name.\n| Argument       | Description       |\n| ------------------ | --------------------- |\n| `DESTINATION_ID`   | The destination id.   |\n| `DESTINATION_NAME` | The destination name. |\n`octavia import source <SOURCE_ID> or <SOURCE_NAME>`\nImport an existing source to manage it with octavia-cli. You can use a source ID or name.\n| Argument  | Description  |\n| ------------- | ---------------- |\n| `SOURCE_ID`   | The source id.   |\n| `SOURCE_NAME` | The source name. |\nExamples:\n`bash\n$ octavia import source poke\n\ud83d\udc19 - Octavia is targetting your Airbyte instance running at http://localhost:8000 on workspace 75658e4f-e5f0-4e35-be0c-bdad33226c94.\n\u2705 - Imported source poke in sources/poke/configuration.yaml. State stored in sources/poke/state_75658e4f-e5f0-4e35-be0c-bdad33226c94.yaml\n\u26a0\ufe0f  - Please update any secrets stored in sources/poke/configuration.yaml`\nYou know have local configuration file for an Airbyte source that was already existing.\nYou need to edit any secret value that exist in this configuration as secrets are not imported.\nYou can edit the configuration and run `octavia apply` to continue managing it with octavia-cli.\n`octavia import destination <DESTINATION_ID> or <DESTINATION_NAME>`\nImport an existing destination to manage it with octavia-cli. You can use a destination ID or name.\n| Argument       | Description       |\n| ------------------ | --------------------- |\n| `DESTINATION_ID`   | The destination id.   |\n| `DESTINATION_NAME` | The destination name. |\nExamples:\n`bash\n$ octavia import destination pg\n\ud83d\udc19 - Octavia is targetting your Airbyte instance running at http://localhost:8000 on workspace 75658e4f-e5f0-4e35-be0c-bdad33226c94.\n\u2705 - Imported destination pg in destinations/pg/configuration.yaml. State stored in destinations/pg/state_75658e4f-e5f0-4e35-be0c-bdad33226c94.yaml\n\u26a0\ufe0f  - Please update any secrets stored in destinations/pg/configuration.yaml`\nYou know have local configuration file for an Airbyte destination that was already existing.\nYou need to edit any secret value that exist in this configuration as secrets are not imported.\nYou can edit the configuration and run `octavia apply` to continue managing it with octavia-cli.\n`octavia import connection <CONNECTION_ID> or <CONNECTION_NAME>`\nImport an existing connection to manage it with octavia-cli. You can use a connection ID or name.\n| Argument      | Description      |\n| ----------------- | -------------------- |\n| `CONNECTION_ID`   | The connection id.   |\n| `CONNECTION_NAME` | The connection name. |\nExamples:\n`bash\n$ octavia import connection poke-to-pg\n\ud83d\udc19 - Octavia is targetting your Airbyte instance running at http://localhost:8000 on workspace 75658e4f-e5f0-4e35-be0c-bdad33226c94.\n\u2705 - Imported connection poke-to-pg in connections/poke-to-pg/configuration.yaml. State stored in connections/poke-to-pg/state_75658e4f-e5f0-4e35-be0c-bdad33226c94.yaml\n\u26a0\ufe0f  - Please update any secrets stored in connections/poke-to-pg/configuration.yaml`\nYou know have local configuration file for an Airbyte connection that was already existing.\nN.B.: You first need to import the source and destination used by the connection.\nYou can edit the configuration and run `octavia apply` to continue managing it with octavia-cli.\n`octavia generate source <DEFINITION_ID> <SOURCE_NAME>`\nGenerate a YAML configuration for a source.\nThe YAML file will be stored at `./sources/<resource_name>/configuration.yaml`.\n| Argument    | Description                                                                               |\n| --------------- | --------------------------------------------------------------------------------------------- |\n| `DEFINITION_ID` | The source connector definition id. Can be retrieved using `octavia list connectors sources`. |\n| `SOURCE_NAME`   | The name you want to give to this source in Airbyte.                                          |\nExample:\n`bash\n$ octavia generate source d8540a80-6120-485d-b7d6-272bca477d9b weather\n\u2705 - Created the source template for weather in ./sources/weather/configuration.yaml.`\n`octavia generate destination <DEFINITION_ID> <DESTINATION_NAME>`\nGenerate a YAML configuration for a destination.\nThe YAML file will be stored at `./destinations/<destination_name>/configuration.yaml`.\n| Argument       | Description                                                                                         |\n| ------------------ | ------------------------------------------------------------------------------------------------------- |\n| `DEFINITION_ID`    | The destination connector definition id. Can be retrieved using `octavia list connectors destinations`. |\n| `DESTINATION_NAME` | The name you want to give to this destination in Airbyte.                                               |\nExample:\n`bash\n$ octavia generate destination 25c5221d-dce2-4163-ade9-739ef790f503 my_db\n\u2705 - Created the destination template for my_db in ./destinations/my_db/configuration.yaml.`\n`octavia generate connection --source <path-to-source-configuration.yaml> --destination <path-to-destination-configuration.yaml> <CONNECTION_NAME>`\nGenerate a YAML configuration for a connection.\nThe YAML file will be stored at `./connections/<connection_name>/configuration.yaml`.\n| Option      | Required | Description                                                                            |\n| --------------- | ------------ | ------------------------------------------------------------------------------------------ |\n| `--source`      | Yes          | Path to the YAML configuration file of the source you want to create a connection from.    |\n| `--destination` | Yes          | Path to the YAML configuration file of the destination you want to create a connection to. |\n| Argument      | Description                                          |\n| ----------------- | -------------------------------------------------------- |\n| `CONNECTION_NAME` | The name you want to give to this connection in Airbyte. |\nExample:\n`bash\n$ octavia generate connection --source sources/weather/configuration.yaml --destination destinations/my_db/configuration.yaml weather_to_pg\n\u2705 - Created the connection template for weather_to_pg in ./connections/weather_to_pg/configuration.yaml.`\n`octavia apply`\nCreate or update the resource on your Airbyte instance according to local configurations found in your octavia project directory.\nIf the resource was not found on your Airbyte instance, apply will create the remote resource.\nIf the resource was found on your Airbyte instance, apply will prompt you for validation of the changes and will run an update of your resource.\nPlease note that if a secret field was updated on your configuration, apply will run this change without prompt.\n| Option | Required | Description                                                    |\n| ---------- | ------------ | ------------------------------------------------------------------ |\n| `--file`   | No           | Path to the YAML configuration files you want to create or update. |\n| `--force`  | No           | Run update without prompting for changes validation.               |\nExample:\n`bash\n$ octavia apply\n\ud83d\udc19 - weather exists on your Airbyte instance, let's check if we need to update it!\n\ud83d\udc40 - Here's the computed diff (\ud83d\udea8 remind that diff on secret fields are not displayed):\n  E - Value of root['lat'] changed from \"46.7603\" to \"45.7603\".\n\u2753 - Do you want to update weather? [y/N]: y\n\u270d\ufe0f - Running update because a diff was detected between local and remote resource.\n\ud83c\udf89 - Successfully updated weather on your Airbyte instance!\n\ud83d\udcbe - New state for weather stored at ./sources/weather/state_<workspace_id>.yaml.\n\ud83d\udc19 - my_db exists on your Airbyte instance, let's check if we need to update it!\n\ud83d\ude34 - Did not update because no change detected.\n\ud83d\udc19 - weather_to_pg exists on your Airbyte instance, let's check if we need to update it!\n\ud83d\udc40 - Here's the computed diff (\ud83d\udea8 remind that diff on secret fields are not displayed):\n  E - Value of root['schedule']['timeUnit'] changed from \"days\" to \"hours\".\n\u2753 - Do you want to update weather_to_pg? [y/N]: y\n\u270d\ufe0f - Running update because a diff was detected between local and remote resource.\n\ud83c\udf89 - Successfully updated weather_to_pg on your Airbyte instance!\n\ud83d\udcbe - New state for weather_to_pg stored at ./connections/weather_to_pg/state_<workspace_id>.yaml.`\nContributing\n\nPlease sign up to Airbyte's Slack workspace and join the `#octavia-cli`. We'll sync up community efforts in this channel.\nPick an existing GitHub issues or open a new one to explain what you'd like to implement.\nAssign the GitHub issue to yourself.\nFork Airbyte's repo, code and test thoroughly.\nOpen a PR on our Airbyte repo from your fork.\n\nDeveloping locally\n\nBuild the project locally (from the root of Airbyte's repo): `SUB_BUILD=OCTAVIA_CLI ./gradlew build # from the root directory of the repo`.\nInstall Python 3.8.12. We suggest doing it through `pyenv`.\nCreate a virtualenv: `python -m venv .venv`.\nActivate the virtualenv: `source .venv/bin/activate`.\nInstall dev dependencies: `pip install -e .\\[tests\\]`.\nInstall `pre-commit` hooks: `pre-commit install`.\nRun the unittest suite: `pytest --cov=octavia_cli`. Note, a local version of airbyte needs to be running (e.g. `docker compose up` from the root directory of the project)\nMake sure the build passes (step 0) before opening a PR.\n\nTelemetry\nThis CLI has some telemetry tooling to send Airbyte some data about the usage of this tool.\nWe will use this data to improve the CLI and measure its adoption.\nThe telemetry sends data about:\n\nWhich command was run (not the arguments or options used).\nSuccess or failure of the command run and the error type (not the error payload).\nThe current Airbyte workspace id if the user has not set the anonymous data collection on their Airbyte instance.\n\nYou can disable telemetry by setting the `OCTAVIA_ENABLE_TELEMETRY` environment variable to `False` or using the `--disable-telemetry` flag.",
    "tag": "airbyte"
  },
  {
    "title": "Collecting Metrics",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/operator-guides/collecting-metrics.md",
    "content": "Collecting Metrics\nAirbyte supports two ways to collect metrics - using datadog or open telemetry. \nFill in `METRIC_CLIENT` field in `.env` file to get started!\nPrerequisite: \nIn order to get metrics from airbyte we need to deploy a container / pod called metrics-reporter like below \n`airbyte-metrics: \n  image: airbyte/metrics-reporter:${VERSION} \n  container_name: airbyte-metrics \n  environment: \n    - METRIC_CLIENT=${METRIC_CLIENT} \n    - OTEL_COLLECTOR_ENDPOINT=${OTEL_COLLECTOR_ENDPOINT}`\nOpen Telemetry\n\nIn `.env` change `METRIC_CLIENT` to `otel`. \nSimilarly, configure `OTEL_COLLECTOR_ENDPOINT` to tell Airbyte where to send metrics RPC to.\n\nExample\nRun Opentelemetry and Airbyte locally\nIn this example we will run Airbyte locally along with an Open Telemetry Collector. The Open telemetry collector\nwill expose port 4317 to the localhost as the receiving endpoint.\n\nSteps:\n\nSetting up Open telemetry. In this example we will use the repository from `opentelemetry-java-docs`. \nRun the following commands to have it up and running.\n\n`bash\n  git clone https://github.com/open-telemetry/opentelemetry-java-docs\n  cd opentelemetry-java-docs/otlp/docker\n  docker-compose up`\n\nConfigure Airbyte `.env` file. \nChange `METRIC_CLIENT` to `otel` to indicate Airbyte to use Open telemetry to emit metric data.\nChange `OTEL_COLLECTOR_ENDPOINT` to `\"http://host.docker.internal:4317\"` because Open Telemetry \n   Collector has enabled port forward from localhost:4317 to container port 4317. To send data to Collector container port 4317, we want to need to export data to physical machine's localhost:4317, which in docker will be represented as `http://host.docker.internal:4317`. \nDo not use `localhost:4317` or you will send data to the same container where Airbyte Worker is running.\n\n\nStart Airbyte server by running `docker compose up` under airbyte repository. Go to `localhost:8000` to visit Airbyte and start a sync, then go to `localhost:9090` to access Prometheus - you should be able to see the metrics there. Alternatively,\n\nRun Opentelemetry and Airbyte on kubernetes\n\nPrerequisite: Read https://github.com/airbytehq/airbyte/blob/master/docs/deploying-airbyte/on-kubernetes.md to understand how to start Airbyte on Kubernetes\n\nWe will use `stable` in this example.\nSteps:\n1. Run open telemetry collector in the same Kubernetes context. Here we follow example in OpenTelemetry doc\n2. edit `kube/overlays/stable/.env` and add the following lines:\n`aidl\nMETRIC_CLIENT=otel\nOTEL_COLLECTOR_ENDPOINT=<address>`\nIf you started open telemetry collector in the link above, the address should be `http://otel-collector:4317`. \nNote the format - unlike the base `.env`, there is no quote in `.env` file under kubernetes.\nTutorial\nDeploy the airbyte metric pod :\n```\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ .Release.Name }}-metrics\n  namespace: {{ .Release.Namespace }}\n  labels: {{ set . \"component\" \"metrics\" | include \"labels\" | nindent 4 }}\nspec:\n  selector:\n    matchLabels: {{ set . \"component\" \"metrics\" | include \"labels\" | nindent 6 }}\n  template:\n    metadata:\n      labels: {{ set . \"component\" \"metrics\" | include \"labels\" | nindent 8 }}\n    spec:\n      containers:\n      - name: airbyte-metrics\n        image: \"airbyte/metrics-reporter:latest\"\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: AIRBYTE_VERSION\n          value: latest\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: {{ include \"airbyte.database.secret.name\" . }}\n              key: DATABASE_PASSWORD\n        - name: DATABASE_URL\n          value: {{ include \"airbyte.database.url\" . | quote }}\n        - name: DATABASE_USER\n          valueFrom:\n            secretKeyRef:\n              name: {{ .Release.Name }}-secrets\n              key: DATABASE_USER\n        - name: CONFIGS_DATABASE_MINIMUM_FLYWAY_MIGRATION_VERSION\n          value: 0.35.15.001\n        - name: METRIC_CLIENT\n          value: otel\n        - name: OTEL_COLLECTOR_ENDPOINT\n          value: http://otel-collector:4317\n```\nDeploy an Open telemetry pod like below :\n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: otel-collector\n  namespace: {{ .Release.Namespace }}\n  labels: {{ set . \"component\" \"otel-collector\" | include \"labels\" | nindent 4 }} \nspec:\n  selector:\n    matchLabels: {{ set . \"component\" \"otel-collector\" | include \"labels\" | nindent 6 }} \n  replicas: 1\n  template:\n    metadata:\n      labels: {{ set . \"component\" \"otel-collector\" | include \"labels\" | nindent 8 }} \n    spec:\n      containers:\n      - command:\n          - \"/otelcol\"\n          - \"--config=/conf/otel-collector-config.yaml\"\n        image: \"otel/opentelemetry-collector:latest\"\n        name: otel-collector\n        ports:\n        - containerPort: 4317 # Default endpoint for OpenTelemetry receiver.\n        - containerPort: 8889 # Port for Prometheus instance\n        volumeMounts:\n        - name: config\n          mountPath: /conf\n      volumes:\n        - configMap:\n            name: otel-collector-conf\n            items:\n              - key: otel-collector-config\n                path: otel-collector-config.yaml\n          name: config`\nWIth this Config Map :\n`apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: otel-collector-conf\n  namespace: {{ .Release.Namespace }}\n  labels: {{ set . \"component\" \"otel-collector\" | include \"labels\" | nindent 4 }} \ndata:\n  otel-collector-config: |\n    receivers:\n      otlp:\n        protocols:\n          grpc:\n           endpoint: \"0.0.0.0:4317\"\n    processors:\n      batch:\n      memory_limiter:\n        limit_mib: 1500\n        spike_limit_mib: 512\n        check_interval: 5s\n    exporters:\n      prometheus:\n        endpoint: \"0.0.0.0:8889\"\n        namespace: \"default\"\n    service:\n      pipelines:\n        metrics:\n          receivers: [otlp]\n          processors: [memory_limiter, batch]\n          exporters: [prometheus]`\nThen we need a service to be able to access both open telemetry GRPC and Prometheus\n`apiVersion: v1\nkind: Service\nmetadata:\n  name: otel-collector\n  namespace: {{ .Release.Namespace }}\n  labels: {{ set . \"component\" \"otel-collector\" | include \"labels\" | nindent 4 }} \nspec:\n  ports:\n  - name: otlp-grpc # Default endpoint for OpenTelemetry gRPC receiver.\n    port: 4317\n    protocol: TCP\n    targetPort: 4317\n  - name: prometheus\n    port: 8889\n  selector: {{ set . \"component\" \"otel-collector\" | include \"labels\" | nindent 6 }}`\nAnd finally We can add a service monitor to receive metrics in prometheus and optionally add some prometheus rules to generate alerts. \nYou can replace with your prometheus name.\n`apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: {{ .Release.Name }}\n  namespace: {{ .Release.Namespace }}\n  labels:\n    {{ set . \"component\" \"metrics\" | include \"labels\" | nindent 4 }}\n    prometheus: <your_prometheus_name>\nspec:\n  endpoints:\n    - interval: 30s\n      port: prometheus\n      path: /metrics\n      relabelings:\n      - action: labeldrop\n        regex: (service|endpoint|namespace|container)\n  selector:\n    matchLabels: {{ set . \"component\" \"otel-collector\" | include \"labels\" | nindent 6 }}`\nOne rule example :\n`apiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: {{ .Release.Name }}-rules\n  namespace: {{ .Release.Namespace }}\n  labels:\n    {{ set . \"component\" \"prometheus-rules\" | include \"labels\" | nindent 4 }}\n    prometheus: <your_prometheus_name>\nspec:\n  groups:\n    - name: airbyte\n      rules:\n        - alert: AirbyteJobFail\n          for: 0m\n          expr: min(airbyte_job_failed_by_release_stage) > 0\n          labels:\n            priority: P2\n          annotations:\n            summary: {{ `\"An Airbyte Job has failed\"` }}`\nDatadog\nThe set up for Datadog is pretty straightforward. \nSet two env vars:\n1) `METRIC_CLIENT` to `datadog`.\n2) `PUBLISH_METRICS` to `true`.\nConfigure two additional env vars to the Datadog endpoint:\n1) Set `DD_AGENT_HOST` to the IP where the Datadog agent is running.\n2) Set `DD_DOGSTATSD_PORT` to the port the Datadog agent is using.\nMetrics\nVisit OssMetricsRegistry.java to get a complete list of metrics Airbyte is sending.\nAdditional information",
    "tag": "airbyte"
  },
  {
    "title": "Using custom connectors",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/operator-guides/using-custom-connectors.md",
    "content": "Using custom connectors\nIf our connector catalog does not fulfill your needs, you can build your own Airbyte connectors. \nThere are two approaches you can take while jumping on connector development project:\n1. You want to build a connector for an external source or destination (public API, off-the-shelf DBMS, data warehouses, etc.). In this scenario, your connector development will probably benefit the community. The right way is to open a PR on our repo to add your connector to our catalog. You will then benefit from an Airbyte team review and potential future improvements and maintenance from the community.\n2. You want to build a connector for an internal source or destination (private API) specific to your organization. This connector has no good reason to be exposed to the community.\nThis guide focuses on the second approach and assumes the following:\n* You followed our other guides and tutorials about connector developments.\n* You finished your connector development, running it locally on an Airbyte development instance.\n* You want to deploy this connector to a production Airbyte instance running on a VM with docker-compose or on a Kubernetes cluster.\nIf you prefer video tutorials, we recorded a demo about uploading connectors images to a GCP Artifact Registry.\n1. Create a private Docker registry\nAirbyte needs to pull its Docker images from a remote Docker registry to consume a connector.\nYou should host your custom connectors image on a private Docker registry. \nHere are some resources to create a private Docker registry, in case your organization does not already have one:\n| Cloud provider | Service name                | Documentation                                                                                                                                                                                                                                                                              |\n|----------------|-----------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Google Cloud   | Artifact Registry           | Quickstart|\n| AWS            | Amazon ECR                  | Getting started with Amazon ECR|\n| Azure          | Container Registry          | Quickstart|\n| DockerHub      | Repositories                | DockerHub Quickstart|\n| Self hosted    | Open-source Docker Registry | Deploy a registry server|\n2. Authenticate to your private Docker registry\nTo push and pull images to your private Docker registry, you need to authenticate to it:\n* Your local or CI environment (where you build your connector image) must be able to push images to your registry.\n* Your Airbyte instance must be able to pull images from your registry.\nFor Docker-compose Airbyte deployments\nOn GCP - Artifact Registry:\nGCP offers the `gcloud` credential helper to log in to your Artifact registry.\nPlease run the command detailed here to authenticate your local environment/CI environment to your Artifact registry.\nRun the same authentication flow on your Compute Engine instance.\nIf you do not want to use `gcloud`, GCP offers other authentication methods detailed here.\nOn AWS - Amazon ECR:\nYou can authenticate to an ECR private registry using the `aws` CLI:\n`aws ecr get-login-password --region region | docker login --username AWS --password-stdin aws_account_id.dkr.ecr.region.amazonaws.com`\nYou can find details about this command and other available authentication methods here.\nYou will have to authenticate your local/CI environment (where you build your image) and your EC2 instance where your Airbyte instance is running.\nOn Azure - Container Registry:\nYou can authenticate to an Azure Container Registry using the `az` CLI:\n`az acr login --name <registry-name>`\nYou can find details about this command here\nYou will have to authenticate both your local/CI environment/ environment (where your image is built) and your Azure Virtual Machine instance where the Airbyte instance is running.\nOn DockerHub - Repositories:\nYou can use Docker Desktop to authenticate your local machine to your DockerHub registry by signing in on the desktop application using your DockerID.\nYou need to use a service account to authenticate your Airbyte instance to your DockerHub registry.\nSelf hosted - Open source Docker Registry:\nIt would be best to set up auth on your Docker registry to make it private. Available authentication options for an open-source Docker registry are listed here.\nTo authenticate your local/CI environment and Airbyte instance you can use the docker login command.\nFor Kubernetes Airbyte deployments\nYou can use the previous section's authentication flow to authenticate your local/CI to your private Docker registry.\nIf you provisioned your Kubernetes cluster using AWS EKS, GCP GKE, or Azure AKS: it is very likely that you already allowed your cluster to pull images from the respective container registry service of your cloud provider.\nIf you want Airbyte to pull images from another private Docker registry, you will have to do the following:\n1. Create a `Secret` in Kubernetes that will host your authentication credentials. This Kubernetes documentation explains how to proceed.\n2. Set the `JOB_KUBE_MAIN_CONTAINER_IMAGE_PULL_SECRET` environment variable on the `airbyte-worker` pod. The value must be the name of your previously created Kubernetes Secret.\n3. Push your connector image to your private Docker registry\n\nBuild and tag your connector image locally, e.g.: `docker build . -t my-custom-connectors/source-custom:0.1.0`\nCreate your image tag with `docker tag` command. The structure of the remote tag depends on your cloud provider's container registry service. Please check their online documentation linked at the top.\nUse `docker push <image-name>:<tag>` to push the image to your private Docker registry.\n\nYou should run all the above commands from your local/CI environment, where your connector source code is available.\n4. Use your custom connector in Airbyte\nAt this step, you should have:\n* A private Docker registry hosting your custom connector image.\n* Authenticated your Airbyte instance to your private Docker registry.\nYou can pull your connector image from your private registry to validate the previous steps. On your Airbyte instance: run `docker pull <image-name>:<tag>` if you are using our `docker-compose` deployment, or start a pod that is using the connector image.\n1. Click on Settings\n\n2. Click on Sources (or Destinations)\n\n3. Click on + New connector\n\n4. Fill the name of your custom connector\n\n5. Fill the Docker image name of your custom connector\n\n6. Fill the Docker Tag of your custom connector image\n\n7. Fill the URL to your connector documentation\nThis is a required field at the moment, but you can fill with any value if you do not have online documentation for your connector.\nThis documentation will be linked in the connector setting page.\n\n8. Click on Add",
    "tag": "airbyte"
  },
  {
    "title": "Upgrading Airbyte",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/operator-guides/upgrading-airbyte.md",
    "content": "Upgrading Airbyte\nOverview\nThis tutorial will describe how to determine if you need to run this upgrade process, and if you do, how to do so. This process does require temporarily turning off Airbyte.\nWhen Airbyte is upgraded, it will attempt to upgrade some connector versions. It follows the following rules: 1. if a connector is not used, it will be upgraded to the latest version 2. if a connector is used, it will NOT be upgraded to avoid disrupting working workflows. If you want to upgrade a connector, do so in the settings page in the webapp.\nDetermining if you need to Upgrade\nAirbyte intelligently performs upgrades automatically based off of your version defined in your `.env` file and will handle data migration for you.\nIf you are running Airbyte on Kubernetes, you will need to use one of the two processes defined here that differ based on your Airbyte version.\nMandatory Intermediate Upgrade\nIf your current version of airbyte is < v0.32.0-alpha-patch-1, you first need to upgrade to this version before upgrading to any later version.\nThe reason for this is that there are breaking changes made in v0.32.0-alpha-patch-1, and the logic for these changes is removed in later versions, making it impossible to upgrade directly.\nTo upgrade to v0.32.0-alpha-patch-1, follow the steps in the following sections, but replace the `docker pull` or `wget` commands with the following:\n\nIf you are in a cloned Airbyte repo, v0.32.0-alpha-patch-1 can be pulled from GitHub with\n\n`git checkout v0.32.0-alpha-patch-1`\n\nIf you are running Airbyte from downloaded `docker-compose.yaml` and `.env` files without a GitHub repo, run `wget -N https://raw.githubusercontent.com/airbytehq/airbyte/v0.32.0-alpha-patch-1/{.env,flags.yml,docker-compose.yaml}` to pull this version and overwrite both files.\n\nIf you use custom connectors, this upgrade requires all of your connector specs to be retrievable from the node running Airbyte, or Airbyte will fail on startup. If the specs are not retrievable, you need to fix this before proceeding. Alternatively, you could delete the custom connector definitions from Airbyte upon upgrade by setting the `VERSION_0_32_0_FORCE_UPGRADE` environment variable to true. This will cause the server to delete any connectors for which specs cannot be retrieved, as well as any connections built on top of them.\nUpgrading on Docker\n:::note\nAirbyte version 0.41.0 or later requires Docker Compose V2 to be installed before upgrading.\n:::\n\nIn a terminal, on the host where Airbyte is running, turn off Airbyte.\n\n`bash\n   docker compose down`\n\nUpgrade the docker instance to new version.\n\ni. If you are running Airbyte from a cloned version of the Airbyte GitHub repo and want to use the current most recent stable version, just `git pull`.\nii. If you are running Airbyte from downloaded `docker-compose.yaml` and `.env` files without a GitHub repo, run `wget -N https://raw.githubusercontent.com/airbytehq/airbyte-platform/main/{.env,flags.yml,docker-compose.yaml}` to pull the latest versions and overwrite both files.\n\nBring Airbyte back online.\n\n`bash\n   docker compose up`\nResetting your Configuration\nIf you did not start Airbyte from the root of the Airbyte monorepo, you may run into issues where existing orphaned Airbyte configurations will prevent you from upgrading with the automatic process. To fix this, we will need to globally remove these lost Airbyte configurations. You can do this with `docker volume rm $(docker volume ls -q | grep airbyte)`.\n:::danger\nThis will completely reset your Airbyte deployment back to scratch and you will lose all data.\n:::\nUpgrading on K8s (0.27.0-alpha and above)\nIf you are upgrading from (i.e. your current version of Airbyte is) Airbyte version 0.27.0-alpha or above on Kubernetes :\n\nIn a terminal, on the host where Airbyte is running, turn off Airbyte.\n\n`bash\n   kubectl delete deployments airbyte-db airbyte-worker airbyte-server airbyte-temporal airbyte-webapp --namespace=<yournamespace or default>`\n\nUpgrade the kube deployment to new version.\n\ni. If you are running Airbyte from a cloned version of the Airbyte GitHub repo and want to use the current most recent stable version, just `git pull`.\n\nBring Airbyte back online.\n\n`bash\n   kubectl apply -k kube/overlays/stable`\nAfter 2-5 minutes, `kubectl get pods | grep airbyte` should show `Running` as the status for all the core Airbyte pods. This may take longer on Kubernetes clusters with slow internet connections.\nRun `kubectl port-forward svc/airbyte-webapp-svc 8000:80` to allow access to the UI/API.\nUpgrading on K8s (0.26.4-alpha and below)\nIf you are upgrading from (i.e. your current version of Airbyte is) Airbyte version before 0.27.0-alpha on Kubernetes we do not support automatic migration. Please follow the following steps to upgrade your Airbyte Kubernetes deployment.\n\nSwitching over to your browser, navigate to the Admin page in the UI. Then go to the Configuration Tab. Click Export. This will download a compressed back-up archive (gzipped tarball) of all of your Airbyte configuration data and sync history locally.\n\nNote: Any secrets that you have entered into Airbyte will be in this archive, so you should treat it as a secret.\n\nBack to the terminal, migrate the local archive to the new version using the Migration App (packaged in a docker container).\n\n`bash\n   docker run --rm -v <path to directory containing downloaded airbyte_archive.tar.gz>:/config airbyte/migration:<version you are upgrading to> --\\\n     --input /config/airbyte_archive.tar.gz\\\n     --output <path to where migrated archive will be written (should end in .tar.gz)>\\\n     [ --target-version <version you are migrating to or empty for latest> ]`\nHere's an example of what it might look like with the values filled in. It assumes that the downloaded `airbyte_archive.tar.gz` is in `/tmp`.\n`bash\n   docker run --rm -v /tmp:/config airbyte/migration:0.41.0 --\\\n   --input /config/airbyte_archive.tar.gz\\\n   --output /config/airbyte_archive_migrated.tar.gz`\n\nTurn off Airbyte fully and (see warning) delete the existing Airbyte Kubernetes volumes.\n\nWARNING: Make sure you have already exported your data (step 1). This command is going to delete your data in Kubernetes, you may lose your airbyte configurations!\nThis is where all airbyte configurations are saved. Those configuration files need to be upgraded and restored with the proper version in the following steps.\n`bash\n   # Careful, this is deleting data!\n   kubectl delete -k kube/overlays/stable`\n\nFollow Step 2 in the `Upgrading on Docker` section to check out the most recent version of Airbyte. Although it is possible to migrate by changing the `.env` file in the kube overlay directory, this is not recommended as it does not capture any changes to the Kubernetes manifests.\nBring Airbyte back up.\n\n`bash\n   kubectl apply -k kube/overlays/stable`\n\nSwitching over to your browser, navigate to the Admin page in the UI. Then go to the Configuration Tab and click on Import. Upload your migrated archive.\n\nIf you prefer to import and export your data via API instead the UI, follow these instructions:\n\nInstead of Step 3 above use the following curl command to export the archive:\n\n`bash\n   curl -H \"Content-Type: application/json\" -X POST localhost:8000/api/v1/deployment/export --output /tmp/airbyte_archive.tar.gz`\n\nInstead of Step X above user the following curl command to import the migrated archive:\n\n`bash\n   curl -H \"Content-Type: application/x-gzip\" -X POST localhost:8000/api/v1/deployment/import --data-binary @<path to arhive>`\nHere is an example of what this request might look like assuming that the migrated archive is called `airbyte_archive_migrated.tar.gz` and is in the `/tmp` directory.\n```bash\ncurl -H \"Content-Type: application/x-gzip\" -X POST localhost:8000/api/v1/deployment/import --data-binary @/tmp/airbyte_archive_migrated.tar.gz",
    "tag": "airbyte"
  },
  {
    "title": "Scaling Airbyte",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/operator-guides/scaling-airbyte.md",
    "content": "Scaling Airbyte\nAs depicted in our High-Level View, Airbyte is made up of several components under the hood: 1. Scheduler 2. Server 3. Temporal 4. Webapp 5. Database\nThese components perform control plane operations that are low-scale, low-resource work. In addition to the work being low cost, these components are efficient and optimized for these jobs, meaning that only uncommonly large workloads will require deployments at scale. In general, you would only encounter scaling issues when running over a thousand connections.\nAs a reference point, the typical Airbyte user has 5 - 20 connectors and 10 - 100 connections configured. Almost all of these connections are scheduled, either hourly or daily, resulting in at most 100 concurrent jobs.\nWhat To Scale\nWorkers do all the heavy lifting within Airbyte. A worker is responsible for executing Airbyte operations (e.g. Discover, Read, Sync etc), and is created on demand whenever these operations are requested. Thus, every job has a corresponding worker executing its work.\nHow a worker executes work depends on the Airbyte deployment. In the Docker deployment, an Airbyte worker spins up at least one Docker container. In the Kubernetes deployment, an Airbyte worker will create at least one Kubernetes pod. The created resource (Docker container or Kubernetes pod) does all the actual work.\nThus, scaling Airbyte is a matter of ensuring that the Docker container or Kubernetes Pod running the jobs has sufficient resources to execute its work.\nJobs-wise, we are mainly concerned with Sync jobs when thinking about scale. Sync jobs sync data from sources to destinations and are the majority of jobs run. Sync jobs use two workers. One worker reads from the source; the other worker writes to the destination.\nIn general, we recommend starting out with a mid-sized cloud instance (e.g. 4 or 8 cores) and gradually tuning instance size to your workload.\nThere are two resources to be aware of when thinking of scale: 1. Memory 2. Disk space\nMemory\nAs mentioned above, we are mainly concerned with scaling Sync jobs. Within a Sync job, the main memory culprit is the Source worker.\nThis is because the Source worker reads up to 10,000 records in memory. This can present problems for database sources with tables that have large row sizes. e.g. a table with an average row size of 0.5MBs will require 0.5 * 10000 / 1000 = 5GBs of RAM. See this issue for more information.\nOur Java connectors currently follow Java's default behaviour with container memory and will only use up to 1/4 of the host's allocated memory. e.g. On a Docker agent with 8GBs of RAM configured, a Java connector limits itself to 2Gbs of RAM and will see Out-of-Memory exceptions if this goes higher. The same applies to Kubernetes pods.\nYou may want to customize this by setting `JOB_MAIN_CONTAINER_MEMORY_REQUEST` and `JOB_MAIN_CONTAINER_MEMORY_LIMIT` environment variables to custom values.\nNote that all Source database connectors are Java connectors. This means that users currently need to over-specify memory resource for Java connectors.\nDisk Space\nAirbyte uses backpressure to try to read the minimal amount of logs required. In the past, disk space was a large concern, but we've since deprecated the expensive on-disk queue approach.\nHowever, disk space might become an issue for the following reasons:\n\nLong-running syncs can produce a fair amount of logs from the Docker agent and Airbyte on Docker deployments. Some work has been done to minimize accidental logging, so this should no longer be an acute problem, but is still an open issue.\nAlthough Airbyte connector images aren't massive, they aren't exactly small either. The typical connector image is ~300MB. An Airbyte deployment with multiple connectors can easily use up to 10GBs of disk space.\n\nBecause of this, we recommend allocating a minimum of 30GBs of disk space per node. Since storage is on the cheaper side, we'd recommend you be safe than sorry, so err on the side of over-provisioning.\nOn Kubernetes\nUsers running Airbyte Kubernetes also have to make sure the Kubernetes cluster can accommodate the number of pods Airbyte creates.\nTo be safe, make sure the Kubernetes cluster can schedule up to `2 x <number-of-possible-concurrent-connections>` pods at once. This is the worse case estimate, and most users should be fine with `2 x <number-of-possible-concurrent-connections>` as a rule of thumb.\nThis is a non-issue for users running Airbyte Docker.\nTemporal DB\nTemporal maintains multiple idle connections. By the default value is `20` and you may want to lower or increase this number. One issue we noticed is\nthat temporal creates multiple pools and the number specified in the `SQL_MAX_IDLE_CONNS` environment variable of the `docker.compose.yaml` file\nmight end up allowing 4-5 times more connections than expected.\nIf you want to increase the amount of allowed idle connexion, you will also need to increase `SQL_MAX_CONNS` as well because `SQL_MAX_IDLE_CONNS`\nis capped by `SQL_MAX_CONNS`.\nFeedback\nThe advice here is best-effort and by no means comprehensive. Please reach out on Slack if anything doesn't make sense or if something can be improved.\nIf you've been running Airbyte in production and have more tips up your sleeve, we welcome contributions!\nMetrics\nAirbyte supports exporting built-in metrics to Datadog or OpenTelemetry\nKey Metrics\n| Key Metrics                     | Description                                                                                                                                      |\n|---------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------|\n| `oldest_pending_job_age_secs` | Shows how long a pending job waits before it is scheduled. If a job is in pending state for a long time, more workers may be required.           |\n| `oldest_running_job_age_secs` | Shows how long the oldest job has been running. A running job that is too large can indicate stuck jobs. This is relative to each job\u2019s runtime. |\n| `job_failed_by_release_stage` | Shows jobs that have failed in that release stage and is tagged as alpha, beta, or GA.                                                           |\n:::note \nMetrics with `by_release_stage` in their name are tagged by connector release stage (alpha, beta, or GA). These tags allow you to filter by release stage. Alpha and beta connectors are less stable and have a higher failure rate than GA connectors, so filtering by those release stages can help you find failed jobs.  \n:::\nRecommended Metrics\n| Recommended Metrics          | Description                                                                                                                           |\n|-----------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------|\n| `num_running_jobs & num_pending_jobs` | Shows how many jobs are currently running and how many jobs are in pending state. These metrics help you understand the general system state. |\n| `job_succeeded_by_release_stage`      | Shows successful jobs in that release stage and is tagged as alpha, beta, or GA.                                                      |\n| `job_created_by_release_stage`        | Shows the jobs created in that release stage and is tagged as alpha, beta, or GA.                                                     |\nExample\nIf a job was created for an Alpha source to a Beta destination and the outcome of the job is a success, the following metrics are displayed:\n`job_created_by_release_stage[\u201calpha\u201d] = 1;  \njob_created_by_release_stage[\u201cbeta\u201d] = 1;  \njob_failed_by_release_stage[\u201calpha\u201d] = 1;  \njob_succeeded_by_release_stage[\u201cbeta\u201d] = 1;`\n:::note \nEach job has a source and destination, so each metric is counted twice \u2014 once for source and once for destination.",
    "tag": "airbyte"
  },
  {
    "title": "Configuring the Airbyte Database",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/operator-guides/configuring-airbyte-db.md",
    "content": "Configuring the Airbyte Database\nAirbyte uses different objects to store internal state and metadata. This data is stored and manipulated by the various Airbyte components, but you have the ability to manage the deployment of this database in the following two ways:\n\nUsing the default Postgres database that Airbyte spins-up as part of the Docker service described in the `docker-compose.yml` file: `airbyte/db`.\nThrough a dedicated custom Postgres instance (the `airbyte/db` is in this case unused, and can therefore be removed or de-activated from the `docker-compose.yml` file). It's not a good practice to deploy mission-critical databases on Docker or Kubernetes. \nUsing a dedicated instance will provide more reliability to your Airbyte deployment. \nMoreover, using a Cloud-managed Postgres instance (such as AWS RDS our GCP Cloud SQL), you will benefit from automatic backup and fine-grained sizing. You can start with a pretty small instance, but according to your Airbyte usage, the job database might grow and require more storage if you are not truncating the job history.\n\nThe various entities are persisted in two internal databases:\n\nJob database\nData about executions of Airbyte Jobs and various runtime metadata.\nData about the internal orchestrator used by Airbyte, Temporal.io (Tasks, Workflow data, Events, and visibility data).\nConfig database\nConnectors, Sync Connections and various Airbyte configuration objects.\n\nNote that no actual data from the source (or destination) connectors ever transits or is retained in this internal database.\nIf you need to interact with it, for example, to make back-ups or perform some clean-up maintenances, you can also gain access to the Export and Import functionalities of this database via the API or the UI (in the Admin page, in the Configuration Tab).\nConnecting to an External Postgres database\nLet's walk through what is required to use a Postgres instance that is not managed by Airbyte. First, for the sake of the tutorial, we will run a new instance of Postgres in its own docker container with the command below. If you already have Postgres running elsewhere, you can skip this step and use the credentials for that in future steps.\n`bash\ndocker run --rm --name airbyte-postgres -e POSTGRES_PASSWORD=password -p 3000:5432 -d postgres`\nIn order to configure Airbyte services with this new database, we need to edit the following environment variables declared in the `.env` file (used by the docker-compose command afterward):\n`bash\nDATABASE_USER=postgres\nDATABASE_PASSWORD=password\nDATABASE_HOST=host.docker.internal # refers to localhost of host\nDATABASE_PORT=3000\nDATABASE_DB=postgres`\nBy default, the Config Database and the Job Database use the same database instance based on the above setting. It is possible, however, to separate the former from the latter by specifying a separate parameters. For example:\n`bash\nCONFIG_DATABASE_USER=airbyte_config_db_user\nCONFIG_DATABASE_PASSWORD=password`\nAdditionally, you must redefine the JDBC URL constructed in the environment variable `DATABASE_URL` to include the correct host, port, and database. If you need to provide extra arguments to the JDBC driver (for example, to handle SSL) you should add it here as well:\n`bash\nDATABASE_URL=jdbc:postgresql://host.docker.internal:3000/postgres?ssl=true&sslmode=require`\nSame for the config database if it is separate from the job database:\n`bash\nCONFIG_DATABASE_URL=jdbc:postgresql://<host>:<port>/<database>?<extra-parameters>`\nInitializing the database\n:::info\nThis step is only required when you setup Airbyte with a custom database for the first time.\n:::\nIf you provide an empty database to Airbyte and start Airbyte up for the first time, the server will automatically create the relevant tables in your database, and copy the data. Please make sure:\n\nThe database exists in the server.\nThe user has both read and write permissions to the database.\nThe database is empty.\nIf the database is not empty, and has a table that shares the same name as one of the Airbyte tables, the server will assume that the database has been initialized, and will not copy the data over, resulting in server failure. If you run into this issue, just wipe out the database, and launch the server again.\n\nAccessing the default database located in docker airbyte-db\nIn extraordinary circumstances while using the default `airbyte-db` Postgres database, if a developer wants to access the data that tracks jobs, they can do so with the following instructions.\nAs we've seen previously, the credentials for the database are specified in the `.env` file that is used to run Airbyte. By default, the values are:\n`text\nDATABASE_USER=docker\nDATABASE_PASSWORD=docker\nDATABASE_DB=airbyte`\nIf you have overridden these defaults, you will need to substitute them in the instructions below.\nThe following command will allow you to access the database instance using `psql`.\n`text\ndocker exec -ti airbyte-db psql -U docker -d airbyte`\nFollowing tables are created \n1. `workspace` : Contains workspace information such as name, notification configuration, etc.\n2. `actor_definition` : Contains the source and destination connector definitions. \n3. `actor` : Contains source and destination connectors information.\n4. `actor_oauth_parameter` : Contains source and destination oauth parameters.\n5. `operation` : Contains dbt and custom normalization operations.\n6. `connection` : Contains connection configuration such as catalog details, source, destination, etc.\n7. `connection_operation` : Contains the operations configured for a given connection.\n8. `state`. Contains the last saved state for a connection.",
    "tag": "airbyte"
  },
  {
    "title": "Configuring Connector Resources",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/operator-guides/configuring-connector-resources.md",
    "content": "Configuring Connector Resources\nAs noted in Workers & Jobs, there are four different types of jobs.\nAlthough it is possible to configure resources for all four jobs, we focus on Sync jobs as it is the most frequently run job.\nThere are three different ways to configure connector resource requirements for a Sync:\n1. Instance-wide - applies to all containers in a Sync.\n2. Connector-specific - applies to all containers of that connector type in a Sync.\n3. Connection-specific - applies to all containers of that connection in a Sync.\nIn general, the narrower scope the requirement, the higher the precedence.\nIn decreasing order of precedence:\n1. Connection-specific - Highest precedence. Overrides all other configuration. We recommend using this on a case-by-case basis.\n2. Connector-specific - Second-highest precedence. Overrides instance-wide configuration. Mostly for internal Airbyte-use. We recommend staying away from this.\n3. Instance-wide - Lowest precedence. Overridden by all other configuration. Intended to be a default. We recommend setting this as a baseline.\nConfiguring Instance-Wide Requirements\nInstance-wide requirements are the simplest requirement to configure. All that is needed is to set the following env vars:\n1. `JOB_MAIN_CONTAINER_CPU_REQUEST` -  Define the job container's minimum CPU usage. Units follow either Docker or Kubernetes, depending on the deployment. Defaults to none.\n2. `JOB_MAIN_CONTAINER_CPU_LIMIT` - Define the job container's maximum CPU usage. Units follow either Docker or Kubernetes, depending on the deployment. Defaults to none.\n3. `JOB_MAIN_CONTAINER_MEMORY_REQUEST` - Define the job container's minimum RAM usage. Units follow either Docker or Kubernetes, depending on the deployment. Defaults to none.\n4. `JOB_MAIN_CONTAINER_MEMORY_LIMIT` - Define the job container's maximum RAM usage. Units follow either Docker or Kubernetes, depending on the deployment. Defaults to none.\nConfiguring Connector-Specific Requirements\n\nConnect to the database and run the following query with the image name replaced to find the connector definition id.\n`sql\nselect * from actor_definition where actor_definition.docker_repository like '%<image-name>';`\nRun the following commend with the resource requirements and the connection definition id filled in.\n`sql\nupdate actor_definition set resource_requirements = '{\"jobSpecific\": [{\"jobType\": \"sync\", \"resourceRequirements\": {\"cpu_limit\": \"0.5\", \"cpu_request\": \"0.5\", \"memory_limit\": \"500Mi\", \"memory_request\": \"500Mi\"}}]}' where id = '<id-from-step-1>';`\n\nConfiguring Connection-Specific Requirements\n\nNavigate to the connection in the Airbyte UI and extract the connection id from the url.\nThe url format is `<base_url>/workspaces/<workspace-id>/connections/<connection-id>/status`.\n      If the url is `localhost:8000/workspaces/92ad8c0e-d204-4bb4-9c9e-30fe25614eee/connections/5432b428-b04a-4562-a12b-21c7b9e8b63a/status`,\n      the connection id is `5432b428-b04a-4562-a12b-21c7b9e8b63a`.\nConnect to the database and run the following command with the connection id and resource requirements filled in.\n`sql\n// SQL command with example\nupdate connection set resource_requirements = '{\"cpu_limit\": \"0.5\", \"cpu_request\": \"0.5\", \"memory_limit\": \"500Mi\", \"memory_request\": \"500Mi\"}' where id = '<id-from-step-1>';`\n\nDebugging Connection Resources\nAirbyte logs the resource requirements as part of the job logs as containers are created. Both source and destination containers are logged.\nIf a job is running out-of-memory, simply navigate to the Job in the UI, and look for the log to confirm the right configuration is being detected.\nOn Docker, the log will look something like this:\n`Creating docker container = destination-e2e-test-write-39-0-vnqtl with resources io.airbyte.config.ResourceRequirements@1d86d7c9[cpuRequest=<null>,cpuLimit=<null>,memoryRequest=200Mi,memoryLimit=200Mi]`\nOn Kubernetes, the log will look something like this:\n```\n2022-08-12 01:22:20 INFO i.a.w.p.KubeProcessFactory(create):100 - Attempting to start pod = source-intercom-check-480195-0-abvnr for airbyte/source-intercom:0.1.24 with resources io.airbyte.config.ResourceRequirements@11cc9fb9[cpuRequest=2,cpuLimit=2,memoryRequest=200Mi,memoryLimit=200Mi]",
    "tag": "airbyte"
  },
  {
    "title": "Using the Prefect Airbyte Task",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/operator-guides/using-prefect-task.md",
    "content": "\ndescription: Start triggering Airbyte jobs with Prefect in minutes\nUsing the Prefect Airbyte Task\nAirbyte is an official integration Task in the Prefect project. The Airbyte Task allows you to trigger synchronization jobs in Prefect, and this tutorial will walk through configuring your Prefect Flow to do so.\nThe Airbyte Task documentation on Prefect project can be found here and the Prefect 2.0 community guide can be found here.\n1. Set up the tools\nFirst, make sure you have Docker installed. We'll be using the `docker-compose` command, so your install should contain `docker-compose`.\nStart Airbyte\nIf this is your first time using Airbyte, we suggest going through our Basic Tutorial. This tutorial will use the Connection set up in the basic tutorial.\nFor the purposes of this tutorial, set your Connection's sync frequency to manual. Prefect will be responsible for manually triggering the Airbyte job.\nStart Prefect\nIf you don't have a Prefect instance, we recommend following this guide to set one up.\n2. Create a Flow in Prefect to trigger your Airbyte job\nCreate a new Prefect Project\n`bash\nprefect create project \"airbyte\"`\nRetrieving the Airbyte Connection ID\nWe'll need the Airbyte Connection ID so our Prefect Flow knows which Airbyte Connection to trigger.\n\nThis ID can be seen in the URL on the connection page in the Airbyte UI. The Airbyte UI can be accessed at `localhost:8000`.\nCreating a simple Prefect DAG to run an Airbyte Sync Job\nCreate a new folder called `airbyte_prefect` and create a file `airbyte_prefect_flow.py`.\n```python\nfrom prefect import Flow\nfrom prefect.tasks.airbyte.airbyte import AirbyteConnectionTask\nairbyte_conn = AirbyteConnectionTask(\n        airbyte_server_host=\"localhost\",\n        airbyte_server_port=8000,\n        airbyte_api_version=\"v1\",\n        connection_id=\"04e128af-1092-4a83-bf33-1b8c85395d74\"\n)\nwith Flow(\"first-airbyte-task\") as flow:\n    flow.add_task(airbyte_conn) \nRegister the flow under the \"airbyte\" project\nflow.register(project_name=\"airbyte\")\n```\nThe Airbyte Prefect Task accepts the following parameters:\n\n`airbyte_server_host`: The host URL to your Airbyte instance.\n`airbyte_server_post`: The port value you have selected for your Airbyte instance.\n`airbyte_api_version`: default value is `v1`.\n`connection_id`: The ID of the Airbyte Connection to be triggered by Prefect.\n\nAfter running the file, `python3 airbyte_prefect_flow.py` this will register the Flow in Prefect Server.\n\nAccess the link from the output from the previous command to see the Flow in Prefect Server, or you can navigate in Prefect UI to find the new Flow -> Access the link from the output from the previous command to see the Flow in the Prefect Server. Alternatively, you can go to the Prefect UI to find the new Flow.\n\nClick on the button `Run` and configure your first run.\n\nAfter a few moments you should see the finished run.\n\nAfter that you have the option to configure a more complex Schedule to your Flow. See the Prefect Schedule docs.\nThat's it!\nDon't be fooled by our simple example of only one Prefect Flow. Airbyte is a powerful data integration platform supporting many sources and destinations. The Airbyte Prefect Task means Airbyte can now be easily used with the Prefect ecosystem - give it a shot!\nWe love to hear any questions or feedback on our Slack. We're still in alpha, so if you see any rough edges or want to request a connector, feel free to create an issue on our Github or thumbs up an existing issue.",
    "tag": "airbyte"
  },
  {
    "title": "Sentry Integration",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/operator-guides/sentry-integration.md",
    "content": "Sentry Integration\nAirbyte provides an opportunity to aggregate connectors' exceptions and errors via Sentry.\nBy default, this option is off. There are 2 possible mechanisms for its activation:\n1. Define the `SENTRY_DSN` environment variable into Dockerfile of necessary connectors.\n2. Define the `SENTRY_DSN` into the workspace environment file (`.env`). Workers will add this variable to all docker connectors automatically.\nMost connectors written using the Airbyte Python or Java CDKs automatically detect this environment variable and activate Sentry profiling accordingly. \nUML diagram",
    "tag": "airbyte"
  },
  {
    "title": "Browsing Output Logs",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/operator-guides/browsing-output-logs.md",
    "content": "Browsing Output Logs\nOverview\nThis tutorial will describe how to explore Airbyte Workspace folders.\nThis is useful if you need to browse the docker volumes where extra output files of Airbyte server and workers are stored since they may not be accessible through the UI.\nExploring the Logs folders\nWhen running a Sync in Airbyte, you have the option to look at the logs in the UI as shown next.\nIdentifying Workspace IDs\nIn the screenshot below, you can notice the highlighted blue boxes are showing the id numbers that were used for the selected \"Attempt\" for this sync job.\nIn this case, the job was running in `/tmp/workspace/9/2/` folder since the tab of the third attempt is being selected in the UI (first attempt would be `/tmp/workspace/9/0/`).\n\nThe highlighted button in the red circle on the right would allow you to download the logs.log file.\nHowever, there are actually more files being recorded in the same workspace folder... Thus, we might want to dive deeper to explore these folders and gain a better understanding of what is being run by Airbyte.\nUnderstanding the Docker run commands\nScrolling down a bit more, we can also read the different docker commands being used internally are starting with:\n`text\ndocker run --rm -i -v airbyte_workspace:/data -v /tmp/airbyte_local:/local -w /data/9/2 --network host ...`\nFrom there, we can observe that Airbyte is calling the `-v` option to use a docker named volume called `airbyte_workspace` that is mounted in the container at the location `/data`.\nFollowing Docker Volume documentation, we can inspect and manipulate persisted configuration data in these volumes.\nOpening a Unix shell prompt to browse the Docker volume\nFor example, we can run any docker container/image to browse the content of this named volume by mounting it similarly, let's use the busybox image.\n`text\ndocker run -it --rm --volume airbyte_workspace:/data busybox`\nThis will drop you into an `sh` shell inside the docker container to allow you to do what you want inside a BusyBox system from which we can browse the filesystem and accessing to log files:\n`text\nls /data/9/2/`\nExample Output:\n`text\ncatalog.json                  normalize                     tap_config.json\nlogs.log                      singer_rendered_catalog.json  target_config.json`\nBrowsing from the host shell\nOr, if you don't want to transfer to a shell prompt inside the docker image, you can simply run Shell commands using docker commands as a proxy like this:\n`bash\ndocker run -it --rm --volume airbyte_workspace:/data busybox ls /data/9/2`\nExample Output:\n`text\ncatalog.json                 singer_rendered_catalog.json\nlogs.log                     tap_config.json\nnormalize                    target_config.json`\nReading the content of the catalog.json file\nFor example, it is often useful to inspect the content of the catalog file. You could do so by running a `cat` command:\n`bash\ndocker run -it --rm --volume airbyte_workspace:/data busybox cat /data/9/2/catalog.json`\nExample Output:\n`text\n{\"streams\":[{\"stream\":{\"name\":\"exchange_rate\",\"json_schema\":{\"type\":\"object\",\"properties\":{\"CHF\":{\"type\":\"number\"},\"HRK\":{\"type\":\"number\"},\"date\":{\"type\":\"string\"},\"MXN\":{\"type\":\"number\"},\"ZAR\":{\"type\":\"number\"},\"INR\":{\"type\":\"number\"},\"CNY\":{\"type\":\"number\"},\"THB\":{\"type\":\"number\"},\"AUD\":{\"type\":\"number\"},\"ILS\":{\"type\":\"number\"},\"KRW\":{\"type\":\"number\"},\"JPY\":{\"type\":\"number\"},\"PLN\":{\"type\":\"number\"},\"GBP\":{\"type\":\"number\"},\"IDR\":{\"type\":\"number\"},\"HUF\":{\"type\":\"number\"},\"PHP\":{\"type\":\"number\"},\"TRY\":{\"type\":\"number\"},\"RUB\":{\"type\":\"number\"},\"HKD\":{\"type\":\"number\"},\"ISK\":{\"type\":\"number\"},\"EUR\":{\"type\":\"number\"},\"DKK\":{\"type\":\"number\"},\"CAD\":{\"type\":\"number\"},\"MYR\":{\"type\":\"number\"},\"USD\":{\"type\":\"number\"},\"BGN\":{\"type\":\"number\"},\"NOK\":{\"type\":\"number\"},\"RON\":{\"type\":\"number\"},\"SGD\":{\"type\":\"number\"},\"CZK\":{\"type\":\"number\"},\"SEK\":{\"type\":\"number\"},\"NZD\":{\"type\":\"number\"},\"BRL\":{\"type\":\"number\"}}},\"supported_sync_modes\":[\"full_refresh\"],\"default_cursor_field\":[]},\"sync_mode\":\"full_refresh\",\"cursor_field\":[]}]}`\nExtract catalog.json file from docker volume\nOr if you want to copy it out from the docker image onto your host machine:\n`bash\ndocker cp airbyte-server:/tmp/workspace/9/2/catalog.json .\ncat catalog.json`\nBrowsing on Kubernetes\nIf you are running on Kubernetes, use the following commands instead to browsing and copy the files to your local.\nTo browse, identify the pod you are interested in and exec into it. You will be presented with a terminal that will accept normal linux commands e.g ls.\n`bash\nkubectl exec -it <pod name> -n <namespace pod is in> -c main bash\ne.g.\nkubectl exec -it destination-bigquery-worker-3607-0-chlle  -n jobs  -c main bash\nroot@destination-bigquery-worker-3607-0-chlle:/config# ls\nFINISHED_UPLOADING  destination_catalog.json  destination_config.json`\nTo copy the file on to your local in order to preserve it's contents:\n`bash\nkubectl cp <namespace pods are in>/<normalisation-pod-name>:/config/destination_catalog.json ./catalog.json\ne.g.\nkubectl cp jobs/normalization-worker-3605-0-sxtox:/config/destination_catalog.json ./catalog.json\ncat ./catalog.json`\nCSV or JSON local Destinations: Check local data folder\nIf you setup a pipeline using one of the local File based destinations (CSV or JSON), Airbyte is writing the resulting files containing the data in the special `/local/` directory in the container. By default, this volume is mounted from `/tmp/airbyte_local` on the host machine. So you need to navigate to this local folder on the filesystem of the machine running the Airbyte deployment to retrieve the local data files.\n:::caution\nPlease make sure that Docker Desktop has access to `/tmp` (and `/private` on a MacOS, as /tmp has a symlink that points to /private. It will not work otherwise). You allow it with \"File sharing\" in `Settings -> Resources -> File sharing -> add the one or two above folder` and hit the \"Apply & restart\" button.\n:::\nOr, you can also run through docker commands as proxy:\n```bash\n!/usr/bin/env bash\necho \"In the container:\"\ndocker run -it --rm -v /tmp/airbyte_local:/local busybox find /local\necho \"\"\necho \"On the host:\"\nfind /tmp/airbyte_local\n```\nExample Output:\n```text\nIn the container:\n/local\n/local/data\n/local/data/exchange_rate_raw.csv\nOn the host:\n/tmp/airbyte_local\n/tmp/airbyte_local/data\n/tmp/airbyte_local/data/exchange_rate_raw.csv\n```\nNotes about running on macOS vs Linux\nNote that Docker for Mac is not a real Docker host, now it actually runs a virtual machine behind the scenes and hides it from you to make things \"simpler\".\nHere are some related links as references on accessing Docker Volumes:\n\non macOS Using Docker containers in 2019\nofficial doc Use Volume\n\nFrom these discussions, we've been using on macOS either:\n\nany docker container/image to browse the virtual filesystem by mounting the volume in order to access them, for example with busybox\nor extract files from the volume by copying them onto the host with Docker cp\n\nHowever, as a side remark on Linux, accessing to named Docker Volume can be easier since you simply need to:\n`text\ndocker volume inspect <volume_name>`\nThen look at the `Mountpoint` value, this is where the volume is actually stored in the host filesystem and you can directly retrieve files directly from that folder.",
    "tag": "airbyte"
  },
  {
    "title": "Using the Airflow Airbyte Operator",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/operator-guides/using-the-airflow-airbyte-operator.md",
    "content": "\ndescription: Start triggering Airbyte jobs with Apache Airflow in minutes\nUsing the Airflow Airbyte Operator\nAirbyte is an official community provider for the Apache Airflow project. The Airbyte operator allows you to trigger synchronization jobs in Apache Airflow, and this tutorial will walk through configuring your Airflow DAG to do so.\n:::caution\nDue to some difficulties in setting up Airflow, we recommend first trying out the deployment using the local example here, as it contains accurate configuration required to get the Airbyte operator up and running.\n:::\nThe Airbyte Provider documentation on Airflow project can be found here.\n1. Set up the tools\nFirst, make sure you have Docker installed. (We'll be using the `docker-compose` command, so your install should contain `docker-compose`.)\nStart Airbyte\nIf this is your first time using Airbyte, we suggest going through our Basic Tutorial. This tutorial will use the Connection set up in the basic tutorial.\nFor the purposes of this tutorial, set your Connection's sync frequency to manual. Airflow will be responsible for manually triggering the Airbyte job.\nStart Apache Airflow\nIf you don't have an Airflow instance, we recommend following this guide to set one up. Additionally, you will need to install the `apache-airflow-providers-airbyte` package to use Airbyte Operator on Apache Airflow. You can read more about it here\n2. Create a DAG in Apache Airflow to trigger your Airbyte job\nCreate an Airbyte connection in Apache Airflow\nOnce Airflow starts, navigate to Airflow's `Connections` page as seen below. The Airflow UI can be accessed at http://localhost:8080/.\n\nAirflow will use the Airbyte API to execute our actions. The Airbyte API uses HTTP, so we'll need to create a HTTP Connection. Airbyte is typically hosted at `localhost:8001`. Configure Airflow's HTTP connection accordingly - we've provided a screenshot example.\n\nDon't forget to click save!\nRetrieving the Airbyte Connection ID\nWe'll need the Airbyte Connection ID so our Airflow DAG knows which Airbyte Connection to trigger.\n\nThis ID can be seen in the URL on the connection page in the Airbyte UI. The Airbyte UI can be accessed at `localhost:8000`.\nCreating a simple Airflow DAG to run an Airbyte Sync Job\nPlace the following file inside the `/dags` directory. Name this file `dag_airbyte_example.py`.\n```python\nfrom airflow import DAG\nfrom airflow.utils.dates import days_ago\nfrom airflow.providers.airbyte.operators.airbyte import AirbyteTriggerSyncOperator\nwith DAG(dag_id='trigger_airbyte_job_example',\n         default_args={'owner': 'airflow'},\n         schedule_interval='@daily',\n         start_date=days_ago(1)\n    ) as dag:\n\n\n```money_to_json = AirbyteTriggerSyncOperator(\n    task_id='airbyte_money_json_example',\n    airbyte_conn_id='airbyte_conn_example',\n    connection_id='1e3b5a72-7bfd-4808-a13c-204505490110',\n    asynchronous=False,\n    timeout=3600,\n    wait_seconds=3\n)\n```\n\n\n```\nThe Airbyte Airflow Operator accepts the following parameters:\n\n`airbyte_conn_id`: Name of the Airflow HTTP Connection pointing at the Airbyte API. Tells Airflow where the Airbyte API is located.\n`connection_id`: The ID of the Airbyte Connection to be triggered by Airflow.\n`asynchronous`: Determines how the Airbyte Operator executes. When true, Airflow will monitor the Airbyte Job using an AirbyteJobSensor. Default value is `false`.\n`timeout`: Maximum time Airflow will wait for the Airbyte job to complete. Only valid when `asynchronous=False`. Default value is `3600` seconds.\n`wait_seconds`: The amount of time to wait between checks. Only valid when `asynchronous=False`. Default value is `3` seconds.\n\nThis code will produce the following simple DAG in the Airbyte UI:\n\nOur DAG will show up in the Airflow UI shortly after we place our DAG file, and be automatically triggered shortly after.\nCheck Airbyte UI's Sync History tab to see if the job started syncing!\n\nUsing the `asynchronous` parameter\nIf your Airflow instance has limited resources and/or is under load, setting the `asynchronous=True` can help. Sensors do not occupy an Airflow worker slot, so this is helps reduce Airflow load.\n```python\nfrom airflow import DAG\nfrom airflow.utils.dates import days_ago\nfrom airflow.providers.airbyte.operators.airbyte import AirbyteTriggerSyncOperator\nfrom airflow.providers.airbyte.sensors.airbyte import AirbyteJobSensor \nwith DAG(dag_id='airbyte_trigger_job_example_async',\n         default_args={'owner': 'airflow'},\n         schedule_interval='@daily',\n         start_date=days_ago(1)\n    ) as dag:\n\n\n```async_money_to_json = AirbyteTriggerSyncOperator(\n    task_id='airbyte_async_money_json_example',\n    airbyte_conn_id='airbyte_conn_example',\n    connection_id='1e3b5a72-7bfd-4808-a13c-204505490110',\n    asynchronous=True,\n)\n\nairbyte_sensor = AirbyteJobSensor(\n    task_id='airbyte_sensor_money_json_example',\n    airbyte_conn_id='airbyte_conn_example',\n    airbyte_job_id=async_money_to_json.output\n)\n\nasync_money_to_json >> airbyte_sensor\n```\n\n\n```\nThat's it!\nDon't be fooled by our simple example of only one Airflow task. Airbyte is a powerful data integration platform supporting many sources and destinations. The Airbyte Airflow Operator means Airbyte can now be easily used with the Airflow ecosystem - give it a shot!\nWe love to hear any questions or feedback on our Slack. We're still in alpha, so if you see any rough edges or want to request a connector, feel free to create an issue on our Github or thumbs up an existing issue.",
    "tag": "airbyte"
  },
  {
    "title": "Configuring Airbyte",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/operator-guides/configuring-airbyte.md",
    "content": "Configuring Airbyte\nThis section covers how to configure Airbyte, and the various configuration Airbyte accepts.\nConfiguration is currently via environment variables. See the below section on how to modify these variables.\nDocker Deployments\nThe recommended way to run an Airbyte Docker deployment is via the Airbyte repo's `docker-compose.yaml` and `.env` file.\nTo configure the default Airbyte Docker deployment, modify the bundled `.env` file. The `docker-compose.yaml` file injects appropriate variables into\nthe containers.\nIf you want to manage your own docker files, please refer to Airbyte's docker file to ensure applications get the correct variables.\nKubernetes Deployments\nThe recommended way to run an Airbyte Kubernetes deployment is via the `Kustomize` overlays.\nWe recommend using the overlays in the `stable` directory as these have preset resource limits.\nTo configure the default Airbyte Kubernetes deployment, modify the `.env` in the respective directory. Each application will consume the appropriate\nenv var from a generated configmap.\nIf you want to manage your own Kube manifests, please refer to the various `Kustomize` overlays for examples.\nReference\nThe following are the possible configuration options organised by deployment type and services.\nInternal-only variables have been omitted for clarity. See `Configs.java` for a full list.\nBe careful using variables marked as `alpha` as they aren't meant for public consumption.\nShared\nThe following variables are relevant to both Docker and Kubernetes.\nCore\n\n`AIRBYTE_VERSION` - Defines the Airbyte deployment version.\n`SPEC_CACHE_BUCKET` - Defines the bucket for caching specs. This immensely speeds up spec operations. This is updated when new versions are published.\n`WORKER_ENVIRONMENT` - Defines if the deployment is Docker or Kubernetes. Airbyte behaves accordingly.\n`CONFIG_ROOT` - Defines the configs directory. Applies only to Docker, and is present in Kubernetes for backward compatibility.\n`WORKSPACE_ROOT` - Defines the Airbyte workspace directory. Applies only to Docker, and is present in Kubernetes for backward compatibility.\n\nAccess\nSet to empty values, e.g. \"\" to disable basic auth. Be sure to change these values.\n\nBASIC_AUTH_USERNAME=airbyte\nBASIC_AUTH_PASSWORD=password\nBASIC_AUTH_PROXY_TIMEOUT=600 - Defines the proxy timeout time for requests to Airbyte Server. Main use should be for dynamic discover when creating a connection (S3, JDBC, etc) that takes a long time.\n\nSecrets\n\n`SECRET_STORE_GCP_PROJECT_ID` - Defines the GCP Project to store secrets in. Alpha support.\n`SECRET_STORE_GCP_CREDENTIALS` - Define the JSON credentials used to read/write Airbyte Configuration to Google Secret Manager. These credentials must have Secret Manager Read/Write access. Alpha support.\n`VAULT_ADDRESS` - Define the vault address to read/write Airbyte Configuration to Hashicorp Vault. Alpha Support.\n`VAULT_PREFIX` - Define the vault path prefix. Empty by default. Alpha Support.\n`VAULT_AUTH_TOKEN` - The token used for vault authentication. Alpha Support.\n`VAULT_AUTH_METHOD` - How vault will preform authentication. Currently, only supports Token auth. Defaults to token. Alpha Support.\n`SECRET_PERSISTENCE` - Defines the Secret Persistence type. Defaults to NONE. Set to GOOGLE_SECRET_MANAGER to use Google Secret Manager. Set to TESTING_CONFIG_DB_TABLE to use the database as a test. Set to VAULT to use Hashicorp Vault, currently only the token based authentication is supported. Alpha support. Undefined behavior will result if this is turned on and then off.\n`AWS_ACCESS_KEY` - Defines the aws_access_key_id from the AWS credentials to use for AWS Secret Manager.\n`AWS_SECRET_ACCESS_KEY`- Defines aws_secret_access_key to use for the AWS Secret Manager.\n\nDatabase\n\n`DATABASE_USER` - Define the Jobs Database user.\n`DATABASE_PASSWORD` - Define the Jobs Database password.\n`DATABASE_URL` - Define the Jobs Database url in the form of `jdbc:postgresql://${DATABASE_HOST}:${DATABASE_PORT/${DATABASE_DB}`. Do not include username or password.\n`JOBS_DATABASE_INITIALIZATION_TIMEOUT_MS` - Define the total time to wait for the Jobs Database to be initialized. This includes migrations.\n`CONFIG_DATABASE_USER` - Define the Configs Database user. Defaults to the Jobs Database user if empty.\n`CONFIG_DATABASE_PASSWORD` - Define the Configs Database password. Defaults to the Jobs Database password if empty.\n`CONFIG_DATABASE_URL` - Define the Configs Database url in the form of `jdbc:postgresql://${DATABASE_HOST}:${DATABASE_PORT/${DATABASE_DB}`. Defaults to the Jobs Database url if empty.\n`CONFIG_DATABASE_INITIALIZATION_TIMEOUT_MS` - Define the total time to wait for the Configs Database to be initialized. This includes migrations.\n`RUN_DATABASE_MIGRATION_ON_STARTUP` - Define if the Bootloader should run migrations on start up.\n\nAirbyte Services\n\n`TEMPORAL_HOST` - Define the url where Temporal is hosted at. Please include the port. Airbyte services use this information.\n`INTERNAL_API_HOST` - Define the url where the Airbyte Server is hosted at. Please include the port. Airbyte services use this information.\n`WEBAPP_URL` - Define the url the Airbyte Webapp is hosted at. Please include the port. Airbyte services use this information.\n\nJobs\n\n`SYNC_JOB_MAX_ATTEMPTS` - Define the number of attempts a sync will attempt before failing.\n`SYNC_JOB_MAX_TIMEOUT_DAYS` - Define the number of days a sync job will execute for before timing out.\n`JOB_MAIN_CONTAINER_CPU_REQUEST` - Define the job container's minimum CPU usage. Units follow either Docker or Kubernetes, depending on the deployment. Defaults to none.\n`JOB_MAIN_CONTAINER_CPU_LIMIT` - Define the job container's maximum CPU usage. Units follow either Docker or Kubernetes, depending on the deployment. Defaults to none.\n`JOB_MAIN_CONTAINER_MEMORY_REQUEST` - Define the job container's minimum RAM usage. Units follow either Docker or Kubernetes, depending on the deployment. Defaults to none.\n`JOB_MAIN_CONTAINER_MEMORY_LIMIT` - Define the job container's maximum RAM usage. Units follow either Docker or Kubernetes, depending on the deployment. Defaults to none.\n\nLogging\n\n`LOG_LEVEL` - Define log levels. Defaults to INFO. This value is expected to be one of the various Log4J log levels.\n\nMonitoring\n\n`PUBLISH_METRICS` - Define whether to publish metrics collected by the Metrics Reporter. Defaults to false.\n`METRIC_CLIENT` - Defines which metrics client to use. Only relevant if `PUBLISH_METRICS` is set to true. Accepts either `datadog` or `otel`. Default to none.\n`DD_AGENT_HOST` - Defines the ip the Datadog metric client sends metrics to. Only relevant if `METRIC_CLIENT` is set to `datadog`. Defaults to none.\n`DD_AGENT_PORT` - Defines the port the Datadog metric client sends metrics to. Only relevant if `METRIC_CLIENT` is set to `datadog`. Defaults to none.\n`OTEL_COLLECTOR_ENDPOIN` - Define the ip:port the OTEL metric client sends metrics to. Only relevant if `METRIC_CLIENT` is set to `otel`. Defaults to none.\n\nWorker\n\n`MAX_SPEC_WORKERS` - Define the maximum number of Spec workers each Airbyte Worker container can support. Defaults to 5.\n`MAX_CHECK_WORKERS` - Define the maximum number of Check workers each Airbyte Worker container can support. Defaults to 5.\n`MAX_SYNC_WORKERS` - Define the maximum number of Sync workers each Airbyte Worker container can support. Defaults to 5.\n`MAX_DISCOVER_WORKERS` - Define the maximum number of Discover workers each Airbyte Worker container can support. Defaults to 5.\n`SENTRY_DSN` - Define the DSN of necessary Sentry instance. Defaults to empty. Integration with Sentry is explained here\n\nDocker-Only\n\n`WORKSPACE_DOCKER_MOUNT` - Defines the name of the Airbyte docker volume.\n`DOCKER_NETWORK` - Defines the docker network the new Scheduler launches jobs on.\n`LOCAL_DOCKER_MOUNT` - Defines the name of the docker mount that is used for local file handling. On Docker, this allows connector pods to interact with a volume for \"local file\" operations.\n\nKubernetes-Only\nJobs\n\n`JOB_KUBE_TOLERATIONS` - Define one or more Job pod tolerations. Tolerations are separated by ';'. Each toleration contains k=v pairs mentioning some/all of key, effect, operator and value and separated by `,`.\n`JOB_KUBE_NODE_SELECTORS` - Define one or more Job pod node selectors. Each k=v pair is separated by a `,`. For example: `key1=value1,key2=value2`. It is the pod node selectors of the sync job and the default pod node selectors fallback for others jobs.\n`JOB_KUBE_ANNOTATIONS` - Define one or more Job pod annotations. Each k=v pair is separated by a `,`. For example: `key1=value1,key2=value2`. It is the pod annotations of the sync job and the default pod annotations fallback for others jobs.\n`JOB_KUBE_MAIN_CONTAINER_IMAGE_PULL_POLICY` - Define the Job pod connector image pull policy.\n`JOB_KUBE_MAIN_CONTAINER_IMAGE_PULL_SECRET` - Define the Job pod connector image pull secret. Useful when hosting private images.\n`JOB_KUBE_SIDECAR_CONTAINER_IMAGE_PULL_POLICY` - Define the image pull policy on the sidecar containers in the Job pod. Useful when there are cluster policies enforcing to always pull.\n`JOB_KUBE_SOCAT_IMAGE` - Define the Job pod socat image.\n`JOB_KUBE_BUSYBOX_IMAGE` - Define the Job pod busybox image.\n`JOB_KUBE_CURL_IMAGE` - Define the Job pod curl image pull.\n`JOB_KUBE_NAMESPACE` - Define the Kubernetes namespace Job pods are created in.\n\nJobs specific\nA job specific variable overwrites the default sync job variable defined above.\n\n`SPEC_JOB_KUBE_NODE_SELECTORS` - Define one or more pod node selectors for the spec job. Each k=v pair is separated by a `,`. For example: `key1=value1,key2=value2`\n`CHECK_JOB_KUBE_NODE_SELECTORS` - Define one or more pod node selectors for the check job. Each k=v pair is separated by a `,`. For example: `key1=value1,key2=value2`\n`DISCOVER_JOB_KUBE_NODE_SELECTORS` - Define one or more pod node selectors for the discover job. Each k=v pair is separated by a `,`. For example: `key1=value1,key2=value2`\n`SPEC_JOB_KUBE_ANNOTATIONS` - Define one or more pod annotations for the spec job. Each k=v pair is separated by a `,`. For example: `key1=value1,key2=value2`\n`CHECK_JOB_KUBE_ANNOTATIONS` - Define one or more pod annotations for the check job. Each k=v pair is separated by a `,`. For example: `key1=value1,key2=value2`\n`DISCOVER_JOB_KUBE_ANNOTATIONS` - Define one or more pod annotations for the discover job. Each k=v pair is separated by a `,`. For example: `key1=value1,key2=value2`\n\nWorker\n\n`TEMPORAL_WORKER_PORTS` - Define the local ports the Airbyte Worker pod uses to connect to the various Job pods. Port 9001 - 9040 are exposed by default in the Kustomize deployments.\n\nLogging\nNote that Airbyte does not support logging to separate Cloud Storage providers.\nPlease see here for more information on configuring Kubernetes logging.\n\n`GCS_LOG_BUCKET` - Define the GCS bucket to store logs.\n`S3_BUCKET` - Define the S3 bucket to store logs.\n`S3_RREGION` - Define the S3 region the S3 log bucket is in.\n`S3_AWS_KEY` - Define the key used to access the S3 log bucket.\n`S3_AWS_SECRET` - Define the secret used to access the S3 log bucket.\n`S3_MINIO_ENDPOINT` - Define the url Minio is hosted at so Airbyte can use Minio to store logs.\n",
    "tag": "airbyte"
  },
  {
    "title": "Using the Dagster Integration ",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/operator-guides/using-dagster-integration.md",
    "content": "\ndescription: Start triggering Airbyte jobs with Dagster in minutes\nUsing the Dagster Integration\nAirbyte is an official integration in the Dagster project. The Airbyte Integration allows you to trigger synchronization jobs in Airbyte, and this tutorial will walk through configuring your Dagster Ops to do so.\nThe Airbyte Task documentation on the Dagster project can be found here. We also have a tutorial on dynamically configuring Airbyte using dagster-airbyte.\n1. Set up the tools\nFirst, make sure you have Docker installed. We'll be using the `docker-compose` command, so your install should contain `docker-compose`.\nStart Airbyte\nIf this is your first time using Airbyte, we suggest going through our Basic Tutorial. This tutorial will use the Connection set up in the basic tutorial.\nFor the purposes of this tutorial, set your Connection's sync frequency to manual. Dagster will be responsible for manually triggering the Airbyte job.\nInstall Dagster\nIf you don't have a Dagster installed, we recommend following this guide to set one up.\n2. Create the Dagster Op to trigger your Airbyte job\nCreating a simple Dagster DAG to run an Airbyte Sync Job\nCreate a new folder called `airbyte_dagster` and create a file `airbyte_dagster.py`.\n```python\nfrom dagster import job\nfrom dagster_airbyte import airbyte_resource, airbyte_sync_op\nmy_airbyte_resource = airbyte_resource.configured(\n    {\n        \"host\": {\"env\": \"AIRBYTE_HOST\"},\n        \"port\": {\"env\": \"AIRBYTE_PORT\"},\n    }\n)\nsync_foobar = airbyte_sync_op.configured({\"connection_id\": \"your-connection-uuid\"}, name=\"sync_foobar\")\n@job(resource_defs={\"airbyte\": my_airbyte_resource})\ndef my_simple_airbyte_job():\n    sync_foobar()\n```\nThe Airbyte Dagster Resource accepts the following parameters:\n\n`host`: The host URL to your Airbyte instance.\n`port`: The port value you have selected for your Airbyte instance.\n`use_https`: If your server use secure HTTP connection.\n`request_max_retries`: The maximum number of times requests to the Airbyte API should be retried before failing.\n`request_retry_delay`: Time in seconds to wait between each request retry.\n\nThe Airbyte Dagster Op accepts the following parameters:\n* `connection_id`: The Connection UUID you want to trigger\n* `poll_interval`: The time in seconds that will be waited between successive polls.\n* `poll_timeout`: he maximum time that will waited before this operation is timed out.\nAfter running the file, `dagster job execute -f airbyte_dagster.py` this will trigger the job with Dagster.\nThat's it!\nDon't be fooled by our simple example of only one Dagster Flow. Airbyte is a powerful data integration platform supporting many sources and destinations. The Airbyte Dagster Integration means Airbyte can now be easily used with the Dagster ecosystem - give it a shot!\nWe love to hear any questions or feedback on our Slack. We're still in alpha, so if you see any rough edges or want to request a connector, feel free to create an issue on our Github or thumbs up an existing issue.",
    "tag": "airbyte"
  },
  {
    "title": "Resetting Your Data",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/operator-guides/reset.md",
    "content": "Resetting Your Data\nThe reset button gives you a blank slate, of sorts, to perform a fresh new sync. This can be useful if you are just testing Airbyte or don't necessarily require the data replicated to your destination to be saved permanently.\n\nAs outlined above, you can click on the `Reset your data` button to give you that clean slate. Just as a heads up, here is what it does and doesn't do:\nThe reset button DOES:\n\nDelete all records in your destination tables\nDelete all records in your destination file\n\nThe reset button DOES NOT:\n\nDelete the destination tables\nDelete a destination file if using the LocalCSV or LocalJSON Destinations\n\nBecause of this, if you have any orphaned tables or files that are no longer being synced to, they will have to be cleaned up later, as Airbyte will not clean them up for you.",
    "tag": "airbyte"
  },
  {
    "title": "Airbyte Security",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/operator-guides/security.md",
    "content": "Airbyte Security\nAirbyte is committed to keeping your data safe by following industry-standard practices for securing physical deployments, setting access policies, and leveraging the security features of leading Cloud providers.\nIf you have any security concerns with Airbyte or believe you have uncovered a vulnerability, contact us at security@airbyte.io\nSecuring your data\nAirbyte connectors operate as the data pipes moving data from Point A to point B: Extracting data from data sources (APIs, files, databases) and loading it into destination platforms (warehouses, data lakes) with optional transformation performed at the data destination. As soon as data is transferred from the source to the destination, it is purged from an Airbyte deployment.\nAn Airbyte deployment stores the following data:\nTechnical Logs\nTechnical logs are stored for troubleshooting purposes and may contain sensitive data based on the connection\u2019s `state` data. If your connection is set to an Incremental sync mode, users choose which column is the cursor for their connection. While we strongly recommend a timestamp like an `updated_at` column, users can choose any column they want to be the cursor.\nConfiguration Metadata\nAirbyte retains configuration details and data points such as table and column names for each integration.\nSensitive Data\u200b\nAs Airbyte is not aware of the data being transferred, users are required to follow the Terms of Services and are ultimately responsible for ensuring their data transfer is compliant with their jurisdiction.\nFor more information, see Airbyte\u2019s Privacy Policy\nSecuring Airbyte Open Source\n:::note\nOur security and reliability commitments are only applicable to Airbyte Cloud. Airbyte Open Source security and reliability depend on your development and production setups.\n:::\nNetwork Security\nDeploy Airbyte Open Source in a private network or use a firewall to filter which IP addresses are allowed to access your host. Airbyte Open Source currently does not include any user management or role-based access controls (RBAC) to prevent unauthorized access to the API or UI. Controlling who has access to the hardware and network your Airbyte deployment runs on is your responsibility.\nYou can secure access to Airbyte using the following methods:\n\nDeploy Airbyte in a private network or use a firewall to filter which IP is allowed to access your host.\n\nDeploy Airbyte behind a reverse proxy and handle the access control and SSL encryption on the reverse proxy side.\n  ```\n  # Example nginx reverse proxy config\n  server {\n    listen 443 ssl;\n    server_name airbyte..com;\n    client_max_body_size 200M;  # required for Airbyte API\n    ssl_certificate .crt.pem; \n    ssl_certificate_key .key.pem;\nlocation / {\n  proxy_pass http://127.0.0.1:8000;\n  proxy_set_header Cookie $http_ccokie;  # if you use Airbytes basic auth\n  proxy_read_timeout 3600;  # set a number in seconds suitable for you\n}\n  }\n  `- Change the default username and password in your environment's `.env` file:`\nProxy Configuration\nSet to empty values, e.g. \"\" to disable basic auth\nBASIC_AUTH_USERNAME=your_new_username_here\nBASIC_AUTH_PASSWORD=your_new_password_here\n  ```\n- If you deployed Airbyte on a cloud provider:\n  - GCP: use the Identity-Aware proxy service\n  - AWS: use the AWS Systems Manager Session Manager service\n\n\nCredential management\nTo allow you to configure syncs and connectors, Airbyte stores the credentials (like API Keys and passwords) you provide in the Airbyte application database. Make sure you protect the configuration management routes.\nIf you\u2019re deploying Airbyte Open Source on GCP, you may use Google Secret Manager to store credentials instead of in the database:\n\nCreate a service account with Google Secret Manager with read/write access. Generate a JSON key.\nIn the Worker and Server applications, set the `SECRET_STORE_GCP_PROJECT_ID` environment variable to the GCP project to which the credentials have access and secrets will be located.\nIn the Worker and Server applications, set the `SECRET_STORE_GCP_CREDENTIALS` environment variable to the JSON key created in step 1.\nIn the Worker and Server applications, set the `SECRET_PERSISTENCE` environment variable to `GOOGLE_SECRET_MANAGER`.\n\nNote that this process is not reversible. Once you have converted to a secret store, you won\u2019t be able to reverse it.\nEncryption\nMost Airbyte Open Source connectors support encryption-in-transit (SSL or HTTPS). We recommend configuring your connectors to use the encryption option whenever available.\nTelemetry\nAirbyte does send anonymized data to our services to improve the product (especially connector reliability and scale). To disable telemetry, modify the .env file and define the following environment variable:\n`TRACKING_STRATEGY=logging`\nSecuring Airbyte Cloud\nAirbyte Cloud leverages the security features of leading Cloud providers and sets least-privilege access policies to ensure data security.\nPhysical infrastructure\nAirbyte Cloud is currently deployed on GCP with all servers located in the United States. We use isolated pods to ensure your data is kept separate from other customers\u2019 data.\nOnly certain Airbyte staff can access Airbyte infrastructure and technical logs for deployments, upgrades, configuration changes, and troubleshooting.\nNetwork security\nDepending on your data residency location, you may need to allowlist the following IP addresses to enable access to Airbyte:\nUnited States and Airbyte Default\nGCP region: us-west3\n* 34.106.109.131\n* 34.106.196.165\n* 34.106.60.246\n* 34.106.229.69\n* 34.106.127.139\n* 34.106.218.58\n* 34.106.115.240\n* 34.106.225.141\nEuropean Union\n:::note \nSome workflows still run in the US, even when the data residency is in the EU. If you use the EU as a data residency, you must allowlist the following IP addresses from both GCP us-west3 and AWS eu-west-3.\n:::\nGCP region: us-west3\n* 34.106.109.131\n* 34.106.196.165\n* 34.106.60.246\n* 34.106.229.69\n* 34.106.127.139\n* 34.106.218.58\n* 34.106.115.240\n* 34.106.225.141\nAWS region: eu-west-3\n* 13.37.4.46\n* 13.37.142.60\n* 35.181.124.238\nCredential management\nMost Airbyte Cloud connectors require keys, secrets, or passwords to allow the connectors to continually sync without prompting credentials on every refresh. Airbyte Cloud fetches credentials using HTTPS and stores them in Google Cloud\u2019s Secret Manager. When persisting connector configurations to disk or the database, we store a version of the configuration that points to the secret in Google Secret Manager instead of the secret itself to limit the parts of the system interacting with secrets.\nEncryption\nSince Airbyte Cloud only transfers data from source to destination and purges the data after the transfer is finished, data in transit is encrypted with TLS, and no in-store encryption is required for the data. Airbyte Cloud does store customer metadata and encrypts it using GCP\u2019s encryption service with AES-256-bit encryption keys.\nAll Airbyte Cloud connectors (APIs, files, databases) pull data through encrypted channels (SSL, SSH tunnel, HTTPS), and the data transfer between our clients' infrastructure and Airbyte infrastructure is fully encrypted.\nAuthentication\nAirbyte Cloud allows you to log in to the platform using your email and password, Google account, or GitHub account.\nAccess Control\nAirbyte Cloud supports user management but doesn\u2019t support role-based access control (RBAC) yet.\nCompliance\nOur compliance efforts for Airbyte Cloud include:\n\nSOC 2 Type II assessment: An independent third-party completed a SOC2 Type II assessment and found effective operational controls in place. Independent third-party audits will continue at a regular cadence, and the most recent report is available upon request.\nISO 27001 certification: We received our ISO 27001 certification in November 2022. A copy of the certificate is available upon request. \nAssessments and penetration tests: We use tools provided by the Cloud platforms as well as third-party assessments and penetration tests.\n\nReporting Vulnerabilities\u200b\n:::warning\nDo not file GitHub issues or post on our community Slack or forum for security vulnerabilities since they're public avenues and may lead to additional security concerns.\n:::\nAirbyte takes security issues very seriously. If you have any concerns about Airbyte or believe you have uncovered a vulnerability, contact us at security@airbyte.io. In the message, try to describe the issue and a way to reproduce it. The security team will get back to you as soon as possible.\nUse this security address only for undisclosed vulnerabilities. For fixed issues or general questions on how to use the security features, use the Discourse forum or Community Slack.",
    "tag": "airbyte"
  },
  {
    "title": "Windows - Browsing Local File Output",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/operator-guides/locating-files-local-destination.md",
    "content": "Windows - Browsing Local File Output\nOverview\nThis tutorial will describe how to look for json and csv files in when using local destinations on Windows on a local deployment.\nThere can be confusion when using local destinations in Airbyte on Windows, especially if you are running WSL2 to power Docker. There are also two folders generated at the root folder of your Docker folder which will point you in the wrong direction.\nLocating where your temp folder is\nWhile running Airbyte's Docker image on Windows with WSL2, you can access your temp folder by doing the following:\n\nOpen File Explorer (Or any folder where you can access the address bar)\nType in `\\\\wsl$` in the address bar\nThe folders below will be displayed\n\n\n\nYou can start digging here, but it is recommended to start searching from here and just search for the folder name you used for your local files. The folder address should be similar to `\\\\wsl$\\docker-desktop\\tmp\\docker-desktop-root\\containers\\services\\docker\\rootfs\\tmp\\airbyte_local`\nYou should be able to locate your local destination CSV or JSON files in this folder.\n\nNote that there are scenarios where you may not be able to browse to the actual files in which case, use the below method to take a local copy.\nUse Docker to Copy your temp folder files\nNote that this method does not allow direct access to any files directly, instead it creates local, readable copies.\n\nOpen and standard CMD shell\nType the following (where `<local path>` is the path on your Windows host machine to place copies)\n   `docker cp airbyte-server:/tmp/airbyte_local <local path>`\nThis will copy the entire `airbyte_local` folder to your host machine.\n\nNote that if you know the specific filename or wildcard, you can add append it to the source path of the `docker cp` command.\nNotes\n\nLocal JSON and Local CSV files do not persist between Docker restarts. This means that once you turn off your Docker image, your data is lost. This is consistent with the `tmp` nature of the folder.\nIn the root folder of your docker files, it might generate tmp and var folders that only have empty folders inside.\n",
    "tag": "airbyte"
  },
  {
    "title": "Configuring Sync Notifications",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/operator-guides/configuring-sync-notifications.md",
    "content": "Configuring Sync Notifications\nOverview\nYou can set up Airbyte to notify you when syncs have failed or succeeded. This is achieved through a webhook, a URL that you can input into other applications to get real time data from Airbyte.\nSet up Slack Notifications on Sync Status\nIf you're more of a visual learner, just head over to this video to learn how to do this. Otherwise, keep reading!\nSet up the bot.\nNavigate to https://api.slack.com/apps/. Hit `Create an App`. \n \nThen click `From scratch`. Enter your App Name (e.g. Airbyte Sync Notifications) and pick your desired Slack workspace. \nSet up the webhook URL.\nNow on the left sidebar, click on `Incoming Webhooks`. \n\nClick the slider button in the top right to turn the feature on. Then click `Add New Webhook to Workspace`.\n\nPick the channel that you want to receive Airbyte notifications in (ideally a dedicated one), and click `Allow` to give it permissions to access the channel. You should see the bot show up in the selected channel now.\nNow you should see an active webhook right above the `Add New Webhook to Workspace` button.\n \nClick `Copy.`\nAdd the webhook to Airbyte.\nAssuming you have a running instance of Airbyte, we can navigate to the UI. Click on Settings and then click on `Notifications`.\n\nSimply paste the copied webhook URL in `Connection status Webhook URL` and you're ready to go! On this page, you can click one or both of the sliders to decide whether you want notifications on sync successes, failures, or both. Make sure to click `Save changes` before you leave.\nYour Webhook URL should look something like this:\n\nTest it out.\nFrom the settings page, you can click `Test` to send a test message to the channel. Or, just run a sync now and try it out! If all goes well, you should receive a notification in your selected channel that looks like this:\n",
    "tag": "airbyte"
  },
  {
    "title": "Transformations with dbt (Part 2/3)",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/operator-guides/transformation-and-normalization/transformations-with-dbt.md",
    "content": "Transformations with dbt (Part 2/3)\nOverview\nThis tutorial will describe how to integrate SQL based transformations with Airbyte syncs using specialized transformation tool: dbt.\nThis tutorial is the second part of the previous tutorial Transformations with SQL. Next, we'll wrap-up with a third part on submitting transformations back in Airbyte: Transformations with Airbyte.\n(Example outputs are updated with Airbyte version 0.23.0-alpha from May 2021)\nTransformations with dbt\nThe tool in charge of transformation behind the scenes is actually called dbt (Data Build Tool).\nBefore generating the SQL files as we've seen in the previous tutorial, Airbyte sets up a dbt Docker instance and automatically generates a dbt project for us. This is created as specified in the dbt project documentation page with the right credentials for the target destination. The dbt models are then run afterward, thanks to the dbt CLI. However, for now, let's run through working with the dbt tool.\nValidate dbt project settings\nLet's say we identified our workspace (as shown in the previous tutorial Transformations with SQL), and we have a workspace ID of:\n`bash\nNORMALIZE_WORKSPACE=\"5/0/\"`\nWe can verify that the dbt project is properly configured for that workspace:\n```bash\n!/usr/bin/env bash\ndocker run --rm -i -v airbyte_workspace:/data -w /data/$NORMALIZE_WORKSPACE/normalize --network host --entrypoint /usr/local/bin/dbt airbyte/normalization debug --profiles-dir=. --project-dir=.\n```\nExample Output:\n```text\nRunning with dbt=0.19.1\ndbt version: 0.19.1\npython version: 3.8.8\npython path: /usr/local/bin/python\nos info: Linux-5.10.25-linuxkit-x86_64-with-glibc2.2.5\nUsing profiles.yml file at ./profiles.yml\nUsing dbt_project.yml file at /data/5/0/normalize/dbt_project.yml\nConfiguration:\n  profiles.yml file [OK found and valid]\n  dbt_project.yml file [OK found and valid]\nRequired dependencies:\n - git [OK found]\nConnection:\n  host: localhost\n  port: 3000\n  user: postgres\n  database: postgres\n  schema: quarantine\n  search_path: None\n  keepalives_idle: 0\n  sslmode: None\n  Connection test: OK connection ok\n```\nCompile and build dbt normalization models\nIf the previous command does not show any errors or discrepancies, it is now possible to invoke the CLI from within the docker image to trigger transformation processing:\n```bash\n!/usr/bin/env bash\ndocker run --rm -i -v airbyte_workspace:/data -w /data/$NORMALIZE_WORKSPACE/normalize --network host --entrypoint /usr/local/bin/dbt airbyte/normalization run --profiles-dir=. --project-dir=.\n```\nExample Output:\n```text\nRunning with dbt=0.19.1\nFound 4 models, 0 tests, 0 snapshots, 0 analyses, 364 macros, 0 operations, 0 seed files, 1 source, 0 exposures\nConcurrency: 32 threads (target='prod')\n1 of 1 START table model quarantine.covid_epidemiology....................................................... [RUN]\n1 of 1 OK created table model quarantine.covid_epidemiology.................................................. [SELECT 35822 in 0.47s]\nFinished running 1 table model in 0.74s.\nCompleted successfully\nDone. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1\n```\nExporting dbt normalization project outside Airbyte\nAs seen in the tutorial on exploring workspace folder, it is possible to browse the `normalize` folder and examine further logs if an error occurs.\nIn particular, we can also take a look at the dbt models generated by Airbyte and export them to the local host filesystem:\n```bash\n!/usr/bin/env bash\nTUTORIAL_DIR=\"$(pwd)/tutorial/\"\nrm -rf $TUTORIAL_DIR/normalization-files\nmkdir -p $TUTORIAL_DIR/normalization-files\ndocker cp airbyte-server:/tmp/workspace/$NORMALIZE_WORKSPACE/normalize/ $TUTORIAL_DIR/normalization-files\nNORMALIZE_DIR=$TUTORIAL_DIR/normalization-files/normalize\ncd $NORMALIZE_DIR\ncat $NORMALIZE_DIR/models/generated/*/.sql\n```\nExample Output:\n```text\n{{ config(alias=\"covid_epidemiology_ab1\", schema=\"_airbyte_quarantine\", tags=[\"top-level-intermediate\"]) }}\n-- SQL model to parse JSON blob stored in a single column and extract into separated field columns as described by the JSON Schema\nselect\n    {{ json_extract_scalar('_airbyte_data', ['key']) }} as {{ adapter.quote('key') }},\n    {{ json_extract_scalar('_airbyte_data', ['date']) }} as {{ adapter.quote('date') }},\n    {{ json_extract_scalar('_airbyte_data', ['new_tested']) }} as new_tested,\n    {{ json_extract_scalar('_airbyte_data', ['new_deceased']) }} as new_deceased,\n    {{ json_extract_scalar('_airbyte_data', ['total_tested']) }} as total_tested,\n    {{ json_extract_scalar('_airbyte_data', ['new_confirmed']) }} as new_confirmed,\n    {{ json_extract_scalar('_airbyte_data', ['new_recovered']) }} as new_recovered,\n    {{ json_extract_scalar('_airbyte_data', ['total_deceased']) }} as total_deceased,\n    {{ json_extract_scalar('_airbyte_data', ['total_confirmed']) }} as total_confirmed,\n    {{ json_extract_scalar('_airbyte_data', ['total_recovered']) }} as total_recovered,\n    _airbyte_emitted_at\nfrom {{ source('quarantine', '_airbyte_raw_covid_epidemiology') }}\n-- covid_epidemiology\n{{ config(alias=\"covid_epidemiology_ab2\", schema=\"_airbyte_quarantine\", tags=[\"top-level-intermediate\"]) }}\n-- SQL model to cast each column to its adequate SQL type converted from the JSON schema type\nselect\n    cast({{ adapter.quote('key') }} as {{ dbt_utils.type_string() }}) as {{ adapter.quote('key') }},\n    cast({{ adapter.quote('date') }} as {{ dbt_utils.type_string() }}) as {{ adapter.quote('date') }},\n    cast(new_tested as {{ dbt_utils.type_float() }}) as new_tested,\n    cast(new_deceased as {{ dbt_utils.type_float() }}) as new_deceased,\n    cast(total_tested as {{ dbt_utils.type_float() }}) as total_tested,\n    cast(new_confirmed as {{ dbt_utils.type_float() }}) as new_confirmed,\n    cast(new_recovered as {{ dbt_utils.type_float() }}) as new_recovered,\n    cast(total_deceased as {{ dbt_utils.type_float() }}) as total_deceased,\n    cast(total_confirmed as {{ dbt_utils.type_float() }}) as total_confirmed,\n    cast(total_recovered as {{ dbt_utils.type_float() }}) as total_recovered,\n    _airbyte_emitted_at\nfrom {{ ref('covid_epidemiology_ab1_558') }}\n-- covid_epidemiology\n{{ config(alias=\"covid_epidemiology_ab3\", schema=\"_airbyte_quarantine\", tags=[\"top-level-intermediate\"]) }}\n-- SQL model to build a hash column based on the values of this record\nselect\n    *,\n    {{ dbt_utils.surrogate_key([\n        adapter.quote('key'),\n        adapter.quote('date'),\n        'new_tested',\n        'new_deceased',\n        'total_tested',\n        'new_confirmed',\n        'new_recovered',\n        'total_deceased',\n        'total_confirmed',\n        'total_recovered',\n    ]) }} as _airbyte_covid_epidemiology_hashid\nfrom {{ ref('covid_epidemiology_ab2_558') }}\n-- covid_epidemiology\n{{ config(alias=\"covid_epidemiology\", schema=\"quarantine\", tags=[\"top-level\"]) }}\n-- Final base SQL model\nselect\n    {{ adapter.quote('key') }},\n    {{ adapter.quote('date') }},\n    new_tested,\n    new_deceased,\n    total_tested,\n    new_confirmed,\n    new_recovered,\n    total_deceased,\n    total_confirmed,\n    total_recovered,\n    _airbyte_emitted_at,\n    _airbyte_covid_epidemiology_hashid\nfrom {{ ref('covid_epidemiology_ab3_558') }}\n-- covid_epidemiology from {{ source('quarantine', '_airbyte_raw_covid_epidemiology') }}\n```\nIf you have dbt installed locally on your machine, you can then view, edit, version, customize, and run the dbt models in your project outside Airbyte syncs.\n```bash\n!/usr/bin/env bash\ndbt deps --profiles-dir=$NORMALIZE_DIR --project-dir=$NORMALIZE_DIR\ndbt run --profiles-dir=$NORMALIZE_DIR --project-dir=$NORMALIZE_DIR --full-refresh\n```\nExample Output:\n```text\nRunning with dbt=0.19.1\nInstalling https://github.com/fishtown-analytics/dbt-utils.git@0.6.4\n  Installed from revision 0.6.4\nRunning with dbt=0.19.1\nFound 4 models, 0 tests, 0 snapshots, 0 analyses, 364 macros, 0 operations, 0 seed files, 1 source, 0 exposures\nConcurrency: 32 threads (target='prod')\n1 of 1 START table model quarantine.covid_epidemiology....................................................... [RUN]\n1 of 1 OK created table model quarantine.covid_epidemiology.................................................. [SELECT 35822 in 0.44s]\nFinished running 1 table model in 0.63s.\nCompleted successfully\nDone. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1\n```\nNow, that you've exported the generated normalization models, you can edit and tweak them as necessary.\nIf you want to know how to push your modifications back to Airbyte and use your updated dbt project during Airbyte syncs, you can continue with the following tutorial on importing transformations into Airbyte...",
    "tag": "airbyte"
  },
  {
    "title": "Transformations with Airbyte (Part 3/3)",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/operator-guides/transformation-and-normalization/transformations-with-airbyte.md",
    "content": "Transformations with Airbyte (Part 3/3)\nOverview\nThis tutorial will describe how to push a custom dbt transformation project back to Airbyte to use during syncs.\nThis guide is the last part of the tutorial series on transformations, following Transformations with SQL and connecting EL with T using dbt.\n(Example outputs are updated with Airbyte version 0.23.0-alpha from May 2021)\nTransformations with Airbyte\nAfter replication of data from a source connector (Extract) to a destination connector (Load), multiple optional transformation steps can now be applied as part of an Airbyte Sync. Possible workflows are:\n\nBasic normalization transformations as automatically generated by Airbyte dbt code generator.\nCustomized normalization transformations as edited by the user (the default generated normalization one should therefore be disabled)\nCustomized business transformations as specified by the user.\n\nPublic Git repository\nIn the connection settings page, I can add new Transformations steps to apply after normalization. For example, I want to run my custom dbt project jaffle_shop, whenever my sync is done replicating and normalizing my data.\nYou can find the jaffle shop test repository by clicking here.\n\n\nPrivate Git repository\nNow, let's connect my mono-repo Business Intelligence project stored in a private git repository to update the related tables and dashboards when my Airbyte syncs complete.\nNote that if you need to connect to a private git repository, the recommended way to do so is to generate a `Personal Access Token` that can be used instead of a password. Then, you'll be able to include the credentials in the git repository url:\n\nGitHub - Personal Access Tokens\nGitlab - Personal Access Tokens\nAzure DevOps - Personal Access Tokens\n\nAnd then use it for cloning:\n`text\ngit clone https://username:token@github.com/user/repo`\nWhere `https://username:token@github.com/user/repo` is the git repository url.\nExample of a private git repo used as transformations\nAs an example, I go through my GitHub account to generate a Personal Access Token to use in Airbyte with permissions to clone my private repositories:\n\nThis provides me with a token to use:\n\nIn Airbyte, I can use the git url as: `https://airbyteuser:ghp_***********ShLrG2yXGYF@github.com/airbyteuser/private-datawarehouse.git`\n\nHow-to use custom dbt tips\nAllows \"chained\" dbt transformations\nSince every transformation leave in his own Docker container, at this moment I can't rely on packages installed using `dbt deps` for the next transformations.\nAccording to the dbt documentation, I can configure the packages folder outside of the container:\n```yaml\ndbt_project.yml\npackages-install-path: '../dbt_packages'\n```\n\nIf I want to chain dbt deps and dbt run, I may use dbt build instead, which is not equivalent to the two previous commands, but will remove the need to alter the configuration of dbt.\n\nRefresh models partially\nSince I am using a mono-repo from my organization, other team members or departments may also contribute their dbt models to this centralized location. This will give us many dbt models and sources to build our complete data warehouse...\nThe whole warehouse is scheduled for full refresh on a different orchestration tool, or as part of the git repository CI. However, here, I want to partially refresh some small relevant tables when attaching this operation to a specific Airbyte sync, in this case, the Covid dataset.\nTherefore, I can restrict the execution of models to a particular tag or folder by specifying in the dbt cli arguments, in this case whatever is related to \"covid_api\":\n`text\nrun --models tag:covid_api opendata.base.*`\nNow, when replications syncs are triggered by Airbyte, my custom transformations from my private git repository are also run at the end!\nUsing a custom run with variables\nIf you want to use a custom run and pass variables you need to use the follow syntax:\n`bash\nrun --vars '{\"table_name\":\"sample\",\"schema_name\":\"other_value\"}'`\nThis string must have no space. There is a Github issue to improve this. If you want to contribute to Airbyte, this is a good opportunity!",
    "tag": "airbyte"
  },
  {
    "title": "Transformations with SQL (Part 1/3)",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/operator-guides/transformation-and-normalization/transformations-with-sql.md",
    "content": "Transformations with SQL (Part 1/3)\nTransformations with SQL (Part 1/3)\nOverview\nThis tutorial will describe how to integrate SQL based transformations with Airbyte syncs using plain SQL queries.\nThis is the first part of ELT tutorial. The second part goes deeper with Transformations with dbt and then wrap-up with a third part on Transformations with Airbyte.\n(Examples outputs are updated with Airbyte version 0.23.0-alpha from May 2021)\nFirst transformation step: Normalization\nAt its core, Airbyte is geared to handle the EL (Extract Load) steps of an ELT process. These steps can also be referred in Airbyte's dialect as \"Source\" and \"Destination\".\nHowever, this is actually producing a table in the destination with a JSON blob column... For the typical analytics use case, you probably want this json blob normalized so that each field is its own column.\nSo, after EL, comes the T (transformation) and the first T step that Airbyte actually applies on top of the extracted data is called \"Normalization\". You can find more information about it here.\nAirbyte runs this step before handing the final data over to other tools that will manage further transformation down the line.\nTo summarize, we can represent the ELT process in the diagram below. These are steps that happens between your \"Source Database or API\" and the final \"Replicated Tables\" with examples of implementation underneath:\n\nAnyway, it is possible to short-circuit this process (no vendor lock-in) and handle it yourself by turning this option off in the destination settings page.\nThis could be useful if:\n\nYou have a use-case not related to analytics that could be handled with data in its raw JSON format. \nYou can implement your own transformer. For example, you could write them in a different language, create them in an analytics engine like Spark, or use a transformation tool such as dbt or Dataform.\nYou want to customize and change how the data is normalized with your own queries.\n\nIn order to do so, we will now describe how you can leverage the basic normalization outputs that Airbyte generates to build your own transformations if you don't want to start from scratch.\nNote: We will rely on docker commands that we've gone over as part of another Tutorial on Exploring Docker Volumes.\n(Optional) Configure some Covid (data) source and Postgres destinations\nIf you have sources and destinations already setup on your deployment, you can skip to the next section.\nFor the sake of this tutorial, let's create some source and destination as an example that we can refer to afterward. We'll be using a file accessible from a public API, so you can easily reproduce this setup:\n`text\nHere are some examples of public API CSV:\nhttps://storage.googleapis.com/covid19-open-data/v2/latest/epidemiology.csv`\n\nAnd a local Postgres Database, making sure that \"Basic normalization\" is enabled:\n\nAfter setting up the connectors, we can trigger the sync and study the logs:\n\nNotice that the process ran in the `/tmp/workspace/5/0` folder.\nIdentify Workspace ID with Normalize steps\nIf you went through the previous setup of source/destination section and run a sync, you were able to identify which workspace was used, let's define some environment variables to remember this:\n`bash\nNORMALIZE_WORKSPACE=\"5/0/\"`\nOr if you want to find any folder where the normalize step was run:\n```bash\nfind automatically latest workspace where normalization was run\nNORMALIZE_WORKSPACE=`docker run --rm -i -v airbyte_workspace:/data  busybox find /data -path \"*normalize/models*\" | sed -E \"s;/data/([0-9]+/[0-9]+/)normalize/.*;\\1;g\" | sort | uniq | tail -n 1`\n```\nExport Plain SQL files\nAirbyte is internally using a specialized tool for handling transformations called dbt.\nThe Airbyte Python module reads the `destination_catalog.json` file and generates dbt code responsible for interpreting and transforming the raw data.\nThe final output of dbt is producing SQL files that can be run on top of the destination that you selected.\nTherefore, it is possible to extract these SQL files, modify them and run it yourself manually outside Airbyte!\nYou would be able to find these at the following location inside the server's docker container:\n`text\n/tmp/workspace/${NORMALIZE_WORKSPACE}/build/run/airbyte_utils/models/generated/airbyte_tables/<schema>/<your_table_name>.sql`\nIn order to extract them, you can run:\n```bash\n!/usr/bin/env bash\ndocker cp airbyte-server:/tmp/workspace/${NORMALIZE_WORKSPACE}/build/run/airbyte_utils/models/generated/ models/\nfind models\n```\nExample Output:\n`text\nmodels/airbyte_tables/quarantine/covid_epidemiology_f11.sql`\nLet's inspect the generated SQL file by running:\n`bash\ncat models/**/covid_epidemiology*.sql`\nExample Output:\n```sql\n create  table \"postgres\".quarantine.\"covid_epidemiology_f11__dbt_tmp\"\n  as (\nwith __dbt__CTE__covid_epidemiology_ab1_558 as (\n-- SQL model to parse JSON blob stored in a single column and extract into separated field columns as described by the JSON Schema\nselect\n    jsonb_extract_path_text(_airbyte_data, 'key') as \"key\",\n    jsonb_extract_path_text(_airbyte_data, 'date') as \"date\",\n    jsonb_extract_path_text(_airbyte_data, 'new_tested') as new_tested,\n    jsonb_extract_path_text(_airbyte_data, 'new_deceased') as new_deceased,\n    jsonb_extract_path_text(_airbyte_data, 'total_tested') as total_tested,\n    jsonb_extract_path_text(_airbyte_data, 'new_confirmed') as new_confirmed,\n    jsonb_extract_path_text(_airbyte_data, 'new_recovered') as new_recovered,\n    jsonb_extract_path_text(_airbyte_data, 'total_deceased') as total_deceased,\n    jsonb_extract_path_text(_airbyte_data, 'total_confirmed') as total_confirmed,\n    jsonb_extract_path_text(_airbyte_data, 'total_recovered') as total_recovered,\n    _airbyte_emitted_at\nfrom \"postgres\".quarantine._airbyte_raw_covid_epidemiology\n-- covid_epidemiology\n),  __dbt__CTE__covid_epidemiology_ab2_558 as (\n-- SQL model to cast each column to its adequate SQL type converted from the JSON schema type\nselect\n    cast(\"key\" as \n    varchar\n) as \"key\",\n    cast(\"date\" as \n    varchar\n) as \"date\",\n    cast(new_tested as \n    float\n) as new_tested,\n    cast(new_deceased as \n    float\n) as new_deceased,\n    cast(total_tested as \n    float\n) as total_tested,\n    cast(new_confirmed as \n    float\n) as new_confirmed,\n    cast(new_recovered as \n    float\n) as new_recovered,\n    cast(total_deceased as \n    float\n) as total_deceased,\n    cast(total_confirmed as \n    float\n) as total_confirmed,\n    cast(total_recovered as \n    float\n) as total_recovered,\n    _airbyte_emitted_at\nfrom __dbt__CTE__covid_epidemiology_ab1_558\n-- covid_epidemiology\n),  __dbt__CTE__covid_epidemiology_ab3_558 as (\n-- SQL model to build a hash column based on the values of this record\nselect\n    *,\n    md5(cast(\n\n\n```coalesce(cast(\"key\" as \nvarchar\n```\n\n\n), '') || '-' || coalesce(cast(\"date\" as \n    varchar\n), '') || '-' || coalesce(cast(new_tested as \n    varchar\n), '') || '-' || coalesce(cast(new_deceased as \n    varchar\n), '') || '-' || coalesce(cast(total_tested as \n    varchar\n), '') || '-' || coalesce(cast(new_confirmed as \n    varchar\n), '') || '-' || coalesce(cast(new_recovered as \n    varchar\n), '') || '-' || coalesce(cast(total_deceased as \n    varchar\n), '') || '-' || coalesce(cast(total_confirmed as \n    varchar\n), '') || '-' || coalesce(cast(total_recovered as \n    varchar\n), '')\nas \n    varchar\n)) as _airbyte_covid_epidemiology_hashid\nfrom __dbt__CTE__covid_epidemiology_ab2_558\n-- covid_epidemiology\n)-- Final base SQL model\nselect\n    \"key\",\n    \"date\",\n    new_tested,\n    new_deceased,\n    total_tested,\n    new_confirmed,\n    new_recovered,\n    total_deceased,\n    total_confirmed,\n    total_recovered,\n    _airbyte_emitted_at,\n    _airbyte_covid_epidemiology_hashid\nfrom __dbt__CTE__covid_epidemiology_ab3_558\n-- covid_epidemiology from \"postgres\".quarantine._airbyte_raw_covid_epidemiology\n  );\n```\nSimple SQL Query\nWe could simplify the SQL query by removing some parts that may be unnecessary for your current usage (such as generating a md5 column; Why exactly would I want to use that?!).\nIt would turn into a simpler query:\n```sql\ncreate table \"postgres\".\"public\".\"covid_epidemiology\"\nas (\n    select\n        _airbyte_emitted_at,\n        (current_timestamp at time zone 'utc')::timestamp as _airbyte_normalized_at,\n\n\n```    cast(jsonb_extract_path_text(\"_airbyte_data\",'key') as varchar) as \"key\",\n    cast(jsonb_extract_path_text(\"_airbyte_data\",'date') as varchar) as \"date\",\n    cast(jsonb_extract_path_text(\"_airbyte_data\",'new_tested') as float) as new_tested,\n    cast(jsonb_extract_path_text(\"_airbyte_data\",'new_deceased') as float) as new_deceased,\n    cast(jsonb_extract_path_text(\"_airbyte_data\",'total_tested') as float) as total_tested,\n    cast(jsonb_extract_path_text(\"_airbyte_data\",'new_confirmed') as float) as new_confirmed,\n    cast(jsonb_extract_path_text(\"_airbyte_data\",'new_recovered') as float) as new_recovered,\n    cast(jsonb_extract_path_text(\"_airbyte_data\",'total_deceased') as float) as total_deceased,\n    cast(jsonb_extract_path_text(\"_airbyte_data\",'total_confirmed') as float) as total_confirmed,\n    cast(jsonb_extract_path_text(\"_airbyte_data\",'total_recovered') as float) as total_recovered\nfrom \"postgres\".public._airbyte_raw_covid_epidemiology\n```\n\n\n);\n```\nCustomize SQL Query\nFeel free to:\n\nRename the columns as you desire\navoiding using keywords such as `\"key\"` or `\"date\"`\nYou can tweak the column data type if the ones generated by Airbyte are not the ones you favor\nFor example, let's use `Integer` instead of `Float` for the number of Covid cases...\nAdd deduplicating logic\n\nif you can identify which columns to use as Primary Keys\n(since airbyte isn't able to detect those automatically yet...)\n\n\n(Note: actually I am not even sure if I can tell the proper primary key in this dataset...)\n\nCreate a View (or materialized views) instead of a Table.\netc\n\n```sql\ncreate view \"postgres\".\"public\".\"covid_epidemiology\" as (\n    with parse_json_cte as (\n        select\n            _airbyte_emitted_at,\n\n\n```        cast(jsonb_extract_path_text(\"_airbyte_data\",'key') as varchar) as id,\n        cast(jsonb_extract_path_text(\"_airbyte_data\",'date') as varchar) as updated_at,\n        cast(jsonb_extract_path_text(\"_airbyte_data\",'new_tested') as float) as new_tested,\n        cast(jsonb_extract_path_text(\"_airbyte_data\",'new_deceased') as float) as new_deceased,\n        cast(jsonb_extract_path_text(\"_airbyte_data\",'total_tested') as float) as total_tested,\n        cast(jsonb_extract_path_text(\"_airbyte_data\",'new_confirmed') as float) as new_confirmed,\n        cast(jsonb_extract_path_text(\"_airbyte_data\",'new_recovered') as float) as new_recovered,\n        cast(jsonb_extract_path_text(\"_airbyte_data\",'total_deceased') as float) as total_deceased,\n        cast(jsonb_extract_path_text(\"_airbyte_data\",'total_confirmed') as float) as total_confirmed,\n        cast(jsonb_extract_path_text(\"_airbyte_data\",'total_recovered') as float) as total_recovered\n    from \"postgres\".public._airbyte_raw_covid_epidemiology\n),\ncte as (\n    select\n        *,\n        row_number() over (\n            partition by id\n            order by updated_at desc\n        ) as row_num\n    from parse_json_cte\n)\nselect\n    substring(id, 1, 2) as id, -- Probably not the right way to identify the primary key in this dataset...\n    updated_at,\n    _airbyte_emitted_at,\n\n    case when new_tested = 'NaN' then 0 else cast(new_tested as integer) end as new_tested,\n    case when new_deceased = 'NaN' then 0 else cast(new_deceased as integer) end as new_deceased,\n    case when total_tested = 'NaN' then 0 else cast(total_tested as integer) end as total_tested,\n    case when new_confirmed = 'NaN' then 0 else cast(new_confirmed as integer) end as new_confirmed,\n    case when new_recovered = 'NaN' then 0 else cast(new_recovered as integer) end as new_recovered,\n    case when total_deceased = 'NaN' then 0 else cast(total_deceased as integer) end as total_deceased,\n    case when total_confirmed = 'NaN' then 0 else cast(total_confirmed as integer) end as total_confirmed,\n    case when total_recovered = 'NaN' then 0 else cast(total_recovered as integer) end as total_recovered\nfrom cte\nwhere row_num = 1\n```\n\n\n);\n```\nThen you can run in your preferred SQL editor or tool!\nIf you are familiar with dbt or want to learn more about it, you can continue with the following tutorial using dbt...",
    "tag": "airbyte"
  },
  {
    "title": "Connector Development Kit (Javascript)",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/cdk-faros-js.md",
    "content": "Connector Development Kit (Javascript)\nThe Faros AI TypeScript/JavaScript CDK allows you to build Airbyte connectors quickly similarly to how our Python CDK does. This CDK currently offers support for creating Airbyte source connectors for:\n\nHTTP APIs\n\nResources\nThis document is the main guide for developing an Airbyte source with the Faros CDK.\nAn example of a source built with the Faros AI CDK can be found here. It's recommended that you follow along with the example source while building for the first time.",
    "tag": "airbyte"
  },
  {
    "title": "Best Practices",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/best-practices.md",
    "content": "Best Practices\nIn order to guarantee the highest quality for connectors, we've compiled the following best practices for connector development. Connectors which follow these best practices will be labelled as \"Airbyte Certified\" to indicate they've passed a high quality bar and will perform reliably in all production use cases. Following these guidelines is not required for your contribution to Airbyte to be accepted, as they add a barrier to entry for contribution (though adopting them certainly doesn't hurt!).\nPrinciples of developing connectors\n\nReliability + usability > more features. It is better to support 1 feature that works reliably and has a great UX than 2 that are unreliable or hard to use. One solid connector is better than 2 finicky ones.\nFail fast. A user should not be able to configure something that will not work. \nFail actionably. If a failure is actionable by the user, clearly let them know what they can do. Otherwise, make it very easy for them to give us necessary debugging information (logs etc.)\n\nFrom these principles we extrapolate the following goals for connectors, in descending priority order:\n\nCorrect user input should result in a successful sync. If there is an issue, it should be extremely easy for the user to see and report. \nIssues arising from bad user input should print an actionable error message. \"Invalid credentials\" is not an actionable message. \"Please verify your username/password is correct\" is better. \nWherever possible, a connector should support incremental sync. This prevents excessive load on the underlying data source.  **\nWhen running a sync, a connector should communicate its status frequently to provide clear feedback that it is working. Output a log message at least every 5 minutes. \nA connector should allow reading or writing as many entities as is feasible. Supporting syncing all entities from an API is preferred to only supporting a small subset which would satisfy narrow use cases. Similarly, a database should support as many data types as is feasible. \n\nNote that in the above list, the least important is the number of features it has (e.g: whether an API connector supports all entities in the API). The most important thing is that for its declared features, it is reliable and usable. The only exception are \u201cminimum viability\u201d features e.g: for some sources, it\u2019s not feasible to pull data without incremental due to rate limiting issues. In this case, those are considered usability issues.\nQuality certification checklist\nWhen reviewing connectors, we'll use the following \"checklist\" to verify whether the connector is considered \"Airbyte certified\" or closer to beta or alpha:\nIntegration Testing\nAs much as possible, prove functionality via testing. This means slightly different things depending on the type of connector:\n\nAll connectors must test all the sync modes they support during integration tests\nDatabase connectors should test that they can replicate all supported data types in both `read` and `discover` operations\nAPI connectors should validate records that every stream outputs data\nIf this causes rate limiting problems, there should be a periodic CI build which tests this on a less frequent cadence to avoid rate limiting\n\nThoroughly test edge cases. While Airbyte provides a Standard Test Suite that all connectors must pass, it's not possible for the standard test suite to cover all edge cases. When in doubt about whether the standard tests provide sufficient evidence of functionality, write a custom test case for your connector.\nCheck Connection\n\nVerify permissions upfront. The \"check connection\" operation should verify any necessary permissions upfront e.g: the provided API token has read access to the API entities. \nIn some cases it's not possible to verify permissions without knowing which streams the user wants to replicate. For example, a provided API token only needs read access to the \"Employees\" entity if the user wants to replicate the \"Employees\" stream. In this case, the CheckConnection operation should verify the minimum needed requirements (e.g: the API token exists), and the \"read\" or \"write\" operation should verify all needed permissions based on the provided catalog, failing if a required permission is not granted.\nProvide actionable feedback for incorrect input. \nExamples of non actionable error messages\n\"Can't connect\". The only recourse this gives the user is to guess whether they need to dig through logs or guess which field of their input configuration is incorrect. \n\n\nExamples of actionable error messages\n\"Your username/password combination is incorrect\"\n\"Unable to reach Database host: please verify that there are no firewall rules preventing Airbyte from connecting to the database\"\netc...\n\n\n\nRate Limiting\nMost APIs enforce rate limits. Your connector should gracefully handle those (i.e: without failing the connector process). The most common way to handle rate limits is to implement backoff.\nMaintaining connectors\nOnce a connector has been published for use within Airbyte, we must take special care to account for the customer impact of updates to the connector.\nSchema Breaking Changes\nFor connectors that are GA certified or highly used by customers, we should not introduce backwards incompatible changes into a stream's schema that impact how existing data is replicated and represented in the downstream destination. The schema serves as a contract with customers to define how data is synchronized. Subtractive changes can COMPLETELY BREAK a customer's workflows built on top of Airbyte, sometimes in a silent way. \nThe following types of changes are to be considered to be breaking:\n\nRemoving a property field from a schema\nRenaming a property field in a schema\nChanging a property's data type\n",
    "tag": "airbyte"
  },
  {
    "title": "Debugging Docker Containers",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/debugging-docker.md",
    "content": "Debugging Docker Containers\nThis guide will cover debugging JVM docker containers either started via Docker Compose or started by the\nworker container, such as a Destination container. This guide will assume use of IntelliJ Community edition,\nhowever the steps could be applied to another IDE or debugger.\nPrerequisites\nYou should have the airbyte repo downloaded and should be able to run the platform locally.\nAlso, if you're on macOS you will need to follow the installation steps for Docker Mac Connect.\nConnecting your debugger\nThis solution utilizes the environment variable `JAVA_TOOL_OPTIONS` which when set to a specific value allows us to connect our debugger. \nWe will also be setting up a Remote JVM Debug run configuration in IntelliJ which uses the IP address or hostname to connect.\n\nNote\nThe Docker Mac Connect tool is what makes it possible for macOS users to connect to a docker container\nby IP address.\n\nDocker Compose Extension\nBy default, the `docker compose` command will look for a `docker-compose.yaml` file in your directory and execute its instructions. However, you can \nprovide multiple files to the `docker compose` command with the `-f` option. You can read more about how Docker compose combines or overrides values when\nyou provide multiple files on Docker's Website.\nIn the Airbyte repo, there is already another file `docker-compose.debug.yaml` which extends the `docker-compose.yaml` file. Our goal is to set the\n`JAVA_TOOL_OPTIONS` environment variable in the environment of the container we wish to debug. If you look at the `server` configuration under `services`\nin the `docker-compose.debug.yaml` file, it should look like this:\n`yaml\n  server:\n    environment:\n      - JAVA_TOOL_OPTIONS=${DEBUG_SERVER_JAVA_OPTIONS}`\nWhat this is saying is: For the Service `server` add an environment variable `JAVA_TOOL_OPTIONS` with the value of the variable `DEBUG_SERVER_JAVA_OPTIONS`.\n`DEBUG_SERVER_JAVA_OPTIONS` has no default value, so if we don't provide one, `JAVA_TOOL_OPTIONS` will be blank or empty. When running the `docker compose` command,\nDocker will look to your local environment variables, to see if you have set a value for `DEBUG_SERVER_JAVA_OPTIONS` and copy that value. To set this value\nyou can either `export` the variable in your environment prior to running the `docker compose` command, or prepend the variable to the command. For our debugging purposes,\nwe want the value to be `-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=*:5005` so to connect our debugger to the `server` container, run the following:\n`bash\nDEBUG_SERVER_JAVA_OPTIONS=\"-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=*:5005\" VERSION=\"dev\" docker compose -f docker-compose.yaml -f docker-compose.debug.yaml up`\n\nNote\nThis command also passes in the `VERSION=dev` environment variable, which is recommended from the comments in the `docker-compose.debug.yaml`\n\nConnecting the Debugger\nNow we need to connect our debugger. In IntelliJ, open `Edit Configurations...` from the run menu (Or search for `Edit Configurations` in the command palette).\nCreate a new Remote JVM Debug Run configuration. The `host` option defaults to `localhost` which if you're on Linux you can leave this unchanged. \nOn a Mac however, you need to find the IP address of your container. Make sure you've installed and started the Docker Mac Connect\nservice prior to running the `docker compose` command. With your containers running, run the following command to easily fetch the IP addresses:\n`bash\n$ docker inspect $(docker ps -q ) --format='{{ printf \"%-50s\" .Name}} {{printf \"%-50s\" .Config.Image}} {{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}'\n/airbyte-proxy                                     airbyte/proxy:dev                                  172.18.0.10172.19.0.4\n/airbyte-server                                    airbyte/server:dev                                 172.18.0.9\n/airbyte-worker                                    airbyte/worker:dev                                 172.18.0.8\n/airbyte-source                                    sha256:5eea76716a190d10fd866f5ac6498c8306382f55c6d910231d37a749ad305960 172.17.0.2\n/airbyte-connector-builder-server                  airbyte/connector-builder-server:dev               172.18.0.6\n/airbyte-webapp                                    airbyte/webapp:dev                                 172.18.0.7\n/airbyte-cron                                      airbyte/cron:dev                                   172.18.0.5\n/airbyte-temporal                                  airbyte/temporal:dev                               172.18.0.2\n/airbyte-db                                        airbyte/db:dev                                     172.18.0.4172.19.0.3\n/airbyte-temporal-ui                               temporalio/web:1.13.0                              172.18.0.3172.19.0.2`\nYou should see an entry for `/airbyte-server` which is the container we've been targeting so copy its IP address (`172.18.0.9` in the example output above)\nand replace `localhost` in your IntelliJ Run configuration with the IP address.\nSave your Remote JVM Debug run configuration and run it with the debug option. You should now be able to place breakpoints in any code that is being executed by the \n`server` container. If you need to debug another container from the original `docker-compose.yaml` file, you could modify the `docker-compose.debug.yaml` file with a similar option.\nDebugging Containers Launched by the Worker container\nThe Airbyte platform launches some containers as needed at runtime, which are not defined in the `docker-compose.yaml` file. These containers are the source or destination\ntasks, among other things. But if we can't pass environment variables to them through the `docker-compose.debug.yaml` file, then how can we set the\n`JAVA_TOOL_OPTIONS` environment variable? Well, the answer is that we can pass it through the container which launches the other containers - the `worker` container.\nFor this example, lets say that we want to debug something that happens in the `destination-postgres` connector container. To follow along with this example, you will\nneed to have set up a connection which uses postgres as a destination, however if you want to use a different connector like `source-postgres`, `destination-bigquery`, etc. that's fine.\nIn the `docker-compose.debug.yaml` file you should see an entry for the `worker` service which looks like this\n`yaml\n  worker:\n    environment:\n      - DEBUG_CONTAINER_IMAGE=${DEBUG_CONTAINER_IMAGE}\n      - DEBUG_CONTAINER_JAVA_OPTS=-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=*:5005`\nSimilar to the previous debugging example, we want to pass an environment variable to the `docker compose` command. This time we're setting the \n`DEBUG_CONTAINER_IMAGE` environment variable to the name of the container we're targeting. For our example that is `destination-postgres` so run the command:\n`bash\nDEBUG_CONTAINER_IMAGE=\"destination-postgres\" VERSION=\"dev\" docker compose -f docker-compose.yaml -f docker-compose.debug.yaml up`\nThe `worker` container now has an environment variable `DEBUG_CONTAINER_IMAGE` with a value of `destination-postgres` which when it compares when it is\nspawning containers. If the container name matches the environment variable, it will set the `JAVA_TOOL_OPTIONS` environment variable in the container to\nthe value of its `DEBUG_CONTAINER_JAVA_OPTS` environment variable, which is the same value we used in the `server` example.\nConnecting the Debugger to a Worker Spawned Container\nTo connect your debugger, the container must be running. This `destination-postgres` container will only run when we're running one of its tasks, \nsuch as when a replication is running. Navigate to a connection in your local Airbyte instance at http://localhost:8000 which uses postgres as a destination.\nIf you ran through the Postgres to Postgres replication tutorial, you can use this connection.\nOn the connection page, trigger a manual sync with the \"Sync now\" button. Because we set the `suspend` option to `y` in our `JAVA_TOOL_OPTIONS` the \ncontainer will pause all execution until the debugger is connected. This can be very useful for methods which run very quickly, such as the Check method.\nHowever, this could be very detrimental if it were pushed into a production environment. For now, it gives us time to set a new Remote JVM Debug Configuraiton. \nThis container will have a different IP than the `server` Remote JVM Debug Run configuration we set up earlier. So lets set up a new one with the IP of \nthe `destination-postgres` container:\n`bash\n$ docker inspect $(docker ps -q ) --format='{{ printf \"%-50s\" .Name}} {{printf \"%-50s\" .Config.Image}} {{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}'\n/destination-postgres-write-52-0-grbsw             airbyte/destination-postgres:0.3.26                \n/airbyte-proxy                                     airbyte/proxy:dev                                  172.18.0.10172.19.0.4\n/airbyte-worker                                    airbyte/worker:dev                                 172.18.0.8\n/airbyte-server                                    airbyte/server:dev                                 172.18.0.9\n/airbyte-destination                               postgres                                           172.17.0.3\n/airbyte-source                                    sha256:5eea76716a190d10fd866f5ac6498c8306382f55c6d910231d37a749ad305960 172.17.0.2\n/airbyte-connector-builder-server                  airbyte/connector-builder-server:dev               172.18.0.6\n/airbyte-webapp                                    airbyte/webapp:dev                                 172.18.0.7\n/airbyte-cron                                      airbyte/cron:dev                                   172.18.0.5\n/airbyte-temporal                                  airbyte/temporal:dev                               172.18.0.3\n/airbyte-db                                        airbyte/db:dev                                     172.18.0.2172.19.0.3\n/airbyte-temporal-ui                               temporalio/web:1.13.0                              172.18.0.4172.19.0.2`\nHuh? No IP address, weird. Interestingly enough, all the IPs are sequential but there is one missing, `172.18.0.1`. If we attempt to use that IP in remote debugger, it works!\nYou can now add breakpoints and debug any code which would be executed in the `destination-postgres` container.\nHappy Debugging!\nConnecting the Debugger to an Integration Test Spawned Container\nYou can also debug code contained in containers spawned in an integration test! This can be used to debug integration tests as well as testing code changes. \nThe steps involved are: \n1. Follow all the steps outlined above to set up the Remote JVM Debug run configuration.\n2. Edit the run configurations associated with the given integration test with the following environment variables:`DEBUG_CONTAINER_IMAGE=source-postgres;DEBUG_CONTAINER_JAVA_OPTS=-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=*:5005`\nNote that you will have to keep repeating this step for every new integration test run configuration you create. \n3. Run the integration test in debug mode. In the debug tab, open up the Remote JVM Debugger run configuration you just created. \n4. Keep trying to attach the Remote JVM Debugger. It will likely fail a couple of times and eventually connect to the test container. If you want a more\ndeterministic way to connect the debugger, you can set a break point in the `DockerProcessFactor.localDebuggingOptions()` method. Resume running the integration test run and\nthen attempt to attach the Remote JVM Debugger (you still might need a couple of tries).\nGotchas\nSo now that your debugger is set up, what else is there to know?\nCode changes\nWhen you're debugging, you might want to make a code change. Anytime you make a code change, your code will become out of sync with the container which is run by the platform.\nEssentially this means that after you've made a change you will need to rebuild the docker container you're debugging. Additionally, for the connector containers, you may have to navigate to\n\"Settings\" in your local Airbyte Platform's web UI and change the version of the container to `dev`. See you connector's `README` for details on how to rebuild the container image.\nPorts\nIn this tutorial we've been using port `5005` for all debugging. It's the default, so we haven't changed it. If you need to debug multiple containers however, they will clash on this port.",
    "tag": "airbyte"
  },
  {
    "title": "Connector Specification Reference",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/connector-specification-reference.md",
    "content": "Connector Specification Reference\nThe connector specification describes what inputs can be used to configure a connector. Like the rest of the Airbyte Protocol, it uses JsonSchema, but with some slight modifications.\nDemoing your specification\nWhile iterating on your specification, you can preview what it will look like in the UI in realtime by following the instructions here.\nSecret obfuscation\nBy default, any fields in a connector's specification are visible can be read in the UI. However, if you want to obfuscate fields in the UI and API (for example when working with a password), add the `airbyte_secret` annotation to your connector's `spec.json` e.g:\n`text\n\"password\": {\n  \"type\": \"string\",\n  \"examples\": [\"hunter2\"],\n  \"airbyte_secret\": true\n},`\nHere is an example of what the password field would look like: \nOrdering fields in the UI\nUse the `order` property inside a definition to determine the order in which it will appear relative to other objects on the same level of nesting in the UI. \nFor example, using the following spec: \n`{\n  \"username\": {\"type\": \"string\", \"order\": 1},\n  \"password\": {\"type\": \"string\", \"order\": 2},\n  \"cloud_provider\": {\n    \"order\": 0,\n    \"type\": \"object\",\n    \"properties\" : {\n      \"name\": {\"type\": \"string\", \"order\": 0},\n      \"region\": {\"type\": \"string\", \"order\": 1}\n    }\n  }\n}`\nwill result in the following configuration on the UI: \n\n:::info\nWithin an object definition, if some fields have the `order` property defined, and others don't, then the fields without the `order` property defined should be rendered last in the UI. Among those elements (which don't have `order` defined), no ordering is guaranteed. \n:::\nMulti-line String inputs\nSometimes when a user is inputting a string field into a connector, newlines need to be preserveed. For example, if we want a connector to use an RSA key which looks like this:\n`text\n---- BEGIN PRIVATE KEY ----\n123\n456\n789\n---- END PRIVATE KEY ----`\nwe need to preserve the line-breaks. In other words, the string `---- BEGIN PRIVATE KEY ----123456789---- END PRIVATE KEY ----` is not equivalent to the one above since it loses linebreaks.\nBy default, string inputs in the UI can lose their linebreaks. In order to accept multi-line strings in the UI, annotate your string field with `multiline: true` e.g:\n`text\n\"private_key\": {\n  \"type\": \"string\",\n  \"description\": \"RSA private key to use for SSH connection\",\n  \"airbyte_secret\": true,\n  \"multiline\": true\n},`\nthis will display a multi-line textbox in the UI like the following screenshot: \nHiding inputs in the UI\nIn some rare cases, a connector may wish to expose an input that is not available in the UI, but is still potentially configurable when running the connector outside of Airbyte, or via the UI. For example, exposing a very technical configuration like the page size of an outgoing HTTP requests may only be relevant to power users, and therefore shouldn't be available via the UI but might make sense to expose via the API. \nIn this case, use the `\"airbyte_hidden\": true` keyword to hide that field from the UI. E.g: \n`{\n  \"first_name\": {\n    \"type\": \"string\",\n    \"title\": \"First Name\"\n  },\n  \"secret_name\": {\n    \"type\": \"string\",\n    \"title\": \"You can't see me!!!\",\n    \"airbyte_hidden\": true\n  }\n}`\nResults in the following form:\n\nAirbyte Modifications to `jsonschema`\nUsing `oneOf`\nIn some cases, a connector needs to accept one out of many options. For example, a connector might need to know the compression codec of the file it will read, which will render in the Airbyte UI as a list of the available codecs. In JSONSchema, this can be expressed using the oneOf keyword.\n:::info\nSome connectors may follow an older format for dropdown lists, we are currently migrating away from that to this standard.\n:::\nIn order for the Airbyte UI to correctly render a specification, however, a few extra rules must be followed:\n\nThe top-level item containing the `oneOf` must have `type: object`.\nEach item in the `oneOf` array must be a property with `type: object`.\nOne `string` field with the same property name must be consistently present throughout each object inside the `oneOf` array. It is required to add a const value unique to that `oneOf` option.\n\nLet's look at the source-file implementation as an example. In this example, we have `provider` as a dropdown list option, which allows the user to select what provider their file is being hosted on. We note that the `oneOf` keyword lives under the `provider` object as follows:\nIn each item in the `oneOf` array, the `option_title` string field exists with the aforementioned `const` value unique to that item. This helps the UI and the connector distinguish between the option that was chosen by the user. This can be displayed with adapting the file source spec to this example:\n`javascript\n{\n  \"connection_specification\": {\n    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n    \"title\": \"File Source Spec\",\n    \"type\": \"object\",\n    \"required\": [\"dataset_name\", \"format\", \"url\", \"provider\"],\n    \"properties\": {\n      \"dataset_name\": {\n        ...\n      },\n      \"format\": {\n        ...\n      },\n      \"reader_options\": {\n        ...\n      },\n      \"url\": {\n        ...\n      },\n      \"provider\": {\n        \"type\": \"object\",\n        \"oneOf\": [\n          {\n            \"required\": [\n              \"option_title\"\n            ],\n            \"properties\": {\n              \"option_title\": {\n                \"type\": \"string\",\n                \"const\": \"HTTPS: Public Web\",\n                \"order\": 0\n              }\n            }\n          },\n          {\n            \"required\": [\n              \"option_title\"\n            ],\n            \"properties\": {\n              \"option_title\": {\n                \"type\": \"string\",\n                \"const\": \"GCS: Google Cloud Storage\",\n                \"order\": 0\n              },\n              \"service_account_json\": {\n                \"type\": \"string\",\n                \"description\": \"In order to access private Buckets stored on Google Cloud, this connector would need a service account json credentials with the proper permissions as described <a href=\\\"https://cloud.google.com/iam/docs/service-accounts\\\" target=\\\"_blank\\\">here</a>. Please generate the credentials.json file and copy/paste its content to this field (expecting JSON formats). If accessing publicly available data, this field is not necessary.\"\n              }\n            }\n          }\n        ]\n      }\n  }\n}`\nUsing `enum`\nIn regular `jsonschema`, some drafts enforce that `enum` lists must contain distinct values, while others do not. For consistency, Airbyte enforces this restriction.\nFor example, this spec is invalid, since `a_format` is listed twice under the enumerated property `format`:\n```javascript\n{\n  \"connection_specification\": {\n    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n    \"title\": \"File Source Spec\",\n    \"type\": \"object\",\n    \"required\": [\"format\"],\n    \"properties\": {\n      \"dataset_name\": {\n        ...\n      },\n      \"format\": {\n        type: \"string\",\n        enum: [\"a_format\", \"another_format\", \"a_format\"]\n      },\n    }\n  }\n}",
    "tag": "airbyte"
  },
  {
    "title": "Connector Development",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development",
    "content": "Connector Development\nAirbyte supports two types of connectors: Sources and Destinations. A connector takes the form of a Docker image which follows the Airbyte specification.\nTo build a new connector in Java or Python, we provide templates so you don't need to start everything from scratch.\nNote: you are not required to maintain the connectors you create. The goal is that the Airbyte core team and the community help maintain the connector.\nLow-code Connector-Development Framework\nYou can use the low-code framework to build source connectors for REST APIs via a connector builder UI or by modifying boilerplate YAML files.\nPython Connector-Development Kit (CDK)\nYou can build a connector very quickly in Python with the Airbyte CDK, which generates 75% of the code required for you.\nC#/.NET Connector-Development Kit (CDK)\nYou can build a connector very quickly in C# .NET with the Airbyte Dotnet CDK, which generates 75% of the code required for you.\nTS/JS Connector-Development Kit (Faros AI Airbyte CDK)\nYou can build a connector in TypeScript/JavaScript with the Faros AI CDK, which generates and boostraps most of the code required for HTTP Airbyte sources.\nThe Airbyte specification\nBefore building a new connector, review Airbyte's data protocol specification.\nAdding a new connector\nRequirements\nTo add a new connector you need to:\n\nImplement & Package your connector in an Airbyte Protocol compliant Docker image\nAdd integration tests for your connector. At a minimum, all connectors must pass Airbyte's standard test suite, but you can also add your own tests. \nDocument how to build & test your connector\nPublish the Docker image containing the connector\n\nEach requirement has a subsection below.\n1. Implement & package the connector\nIf you are building a connector in any of the following languages/frameworks, then you're in luck! We provide autogenerated templates to get you started quickly:\nSources\n\nPython Source Connector\nSinger-based Python Source Connector. Singer.io is an open source framework with a large community and many available connectors (known as taps & targets). To build an Airbyte connector from a Singer tap, wrap the tap in a thin Python package to make it Airbyte Protocol-compatible. See the Github Connector for an example of an Airbyte Connector implemented on top of a Singer tap.\nGeneric Connector: This template provides a basic starting point for any language.\n\nDestinations\n\nJava Destination Connector\nPython Destination Connector\n\nCreating a connector from a template\nRun the interactive generator:\n`text\ncd airbyte-integrations/connector-templates/generator\n./generate.sh`\nand choose the relevant template by using the arrow keys. This will generate a new connector in the `airbyte-integrations/connectors/<your-connector>` directory.\nSearch the generated directory for \"TODO\"s and follow them to implement your connector. For more detailed walkthroughs and instructions, follow the relevant tutorial:\n\nSpeedrun: Building a HTTP source with the CDK\nBuilding a HTTP source with the CDK\nBuilding a Python source \nBuilding a Python destination\nBuilding a Java destination\n\nAs you implement your connector, make sure to review the Best Practices for Connector Development guide. Following best practices is not a requirement for merging your contribution to Airbyte, but it certainly doesn't hurt ;)\n2. Integration tests\nAt a minimum, your connector must implement the acceptance tests described in Testing Connectors\nNote: Acceptance tests are not yet available for Python destination connectors. Coming soon!\n3. Document building & testing your connector\nIf you're writing in Python or Java, skip this section -- it is provided automatically.\nIf you're writing in another language, please document the commands needed to:\n\nBuild your connector docker image (usually this is just `docker build .` but let us know if there are necessary flags, gotchas, etc..) \nRun any unit or integration tests in a Docker image.\n\nYour integration and unit tests must be runnable entirely within a Docker image. This is important to guarantee consistent build environments.\nWhen you submit a PR to Airbyte with your connector, the reviewer will use the commands you provide to integrate your connector into Airbyte's build system as follows:\n\n`:airbyte-integrations:connectors:source-<name>:build` should run unit tests and build the integration's Docker image \n`:airbyte-integrations:connectors:source-<name>:integrationTest` should run integration tests including Airbyte's Standard test suite.\n\n4. Publish the connector\nTypically this will be handled as part of code review by an Airbyter. There is a section below on what steps are needed for publishing a connector and will mostly be used by Airbyte employees publishing the connector.\nUpdating an existing connector\nThe steps for updating an existing connector are the same as for building a new connector minus the need to use the autogenerator to create a new connector. Therefore the steps are:\n\nIterate on the connector to make the needed changes\nRun tests\nAdd any needed docs updates\nCreate a PR to get the connector published\n\nAdding normalization to a connector\nIn order to enable normalization for a destination connector, you'll need to set some fields on the destination definitions entry for the connector. This is done in the `airbyte-config/init/src/main/resources/seed/destination_definitions.yaml` file.\nNew connectors\nIf you're adding normalization to a new connector, you'll need to first add a destination definitions entry:\n1. Add a new connector definition in `airbyte-config/init/src/main/resources/seed/destination_definitions.yaml`. You can copy an existing entry and modify it to match your connector, generating a new UUIDv4 for the `destinationDefinitionId`.\n2. Run the command `./gradlew :airbyte-config:init:processResources` to generate the seed spec yaml files, and commit the changes to the PR. See this readme for more information.\nAdd normalization fields\nOnce you have a destination definitions entry, you'll need to add a `normaliationConfig` field to enable normalization.\nHere's an example of normalization fields being set to enable normalization for the Postgres destination:\n`yaml\nnormalizationConfig:\n    normalizationRepository: airbyte/normalization\n    normalizationTag: 0.2.25\n    normalizationIntegrationType: postgres`\nFor more information about what these fields mean, see the NormalizationDestinationDefinitionConfig schema.\nThe presence of these fields will enable normalization for the connector, and determine which docker image will run.\nPublishing a connector\nOnce you've finished iterating on the changes to a connector as specified in its `README.md`, follow these instructions to ship the new version of the connector with Airbyte out of the box.\n\nBump the version in the `Dockerfile` of the connector (`LABEL io.airbyte.version=X.X.X`). \nSubmit a PR containing the changes you made.\nOne of Airbyte maintainers will review the change and publish the new version of the connector to Docker hub. Triggering tests and publishing connectors can be done by leaving a comment on the PR with the following format (the PR must be from the Airbyte repo, not a fork):\n\n```text\n   # to run integration tests for the connector\n   # Example: /test connector=connectors/source-hubspot\n   /test connector=(connectors|bases)/ \n# to run integration tests, publish the connector, and use the updated connector version in our config/metadata files\n   # Example: /publish connector=connectors/source-hubspot\n   /publish connector=(connectors|bases)/\n   ```\n\nOPTIONAL: Necessary if this is a new connector, or the automated connector version bump fails\n\nUpdate/Add the connector definition in the Airbyte connector index to use the new version:\n        * `airbyte-config/init/src/main/resources/seed/source_definitions.yaml` if it is a source\n        * `airbyte-config/init/src/main/resources/seed/destination_definitions.yaml` if it is a destination.\n\n\nThen run the command `./gradlew :airbyte-config:init:processResources` to generate the seed spec yaml files, and commit the changes to the PR. See this readme for more information.\n\n\nIf the `README.md` file of the connector contains a `Changelog` section, add the new version and relevant release information to the table in the section.\n\nThe new version of the connector is now available for everyone who uses it. Thank you!\n\nThe /publish command\nPublishing a connector can be done using the `/publish` command as outlined in the above section. The command runs a github workflow, and has the following configurable parameters:\n* connector - Required. This tells the workflow which connector to publish. e.g. `connector=connectors/source-amazon-ads`. This can also be a comma-separated list of many connectors, e.g. `connector=connectors/source-s3,connectors/destination-postgres,connectors/source-facebook-marketing`. See the parallel flag below if publishing multiple connectors.\n* repo - Defaults to the main airbyte repo. Set this when building connectors from forked repos. e.g. `repo=userfork/airbyte`\n* gitref - Defaults to the branch of the PR where the /publish command is run as a comment. If running manually, set this to your branch where you made changes e.g. `gitref=george/s3-update`\n* comment-id - This is automatically filled if you run /publish from a comment and enables the workflow to write back success/fail logs to the git comment.\n* auto-bump-version - Defaults to true, automates the post-publish process of bumping the connector's version in the yaml seed definitions and generating spec.\n* parallel - Defaults to false. If set to true, a pool of runner agents will be spun up to allow publishing multiple connectors in parallel. Only switch this to true if publishing multiple connectors at once to avoid wasting $$$.\nUsing credentials in CI\nIn order to run integration tests in CI, you'll often need to inject credentials into CI. There are a few steps for doing this:\n1. Place the credentials into Google Secret Manager(GSM): Airbyte uses a project 'Google Secret Manager' service as the source of truth for all CI secrets. Place the credentials exactly as they should be used by the connector into a GSM secret here i.e.: it should basically be a copy paste of the `config.json` passed into a connector via the `--config` flag. We use the following naming pattern: `SECRET_<capital source OR destination name>_CREDS` e.g: `SECRET_SOURCE-S3_CREDS` or `SECRET_DESTINATION-SNOWFLAKE_CREDS`.\n2. Add the GSM secret's labels:\n    * `connector` (required) -- unique connector's name or set of connectors' names with '_' as delimiter i.e.: `connector=source-s3`, `connector=destination-snowflake`\n    * `filename` (optional) -- custom target secret file. Unfortunately Google doesn't use '.' into labels' values and so Airbyte CI scripts will add '.json' to the end automatically. By default secrets will be saved to `./secrets/config.json` i.e: `filename=config_auth` => `secrets/config_auth.json`\n3. Save a necessary JSON value Example.\n4. That should be it.\nAccess CI secrets on GSM\nAccess to GSM storage is limited to Airbyte employees. To give an employee permissions to the project:\n1. Go to the permissions' page\n2. Add a new principal to `dataline-integration-testing`:\n- input their login email\n- select the role `Development_CI_Secrets`\n3. Save",
    "tag": "airbyte"
  },
  {
    "title": "UX Handbook",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/ux-handbook.md",
    "content": "UX Handbook\nConnector Development UX Handbook\n\nOverview\nThe goal of this handbook is to allow scaling high quality decision making when developing connectors.\nThe Handbook is a living document, meant to be continuously updated. It is the best snapshot we can produce of the lessons learned from building and studying hundreds of connectors. While helpful, this snapshot is never perfect. Therefore, this Handbook is not a replacement for good judgment, but rather learnings that should help guide your work.\nHow to use this handbook\n\nWhen thinking about a UX-impacting decision regarding connectors, consult this Handbook.\nIf the Handbook does not answer your question, then consider proposing an update to the Handbook if you believe your question will be applicable to more cases.\n\nDefinition of UX-impacting changes\nUX-impacting changes are ones which impact how the user directly interacts with, consumes, or perceives the product.\nExamples:\n\nPublic-facing documentation\nInput configuration\nOutput schema\nPrerequisite configuration by the user (e.g: you need to link an instagram account to your Facebook page for this connector to work properly)\nConsolidating two connectors into one, or splitting one connector into two\nWait time for human-at-keyboard\nAnything that negatively impacts the runtime of the connector (e.g: a change that makes the runtime go from 10 minutes to 20 minutes on the same data size)\nAny other change which you deem UX-impacting\nThe guide can\u2019t cover everything, so this is an escape hatch based on the developer\u2019s judgment.\n\nExamples of UX-impacting changes:\n\nAdding or removing an input field to/from spec.json\nAdding or removing fields from the output schema\nAdding a new stream or category of stream (e.g: supporting views in databases)\nAdding OAuth support\n\nExamples of non-UX-impacting changes:\n\nRefactoring without changing functionality\nBugfix (e.g: pagination doesn\u2019t work correctly)\n\nGuiding Principles\nWould you trust AWS or Docker if it only worked 70, 80, or 90% of the time or if it leaked your business secrets? Yeah, me neither. You would only build on a tool if it worked at least 99% of the time. Infrastructure should give you back your time, rather than become a debugging timesink.\nThe same is true with Airbyte: if it worked less than 99% of the time, many users will stop using it. Airbyte is an infrastructure component within a user\u2019s data pipeline. Our users\u2019 goal is to move data; Airbyte is an implementation detail. In that sense, it is much closer to Terraform, Docker, or AWS than an end application.\nTrust & Reliability are the top concerns\nOur users have the following hierarchy of needs: \n\nSecurity\nUsers often move very confidential data like revenue numbers, salaries, or confidential documents through Airbyte. A user therefore must trust that their data is secure. This means no leaking credentials in logs or plain text, no vulnerabilities in the product, no frivolous sharing of credentials or data over internal slack channels, video calls, or other communications etc.\nData integrity\nData replicated by Airbyte must be correct and complete. If a user moves data with Airbyte, then all of the data must be present, and it must all be correct - no corruption, incorrect values, or wrongly formatted data.\nSome tricky examples which can break data integrity if not handled correctly:\n\nZipcodes for the US east coast should not lose their leading zeros because of being detected as integer\nDatabase timezones could affect the value of timestamps\nEsoteric text values (e.g: weird UTF characters)\n\nReliability\nA connector needs to be reliable. Otherwise, a user will need to spend a lot of time debugging, and at that point, they\u2019re better off using a competing product. The connector should be able to handle large inputs, weirdly formatted inputs, all data types, and basically anything a user should throw at it.\nIn other words, a connector should work 100% of the time, but 99.9% is occasionally acceptable.\nSpeed\nSync speed minimizes the time needed for deriving value from data. It also provides a better user experience as it allows quick iteration on connector configurations without suffering through long wait times. \nEase of use\nPeople love and trust a product that is easy to use. This means that it works as quickly as possible, introduces no friction, and uses sensible defaults that are good enough for 95% of users.\nAn important component of usability is predictability. That is, as much as possible, a user should know before running a connector what its output will be and what the different tables will mean.\nIdeally, they would even see an ERD describing the output schema they can expect to find in the destination. (This particular feature is tracked here).\nFeature Set\nOur connectors should cover as many use cases as is feasible. While it may not always work like that given our incremental delivery preference, we should always strive to provide the most featureful connectors which cover as much of the underlying API or database surface as possible.\nThere is also a tension between featureset and ease of use. The more features are available, the more thought it takes to make the product easy and intuitive to use. We\u2019ll elaborate on this later.\nAirbyte's Target Personas\nWithout repeating too many details mentioned elsewhere, the important thing to know is Airbyte serves all the following personas:\n| Persona        | Level of technical knowledge                                                                                                                                                                                                                                                                                                |\n| ------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| Data Analyst       | Proficient with:Data manipulation tools like Excel or SQLDashboard tools like LookerNot very familiar with reading API docs and doesn't know what a curl request is. But might be able to generate an API key if you tell them exactly how.                                                          |\n| Analytics Engineer | Proficient with:SQL & DBTGitA scripting language like PythonShallow familiarity with infra tools like DockerMuch more technical than a data analyst, but not as much as a data engineer                                                                                                 |\n| Data Engineer      | Proficient with:SQL & DBTGit2 or more programming languagesInfra tools like Docker or KubernetesCloud technologies like AWS or GCPBuilding or consuming APIsorhestartion tools like AirflowThe most technical persona we serve. Think of them like an engineer on your team |\nKeep in mind that the distribution of served personas will differ per connector. Data analysts are highly unlikely to form the majority of users for a very technical connector like say, Kafka.\nSpecific Guidelines\nInput Configuration\naka spec.json\nAvoid configuration completely when possible\nConfiguration means more work for the user and more chances for confusion, friction, or misconfiguration. If I could wave a magic wand, a user wouldn\u2019t have to configure anything at all. Unfortunately, this is not reality, and some configuration is strictly required. When this is the case, follow the guidelines below.\nAvoid exposing implementation details in configuration\nIf a configuration controls an implementation detail (like how many retries a connector should make before failing), then there should be almost no reason to expose this. If you feel a need to expose it, consider it might be a smell that the connector implementation is not robust.\nPut another way, if a configuration tells the user how to do its job of syncing data rather than what job to achieve, it\u2019s a code smell.\nFor example, the memory requirements for a database connector which syncs a table with very wide rows (50mb rows) can be very different than when syncing a table with very narrow rows (10kb per row). In this case, it may be acceptable to ask the user for some sort of \u201chint\u201d/tuning parameter in configuration (hidden behind advanced configuration) to ensure the connector performs reliably or quickly. But even then, this option would strictly be a necessary evil/escape hatch. It is much more preferable for the connector to auto-detect what this setting should be and never need to bother the user with it.\nMinimize required configurations by setting defaults whenever possible\nIn many cases, a configuration can be avoided by setting a default value for it but still making it possible to set other values. Whenever possible, follow this pattern.\nHide technical or niche parameters under an \u201cAdvanced\u201d section\nSometimes, it\u2019s inevitable that we need to expose some advanced or technical configuration. For example, the option to upload a TLS certificate to connect to a database, or the option to configure the number of retries done by an API connector: while these may be useful to some small percentage of users, it\u2019s not worth making all users think or get confused about them.\nNote: this is currently blocked by this issue.\nAdd a \u201ctitle\u201d and \u201cdescription\u201d property for every input parameter\nThis displays this information to the user in a polished way and gives less technical users (e.g: analysts) confidence that they can use this product. Be specific and unambiguous in the wording, explaining more than just the field name alone provides.\nFor example, the following spec:\n`json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"user_name\": {\n      \"type\": \"string\"\n    }\n  }\n}`\nproduces the following input field in the UI: \n\nWhereas the following specification:\n`json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"user_name\": {\n      \"type\": \"string\",\n      \"description\": \"The username you use to login to the database\",\n      \"title\": \"Username\"\n    }\n  }\n}`\nproduces the following UI:\n\nThe title should use Pascal Case \u201cwith spaces\u201d e.g: \u201cAttribution Lookback Window\u201d, \u201cHost URL\u201d, etc...\nClearly document the meaning and impact of all parameters\nAll configurations must have an unmistakable explanation describing their purpose and impact, even the obvious ones. Remember, something that is obvious to an analyst may not be obvious to an engineer, and vice-versa.\nFor example, in some Ads APIs like Facebook, the user\u2019s data may continue to be updated up to 28 days after it is created. This happens because a user may take action because of an ad (like buying a product) many days after they see the ad. In this case, the user may want to configure a \u201clookback\u201d window for attributing.\nAdding a parameter \u201cattribution_lookback_window\u201d with no explanation might confuse the user more than it helps them. Instead, we should add a clear title and description which describes what this parameter is and how different values will impact the data output by the connector.\nDocument how users can obtain configuration parameters\nIf a user needs to obtain an API key or host name, tell them exactly where to find it. Ideally you would show them screenshots, though include a date and API version in those if possible, so it\u2019s clear when they\u2019ve aged out of date.\nLinks should point to page anchors where applicable. \nOften, you are trying to redirect the user to a specific part of the page. For example, if you wanted to point someone to the \"Input Configuration\" section of this doc, it is better to point them to `https://docs.airbyte.com/connector-development/ux-handbook#input-configuration` instead of `https://docs.airbyte.com/connector-development/ux-handbook`.\nFail fast & actionably\nA user should not be able to configure something that will not work. If a user\u2019s configuration is invalid, we should inform them as precisely as possible about what they need to do to fix the issue.\nOne helpful aid is to use the json-schema \u201cpattern\u201d keyword to accept inputs which adhere to the correct input shape.\nOutput Data & Schemas\nStrongly Favor ELT over ETL\nExtract-Load-Transform (ELT) means extracting and loading the data into a destination while leaving its format/schema as unchanged as possible, and making transformation the responsibility of the consumer. By contrast, ETL means transforming data before it is sent to the destination, for example changing its schema to make it easier to consume in the destination.\nWhen extracting data, strongly prefer ELT to ETL for the following reasons:\nRemoves Airbyte as a development bottleneck\nIf we get into the habit of structuring the output of each source according to how some users want to use it, then we will invite more feature requests from users asking us to transform data in a particular way. This introduces Airbyte\u2019s dev team as an unnecessary bottleneck for these users.\nInstead, we should set the standard that a user should be responsible for transformations once they\u2019ve loaded data in a destination.\nWill always be backwards compatible\nAPIs already follow strong conventions to maintain backwards compatibility. By transforming data, we break this guarantee, which means we may break downstream flows for our users.\nFuture proof\nWe may have a vision of what a user needs today. But if our persona evolves next year, then we\u2019ll probably also need to adapt our transformation logic, which would require significant dev and data migration efforts.\nMore flexible\nCurrent users have different needs from data. By being opinionated on how they should consume data, we are effectively favoring one user persona over the other. While there might be some cases where this is warranted, it should be done with extreme intentionality.\nMore efficient\nWith ETL, if the \u201cT\u201d ever needs to change, then we need to re-extract all data for all users. This is computationally and financially expensive and will place a lot of pressure on the source systems as we re-extract all data.\nDescribe output schemas as completely and reliably as possible\nOur most popular destinations are strongly typed like Postgres, BigQuery, or Parquet & Avro.\nBeing strongly typed enables optimizations and syntactic sugar to make it very easy & performant for the user to query data.\nTo provide the best UX when moving data to these destinations, Airbyte source connectors should describe their schema in as much detail as correctness allows.\nIn some cases, describing schemas is impossible to do reliably. For example, MongoDB doesn\u2019t have any schemas. To infer the schema, one needs to read all the records in a particular table. And even then, once new records are added, they also must all be read in order to update the inferred schema. At the time of writing, this is infeasible to do performantly in Airbyte since we do not have an intermediate staging area to do this. In this case, we should do the \u201cbest we can\u201d to describe the schema, keeping in mind that reliability of the described schema is more important than expressiveness.\nThat is, we would rather not describe a schema at all than describe it incorrectly, as incorrect descriptions will lead to failures downstream.\nTo keep schema descriptions reliable, automate schema generation whenever possible.\nBe very cautious about breaking changes to output schemas\nAssuming we follow ELT over ETL, and automate generation of output schemas, this should come up very rarely. However, it is still important enough to warrant mention.\nIf for any reason we need to change the output schema declared by a connector in a backwards breaking way, consider it a necessary evil that should be avoided if possible. Basically, the only reasons for a backwards breaking change should be:\n\na connector previously had an incorrect schema, or\nIt was not following ELT principles and is now being changed to follow them\n\nOther breaking changes should probably be escalated for approval.\nPrerequisite Configurations & assumptions\nDocument all assumptions\nIf a connector makes assumptions about the underlying data source, then these assumptions must be documented. For example: for some features of the Facebook Pages connector to work, a user must have an Instagram Business account linked to an Instagram page linked to their Facebook Page. In this case, the externally facing documentation page for the connector must be very clear about this.\nProvide how-tos for prerequisite configuration\nIf a user needs to set up their data source in a particular way to pull data, then we must provide documentation for how they should do it.\nFor example, to set up CDC for databases, a user must create logical replication slots and do a few other things. These steps should be documented with examples or screenshots wherever possible (e.g: here are the SQL statements you need to run, with the following permissions, on the following screen, etc.)\nExternal Documentation\nThis section is concerned with the external-facing documentation of a connector that goes in https://docs.airbyte.io e.g: this one\nDocumentation should communicate persona-impacting behaviors\nWhen writing documentation ask: who is the intended target persona for a piece of documentation, and what information do they need to understand how this connector impacts their workflows?\nFor example, data analysts & analytics engineers probably don\u2019t care if we use Debezium for database replication. To them, the important thing is that we provide Change Data Capture (CDC) -- Debezium is an implementation detail. Therefore, when communicating information about our database replication logic, we should emphasize the end behaviors, rather than implementation details.\nExample: Don\u2019t say: \u201cDebezium cannot process UTF-16 character set\u201c.\nInstead, say: \u201cWhen using CDC, UTF-16 characters are not currently supported\u201d\nA user who doesn\u2019t already know what Debezium is might be left confused by the first phrasing, so we should use the second phrasing.",
    "tag": "airbyte"
  },
  {
    "title": "Connector Development Kit \\(C# .NET\\)",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/cdk-dotnet",
    "content": "Connector Development Kit (C# .NET)\nThe Airbyte Dotnet CDK is a framework for rapidly developing production-grade Airbyte connectors. The CDK currently offers helpers specific for creating Airbyte source connectors for:\n\nHTTP APIs (REST APIs, GraphQL, etc..)\nGeneric Dotnet sources (anything not covered by the above)\n\nThe CDK provides an improved developer experience by providing basic implementation structure and abstracting away low-level glue boilerplate.\nResources\nThis document is the main guide for developing an Airbyte source with the Dotnet CDK.",
    "tag": "airbyte"
  },
  {
    "title": "Acceptance Tests Reference",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/testing-connectors/connector-acceptance-tests-reference.md",
    "content": "Acceptance Tests Reference\nTo ensure a minimum quality bar, Airbyte runs all connectors against the same set of integration tests. Those tests ensure that each connector adheres to the Airbyte Specification and responds correctly to Airbyte commands when provided valid (or invalid) inputs.\nNote: If you are looking for reference documentation for the deprecated first version of test suites, see Standard Tests (Legacy).\nArchitecture of standard tests\nThe Standard Test Suite runs its tests against the connector's Docker image. It takes as input the configuration file `acceptance-tests-config.yml`.\n\nThe Standard Test Suite use pytest as a test runner and was built as pytest plugin `connector-acceptance-test`. This plugin adds a new configuration option `\u2014acceptance-test-config` - it should points to the folder with `acceptance-tests-config.yml`.\nEach test suite has a timeout and will fail if the limit is exceeded.\nSee all the test cases, their description, and inputs in Connector Acceptance Tests.\nSetting up standard acceptance tests for your connector\nCreate `acceptance-test-config.yml`. In most cases, your connector already has this file in its root folder. Here is an example of the minimal `acceptance-test-config.yml`:\n`yaml\nconnector_image: airbyte/source-some-connector:dev\nacceptance-tests:\n  spec:\n    tests:\n      - spec_path: \"some_folder/spec.yaml\"`\nNote: Not all types of tests work for all connectors, only configure the ones that make sense in your scenario. The `spec` and `check` test suites are universal for all sources and destinations, the other test suites are only applicable to sources, and the `incremental` test suite is only applicable if the source connector supports incremental syncs.\nBuild your connector image if needed.\n`text\ndocker build .`\nRun one of the two scripts in the root of the connector:\n\n\n`python -m pytest -p integration_tests.acceptance` - to run tests inside virtual environment\n\n\nOn test completion, a log will be outputted to the terminal verifying:\n\nThe connector the tests were ran for\nThe git hash of the code used\nWhether the tests passed or failed\n\nThis is useful to provide in your PR as evidence of the acceptance tests passing locally.\n\n\n`./acceptance-test-docker.sh` - to run tests from a docker container\n\n\nIf the test fails you will see detail about the test and where to find its inputs and outputs to reproduce it. You can also debug failed tests by adding `\u2014pdb \u2014last-failed`:\n`text\npython -m pytest -p integration_tests.acceptance --pdb --last-failed`\nSee other useful pytest options here\nDynamically managing inputs & resources used in standard tests\nSince the inputs to standard tests are often static, the file-based runner is sufficient for most connectors. However, in some cases, you may need to run pre or post hooks to dynamically create or destroy resources for use in standard tests. For example, if we need to spin up a Redshift cluster to use in the test then tear it down afterwards, we need the ability to run code before and after the tests, as well as customize the Redshift cluster URL we pass to the standard tests. If you have need for this use case, please reach out to us via Github or Slack. We currently support it for Java & Python, and other languages can be made available upon request.\nPython\nCreate pytest yield-fixture with your custom setup/teardown code and place it in `integration_tests/acceptance.py`, Example of fixture that starts a docker container before tests and stops before exit:\n`python\n@pytest.fixture(scope=\"session\", autouse=True)\ndef connector_setup():\n    \"\"\" This fixture is a placeholder for external resources that acceptance test might require.\n    \"\"\"\n    client = docker.from_env()\n    container = client.containers.run(\"your/docker-image\", detach=True)\n    yield\n    container.stop()`\nThese tests are configurable via `acceptance-test-config.yml`. Each test has a number of inputs, you can provide multiple sets of inputs which will cause the same to run multiple times - one for each set of inputs.\nExample of `acceptance-test-config.yml`:\n`yaml\nconnector_image: string # Docker image to test, for example 'airbyte/source-pokeapi:0.1.0'\nbase_path: string # Base path for all relative paths, optional, default - ./\ncustom_environment_variables:\n  foo: bar\nacceptance_tests: # Tests configuration\n  spec: # list of the test inputs\n    bypass_reason: \"Explain why you skipped this test\"\n  connection: # list of the test inputs\n    tests:\n      - config_path: string # set #1 of inputs\n        status: string\n      - config_path: string # set #2 of inputs\n        status: string\n    # discovery:  # skip this test\n  incremental:\n    bypass_reason: \"Incremental sync are not supported on this connector\"`\nTest Spec\nVerify that a `spec` operation issued to the connector returns a valid connector specification.\nAdditional tests are validating the backward compatibility of the current specification compared to the specification of the previous connector version. If no previous connector version is found (by default the test looks for a docker image with the same name but with the `latest` tag), this test is skipped.\nThese backward compatibility tests can be bypassed by changing the value of the `backward_compatibility_tests_config.disable_for_version` input in `acceptance-test-config.yml` (see below).\nOne more test validates the specification against containing exposed secrets. This means fields that potentially could hold a secret value should be explicitly marked with `\"airbyte_secret\": true`. If an input field like `api_key` / `password` / `client_secret` / etc. is exposed, the test will fail.\n| Input                                                            | Type   | Default             | Note                                                                                                                  |\n| :--------------------------------------------------------------- | :----- | :------------------ | :-------------------------------------------------------------------------------------------------------------------- |\n| `spec_path`                                                      | string | `secrets/spec.json` | Path to a YAML or JSON file representing the spec expected to be output by this connector                             |\n| `backward_compatibility_tests_config.previous_connector_version` | string | `latest`            | Previous connector version to use for backward compatibility tests (expects a version following semantic versioning). |\n| `backward_compatibility_tests_config.disable_for_version`        | string | None                | Disable the backward compatibility test for a specific version (expects a version following semantic versioning).     |\n| `timeout_seconds`                                                | int    | 10                  | Test execution timeout in seconds                                                                                     |\nTest Connection\nVerify that a check operation issued to the connector with the input config file returns a successful response.\n| Input             | Type                           | Default               | Note                                                               |\n| :---------------- | :----------------------------- | :-------------------- | :----------------------------------------------------------------- |\n| `config_path`     | string                         | `secrets/config.json` | Path to a JSON object representing a valid connector configuration |\n| `status`          | `succeed` `failed` `exception` |                       | Indicate if connection check should succeed with provided config   |\n| `timeout_seconds` | int                            | 30                    | Test execution timeout in seconds                                  |\nTest Discovery\nVerifies when a `discover` operation is run on the connector using the given config file, a valid catalog is produced by the connector.\nAdditional tests are validating the backward compatibility of the discovered catalog compared to the catalog of the previous connector version. If no previous connector version is found (by default the test looks for a docker image with the same name but with the `latest` tag), this test is skipped.\nThese backward compatibility tests can be bypassed by changing the value of the `backward_compatibility_tests_config.disable_for_version` input in `acceptance-test-config.yml` (see below).\n| Input                                                            | Type   | Default                                     | Note                                                                                                                  |\n| :--------------------------------------------------------------- | :----- | :------------------------------------------ | :-------------------------------------------------------------------------------------------------------------------- |\n| `config_path`                                                    | string | `secrets/config.json`                       | Path to a JSON object representing a valid connector configuration                                                    |\n| `configured_catalog_path`                                        | string | `integration_tests/configured_catalog.json` | Path to configured catalog                                                                                            |\n| `timeout_seconds`                                                | int    | 30                                          | Test execution timeout in seconds                                                                                     |\n| `backward_compatibility_tests_config.previous_connector_version` | string | `latest`                                    | Previous connector version to use for backward compatibility tests (expects a version following semantic versioning). |\n| `backward_compatibility_tests_config.disable_for_version`        | string | None                                        | Disable the backward compatibility test for a specific version (expects a version following semantic versioning).     |\nTest Basic Read\nConfiguring all streams in the input catalog to full refresh mode verifies that a read operation produces some RECORD messages. Each stream should have some data, if you can't guarantee this for particular streams - add them to the `empty_streams` list.\nSet `validate_data_points=True` if possible. This validation is going to be enabled by default and won't be configurable in future releases.\n| Input                             | Type             | Default                                     | Note                                                                                                          |\n| :-------------------------------- | :--------------- | :------------------------------------------ | :------------------------------------------------------------------------------------------------------------ |\n| `config_path`                     | string           | `secrets/config.json`                       | Path to a JSON object representing a valid connector configuration                                            |\n| `configured_catalog_path`         | string           | `integration_tests/configured_catalog.json` | Path to configured catalog                                                                                    |\n| `empty_streams`                   | array of objects | []                                        | List of streams that might be empty with a `bypass_reason`                                                    |\n| `empty_streams[0].name`           | string           |                                             | Name of the empty stream                                                                                      |\n| `empty_streams[0].bypass_reason`  | string           | None                                        | Reason why this stream is empty                                                                               |\n| `validate_schema`                 | boolean          | True                                        | Verify that structure and types of records matches the schema from discovery command                          |\n| `validate_data_points`            | boolean          | False                                       | Validate that all fields in all streams contained at least one data point                                     |\n| `timeout_seconds`                 | int              | 5*60                                       | Test execution timeout in seconds                                                                             |\n| `expect_trace_message_on_failure` | boolean          | True                                        | Ensure that a trace message is emitted when the connector crashes                                             |\n| `expect_records`                  | object           | None                                        | Compare produced records with expected records, see details below                                             |\n| `expect_records.path`             | string           |                                             | File with expected records                                                                                    |\n| `expect_records.bypass_reason`    | string           |                                             | Explain why this test is bypassed                                                                             |\n| `expect_records.extra_fields`     | boolean          | False                                       | Allow output records to have other fields i.e: expected records are a subset                                  |\n| `expect_records.exact_order`      | boolean          | False                                       | Ensure that records produced in exact same order                                                              |\n| `expect_records.extra_records`    | boolean          | True                                        | Allow connector to produce extra records, but still enforce all records from the expected file to be produced |\n`expect_records` is a nested configuration, if omitted - the part of the test responsible for record matching will be skipped. Due to the fact that we can't identify records without primary keys, only the following flag combinations are supported:\n| extra_fields | exact_order | extra_records |\n| :----------- | :---------- | :------------ |\n| x            | x           |               |\n|              | x           | x             |\n|              | x           |               |\n|              |             | x             |\n|              |             |               |\nSchema format checking\nIf some field has format attribute specified on its catalog json schema, Connector Acceptance Testing framework performs checking against format. It support checking of all builtin jsonschema formats for draft 7 specification: email, hostnames, ip addresses, time, date and date-time formats.\nNote: For date-time we are not checking against compliance against ISO8601 (and RFC3339 as subset of it). Since we are using specified format to set database column type on db normalization stage, value should be compliant to bigquery timestamp and SQL \"timestamp with timezone\" formats.\nExample of `expected_records.jsonl`:\nIn general, the expected_records.jsonl should contain the subset of output of the records of particular stream you need to test. The required fields are: `stream, data, emitted_at`\n`javascript\n{\"stream\": \"my_stream\", \"data\": {\"field_1\": \"value0\", \"field_2\": \"value0\", \"field_3\": null, \"field_4\": {\"is_true\": true}, \"field_5\": 123}, \"emitted_at\": 1626172757000}\n{\"stream\": \"my_stream\", \"data\": {\"field_1\": \"value1\", \"field_2\": \"value1\", \"field_3\": null, \"field_4\": {\"is_true\": false}, \"field_5\": 456}, \"emitted_at\": 1626172757000}\n{\"stream\": \"my_stream\", \"data\": {\"field_1\": \"value2\", \"field_2\": \"value2\", \"field_3\": null, \"field_4\": {\"is_true\": true}, \"field_5\": 678}, \"emitted_at\": 1626172757000}\n{\"stream\": \"my_stream\", \"data\": {\"field_1\": \"value3\", \"field_2\": \"value3\", \"field_3\": null, \"field_4\": {\"is_true\": false}, \"field_5\": 91011}, \"emitted_at\": 1626172757000}`\nTest Full Refresh sync\nTestSequentialReads\nThis test performs two read operations on all streams which support full refresh syncs. It then verifies that the RECORD messages output from both were identical or the former is a strict subset of the latter.\n| Input                     | Type   | Default                                     | Note                                                                   |\n| :------------------------ | :----- | :------------------------------------------ | :--------------------------------------------------------------------- |\n| `config_path`             | string | `secrets/config.json`                       | Path to a JSON object representing a valid connector configuration     |\n| `configured_catalog_path` | string | `integration_tests/configured_catalog.json` | Path to configured catalog                                             |\n| `timeout_seconds`         | int    | 20*60                                      | Test execution timeout in seconds                                      |\n| `ignored_fields`          | dict   | None                                        | For each stream, list of fields path ignoring in sequential reads test |\nTest Incremental sync\nTestTwoSequentialReads\nThis test verifies that all streams in the input catalog which support incremental sync can do so correctly. It does this by running two read operations: the first takes the configured catalog and config provided to this test as input. It then verifies that the sync produced a non-zero number of `RECORD` and `STATE` messages. The second read takes the same catalog and config used in the first test, plus the last `STATE` message output by the first read operation as the input state file. It verifies that either no records are produced (since we read all records in the first sync) or all records that produced have cursor value greater or equal to cursor value from `STATE` message. This test is performed only for streams that support incremental. Streams that do not support incremental sync are ignored. If no streams in the input catalog support incremental sync, this test is skipped.\n| Input                     | Type   | Default                                     | Note                                                                                                                                                                |\n| :------------------------ | :----- | :------------------------------------------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| `config_path`             | string | `secrets/config.json`                       | Path to a JSON object representing a valid connector configuration                                                                                                  |\n| `configured_catalog_path` | string | `integration_tests/configured_catalog.json` | Path to configured catalog                                                                                                                                          |\n| `cursor_paths`            | dict   | {}                                          | For each stream, the path of its cursor field in the output state messages. If omitted the path will be taken from the last piece of path from stream cursor_field. |\n| `timeout_seconds`         | int    | 20*60                                      | Test execution timeout in seconds                                                                                                                                   |\n| `threshold_days`          | int    | 0                                           | For date-based cursors, allow records to be emitted with a cursor value this number of days before the state value.                                                 |\nTestReadSequentialSlices\nThis test offers more comprehensive verification that all streams in the input catalog which support incremental syncs perform the sync correctly. It does so in two phases. The first phase uses the configured catalog and config provided to this test as input to make a request to the partner API and assemble the complete set of messages to be synced. It then verifies that the sync produced a non-zero number of `RECORD` and `STATE` messages. This set of messages is partitioned into batches of a `STATE` message followed by zero or more `RECORD` messages. For each batch of messages, the initial `STATE` message is used as input for a read operation to get records with respect to the cursor. The test then verifies that all of the `RECORDS` retrieved have a cursor value greater or equal to the cursor from the current `STATE` message. This test is performed only for streams that support incremental. Streams that do not support incremental sync are ignored. If no streams in the input catalog support incremental sync, this test is skipped.\n| Input                                  | Type   | Default                                     | Note                                                                                                                                                                |\n| :------------------------------------- | :----- | :------------------------------------------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| `config_path`                          | string | `secrets/config.json`                       | Path to a JSON object representing a valid connector configuration                                                                                                  |\n| `configured_catalog_path`              | string | `integration_tests/configured_catalog.json` | Path to configured catalog                                                                                                                                          |\n| `cursor_paths`                         | dict   | {}                                          | For each stream, the path of its cursor field in the output state messages. If omitted the path will be taken from the last piece of path from stream cursor_field. |\n| `timeout_seconds`                      | int    | 20*60                                      | Test execution timeout in seconds                                                                                                                                   |\n| `threshold_days`                       | int    | 0                                           | For date-based cursors, allow records to be emitted with a cursor value this number of days before the state value.                                                 |\n| `skip_comprehensive_incremental_tests` | bool   | false                                       | For non-GA and in-development connectors, control whether the more comprehensive incremental tests will be skipped                                                  |\nNote that this test samples a fraction of stream slices across an incremental sync in order to reduce test duration and avoid spamming partner APIs\nTestStateWithAbnormallyLargeValues\nThis test verifies that sync produces no records when run with the STATE with abnormally large values\n| Input                     | Type   | Default                                     | Note                                                               |     |\n| :------------------------ | :----- | :------------------------------------------ | :----------------------------------------------------------------- | :-- |\n| `config_path`             | string | `secrets/config.json`                       | Path to a JSON object representing a valid connector configuration |     |\n| `configured_catalog_path` | string | `integration_tests/configured_catalog.json` | Path to configured catalog                                         |     |\n| `future_state_path`       | string | None                                        | Path to the state file with abnormally large cursor values         |     |\n| `timeout_seconds`         | int    | 20*60                                      | Test execution timeout in seconds                                  |     |\n| `bypass_reason`           | string | None                                        | Explain why this test is bypassed                                  |     |\nStrictness level\nTo enforce maximal coverage of acceptances tests we expose a `test_strictness_level` field at the root of the `acceptance-test-config.yml` configuration.\nThe default `test_strictness_level` is `low`, but for generally available connectors it is expected to be eventually set to `high`.\nNote: For now, the strictness level can only be applied for sources, not for destination connectors\nTest enforcements in `high` test strictness level\nAll acceptance tests are declared, a `bypass_reason` is filled if a test can't run\nIn `high` test strictness level we expect all acceptance tests to be declared:\n\n`spec`\n`connection`\n`discovery`\n`basic_read`\n`full_refresh`\n`incremental`\n\nIf a test can't be run for a valid technical or organizational reason a `bypass_reason` can be declared to skip this test.\nE.G. `source-pokeapi` does not support incremental syncs, we can skip this test when `test_strictness_level` is `high` by setting a `bypass_reason` under `incremental`.\n`yaml\nconnector_image: \"airbyte/source-pokeapi\"\ntest_strictness_level: high\nacceptance_tests:\n  spec:\n    tests:\n      - spec_path: \"source_pokeapi/spec.json\"\n  connection:\n    tests:\n      - config_path: \"integration_tests/config.json\"\n        status: \"succeed\"\n  discovery:\n    tests:\n      - config_path: \"integration_tests/config.json\"\n  basic_read:\n    tests:\n      - config_path: \"integration_tests/config.json\"\n  full_refresh:\n    tests:\n      - config_path: \"integration_tests/config.json\"\n        configured_catalog_path: \"integration_tests/configured_catalog.json\"\n  incremental:\n    bypass_reason: \"Incremental syncs are not supported on this connector.\"`\nBasic read: no empty streams are allowed without a `bypass_reason`\nIn `high` test strictness level we expect that all streams declared in `empty-streams` to have a `bypass_reason` filled in.\nE.G. Two streams from `source-recharge` can't be seeded with test data, they are declared as `empty_stream` we an explicit bypass reason.\n`yaml\nconnector_image: airbyte/source-recharge:dev\ntest_strictness_level: high\nacceptance_tests:\n  basic_read:\n    tests:\n      - config_path: secrets/config.json\n        empty_streams:\n          - name: collections\n            bypass_reason: \"This stream can't be seeded in our sandbox account\"\n          - name: discounts\n            bypass_reason: \"This stream can't be seeded in our sandbox account\"\n        timeout_seconds: 1200`\nBasic read: `expect_records` must be set\nIn `high` test strictness level we expect the `expect_records` subtest to be set.\nIf you can't create an `expected_records.jsonl` with all the existing stream you need to declare the missing streams in the `empty_streams` section.\nIf you can't get an `expected_records.jsonl` file at all, you must fill in a `bypass_reason`.\nBasic read: no `configured_catalog_path` can be set\nIn `high` test strictness level we want to run the `basic_read` test on a configured catalog created from the discovered catalog from which we remove declared empty streams. Declaring `configured_catalog_path` in the test configuration is not allowed.\n`yaml\nconnector_image: airbyte/source-recharge:dev\ntest_strictness_level: high\nacceptance_tests:\n  basic_read:\n    tests:\n      - config_path: secrets/config.json\n        empty_streams:\n          - name: collections\n            bypass_reason: \"This stream can't be seeded in our sandbox account\"\n          - name: discounts\n            bypass_reason: \"This stream can't be seeded in our sandbox account\"\n        timeout_seconds: 1200`\nIncremental: `future_state` must be set\nIn `high` test strictness level we expect the `future_state` configuration to be set.\nThe future state JSON file (usually `abnormal_states.json`) must contain one state for each stream declared in the configured catalog.\n`missing_streams` can be set to ignore a subset of the streams with a valid bypass reason. E.G:\n`yaml\ntest_strictness_level: high\nconnector_image: airbyte/source-my-connector:dev\nacceptance_tests:\n  ...\n  incremental:\n    tests:\n      - config_path: secrets/config.json\n        configured_catalog_path: integration_tests/configured_catalog.json\n        cursor_paths:\n          ...\n        future_state:\n          future_state_path: integration_tests/abnormal_state.json\n          missing_streams:\n            - name: my_missing_stream\n              bypass_reason: \"Please fill a good reason\"`\nCaching\nWe cache discovered catalogs by default for performance and reuse the same discovered catalog through all tests.\nYou can disable this behavior by setting `cached_discovered_catalog: False` at the root of the configuration.\nAdditional Checks\nWhile not necessarily related to Connector Acceptance Testing, Airbyte employs a number of additional checks which run on connector Pull Requests which check the following items:\nStrictness Level\nGenerally Available Connectors must enable high-strictness testing for the Connector Acceptance Test suite. This ensures that these connectors have implemented the most robust collection of tests.\nAllowed Hosts\nGA and Beta connectors are required to provide an entry for Allowed Hosts in the Actor Definition for the connector. Actor Definitions are stored in either source_definitions.yaml or destination_definitions.yaml in the codebase. You can provide:\nA list of static hostnames or IP addresses. Wildcards are valid.\n`yaml\nallowedHosts:\n  hosts:\n    - \"api.github.com\"\n    - \"*.hubspot.com\"`\nA list of dynamic hostnames or IP addresses which reference values from the connector's configuration. The variable names need to match the connector's config exactly. In this example, `subdomain` is a required option defined by the connector's SPEC response. It is also possible to refrence sub-fields with dot-notation, e.g. `networking_options.tunnel_host`.\n`yaml\nallowedHosts:\n  hosts:\n    - \"${subdomain}.vendor.com\"\n    - \"${networking_options.tunnel_host}\"`\nor prevent network access for this connector entirely\n`yaml\nallowedHosts:\n  hosts: []`\nCustom environment variable\nThe connector under tests can be run with custom environment variables:\n```yaml\nconnector_image: \"airbyte/source-pokeapi\"\ncustom_environment_variables:\n  my_custom_environment_variable: value\n...",
    "tag": "airbyte"
  },
  {
    "title": "Testing The Local Catalog",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/testing-connectors/testing-a-local-catalog-in-development.md",
    "content": "Testing The Local Catalog\nPurpose of this document\nThis document describes how to\n1. Generate the OSS catalog in the `airbyte` repo\n1. Move the OSS catalog to the `airbyte-platform` repo\n1. Point the platform to the new catalog\n1. Run the platform\nWhy you might need to\n\nYou've added/updated/deleted a generally available connector and want to test it in the platform UI\nYou've added/updated/deleted a generally available connector and want to test it in the platform API\n\nSteps\n1. Generate the OSS Catalog\nIn the `airbyte` repo run\n`sh\nSUB_BUILD=ALL_CONNECTORS ./gradlew :airbyte-config:specs:generateOssConnectorCatalog`\n2. Move the OSS catalog to platform\nIn the `airbyte` repo run\n`sh\ncp airbyte-config/init/src/main/resources/seed/oss_catalog.json <PATH_TO_PLATFORM_REPO>/airbyte-config/init/src/main/resources/seed/local_dev_catalog.json`\n3. Point the platform to the new catalog\nIn the `airbyte-platform` repo modify `.env` add the environment variable\n`sh\nLOCAL_CONNECTOR_CATALOG_PATH=seed/local_dev_catalog.json`\n4. Build the platform\nIn the `airbyte-platform` run\n`sh\nSUB_BUILD=PLATFORM ./gradlew build -x test`\n4. Run platform\nIn the `airbyte-platform` run\n`sh\nVERSION=dev docker-compose up`",
    "tag": "airbyte"
  },
  {
    "title": "Testing Connectors",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/testing-connectors",
    "content": "Testing Connectors\nRunning Integration tests\nThe GitHub `master` and branch builds will build the core Airbyte infrastructure (scheduler, ui, etc) as well as the images for all connectors. Integration tests (tests that run a connector's image against an external resource) can be run one of three ways.\n1. Local iteration\nFirst, you can run the image locally. Connectors should have instructions in the connector's README on how to create or pull credentials necessary for the test. Also, during local development, there is usually a `main` entrypoint for Java integrations or `main_dev.py` for Python integrations that let you run your connector without containerization, which is fastest for iteration.\n2. Code Static Checkers\nPython Code\nUsing the following tools:\n1. flake8\n2. black\n3. isort\n4. mypy\nAirbyte CI/CD workflows use them during \"test/publish\" commands obligatorily. \nAll their settings are aggregated into the single file `pyproject.toml` into Airbyte project root.\nLocally all these tools can be launched by the following gradle command:\n`./gradlew --no-daemon  :airbyte-integrations:connectors:<connector_name>:airbytePythonFormat`\nFor instance:\n`./gradlew --no-daemon  :airbyte-integrations:connectors:source-s3:airbytePythonFormat\n./gradlew --no-daemon  :airbyte-integrations:connectors:source-salesforce:airbytePythonFormat`\n3. Requesting GitHub PR Integration Test Runs\n:::caution\nThis option is not available to PRs from forks, so it is effectively limited to Airbyte employees.\n:::\nIf you don't want to handle secrets, you're making a relatively minor change, or you want to ensure the connector's integration test will run remotely, you should request builds on GitHub. You can request an integration test run by creating a comment with a slash command.\nHere are some example commands:\n\n`/test connector=all` - Runs integration tests for all connectors in a single GitHub workflow. Some of our integration tests interact with rate-limited resources, so please use this judiciously.\n`/test connector=source-sendgrid` - Runs integration tests for a single connector on the latest PR commit.\n`/test connector=connectors/source-sendgrid` - Runs integration tests for a single connector on the latest PR commit.\n`/test connector=source-sendgrid ref=master` - Runs integration tests for a single connector on a different branch.\n`/test connector=source-sendgrid ref=d5c53102` - Runs integration tests for a single connector on a specific commit.\n\nA command dispatcher GitHub workflow will launch on comment submission. This dispatcher will add an :eyes: reaction to the comment when it starts processing. If there is an error dispatching your request, an error will be appended to your comment. If it launches the test run successfully, a :rocket: reaction will appear on your comment.\nOnce the integration test workflow launches, it will append a link to the workflow at the end of the comment. A success or failure response will also be added upon workflow completion.\nIntegration tests can also be manually requested by clicking \"Run workflow\" and specifying the connector and GitHub ref.\n4. Requesting GitHub PR publishing Docker Images\nIn order for users to reference the new versions of a connector, it needs to be published and available in the dockerhub with the latest tag updated.\nAs seen previously, GitHub workflow can be triggered by comment submission. Publishing docker images to the dockerhub repository can also be submitted likewise:\nNote that integration tests can be triggered with a slightly different syntax for arguments. This second set is required to distinguish between `connectors` and `bases` folders. Thus, it is also easier to switch between the `/test` and `/publish` commands:\n\n`/test connector=connectors/source-sendgrid` - Runs integration tests for a single connector on the latest PR commit.\n`/publish connector=connectors/source-sendgrid` - Publish the docker image if it doesn't exist for a single connector on the latest PR commit.\n\n5. Automatically Run From `master`\nCommits to `master` attempt to launch integration tests. Two workflows launch for each commit: one is a launcher for integration tests, the other is the core build (the same as the default for PR and branch builds).\nSince some of our connectors use rate-limited external resources, we don't want to overload from multiple commits to master. If a certain threshold of `master` integration tests are running, the integration test launcher passes but does not launch any tests. This can manually be re-run if necessary. The `master` build also runs every few hours automatically, and will launch the integration tests at that time.",
    "tag": "airbyte"
  },
  {
    "title": "Standard Tests (Legacy)",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/testing-connectors/legacy-standard-source-tests.md",
    "content": "Standard Tests (Legacy)\nAirbyte's Standard Tests (v1)\nThis document describes the old version Standard Tests, please check the latest version here\nTo ensure a minimum quality bar, Airbyte runs all connectors against the same set of integration tests (sources & destinations have two different test suites). Those tests ensure that each connector adheres to the Airbyte Specification and responds correctly to Airbyte commands when provided valid (or invalid) inputs.\nArchitecture of standard tests\n\nThe Standard Test Suite runs its tests against the connector's Docker image. It takes as input the following:\n\nThe Connector's Docker image name, so it can run tests against that image\nA config file containing necessary credentials/information to connect to the underlying data source/destination\nA configured catalog that will be used when running read/write operations against the connector\n(Optional) A state file for use in incremental sync test scenarios\n\nThe test suite then runs its test cases, which include:\n\nUsing the input config file, running the `check` operation should succeed. \nUsing a made up/fake config file, running the `check` operation should fail. \nRunning a `read` operation should produce at least one record. \nRunning two consecutive full refresh reads should produce identical records. \n\nSee all the test cases and their description in Standard Source Tests.\nSetting up standard tests for your connector\nStandard tests are typically run from a docker container. The default standard test runner is the File-based Standard Test suite, which gets its name because its inputs are passed as files via Docker volume mounts. This is the simplest way to run the standard test suite: the only requirements are that you place its input files inside your connector's directory, and to pass the paths to those input files as arguments to the Gradle plugin required to invoke it. This is setup by default inside the `build.gradle` file on all connectors generated from templates.\nFor reference, to configure the file-based standard test suite the only requirement is to add the following block in your connectors `build.gradle` file:\n`text\napply plugin: 'airbyte-standard-source-test-file'\nairbyteStandardSourceTestFile { \n  // all these paths must be inside your connector's directory\n  configPath = \"/path/to/config\"\n  specPath = \"/path/to/spec\"\n  configuredCatalogPath = \"/path/to/catalog\"\n}`\nThese inputs are all described in the Airbyte Specification and will be used as follows:\n\nSpec file will be compared to the spec file output by the connector when the `spec` command is called. \nConfig file is expected to be a valid config file. It's expected that calling `check` with this config will succeed. \nConfigured Catalog read operations will be performed on all the streams found in this catalog. All sync modes supported for each stream will be tested. If any stream requires a user-defined cursor, this should be defined in this catalog file. (If this sounds like gibberish, make sure to read about incremental sync). \n\nDynamically managing inputs & resources used in standard tests\nSince the inputs to standard tests are often static, the file-based runner is sufficient for most connectors. However, in some cases, you may need to run pre or post hooks to dynamically create or destroy resources for use in standard tests. For example, if we need to spin up a Redshift cluster to use in the test then tear it down afterwards, we need the ability to run code before and after the tests, as well as customize the Redshift cluster URL we pass to the standard tests. If you have need for this use case, please reach out to us via Github or Slack. We currently support it for Java & Python, and other languages can be made available upon request.\nRunning Integration tests\nThe GitHub `master` and branch builds will build the core Airbyte infrastructure (scheduler, ui, etc) as well as the images for all connectors. Integration tests (tests that run a connector's image against an external resource) can be run one of three ways.\n1. Local iteration\nFirst, you can run the image locally. Connectors should have instructions in the connector's README on how to create or pull credentials necessary for the test. Also, during local development, there is usually a `main` entrypoint for Java integrations or `main_dev.py` for Python integrations that let you run your connector without containerization, which is fastest for iteration.\n2. Requesting GitHub PR Integration Test Runs\n:::caution\nThis option is not available to PRs from forks, so it is effectively limited to Airbyte employees.\n:::\nIf you don't want to handle secrets, you're making a relatively minor change, or you want to ensure the connector's integration test will run remotely, you should request builds on GitHub. You can request an integration test run by creating a comment with a slash command.\nHere are some example commands:\n\n`/test connector=all` - Runs integration tests for all connectors in a single GitHub workflow. Some of our integration tests interact with rate-limited resources, so please use this judiciously.\n`/test connector=source-sendgrid` - Runs integration tests for a single connector on the latest PR commit.\n`/test connector=connectors/source-sendgrid` - Runs integration tests for a single connector on the latest PR commit.\n`/test connector=source-sendgrid ref=master` - Runs integration tests for a single connector on a different branch. \n`/test connector=source-sendgrid ref=d5c53102` - Runs integration tests for a single connector on a specific commit.\n\nA command dispatcher GitHub workflow will launch on comment submission. This dispatcher will add an :eyes: reaction to the comment when it starts processing. If there is an error dispatching your request, an error will be appended to your comment. If it launches the test run successfully, a :rocket: reaction will appear on your comment.\nOnce the integration test workflow launches, it will append a link to the workflow at the end of the comment. A success or failure response will also be added upon workflow completion.\nIntegration tests can also be manually requested by clicking \"Run workflow\" and specifying the connector and GitHub ref.\n3. Requesting GitHub PR publishing Docker Images\nIn order for users to reference the new versions of a connector, it needs to be published and available in the dockerhub with the latest tag updated.\nAs seen previously, GitHub workflow can be triggered by comment submission. Publishing docker images to the dockerhub repository can also be submitted likewise:\nNote that integration tests can be triggered with a slightly different syntax for arguments. This second set is required to distinguish between `connectors` and `bases` folders. Thus, it is also easier to switch between the `/test` and `/publish` commands:\n\n`/test connector=connectors/source-sendgrid` - Runs integration tests for a single connector on the latest PR commit.\n`/publish connector=connectors/source-sendgrid` - Publish the docker image if it doesn't exist for a single connector on the latest PR commit.\n\n4. Automatically Run From `master`\nCommits to `master` attempt to launch integration tests. Two workflows launch for each commit: one is a launcher for integration tests, the other is the core build (the same as the default for PR and branch builds).",
    "tag": "airbyte"
  },
  {
    "title": "Building a Java Destination",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/tutorials/building-a-java-destination.md",
    "content": "Building a Java Destination\nSummary\nThis article provides a checklist for how to create a Java destination. Each step in the checklist has a link to a more detailed explanation below.\nRequirements\nDocker and Java with the versions listed in the tech stack section.\nChecklist\nCreating a destination\n\nStep 1: Create the destination using the template generator\nStep 2: Build the newly generated destination \nStep 3: Implement `spec` to define the configuration required to run the connector\nStep 4: Implement `check` to provide a way to validate configurations provided to the connector\nStep 5: Implement `write` to write data to the destination\nStep 6: Set up Acceptance Tests\nStep 7: Write unit tests or integration tests\nStep 8: Update the docs (in `docs/integrations/destinations/<destination-name>.md`)\n\n:::info\nAll `./gradlew` commands must be run from the root of the airbyte project.\n:::\n:::info\nIf you need help with any step of the process, feel free to submit a PR with your progress and any questions you have, or ask us on slack.\n:::\nExplaining Each Step\nStep 1: Create the destination using the template\nAirbyte provides a code generator which bootstraps the scaffolding for our connector.\n`bash\n$ cd airbyte-integrations/connector-templates/generator # assumes you are starting from the root of the Airbyte project.\n$ ./generate.sh`\nSelect the `Java Destination` template and then input the name of your connector. We'll refer to the destination as `<name>-destination` in this tutorial, but you should replace `<name>` with the actual name you used for your connector e.g: `BigQueryDestination` or `bigquery-destination`.\nStep 2: Build the newly generated destination\nYou can build the destination by running:\n```bash\nMust be run from the Airbyte project root\n./gradlew :airbyte-integrations:connectors:destination-:build\n```\nThis compiles the Java code for your destination and builds a Docker image with the connector. At this point, we haven't implemented anything of value yet, but once we do, you'll use this command to compile your code and Docker image.\n:::info\nAirbyte uses Gradle to manage Java dependencies. To add dependencies for your connector, manage them in the `build.gradle` file inside your connector's directory.\n:::\nIterating on your implementation\nWe recommend the following ways of iterating on your connector as you're making changes:\n\nTest-driven development (TDD) in Java \nTest-driven development (TDD) using Airbyte's Acceptance Tests\nDirectly running the docker image \n\nTest-driven development in Java\nThis should feel like a standard flow for a Java developer: you make some code changes then run java tests against them. You can do this directly in your IDE, but you can also run all unit tests via Gradle by running the command to build the connector:\n`text\n./gradlew :airbyte-integrations:connectors:destination-<name>:build`\nThis will build the code and run any unit tests. This approach is great when you are testing local behaviors and writing unit tests.\nTDD using acceptance tests & integration tests\nAirbyte provides a standard test suite (dubbed \"Acceptance Tests\") that runs against every destination connector. They are \"free\" baseline tests to ensure the basic functionality of the destination. When developing a connector, you can simply run the tests between each change and use the feedback to guide your development.\nIf you want to try out this approach, check out Step 6 which describes what you need to do to set up the acceptance Tests for your destination.\nThe nice thing about this approach is that you are running your destination exactly as Airbyte will run it in the CI. The downside is that the tests do not run very quickly. As such, we recommend this iteration approach only once you've implemented most of your connector and are in the finishing stages of implementation. Note that Acceptance Tests are required for every connector supported by Airbyte, so you should make sure to run them a couple of times while iterating to make sure your connector is compatible with Airbyte.\nDirectly running the destination using Docker\nIf you want to run your destination exactly as it will be run by Airbyte (i.e. within a docker container), you can use the following commands from the connector module directory (`airbyte-integrations/connectors/destination-<name>`):\n```text\nFirst build the container\n./gradlew :airbyte-integrations:connectors:destination-:build\nThen use the following commands to run it\nRuns the \"spec\" command, used to find out what configurations are needed to run a connector\ndocker run --rm airbyte/destination-:dev spec\nRuns the \"check\" command, used to validate if the input configurations are valid\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-:dev check --config /secrets/config.json\nRuns the \"write\" command which reads records from stdin and writes them to the underlying destination\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/sample_files:/sample_files airbyte/destination-:dev write --config /secrets/config.json --catalog /sample_files/configured_catalog.json\n```\nNote: Each time you make a change to your implementation you need to re-build the connector image via `./gradlew :airbyte-integrations:connectors:destination-<name>:build`.\nThe nice thing about this approach is that you are running your destination exactly as it will be run by Airbyte. The tradeoff is that iteration is slightly slower, because you need to re-build the connector between each change.\nHandling Exceptions\nIn order to best propagate user-friendly error messages and log error information to the platform, the Airbyte Protocol implements AirbyteTraceMessage.\nWe recommend using AirbyteTraceMessages for known errors, as in these cases you can likely offer the user a helpful message as to what went wrong and suggest how they can resolve it.\nAirbyte provides a static utility class, `io.airbyte.integrations.base.AirbyteTraceMessageUtility`, to give you a clear and straight-forward way to emit these AirbyteTraceMessages. Example usage:\n`java\ntry {\n  // some connector code responsible for doing X\n} \ncatch (ExceptionIndicatingIncorrectCredentials credErr) {\n  AirbyteTraceMessageUtility.emitConfigErrorTrace(\n    credErr, \"Connector failed due to incorrect credentials while doing X. Please check your connection is using valid credentials.\")\n  throw credErr\n} \ncatch (ExceptionIndicatingKnownErrorY knownErr) {\n  AirbyteTraceMessageUtility.emitSystemErrorTrace(\n    knownErr, \"Connector failed because of reason Y while doing X. Please check/do/make ... to resolve this.\")\n  throw knownErr\n} \ncatch (Exception e) {\n  AirbyteTraceMessageUtility.emitSystemErrorTrace(\n    e, \"Connector failed while doing X. Possible reasons for this could be ...\")\n  throw e \n}`\nNote the two different error trace methods.\n- Where possible `emitConfigErrorTrace` should be used when we are certain the issue arises from a problem with the user's input configuration, e.g. invalid credentials.\n- For everything else or if unsure, use `emitSystemErrorTrace`.\nStep 3: Implement `spec`\nEach destination contains a specification written in JsonSchema that describes its inputs. Defining the specification is a good place to start when developing your destination. Check out the documentation here to learn the syntax. Here's an example of what the `spec.json` looks like for the postgres destination.\nYour generated template should have the spec file in `airbyte-integrations/connectors/destination-<name>/src/main/resources/spec.json`. The generated connector will take care of reading this file and converting it to the correct output. Edit it and you should be done with this step.\nFor more details on what the spec is, you can read about the Airbyte Protocol here.\nSee the `spec` operation in action:\n```bash\nFirst build the connector\n./gradlew :airbyte-integrations:connectors:destination-:build\nRun the spec operation\ndocker run --rm airbyte/destination-:dev spec\n```\nStep 4: Implement `check`\nThe check operation accepts a JSON object conforming to the `spec.json`. In other words if the `spec.json` said that the destination requires a `username` and `password` the config object might be `{ \"username\": \"airbyte\", \"password\": \"password123\" }`. It returns a json object that reports, given the credentials in the config, whether we were able to connect to the destination.\nWhile developing, we recommend storing any credentials in `secrets/config.json`. Any `secrets` directory in the Airbyte repo is gitignored by default.\nImplement the `check` method in the generated file `<Name>Destination.java`. Here's an example implementation from the BigQuery destination.\nVerify that the method is working by placing your config in `secrets/config.json` then running:\n```text\nFirst build the connector\n./gradlew :airbyte-integrations:connectors:destination-:build\nRun the check method\ndocker run -v $(pwd)/secrets:/secrets --rm airbyte/destination-:dev check --config /secrets/config.json\n```\nStep 5: Implement `write`\nThe `write` operation is the main workhorse of a destination connector: it reads input data from the source and writes it to the underlying destination. It takes as input the config file used to run the connector as well as the configured catalog: the file used to describe the schema of the incoming data and how it should be written to the destination. Its \"output\" is two things:\n\nData written to the underlying destination\n`AirbyteMessage`s of type `AirbyteStateMessage`, written to stdout to indicate which records have been written so far during a sync. It's important to output these messages when possible in order to avoid re-extracting messages from the source. See the write operation protocol reference for more information.\n\nTo implement the `write` Airbyte operation, implement the `getConsumer` method in your generated `<Name>Destination.java` file. Here are some example implementations from different destination conectors:\n\nBigQuery\nGoogle Pubsub \nLocal CSV\nPostgres\n\n:::info\nThe Postgres destination leverages the `AbstractJdbcDestination` superclass which makes it extremely easy to create a destination for a database or data warehouse if it has a compatible JDBC driver. If the destination you are implementing has a JDBC driver, be sure to check out `AbstractJdbcDestination`.\n:::\nFor a brief overview on the Airbyte catalog check out the Beginner's Guide to the Airbyte Catalog.\nStep 6: Set up Acceptance Tests\nThe Acceptance Tests are a set of tests that run against all destinations. These tests are run in the Airbyte CI to prevent regressions and verify a baseline of functionality. The test cases are contained and documented in the following file.\nTo setup acceptance Tests for your connector, follow the `TODO`s in the generated file `<name>DestinationAcceptanceTest.java`. Once setup, you can run the tests using `./gradlew :airbyte-integrations:connectors:destination-<name>:integrationTest`. Make sure to run this command from the Airbyte repository root.\nStep 7: Write unit tests and/or integration tests\nThe Acceptance Tests are meant to cover the basic functionality of a destination. Think of it as the bare minimum required for us to add a destination to Airbyte. You should probably add some unit testing or custom integration testing in case you need to test additional functionality of your destination.\nStep 8: Update the docs\nEach connector has its own documentation page. By convention, that page should have the following path: in `docs/integrations/destinations/<destination-name>.md`. For the documentation to get packaged with the docs, make sure to add a link to it in `docs/SUMMARY.md`. You can pattern match doing that from existing connectors.\nWrapping up\nWell done on making it this far! If you'd like your connector to ship with Airbyte by default, create a PR against the Airbyte repo and we'll work with you to get it across the finish line.",
    "tag": "airbyte"
  },
  {
    "title": "Building a Python Destination",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/tutorials/building-a-python-destination.md",
    "content": "Building a Python Destination\nSummary\nThis article provides a checklist for how to create a Python destination. Each step in the checklist has a link to a more detailed explanation below.\nRequirements\nDocker and Python with the versions listed in the tech stack section. You can use any Python version between 3.7 and 3.9, but this tutorial was tested with 3.7.\nChecklist\nCreating a destination\n\nStep 1: Create the destination using the template generator\nStep 2: Setup the virtual environment \nStep 3: Implement `spec` to define the configuration required to run the connector\nStep 4: Implement `check` to provide a way to validate configurations provided to the connector\nStep 5: Implement `write` to write data to the destination\nStep 6: Set up Acceptance Tests\nStep 7: Write unit tests or integration tests\nStep 8: Update the docs (in `docs/integrations/destinations/<destination-name>.md`)\n\n:::info\nIf you need help with any step of the process, feel free to submit a PR with your progress and any questions you have, or ask us on slack. Also reference the KvDB python destination implementation if you want to see an example of a working destination.\n:::\nExplaining Each Step\nStep 1: Create the destination using the template\nAirbyte provides a code generator which bootstraps the scaffolding for our connector.\n`bash\n$ cd airbyte-integrations/connector-templates/generator # assumes you are starting from the root of the Airbyte project.\n$ ./generate.sh`\nSelect the `Python Destination` template and then input the name of your connector. We'll refer to the destination as `destination-<name>` in this tutorial, but you should replace `<name>` with the actual name you used for your connector e.g: `redis` or `google-sheets`.\nStep 2: Setup the dev environment\nSetup your Python virtual environment:\n```bash\ncd airbyte-integrations/connectors/destination-\nCreate a virtual environment in the .venv directory\npython -m venv .venv \nactivate the virtualenv\nsource .venv/bin/activate \nInstall with the \"tests\" extra which provides test requirements\npip install '.[tests]'\n```\nThis step sets up the initial python environment. All subsequent `python` or `pip` commands assume you have activated your virtual environment.\nIf you want your IDE to auto complete and resolve dependencies properly, point it at the python binary in `airbyte-integrations/connectors/destination-<name>/.venv/bin/python`. Also anytime you change the dependencies in the `setup.py` make sure to re-run the build command. The build system will handle installing all dependencies in the `setup.py` into the virtual environment.\nLet's quickly get a few housekeeping items out of the way.\nDependencies\nPython dependencies for your destination should be declared in `airbyte-integrations/connectors/destination-<name>/setup.py` in the `install_requires` field. You might notice that a couple of Airbyte dependencies are already declared there (mainly the Airbyte CDK and potentially some testing libraries or helpers). Keep those as they will be useful during development.\nYou may notice that there is a `requirements.txt` in your destination's directory as well. Do not touch this. It is autogenerated and used to install local Airbyte dependencies which are not published to PyPI. All your dependencies should be declared in `setup.py`.\nIterating on your implementation\nPretty much all it takes to create a destination is to implement the `Destination` interface. Let's briefly recap the three methods implemented by a Destination:\n\n`spec`: declares the user-provided credentials or configuration needed to run the connector\n`check`: tests if the user-provided configuration can be used to connect to the underlying data destination, and with the correct write permissions\n`write`: writes data to the underlying destination by reading a configuration, a stream of records from stdin, and a configured catalog describing the schema of the data and how it should be written to the destination\n\nThe destination interface is described in detail in the Airbyte Specification reference.\nThe generated files fill in a lot of information for you and have docstrings describing what you need to do to implement each method. The next few steps are just implementing that interface.\n:::info\nAll logging should be done through the `self.logger` object available in the `Destination` class. Otherwise, logs will not be shown properly in the Airbyte UI.\n:::\nEveryone develops differently but here are 3 ways that we recommend iterating on a destination. Consider using whichever one matches your style.\nRun the destination using Python\nYou'll notice in your destination's directory that there is a python file called `main.py`. This file is the entrypoint for the connector:\n```bash\nfrom airbyte-integrations/connectors/destination-\npython main.py spec\npython main.py check --config secrets/config.json\nmessages.jsonl should contain AirbyteMessages (described in the Airbyte spec)\ncat messages.jsonl | python main.py write --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\nThe nice thing about this approach is that you can iterate completely within in python. The downside is that you are not quite running your destination as it will actually be run by Airbyte. Specifically you're not running it from within the docker container that will house it.\nRun using Docker If you want to run your destination exactly as it will be run by Airbyte (i.e. within a docker container), you can use the following commands from the connector module directory (`airbyte-integrations/connectors/destination-<name>`):\n```bash\nFirst build the container\ndocker build . -t airbyte/destination-:dev\nThen use the following commands to run it\ndocker run --rm airbyte/destination-:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/destination-:dev check --config /secrets/config.json\ncat messages.jsonl | docker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/sample_files:/sample_files airbyte/destination-:dev read --config /secrets/config.json --catalog /sample_files/configured_catalog.json\n```\nNote: Each time you make a change to your implementation you need to re-build the connector image. `docker build . -t airbyte/destination-<name>:dev`. This ensures the new python code is added into the docker container.\nThe nice thing about this approach is that you are running your source exactly as it will be run by Airbyte. The tradeoff is that iteration is slightly slower, because you need to re-build the connector between each change.\nTDD using standard tests\nnote: these tests aren't yet available for Python connectors but will be very soon. Until then you should use custom unit or integration tests for TDD.\nAirbyte provides a standard test suite that is run against every destination. The objective of these tests is to provide some \"free\" tests that can sanity check that the basic functionality of the destination works. One approach to developing your connector is to simply run the tests between each change and use the feedback from them to guide your development.\nIf you want to try out this approach, check out Step 6 which describes what you need to do to set up the standard tests for your destination.\nThe nice thing about this approach is that you are running your destination exactly as Airbyte will run it in the CI. The downside is that the tests do not run very quickly.\nStep 3: Implement `spec`\nEach destination contains a specification written in JsonSchema that describes the inputs it requires and accepts. Defining the specification is a good place to start development. To do this, find the spec file generated in `airbyte-integrations/connectors/destination-<name>/src/main/resources/spec.json`. Edit it and you should be done with this step. The generated connector will take care of reading this file and converting it to the correct output.\nSome notes about fields in the output spec:\n\n`supportsNormalization` is a boolean which indicates if this connector supports basic normalization via DBT. If true, `supportsDBT` must also be true. \n`supportsDBT` is a boolean which indicates whether this destination is compatible with DBT. If set to true, the user can define custom DBT transformations that run on this destination after each successful sync. This must be true if `supportsNormalization` is set to true.\n`supported_destination_sync_modes`: An array of strings declaring the sync modes supported by this connector. The available options are: \n`overwrite`: The connector can be configured to wipe any existing data in a stream before writing new data\n`append`: The connector can be configured to append new data to existing data \n`append_dedup`: The connector can be configured to deduplicate (i.e: UPSERT) data in the destination based on the new data and primary keys\n`supportsIncremental`: Whether the connector supports any `append` sync mode. Must be set to true if `append` or `append_dedupe` are included in the `supported_destination_sync_modes`.  \n\nSome helpful resources:\n\nJSONSchema website \nDefinition of Airbyte Protocol data models. The output of `spec` is described by the `ConnectorSpecification` model (which is wrapped in an `AirbyteConnectionStatus` message). \nPostgres Destination's spec.json file as an example `spec.json`.\n\nOnce you've edited the file, see the `spec` operation in action:\n`bash\npython main.py spec`\nStep 4: Implement `check`\nThe check operation accepts a JSON object conforming to the `spec.json`. In other words if the `spec.json` said that the destination requires a `username` and `password`, the config object might be `{ \"username\": \"airbyte\", \"password\": \"password123\" }`. It returns a json object that reports, given the credentials in the config, whether we were able to connect to the destination.\nWhile developing, we recommend storing any credentials in `secrets/config.json`. Any `secrets` directory in the Airbyte repo is gitignored by default.\nImplement the `check` method in the generated file `destination_<name>/destination.py`. Here's an example implementation from the KvDB destination.\nVerify that the method is working by placing your config in `secrets/config.json` then running:\n`bash\npython main.py check --config secrets/config.json`\nStep 5: Implement `write`\nThe `write` operation is the main workhorse of a destination connector: it reads input data from the source and writes it to the underlying destination. It takes as input the config file used to run the connector as well as the configured catalog: the file used to describe the schema of the incoming data and how it should be written to the destination. Its \"output\" is two things:\n\nData written to the underlying destination\n`AirbyteMessage`s of type `AirbyteStateMessage`, written to stdout to indicate which records have been written so far during a sync. It's important to output these messages when possible in order to avoid re-extracting messages from the source. See the write operation protocol reference for more information.\n\nTo implement the `write` Airbyte operation, implement the `write` method in your generated `destination.py` file. Here is an example implementation from the KvDB destination connector.\nStep 6: Set up Acceptance Tests\nComing soon. These tests are not yet available for Python destinations but will be very soon. For now please skip this step and rely on copious amounts of integration and unit testing.\nStep 7: Write unit tests and/or integration tests\nThe Acceptance Tests are meant to cover the basic functionality of a destination. Think of it as the bare minimum required for us to add a destination to Airbyte. You should probably add some unit testing or custom integration testing in case you need to test additional functionality of your destination.\nAdd unit tests in `unit_tests/` directory and integration tests in the `integration_tests/` directory. Run them via\n`bash\npython -m pytest -s -vv integration_tests/`\nSee the KvDB integration tests for an example of tests you can implement.\nStep 8: Update the docs\nEach connector has its own documentation page. By convention, that page should have the following path: in `docs/integrations/destinations/<destination-name>.md`. For the documentation to get packaged with the docs, make sure to add a link to it in `docs/SUMMARY.md`. You can pattern match doing that from existing connectors.\nWrapping up\nWell done on making it this far! If you'd like your connector to ship with Airbyte by default, create a PR against the Airbyte repo and we'll work with you to get it across the finish line.",
    "tag": "airbyte"
  },
  {
    "title": "Python CDK Speedrun: Creating a Source",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/tutorials/cdk-speedrun.md",
    "content": "Python CDK Speedrun: Creating a Source\nCDK Speedrun (HTTP API Source Creation Any Route)\nThis is a blazing fast guide to building an HTTP source connector. Think of it as the TL;DR version of this tutorial.\nIf you are a visual learner and want to see a video version of this guide going over each part in detail, check it out below.\nA speedy CDK overview.\nDependencies\n\nPython >= 3.9\nDocker\nNodeJS\n\nGenerate the Template\n```bash\n# clone the repo if you havent already\ngit clone --depth 1 https://github.com/airbytehq/airbyte/\ncd airbyte # start from repo root\ncd airbyte-integrations/connector-templates/generator \n./generate.sh\n```\nSelect the `Python HTTP API Source` and name it `python-http-example`.\nCreate Dev Environment\n`bash\ncd ../../connectors/source-python-http-example\npython -m venv .venv # Create a virtual environment in the .venv directory\nsource .venv/bin/activate\npip install -r requirements.txt`\nDefine Connector Inputs\n`bash\ncd source_python_http_example`\nWe're working with the PokeAPI, so we need to define our input schema to reflect that. Open the `spec.yaml` file here and replace it with:\n`yaml\ndocumentationUrl: https://docs.airbyte.io/integrations/sources/pokeapi\nconnectionSpecification:\n  $schema: http://json-schema.org/draft-07/schema#\n  title: Pokeapi Spec\n  type: object\n  required:\n    - pokemon_name\n  properties:\n    pokemon_name:\n      type: string\n      description: Pokemon requested from the API.\n      pattern: ^[a-z0-9_\\-]+$\n      examples:\n        - ditto\n        - luxray\n        - snorlax`\nAs you can see, we have one input to our input schema, which is `pokemon_name`, which is required. Normally, input schemas will contain information such as API keys and client secrets that need to get passed down to all endpoints or streams.\nOk, let's write a function that checks the inputs we just defined. Nuke the `source.py` file. Now add this code to it. For a crucial time skip, we're going to define all the imports we need in the future here. Also note that your `AbstractSource` class name must be a camel-cased version of the name you gave in the generation phase. In our case, this is `SourcePythonHttpExample`.\n```python\nfrom typing import Any, Iterable, List, Mapping, MutableMapping, Optional, Tuple\nimport requests\nimport logging\nfrom airbyte_cdk.sources import AbstractSource\nfrom airbyte_cdk.sources.streams import Stream\nfrom airbyte_cdk.sources.streams.http import HttpStream\nfrom . import pokemon_list\nlogger = logging.getLogger(\"airbyte\")\nclass SourcePythonHttpExample(AbstractSource):\n    def check_connection(self, logger, config) -> Tuple[bool, any]:\n        logger.info(\"Checking Pokemon API connection...\")\n        input_pokemon = config[\"pokemon_name\"]\n        if input_pokemon not in pokemon_list.POKEMON_LIST:\n            result = f\"Input Pokemon {input_pokemon} is invalid. Please check your spelling and input a valid Pokemon.\"\n            logger.info(f\"PokeAPI connection failed: {result}\")\n            return False, result\n        else:\n            logger.info(f\"PokeAPI connection success: {input_pokemon} is a valid Pokemon\")\n            return True, None\n\n\n```def streams(self, config: Mapping[str, Any]) -> List[Stream]:\n    return [Pokemon(pokemon_name=config[\"pokemon_name\"])]\n```\n\n\n```\nCreate a new file called `pokemon_list.py` at the same level. This will handle input validation for us so that we don't input invalid Pokemon. Let's start with a very limited list - any Pokemon not included in this list will get rejected.\n```python\n\"\"\"\npokemon_list.py includes a list of all known pokemon for config validation in source.py.\n\"\"\"\nPOKEMON_LIST = [\n    \"bulbasaur\",\n    \"charizard\",\n    \"wartortle\",\n    \"pikachu\",\n    \"crobat\",\n]\n```\nTest it.\n`bash\ncd ..\nmkdir sample_files\necho '{\"pokemon_name\": \"pikachu\"}'  > sample_files/config.json\necho '{\"pokemon_name\": \"chikapu\"}'  > sample_files/invalid_config.json\npython main.py check --config sample_files/config.json\npython main.py check --config sample_files/invalid_config.json`\nExpected output:\n```text\n\npython main.py check --config sample_files/config.json\n{\"type\": \"CONNECTION_STATUS\", \"connectionStatus\": {\"status\": \"SUCCEEDED\"}}\npython main.py check --config sample_files/invalid_config.json\n{\"type\": \"CONNECTION_STATUS\", \"connectionStatus\": {\"status\": \"FAILED\", \"message\": \"'Input Pokemon chikapu is invalid. Please check your spelling our input a valid Pokemon.'\"}}\n```\n\nDefine your Stream\nIn your `source.py` file, add this `Pokemon` class. This stream represents an endpoint you want to hit, which in our case, is the single Pokemon endpoint.\n```python\nclass Pokemon(HttpStream):\n    url_base = \"https://pokeapi.co/api/v2/\"\n\n\n```# Set this as a noop.\nprimary_key = None\n\ndef __init__(self, pokemon_name: str, **kwargs):\n    super().__init__(**kwargs)\n    self.pokemon_name = pokemon_name\n\ndef next_page_token(self, response: requests.Response) -> Optional[Mapping[str, Any]]:\n    # The API does not offer pagination, so we return None to indicate there are no more pages in the response\n    return None\n\ndef path(\n    self, \n) -> str:\n    return \"\"  # TODO\n\ndef parse_response(\n    self,\n) -> Iterable[Mapping]:\n    return None  # TODO\n```\n\n\n```\nNow download this file. Name it `pokemon.json` and place it in `/source_python_http_example/schemas`.\nThis file defines your output schema for every endpoint that you want to implement. Normally, this will likely be the most time-consuming section of the connector development process, as it requires defining the output of the endpoint exactly. This is really important, as Airbyte needs to have clear expectations for what the stream will output. Note that the name of this stream will be consistent in the naming of the JSON schema and the `HttpStream` class, as `pokemon.json` and `Pokemon` respectively in this case. Learn more about schema creation here.\nTest your discover function. You should receive a fairly large JSON object in return.\n`bash\npython main.py discover --config sample_files/config.json`\nNote that our discover function is using the `pokemon_name` config variable passed in from the `Pokemon` stream when we set it in the `__init__` function.\nReading Data from the Source\nUpdate your `Pokemon` class to implement the required functions as follows:\n```python\nclass Pokemon(HttpStream):\n    url_base = \"https://pokeapi.co/api/v2/\"\n\n\n```# Set this as a noop.\nprimary_key = None\n\ndef __init__(self, pokemon_name: str, **kwargs):\n    super().__init__(**kwargs)\n    # Here's where we set the variable from our input to pass it down to the source.\n    self.pokemon_name = pokemon_name\n\ndef path(self, **kwargs) -> str:\n    pokemon_name = self.pokemon_name\n    # This defines the path to the endpoint that we want to hit.\n    return f\"pokemon/{pokemon_name}\"\n\ndef request_params(\n        self,\n        stream_state: Mapping[str, Any],\n        stream_slice: Mapping[str, Any] = None,\n        next_page_token: Mapping[str, Any] = None,\n) -> MutableMapping[str, Any]:\n    # The api requires that we include the Pokemon name as a query param so we do that in this method.\n    return {\"pokemon_name\": self.pokemon_name}\n\ndef parse_response(\n        self,\n        response: requests.Response,\n        stream_state: Mapping[str, Any],\n        stream_slice: Mapping[str, Any] = None,\n        next_page_token: Mapping[str, Any] = None,\n) -> Iterable[Mapping]:\n    # The response is a simple JSON whose schema matches our stream's schema exactly,\n    # so we just return a list containing the response.\n    return [response.json()]\n\ndef next_page_token(self, response: requests.Response) -> Optional[Mapping[str, Any]]:\n    # While the PokeAPI does offer pagination, we will only ever retrieve one Pokemon with this implementation,\n    # so we just return None to indicate that there will never be any more pages in the response.\n    return None\n```\n\n\n```\nWe now need a catalog that defines all of our streams. We only have one stream: `Pokemon`. Download that file here. Place it in `/sample_files` named as `configured_catalog.json`. More clearly, this is where we tell Airbyte all the streams/endpoints we support for the connector and in which sync modes Airbyte can run the connector on. Learn more about the AirbyteCatalog here and learn more about sync modes here.\nLet's read some data.\n`bash\npython main.py read --config sample_files/config.json --catalog sample_files/configured_catalog.json`\nIf all goes well, containerize it so you can use it in the UI:\n`bash\ndocker build . -t airbyte/source-python-http-example:dev`\nYou're done. Stop the clock :)\nFurther reading",
    "tag": "airbyte"
  },
  {
    "title": "Building a Source Connector: The Hard Way",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/tutorials/build-a-connector-the-hard-way.md",
    "content": "\ndescription: Building a source connector without using any helpers to learn the Airbyte Specification for sources\nBuilding a Source Connector: The Hard Way\nThis tutorial walks you through building a simple Airbyte source without using any helpers to demonstrate the following concepts in Action:\n\nThe Airbyte Specification and the interface implemented by a source connector\nThe AirbyteCatalog\nPackaging your connector\nTesting your connector\n\nAt the end of this tutorial, you will have a working source that you will be able to use in the Airbyte UI.\nThis tutorial is meant for those interested in learning how the Airbyte Specification works in detail, not for creating production connectors. We intentionally don't use helper libraries provided by Airbyte so that this tutorial is self-contained. If you were building a \"real\" source, you'll want to use the helper modules such as the Connector Development Kit.\nThis tutorial can be done entirely on your local workstation.\nRequirements\nTo run this tutorial, you'll need:\n\nDocker, Python, and Java with the versions listed in the tech stack section.\nThe `requests` Python package installed via `pip install requests` (or `pip3` if `pip` is linked to a Python2 installation on your system)\n\nA note on running Python: all the commands below assume that `python` points to a version of Python 3.9 or greater. Verify this by running\n`bash\n$ python --version\nPython 3.9.11`\nOn some systems, `python` points to a Python2 installation and `python3` points to Python3. If this is the case on your machine, substitute all `python` commands in this guide with `python3` . Otherwise, make sure to install Python 3 before beginning.\nYou need also to install `requests` python library:\n`bash\npip install requests`\nOur connector: a stock ticker API\nOur connector will output the daily price of a stock since a given date. We'll leverage the free Polygon.io API for this. We'll use Python to implement the connector because its syntax is accessible to most programmers, but the process described here can be applied to any language.\nHere's the outline of what we'll do to build our connector:\n\nUse the Airbyte connector template to bootstrap the connector package\nImplement the methods required by the Airbyte Specification for our connector:\n`spec`: declares the user-provided credentials or configuration needed to run the connector\n`check`: tests if the connector can connect with the underlying data source  with the user-provided configuration\n`discover`: declares the different streams of data that this connector can output\n`read`: reads data from the underlying data source (The stock ticker API)\nPackage the connector in a Docker image\nTest the connector using Airbyte's Standard Test Suite\nUse the connector to create a new Connection and run a sync in Airbyte UI\n\nOnce we've completed the above steps, we will have built a functioning connector. Then, we'll add some optional functionality:\n\nSupport incremental sync\nAdd custom integration tests\n\n1. Bootstrap the connector package\nWe'll start the process from the Airbyte repository root:\n`bash\n$ pwd\n/Users/sherifnada/code/airbyte`\nFirst, let's create a new branch:\n`bash\n$ git checkout -b $(whoami)/source-connector-tutorial\nSwitched to a new branch 'sherifnada/source-connector-tutorial'`\nAirbyte provides a code generator which bootstraps the scaffolding for our connector. Let's use it by running:\n`bash\n$ cd airbyte-integrations/connector-templates/generator\n$ ./generate.sh`\nWe'll select the `generic` template and call the connector `stock-ticker-api`:\n\nNote: The generic template is very bare. If you are planning on developing a Python source, we recommend using the `python` template. It provides some convenience code to help reduce boilerplate. This tutorial uses the bare-bones version because it makes it easier to see how all the pieces of a connector work together. You can find a walk through on how to build a Python connector here (coming soon).\nHead to the connector directory and we should see the following files have been generated:\n`bash\n$ cd ../../connectors/source-stock-ticker-api\n$ ls\nDockerfile                 README.md                  acceptance-test-config.yml acceptance-test-docker.sh  build.gradle`\nWe'll use each of these files later. But first, let's write some code!\n2. Implement the connector in line with the Airbyte Specification\nIn the connector package directory, create a single Python file `source.py` that will hold our implementation:\n`bash\ntouch source.py`\nImplement the spec operation\nAt this stage in the tutorial, we just want to implement the `spec` operation as described in the Airbyte Protocol. This involves a couple of steps:\n\nDecide which inputs we need from the user in order to connect to the stock ticker API (i.e: the connector's specification) and encode it as a JSON file.\nIdentify when the connector has been invoked with the `spec` operation and return the specification as an `AirbyteMessage`\n\nTo contact the stock ticker API, we need two things:\n\nWhich stock ticker we're interested in\nThe API key to use when contacting the API (you can obtain a free API token from Polygon.io free plan)\n\nFor reference, the API docs we'll be using can be found here.\nLet's create a JSONSchema file `spec.json` encoding these two requirements:\n`javascript\n{\n  \"documentationUrl\": \"https://polygon.io/docs/stocks/get_v2_aggs_ticker__stocksticker__range__multiplier___timespan___from___to\",\n  \"connectionSpecification\": {\n    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n    \"type\": \"object\",\n    \"required\": [\"stock_ticker\", \"api_key\"],\n    \"properties\": {\n      \"stock_ticker\": {\n        \"type\": \"string\",\n        \"title\": \"Stock Ticker\",\n        \"description\": \"The stock ticker to track\",\n        \"examples\": [\"AAPL\", \"TSLA\", \"AMZN\"]\n      },\n      \"api_key\": {\n        \"title\": \"API Key\",\n        \"type\": \"string\",\n        \"description\": \"The Polygon.io Stocks API key to use to hit the API.\",\n        \"airbyte_secret\": true\n      }\n    }\n  }\n}`\n\n`documentationUrl` is the URL that will appear in the UI for the user to gain more info about this connector. Typically this points to `docs.airbyte.io/integrations/sources/source-<connector_name>` but to keep things simple we won't show adding documentation\n`title` is the \"human readable\" title displayed in the UI. Without this field, The Stock Ticker field will have the title `stock_ticker` in the UI\n`description` will be shown in the Airbyte UI under each field to help the user understand it\n`airbyte_secret` used by Airbyte to determine if the field should be displayed as a password (e.g: `********`) in the UI and not readable from the API\n\nWe'll save this file in the root directory of our connector. Now we have the following files:\n`bash\n$ ls -1\nDockerfile\nREADME.md\nacceptance-test-config.yml\nacceptance-test-docker.sh\nbuild.gradle\nsource.py\nspec.json`\nNow, let's edit `source.py` to detect if the program was invoked with the `spec` argument and if so, output the connector specification:\n```python\nsource.py\nimport argparse  # helps parse commandline arguments\nimport json\nimport sys\nimport os\ndef read_json(filepath):\n    with open(filepath, \"r\") as f:\n        return json.loads(f.read())\ndef log(message):\n    log_json = {\"type\": \"LOG\", \"log\": message}\n    print(json.dumps(log_json))\ndef spec():\n    # Read the file named spec.json from the module directory as a JSON file\n    current_script_directory = os.path.dirname(os.path.realpath(file))\n    spec_path = os.path.join(current_script_directory, \"spec.json\")\n    specification = read_json(spec_path)\n\n\n```# form an Airbyte Message containing the spec and print it to stdout\nairbyte_message = {\"type\": \"SPEC\", \"spec\": specification}\n# json.dumps converts the JSON (Python dict) to a string\nprint(json.dumps(airbyte_message))\n```\n\n\ndef run(args):\n    parent_parser = argparse.ArgumentParser(add_help=False)\n    main_parser = argparse.ArgumentParser()\n    subparsers = main_parser.add_subparsers(title=\"commands\", dest=\"command\")\n\n\n```# Accept the spec command\nsubparsers.add_parser(\"spec\", help=\"outputs the json configuration specification\", parents=[parent_parser])\n\nparsed_args = main_parser.parse_args(args)\ncommand = parsed_args.command\n\nif command == \"spec\":\n    spec()\nelse:\n    # If we don't recognize the command log the problem and exit with an error code greater than 0 to indicate the process\n    # had a failure\n    log_error(\"Invalid command. Allowable commands: [spec]\")\n    sys.exit(1)\n\n# A zero exit code means the process successfully completed\nsys.exit(0)\n```\n\n\ndef main():\n    arguments = sys.argv[1:]\n    run(arguments)\nif name == \"main\":\n    main()\n```\nSome notes on the above code:\n\nAs described in the specification, Airbyte connectors are CLIs which communicate via stdout, so the output of the command is simply a JSON string formatted according to the Airbyte Specification. So to \"return\" a value we use `print` to output the return value to stdout\nAll Airbyte commands can output log messages that take the form `{\"type\":\"LOG\", \"log\":\"message\"}`, so we create a helper method `log(message)` to allow logging\nAll Airbyte commands can output error messages that take the form `{\"type\":\"TRACE\", \"trace\": {\"type\": \"ERROR\", \"emitted_at\": current_time_in_ms, \"error\": {\"message\": error_message}}}}`, so we create a helper method `log_error(message)` to allow error messages\n\nNow if we run `python source.py spec` we should see the specification printed out:\n`bash\npython source.py spec\n{\"type\": \"SPEC\", \"spec\": {\"documentationUrl\": \"https://polygon.io/docs/stocks/get_v2_aggs_ticker__stocksticker__range__multiplier___timespan___from___to\", \"connectionSpecification\": {\"$schema\": \"http://json-schema.org/draft-07/schema#\", \"type\": \"object\", \"required\": [\"stock_ticker\", \"api_key\"], \"properties\": {\"stock_ticker\": {\"type\": \"string\", \"title\": \"Stock Ticker\", \"description\": \"The stock ticker to track\", \"examples\": [\"AAPL\", \"TSLA\", \"AMZN\"]}, \"api_key\": {\"type\": \"string\", \"description\": \"The Polygon.io Stocks API key to use to hit the API.\", \"airbyte_secret\": true}}}}}`\nWe've implemented the first command! Three more and we'll have a working connector.\nImplementing check connection\nThe second command to implement is the check operation `check --config <config_name>`, which tells the user whether a config file they gave us is correct. In our case, \"correct\" means they input a valid stock ticker and a correct API key like we declare via the `spec` operation.\nTo achieve this, we'll:\n\nCreate valid and invalid configuration files to test the success and failure cases with our connector. We'll place config files in the `secrets/` directory which is gitignored everywhere in the Airbyte monorepo by default to avoid accidentally checking in API keys\nAdd a `check` method which calls the Polygon.io API to verify if the provided token & stock ticker are correct and output the correct airbyte message\nExtend the argument parser to recognize the `check --config <config>` command and call the `check` method when the `check` command is invoked\n\nLet's first add the configuration files:\n`bash\n$ mkdir secrets\n$ echo '{\"api_key\": \"put_your_key_here\", \"stock_ticker\": \"TSLA\"}' > secrets/valid_config.json\n$ echo '{\"api_key\": \"not_a_real_key\", \"stock_ticker\": \"TSLA\"}' > secrets/invalid_config.json`\nMake sure to add your actual API key instead of the placeholder value `<put_your_key_here>` when following the tutorial.\nThen we'll add the `check_method`:\n```python\nimport requests\nfrom datetime import date\nfrom datetime import datetime\nfrom datetime import timedelta\ndef _call_api(ticker, token):\n    today = date.today()\n    to_day = today.strftime(\"%Y-%m-%d\")\n    from_day = (today - timedelta(days=7)).strftime(\"%Y-%m-%d\")\n    return requests.get(f\"https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/{from_day}/{to_day}?sort=asc&limit=120&apiKey={token}\")\ndef check(config):\n    # Validate input configuration by attempting to get the daily closing prices of the input stock ticker\n    response = _call_api(ticker=config[\"stock_ticker\"], token=config[\"api_key\"])\n    if response.status_code == 200:\n        result = {\"status\": \"SUCCEEDED\"}\n    elif response.status_code == 403:\n        # HTTP code 403 means authorization failed so the API key is incorrect\n        result = {\"status\": \"FAILED\", \"message\": \"API Key is incorrect.\"}\n    else:\n        result = {\"status\": \"FAILED\", \"message\": \"Input configuration is incorrect. Please verify the input stock ticker and API key.\"}\n\n\n```output_message = {\"type\": \"CONNECTION_STATUS\", \"connectionStatus\": result}\nprint(json.dumps(output_message))\n```\n\n\n```\nLastly we'll extend the `run` method to accept the `check` command and call the `check` method. First we'll add a helper method for reading input:\n`python\ndef get_input_file_path(path):\n    if os.path.isabs(path):\n        return path\n    else:\n        return os.path.join(os.getcwd(), path)`\nIn Airbyte, the contract for input files is that they will be available in the current working directory if they are not provided as an absolute path. This method helps us achieve that.\nWe also need to extend the arguments parser by adding the following two blocks to the `run` method:\n`python\n    # Accept the check command\n    check_parser = subparsers.add_parser(\"check\", help=\"checks the config used to connect\", parents=[parent_parser])\n    required_check_parser = check_parser.add_argument_group(\"required named arguments\")\n    required_check_parser.add_argument(\"--config\", type=str, required=True, help=\"path to the json configuration file\")`\nand\n`python\nelif command == \"check\":\n    config_file_path = get_input_file_path(parsed_args.config)\n    config = read_json(config_file_path)\n    check(config)`\nThen we need to update our list of available commands:\n`python\n        log(\"Invalid command. Allowable commands: [spec, check]\")`\nThis results in the following `run` method.\n```python\ndef run(args):\n    parent_parser = argparse.ArgumentParser(add_help=False)\n    main_parser = argparse.ArgumentParser()\n    subparsers = main_parser.add_subparsers(title=\"commands\", dest=\"command\")\n\n\n```# Accept the spec command\nsubparsers.add_parser(\"spec\", help=\"outputs the json configuration specification\", parents=[parent_parser])\n\n# Accept the check command\ncheck_parser = subparsers.add_parser(\"check\", help=\"checks the config used to connect\", parents=[parent_parser])\nrequired_check_parser = check_parser.add_argument_group(\"required named arguments\")\nrequired_check_parser.add_argument(\"--config\", type=str, required=True, help=\"path to the json configuration file\")\n\nparsed_args = main_parser.parse_args(args)\ncommand = parsed_args.command\n\nif command == \"spec\":\n    spec()\nelif command == \"check\":\n    config_file_path = get_input_file_path(parsed_args.config)\n    config = read_json(config_file_path)\n    check(config)\nelse:\n    # If we don't recognize the command log the problem and exit with an error code greater than 0 to indicate the process\n    # had a failure\n    log_error(\"Invalid command. Allowable commands: [spec, check]\")\n    sys.exit(1)\n\n# A zero exit code means the process successfully completed\nsys.exit(0)\n```\n\n\n```\nand that should be it.\nLet's test our new method:\n`bash\n$ python source.py check --config secrets/valid_config.json\n{'type': 'CONNECTION_STATUS', 'connectionStatus': {'status': 'SUCCEEDED'}}\n$ python source.py check --config secrets/invalid_config.json\n{'type': 'CONNECTION_STATUS', 'connectionStatus': {'status': 'FAILED', 'message': 'API Key is incorrect.'}}`\nOur connector is able to detect valid and invalid configs correctly. Two methods down, two more to go!\nImplementing Discover\nThe `discover` command outputs a Catalog, a struct that declares the Streams and Fields (Airbyte's equivalents of tables and columns) output by the connector. It also includes metadata around which features a connector supports (e.g. which sync modes). In other words it describes what data is available in the source. If you'd like to read a bit more about this concept check out our Beginner's Guide to the Airbyte Catalog or for a more detailed treatment read the Airbyte Specification.\nThe data output by this connector will be structured in a very simple way. This connector outputs records belonging to exactly one Stream (table). Each record contains three Fields (columns): `date`, `price`, and `stock_ticker`, corresponding to the price of a stock on a given day.\nTo implement `discover`, we'll:\n\nAdd a method `discover` in `source.py` which outputs the Catalog. To better understand what a catalog is, check out our Beginner's Guide to the AirbyteCatalog\nExtend the arguments parser to use detect the `discover --config <config_path>` command and call the `discover` method\n\nLet's implement `discover` by adding the following in `source.py`:\n`python\ndef discover():\n    catalog = {\n        \"streams\": [{\n            \"name\": \"stock_prices\",\n            \"supported_sync_modes\": [\"full_refresh\"],\n            \"json_schema\": {\n                \"properties\": {\n                    \"date\": {\n                        \"type\": \"string\"\n                    },\n                    \"price\": {\n                        \"type\": \"number\"\n                    },\n                    \"stock_ticker\": {\n                        \"type\": \"string\"\n                    }\n                }\n            }\n        }]\n    }\n    airbyte_message = {\"type\": \"CATALOG\", \"catalog\": catalog}\n    print(json.dumps(airbyte_message))`\nNote that we describe the schema of the output stream using JSONSchema.\nThen we'll extend the arguments parser by adding the following blocks to the `run` method:\n```python\nAccept the discover command\ndiscover_parser = subparsers.add_parser(\"discover\", help=\"outputs a catalog describing the source's schema\", parents=[parent_parser])\nrequired_discover_parser = discover_parser.add_argument_group(\"required named arguments\")\nrequired_discover_parser.add_argument(\"--config\", type=str, required=True, help=\"path to the json configuration file\")\n```\nand\n`python\nelif command == \"discover\":\n    discover()`\nWe need to update our list of available commands:\n`python\n        log(\"Invalid command. Allowable commands: [spec, check, discover]\")`\nYou may be wondering why `config` is a required input to `discover` if it's not used. This is done for consistency: the Airbyte Specification requires `--config` as an input to `discover` because many sources require it (e.g: to discover the tables available in a Postgres database, you must supply a password). So instead of guessing whether the flag is required depending on the connector, we always assume it is required, and the connector can choose whether to use it.\nThe full run method is now below:\n```python\ndef run(args):\n    parent_parser = argparse.ArgumentParser(add_help=False)\n    main_parser = argparse.ArgumentParser()\n    subparsers = main_parser.add_subparsers(title=\"commands\", dest=\"command\")\n\n\n```# Accept the spec command\nsubparsers.add_parser(\"spec\", help=\"outputs the json configuration specification\", parents=[parent_parser])\n\n# Accept the check command\ncheck_parser = subparsers.add_parser(\"check\", help=\"checks the config used to connect\", parents=[parent_parser])\nrequired_check_parser = check_parser.add_argument_group(\"required named arguments\")\nrequired_check_parser.add_argument(\"--config\", type=str, required=True, help=\"path to the json configuration file\")\n\n# Accept the discover command\ndiscover_parser = subparsers.add_parser(\"discover\", help=\"outputs a catalog describing the source's schema\", parents=[parent_parser])\nrequired_discover_parser = discover_parser.add_argument_group(\"required named arguments\")\nrequired_discover_parser.add_argument(\"--config\", type=str, required=True, help=\"path to the json configuration file\")\n\nparsed_args = main_parser.parse_args(args)\ncommand = parsed_args.command\n\nif command == \"spec\":\n    spec()\nelif command == \"check\":\n    config_file_path = get_input_file_path(parsed_args.config)\n    config = read_json(config_file_path)\n    check(config)\nelif command == \"discover\":\n    discover()\nelse:\n    # If we don't recognize the command log the problem and exit with an error code greater than 0 to indicate the process\n    # had a failure\n    log_error(\"Invalid command. Allowable commands: [spec, check, discover]\")\n    sys.exit(1)\n\n# A zero exit code means the process successfully completed\nsys.exit(0)\n```\n\n\n```\nLet's test our new command:\n`bash\n$ python source.py discover --config secrets/valid_config.json\n{\"type\": \"CATALOG\", \"catalog\": {\"streams\": [{\"name\": \"stock_prices\", \"supported_sync_modes\": [\"full_refresh\"], \"json_schema\": {\"properties\": {\"date\": {\"type\": \"string\"}, \"price\": {\"type\": \"number\"}, \"stock_ticker\": {\"type\": \"string\"}}}}]}}`\nWith that, we're done implementing the `discover` command.\nImplementing the read operation\nWe've done a lot so far, but a connector ultimately exists to read data! This is where the read command comes in. The format of the command is:\n`bash\npython source.py read --config <config_file_path> --catalog <configured_catalog.json> [--state <state_file_path>]`\nEach of these are described in the Airbyte Specification in detail, but we'll give a quick description of the two options we haven't seen so far:\n\n`--catalog` points to a Configured Catalog. The Configured Catalog contains the contents for the Catalog (remember the Catalog we output from discover?). It also contains some configuration information that describes how the data will by replicated. For example, we had `supported_sync_modes` in the Catalog. In the Configured Catalog, we select which of the `supported_sync_modes` we want to use by specifying the `sync_mode` field. (This is the most complicated concept when working Airbyte, so if it is still not making sense that's okay for now. If you're just dying to understand how the Configured Catalog works checkout the Beginner's Guide to the Airbyte Catalog)\n`--state` points to a state file. The state file is only relevant when some Streams are synced with the sync mode `incremental`, so we'll cover the state file in more detail in the incremental section below\n\nFor our connector, the contents of those two files should be very unsurprising: the connector only supports one Stream, `stock_prices`, so we'd expect the input catalog to contain that stream configured to sync in full refresh. Since our connector doesn't support incremental sync (yet!) we'll ignore the state option for now.\nTo read data in our connector, we'll:\n\nCreate a configured catalog which tells our connector that we want to sync the `stock_prices` stream\nImplement a method `read` in `source.py`. For now we'll always read the last 7 days of a stock price's data\nExtend the arguments parser to recognize the `read` command and its arguments\n\nFirst, let's create a configured catalog `fullrefresh_configured_catalog.json` to use as test input for the read operation:\n```javascript\n{\n    \"streams\": [\n        {\n            \"stream\": {\n                \"name\": \"stock_prices\",\n                \"supported_sync_modes\": [\n                    \"full_refresh\"\n                ],\n                \"json_schema\": {\n                    \"properties\": {\n                        \"date\": {\n                            \"type\": \"string\"\n                        },\n                        \"price\": {\n                            \"type\": \"number\"\n                        },\n                        \"stock_ticker\": {\n                            \"type\": \"string\"\n                        }\n                    }\n                }\n            },\n            \"sync_mode\": \"full_refresh\",\n            \"destination_sync_mode\": \"overwrite\"\n        }\n    ]\n}\n```\nThen we'll define the `read` method in `source.py`:\n```python\ndef log_error(error_message):\n    current_time_in_ms = int(datetime.now().timestamp()) * 1000\n    log_json = {\"type\": \"TRACE\", \"trace\": {\"type\": \"ERROR\", \"emitted_at\": current_time_in_ms, \"error\": {\"message\": error_message}}}\n    print(json.dumps(log_json))\ndef read(config, catalog):\n    # Assert required configuration was provided\n    if \"api_key\" not in config or \"stock_ticker\" not in config:\n        log_error(\"Input config must contain the properties 'api_key' and 'stock_ticker'\")\n        sys.exit(1)\n\n\n```# Find the stock_prices stream if it is present in the input catalog\nstock_prices_stream = None\nfor configured_stream in catalog[\"streams\"]:\n    if configured_stream[\"stream\"][\"name\"] == \"stock_prices\":\n        stock_prices_stream = configured_stream\n\nif stock_prices_stream is None:\n    log_error(\"No stream selected.\")\n    return\n\n# We only support full_refresh at the moment, so verify the user didn't ask for another sync mode\nif stock_prices_stream[\"sync_mode\"] != \"full_refresh\":\n    log_error(\"This connector only supports full refresh syncs! (for now)\")\n    sys.exit(1)\n\n# If we've made it this far, all the configuration is good and we can pull the last 7 days of market data\nresponse = _call_api(ticker=config[\"stock_ticker\"], token = config[\"api_key\"])\nif response.status_code != 200:\n    # In a real scenario we'd handle this error better :)\n    log_error(\"Failure occurred when calling Polygon.io API\")\n    sys.exit(1)\nelse:\n    # Stock prices are returned sorted by date in ascending order\n    # We want to output them one by one as AirbyteMessages\n    results = response.json()[\"results\"]\n    for result in results:\n        data = {\"date\": date.fromtimestamp(result[\"t\"]/1000).isoformat(), \"stock_ticker\": config[\"stock_ticker\"], \"price\": result[\"c\"]}\n        record = {\"stream\": \"stock_prices\", \"data\": data, \"emitted_at\": int(datetime.now().timestamp()) * 1000}\n        output_message = {\"type\": \"RECORD\", \"record\": record}\n        print(json.dumps(output_message))\n```\n\n\n```\nNote we've added a `log_error()` function to simplify formatting error messages from within connector functions as AirbyteTraceMessages, specifically `AirbyteErrorTraceMessage`s.\nAfter doing some input validation, the code above calls the API to obtain daily prices for the input stock ticker, then outputs the prices. As always, our output is formatted according to the Airbyte Specification. Let's update our args parser with the following blocks:\n```python\nAccept the read command\nread_parser = subparsers.add_parser(\"read\", help=\"reads the source and outputs messages to STDOUT\", parents=[parent_parser])\nread_parser.add_argument(\"--state\", type=str, required=False, help=\"path to the json-encoded state file\")\nrequired_read_parser = read_parser.add_argument_group(\"required named arguments\")\nrequired_read_parser.add_argument(\"--config\", type=str, required=True, help=\"path to the json configuration file\")\nrequired_read_parser.add_argument(\n    \"--catalog\", type=str, required=True, help=\"path to the catalog used to determine which data to read\"\n)\n```\nand:\n`python\nelif command == \"read\":\n    config = read_json(get_input_file_path(parsed_args.config))\n    configured_catalog = read_json(get_input_file_path(parsed_args.catalog))\n    read(config, configured_catalog)`\nand: \n`python\n        log_error(\"Invalid command. Allowable commands: [spec, check, discover, read]\")`\nthis yields the following `run` method:\n```python\ndef run(args):\n    parent_parser = argparse.ArgumentParser(add_help=False)\n    main_parser = argparse.ArgumentParser()\n    subparsers = main_parser.add_subparsers(title=\"commands\", dest=\"command\")\n\n\n```# Accept the spec command\nsubparsers.add_parser(\"spec\", help=\"outputs the json configuration specification\", parents=[parent_parser])\n\n# Accept the check command\ncheck_parser = subparsers.add_parser(\"check\", help=\"checks the config used to connect\", parents=[parent_parser])\nrequired_check_parser = check_parser.add_argument_group(\"required named arguments\")\nrequired_check_parser.add_argument(\"--config\", type=str, required=True, help=\"path to the json configuration file\")\n\n# Accept the discover command\ndiscover_parser = subparsers.add_parser(\"discover\", help=\"outputs a catalog describing the source's schema\", parents=[parent_parser])\nrequired_discover_parser = discover_parser.add_argument_group(\"required named arguments\")\nrequired_discover_parser.add_argument(\"--config\", type=str, required=True, help=\"path to the json configuration file\")\n\n# Accept the read command\nread_parser = subparsers.add_parser(\"read\", help=\"reads the source and outputs messages to STDOUT\", parents=[parent_parser])\nread_parser.add_argument(\"--state\", type=str, required=False, help=\"path to the json-encoded state file\")\nrequired_read_parser = read_parser.add_argument_group(\"required named arguments\")\nrequired_read_parser.add_argument(\"--config\", type=str, required=True, help=\"path to the json configuration file\")\nrequired_read_parser.add_argument(\n    \"--catalog\", type=str, required=True, help=\"path to the catalog used to determine which data to read\"\n)\n\nparsed_args = main_parser.parse_args(args)\ncommand = parsed_args.command\n\nif command == \"spec\":\n    spec()\nelif command == \"check\":\n    config_file_path = get_input_file_path(parsed_args.config)\n    config = read_json(config_file_path)\n    check(config)\nelif command == \"discover\":\n    discover()\nelif command == \"read\":\n    config = read_json(get_input_file_path(parsed_args.config))\n    configured_catalog = read_json(get_input_file_path(parsed_args.catalog))\n    read(config, configured_catalog)\nelse:\n    # If we don't recognize the command log the problem and exit with an error code greater than 0 to indicate the process\n    # had a failure\n    log_error(\"Invalid command. Allowable commands: [spec, check, discover, read]\")\n    sys.exit(1)\n\n# A zero exit code means the process successfully completed\nsys.exit(0)\n```\n\n\n```\nLet's test out our new command:\n`bash\n$ python source.py read --config secrets/valid_config.json --catalog fullrefresh_configured_catalog.json\n{'type': 'RECORD', 'record': {'stream': 'stock_prices', 'data': {'date': '2020-12-15', 'stock_ticker': 'TSLA', 'price': 633.25}, 'emitted_at': 1608626365000}}\n{'type': 'RECORD', 'record': {'stream': 'stock_prices', 'data': {'date': '2020-12-16', 'stock_ticker': 'TSLA', 'price': 622.77}, 'emitted_at': 1608626365000}}\n{'type': 'RECORD', 'record': {'stream': 'stock_prices', 'data': {'date': '2020-12-17', 'stock_ticker': 'TSLA', 'price': 655.9}, 'emitted_at': 1608626365000}}\n{'type': 'RECORD', 'record': {'stream': 'stock_prices', 'data': {'date': '2020-12-18', 'stock_ticker': 'TSLA', 'price': 695}, 'emitted_at': 1608626365000}}\n{'type': 'RECORD', 'record': {'stream': 'stock_prices', 'data': {'date': '2020-12-21', 'stock_ticker': 'TSLA', 'price': 649.86}, 'emitted_at': 1608626365000}}`\nWith this method, we now have a fully functioning connector! Let's pat ourselves on the back for getting there.\nFor reference, the full `source.py` file now looks like this:\n```python\nMIT License\n\nCopyright (c) 2020 Airbyte\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\nimport argparse  # helps parse commandline arguments\nimport json\nimport sys\nimport os\nimport requests\nfrom datetime import date\nfrom datetime import datetime\nfrom datetime import timedelta\ndef read(config, catalog):\n    # Assert required configuration was provided\n    if \"api_key\" not in config or \"stock_ticker\" not in config:\n        log_error(\"Input config must contain the properties 'api_key' and 'stock_ticker'\")\n        sys.exit(1)\n\n\n```# Find the stock_prices stream if it is present in the input catalog\nstock_prices_stream = None\nfor configured_stream in catalog[\"streams\"]:\n    if configured_stream[\"stream\"][\"name\"] == \"stock_prices\":\n        stock_prices_stream = configured_stream\n\nif stock_prices_stream is None:\n    log_error(\"No streams selected\")\n    return\n\n# We only support full_refresh at the moment, so verify the user didn't ask for another sync mode\nif stock_prices_stream[\"sync_mode\"] != \"full_refresh\":\n    log_error(\"This connector only supports full refresh syncs! (for now)\")\n    sys.exit(1)\n\n# If we've made it this far, all the configuration is good and we can pull the last 7 days of market data\nresponse = _call_api(ticker=config[\"stock_ticker\"], token = config[\"api_key\"])\nif response.status_code != 200:\n    # In a real scenario we'd handle this error better :)\n    log_error(\"Failure occurred when calling Polygon.io API\")\n    sys.exit(1)\nelse:\n    # Stock prices are returned sorted by date in ascending order\n    # We want to output them one by one as AirbyteMessages\n    results = response.json()[\"results\"]\n    for result in results:\n        data = {\"date\": date.fromtimestamp(result[\"t\"]/1000).isoformat(), \"stock_ticker\": config[\"stock_ticker\"], \"price\": result[\"c\"]}\n        record = {\"stream\": \"stock_prices\", \"data\": data, \"emitted_at\": int(datetime.now().timestamp()) * 1000}\n        output_message = {\"type\": \"RECORD\", \"record\": record}\n        print(json.dumps(output_message))\n```\n\n\ndef read_json(filepath):\n    with open(filepath, \"r\") as f:\n        return json.loads(f.read())\ndef _call_api(ticker, token):\n    today = date.today()\n    to_day = today.strftime(\"%Y-%m-%d\")\n    from_day = (today - timedelta(days=7)).strftime(\"%Y-%m-%d\")\n    return requests.get(f\"https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/{from_day}/{to_day}?sort=asc&limit=120&apiKey={token}\")\ndef check(config):\n    # Assert required configuration was provided\n    if \"api_key\" not in config or \"stock_ticker\" not in config:\n        log_error(\"Input config must contain the properties 'api_key' and 'stock_ticker'\")\n        sys.exit(1)\n    else:\n        # Validate input configuration by attempting to get the daily closing prices of the input stock ticker\n        response = _call_api(ticker=config[\"stock_ticker\"], token=config[\"api_key\"])\n        if response.status_code == 200:\n            result = {\"status\": \"SUCCEEDED\"}\n        elif response.status_code == 403:\n            # HTTP code 403 means authorization failed so the API key is incorrect\n            result = {\"status\": \"FAILED\", \"message\": \"API Key is incorrect.\"}\n        else:\n            # Consider any other code a \"generic\" failure and tell the user to make sure their config is correct.\n            result = {\"status\": \"FAILED\", \"message\": \"Input configuration is incorrect. Please verify the input stock ticker and API key.\"}\n\n\n```    # Format the result of the check operation according to the Airbyte Specification\n    output_message = {\"type\": \"CONNECTION_STATUS\", \"connectionStatus\": result}\n    print(json.dumps(output_message))\n```\n\n\ndef log(message):\n    log_json = {\"type\": \"LOG\", \"log\": message}\n    print(json.dumps(log_json))\ndef log_error(error_message):\n   current_time_in_ms = int(datetime.now().timestamp()) * 1000\n   log_json = {\"type\": \"TRACE\", \"trace\": {\"type\": \"ERROR\", \"emitted_at\": current_time_in_ms, \"error\": {\"message\": error_message}}}\n   print(json.dumps(log_json))\ndef discover():\n    catalog = {\n        \"streams\": [{\n            \"name\": \"stock_prices\",\n            \"supported_sync_modes\": [\"full_refresh\"],\n            \"json_schema\": {\n                \"properties\": {\n                    \"date\": {\n                        \"type\": \"string\"\n                    },\n                    \"price\": {\n                        \"type\": \"number\"\n                    },\n                    \"stock_ticker\": {\n                        \"type\": \"string\"\n                    }\n                }\n            }\n        }]\n    }\n    airbyte_message = {\"type\": \"CATALOG\", \"catalog\": catalog}\n    print(json.dumps(airbyte_message))\ndef get_input_file_path(path):\n    if os.path.isabs(path):\n        return path\n    else:\n        return os.path.join(os.getcwd(), path)\ndef spec():\n    # Read the file named spec.json from the module directory as a JSON file\n    current_script_directory = os.path.dirname(os.path.realpath(file))\n    spec_path = os.path.join(current_script_directory, \"spec.json\")\n    specification = read_json(spec_path)\n\n\n```# form an Airbyte Message containing the spec and print it to stdout\nairbyte_message = {\"type\": \"SPEC\", \"spec\": specification}\n# json.dumps converts the JSON (Python dict) to a string\nprint(json.dumps(airbyte_message))\n```\n\n\ndef run(args):\n    parent_parser = argparse.ArgumentParser(add_help=False)\n    main_parser = argparse.ArgumentParser()\n    subparsers = main_parser.add_subparsers(title=\"commands\", dest=\"command\")\n\n\n```# Accept the spec command\nsubparsers.add_parser(\"spec\", help=\"outputs the json configuration specification\", parents=[parent_parser])\n\n# Accept the check command\ncheck_parser = subparsers.add_parser(\"check\", help=\"checks the config used to connect\", parents=[parent_parser])\nrequired_check_parser = check_parser.add_argument_group(\"required named arguments\")\nrequired_check_parser.add_argument(\"--config\", type=str, required=True, help=\"path to the json configuration file\")\n\n# Accept the discover command\ndiscover_parser = subparsers.add_parser(\"discover\", help=\"outputs a catalog describing the source's schema\", parents=[parent_parser])\nrequired_discover_parser = discover_parser.add_argument_group(\"required named arguments\")\nrequired_discover_parser.add_argument(\"--config\", type=str, required=True, help=\"path to the json configuration file\")\n\n# Accept the read command\nread_parser = subparsers.add_parser(\"read\", help=\"reads the source and outputs messages to STDOUT\", parents=[parent_parser])\nread_parser.add_argument(\"--state\", type=str, required=False, help=\"path to the json-encoded state file\")\nrequired_read_parser = read_parser.add_argument_group(\"required named arguments\")\nrequired_read_parser.add_argument(\"--config\", type=str, required=True, help=\"path to the json configuration file\")\nrequired_read_parser.add_argument(\n    \"--catalog\", type=str, required=True, help=\"path to the catalog used to determine which data to read\"\n)\n\nparsed_args = main_parser.parse_args(args)\ncommand = parsed_args.command\n\nif command == \"spec\":\n    spec()\nelif command == \"check\":\n    config_file_path = get_input_file_path(parsed_args.config)\n    config = read_json(config_file_path)\n    check(config)\nelif command == \"discover\":\n    discover()\nelif command == \"read\":\n    config = read_json(get_input_file_path(parsed_args.config))\n    configured_catalog = read_json(get_input_file_path(parsed_args.catalog))\n    read(config, configured_catalog)\nelse:\n    # If we don't recognize the command log the problem and exit with an error code greater than 0 to indicate the process\n    # had a failure\n    log_error(\"Invalid command. Allowable commands: [spec, check, discover, read]\")\n    sys.exit(1)\n\n# A zero exit code means the process successfully completed\nsys.exit(0)\n```\n\n\ndef main():\n    arguments = sys.argv[1:]\n    run(arguments)\nif name == \"main\":\n    main()\n```\nA full connector in less than 200 lines of code. Not bad! We're now ready to package & test our connector then use it in the Airbyte UI.\n3. Package the connector in a Docker image\nOur connector is very lightweight, so the Dockerfile needed to run it is very light as well. We edit the autogenerated `Dockerfile` so that its contents are as followed:\n```Dockerfile\nFROM python:3.9-slim\nWe change to a directory unique to us\nWORKDIR /airbyte/integration_code\nInstall any needed Python dependencies\nRUN pip install requests\nCopy source files\nCOPY source.py .\nCOPY spec.json .\nWhen this container is invoked, append the input argemnts to `python source.py`\nENTRYPOINT [\"python\", \"/airbyte/integration_code/source.py\"]\nAirbyte's build system uses these labels to know what to name and tag the docker images produced by this Dockerfile.\nLABEL io.airbyte.name=airbyte/source-stock-ticker-api\nLABEL io.airbyte.version=0.1.0\nIn order to launch a source on Kubernetes in a pod, we need to be able to wrap the entrypoint.\nThe source connector must specify its entrypoint in the AIRBYTE_ENTRYPOINT variable.\nENV AIRBYTE_ENTRYPOINT='python /airbyte/integration_code/source.py'\n```\nOnce we save the `Dockerfile`, we can build the image by running:\n`bash\ndocker build . -t airbyte/source-stock-ticker-api:dev`\nThen we can run the image using:\n`bash\ndocker run airbyte/source-stock-ticker-api:dev`\nTo run any of our commands, we'll need to mount all the inputs into the Docker container first, then refer to their mounted paths when invoking the connector. This allows the connector to access your secrets without having to build them into the container. For example, we'd run `check` or `read` as follows:\n```bash\n$ docker run airbyte/source-stock-ticker-api:dev spec\n{\"type\": \"SPEC\", \"spec\": {\"documentationUrl\": \"https://polygon.io/docs/stocks/get_v2_aggs_ticker__stocksticker__range__multipliertimespanfrom___to\", \"connectionSpecification\": {\"$schema\": \"http://json-schema.org/draft-07/schema#\", \"type\": \"object\", \"required\": [\"stock_ticker\", \"api_key\"], \"properties\": {\"stock_ticker\": {\"type\": \"string\", \"title\": \"Stock Ticker\", \"description\": \"The stock ticker to track\", \"examples\": [\"AAPL\", \"TSLA\", \"AMZN\"]}, \"api_key\": {\"type\": \"string\", \"description\": \"The Polygon.io Stocks API key to use to hit the API.\", \"airbyte_secret\": true}}}}}\n$ docker run -v $(pwd)/secrets/valid_config.json:/data/config.json airbyte/source-stock-ticker-api:dev check --config /data/config.json\n{'type': 'CONNECTION_STATUS', 'connectionStatus': {'status': 'SUCCEEDED'}}\n$ docker run -v $(pwd)/secrets/valid_config.json:/data/config.json airbyte/source-stock-ticker-api:dev discover --config /data/config.json\n{\"type\": \"CATALOG\", \"catalog\": {\"streams\": [{\"name\": \"stock_prices\", \"supported_sync_modes\": [\"full_refresh\"], \"json_schema\": {\"properties\": {\"date\": {\"type\": \"string\"}, \"price\": {\"type\": \"number\"}, \"stock_ticker\": {\"type\": \"string\"}}}}]}}\n$ docker run -v $(pwd)/secrets/valid_config.json:/data/config.json -v $(pwd)/fullrefresh_configured_catalog.json:/data/fullrefresh_configured_catalog.json airbyte/source-stock-ticker-api:dev read --config /data/config.json --catalog /data/fullrefresh_configured_catalog.json\n{'type': 'RECORD', 'record': {'stream': 'stock_prices', 'data': {'date': '2020-12-15', 'stock_ticker': 'TSLA', 'price': 633.25}, 'emitted_at': 1608628424000}}\n{'type': 'RECORD', 'record': {'stream': 'stock_prices', 'data': {'date': '2020-12-16', 'stock_ticker': 'TSLA', 'price': 622.77}, 'emitted_at': 1608628424000}}\n{'type': 'RECORD', 'record': {'stream': 'stock_prices', 'data': {'date': '2020-12-17', 'stock_ticker': 'TSLA', 'price': 655.9}, 'emitted_at': 1608628424000}}\n{'type': 'RECORD', 'record': {'stream': 'stock_prices', 'data': {'date': '2020-12-18', 'stock_ticker': 'TSLA', 'price': 695}, 'emitted_at': 1608628424000}}\n{'type': 'RECORD', 'record': {'stream': 'stock_prices', 'data': {'date': '2020-12-21', 'stock_ticker': 'TSLA', 'price': 649.86}, 'emitted_at': 1608628424000}}\n```\nand with that, we've packaged our connector in a functioning Docker image. The last requirement before calling this connector finished is to pass the Airbyte Connector Acceptance Test suite.\n4. Test the connector\nThe minimum requirement for testing your connector is to pass the Connector Acceptance Test suite. The connector acceptence test is a blackbox test suite containing a number of tests that validate your connector behaves as intended by the Airbyte Specification. You're encouraged to add custom test cases for your connector where it makes sense to do so e.g: to test edge cases that are not covered by the standard suite. But at the very least, you must pass Airbyte's acceptance test suite.\nThe code generator should have already generated a YAML file which configures the test suite. In order to run it, modify the `acceptance-test-config.yaml` file to look like this: \n```yaml\nSee Connector Acceptance Tests\nfor more information about how to configure these tests\nconnector_image: airbyte/source-stock-ticker-api:dev\nacceptance_tests:\n  basic_read:\n    tests:\n    - config_path: secrets/valid_config.json\n      configured_catalog_path: fullrefresh_configured_catalog.json\n      empty_streams: []\n  connection:\n    tests:\n    - config_path: secrets/valid_config.json\n      status: succeed\n    - config_path: secrets/invalid_config.json\n      status: failed\n  discovery:\n    tests:\n    - config_path: secrets/valid_config.json\n  full_refresh:\n    tests:\n    - config_path: secrets/valid_config.json\n      configured_catalog_path: fullrefresh_configured_catalog.json\n  spec:\n    tests:\n    - config_path: secrets/valid_config.json\n      spec_path: spec.json\nincremental: # TODO uncomment this once you implement incremental sync in part 2 of the tutorial\ntests:\n- config_path: \"secrets/config.json\"\nconfigured_catalog_path: \"integration_tests/configured_catalog.json\"\nfuture_state_path: \"integration_tests/abnormal_state.json\"\n```\nThen from the connector module directory run\n`bash\n./acceptance-test-docker.sh`\nAfter tests have run, you should see a test summary like:\n```text\ncollecting ...\n test_core.py \u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713                                                                                                                                                                                                                                                                                                                           95% \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c\n test_full_refresh.py \u2713                                                                                                                                                                                                                                                                                                                                    100% \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n================== short test summary info ================== \nSKIPPED [1] connector_acceptance_test/plugin.py:56: Skipping TestIncremental.test_two_sequential_reads because not found in the config\nResults (8.91s):\n      20 passed\n```\nThat's it! We've created a fully functioning connector. Now let's get to the exciting part: using it from the Airbyte UI.\nUse the connector in the Airbyte UI\nLet's recap what we've achieved so far:\n\nImplemented a connector\nPackaged it in a Docker image\nIntegrated it with the Airbyte Standard Test suite\n\nTo use it from the Airbyte UI, we need to:\n\nPublish our connector's Docker image somewhere accessible by Airbyte Core (Airbyte's server, scheduler, workers, and webapp infrastructure)\nAdd the connector via the Airbyte UI and setup a connection from our new connector to a local CSV file for illustration purposes\nRun a sync and inspect the output\n\n1. Publish the Docker image\nSince we're running this tutorial locally, Airbyte will have access to any Docker images available to your local `docker` daemon. So all we need to do is build & tag our connector. If you want your connector to be available to everyone using Airbyte, you'll need to publish it to `Dockerhub`. Open a PR or visit our Slack for help with this.\nAirbyte's build system builds and tags your connector's image correctly by default as part of the connector's standard `build` process. From the Airbyte repo root, run:\n`bash\n./gradlew clean :airbyte-integrations:connectors:source-stock-ticker-api:build`\nThis is the equivalent of running `docker build . -t airbyte/source-stock-ticker-api:dev` from the connector root, where the tag `airbyte/source-stock-ticker-api` is extracted from the label `LABEL io.airbyte.name` inside your `Dockerfile`.\nVerify the image was built by running:\n`bash\n$  docker images | head\n  REPOSITORY                                                    TAG                 IMAGE ID       CREATED          SIZE\n  airbyte/source-stock-ticker-api                               dev                 9494ea93b7d0   16 seconds ago   121MB\n  <none>                                                        <none>              8fe5b49f9ae5   3 hours ago      121MB\n  <none>                                                        <none>              4cb00a551b3c   3 hours ago      121MB\n  <none>                                                        <none>              1caf57c72afd   3 hours ago      121MB`\n`airbyte/source-stock-ticker-api` was built and tagged with the `dev` tag. Now let's head to the last step.\n2. Add the connector via the Airbyte UI\nIf the Airbyte server isn't already running, start it by running from the Airbyte repository root:\n`bash\ndocker compose up`\nWhen Airbyte server is done starting up, it prints the following banner in the log output (it can take 10-20 seconds for the server to start):\n`bash\nairbyte-server      | 2022-03-11 18:38:33 INFO i.a.s.ServerApp(start):121 - \nairbyte-server      |     ___    _      __          __\nairbyte-server      |    /   |  (_)____/ /_  __  __/ /____\nairbyte-server      |   / /| | / / ___/ __ \\/ / / / __/ _ \\\nairbyte-server      |  / ___ |/ / /  / /_/ / /_/ / /_/  __/\nairbyte-server      | /_/  |_/_/_/  /_.___/\\__, /\\__/\\___/\nairbyte-server      |                     /____/\nairbyte-server      | --------------------------------------\nairbyte-server      |  Now ready at http://localhost:8000/\nairbyte-server      | --------------------------------------\nairbyte-server      | Version: dev\nairbyte-server      |`\nAfter you see the above banner printed out in the terminal window where you are running `docker compose up`, visit http://localhost:8000 in your browser and log in with the default credentials: username `airbyte` and password `password`.\nIf this is the first time using the Airbyte UI, then you will be prompted to go through a first-time wizard. To skip it, click the \"Skip Onboarding\" button.\nIn the UI, click the \"Settings\" button in the left side bar:\n\nThen on the Settings page, select Sources\n\nThen on the Settings/Sources page, click \"+ New Connector\" button at the top right:\n\nOn the modal that pops up, enter the following information then click \"Add\"\n\nAfter you click \"Add\", the modal will close and you will be back at the Settings page.\nNow click \"Sources\" in the navigation bar on the left:\n\nYou will be redirected to Sources page, which, if you have not set up any connections, will be empty.\nOn the Sources page click \"+ new source\" in the top right corner:\n\nA new modal will prompt you for details of the new source. Type \"Stock Ticker\" in the Name field.\nThen, find your connector in the Source type dropdown. We have lots of connectors already, so it might be easier\nto find your connector by typing part of its name:\n\nAfter you select your connector in the Source type dropdown, the modal will show two more fields: API Key and Stock Ticker.\nRemember that `spec.json` file you created at the very beginning of this tutorial? These fields should correspond to the `properties`\nsection of that file. Copy-paste your Polygon.io API key and a stock ticker into these fields and then click \"Set up source\"\nbutton at the bottom right of the modal. \n\nOnce you click \"Set up source\", Airbyte will spin up your connector and run \"check\" method to verify the configuration.\nYou will see a progress bar briefly and if the configuration is valid, you will see a success message, \nthe modal will close and you will see your connector on the updated Sources page.\n\nNext step is to add a destination. On the same page, click \"add destination\" and then click \"+ add a new destination\":\n\n\"New destination\" wizard will show up. Type a name (e.g. \"Local JSON\") into the Name field and select \"Local JSON\" in Destination type drop-down.\nAfter you select the destination type, type `/local/tutorial_json` into Destination path field.\nWhen we run syncs, we'll find the output on our local filesystem in `/tmp/airbyte_local/tutorial_json`.\nClick \"Set up destination\" at the lower right of the form.\n\nAfter that Airbyte will test the destination and prompt you to configure the connection between Stock Ticker source and Local JSON destination.\nSelect \"Mirror source structure\" in the Destination Namespace, check the checkbox next to the stock_prices stream, and click \"Set up connection\" button at the bottom of the form:\n\nTa-da! Your connection is now configured to sync once a day. You will see your new connection on the next screen: \n\nAirbyte will run the first sync job as soon as your connection is saved. Navigate to \"Connections\" in the side bar and wait for the first sync to succeed:\n\nLet's verify the output. From your shell, run:\n`bash\n$ cat /tmp/airbyte_local/tutorial_json/_airbyte_raw_stock_prices.jsonl\n{\"_airbyte_ab_id\":\"7383c6c1-783a-4a8a-a39c-3890ab562495\",\"_airbyte_emitted_at\":1647026803000,\"_airbyte_data\":{\"date\":\"2022-03-04\",\"stock_ticker\":\"TSLA\",\"price\":838.29}}\n{\"_airbyte_ab_id\":\"cf7dc8d9-1ece-4a40-a7d6-35cae54b94e5\",\"_airbyte_emitted_at\":1647026803000,\"_airbyte_data\":{\"date\":\"2022-03-07\",\"stock_ticker\":\"TSLA\",\"price\":804.58}}\n{\"_airbyte_ab_id\":\"da7da131-41d2-4ba7-bba1-1a0a5329a30a\",\"_airbyte_emitted_at\":1647026803000,\"_airbyte_data\":{\"date\":\"2022-03-08\",\"stock_ticker\":\"TSLA\",\"price\":824.4}}\n{\"_airbyte_ab_id\":\"20df0d78-5a5e-437b-95d8-aa57cf19fce1\",\"_airbyte_emitted_at\":1647026803000,\"_airbyte_data\":{\"date\":\"2022-03-09\",\"stock_ticker\":\"TSLA\",\"price\":858.97}}\n{\"_airbyte_ab_id\":\"0b7a8d33-4500-4a6d-9d74-11716bd22f01\",\"_airbyte_emitted_at\":1647026803000,\"_airbyte_data\":{\"date\":\"2022-03-10\",\"stock_ticker\":\"TSLA\",\"price\":838.3}}`\nCongratulations! We've successfully written a fully functioning Airbyte connector. You're an Airbyte contributor now ;)\nArmed with the knowledge you gained in this guide, here are some places you can go from here:\n\nImplement Incremental Sync for your connector (described in the sections below)\nImplement another connector using the language specific helpers listed below\nWhile not required, we love contributions! if you end up creating a new connector, we're here to help you make it available to everyone using Airbyte. Remember that you're never expected to maintain a connector by yourself if you merge it to Airbyte -- we're committed to supporting connectors if you can't do it yourself\n\nOptional additions\nThis section is not yet complete and will be completed soon. Please reach out to us on Slack or Github if you need the information promised by these sections immediately.\nIncremental sync\nFollow the next tutorial to implement incremental sync.\nConnector Development Kit\nLike we mention at the beginning of the tutorial, this guide is meant more for understanding than as a blueprint for implementing production connectors. See the Connector Development Kit for the frameworks you should use to build production-ready connectors.\nLanguage specific helpers\n\nBuilding a Python Source\nBuilding a Python Destination\n",
    "tag": "airbyte"
  },
  {
    "title": "Profile Java Connector Memory Usage",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/tutorials/profile-java-connector-memory.md",
    "content": "Profile Java Connector Memory Usage\nThis tutorial demos how to profile the memory usage of a Java connector with Visual VM. Such profiling can be useful when we want to debug memory leaks, or optimize the connector's memory footprint.\nThe example focuses on docker deployment, because it is more straightforward. It is also possible to apply the same procedure to Kubernetes deployments.\nPrerequisite\n\nDocker running locally.\nVisualVM preinstalled.\n\nStep-by-Step\n\n\nEnable JMX in `airbyte-integrations/connectors/<connector-name>/build.gradle`, and expose it on port 6000. The port is chosen arbitrary, and can be port number that's available.\n\n`<connector-name>` examples: `source-mysql`, `source-github`,  `destination-snowflake`.\n\n```groovy\napplication {\n  mainClass = 'io.airbyte.integrations.'\n  applicationDefaultJvmArgs = [\n    '-XX:+ExitOnOutOfMemoryError',\n    '-XX:MaxRAMPercentage=75.0',\n\n\n```// add the following JVM arguments to enable JMX:\n'-XX:NativeMemoryTracking=detail',\n'-XX:+UsePerfData',\n'-Djava.rmi.server.hostname=localhost',\n'-Dcom.sun.management.jmxremote=true',\n'-Dcom.sun.management.jmxremote.port=6000',\n\"-Dcom.sun.management.jmxremote.rmi.port=6000\",\n'-Dcom.sun.management.jmxremote.local.only=false',\n'-Dcom.sun.management.jmxremote.authenticate=false',\n'-Dcom.sun.management.jmxremote.ssl=false',\n\n// optionally, add a max heap size to limit the memory usage\n'-Xmx2000m',\n```\n\n\n]\n   }\n   ```\n\n\nModify `airbyte-integrations/connectors/<connector-name>/Dockerfile` to expose the JMX port.\n```dockerfile\n// optionally install procps to enable the ps command in the connector container\nRUN apt-get update && apt-get install -y procps && rm -rf /var/lib/apt/lists/*\n// expose the same JMX port specified in the previous step\nEXPOSE 6000\n```\n\n\nExpose the same port in `airbyte-workers/src/main/java/io/airbyte/workers/process/DockerProcessFactory.java`.\n`java\n// map local 6000 to the JMX port from the container\nif (imageName.startsWith(\"airbyte/<connector-name>\")) {\n  LOGGER.info(\"Exposing image {} port 6000\", imageName);\n  cmd.add(\"-p\");\n  cmd.add(\"6000:6000\");\n}`\nDisable the host network mode by removing the following code block in the same file. This is necessary because under the `host` network mode, published ports are discarded.\n`java\nif (networkName != null) {\n  cmd.add(\"--network\");\n  cmd.add(networkName);\n}`\n\n\n(This commit can be used as a reference. It reverts them. So just do the opposite.)\n\n\nBuild and launch Airbyte locally. It is necessary to build it because we have modified the `DockerProcessFactory.java`.\n`sh\nSUB_BUILD=PLATFORM ./gradlew build -x test\nVERSION=dev docker compose up`\n\n\nBuild the connector to be profiled locally. It will create a `dev` version local image: `airbyte/<connector-name>:dev`.\n`sh\n./gradlew :airbyte-integrations:connectors:<connector-name>:airbyteDocker`\n\n\nConnect to the launched local Airbyte server at `localhost:8000`, go to the `Settings` page, and change the version of the connector to be profiled to `dev` which was just built in the previous step.\n\n\nCreate a connection using the connector to be profiled.\n\nThe `Replication frequency` of this connector should be `manual` so that we can control when it starts.\nWe can use the e2e test connectors as either the source or destination for convenience.\nThe e2e test connectors are usually very reliable, and requires little configuration.\nFor example, if we are profiling a source connector, create an e2e test destination at the other end of the connection.\n\n\n\nProfile the connector in question.\n\nLaunch a data sync run.\nAfter the run starts, open Visual VM, and click `File` / `Add JMX Connection...`. A modal will show up. Type in `localhost:6000`, and click `OK`.\nNow we can see a new connection shows up under the `Local` category on the left, and the information about the connector's JVM gets retrieved.\n\n\n",
    "tag": "airbyte"
  },
  {
    "title": "Building a Python Source",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/tutorials/building-a-python-source.md",
    "content": "Building a Python Source\nSummary\nThis article provides a checklist for how to create a python source. Each step in the checklist has a link to a more detailed explanation below.\nRequirements\nDocker, Python, and Java with the versions listed in the tech stack section.\n:::info\nAll the commands below assume that `python` points to a version of python >3.7. On some systems, `python` points to a Python2 installation and `python3` points to Python3. If this is the case on your machine, substitute all `python` commands in this guide with `python3` . Otherwise, make sure to install Python 3 before beginning.\n:::\nChecklist\nCreating a Source\n\nStep 1: Create the source using template\nStep 2: Build the newly generated source \nStep 3: Set up your Airbyte development environment \nStep 4: Implement `spec` (and define the specification for the source `airbyte-integrations/connectors/source-<source-name>/spec.yaml`)\nStep 5: Implement `check`\nStep 6: Implement `discover`\nStep 7: Implement `read`\nStep 8: Set up Standard Tests\nStep 9: Write unit tests or integration tests\nStep 10: Update the `README.md` (If API credentials are required to run the integration, please document how they can be obtained or link to a how-to guide.)\nStep 11: Add the connector to the API/UI (by adding an entry in `airbyte-config/init/src/main/resources/seed/source_definitions.yaml`)\nStep 12: Add docs (in `docs/integrations/sources/<source-name>.md`)\n\n:::info\nEach step of the Creating a Source checklist is explained in more detail below.\n:::\n:::info\nAll `./gradlew` commands must be run from the root of the airbyte project.\n:::\nSubmitting a Source to Airbyte\n\nIf you need help with any step of the process, feel free to submit a PR with your progress and any questions you have. \nSubmit a PR.\nTo run integration tests, Airbyte needs access to a test account/environment. Coordinate with an Airbyte engineer (via the PR) to add test credentials so that we can run tests for the integration in the CI. (We will create our own test account once you let us know what source we need to create it for.)\nOnce the config is stored in Github Secrets, edit `.github/workflows/test-command.yml` and `.github/workflows/publish-command.yml` to inject the config into the build environment.\nEdit the `airbyte/tools/bin/ci_credentials.sh` script to pull the script from the build environment and write it to `secrets/config.json` during the build.\n\n:::info\nIf you have a question about a step the Submitting a Source to Airbyte checklist include it in your PR or ask it on slack.\n:::\nExplaining Each Step\nStep 1: Create the source using template\nAirbyte provides a code generator which bootstraps the scaffolding for our connector.\n`bash\n$ cd airbyte-integrations/connector-templates/generator # assumes you are starting from the root of the Airbyte project.\n$ ./generate.sh`\nSelect the `python` template and then input the name of your connector. For this walk through we will refer to our source as `example-python`\nStep 2: Build the newly generated source\nBuild the source by running:\n`text\ncd airbyte-integrations/connectors/source-<name>\npython -m venv .venv # Create a virtual environment in the .venv directory\nsource .venv/bin/activate # enable the venv\npip install -r requirements.txt`\nThis step sets up the initial python environment. All subsequent `python` or `pip` commands assume you have activated your virtual environment.\nStep 3: Set up your Airbyte development environment\nThe generator creates a file `source_<source_name>/source.py`. This will be where you implement the logic for your source. The templated `source.py` contains extensive comments explaining each method that needs to be implemented. Briefly here is an overview of each of these methods.\n\n`spec`: declares the user-provided credentials or configuration needed to run the connector\n`check`: tests if with the user-provided configuration the connector can connect with the underlying data source.\n`discover`: declares the different streams of data that this connector can output\n`read`: reads data from the underlying data source (The stock ticker API)\n\nDependencies\nPython dependencies for your source should be declared in `airbyte-integrations/connectors/source-<source-name>/setup.py` in the `install_requires` field. You will notice that a couple of Airbyte dependencies are already declared there. Do not remove these; they give your source access to the helper interface that is provided by the generator.\nYou may notice that there is a `requirements.txt` in your source's directory as well. Do not touch this. It is autogenerated and used to provide Airbyte dependencies. All your dependencies should be declared in `setup.py`.\nDevelopment Environment\nThe commands we ran above created a virtual environment for your source. If you want your IDE to auto complete and resolve dependencies properly, point it at the virtual env `airbyte-integrations/connectors/source-<source-name>/.venv`. Also anytime you change the dependencies in the `setup.py` make sure to re-run the build command. The build system will handle installing all dependencies in the `setup.py` into the virtual environment.\nPretty much all it takes to create a source is to implement the `Source` interface. The template fills in a lot of information for you and has extensive docstrings describing what you need to do to implement each method. The next 4 steps are just implementing that interface.\n:::info\nAll logging should be done through the `logger` object passed into each method. Otherwise, logs will not be shown in the Airbyte UI.\n:::\nIterating on your implementation\nEveryone develops differently but here are 3 ways that we recommend iterating on a source. Consider using whichever one matches your style.\nRun the source using python\nYou'll notice in your source's directory that there is a python file called `main.py`. This file exists as convenience for development. You can call it from within the virtual environment mentioned above `. ./.venv/bin/activate` to test out that your source works.\n```text\nfrom airbyte-integrations/connectors/source-\npython main.py spec\npython main.py check --config secrets/config.json\npython main.py discover --config secrets/config.json\npython main.py read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\nThe nice thing about this approach is that you can iterate completely within in python. The downside is that you are not quite running your source as it will actually be run by Airbyte. Specifically you're not running it from within the docker container that will house it.\nRun the source using docker\nIf you want to run your source exactly as it will be run by Airbyte (i.e. within a docker container), you can use the following commands from the connector module directory (`airbyte-integrations/connectors/source-example-python`):\n```text\nFirst build the container\ndocker build . -t airbyte/source-example-python:dev\nThen use the following commands to run it\ndocker run --rm airbyte/source-example-python:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-example-python:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-example-python:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/sample_files:/sample_files airbyte/source-example-python:dev read --config /secrets/config.json --catalog /sample_files/configured_catalog.json\n```\nNote: Each time you make a change to your implementation you need to re-build the connector image. `docker build . -t airbyte/source-example-python:dev`. This ensures the new python code is added into the docker container.\nThe nice thing about this approach is that you are running your source exactly as it will be run by Airbyte. The tradeoff is that iteration is slightly slower, because you need to re-build the connector between each change.\nDetailed Debug Messages\nDuring development of your connector, you can enable the printing of detailed debug information during a sync by specifying the `--debug` flag. This will allow you to get a better picture of what is happening during each step of your sync.\n`text\npython main.py read --config secrets/config.json --catalog sample_files/configured_catalog.json --debug`\nIn addition to the preset CDK debug statements, you can also emit custom debug information from your connector by introducing your own debug statements:\n`python\nself.logger.debug(\n    \"your debug message here\",\n    extra={\n        \"debug_field\": self.value,\n        \"custom_field\": your_object.field\n    }\n)`\nTDD using standard tests\nAirbyte provides a standard test suite that is run against every source. The objective of these tests is to provide some \"free\" tests that can sanity check that the basic functionality of the source works. One approach to developing your connector is to simply run the tests between each change and use the feedback from them to guide your development.\nIf you want to try out this approach, check out Step 8 which describes what you need to do to set up the standard tests for your source.\nThe nice thing about this approach is that you are running your source exactly as Airbyte will run it in the CI. The downside is that the tests do not run very quickly.\nStep 4: Implement `spec`\nEach source contains a specification that describes what inputs it needs in order for it to pull data. This file can be found in `airbyte-integrations/connectors/source-<source-name>/spec.yaml`. This is a good place to start when developing your source. Using JsonSchema define what the inputs are (e.g. username and password). Here's an example of what the `spec.yaml` looks like for the stripe source.\nFor more details on what the spec is, you can read about the Airbyte Protocol here.\nThe generated code that Airbyte provides, handles implementing the `spec` method for you. It assumes that there will be a file called `spec.yaml` in the same directory as `source.py`. If you have declared the necessary JsonSchema in `spec.yaml` you should be done with this step.\nStep 5: Implement `check`\nAs described in the template code, this method takes in a json object called config that has the values described in the `spec.yaml` filled in. In other words if the `spec.yaml` said that the source requires a `username` and `password` the config object might be `{ \"username\": \"airbyte\", \"password\": \"password123\" }`. It returns a json object that reports, given the credentials in the config, whether we were able to connect to the source. For example, with the given credentials could the source connect to the database server.\nWhile developing, we recommend storing this object in `secrets/config.json`. The `secrets` directory is gitignored by default.\nStep 6: Implement `discover`\nAs described in the template code, this method takes in the same config object as `check`. It then returns a json object called a `catalog` that describes what data is available and metadata on what options are available for how to replicate it.\nFor a brief overview on the catalog check out Beginner's Guide to the Airbyte Catalog.\nStep 7: Implement `read`\nAs described in the template code, this method takes in the same config object as the previous methods. It also takes in a \"configured catalog\". This object wraps the catalog emitted by the `discover` step and includes configuration on how the data should be replicated. For a brief overview on the configured catalog check out Beginner's Guide to the Airbyte Catalog. It then returns a generator which returns each record in the stream.\nStep 8: Set up Standard Tests\nThe Standard Tests are a set of tests that run against all sources. These tests are run in the Airbyte CI to prevent regressions. They also can help you sanity check that your source works as expected. The following article explains Standard Tests and how to run them.\nYou can run the tests using `./gradlew :airbyte-integrations:connectors:source-<source-name>:integrationTest`. Make sure to run this command from the Airbyte repository root.\n:::info\nIn some rare cases we make exceptions and allow a source to not need to pass all the standard tests. If for some reason you think your source cannot reasonably pass one of the tests cases, reach out to us on github or slack, and we can determine whether there's a change we can make so that the test will pass or if we should skip that test for your source.\n:::\nStep 9: Write unit tests and/or integration tests\nThe Standard Tests are meant to cover the basic functionality of a source. Think of it as the bare minimum required for us to add a source to Airbyte. In case you need to test additional functionality of your source, write unit or integration tests.\nUnit Tests\nAdd any relevant unit tests to the `unit_tests` directory. Unit tests should not depend on any secrets.\nYou can run the tests using `python -m pytest -s unit_tests`\nIntegration Tests\nPlace any integration tests in the `integration_tests` directory such that they can be discovered by pytest.\nRun integration tests using `python -m pytest -s integration_tests`.\nStep 10: Update the `README.md`\nThe template fills in most of the information for the readme for you. Unless there is a special case, the only piece of information you need to add is how one can get the credentials required to run the source. e.g. Where one can find the relevant API key, etc.\nStep 11: Add the connector to the API/UI\nOpen the following file: `airbyte-config/init/src/main/resources/seed/source_definitions.yaml`. You'll find a list of all the connectors that Airbyte displays in the UI. Pattern match to add your own connector. Make sure to generate a new unique UUIDv4 for the `sourceDefinitionId` field. You can get one here. Note that modifications to source_definitions.yaml will only be picked-up the first time you start Airbyte, or when you upgrade Airbyte, or if you entirely wipe our your instance of Airbyte and start from scratch.\nNote that for simple and quick testing use cases, you can also do this step using the UI.\nStep 12: Add docs\nEach connector has its own documentation page. By convention, that page should have the following path: in `docs/integrations/sources/<source-name>.md`. For the documentation to get packaged with the docs, make sure to add a link to it in `docs/SUMMARY.md`. You can pattern match doing that from existing connectors.\nRelated tutorials\nFor additional examples of how to use the Python CDK to build an Airbyte source connector, see the following tutorials:\n- Python CDK Speedrun: Creating a Source",
    "tag": "airbyte"
  },
  {
    "title": "Adding Incremental Sync to a Source",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/tutorials/adding-incremental-sync.md",
    "content": "Adding Incremental Sync to a Source\nOverview\nThis tutorial will assume that you already have a working source. If you do not, feel free to refer to the Building a Toy Connector tutorial. This tutorial will build directly off the example from that article. We will also assume that you have a basic understanding of how Airbyte's Incremental-Append replication strategy works. We have a brief explanation of it here.\nUpdate Catalog in `discover`\nFirst we need to identify a given stream in the Source as supporting incremental. This information is declared in the catalog that the `discover` method returns. You will notice in the stream object contains a field called `supported_sync_modes`. If we are adding incremental to an existing stream, we just need to add `\"incremental\"` to that array. This tells Airbyte that this stream can either be synced in an incremental fashion. In practice, this will mean that in the UI, a user will have the ability to configure this type of sync.\nIn the example we used in the Toy Connector tutorial, the `discover` method would not look like this. Note: that \"incremental\" has been added to the `supported_sync_modes` array. We also set `source_defined_cursor` to `True` and `default_cursor_field` to `[\"date\"]` to declare that the Source knows what field to use for the cursor, in this case the date field, and does not require user input. Nothing else has changed.\n`python\ndef discover():\n    catalog = {\n        \"streams\": [{\n            \"name\": \"stock_prices\",\n            \"supported_sync_modes\": [\"full_refresh\", \"incremental\"],\n            \"source_defined_cursor\": True,\n            \"default_cursor_field\": [\"date\"],\n            \"json_schema\": {\n                \"properties\": {\n                    \"date\": {\n                        \"type\": \"string\"\n                    },\n                    \"price\": {\n                        \"type\": \"number\"\n                    },\n                    \"stock_ticker\": {\n                        \"type\": \"string\"\n                    }\n                }\n            }\n        }]\n    }\n    airbyte_message = {\"type\": \"CATALOG\", \"catalog\": catalog}\n    print(json.dumps(airbyte_message))`\nAlso, create a file called `incremental_configured_catalog.json` with the following content:\n`javascript\n{\n    \"streams\": [\n        {\n            \"stream\": {\n                \"name\": \"stock_prices\",\n                \"supported_sync_modes\": [\n                    \"full_refresh\",\n                    \"incremental\"\n                ],\n                \"json_schema\": {\n                    \"properties\": {\n                        \"date\": {\n                            \"type\": \"string\"\n                        },\n                        \"price\": {\n                            \"type\": \"number\"\n                        },\n                        \"stock_ticker\": {\n                            \"type\": \"string\"\n                        }\n                    }\n                }\n            },\n            \"sync_mode\": \"incremental\",\n            \"destination_sync_mode\": \"overwrite\"\n        }\n    ]\n}`\nUpdate `read`\nNext we will adapt the `read` method that we wrote previously. We need to change three things.\nFirst, we need to pass it information about what data was replicated in the previous sync. In Airbyte this is called a `state` object. The structure of the state object is determined by the Source. This means that each Source can construct a state object that makes sense to it and does not need to worry about adhering to any other convention. That being said, a pretty typical structure for a state object is a map of stream name to the last value in the cursor field for that stream.\nIn this case we might choose something like this:\n`javascript\n{\n  \"stock_prices\": {\n    \"date\": \"2020-02-01\"\n  }\n}`\nThe second change we need to make to the `read` method is to use the state object so that we only emit new records. \nLastly, we need to emit an updated state object, so that the next time this Source runs we do not resend messages that we have already sent.\nHere's what our updated `read` method would look like.\n```python\ndef read(config, catalog, state):\n    # Assert required configuration was provided\n    if \"api_key\" not in config or \"stock_ticker\" not in config:\n        log_error(\"Input config must contain the properties 'api_key' and 'stock_ticker'\")\n        sys.exit(1)\n\n\n```# Find the stock_prices stream if it is present in the input catalog\nstock_prices_stream = None\nfor configured_stream in catalog[\"streams\"]:\n    if configured_stream[\"stream\"][\"name\"] == \"stock_prices\":\n        stock_prices_stream = configured_stream\n\nif stock_prices_stream is None:\n    log_error(\"No streams selected\")\n    return\n\n# By default we fetch stock prices for the 7 day period ending with today\ntoday = date.today()\ncursor_value = today.strftime(\"%Y-%m-%d\")\nfrom_day = (today - timedelta(days=7)).strftime(\"%Y-%m-%d\")\n\n# In case of incremental sync, state should contain the last date when we fetched stock prices\nif stock_prices_stream[\"sync_mode\"] == \"incremental\":\n    if state and \"stock_prices\" in state and state[\"stock_prices\"].get(\"date\"):\n        from_date = datetime.strptime(state[\"stock_prices\"].get(\"date\"), \"%Y-%m-%d\")\n        from_day = (from_date + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n\n# If the state indicates that we have already ran the sync up to cursor_value, we can skip the sync\nif cursor_value > from_day:\n    # If we've made it this far, all the configuration is good and we can pull the market data\n    response = _call_api(ticker=config[\"stock_ticker\"], token = config[\"api_key\"], from_day=from_day, to_day=cursor_value)\n    if response.status_code != 200:\n        # In a real scenario we'd handle this error better :)\n        log_error(\"Failure occurred when calling Polygon.io API\")\n        sys.exit(1)\n    else:\n        # Stock prices are returned sorted by date in ascending order\n        # We want to output them one by one as AirbyteMessages\n        response_json = response.json()\n        if response_json[\"resultsCount\"] > 0:\n            results = response_json[\"results\"]\n            for result in results:\n                data = {\"date\": datetime.fromtimestamp(result[\"t\"]/1000, tz=timezone.utc).strftime(\"%Y-%m-%d\"), \"stock_ticker\": config[\"stock_ticker\"], \"price\": result[\"c\"]}\n                record = {\"stream\": \"stock_prices\", \"data\": data, \"emitted_at\": int(datetime.now().timestamp()) * 1000}\n                output_message = {\"type\": \"RECORD\", \"record\": record}\n                print(json.dumps(output_message))\n\n                # We update the cursor as we print out the data, so that next time sync starts where we stopped printing out results\n                if stock_prices_stream[\"sync_mode\"] == \"incremental\":\n                    cursor_value = datetime.fromtimestamp(results[len(results)-1][\"t\"]/1000, tz=timezone.utc).strftime(\"%Y-%m-%d\")\n\n# Emit new state message.\nif stock_prices_stream[\"sync_mode\"] == \"incremental\":\n    output_message = {\"type\": \"STATE\", \"state\": {\"data\": {\"stock_prices\": {\"date\": cursor_value}}}}\n    print(json.dumps(output_message))\n```\n\n\n```\nThat code requires to add a new library import in the `source.py` file:\n`python\nfrom datetime import timezone`\nWe will also need to parse `state` argument in the `run` method. In order to do that, we will modify the code that \ncalls `read` method from `run` method:\n```python\n    elif command == \"read\":\n        config = read_json(get_input_file_path(parsed_args.config))\n        configured_catalog = read_json(get_input_file_path(parsed_args.catalog))\n        state = None\n        if parsed_args.state:\n            state = read_json(get_input_file_path(parsed_args.state))\n\n\n```    read(config, configured_catalog, state)\n```\n\n\n`Finally, we need to pass more arguments to our `_call_api` method in order to fetch only new prices for incremental sync:`python\ndef _call_api(ticker, token, from_day, to_day):\n    return requests.get(f\"https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/{from_day}/{to_day}?sort=asc&limit=120&apiKey={token}\")\n```\nYou will notice that in order to test these changes you need a `state` object. If you run an incremental sync\nwithout passing a state object, the new code will output a state object that you can use with the next sync. If you run this:\n`bash\npython source.py read --config secrets/valid_config.json --catalog incremental_configured_catalog.json`\nThe output will look like following:\n`bash\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"stock_prices\", \"data\": {\"date\": \"2022-03-07\", \"stock_ticker\": \"TSLA\", \"price\": 804.58}, \"emitted_at\": 1647294277000}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"stock_prices\", \"data\": {\"date\": \"2022-03-08\", \"stock_ticker\": \"TSLA\", \"price\": 824.4}, \"emitted_at\": 1647294277000}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"stock_prices\", \"data\": {\"date\": \"2022-03-09\", \"stock_ticker\": \"TSLA\", \"price\": 858.97}, \"emitted_at\": 1647294277000}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"stock_prices\", \"data\": {\"date\": \"2022-03-10\", \"stock_ticker\": \"TSLA\", \"price\": 838.3}, \"emitted_at\": 1647294277000}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"stock_prices\", \"data\": {\"date\": \"2022-03-11\", \"stock_ticker\": \"TSLA\", \"price\": 795.35}, \"emitted_at\": 1647294277000}}\n{\"type\": \"STATE\", \"state\": {\"data\": {\"stock_prices\": {\"date\": \"2022-03-11\"}}}}`\nNotice that the last line of output is the state object. Copy the state object:\n`json\n{\"stock_prices\": {\"date\": \"2022-03-11\"}}`\nand paste it into a new file (i.e. `state.json`). Now you can run an incremental sync:\n`bash\npython source.py read --config secrets/valid_config.json --catalog incremental_configured_catalog.json --state state.json`\nRun the incremental tests\nThe Source Acceptance Test (SAT) suite also includes test cases to ensure that incremental mode is working correctly. \nTo enable these tests, modify the existing `acceptance-test-config.yml` by adding the following:\n`yaml\n  incremental:\n    - config_path: \"secrets/valid_config.json\"\n      configured_catalog_path: \"incremental_configured_catalog.json\"\n      future_state_path: \"abnormal_state.json\"`\nYour full `acceptance-test-config.yml` should look something like this:\n```yaml\nSee Connector Acceptance Tests\nfor more information about how to configure these tests\nconnector_image: airbyte/source-stock-ticker-api:dev\ntests:\n  spec:\n    - spec_path: \"spec.json\"\n      config_path: \"secrets/valid_config.json\"\n  connection:\n    - config_path: \"secrets/valid_config.json\"\n      status: \"succeed\"\n    - config_path: \"secrets/invalid_config.json\"\n      status: \"failed\"\n  discovery:\n    - config_path: \"secrets/valid_config.json\"\n  basic_read:\n    - config_path: \"secrets/valid_config.json\"\n      configured_catalog_path: \"fullrefresh_configured_catalog.json\"\n      empty_streams: []\n  full_refresh:\n    - config_path: \"secrets/valid_config.json\"\n      configured_catalog_path: \"fullrefresh_configured_catalog.json\"\n  incremental:\n    - config_path: \"secrets/valid_config.json\"\n      configured_catalog_path: \"incremental_configured_catalog.json\"\n      future_state_path: \"abnormal_state.json\"\n```\nYou will also need to create an `abnormal_state.json` file with a date in the future, which should not produce any records:\n`{\"stock_prices\": {\"date\": \"2121-01-01\"}}`\nAnd lastly you need to modify the `check` function call to include the new parameters `from_day` and `to_day` in `source.py`:\n```python\ndef check(config):\n    # Validate input configuration by attempting to get the daily closing prices of the input stock ticker\n    response = _call_api(ticker=config[\"stock_ticker\"], token=config[\"api_key\"], from_day=datetime.now().date()-timedelta(days=1), to_day=datetime.now().date())\n    if response.status_code == 200:\n        result = {\"status\": \"SUCCEEDED\"}\n    elif response.status_code == 403:\n        # HTTP code 403 means authorization failed so the API key is incorrect\n        result = {\"status\": \"FAILED\", \"message\": \"API Key is incorrect.\"}\n    else:\n        result = {\"status\": \"FAILED\", \"message\": \"Input configuration is incorrect. Please verify the input stock ticker and API key.\"}\n\n\n```output_message = {\"type\": \"CONNECTION_STATUS\", \"connectionStatus\": result}\nprint(json.dumps(output_message))\n```\n\n\n```\nRun the tests once again:\n`bash\n./acceptance-test-docker.sh`\nAnd finally, you should see a successful test summary:\n```\ncollecting ... \n test_core.py \u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713                                                                                                                                                                                         86% \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b \n test_full_refresh.py \u2713                                                                                                                                                                                                   91% \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f\n test_incremental.py \u2713\u2713                                                                                                                                                                                                  100% \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\nResults (8.90s):\n      22 passed\n```\nThat's all you need to do to add incremental functionality to the stock ticker Source.\nYou can deploy the new version of your connector simply by running:\n`bash\n./gradlew clean :airbyte-integrations:connectors:source-stock-ticker-api:build`\nBonus points: go to Airbyte UI and reconfigure the connection to use incremental sync.\nIncremental definitely requires more configurability than full refresh, so your implementation may deviate slightly depending on whether your cursor\nfield is source defined or user-defined. If you think you are running into one of those cases, check out \nour incremental documentation for more information on different types of\nconfiguration.",
    "tag": "airbyte"
  },
  {
    "title": "Python CDK Speedrun: Creating a Source",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/tutorials/cdk-speedrun-deprecated.md",
    "content": "Python CDK Speedrun: Creating a Source\nCDK Speedrun (HTTP API Source Creation Any% Route)\nThis is a blazing fast guide to building an HTTP source connector. Think of it as the TL;DR version of this tutorial.\nDependencies\n\nPython >= 3.9\nDocker\nNodeJS\n\nGenerate the Template\n`bash\n$ cd airbyte-integrations/connector-templates/generator # start from repo root\n$ ./generate.sh`\nSelect the `Python HTTP API Source` and name it `python-http-example`.\nCreate Dev Environment\n`bash\ncd ../../connectors/source-python-http-example\npython -m venv .venv # Create a virtual environment in the .venv directory\nsource .venv/bin/activate\npip install -r requirements.txt`\nDefine Connector Inputs\n`bash\ncd source_python_http_example`\nWe're working with the Exchange Rates API, so we need to define our input schema to reflect that. Open the `spec.json` file here and replace it with:\n`javascript\n{\n  \"documentationUrl\": \"https://docs.airbyte.io/integrations/sources/exchangeratesapi\",\n  \"connectionSpecification\": {\n    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n    \"title\": \"Python Http Example Spec\",\n    \"type\": \"object\",\n    \"required\": [\"start_date\", \"currency_base\"],\n    \"properties\": {\n      \"start_date\": {\n        \"type\": \"string\",\n        \"description\": \"Start getting data from that date.\",\n        \"pattern\": \"^[0-9]{4}-[0-9]{2}-[0-9]{2}$\",\n        \"examples\": [\"%Y-%m-%d\"]\n      },\n      \"base\": {\n        \"type\": \"string\",\n        \"examples\": [\"USD\", \"EUR\"],\n        \"description\": \"ISO reference currency. See <a href=\\\"https://www.ecb.europa.eu/stats/policy_and_exchange_rates/euro_reference_exchange_rates/html/index.en.html\\\">here</a>.\"\n      }\n    }\n  }\n}`\nOk, let's write a function that checks the inputs we just defined. Nuke the `source.py` file. Now add this code to it. For a crucial time skip, we're going to define all the imports we need in the future here.\n```python\nfrom datetime import datetime, timedelta\nfrom typing import Any, Iterable, List, Mapping, MutableMapping, Optional, Tuple\nimport requests\nfrom airbyte_cdk.sources import AbstractSource\nfrom airbyte_cdk.sources.streams import Stream\nfrom airbyte_cdk.sources.streams.http import HttpStream\nfrom airbyte_cdk.sources.streams.http.auth import NoAuth\nclass SourcePythonHttpExample(AbstractSource):\n    def check_connection(self, logger, config) -> Tuple[bool, any]:\n        accepted_currencies = {\n            \"USD\",\n            \"JPY\",\n            \"BGN\",\n            \"CZK\",\n            \"DKK\",\n        }  # there are more currencies but let's assume these are the only allowed ones\n        input_currency = config[\"base\"]\n        if input_currency not in accepted_currencies:\n            return False, f\"Input currency {input_currency} is invalid. Please input one of the following currencies: {accepted_currencies}\"\n        else:\n            return True, None\n\n\n```def streams(self, config: Mapping[str, Any]) -> List[Stream]:\n    # Parse the date from a string into a datetime object.\n    start_date = datetime.strptime(config[\"start_date\"], \"%Y-%m-%d\")\n\n    # NoAuth just means there is no authentication required for this API and is included for completeness.\n    # Skip passing an authenticator if no authentication is required.\n    # Other authenticators are available for API token-based auth and Oauth2. \n    auth = NoAuth()\n    return [ExchangeRates(authenticator=auth, base=config[\"base\"], start_date=start_date)]\n```\n\n\n```\nTest it.\n`bash\ncd ..\nmkdir sample_files\necho '{\"start_date\": \"2021-04-01\", \"base\": \"USD\"}'  > sample_files/config.json\necho '{\"start_date\": \"2021-04-01\", \"base\": \"BTC\"}'  > sample_files/invalid_config.json\npython main.py check --config sample_files/config.json\npython main.py check --config sample_files/invalid_config.json`\nExpected output:\n```text\n\npython main.py check --config sample_files/config.json\n{\"type\": \"CONNECTION_STATUS\", \"connectionStatus\": {\"status\": \"SUCCEEDED\"}}\npython main.py check --config sample_files/invalid_config.json\n{\"type\": \"CONNECTION_STATUS\", \"connectionStatus\": {\"status\": \"FAILED\", \"message\": \"Input currency BTC is invalid. Please input one of the following currencies: {'DKK', 'USD', 'CZK', 'BGN', 'JPY'}\"}}\n```\n\nDefine your Stream\nIn your `source.py` file, add this `ExchangeRates` class. This stream represents an endpoint you want to hit.\n```python\nfrom airbyte_cdk.sources.streams.http import HttpStream\nclass ExchangeRates(HttpStream):\n    url_base = \"https://api.exchangeratesapi.io/\"\n\n\n```# Set this as a noop.\nprimary_key = None\n\ndef next_page_token(self, response: requests.Response) -> Optional[Mapping[str, Any]]:\n    # The API does not offer pagination, so we return None to indicate there are no more pages in the response\n    return None\n\ndef path(\n    self, \n) -> str:\n    return \"\"  # TODO\n\ndef parse_response(\n    self,\n) -> Iterable[Mapping]:\n    return None  # TODO\n```\n\n\n```\nNow download this file. Name it `exchange_rates.json` and place it in `/source_python_http_example/schemas`. It defines your output schema.\nTest your discover function. You should receive a fairly large JSON object in return.\n`bash\npython main.py discover --config sample_files/config.json`\nReading Data from the Source\nUpdate your `ExchangeRates` class to implement the required functions as follows:\n```python\nclass ExchangeRates(HttpStream):\n    url_base = \"https://api.exchangeratesapi.io/\"\n\n\n```primary_key = None\n\ndef __init__(self, base: str, **kwargs):\n    super().__init__()\n    self.base = base\n\n\ndef path(\n        self,\n        stream_state: Mapping[str, Any] = None,\n        stream_slice: Mapping[str, Any] = None,\n        next_page_token: Mapping[str, Any] = None\n) -> str:\n    # The \"/latest\" path gives us the latest currency exchange rates\n    return \"latest\"\n\ndef request_params(\n        self,\n        stream_state: Mapping[str, Any],\n        stream_slice: Mapping[str, Any] = None,\n        next_page_token: Mapping[str, Any] = None,\n) -> MutableMapping[str, Any]:\n    # The api requires that we include the base currency as a query param so we do that in this method\n    return {'base': self.base}\n\ndef parse_response(\n        self,\n        response: requests.Response,\n        stream_state: Mapping[str, Any],\n        stream_slice: Mapping[str, Any] = None,\n        next_page_token: Mapping[str, Any] = None,\n) -> Iterable[Mapping]:\n    # The response is a simple JSON whose schema matches our stream's schema exactly, \n    # so we just return a list containing the response\n    return [response.json()]\n\ndef next_page_token(self, response: requests.Response) -> Optional[Mapping[str, Any]]:\n    # The API does not offer pagination, \n    # so we return None to indicate there are no more pages in the response\n    return None\n```\n\n\n```\nUpdate your `streams` method in your `SourcePythonHttpExample` class to use the currency base passed in from the stream above.\n`python\ndef streams(self, config: Mapping[str, Any]) -> List[Stream]:\n        auth = NoAuth()\n        return [ExchangeRates(authenticator=auth, base=config['base'])]`\nWe now need a catalog that defines all of our streams. We only have one, `ExchangeRates`. Download that file here. Place it in `/sample_files` named as `configured_catalog.json`.\nLet's read some data.\n`bash\npython main.py read --config sample_files/config.json --catalog sample_files/configured_catalog.json`\nIf all goes well, containerize it so you can use it in the UI:\n`bash\ndocker build . -t airbyte/source-python-http-example:dev`\nYou're done. Stop the clock :)",
    "tag": "airbyte"
  },
  {
    "title": "Step 1: Creating the Source",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/tutorials/cdk-tutorial-python-http/creating-the-source.md",
    "content": "Step 1: Creating the Source\nAirbyte provides a code generator which bootstraps the scaffolding for our connector.\n```bash\n$ cd airbyte-integrations/connector-templates/generator # assumes you are starting from the root of the Airbyte project.\nInstall NPM from https://www.npmjs.com/get-npm if you don't have it\n$ ./generate.sh\n```\nThis will bring up an interactive helper application. Use the arrow keys to pick a template from the list. Select the `Python HTTP API Source` template and then input the name of your connector. The application will create a new directory in airbyte/airbyte-integrations/connectors/ with the name of your new connector.\nFor this walk-through we will refer to our source as `python-http-example`. The finalized source code for this tutorial can be found here.\nThe source we will build in this tutorial will pull data from the Rates API, a free and open API which documents historical exchange rates for fiat currencies.",
    "tag": "airbyte"
  },
  {
    "title": "Step 5: Declare the Schema",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/tutorials/cdk-tutorial-python-http/declare-schema.md",
    "content": "Step 5: Declare the Schema\nThe `discover` method of the Airbyte Protocol returns an `AirbyteCatalog`: an object which declares all the streams output by a connector and their schemas. It also declares the sync modes supported by the stream (full refresh or incremental). See the catalog tutorial for more information.\nThis is a simple task with the Airbyte CDK. For each stream in our connector we'll need to: \n\nCreate a python `class` in `source.py` which extends `HttpStream`. \nPlace a `<stream_name>.json` file in the `source_<name>/schemas/` directory. The name of the file should be the snake_case name of the stream whose schema it describes, and its contents should be the JsonSchema describing the output from that stream.\n\nLet's create a class in `source.py` which extends `HttpStream`. You'll notice there are classes with extensive comments describing what needs to be done to implement various connector features. Feel free to read these classes as needed. But for the purposes of this tutorial, let's assume that we are adding classes from scratch either by deleting those generated classes or editing them to match the implementation below.\nWe'll begin by creating a stream to represent the data that we're pulling from the Exchange Rates API:\n```python\nclass ExchangeRates(HttpStream):\n    url_base = \"https://api.apilayer.com/exchangerates_data/\"\n\n\n```# Set this as a noop.\nprimary_key = None\n\ndef next_page_token(self, response: requests.Response) -> Optional[Mapping[str, Any]]:\n    # The API does not offer pagination, so we return None to indicate there are no more pages in the response\n    return None\n\ndef path(\n    self, \n    stream_state: Mapping[str, Any] = None, \n    stream_slice: Mapping[str, Any] = None, \n    next_page_token: Mapping[str, Any] = None\n) -> str:\n    return \"\"  # TODO\n\ndef parse_response(\n    self,\n    response: requests.Response,\n    stream_state: Mapping[str, Any],\n    stream_slice: Mapping[str, Any] = None,\n    next_page_token: Mapping[str, Any] = None,\n) -> Iterable[Mapping]:\n    return None  # TODO\n```\n\n\n```\nNote that this implementation is entirely empty -- we haven't actually done anything. We'll come back to this in the next step. But for now we just want to declare the schema of this stream. We'll declare this as a stream that the connector outputs by returning it from the `streams` method:\n```python\nfrom airbyte_cdk.sources.streams.http.auth import NoAuth\nclass SourcePythonHttpTutorial(AbstractSource):\n\n\n```def check_connection(self, logger, config) -> Tuple[bool, any]:\n    ...\n\ndef streams(self, config: Mapping[str, Any]) -> List[Stream]:\n    # NoAuth just means there is no authentication required for this API and is included for completeness.\n    # Skip passing an authenticator if no authentication is required.\n    # Other authenticators are available for API token-based auth and Oauth2. \n    auth = NoAuth()  \n    return [ExchangeRates(authenticator=auth)]\n```\n\n\n```\nHaving created this stream in code, we'll put a file `exchange_rates.json` in the `schemas/` folder. You can download the JSON file describing the output schema here for convenience and place it in `schemas/`.\nWith `.json` schema file in place, let's see if the connector can now find this schema and produce a valid catalog:\n`text\npython main.py discover --config secrets/config.json # this is not a mistake, the schema file is found by naming snake_case naming convention as specified above`\nyou should see some output like:\n`text\n{\"type\": \"CATALOG\", \"catalog\": {\"streams\": [{\"name\": \"exchange_rates\", \"json_schema\": {\"$schema\": \"http://json-schema.org/draft-04/schema#\", \"type\": \"object\", \"properties\": {\"base\": {\"type\": \"string\"}, \"rates\": {\"type\": \"object\", \"properties\": {\"GBP\": {\"type\": \"number\"}, \"HKD\": {\"type\": \"number\"}, \"IDR\": {\"type\": \"number\"}, \"PHP\": {\"type\": \"number\"}, \"LVL\": {\"type\": \"number\"}, \"INR\": {\"type\": \"number\"}, \"CHF\": {\"type\": \"number\"}, \"MXN\": {\"type\": \"number\"}, \"SGD\": {\"type\": \"number\"}, \"CZK\": {\"type\": \"number\"}, \"THB\": {\"type\": \"number\"}, \"BGN\": {\"type\": \"number\"}, \"EUR\": {\"type\": \"number\"}, \"MYR\": {\"type\": \"number\"}, \"NOK\": {\"type\": \"number\"}, \"CNY\": {\"type\": \"number\"}, \"HRK\": {\"type\": \"number\"}, \"PLN\": {\"type\": \"number\"}, \"LTL\": {\"type\": \"number\"}, \"TRY\": {\"type\": \"number\"}, \"ZAR\": {\"type\": \"number\"}, \"CAD\": {\"type\": \"number\"}, \"BRL\": {\"type\": \"number\"}, \"RON\": {\"type\": \"number\"}, \"DKK\": {\"type\": \"number\"}, \"NZD\": {\"type\": \"number\"}, \"EEK\": {\"type\": \"number\"}, \"JPY\": {\"type\": \"number\"}, \"RUB\": {\"type\": \"number\"}, \"KRW\": {\"type\": \"number\"}, \"USD\": {\"type\": \"number\"}, \"AUD\": {\"type\": \"number\"}, \"HUF\": {\"type\": \"number\"}, \"SEK\": {\"type\": \"number\"}}}, \"date\": {\"type\": \"string\"}}}, \"supported_sync_modes\": [\"full_refresh\"]}]}}`\nIt's that simple! Now the connector knows how to declare your connector's stream's schema. We declare only one stream since our source is simple, but the principle is exactly the same if you had many streams.\nYou can also dynamically define schemas, but that's beyond the scope of this tutorial. See the schema docs for more information.",
    "tag": "airbyte"
  },
  {
    "title": "Step 8: Test Connector",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/tutorials/cdk-tutorial-python-http/test-your-connector.md",
    "content": "Step 8: Test Connector\nUnit Tests\nAdd any relevant unit tests to the `unit_tests` directory. Unit tests should not depend on any secrets.\nYou can run the tests using `python -m pytest -s unit_tests`.\nIntegration Tests\nPlace any integration tests in the `integration_tests` directory such that they can be discovered by pytest.\nRun integration tests using `python -m pytest -s integration_tests`.\nMore information on integration testing can be found on the Testing Connectors doc.\nStandard Tests\nStandard tests are a fixed set of tests Airbyte provides that every Airbyte source connector must pass. While they're only required if you intend to submit your connector to Airbyte, you might find them helpful in any case. See Testing your connectors\nIf you want to submit this connector to become a default connector within Airbyte, follow steps 8 onwards from the Python source checklist",
    "tag": "airbyte"
  },
  {
    "title": "Step 7: Use the Connector in Airbyte",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/tutorials/cdk-tutorial-python-http/use-connector-in-airbyte.md",
    "content": "Step 7: Use the Connector in Airbyte\nTo use your connector in your own installation of Airbyte, build the docker image for your container by running `docker build . -t airbyte/source-python-http-example:dev`. Then, follow the instructions from the building a Python source tutorial for using the connector in the Airbyte UI, replacing the name as appropriate.\nNote: your built docker image must be accessible to the `docker` daemon running on the Airbyte node. If you're doing this tutorial locally, these instructions are sufficient. Otherwise you may need to push your Docker image to Dockerhub.",
    "tag": "airbyte"
  },
  {
    "title": "Step 6: Read Data",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/tutorials/cdk-tutorial-python-http/read-data.md",
    "content": "Step 6: Read Data\nDescribing schemas is good and all, but at some point we have to start reading data! So let's get to work. But before, let's describe what we're about to do:\nThe `HttpStream` superclass, like described in the concepts documentation, is facilitating reading data from HTTP endpoints. It contains built-in functions or helpers for:\n\nauthentication\npagination\nhandling rate limiting or transient errors\nand other useful functionality\n\nIn order for it to be able to do this, we have to provide it with a few inputs:\n\nthe URL base and path of the endpoint we'd like to hit\nhow to parse the response from the API\nhow to perform pagination\n\nOptionally, we can provide additional inputs to customize requests:\n\nrequest parameters and headers\nhow to recognize rate limit errors, and how long to wait (by default it retries 429 and 5XX errors using exponential backoff)\nHTTP method and request body if applicable\nconfigure exponential backoff policy\n\nBackoff policy options:\n\n`retry_factor` Specifies factor for exponential backoff policy (by default is 5)\n`max_retries` Specifies maximum amount of retries for backoff policy (by default is 5)\n`raise_on_http_errors` If set to False, allows opting-out of raising HTTP code exception (by default is True)\n\nThere are many other customizable options - you can find them in the airbyte_cdk.sources.streams.http.HttpStream class.\nSo in order to read data from the exchange rates API, we'll fill out the necessary information for the stream to do its work. First, we'll implement a basic read that just reads the last day's exchange rates, then we'll implement incremental sync using stream slicing.\nLet's begin by pulling data for the last day's rates by using the `/latest` endpoint:\n```python\nclass ExchangeRates(HttpStream):\n    url_base = \"https://api.apilayer.com/exchangerates_data/\"\n\n\n```primary_key = None\n\ndef __init__(self, config: Mapping[str, Any], **kwargs):\n    super().__init__()\n    self.base = config['base']\n    self.apikey = config['apikey']\n\n\ndef path(\n    self, \n    stream_state: Mapping[str, Any] = None, \n    stream_slice: Mapping[str, Any] = None, \n    next_page_token: Mapping[str, Any] = None\n) -> str:\n    # The \"/latest\" path gives us the latest currency exchange rates\n    return \"latest\"\n\ndef request_headers(\n    self, stream_state: Mapping[str, Any], stream_slice: Mapping[str, Any] = None, next_page_token: Mapping[str, Any] = None\n) -> Mapping[str, Any]:\n    # The api requires that we include apikey as a header so we do that in this method\n    return {'apikey': self.apikey}\n\ndef request_params(\n        self,\n        stream_state: Mapping[str, Any],\n        stream_slice: Mapping[str, Any] = None,\n        next_page_token: Mapping[str, Any] = None,\n) -> MutableMapping[str, Any]:\n    # The api requires that we include the base currency as a query param so we do that in this method\n    return {'base': self.base}\n\ndef parse_response(\n        self,\n        response: requests.Response,\n        stream_state: Mapping[str, Any],\n        stream_slice: Mapping[str, Any] = None,\n        next_page_token: Mapping[str, Any] = None,\n) -> Iterable[Mapping]:\n    # The response is a simple JSON whose schema matches our stream's schema exactly, \n    # so we just return a list containing the response\n    return [response.json()]\n\ndef next_page_token(self, response: requests.Response) -> Optional[Mapping[str, Any]]:\n    # The API does not offer pagination, \n    # so we return None to indicate there are no more pages in the response\n    return None\n```\n\n\n```\nThis may look big, but that's just because there are lots of (unused, for now) parameters in these methods (those can be hidden with Python's `**kwargs`, but don't worry about it for now). Really we just added a few lines of \"significant\" code: \n\nAdded a constructor `__init__` which stores the `base` currency to query for and the `apikey` used for authentication.\n`return {'base': self.base}` to add the `?base=<base-value>` query parameter to the request based on the `base` input by the user.\n`return {'apikey': self.apikey}` to add the header `apikey=<apikey-string>` to the request based on the `apikey` input by the user.\n`return [response.json()]` to parse the response from the API to match the schema of our schema `.json` file.\n`return \"latest\"` to indicate that we want to hit the `/latest` endpoint of the API to get the latest exchange rate data.\n\nLet's also pass the config specified by the user to the stream class:\n`python\n    def streams(self, config: Mapping[str, Any]) -> List[Stream]:\n        auth = NoAuth()\n        return [ExchangeRates(authenticator=auth, config=config)]`\nWe're now ready to query the API!\nTo do this, we'll need a ConfiguredCatalog. We've prepared one here -- download this and place it in `sample_files/configured_catalog.json`. Then run:\n`text\n python main.py read --config secrets/config.json --catalog sample_files/configured_catalog.json`\nyou should see some output lines, one of which is a record from the API:\n`text\n\"type\": \"RECORD\", \"record\": {\"stream\": \"exchange_rates\", \"data\": {\"success\": true, \"timestamp\": 1651129443, \"base\": \"EUR\", \"date\": \"2022-04-28\", \"rates\": {\"AED\": 3.86736, \"AFN\": 92.13195, \"ALL\": 120.627843, \"AMD\": 489.819318, \"ANG\": 1.910347, \"AOA\": 430.073735, \"ARS\": 121.119674, \"AUD\": 1.478877, \"AWG\": 1.895762, \"AZN\": 1.794932, \"BAM\": 1.953851, \"BBD\": 2.140212, \"BDT\": 91.662775, \"BGN\": 1.957013, \"BHD\": 0.396929, \"BIF\": 2176.669098, \"BMD\": 1.052909, \"BND\": 1.461004, \"BOB\": 7.298009, \"BRL\": 5.227798, \"BSD\": 1.060027, \"BTC\": 2.6717761e-05, \"BTN\": 81.165435, \"BWP\": 12.802036, \"BYN\": 3.565356, \"BYR\": 20637.011334, \"BZD\": 2.136616, \"CAD\": 1.349329, \"CDF\": 2118.452361, \"CHF\": 1.021627, \"CLF\": 0.032318, \"CLP\": 891.760584, \"CNY\": 6.953724, \"COP\": 4171.971894, \"CRC\": 701.446322, \"CUC\": 1.052909, \"CUP\": 27.902082, \"CVE\": 110.15345, \"CZK\": 24.499027, \"DJF\": 188.707108, \"DKK\": 7.441548, \"DOP\": 58.321493, \"DZD\": 152.371647, \"EGP\": 19.458297, \"ERN\": 15.793633, \"ETB\": 54.43729, \"EUR\": 1, \"FJD\": 2.274651, \"FKP\": 0.80931, \"GBP\": 0.839568, \"GEL\": 3.20611, \"GGP\": 0.80931, \"GHS\": 7.976422, \"GIP\": 0.80931, \"GMD\": 56.64554, \"GNF\": 9416.400803, \"GTQ\": 8.118402, \"GYD\": 221.765423, \"HKD\": 8.261854, \"HNL\": 26.0169, \"HRK\": 7.563467, \"HTG\": 115.545574, \"HUF\": 377.172734, \"IDR\": 15238.748216, \"ILS\": 3.489582, \"IMP\": 0.80931, \"INR\": 80.654494, \"IQD\": 1547.023976, \"IRR\": 44538.040218, \"ISK\": 137.457233, \"JEP\": 0.80931, \"JMD\": 163.910125, \"JOD\": 0.746498, \"JPY\": 137.331903, \"KES\": 121.87429, \"KGS\": 88.581418, \"KHR\": 4286.72178, \"KMF\": 486.443591, \"KPW\": 947.617993, \"KRW\": 1339.837191, \"KWD\": 0.322886, \"KYD\": 0.883397, \"KZT\": 473.770223, \"LAK\": 12761.755235, \"LBP\": 1602.661797, \"LKR\": 376.293562, \"LRD\": 159.989586, \"LSL\": 15.604181, \"LTL\": 3.108965, \"LVL\": 0.636894, \"LYD\": 5.031557, \"MAD\": 10.541225, \"MDL\": 19.593772, \"MGA\": 4284.002369, \"MKD\": 61.553251, \"MMK\": 1962.574442, \"MNT\": 3153.317641, \"MOP\": 8.567461, \"MRO\": 375.88824, \"MUR\": 45.165684, \"MVR\": 16.199478, \"MWK\": 865.62318, \"MXN\": 21.530268, \"MYR\": 4.594366, \"MZN\": 67.206888, \"NAD\": 15.604214, \"NGN\": 437.399752, \"NIO\": 37.965356, \"NOK\": 9.824365, \"NPR\": 129.86672, \"NZD\": 1.616441, \"OMR\": 0.405421, \"PAB\": 1.060027, \"PEN\": 4.054233, \"PGK\": 3.73593, \"PHP\": 55.075028, \"PKR\": 196.760944, \"PLN\": 4.698101, \"PYG\": 7246.992296, \"QAR\": 3.833603, \"RON\": 4.948144, \"RSD\": 117.620172, \"RUB\": 77.806269, \"RWF\": 1086.709833, \"SAR\": 3.949063, \"SBD\": 8.474149, \"SCR\": 14.304711, \"SDG\": 470.649944, \"SEK\": 10.367719, \"SGD\": 1.459695, \"SHP\": 1.45028, \"SLL\": 13082.391386, \"SOS\": 609.634325, \"SRD\": 21.904702, \"STD\": 21793.085136, \"SVC\": 9.275519, \"SYP\": 2645.380032, \"SZL\": 16.827859, \"THB\": 36.297991, \"TJS\": 13.196811, \"TMT\": 3.685181, \"TND\": 3.22348, \"TOP\": 2.428117, \"TRY\": 15.575532, \"TTD\": 7.202107, \"TWD\": 31.082183, \"TZS\": 2446.960099, \"UAH\": 32.065033, \"UGX\": 3773.578577, \"USD\": 1.052909, \"UYU\": 43.156886, \"UZS\": 11895.19696, \"VEF\": 225143710305.04727, \"VND\": 24171.62598, \"VUV\": 118.538204, \"WST\": 2.722234, \"XAF\": 655.287181, \"XAG\": 0.045404, \"XAU\": 0.000559, \"XCD\": 2.845538, \"XDR\": 0.783307, \"XOF\": 655.293398, \"XPF\": 118.347299, \"YER\": 263.490114, \"ZAR\": 16.77336, \"ZMK\": 9477.445964, \"ZMW\": 18.046154, \"ZWL\": 339.036185}}, \"emitted_at\": 1651130169364}}`\nThere we have it - a stream which reads data in just a few lines of code!\nWe theoretically could stop here and call it a connector. But let's give adding incremental sync a shot.\nAdding incremental sync\nTo add incremental sync, we'll do a few things: \n1. Pass the `start_date` param input by the user into the stream. \n2. Declare the stream's `cursor_field`. \n3. Declare the stream's property `_cursor_value` to hold the state value\n4. Add `IncrementalMixin` to the list of the ancestors of the stream and implement setter and getter of the `state`.\n5. Implement the `stream_slices` method. \n6. Update the `path` method to specify the date to pull exchange rates for. \n7. Update the configured catalog to use `incremental` sync when we're testing the stream.\nWe'll describe what each of these methods do below. Before we begin, it may help to familiarize yourself with how incremental sync works in Airbyte by reading the docs on incremental.\nTo keep things concise, we'll only show functions as we edit them one by one.\nLet's get the easy parts out of the way and pass the `start_date`:\n`python\ndef streams(self, config: Mapping[str, Any]) -> List[Stream]:\n    auth = NoAuth()\n    # Parse the date from a string into a datetime object\n    start_date = datetime.strptime(config['start_date'], '%Y-%m-%d')\n    return [ExchangeRates(authenticator=auth, config=config, start_date=start_date)]`\nLet's also add this parameter to the constructor and declare the `cursor_field`:\n```python\nfrom datetime import datetime, timedelta\nfrom airbyte_cdk.sources.streams import IncrementalMixin\nclass ExchangeRates(HttpStream, IncrementalMixin):\n    url_base = \"https://api.apilayer.com/exchangerates_data/\"\n    cursor_field = \"date\"\n    primary_key = \"date\"\n\n\n```def __init__(self, config: Mapping[str, Any], start_date: datetime, **kwargs):\n    super().__init__()\n    self.base = config['base']\n    self.apikey = config['apikey']\n    self.start_date = start_date\n    self._cursor_value = None\n```\n\n\n```\nDeclaring the `cursor_field` informs the framework that this stream now supports incremental sync. The next time you run `python main_dev.py discover --config secrets/config.json` you'll find that the `supported_sync_modes` field now also contains `incremental`.\nBut we're not quite done with supporting incremental, we have to actually emit state! We'll structure our state object very simply: it will be a `dict` whose single key is `'date'` and value is the date of the last day we synced data from. For example, `{'date': '2021-04-26'}` indicates the connector previously read data up until April 26th and therefore shouldn't re-read anything before April 26th.\nLet's do this by implementing the getter and setter for the `state` inside the `ExchangeRates` class.\n```python\n    @property\n    def state(self) -> Mapping[str, Any]:\n        if self._cursor_value:\n            return {self.cursor_field: self._cursor_value.strftime('%Y-%m-%d')}\n        else:\n            return {self.cursor_field: self.start_date.strftime('%Y-%m-%d')}\n\n\n```@state.setter\ndef state(self, value: Mapping[str, Any]):\n   self._cursor_value = datetime.strptime(value[self.cursor_field], '%Y-%m-%d')\n```\n\n\n```\nUpdate internal state `cursor_value` inside `read_records` method\n```python\n    def read_records(self, args, kwargs) -> Iterable[Mapping[str, Any]]:\n        for record in super().read_records(args, **kwargs):\n            if self._cursor_value:\n                latest_record_date = datetime.strptime(record[self.cursor_field], '%Y-%m-%d')\n                self._cursor_value = max(self._cursor_value, latest_record_date)\n            yield record\n```\nThis implementation compares the date from the latest record with the date in the current state and takes the maximum as the \"new\" state object.\nWe'll implement the `stream_slices` method to return a list of the dates for which we should pull data based on the stream state if it exists:\n```python\n    def _chunk_date_range(self, start_date: datetime) -> List[Mapping[str, Any]]:\n        \"\"\"\n        Returns a list of each day between the start date and now.\n        The return value is a list of dicts {'date': date_string}.\n        \"\"\"\n        dates = []\n        while start_date < datetime.now():\n            dates.append({self.cursor_field: start_date.strftime('%Y-%m-%d')})\n            start_date += timedelta(days=1)\n        return dates\n\n\n```def stream_slices(self, sync_mode, cursor_field: List[str] = None, stream_state: Mapping[str, Any] = None) -> Iterable[Optional[Mapping[str, Any]]]:\n    start_date = datetime.strptime(stream_state[self.cursor_field], '%Y-%m-%d') if stream_state and self.cursor_field in stream_state else self.start_date\n    return self._chunk_date_range(start_date)\n```\n\n\n```\nEach slice will cause an HTTP request to be made to the API. We can then use the information present in the `stream_slice` parameter (a single element from the list we constructed in `stream_slices` above) to set other configurations for the outgoing request like `path` or `request_params`. For more info about stream slicing, see the slicing docs.\nIn order to pull data for a specific date, the Exchange Rates API requires that we pass the date as the path component of the URL. Let's override the `path` method to achieve this:\n`python\ndef path(self, stream_state: Mapping[str, Any] = None, stream_slice: Mapping[str, Any] = None, next_page_token: Mapping[str, Any] = None) -> str:\n    return stream_slice['date']`\nWith these changes, your implementation should look like the file here.\nThe last thing we need to do is change the `sync_mode` field in the `sample_files/configured_catalog.json` to `incremental`:\n`text\n\"sync_mode\": \"incremental\",`\nWe should now have a working implementation of incremental sync!\nLet's try it out:\n`text\npython main.py read --config secrets/config.json --catalog sample_files/configured_catalog.json`\nYou should see a bunch of `RECORD` messages and `STATE` messages. To verify that incremental sync is working, pass the input state back to the connector and run it again:\n```text\nSave the latest state to sample_files/state.json\npython main.py read --config secrets/config.json --catalog sample_files/configured_catalog.json | grep STATE | tail -n 1 | jq .state.data > sample_files/state.json\nRun a read operation with the latest state message\npython main.py read --config secrets/config.json --catalog sample_files/configured_catalog.json --state sample_files/state.json\n```\nYou should see that only the record from the last date is being synced! This is acceptable behavior, since Airbyte requires at-least-once delivery of records, so repeating the last record twice is OK.\nWith that, we've implemented incremental sync for our connector!",
    "tag": "airbyte"
  },
  {
    "title": "Getting Started",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/tutorials/cdk-tutorial-python-http/getting-started.md",
    "content": "Getting Started\nSummary\nThis is a step-by-step guide for how to create an Airbyte source in Python to read data from an HTTP API. We'll be using the Exchange Rates API as an example since it is simple and demonstrates a lot of the capabilities of the CDK.\nRequirements\n\nPython >= 3.9\nDocker\nNodeJS (only used to generate the connector). We'll remove the NodeJS dependency soon.\n\nAll the commands below assume that `python` points to a version of python >=3.9.0. On some systems, `python` points to a Python2 installation and `python3` points to Python3. If this is the case on your machine, substitute all `python` commands in this guide with `python3`.\nExchange Rates API Setup\nFor this guide we will be making API calls to the Exchange Rates API. In order to generate the API access key that will be used by the new connector, you will have to follow steps on the Exchange Rates API by signing up for the Free tier plan. Once you have an API access key, you can continue with the guide.\nChecklist\n\nStep 1: Create the source using the template\nStep 2: Install dependencies for the new source\nStep 3: Define the inputs needed by your connector\nStep 4: Implement connection checking\nStep 5: Declare the schema of your streams\nStep 6: Implement functionality for reading your streams\nStep 7: Use the connector in Airbyte\nStep 8: Write unit tests or integration tests\n\nEach step of the Creating a Source checklist is explained in more detail in the following steps. We also mention how you can submit the connector to be included with the general Airbyte release at the end of the tutorial.",
    "tag": "airbyte"
  },
  {
    "title": "Step 4: Connection Checking",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/tutorials/cdk-tutorial-python-http/connection-checking.md",
    "content": "Step 4: Connection Checking\nThe second operation in the Airbyte Protocol that we'll implement is the `check` operation.\nThis operation verifies that the input configuration supplied by the user can be used to connect to the underlying data source. Note that this user-supplied configuration has the values described in the `spec.yaml` filled in. In other words if the `spec.yaml` said that the source requires a `username` and `password` the config object might be `{ \"username\": \"airbyte\", \"password\": \"password123\" }`. You should then implement something that returns a json object reporting, given the credentials in the config, whether we were able to connect to the source.\nIn order to make requests to the API, we need to specify the access.\nIn our case, this is a fairly trivial check since the API requires no credentials. Instead, let's verify that the user-input `base` currency is a legitimate currency. In `source.py` we'll find the following autogenerated source:\n```python\nclass SourcePythonHttpTutorial(AbstractSource):\n\n\n```def check_connection(self, logger, config) -> Tuple[bool, any]:\n    \"\"\"\n    TODO: Implement a connection check to validate that the user-provided config can be used to connect to the underlying API\n\n    See https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-stripe/source_stripe/source.py#L232\n    for an example.\n\n    :param config:  the user-input config object conforming the connector's spec.yaml\n    :param logger:  logger object\n    :return Tuple[bool, any]: (True, None) if the input config can be used to connect to the API successfully, (False, error) otherwise.\n    \"\"\"\n    return True, None\n```\n\n\n...\n```\nFollowing the docstring instructions, we'll change the implementation to verify that the input currency is a real currency:\n`python\n    def check_connection(self, logger, config) -> Tuple[bool, any]:\n        accepted_currencies = {\"USD\", \"JPY\", \"BGN\", \"CZK\", \"DKK\"}  # assume these are the only allowed currencies\n        input_currency = config['base']\n        if input_currency not in accepted_currencies:\n            return False, f\"Input currency {input_currency} is invalid. Please input one of the following currencies: {accepted_currencies}\"\n        else:\n            return True, None`\nNote: in a real implementation you should write code to connect to the API to validate connectivity and not just validate inputs - for an example see `check_connection` in the OneSignal source connector implementation\nLet's test out this implementation by creating two objects: a valid and an invalid config and attempt to give them as input to the connector. For this section, you will need to take the API access key generated earlier and add it to both configs. Because these configs contain secrets, we recommend storing configs which contain secrets in `secrets/config.json` because the `secrets` directory is gitignored by default.\n`text\nmkdir sample_files\necho '{\"start_date\": \"2022-04-01\", \"base\": \"USD\", \"apikey\": <your_apikey>}'  > secrets/config.json\necho '{\"start_date\": \"2022-04-01\", \"base\": \"BTC\", \"apikey\": <your_apikey>}'  > secrets/invalid_config.json\npython main.py check --config secrets/config.json\npython main.py check --config secrets/invalid_config.json`\nYou should see output like the following:\n```text\n\npython main.py check --config secrets/config.json\n{\"type\": \"CONNECTION_STATUS\", \"connectionStatus\": {\"status\": \"SUCCEEDED\"}}\npython main.py check --config secrets/invalid_config.json\n{\"type\": \"CONNECTION_STATUS\", \"connectionStatus\": {\"status\": \"FAILED\", \"message\": \"Input currency BTC is invalid. Please input one of the following currencies: {'DKK', 'USD', 'CZK', 'BGN', 'JPY'}\"}}\n```\n",
    "tag": "airbyte"
  },
  {
    "title": "Step 2: Install Dependencies",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/tutorials/cdk-tutorial-python-http/install-dependencies.md",
    "content": "Step 2: Install Dependencies\nNow that you've generated the module, let's navigate to its directory and install dependencies:\n`text\ncd ../../connectors/source-<name>\npython -m venv .venv # Create a virtual environment in the .venv directory\nsource .venv/bin/activate # enable the venv\npip install -r requirements.txt`\nThis step sets up the initial python environment. All subsequent `python` or `pip` commands assume you have activated your virtual environment.\nLet's verify everything is working as intended. Run:\n`text\npython main.py spec`\nYou should see some output:\n`text\n{\"type\": \"SPEC\", \"spec\": {\"documentationUrl\": \"https://docsurl.com\", \"connectionSpecification\": {\"$schema\": \"http://json-schema.org/draft-07/schema#\", \"title\": \"Python Http Tutorial Spec\", \"type\": \"object\", \"required\": [\"TODO\"], \"properties\": {\"TODO: This schema defines the configuration required for the source. This usually involves metadata such as database and/or authentication information.\": {\"type\": \"string\", \"description\": \"describe me\"}}}}}`\nWe just ran Airbyte Protocol's `spec` command! We'll talk more about this later, but this is a simple sanity check to make sure everything is wired up correctly.\nNote that the `main.py` file is a simple script that makes it easy to run your connector. Its invocation format is `python main.py <command> [args]`. See the module's generated `README.md` for the commands it supports.\nNotes on iteration cycle\nDependencies\nPython dependencies for your source should be declared in `airbyte-integrations/connectors/source-<source-name>/setup.py` in the `install_requires` field. You will notice that a couple of Airbyte dependencies are already declared there. Do not remove these; they give your source access to the helper interfaces provided by the generator.\nYou may notice that there is a `requirements.txt` in your source's directory as well. Don't edit this. It is autogenerated and used to provide Airbyte dependencies. All your dependencies should be declared in `setup.py`.\nDevelopment Environment\nThe commands we ran above created a Python virtual environment for your source. If you want your IDE to auto complete and resolve dependencies properly, point it at the virtual env `airbyte-integrations/connectors/source-<source-name>/.venv`. Also anytime you change the dependencies in the `setup.py` make sure to re-run `pip install -r requirements.txt`.\nIterating on your implementation\nThere are two ways we recommend iterating on a source. Consider using whichever one matches your style.\nRun the source using python\nYou'll notice in your source's directory that there is a python file called `main.py`. This file exists as convenience for development. You run it to test that your source works:\n```text\nfrom airbyte-integrations/connectors/source-\npython main.py spec\npython main.py check --config secrets/config.json\npython main.py discover --config secrets/config.json\npython main.py read --config secrets/config.json --catalog sample_files/configured_catalog.json\n```\nThe nice thing about this approach is that you can iterate completely within python. The downside is that you are not quite running your source as it will actually be run by Airbyte. Specifically, you're not running it from within the docker container that will house it.\nRun the source using docker\nIf you want to run your source exactly as it will be run by Airbyte (i.e. within a docker container), you can use the following commands from the connector module directory (`airbyte-integrations/connectors/source-python-http-example`):\n```text\nFirst build the container\ndocker build . -t airbyte/source-:dev\nThen use the following commands to run it\ndocker run --rm airbyte/source-python-http-example:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-python-http-example:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-python-http-example:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/sample_files:/sample_files airbyte/source-python-http-example:dev read --config /secrets/config.json --catalog /sample_files/configured_catalog.json\n```\nNote: Each time you make a change to your implementation you need to re-build the connector image via `docker build . -t airbyte/source-<name>:dev`. This ensures the new python code is added into the docker container.\nThe nice thing about this approach is that you are running your source exactly as it will be run by Airbyte. The tradeoff is iteration is slightly slower, as the connector is re-built between each change.",
    "tag": "airbyte"
  },
  {
    "title": "Step 3: Define Inputs",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/tutorials/cdk-tutorial-python-http/define-inputs.md",
    "content": "Step 3: Define Inputs\nEach connector declares the inputs it needs to read data from the underlying data source. This is the Airbyte Protocol's `spec` operation.\nThe simplest way to implement this is by creating a `spec.yaml` file in `source_<name>/spec.yaml` which describes your connector's inputs according to the ConnectorSpecification schema. This is a good place to start when developing your source. Using JsonSchema, define what the inputs are (e.g. username and password). Here's an example of what the `spec.yaml` looks like for the Stripe API source.\nFor more details on what the spec is, you can read about the Airbyte Protocol here.\nThe generated code that Airbyte provides, handles implementing the `spec` method for you. It assumes that there will be a file called `spec.yaml` in the same directory as `source.py`. If you have declared the necessary JsonSchema in `spec.yaml` you should be done with this step.\nGiven that we'll pulling currency data for our example source, we'll define the following `spec.yaml`:\n`yaml\ndocumentationUrl: https://docs.airbyte.io/integrations/sources/exchangeratesapi\nconnectionSpecification:\n  $schema: http://json-schema.org/draft-07/schema#\n  title: Python Http Tutorial Spec\n  type: object\n  required:\n    - apikey\n    - start_date\n    - base\n  properties:\n    apikey:\n      type: string\n      description: API access key used to retrieve data from the Exchange Rates API.\n      airbyte_secret: true\n    start_date:\n      type: string\n      description: Start getting data from that date.\n      pattern: ^[0-9]{4}-[0-9]{2}-[0-9]{2}$\n      examples:\n        - \"%Y-%m-%d\"\n    base:\n      type: string\n      examples:\n        - USD\n        - EUR\n      description: \"ISO reference currency. See <a href=\\\"https://www.ecb.europa.eu/stats/policy_and_exchange_rates/euro_reference_exchange_rates/html/index.en.html\\\">here</a>.\"`\nIn addition to metadata, we define three inputs:\n\n`apikey`: The API access key used to authenticate requests to the API\n`start_date`: The beginning date to start tracking currency exchange rates from\n`base`: The currency whose rates we're interested in tracking\n",
    "tag": "airbyte"
  },
  {
    "title": "Advanced Topics",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/config-based/advanced-topics.md",
    "content": "Advanced Topics\nObject instantiation\nThis section describes the object that are to be instantiated from the YAML definition.\nIf the component is a literal, then it is returned as is:\n`3`\nwill result in\n`3`\nIf the component definition is a mapping with a \"type\" field,\nthe factory will lookup the CLASS_TYPES_REGISTRY and replace the \"type\" field by \"class_name\" -> CLASS_TYPES_REGISTRY[type]\nand instantiate the object from the resulting mapping\nIf the component definition is a mapping with neither a \"class_name\" nor a \"type\" field,\nthe factory will do a best-effort attempt at inferring the component type by looking up the parent object's constructor type hints.\nIf the type hint is an interface present in [DEFAULT_IMPLEMENTATIONS_REGISTRY](https://github.com/airbytehq/airbyte/blob/master/airbyte-cdk/python/airbyte_cdk/sources/declarative/parsers/default_implementation_registry.py,\nthen the factory will create an object of its default implementation.\nIf the component definition is a list, then the factory will iterate over the elements of the list,\ninstantiate its subcomponents, and return a list of instantiated objects.\nIf the component has subcomponents, the factory will create the subcomponents before instantiating the top level object\n`{\n  \"type\": TopLevel\n  \"param\":\n    {\n      \"type\": \"ParamType\"\n      \"k\": \"v\"\n    }\n}`\nwill result in\n`TopLevel(param=ParamType(k=\"v\"))`\nMore details on object instantiation can be found here.\n$parameters\nParameters can be passed down from a parent component to its subcomponents using the $parameters key.\nThis can be used to avoid repetitions.\nSchema:\n`yaml\n  \"$parameters\":\n    type: object\n    additionalProperties: true`\nExample:\n`yaml\nouter:\n  $parameters:\n    MyKey: MyValue\n  inner:\n    k2: v2`\nThis the example above, if both outer and inner are types with a \"MyKey\" field, both of them will evaluate to \"MyValue\".\nThese parameters can be overwritten by subcomponents as a form of specialization:\n`yaml\nouter:\n  $parameters:\n    MyKey: MyValue\n  inner:\n    $parameters:\n      MyKey: YourValue\n    k2: v2`\nIn this example, \"outer.MyKey\" will evaluate to \"MyValue\", and \"inner.MyKey\" will evaluate to \"YourValue\".\nThe value can also be used for string interpolation:\n`yaml\nouter:\n  $parameters:\n    MyKey: MyValue\n  inner:\n    k2: \"MyKey is {{ parameters['MyKey'] }}\"`\nIn this example, outer.inner.k2 will evaluate to \"MyKey is MyValue\"\nReferences\nStrings can contain references to previously defined values.\nThe parser will dereference these values to produce a complete object definition.\nReferences can be defined using a \"#/{arg}\" string.\n`yaml\nkey: 1234\nreference: \"#/key\"`\nwill produce the following definition:\n`yaml\nkey: 1234\nreference: 1234`\nThis also works with objects:\n`yaml\nkey_value_pairs:\n  k1: v1\n  k2: v2\nsame_key_value_pairs: \"#/key_value_pairs\"`\nwill produce the following definition:\n`yaml\nkey_value_pairs:\n  k1: v1\n  k2: v2\nsame_key_value_pairs:\n  k1: v1\n  k2: v2`\nThe $ref keyword can be used to refer to an object and enhance it with addition key-value pairs\n`yaml\nkey_value_pairs:\n  k1: v1\n  k2: v2\nsame_key_value_pairs:\n  $ref: \"#/key_value_pairs\"\n  k3: v3`\nwill produce the following definition:\n`yaml\nkey_value_pairs:\n  k1: v1\n  k2: v2\nsame_key_value_pairs:\n  k1: v1\n  k2: v2\n  k3: v3`\nReferences can also point to nested values.\nNested references are ambiguous because one could define a key containing with `/`\nin this example, we want to refer to the limit key in the dict object:\n`yaml\ndict:\n  limit: 50\nlimit_ref: \"#/dict/limit\"`\nwill produce the following definition:\n`yaml\ndict\nlimit: 50\nlimit-ref: 50`\nwhereas here we want to access the `nested/path` value.\n`yaml\nnested:\n  path: \"first one\"\nnested.path: \"uh oh\"\nvalue: \"ref(nested.path)`\nwill produce the following definition:\n`yaml\nnested:\n  path: \"first one\"\nnested/path: \"uh oh\"\nvalue: \"uh oh\"`\nTo resolve the ambiguity, we try looking for the reference key at the top-level, and then traverse the structs downward\nuntil we find a key with the given path, or until there is nothing to traverse.\nMore details on referencing values can be found here.\nString interpolation\nString values can be evaluated as Jinja2 templates.\nIf the input string is a raw string, the interpolated string will be the same.\n`\"hello world\" -> \"hello world\"`\nThe engine will evaluate the content passed within `{{...}}`, interpolating the keys from context-specific arguments.\nThe \"parameters\" keyword see ($parameters) can be referenced.\nFor example, some_object.inner_object.key will evaluate to \"Hello airbyte\" at runtime.\n`yaml\nsome_object:\n  $parameters:\n    name: \"airbyte\"\n  inner_object:\n    key: \"Hello {{ parameters.name }}\"`\nSome components also pass in additional arguments to the context.\nThis is the case for the record selector, which passes in an additional `response` argument.\nBoth dot notation and bracket notations (with single quotes ( `'`)) are interchangeable.\nThis means that both these string templates will evaluate to the same string:\n\n`\"{{ parameters.name }}\"`\n`\"{{ parameters['name'] }}\"`\n\nIn addition to passing additional values through the $parameters argument, macros can be called from within the string interpolation.\nFor example,\n`\"{{ max(2, 3) }}\" -> 3`\nThe macros available can be found here.\nAdditional information on jinja templating can be found at https://jinja.palletsprojects.com/en/3.1.x/templates/#\nComponent schema reference\nA JSON schema representation of the relationships between the components that can be used in the YAML configuration can be found here.\nCustom components\n:::info\nPlease help us improve the low code CDK! If you find yourself needing to build a custom component,please create a feature request issue. If appropriate, we'll add it directly to the framework (or you can submit a PR)!\nIf an issue already exist for the missing feature you need, please upvote or comment on it so we can prioritize the issue accordingly.\n:::\nAny built-in components can be overloaded by a custom Python class.\nTo create a custom component, define a new class in a new file in the connector's module.\nThe class must implement the interface of the component it is replacing. For instance, a pagination strategy must implement `airbyte_cdk.sources.declarative.requesters.paginators.strategies.pagination_strategy.PaginationStrategy`.\nThe class must also be a dataclass where each field represents an argument to configure from the yaml file, and an `InitVar` named parameters.\nFor example:\n```\n@dataclass\nclass MyPaginationStrategy(PaginationStrategy):\n  my_field: Union[InterpolatedString, str]\n  parameters: InitVar[Mapping[str, Any]]\ndef post_init(self, parameters: Mapping[str, Any]):\n    pass\ndef next_page_token(self, response: requests.Response, last_records: List[Mapping[str, Any]]) -> Optional[Any]:\n    pass\ndef reset(self):\n    pass\n```\nThis class can then be referred from the yaml file by specifying the type of custom component and using its fully qualified class name:\n`yaml\npagination_strategy:\n  type: \"CustomPaginationStrategy\"\n  class_name: \"my_connector_module.MyPaginationStrategy\"\n  my_field: \"hello world\"`\nCustom Components that pass fields to child components\nThere are certain scenarios where a child subcomponent might rely on a field defined on a parent component. For regular components, we perform this propagation of fields from the parent component to the child automatically.\nHowever, custom components do not support this behavior. If you have a child subcomponent of your custom component that falls under this use case, you will see an error message like:\n`Error creating component 'DefaultPaginator' with parent custom component source_example.components.CustomRetriever: Please provide DefaultPaginator.$parameters.url_base`\nWhen you receive this error, you can address this by defining the missing field within the `$parameters` block of the child component.\n`yaml\n  paginator:\n    type: \"DefaultPaginator\"\n    <...>\n    $parameters:\n      url_base: \"https://example.com\"`\nHow the framework works\n\nGiven the connection config and an optional stream state, the `PartitionRouter` computes the partitions that should be routed to read data.\nIterate over all the partitions defined by the stream's partition router.\nFor each partition,\nSubmit a request to the partner API as defined by the requester\nSelect the records from the response\nRepeat for as long as the paginator points to a next page\n\n\n\nconnector-flow\nMore readings\n\nRecord selector\nPartition routers\n",
    "tag": "airbyte"
  },
  {
    "title": "Connector Builder UI",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/config-based/connector-builder-ui.md",
    "content": "Connector Builder UI\nThe connector builder UI provides an ergonomic iteration interface on top of the low-code YAML format. We recommend using it to iterate on your low-code connectors.\n:::caution\nThe connector builder UI is in alpha, which means it\u2019s still in active development and may include backward-incompatible changes. Share feedback and requests with us on our Slack channel or email us at feedback@airbyte.io\n:::\nGetting started\nThe high level flow for using the connector builder is as follows:\n\nRun the Airbyte webapp to access the connector builder\nUse the connector builder to iterate on your low-code connector\nExport the YAML into a low-code connector module on your local machine\nBuild the connector's Docker image\nUse the built connector image in Airbyte\n\nRun an Airbyte instance\n:::info\nYou need at least Airbyte v0.40.27 to run the connector builder UI.\n:::\n:::tip\nWe recommend using a dedicated Airbyte instance for the connector builder UI. This will allow you to pick up connector builder UI upgrades without impacting your production data pipelines.\n:::\nThe connector builder UI is bundled as part of the Airbyte webapp. To run it, follow the instructions for deploying Airbyte locally to run Airbyte on your machine. Once you do, Airbyte should be reachable at `http://localhost:8000`.\nVisit the connector builder\nOnce your Airbyte instance has started and you've moved past the initial setup screen, visit `http://localhost:8000/connector-builder`. You will be redirected to a URL of the form `localhost:8000/workspaces/<UUID>/connector-builder` where `<UUID>` is the ID automatically generated for your workspace by Airbyte\nOn this page you will find the Connector Builder landing page. It should look like this:\n\nHere you can either upload an existing low-code YAML manifest, or start building a brand new connector in the UI. If you click `Start from scratch`, you will be redirected to `localhost:8000/workspaces/<UUID>/connector-builder/edit`, where you will see the following screen:\n\nYou can now use this UI to build your connector. See the Testing Panel section for more information on how to use the UI to iterate on your connector.\nThe output of this UI is a low-code YAML representation of your connector, which you can preview by clicking the `UI | YAML` toggle button the the top-left. This screen also allows you to edit the low-code YAML directly if desired, and continue to test it in the testing panel.\nExporting the YAML\nOnce you're done iterating on your connector in the UI, you'll need to export the low-code YAML representation of the connector to your local filesystem into a connector module. This YAML can be downloaded by clicking the `Download Config` button in the bottom-left.\nIf you haven't already, create a low-code connector module using the connector generator (see this YAML tutorial for an example) using the name you'd like to use for your connector. For this section, let's assume our connector is called `exchange-rates`. After creating the connector, overwrite the contents of `airbyte-integrations/connectors/source-exchange-rates/source_exchange_rates/manifest.yaml` with the YAML you created in the UI.\nBuilding the connector image\nFollow the instructions in the connector README to build the Docker image. Typically this will be something like `docker build . -t airbyte/source-<name>:<version>`.\nOnce you've built the connector image, follow these instructions to add your connector to your Airbyte instance.\nConnector Builder Testing Panel\nThe UI contains two main components: the Builder UI where you can fill out inputs to build your connector (left), and the testing panel (right) where you can get feedback on how your connector works.\n\n\nInput parameters panel: Configure the input parameters to be used in testing. For example, if the connector requires an API key, open this menu to input your API which will be used for testing.\nStream Picker dropdown: Use this dropdown to choose which stream you want to test\nEndpoint URL: Displays the URL queried by the CDK to retrieve data for the current stream\nTest button: When clicked, retrieves the data for the selected stream using the stream configuration setup in the UI. This is the equivalent of running the `read` command on the terminal for a single stream (the selected stream).\nRecords tab: Displays the final output returned by the connector for the selected page of data in this stream\nRequest tab: Displays the outgoing HTTP request made by the connector to retrieve the selected page of data. Useful for debugging.\nResponse tab: Displays the full HTTP response received by the connector for the selected page of data. Useful for debugging.\nResults view: Displays information based on the selected tab\nPage selector Displays the selected page\nLogs view: Displays the logs emitted by the connector while running\n\n\nUpgrading",
    "tag": "airbyte"
  },
  {
    "title": "Low-code connector development",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/config-based/low-code-cdk-overview.md",
    "content": "Low-code connector development\nAirbyte\u2019s low-code framework enables you to build source connectors for REST APIs via a connector builder UI or by modifying boilerplate YAML files via terminal or text editor.\n:::caution\nThe low-code framework is in alpha, which means it\u2019s still in active development and may include backward-incompatible changes. Share feedback and requests with us on our Slack channel or email us at feedback@airbyte.io\n:::\nWhy low-code?\nAPI Connectors are common and formulaic\nIn building and maintaining hundreds of connectors at Airbyte, we've observed that whereas API source connectors constitute the overwhelming majority of connectors, they are also the most formulaic. API connector code almost always solves small variations of these problems: \n\nMaking requests to various endpoints under the same API URL e.g: `https://api.stripe.com/customers`, `https://api.stripe.com/transactions`, etc.. \nAuthenticating using a common auth strategy such as Oauth or API keys\nPagination using one of the 4 ubiquitous pagination strategies: limit-offset, page-number, cursor pagination, and header link pagination\nGracefully handling rate limiting by implementing exponential backoff, fixed-time backoff, or variable-time backoff\nDescribing the schema of the data returned by the API, so that downstream warehouses can create normalized tables\nDecoding the format of the data returned by the API (e.g JSON, XML, CSV, etc..) and handling compression (GZIP, BZIP, etc..) \nSupporting incremental data exports by remembering what data was already synced, usually using date-based cursors\n\nand so on. \nA declarative, low-code paradigm commoditizes solving formulaic problems\nGiven that these problems each have a very finite number of solutions, we can remove the need for writing the code to build these API connectors by providing configurable off-the-shelf components to solve them. In doing so, we significantly decrease development effort and bugs while improving maintainability and accessibility. In this paradigm, instead of having to write the exact lines of code to solve this problem over and over, a developer can pick the solution to each problem from an available component, and rely on the framework to run the logic for them. \nWhat connectors can I build using the low-code framework?\nRefer to the REST API documentation for the source you want to build the connector for and answer the following questions:\n\nDoes the REST API documentation show which HTTP method to use to retrieve data, and that the response is a JSON object?\nDo the queries return data synchronously?\nDoes the API support any of the following pagination mechanisms:\nOffset count passed either by query params or request header\nPage count passed either by query params or request header\nCursor field pointing to the URL of the next page of records\n\n\nDoes the API support any of the following authentication mechanisms:\nA query param or a HTTP header\nBasic Auth over HTTPS\nOAuth 2.0\n\n\nDoes the API support static schema?\nDoes the endpoint have a strict rate limit?\n  Throttling is not supported, but the connector can use exponential backoff to avoid API bans in case it gets rate limited. This can work for APIs with high rate limits, but not for those that have strict limits on a small time-window.\nAre the following features sufficient:\n\n| Feature                                                                                | Support                                                                                                                                                                                                                                                                                                                                               |\n|----------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Resource type                                                                          | CollectionsSub-collection                                                                                                                                                                                                                                            |\n| Sync mode                                                                              | Full refreshIncremental                                                                                                                                                                                                                                                                                                                          |\n| Schema discovery                                                                       | Static schemas                                                                                                                                                                                                                                                                                                                                        |\n| Incremental syncs                  | Sync checkpointing by date                                                                                                                                                                                                                                                    |\n| Partition routing                   | lists, parent-resource id                                                                                                                                                                          |\n| Record transformation                | Field selectionAdding fieldsRemoving fieldsFiltering records |\n| Error detection                       | From HTTP status  codeFrom error message                                                                                                                                                                   |\n| Backoff strategies | ExponentialConstantDerived from headers                                                                                 |\nIf the answer to all questions is yes, you can use the low-code framework to build a connector for the source. If not, use the Python CDK.\nPrerequisites\n\nAn API key for the source you want to build a connector for\nPython >= 3.9\nDocker\nNodeJS\n\nOverview of the process\nTo use the low-code framework to build an REST API Source connector:\n\nGenerate the API key or credentials for the source you want to build a connector for\nSet up the project on your local machine\nSet up your local development environment\nUse the connector builder UI to define the connector YAML manifest and test the connector\nSpecify stream schemas\nAdd the connector to the Airbyte platform\n\nFor a step-by-step tutorial, refer to the Getting Started tutorial or the video tutorial\nConnector Builder UI\nThe main concept powering the lowcode connector framework is the Connector Manifest, a YAML file which describes the features and functionality of the connector. The structure of this YAML file is described in more detail here. \nWe recommend iterating on this YAML file is via the connector builder UI as it makes it easy to inspect and debug your connector in greater detail than you would be able to through the commandline. While you can still iterate via the commandline (and the docs contain instructions for how to do it), we're investing heavily in making the UI give you iteration superpowers, so we recommend you check it out!\nConfiguring the YAML file\nThe low-code framework involves editing a boilerplate YAML file. The general structure of the YAML file is as follows:\n`version: \"0.1.0\"\ndefinitions:\n  <key-value pairs defining objects which will be reused in the YAML connector>\nstreams:\n  <list stream definitions>\ncheck:\n  <definition of connection checker>\nspec: \n  <connector spec>`\nThe following table describes the components of the YAML file:\n| Component     | Description                                                                                                                                            |\n|---------------|--------------------------------------------------------------------------------------------------------------------------------------------------------|\n| `version`     | Indicates the framework version                                                                                                                        |\n| `definitions` | Describes the objects to be reused in the YAML connector                                                                                               |\n| `streams`     | Lists the streams of the source                                                                                                                        |\n| `check`       | Describes how to test the connection to the source by trying to read a record from a specified list of streams and failing if no records could be read |\n| `spec`       | A connector specification which describes the required and optional parameters which can be input by the end user to configure this connector |\n:::tip\nStreams define the schema of the data to sync, as well as how to read it from the underlying API source. A stream generally corresponds to a resource within the API. They are analogous to tables for a relational database source.\n:::\nFor each stream, configure the following components:\n| Component              | Sub-component    | Description                                                                                                                                                                                                                           |\n|------------------------|------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Name                   |                  | Name of the stream                                                                                                                                                                                                                    |\n| Primary key (Optional) |                  | Used to uniquely identify records, enabling deduplication. Can be a string for single primary keys, a list of strings for composite primary keys, or a list of list of strings for composite primary keys consisting of nested fields |\n| Schema                 |                  | Describes the data to sync                                                                                                                                                                                                            |\n | Incremental sync       |                  | Describes the behavior of an incremental sync which enables checkpointing and replicating only the data that has changed since the last sync to a destination.                                                                        |\n | Data retriever         |                  | Describes how to retrieve data from the API                                                                                                                                                                                           |\n |                        | Requester        | Describes how to prepare HTTP requests to send to the source API and defines the base URL and path, the request options provider, the HTTP method, authenticator, error handler components                                            |\n |                        | Pagination       | Describes how to navigate through the API's pages                                                                                                                                                                                     |\n |                        | Record Selector  | Describes how to extract records from a HTTP response                                                                                                                                                                                 |\n |                        | Partition Router | Describes how to partition the stream, enabling incremental syncs and checkpointing                                                                                                                                                   |\n | Cursor field           |                  | Field to use as stream cursor. Can either be a string, or a list of strings if the cursor is a nested field.                                                                                                                          |\n | Transformations        |                  | A set of transformations to be applied on the records read from the source before emitting them to the destination                                                                                                                    |\nFor a deep dive into each of the components, refer to Understanding the YAML file or the full YAML Schema definition\nTutorial\nThis section a tutorial that will guide you through the end-to-end process of implementing a low-code connector.\n\nGetting started\nCreating a source\nInstalling dependencies\nConnecting to the API\nReading data\nIncremental reads\nTesting\n\nSample connectors\nFor examples of production-ready config-based connectors, refer to:\n\nGreenhouse\nSendgrid\n",
    "tag": "airbyte"
  },
  {
    "title": "Step  1: Generate the source connector project locally",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/config-based/tutorial/1-create-source.md",
    "content": "Step  1: Generate the source connector project locally\nLet's start by cloning the Airbyte repository:\n`bash\ngit clone git@github.com:airbytehq/airbyte.git\ncd airbyte`\nAirbyte provides a code generator which bootstraps the scaffolding for our connector.\n`bash\ncd airbyte-integrations/connector-templates/generator\n./generate.sh`\nThis will bring up an interactive helper application. Use the arrow keys to pick a template from the list. Select the `Configuration Based Source` template and then input the name of your connector. The application will create a new directory in `airbyte/airbyte-integrations/connectors/` with the name of your new connector.\nThe generator will create a new module for your connector with the name `source-<connector-name>`.\n`Configuration Based Source\nSource name: exchange-rates-tutorial`\nFor this walkthrough, we'll refer to our source as `exchange-rates-tutorial`.\nNext steps\nNext, we'll install dependencies required to run the connector\nMore readings",
    "tag": "airbyte"
  },
  {
    "title": "Step 4: Reading data",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/config-based/tutorial/4-reading-data.md",
    "content": "Step 4: Reading data\nNow that we're able to authenticate to the source API, we'll want to select data from the HTTP responses.\nLet's first add the stream to the configured catalog in `source-exchange-rates-tutorial/integration_tests/configured_catalog.json`\n`json\n{\n  \"streams\": [\n    {\n      \"stream\": {\n        \"name\": \"rates\",\n        \"json_schema\": {},\n        \"supported_sync_modes\": [\n          \"full_refresh\"\n        ]\n      },\n      \"sync_mode\": \"full_refresh\",\n      \"destination_sync_mode\": \"overwrite\"\n    }\n  ]\n}`\nThe configured catalog declares the sync modes supported by the stream (full refresh or incremental).\nSee the catalog guide for more information.\nLet's define the stream schema in `source-exchange-rates-tutorial/source_exchange_rates_tutorial/schemas/rates.json`\nYou can download the JSON file describing the output schema with all currencies here for convenience and place it in `schemas/`.\n`bash\ncurl https://raw.githubusercontent.com/airbytehq/airbyte/master/airbyte-cdk/python/docs/tutorials/http_api_source_assets/exchange_rates.json > source_exchange_rates_tutorial/schemas/rates.json`\nWe can also delete the boilerplate schema files\n`bash\nrm source_exchange_rates_tutorial/schemas/customers.json\nrm source_exchange_rates_tutorial/schemas/employees.json`\nAs an alternative to storing the stream's data schema to the `schemas/` directory, we can store it inline in the YAML file, by including the optional `schema_loader` key and associated schema in the entry for each stream. More information on how to define a stream's schema in the YAML file can be found here.\nReading from the source can be done by running the `read` operation\n`bash\npython main.py read --config secrets/config.json --catalog integration_tests/configured_catalog.json`\nThe logs should show that 1 record was read from the stream.\n`{\"type\": \"LOG\", \"log\": {\"level\": \"INFO\", \"message\": \"Read 1 records from rates stream\"}}\n{\"type\": \"LOG\", \"log\": {\"level\": \"INFO\", \"message\": \"Finished syncing rates\"}}`\nThe `--debug` flag can be set to print out debug information, including the outgoing request and its associated response\n`bash\npython main.py read --config secrets/config.json --catalog integration_tests/configured_catalog.json --debug`\nNext steps\nWe now have a working implementation of a connector reading the latest exchange rates for a given currency.\nWe're however limited to only reading the latest exchange rate value.\nNext, we'll enhance the connector to read data for a given date, which will enable us to backfill the stream with historical data.\nMore readings\n\nRecord selector\n",
    "tag": "airbyte"
  },
  {
    "title": "Step 6: Testing",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/config-based/tutorial/6-testing.md",
    "content": "Step 6: Testing\nWe should make sure the connector respects the Airbyte specifications before we start using it in production.\nThis can be done by executing the Connector Acceptance Tests.\nThese tests will assert the most basic functionalities work as expected and are configured in `acceptance-test-config.yml`.\nBefore running the tests, we'll create an invalid config to make sure the `check` operation fails if the credentials are wrong, and an abnormal state to verify the connector's behavior when running with an abnormal state.\nUpdate `integration_tests/invalid_config.json` with this content\n`json\n{\n  \"access_key\": \"<invalid_key>\",\n  \"start_date\": \"2022-07-21\",\n  \"base\": \"USD\"\n}`\nand `integration_tests/abnormal_state.json` with\n`json\n{\n  \"rates\": {\n    \"date\": \"2999-12-31\"\n  }\n}`\nYou can run the acceptance tests with the following commands:\n`bash\ndocker build . -t airbyte/source-exchange-rates-tutorial:dev\npython -m pytest integration_tests -p integration_tests.acceptance`\nNext steps:\nNext, we'll add the connector to the Airbyte platform.\nRead more:\n\nError handling\nPagination\nTesting connectors\nContribution guide\nGreenhouse source\nSendgrid source\n",
    "tag": "airbyte"
  },
  {
    "title": "Step 2: Install dependencies",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/config-based/tutorial/2-install-dependencies.md",
    "content": "Step 2: Install dependencies\nLet's create a python virtual environment for our source.\nYou can do this by executing the following commands from the root of the Airbyte repository.\nThe command below assume that `python` points to a version of python >=3.9.0. On some systems, `python` points to a Python2 installation and `python3` points to Python3.\nIf this is the case on your machine, substitute the `python` commands with `python3`.\nThe subsequent `python` invocations will use the virtual environment created for the connector.\n`bash\ncd ../../connectors/source-exchange-rates-tutorial\npython -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt`\nThese steps create an initial python environment, and install the dependencies required to run an API Source connector.\nLet's verify everything works as expected by running the Airbyte `spec` operation:\n`bash\npython main.py spec`\nYou should see an output similar to the one below:\n`{\"type\": \"SPEC\", \"spec\": {\"documentationUrl\": \"https://docsurl.com\", \"connectionSpecification\": {\"$schema\": \"http://json-schema.org/draft-07/schema#\", \"title\": \"Python Http Tutorial Spec\", \"type\": \"object\", \"required\": [\"TODO\"], \"additionalProperties\": false, \"properties\": {\"TODO: This schema defines the configuration required for the source. This usually involves metadata such as database and/or authentication information.\": {\"type\": \"string\", \"description\": \"describe me\"}}}}}`\nThis is a simple sanity check to make sure everything is wired up correctly.\nMore details on the `spec` operation can be found in Basic Concepts and Defining Stream Schemas.\nFor now, note that the `main.py` file is a convenience wrapper to help run the connector.\nIts invocation format is `python main.py <command> [args]`.\nThe module's generated `README.md` contains more details on the supported commands.\nNext steps\nNext, we'll connect to the API source\nMore readings\n\nBasic Concepts\nDefining Stream Schemas\n",
    "tag": "airbyte"
  },
  {
    "title": "Getting Started",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/config-based/tutorial/0-getting-started.md",
    "content": "Getting Started\n:warning: This framework is in alpha. It is still in active development and may include backward-incompatible changes. Please share feedback and requests directly with us at feedback@airbyte.io :warning:\nSummary\nThroughout this tutorial, we'll walk you through the creation of an Airbyte source to read and extract data from an HTTP API.\nWe'll build a connector reading data from the Exchange Rates API, but the steps apply to other HTTP APIs you might be interested in integrating with.\nThe API documentations can be found here.\nIn this tutorial, we will read data from the following endpoints:\n\n`Latest Rates Endpoint`\n`Historical Rates Endpoint`\n\nWith the end goal of implementing a `Source` with a single `Stream` containing exchange rates going from a base currency to many other currencies.\nThe output schema of our stream will look like the following:\n`json\n{\n  \"base\": \"USD\",\n  \"date\": \"2022-07-15\",\n  \"rates\": {\n    \"CAD\": 1.28,\n    \"EUR\": 0.98\n  }\n}`\nExchange Rates API Setup\nBefore we get started, you'll need to generate an API access key for the Exchange Rates API.\nThis can be done by signing up for the Free tier plan on Exchange Rates API:\n\nVisit https://exchangeratesapi.io and click \"Get free API key\" on the top right\nYou'll be taken to https://apilayer.com -- finish the sign up process, signing up for the free tier\nOnce you're signed in, visit https://apilayer.com/marketplace/exchangerates_data-api#documentation-tab and click \"Live Demo\"\nInside that editor, you'll see an API key. This is your API key.\n\nRequirements\n\nAn Exchange Rates API key\nPython >= 3.9\nDocker must be running\nNodeJS\n\nNext Steps",
    "tag": "airbyte"
  },
  {
    "title": "Step 5: Incremental Reads",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/config-based/tutorial/5-incremental-reads.md",
    "content": "Step 5: Incremental Reads\nWe now have a working implementation of a connector reading the latest exchange rates for a given currency.\nIn this section, we'll update the source to read historical data instead of only reading the latest exchange rates.\nAccording to the API documentation, we can read the exchange rate for a specific date by querying the `\"/exchangerates_data/{date}\"` endpoint instead of `\"/exchangerates_data/latest\"`.\nWe'll now add a `start_date` property to the connector.\nFirst we'll update the spec block in `source_exchange_rates_tutorial/manifest.yaml`\n`yaml\nspec: \n  documentation_url: https://docs.airbyte.io/integrations/sources/exchangeratesapi\n  connection_specification:\n    $schema: http://json-schema.org/draft-07/schema#\n    title: exchangeratesapi.io Source Spec\n    type: object\n    required:\n      - start_date\n      - access_key\n      - base\n    additionalProperties: true\n    properties:\n      start_date:\n        type: string\n        description: Start getting data from that date.\n        pattern: ^[0-9]{4}-[0-9]{2}-[0-9]{2}$\n        examples:\n          - YYYY-MM-DD\n      access_key:\n        type: string\n        description: >-\n          Your API Access Key. See <a\n          href=\"https://exchangeratesapi.io/documentation/\">here</a>. The key is\n          case sensitive.\n        airbyte_secret: true\n      base:\n        type: string\n        description: >-\n          ISO reference currency. See <a\n          href=\"https://www.ecb.europa.eu/stats/policy_and_exchange_rates/euro_reference_exchange_rates/html/index.en.html\">here</a>.\n        examples:\n          - EUR\n          - USD`\nThen we'll set the `start_date` to last week in our connection config in `secrets/config.json`.\nLet's add a start_date field to `secrets/config.json`.\nThe file should look like\n`json\n{\n  \"access_key\": \"<your_access_key>\",\n  \"start_date\": \"2022-07-26\",\n  \"base\": \"USD\"\n}`\nwhere the start date should be 7 days in the past.\nAnd we'll update the `path` in the connector definition to point to `/{{ config.start_date }}`.\nNote that we are setting a default value because the `check` operation does not know the `start_date`. We'll default to hitting `/exchangerates_data/latest`:\n`yaml\ndefinitions:\n  <...>\n  rates_stream:\n    $ref: \"#/definitions/base_stream\"\n    $parameters:\n      name: \"rates\"\n      primary_key: \"date\"\n      path: \"/exchangerates_data/{{config['start_date'] or 'latest'}}\"`\nYou can test these changes by executing the `read` operation:\n`bash\npython main.py read --config secrets/config.json --catalog integration_tests/configured_catalog.json`\nBy reading the output record, you should see that we read historical data instead of the latest exchange rate.\nFor example:\n\n\"historical\": true, \"base\": \"USD\", \"date\": \"2022-07-18\"\n\nThe connector will now always read data for the start date, which is not exactly what we want.\nInstead, we would like to iterate over all the dates between the `start_date` and today and read data for each day.\nWe can do this by adding a `DatetimeBasedCursor` to the connector definition, and update the `path` to point to the stream_slice's `start_date`:\nMore details on incremental syncs can be found here.\nLet's first define a datetime cursor at the top level of the connector definition:\n`yaml\ndefinitions:\n  datetime_cursor:\n    type: \"DatetimeBasedCursor\"\n    start_datetime:\n      datetime: \"{{ config['start_date'] }}\"\n      datetime_format: \"%Y-%m-%d\"\n    end_datetime:\n      datetime: \"{{ now_utc() }}\"\n      datetime_format: \"%Y-%m-%d %H:%M:%S.%f+00:00\"\n    step: \"P1D\"\n    datetime_format: \"%Y-%m-%d\"\n    cursor_granularity: \"P1D\"\n    cursor_field: \"date\"`\nand refer to it in the stream.\nThis will generate time windows from the start time until the end time, where each window is exactly one day.\nThe start time is defined in the config file, while the end time is defined by the `now_utc()` macro, which will evaluate to the current date in the current timezone at runtime. See the section on string interpolation for more details.\n`yaml\ndefinitions:\n  <...>\n  rates_stream:\n    $ref: \"#/definitions/base_stream\"\n    $parameters:\n      name: \"rates\"\n      primary_key: \"date\"\n      path: \"/exchangerates_data/{{config['start_date'] or 'latest'}}\"`\nWe'll also update the base stream to use the datetime cursor:\n`yaml\ndefinitions:\n  <...>\n  base_stream:\n    <...>\n    incremental_sync:\n      $ref: \"#/definitions/datetime_cursor\"`\nFinally, we'll update the path to point to the `stream_slice`'s start_time\n`yaml\ndefinitions:\n  <...>\n  rates_stream:\n    $ref: \"#/definitions/base_stream\"\n    $parameters:\n      name: \"rates\"\n      primary_key: \"date\"\n      path: \"/exchangerates_data/{{stream_slice['start_time'] or 'latest'}}\"`\nThe full connector definition should now look like `./source_exchange_rates_tutorial/manifest.yaml`:\n```yaml\nversion: \"0.1.0\"\ndefinitions:\n  selector:\n    extractor:\n      field_path: [ ]\n  requester:\n    url_base: \"https://api.apilayer.com\"\n    http_method: \"GET\"\n    authenticator:\n      type: ApiKeyAuthenticator\n      header: \"apikey\"\n      api_token: \"{{ config['access_key'] }}\"\n    request_options_provider:\n      request_parameters:\n        base: \"{{ config['base'] }}\"\n  datetime_cursor:\n    type: \"DatetimeBasedCursor\"\n    start_datetime:\n      datetime: \"{{ config['start_date'] }}\"\n      datetime_format: \"%Y-%m-%d\"\n    end_datetime:\n      datetime: \"{{ now_utc() }}\"\n      datetime_format: \"%Y-%m-%d %H:%M:%S.%f+00:00\"\n    step: \"P1D\"\n    datetime_format: \"%Y-%m-%d\"\n    cursor_granularity: \"P1D\"\n    cursor_field: \"date\"\n  retriever:\n    record_selector:\n      $ref: \"#/definitions/selector\"\n    paginator:\n      type: NoPagination\n    requester:\n      $ref: \"#/definitions/requester\"\n  base_stream:\n    incremental_sync:\n      $ref: \"#/definitions/datetime_cursor\"\n    retriever:\n      $ref: \"#/definitions/retriever\"\n  rates_stream:\n    $ref: \"#/definitions/base_stream\"\n    $parameters:\n      name: \"rates\"\n      primary_key: \"date\"\n      path: \"/exchangerates_data/{{stream_slice['start_time'] or 'latest'}}\"\nstreams:\n  - \"#/definitions/rates_stream\"\ncheck:\n  stream_names:\n    - \"rates\"\nspec: \n  documentation_url: https://docs.airbyte.io/integrations/sources/exchangeratesapi\n  connection_specification:\n    $schema: http://json-schema.org/draft-07/schema#\n    title: exchangeratesapi.io Source Spec\n    type: object\n    required:\n      - start_date\n      - access_key\n      - base\n    additionalProperties: true\n    properties:\n      start_date:\n        type: string\n        description: Start getting data from that date.\n        pattern: ^[0-9]{4}-[0-9]{2}-[0-9]{2}$\n        examples:\n          - YYYY-MM-DD\n      access_key:\n        type: string\n        description: >-\n          Your API Access Key. See here. The key is\n          case sensitive.\n        airbyte_secret: true\n      base:\n        type: string\n        description: >-\n          ISO reference currency. See here.\n        examples:\n          - EUR\n          - USD\n```\nRunning the `read` operation will now read all data for all days between start_date and now:\n`bash\npython main.py read --config secrets/config.json --catalog integration_tests/configured_catalog.json`\nThe operation should now output more than one record:\n`{\"type\": \"LOG\", \"log\": {\"level\": \"INFO\", \"message\": \"Read 8 records from rates stream\"}}`\nSupporting incremental syncs\nInstead of always reading data for all dates, we would like the connector to only read data for dates we haven't read yet.\nThis can be achieved by updating the catalog to run in incremental mode (`integration_tests/configured_catalog.json`):\n`json\n{\n  \"streams\": [\n    {\n      \"stream\": {\n        \"name\": \"rates\",\n        \"json_schema\": {},\n        \"supported_sync_modes\": [\n          \"full_refresh\",\n          \"incremental\"\n        ]\n      },\n      \"sync_mode\": \"incremental\",\n      \"destination_sync_mode\": \"overwrite\"\n    }\n  ]\n}`\nIn addition to records, the `read` operation now also outputs state messages:\n`{\"type\": \"STATE\", \"state\": {\"data\": {\"rates\": {\"date\": \"2022-07-15\"}}}}`\nWhere the date (\"2022-07-15\") should be replaced by today's date.\nWe can simulate incremental syncs by creating a state file containing the last state produced by the `read` operation.\n`source-exchange-rates-tutorial/integration_tests/sample_state.json`:\n`json\n{\n  \"rates\": {\n    \"date\": \"2022-07-15\"\n  }\n}`\nRunning the `read` operation will now only read data for dates later than the given state:\n`bash\npython main.py read --config secrets/config.json --catalog integration_tests/configured_catalog.json --state integration_tests/sample_state.json`\nThere shouldn't be any data read if the state is today's date:\n`{\"type\": \"LOG\", \"log\": {\"level\": \"INFO\", \"message\": \"Setting state of rates stream to {'date': '2022-07-15'}\"}}\n{\"type\": \"LOG\", \"log\": {\"level\": \"INFO\", \"message\": \"Read 0 records from rates stream\"}}`\nNext steps:\nNext, we'll run the Connector Acceptance Tests suite to ensure the connector invariants are respected.\nMore readings\n\nIncremental syncs\nPartition routers\n",
    "tag": "airbyte"
  },
  {
    "title": "Step 3: Connecting to the API",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/config-based/tutorial/3-connecting-to-the-API-source.md",
    "content": "Step 3: Connecting to the API\nWe're now ready to start implementing the connector.\nOver the course of this tutorial, we'll be editing a few files that were generated by the code generator:\n\n`source-exchange-rates-tutorial/source_exchange_rates_tutorial/manifest.yaml`: This is the connector manifest. It describes how the data should be read from the API source, as well as what inputs can be used to configure the connector.\n`source-exchange_rates-tutorial/integration_tests/configured_catalog.json`: This is the connector's catalog. It describes what data is available in a source\n`source-exchange-rates-tutorial/integration_tests/sample_state.json`: This is a sample state object to be used to test incremental syncs.\n\nWe'll also be creating the following files:\n\n`source-exchange-rates-tutorial/secrets/config.json`: This is the configuration file we'll be using to test the connector. Its schema should match the schema defined in the spec file.\n`source-exchange-rates-tutorial/secrets/invalid_config.json`: This is an invalid configuration file we'll be using to test the connector. Its schema should match the schema defined in the spec file.\n`source_exchange_rates_tutorial/schemas/rates.json`: This is the schema definition for the stream we'll implement.\n\nUpdating the connector spec and config\nLet's populate the specification (`spec`) and the configuration (`secrets/config.json`) so the connector can access the access key and base currency.\n\nWe'll add these properties to the `spec` block in the `source-exchange-rates-tutorial/source_exchange_rates_tutorial/manifest.yaml`\n\n`yaml\nspec: \n  documentation_url: https://docs.airbyte.io/integrations/sources/exchangeratesapi\n  connection_specification:\n    $schema: http://json-schema.org/draft-07/schema#\n    title: exchangeratesapi.io Source Spec\n    type: object\n    required:\n      - access_key\n      - base\n    additionalProperties: true\n    properties:\n      access_key:\n        type: string\n        description: >-\n          Your API Access Key. See <a\n          href=\"https://exchangeratesapi.io/documentation/\">here</a>. The key is\n          case sensitive.\n        airbyte_secret: true\n      base:\n        type: string\n        description: >-\n          ISO reference currency. See <a\n          href=\"https://www.ecb.europa.eu/stats/policy_and_exchange_rates/euro_reference_exchange_rates/html/index.en.html\">here</a>.\n        examples:\n          - EUR\n          - USD`\n\nWe also need to fill in the connection config in the `secrets/config.json`\n   Because of the sensitive nature of the access key, we recommend storing this config in the `secrets` directory because it is ignored by git.\n\n`bash\necho '{\"access_key\": \"<your_access_key>\", \"base\": \"USD\"}'  > secrets/config.json`\nUpdating the connector definition\nNext, we'll update the connector definition (`source-exchange-rates-tutorial/source_exchange_rates_tutorial/manifest.yaml`). It was generated by the code generation script.\nMore details on the connector definition file can be found in the overview and connection definition sections.\nLet's fill this out these TODOs with the information found in the Exchange Rates API docs.\n\nWe'll first set the API's base url. According to the API documentation, the base url is `\"https://api.apilayer.com\"`.\n\n`yaml\ndefinitions:\n  <...>\n  requester:\n    url_base: \"https://api.apilayer.com\"`\n\nThen, let's rename the stream from `customers` to `rates`, update the primary key to `date`, and set the path to \"/exchangerates_data/latest\" as per the API's documentation. This path is specific to the stream, so we'll set it within the `rates_stream` definition\n\n`yaml\n  rates_stream:\n    $ref: \"#/definitions/base_stream\"\n    $parameters:\n      name: \"rates\"\n      primary_key: \"date\"\n      path: \"/exchangerates_data/latest\"`\nWe'll also update the reference in the `streams` block\n`yaml\nstreams:\n  - \"#/definitions/rates_stream\"`\n\nUpdate the references in the `check` block\n\n`yaml\ncheck:\n  stream_names:\n    - \"rates\"`\nAdding the reference in the `check` tells the `check` operation to use that stream to test the connection.\n\nNext, we'll set up the authentication.\n   The Exchange Rates API requires an access key to be passed as header named \"apikey\".\n   This can be done using an `ApiKeyAuthenticator`, which we'll configure to point to the config's `access_key` field.\n\n`yaml\ndefinitions:\n  <...>\n  requester:\n    url_base: \"https://api.apilayer.com\"\n    http_method: \"GET\"\n    authenticator:\n      type: ApiKeyAuthenticator\n      header: \"apikey\"\n      api_token: \"{{ config['access_key'] }}\"`\n\nAccording to the ExchangeRatesApi documentation, we can specify the base currency of interest in a request parameter. Let's assume the user will configure this via the connector configuration in parameter called `base`; we'll pass the value input by the user as a request parameter:\n\n`yaml\ndefinitions:\n  <...>\n  requester:\n    <...>\n    request_options_provider:\n      request_parameters:\n        base: \"{{ config['base'] }}\"`\nThe full connector definition should now look like\n```yaml\nversion: \"0.1.0\"\ndefinitions:\n  selector:\n    extractor:\n      field_path: [ ]\n  requester:\n    url_base: \"https://api.apilayer.com\"\n    http_method: \"GET\"\n    authenticator:\n      type: ApiKeyAuthenticator\n      header: \"apikey\"\n      api_token: \"{{ config['access_key'] }}\"\n    request_options_provider:\n      request_parameters:\n        base: \"{{ config['base'] }}\"\n  retriever:\n    record_selector:\n      $ref: \"#/definitions/selector\"\n    paginator:\n      type: NoPagination\n    requester:\n      $ref: \"#/definitions/requester\"\n  base_stream:\n    retriever:\n      $ref: \"#/definitions/retriever\"\n  rates_stream:\n    $ref: \"#/definitions/base_stream\"\n    $parameters:\n      name: \"rates\"\n      primary_key: \"date\"\n      path: \"/exchangerates_data/latest\"\nstreams:\n  - \"#/definitions/rates_stream\"\ncheck:\n  stream_names:\n    - \"rates\"\nspec: \n  documentation_url: https://docs.airbyte.io/integrations/sources/exchangeratesapi\n  connection_specification:\n    $schema: http://json-schema.org/draft-07/schema#\n    title: exchangeratesapi.io Source Spec\n    type: object\n    required:\n      - access_key\n      - base\n    additionalProperties: true\n    properties:\n      access_key:\n        type: string\n        description: >-\n          Your API Access Key. See here. The key is\n          case sensitive.\n        airbyte_secret: true\n      base:\n        type: string\n        description: >-\n          ISO reference currency. See here.\n        examples:\n          - EUR\n          - USD\n```\nWe can now run the `check` operation, which verifies the connector can connect to the API source.\n`bash\npython main.py check --config secrets/config.json`\nwhich should now succeed with logs similar to:\n`{\"type\": \"LOG\", \"log\": {\"level\": \"INFO\", \"message\": \"Check succeeded\"}}\n{\"type\": \"CONNECTION_STATUS\", \"connectionStatus\": {\"status\": \"SUCCEEDED\"}}`\nNext steps\nNext, we'll extract the records from the response\nMore readings\n\nConfig-based connectors overview\nAuthentication\nRequest options providers\nSchema definition\nConnector specification reference\n",
    "tag": "airbyte"
  },
  {
    "title": "Authentication",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/config-based/understanding-the-yaml-file/authentication.md",
    "content": "Authentication\nThe `Authenticator` defines how to configure outgoing HTTP requests to authenticate on the API source.\nSchema:\n`yaml\n  Authenticator:\n    type: object\n    description: \"Authenticator type\"\n    anyOf:\n      - \"$ref\": \"#/definitions/OAuth\"\n      - \"$ref\": \"#/definitions/ApiKeyAuthenticator\"\n      - \"$ref\": \"#/definitions/BearerAuthenticator\"\n      - \"$ref\": \"#/definitions/BasicHttpAuthenticator\"`\nAuthenticators\nApiKeyAuthenticator\nThe `ApiKeyAuthenticator` sets an HTTP header on outgoing requests.\nThe following definition will set the header \"Authorization\" with a value \"Bearer hello\":\nSchema:\n`yaml\n  ApiKeyAuthenticator:\n    type: object\n    additionalProperties: true\n    required:\n      - header\n      - api_token\n    properties:\n      \"$parameters\":\n        \"$ref\": \"#/definitions/$parameters\"\n      header:\n        type: string\n      api_token:\n        type: string`\nExample:\n`yaml\nauthenticator:\n  type: \"ApiKeyAuthenticator\"\n  header: \"Authorization\"\n  api_token: \"Bearer hello\"`\nBearerAuthenticator\nThe `BearerAuthenticator` is a specialized `ApiKeyAuthenticator` that always sets the header \"Authorization\" with the value \"Bearer {token}\".\nThe following definition will set the header \"Authorization\" with a value \"Bearer hello\"\nSchema:\n`yaml\n  BearerAuthenticator:\n    type: object\n    additionalProperties: true\n    required:\n      - api_token\n    properties:\n      \"$parameters\":\n        \"$ref\": \"#/definitions/$parameters\"\n      api_token:\n        type: string`\nExample:\n`yaml\nauthenticator:\n  type: \"BearerAuthenticator\"\n  api_token: \"hello\"`\nMore information on bearer authentication can be found here.\nBasicHttpAuthenticator\nThe `BasicHttpAuthenticator` set the \"Authorization\" header with a (USER ID/password) pair, encoded using base64 as per RFC 7617.\nThe following definition will set the header \"Authorization\" with a value \"Basic {encoded credentials}\"\nSchema:\n`yaml\n  BasicHttpAuthenticator:\n    type: object\n    additionalProperties: true\n    required:\n      - username\n    properties:\n      \"$parameters\":\n        \"$ref\": \"#/definitions/$parameters\"\n      username:\n        type: string\n      password:\n        type: string`\nExample:\n`yaml\nauthenticator:\n  type: \"BasicHttpAuthenticator\"\n  username: \"hello\"\n  password: \"world\"`\nThe password is optional. Authenticating with APIs using Basic HTTP and a single API key can be done as:\nExample:\n`yaml\nauthenticator:\n  type: \"BasicHttpAuthenticator\"\n  username: \"hello\"`\nOAuth\nOAuth authentication is supported through the `OAuthAuthenticator`, which requires the following parameters:\n\ntoken_refresh_endpoint: The endpoint to refresh the access token\nclient_id: The client id\nclient_secret: The client secret\nrefresh_token: The token used to refresh the access token\nscopes (Optional): The scopes to request. Default: Empty list\ntoken_expiry_date (Optional): The access token expiration date formatted as RFC-3339 (\"%Y-%m-%dT%H:%M:%S.%f%z\")\naccess_token_name (Optional): The field to extract access token from in the response. Default: \"access_token\".\nexpires_in_name (Optional): The field to extract expires_in from in the response. Default: \"expires_in\"\nrefresh_request_body (Optional): The request body to send in the refresh request. Default: None\ngrant_type (Optional): The parameter specified grant_type to request access_token. Default: \"refresh_token\"\n\nSchema:\n`yaml\n  OAuth:\n    type: object\n    additionalProperties: true\n    required:\n      - token_refresh_endpoint\n      - client_id\n      - client_secret\n      - refresh_token\n      - access_token_name\n      - expires_in_name\n    properties:\n      \"$parameters\":\n        \"$ref\": \"#/definitions/$parameters\"\n      token_refresh_endpoint:\n        type: string\n      client_id:\n        type: string\n      client_secret:\n        type: string\n      refresh_token:\n        type: string\n      scopes:\n        type: array\n        items:\n          type: string\n        default: [ ]\n      token_expiry_date:\n        type: string\n      access_token_name:\n        type: string\n        default: \"access_token\"\n      expires_in_name:\n        type: string\n        default: \"expires_in\"\n      refresh_request_body:\n        type: object\n      grant_type:\n        type: string\n        default: \"refresh_token\"`\nExample:\n`yaml\nauthenticator:\n  type: \"OAuthAuthenticator\"\n  token_refresh_endpoint: \"https://api.searchmetrics.com/v4/token\"\n  client_id: \"{{ config['api_key'] }}\"\n  client_secret: \"{{ config['client_secret'] }}\"\n  refresh_token: \"\"`\nMore readings\n\nRequester\n",
    "tag": "airbyte"
  },
  {
    "title": "Understanding the YAML file",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/config-based/understanding-the-yaml-file/yaml-overview.md",
    "content": "Understanding the YAML file\nThe low-code framework involves editing a boilerplate YAML file. This section deep dives into the components of the YAML file.\nStream\nStreams define the schema of the data to sync, as well as how to read it from the underlying API source.\nA stream generally corresponds to a resource within the API. They are analogous to tables for a relational database source.\nBy default, the schema of a stream's data is defined as a JSONSchema file in `<source_connector_name>/schemas/<stream_name>.json`. \nAlternately, the stream's data schema can be stored in YAML format inline in the YAML file, by including the optional `schema_loader` key. If the data schema is provided inline, any schema on disk for that stream will be ignored.\nMore information on how to define a stream's schema can be found here\nThe stream object is represented in the YAML file as:\n`yaml\n  DeclarativeStream:\n    description: A stream whose behavior is described by a set of declarative low code components\n    type: object\n    additionalProperties: true\n    required:\n      - type\n      - retriever\n    properties:\n      type:\n        type: string\n        enum: [DeclarativeStream]\n      retriever:\n        \"$ref\": \"#/definitions/Retriever\"\n      schema_loader:\n        definition: The schema loader used to retrieve the schema for the current stream\n        anyOf:\n          - \"$ref\": \"#/definitions/InlineSchemaLoader\"\n          - \"$ref\": \"#/definitions/JsonFileSchemaLoader\"\n      stream_cursor_field:\n        definition: The field of the records being read that will be used during checkpointing\n        anyOf:\n          - type: string\n          - type: array\n            items:\n              - type: string\n      transformations:\n        definition: A list of transformations to be applied to each output record in the\n        type: array\n        items:\n          anyOf:\n            - \"$ref\": \"#/definitions/AddFields\"\n            - \"$ref\": \"#/definitions/CustomTransformation\"\n            - \"$ref\": \"#/definitions/RemoveFields\"\n      $parameters:\n        type: object\n        additional_properties: true`\nMore details on streams and sources can be found in the basic concepts section.\nConfiguring a stream for incremental syncs\nIf you want to allow your stream to be configured so that only data that has changed since the prior sync is replicated to a destination, you can specify a `DatetimeBasedCursor` on your `Streams`'s `incremental_sync` field.\nGiven a start time, an end time, and a step function, it will partition the interval [start, end] into small windows of the size described by the step.\nMore information on `incremental_sync` configurations and the `DatetimeCursorBased` component can be found in the incremental syncs section.\nData retriever\nThe data retriever defines how to read the data for a Stream and acts as an orchestrator for the data retrieval flow.\nIt is described by:\n\nRequester: Describes how to submit requests to the API source\nPaginator: Describes how to navigate through the API's pages\nRecord selector: Describes how to extract records from a HTTP response\nPartition router: Describes how to retrieve data across multiple resource locations \n\nEach of those components (and their subcomponents) are defined by an explicit interface and one or many implementations.\nThe developer can choose and configure the implementation they need depending on specifications of the integration they are building against.\nSince the `Retriever` is defined as part of the Stream configuration, different Streams for a given Source can use different `Retriever` definitions if needed.\nThe schema of a retriever object is:\n`yaml\n  retriever:\n    description: Retrieves records by synchronously sending requests to fetch records. The retriever acts as an orchestrator between the requester, the record selector, the paginator, and the partition router.\n    type: object\n    required:\n      - requester\n      - record_selector\n      - requester\n    properties:\n      \"$parameters\":\n        \"$ref\": \"#/definitions/$parameters\"\n      requester:\n        \"$ref\": \"#/definitions/Requester\"\n      record_selector:\n        \"$ref\": \"#/definitions/HttpSelector\"\n      paginator:\n        \"$ref\": \"#/definitions/Paginator\"\n      stream_slicer:\n        \"$ref\": \"#/definitions/StreamSlicer\"\n  PrimaryKey:\n    type: string`\nRouting to Data that is Partitioned in Multiple Locations\nSome sources might require specifying additional parameters that are needed to retrieve data. Using the `PartitionRouter` component, you can specify a static or dynamic set of elements which will be iterated upon and made available for use when a connector dispatches requests to get data from a source.\nMore information on how to configure the `partition_router` field on a Retriever to retrieve data from multiple location can be found in the iteration section.\nCombining Incremental Syncs and Iterable Locations\nA stream can be configured to support incrementally syncing data that is spread across multiple partitions by defining `incremental_sync` on the `Stream` and `partition_router` on the `Retriever`.\nDuring a sync where both are configured, the Cartesian product of these parameters will be calculated and the connector will repeat requests to the source using the different combinations of parameters to get all of the data.\nFor example, if we had a `DatetimeBasedCursor` requesting data over a 3-day range partitioned by day and a `ListPartitionRouter` with the following locations `A`, `B`, and `C`. This would result in the following combinations that will be used to request data.\n| Partition | Date Range                                |\n|-----------|-------------------------------------------|\n| A         | 2022-01-01T00:00:00 - 2022-01-01T23:59:59 |\n| B         | 2022-01-01T00:00:00 - 2022-01-01T23:59:59 |\n| C         | 2022-01-01T00:00:00 - 2022-01-01T23:59:59 |\n| A         | 2022-01-02T00:00:00 - 2022-01-02T23:59:59 |\n| B         | 2022-01-02T00:00:00 - 2022-01-02T23:59:59 |\n| C         | 2022-01-02T00:00:00 - 2022-01-02T23:59:59 |\n| A         | 2022-01-03T00:00:00 - 2022-01-03T23:59:59 |\n| B         | 2022-01-03T00:00:00 - 2022-01-03T23:59:59 |\n| C         | 2022-01-03T00:00:00 - 2022-01-03T23:59:59 |\nMore readings\n\nRequester\nIncremental Syncs\n",
    "tag": "airbyte"
  },
  {
    "title": "Record selector",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/config-based/understanding-the-yaml-file/record-selector.md",
    "content": "Record selector\nThe record selector is responsible for translating an HTTP response into a list of Airbyte records by extracting records from the response and optionally filtering and shaping records based on a heuristic.\nSchema:\n`yaml\n  HttpSelector:\n    type: object\n    anyOf:\n      - \"$ref\": \"#/definitions/RecordSelector\"\n  RecordSelector:\n    type: object\n    required:\n      - extractor\n    properties:\n      \"$parameters\":\n        \"$ref\": \"#/definitions/$parameters\"\n      extractor:\n        \"$ref\": \"#/definitions/RecordExtractor\"\n      record_filter:\n        \"$ref\": \"#/definitions/RecordFilter\"`\nThe current record extraction implementation uses dpath to select records from the json-decoded HTTP response.\nFor nested structures `*` can be used to iterate over array elements.\nSchema:\n`yaml\n  DpathExtractor:\n    type: object\n    additionalProperties: true\n    required:\n      - field_path\n    properties:\n      \"$parameters\":\n        \"$ref\": \"#/definitions/$parameters\"\n      field_path:\n        type: array\n        items:\n          type: string`\nCommon recipes:\nHere are some common patterns:\nSelecting the whole response\nIf the root of the response is an array containing the records, the records can be extracted using the following definition:\n`yaml\nselector:\n  extractor:\n    field_path: [ ]`\nIf the root of the response is a json object representing a single record, the record can be extracted and wrapped in an array.\nFor example, given a response body of the form\n`json\n{\n  \"id\": 1\n}`\nand a selector\n`yaml\nselector:\n  extractor:\n    field_path: [ ]`\nThe selected records will be\n`json\n[\n  {\n    \"id\": 1\n  }\n]`\nSelecting a field\nGiven a response body of the form\n`{\n  \"data\": [{\"id\": 0}, {\"id\": 1}],\n  \"metadata\": {\"api-version\": \"1.0.0\"}\n}`\nand a selector\n`yaml\nselector:\n  extractor:\n    field_path: [ \"data\" ]`\nThe selected records will be\n`json\n[\n  {\n    \"id\": 0\n  },\n  {\n    \"id\": 1\n  }\n]`\nSelecting an inner field\nGiven a response body of the form\n`json\n{\n  \"data\": {\n    \"records\": [\n      {\n        \"id\": 1\n      },\n      {\n        \"id\": 2\n      }\n    ]\n  }\n}`\nand a selector\n`yaml\nselector:\n  extractor:\n    field_path: [ \"data\", \"records\" ]`\nThe selected records will be\n`json\n[\n  {\n    \"id\": 1\n  },\n  {\n    \"id\": 2\n  }\n]`\nSelecting fields nested in arrays\nGiven a response body of the form\n```json\n{\n  \"data\": [\n    {\n      \"record\": {\n        \"id\": \"1\"\n      }\n    },\n    {\n      \"record\": {\n        \"id\": \"2\"\n      }\n    }\n  ]\n}\n```\nand a selector\n`yaml\nselector:\n  extractor:\n    field_path: [ \"data\", \"*\", \"record\" ]`\nThe selected records will be\n`json\n[\n  {\n    \"id\": 1\n  },\n  {\n    \"id\": 2\n  }\n]`\nFiltering records\nRecords can be filtered by adding a record_filter to the selector.\nThe expression in the filter will be evaluated to a boolean returning true if the record should be included.\nIn this example, all records with a `created_at` field greater than the stream slice's `start_time` will be filtered out:\n`yaml\nselector:\n  extractor:\n    field_path: [ ]\n  record_filter:\n    condition: \"{{ record['created_at'] < stream_slice['start_time'] }}\"`\nTransformations\nFields can be added or removed from records by adding `Transformation`s to a stream's definition.\nSchema:\n`yaml\n  RecordTransformation:\n    type: object\n    anyOf:\n      - \"$ref\": \"#/definitions/AddFields\"\n      - \"$ref\": \"#/definitions/RemoveFields\"`\nAdding fields\nFields can be added with the `AddFields` transformation.\nThis example adds a top-level field \"field1\" with a value \"static_value\"\nSchema:\n`yaml\n  AddFields:\n    type: object\n    required:\n      - fields\n    additionalProperties: true\n    properties:\n      \"$parameters\":\n        \"$ref\": \"#/definitions/$parameters\"\n      fields:\n        type: array\n        items:\n          \"$ref\": \"#/definitions/AddedFieldDefinition\"\n  AddedFieldDefinition:\n    type: object\n    required:\n      - path\n      - value\n    additionalProperties: true\n    properties:\n      \"$parameters\":\n        \"$ref\": \"#/definitions/$parameters\"\n      path:\n        \"$ref\": \"#/definitions/FieldPointer\"\n      value:\n        type: string\n  FieldPointer:\n    type: array\n    items:\n      type: string`\nExample:\n`yaml\nstream:\n  <...>\n  transformations:\n      - type: AddFields\n        fields:\n          - path: [ \"field1\" ]\n            value: \"static_value\"`\nThis example adds a top-level field \"start_date\", whose value is evaluated from the stream slice:\n`yaml\nstream:\n  <...>\n  transformations:\n      - type: AddFields\n        fields:\n          - path: [ \"start_date\" ]\n            value: { { stream_slice[ 'start_date' ] } }`\nFields can also be added in a nested object by writing the fields' path as a list.\nGiven a record of the following shape:\n`{\n  \"id\": 0,\n  \"data\":\n  {\n    \"field0\": \"some_data\"\n  }\n}`\nthis definition will add a field in the \"data\" nested object:\n`yaml\nstream:\n  <...>\n  transformations:\n      - type: AddFields\n        fields:\n          - path: [ \"data\", \"field1\" ]\n            value: \"static_value\"`\nresulting in the following record:\n`{\n  \"id\": 0,\n  \"data\":\n  {\n    \"field0\": \"some_data\",\n    \"field1\": \"static_value\"\n  }\n}`\nRemoving fields\nFields can be removed from records with the `RemoveFields` transformation.\nSchema:\n```yaml\n  RemoveFields:\n    type: object\n    required:\n      - field_pointers\n    additionalProperties: true\n    properties:\n      \"$parameters\":\n        \"$ref\": \"#/definitions/$parameters\"\n      field_pointers:\n        type: array\n        items:\n          \"$ref\": \"#/definitions/FieldPointer\"\n```\nGiven a record of the following shape:\n`{\n  \"path\": \n  {\n    \"to\":\n    {\n      \"field1\": \"data_to_remove\",\n      \"field2\": \"data_to_keep\"\n    }\n  },\n  \"path2\": \"data_to_remove\",\n  \"path3\": \"data_to_keep\"\n}`\nthis definition will remove the 2 instances of \"data_to_remove\" which are found in \"path2\" and \"path.to.field1\":\n`yaml\nthe_stream:\n  <...>\n  transformations:\n      - type: RemoveFields\n        field_pointers:\n          - [ \"path\", \"to\", \"field1\" ]\n          - [ \"path2\" ]`\nresulting in the following record:\n```\n{\n  \"path\": \n  {\n    \"to\":\n    {\n      \"field2\": \"data_to_keep\"\n    }\n  },\n  \"path3\": \"data_to_keep\"\n}",
    "tag": "airbyte"
  },
  {
    "title": "Pagination",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/config-based/understanding-the-yaml-file/pagination.md",
    "content": "Pagination\nGiven a page size and a pagination strategy, the `DefaultPaginator` will point to pages of results for as long as its strategy returns a `next_page_token`.\nIterating over pages of result is different from iterating over stream slices.\nStream slices have semantic value, for instance, a Datetime stream slice defines data for a specific date range. Two stream slices will have data for different date ranges.\nConversely, pages don't have semantic value. More pages simply means that more records are to be read, without specifying any meaningful difference between the records of the first and later pages.\nSchema:\n`yaml\n  Paginator:\n    type: object\n    anyOf:\n      - \"$ref\": \"#/definitions/DefaultPaginator\"\n      - \"$ref\": \"#/definitions/NoPagination\"\n  NoPagination:\n    type: object\n    additionalProperties: true`\nDefault paginator\nThe default paginator is defined by\n\n`page_size_option`: How to specify the page size in the outgoing HTTP request\n`pagination_strategy`: How to compute the next page to fetch\n`page_token_option`: How to specify the next page to fetch in the outgoing HTTP request\n\nSchema:\n`yaml\n  DefaultPaginator:\n    type: object\n    additionalProperties: true\n    required:\n      - page_token_option\n      - pagination_strategy\n    properties:\n      \"$parameters\":\n        \"$ref\": \"#/definitions/$parameters\"\n      page_size:\n        type: integer\n      page_size_option:\n        \"$ref\": \"#/definitions/RequestOption\"\n      page_token_option:\n        anyOf:\n          - \"$ref\": \"#/definitions/RequestOption\"\n          - \"$ref\": \"#/definitions/RequestPath\"\n      pagination_strategy:\n        \"$ref\": \"#/definitions/PaginationStrategy\"`\n3 pagination strategies are supported\n\nPage increment\nOffset increment\nCursor-based\n\nPagination Strategies\nSchema:\n`yaml\n  PaginationStrategy:\n    type: object\n    anyOf:\n      - \"$ref\": \"#/definitions/CursorPaginator\"\n      - \"$ref\": \"#/definitions/OffsetIncrement\"\n      - \"$ref\": \"#/definitions/PageIncrement\"`\nPage increment\nWhen using the `PageIncrement` strategy, the page number will be set as part of the `page_token_option`.\nSchema:\n`yaml\n  PageIncrement:\n    type: object\n    additionalProperties: true\n    required:\n      - page_size\n    properties:\n      \"$parameters\":\n        \"$ref\": \"#/definitions/$parameters\"\n      page_size:\n        type: integer`\nThe following paginator example will fetch 5 records per page, and specify the page number as a request_parameter:\nExample:\n`yaml\npaginator:\n  type: \"DefaultPaginator\"\n  page_size_option:\n    type: \"RequestOption\"\n    inject_into: \"request_parameter\"\n    field_name: \"page_size\"\n  pagination_strategy:\n    type: \"PageIncrement\"\n    page_size: 5\n  page_token_option:\n    type: \"RequestOption\"\n    inject_into: \"request_parameter\"\n    field_name: \"page\"`\nIf the page contains less than 5 records, then the paginator knows there are no more pages to fetch.\nIf the API returns more records than requested, all records will be processed.\nAssuming the endpoint to fetch data from is `https://cloud.airbyte.com/api/get_data`,\nthe first request will be sent as `https://cloud.airbyte.com/api/get_data?page_size=5&page=0`\nand the second request as `https://cloud.airbyte.com/api/get_data?page_size=5&page=1`,\nOffset increment\nWhen using the `OffsetIncrement` strategy, the number of records read will be set as part of the `page_token_option`.\nSchema:\n`yaml\n  OffsetIncrement:\n    type: object\n    additionalProperties: true\n    required:\n      - page_size\n    properties:\n      \"$parameters\":\n        \"$ref\": \"#/definitions/$parameters\"\n      page_size:\n        type: integer`\nThe following paginator example will fetch 5 records per page, and specify the offset as a request_parameter:\nExample:\n`yaml\npaginator:\n  type: \"DefaultPaginator\"\n  page_size_option:\n    type: \"RequestOption\"\n    inject_into: \"request_parameter\"\n    field_name: \"page_size\"\n  pagination_strategy:\n    type: \"OffsetIncrement\"\n    page_size: 5\n  page_token_option:\n    type: \"RequestOption\"\n    field_name: \"offset\"\n    inject_into: \"request_parameter\"`\nAssuming the endpoint to fetch data from is `https://cloud.airbyte.com/api/get_data`,\nthe first request will be sent as `https://cloud.airbyte.com/api/get_data?page_size=5&offset=0`\nand the second request as `https://cloud.airbyte.com/api/get_data?page_size=5&offset=5`,\nCursor\nThe `CursorPagination` outputs a token by evaluating its `cursor_value` string with the following parameters:\n\n`response`: The decoded response\n`headers`: HTTP headers on the response\n`last_records`: List of records selected from the last response\n\nThis cursor value can be used to request the next page of record.\nSchema:\n`yaml\n  CursorPagination:\n    type: object\n    additionalProperties: true\n    required:\n      - cursor_value\n    properties:\n      \"$parameters\":\n        \"$ref\": \"#/definitions/$parameters\"\n      cursor_value:\n        type: string\n      stop_condition:\n        type: string\n      page_size:\n        type: integer`\nCursor paginator in request parameters\nIn this example, the next page of record is defined by setting the `from` request parameter to the id of the last record read:\n`yaml\npaginator:\n  type: \"DefaultPaginator\"\n  <...>\n  pagination_strategy:\n    type: \"CursorPagination\"\n    cursor_value: \"{{ last_records[-1]['id'] }}\"\n  page_token_option:\n    type: \"RequestPath\"\n    field_name: \"from\"\n    inject_into: \"request_parameter\"`\nAssuming the endpoint to fetch data from is `https://cloud.airbyte.com/api/get_data`,\nthe first request will be sent as `https://cloud.airbyte.com/api/get_data`.\nAssuming the id of the last record fetched is 1000,\nthe next request will be sent as `https://cloud.airbyte.com/api/get_data?from=1000`.\nCursor paginator in path\nSome APIs directly point to the URL of the next page to fetch. In this example, the URL of the next page is extracted from the response headers:\n`yaml\npaginator:\n  type: \"DefaultPaginator\"\n  <...>\n  pagination_strategy:\n    type: \"CursorPagination\"\n    cursor_value: \"{{ headers['urls']['next'] }}\"\n  page_token_option:\n    type: \"RequestPath\"`\nAssuming the endpoint to fetch data from is `https://cloud.airbyte.com/api/get_data`,\nthe first request will be sent as `https://cloud.airbyte.com/api/get_data`\nAssuming the response's next url is `https://cloud.airbyte.com/api/get_data?page=1&page_size=100`,",
    "tag": "airbyte"
  },
  {
    "title": "Request Options",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/config-based/understanding-the-yaml-file/request-options.md",
    "content": "Request Options\nThe primary way to set request parameters and headers is to define them as key-value pairs using a `RequestOptionsProvider`.\nOther components, such as an `Authenticator` can also set additional request params or headers as needed.\nAdditionally, some stateful components use a `RequestOption` to configure the options and update the value. Example of such components are Paginators and DatetimeBasedCursors.\nRequest Options Provider\nThe primary way to set request options is through the `Requester`'s `RequestOptionsProvider`.\nThe options can be configured as key value pairs:\nSchema:\n`yaml\n  RequestOptionsProvider:\n    type: object\n    anyOf:\n      - \"$ref\": \"#/definitions/InterpolatedRequestOptionsProvider\"\n  InterpolatedRequestOptionsProvider:\n    type: object\n    additionalProperties: true\n    properties:\n      \"$parameters\":\n        \"$ref\": \"#/definitions/$parameters\"\n      request_parameters:\n        \"$ref\": \"#/definitions/RequestInput\"\n      request_headers:\n        \"$ref\": \"#/definitions/RequestInput\"\n      request_body_data:\n        \"$ref\": \"#/definitions/RequestInput\"\n      request_body_json:\n        \"$ref\": \"#/definitions/RequestInput\"`\nExample:\n`yaml\nrequester:\n  type: HttpRequester\n  url_base: \"https://api.exchangeratesapi.io/v1/\"\n  http_method: \"GET\"\n  request_options_provider:\n    request_parameters:\n      k1: v1\n      k2: v2\n    request_headers:\n      header_key1: header_value1\n      header_key2: header_value2`\nIt is also possible to configure add a json-encoded body to outgoing requests.\n`yaml\nrequester:\n  type: HttpRequester\n  url_base: \"https://api.exchangeratesapi.io/v1/\"\n  http_method: \"GET\"\n  request_options_provider:\n    request_body_json:\n      key: value`\nRequest Options\nSome components can add request options to the requests sent to the API endpoint.\nSchema:\n`yaml\n  RequestOption:\n    description: A component that specifies the key field and where in the request a component's value should be inserted into.\n    type: object\n    required:\n      - type\n      - field_name\n      - inject_into\n    properties:\n      type:\n        type: string\n        enum: [RequestOption]\n      field_name:\n        type: string\n      inject_into:\n        enum:\n          - request_parameter\n          - header\n          - body_data\n          - body_json`\nRequest Path\nAs an alternative to adding various options to the request being sent, some components can be configured to\nmodify the HTTP path of the API endpoint being accessed.\nSchema:\n`yaml\n  RequestPath:\n    description: A component that specifies where in the request path a component's value should be inserted into.\n    type: object\n    required:\n      - type\n    properties:\n      type:\n        type: string\n        enum: [RequestPath]`\nAuthenticators\nIt is also possible for authenticators to set request parameters or headers as needed.\nFor instance, the `BearerAuthenticator` will always set the `Authorization` header.\nMore details on the various authenticators can be found in the authentication section.\nPaginators\nThe `DefaultPaginator` can optionally set request options through the `page_size_option` and the `page_token_option`.\nThe respective values can be set on the outgoing HTTP requests by specifying where it should be injected.\nThe following example will set the \"page\" request parameter value to the page to fetch, and the \"page_size\" request parameter to 5:\n`yaml\npaginator:\n  type: \"DefaultPaginator\"\n  page_size_option:\n    type: \"RequestOption\"\n    inject_into: request_parameter\n    field_name: page_size\n  pagination_strategy:\n    type: \"PageIncrement\"\n    page_size: 5\n  page_token:\n    type: \"RequestOption\"\n    inject_into: \"request_parameter\"\n    field_name: \"page\"`\nMore details on paginators can be found in the pagination section.\nIncremental syncs\nThe `DatetimeBasedCursor` can optionally set request options through the `start_time_option` and `end_time_option` fields.\nThe respective values can be set on the outgoing HTTP requests by specifying where it should be injected.\nThe following example will set the \"created[gte]\" request parameter value to the start of the time window, and \"created[lte]\" to the end of the time window.\n`yaml\nincremental_sync:\n  type: DatetimeBasedCursor\n  start_datetime: \"2021-02-01T00:00:00.000000+0000\",\n  end_datetime: \"2021-03-01T00:00:00.000000+0000\",\n  step: \"P1D\"\n  start_time_option:\n    type: \"RequestOption\"\n    field_name: \"created[gte]\"\n    inject_into: \"request_parameter\"\n  end_time_option:\n    type: \"RequestOption\"\n    field_name: \"created[lte]\"\n    inject_into: \"request_parameter\"`\nMore details on incremental syncs can be found in the incremental syncs section.\nMore readings\n\nRequester\nPagination\n",
    "tag": "airbyte"
  },
  {
    "title": "Error handling",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/config-based/understanding-the-yaml-file/error-handling.md",
    "content": "Error handling\nBy default, only server errors (HTTP 5XX) and too many requests (HTTP 429) will be retried up to 5 times with exponential backoff.\nOther HTTP errors will result in a failed read.\nOther behaviors can be configured through the `Requester`'s `error_handler` field.\nSchema:\n`yaml\n  ErrorHandler:\n    type: object\n    description: \"Error handler\"\n    anyOf:\n      - \"$ref\": \"#/definitions/DefaultErrorHandler\"\n      - \"$ref\": \"#/definitions/CompositeErrorHandler\"`\nDefault error handler\nSchema:\n`yaml\n  DefaultErrorHandler:\n    type: object\n    required:\n      - max_retries\n    additionalProperties: true\n    properties:\n      \"$parameters\":\n        \"$ref\": \"#/definitions/$parameters\"\n      response_filters:\n        type: array\n        items:\n          \"$ref\": \"#/definitions/HttpResponseFilter\"\n      max_retries:\n        type: integer\n        default: 5\n      backoff_strategies:\n        type: array\n        items:\n          \"$ref\": \"#/definitions/BackoffStrategy\"\n        default: [ ]`\nDefining errors\nFrom status code\nResponse filters can be used to define how to handle requests resulting in responses with a specific HTTP status code.\nFor instance, this example will configure the handler to also retry responses with 404 error:\nSchema:\n`yaml\n  HttpResponseFilter:\n    type: object\n    required:\n      - action\n    additionalProperties: true\n    properties:\n      \"$parameters\":\n        \"$ref\": \"#/definitions/$parameters\"\n      action:\n        \"$ref\": \"#/definitions/ResponseAction\"\n      http_codes:\n        type: array\n        items:\n          type: integer\n        default: [ ]\n      error_message_contains:\n        type: string\n      predicate:\n        type: string\n  ResponseAction:\n    type: string\n    enum:\n      - SUCCESS\n      - FAIL\n      - IGNORE\n      - RETRY`\nExample:\n`yaml\nrequester:\n  <...>\n  error_handler:\n    response_filters:\n        - http_codes: [ 404 ]\n          action: RETRY`\nResponse filters can be used to specify HTTP errors to ignore.\nFor instance, this example will configure the handler to ignore responses with 404 error:\n`yaml\nrequester:\n  <...>\n  error_handler:\n    response_filters:\n        - http_codes: [ 404 ]\n          action: IGNORE`\nFrom error message\nErrors can also be defined by parsing the error message.\nFor instance, this error handler will ignore responses if the error message contains the string \"ignorethisresponse\"\n`yaml\nrequester:\n  <...>\n  error_handler:\n    response_filters:\n        - error_message_contains: \"ignorethisresponse\"\n          action: IGNORE`\nThis can also be done through a more generic string interpolation strategy with the following parameters:\n\nresponse: the decoded response\n\nThis example ignores errors where the response contains a \"code\" field:\n`yaml\nrequester:\n  <...>\n  error_handler:\n    response_filters:\n        - predicate: \"{{ 'code' in response }}\"\n          action: IGNORE`\nThe error handler can have multiple response filters.\nThe following example is configured to ignore 404 errors, and retry 429 errors:\n`yaml\nrequester:\n  <...>\n  error_handler:\n    response_filters:\n        - http_codes: [ 404 ]\n          action: IGNORE\n        - http_codes: [ 429 ]\n          action: RETRY`\nBackoff Strategies\nThe error handler supports a few backoff strategies, which are described in the following sections.\nSchema:\n`yaml\n  BackoffStrategy:\n    type: object\n    anyOf:\n      - \"$ref\": \"#/definitions/ExponentialBackoffStrategy\"\n      - \"$ref\": \"#/definitions/ConstantBackoffStrategy\"\n      - \"$ref\": \"#/definitions/WaitTimeFromHeader\"\n      - \"$ref\": \"#/definitions/WaitUntilTimeFromHeader\"`\nExponential backoff\nThis is the default backoff strategy. The requester will backoff with an exponential backoff interval\nSchema:\n`yaml\n  ExponentialBackoffStrategy:\n    type: object\n    additionalProperties: true\n    properties:\n      \"$parameters\":\n        \"$ref\": \"#/definitions/$parameters\"\n      factor:\n        type: integer\n        default: 5`\nConstant Backoff\nWhen using the `ConstantBackoffStrategy` strategy, the requester will backoff with a constant interval.\nSchema:\n`yaml\n  ConstantBackoffStrategy:\n    type: object\n    additionalProperties: true\n    required:\n      - backoff_time_in_seconds\n    properties:\n      \"$parameters\":\n        \"$ref\": \"#/definitions/$parameters\"\n      backoff_time_in_seconds:\n        type: number`\nWait time defined in header\nWhen using the `WaitTimeFromHeader`, the requester will backoff by an interval specified in the response header.\nIn this example, the requester will backoff by the response's \"wait_time\" header value:\nSchema:\n`yaml\n  WaitTimeFromHeader:\n    type: object\n    additionalProperties: true\n    required:\n      - header\n    properties:\n      \"$parameters\":\n        \"$ref\": \"#/definitions/$parameters\"\n      header:\n        type: string\n      regex:\n        type: string`\nExample:\n`yaml\nrequester:\n  <...>\n  error_handler:\n    <...>\n    backoff_strategies:\n        - type: \"WaitTimeFromHeader\"\n          header: \"wait_time\"`\nOptionally, a regular expression can be configured to extract the wait time from the header value.\nExample:\n`yaml\nrequester:\n  <...>\n  error_handler:\n    <...>\n    backoff_strategies:\n        - type: \"WaitTimeFromHeader\"\n          header: \"wait_time\"\n          regex: \"[-+]?\\d+\"`\nWait until time defined in header\nWhen using the `WaitUntilTimeFromHeader` backoff strategy, the requester will backoff until the time specified in the response header.\nIn this example, the requester will wait until the time specified in the \"wait_until\" header value:\nSchema:\n`yaml\n  WaitUntilTimeFromHeader:\n    type: object\n    additionalProperties: true\n    required:\n      - header\n    properties:\n      \"$parameters\":\n        \"$ref\": \"#/definitions/$parameters\"\n      header:\n        type: string\n      regex:\n        type: string\n      min_wait:\n        type: number`\nExample:\n`yaml\nrequester:\n  <...>\n  error_handler:\n    <...>\n    backoff_strategies:\n        - type: \"WaitUntilTimeFromHeader\"\n          header: \"wait_until\"\n          regex: \"[-+]?\\d+\"\n          min_wait: 5`\nThe strategy accepts an optional regular expression to extract the time from the header value, and a minimum time to wait.\nAdvanced error handling\nThe error handler can have multiple backoff strategies, allowing it to fallback if a strategy cannot be evaluated.\nFor instance, the following defines an error handler that will read the backoff time from a header, and default to a constant backoff if the wait time could not be extracted from the response:\nExample:\n`yaml\nrequester:\n  <...>\n  error_handler:\n    <...>\n    backoff_strategies:\n        - type: \"WaitTimeFromHeader\"\n          header: \"wait_time\"\n            - type: \"ConstantBackoff\"\n              backoff_time_in_seconds: 5`\nThe `requester` can be configured to use a `CompositeErrorHandler`, which sequentially iterates over a list of error handlers, enabling different retry mechanisms for different types of errors.\nIn this example, a constant backoff of 5 seconds, will be applied if the response contains a \"code\" field, and an exponential backoff will be applied if the error code is 403:\nSchema:\n`yaml\n  CompositeErrorHandler:\n    type: object\n    required:\n      - error_handlers\n    additionalProperties:\n      \"$parameters\":\n        \"$ref\": \"#/definitions/$parameters\"\n      error_handlers:\n        type: array\n        items:\n          \"$ref\": \"#/definitions/ErrorHandler\"`\nExample:\n`yaml\nrequester:\n  <...>\n  error_handler:\n    type: \"CompositeErrorHandler\"\n    error_handlers:\n      - response_filters:\n          - predicate: \"{{ 'code' in response }}\"\n            action: RETRY\n        backoff_strategies:\n          - type: \"ConstantBackoffStrategy\"\n            backoff_time_in_seconds: 5\n      - response_filters:\n          - http_codes: [ 403 ]\n            action: RETRY\n        backoff_strategies:\n          - type: \"ExponentialBackoffStrategy\"`\nMore readings",
    "tag": "airbyte"
  },
  {
    "title": "Retrieving Records Spread Across Partitions",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/config-based/understanding-the-yaml-file/partition-router.md",
    "content": "Retrieving Records Spread Across Partitions\nIn some cases, the data you are replicating is spread across multiple partitions. You can specify a set of parameters to be iterated over and used while requesting all of your data. On each iteration, using the current element being iterated upon, the connector will perform a cycle of requesting data from your source.\n`PartitionRouter`s gives you the ability to specify either a static or dynamic set of elements that will be iterated over one at a time. This in turn is used to route requests to a partition of your data according to the elements iterated over.\nThe most common use case for the `PartitionRouter` component is the retrieval of data from an API endpoint that requires extra request inputs to indicate which partition of data to fetch.\nSchema:\n`yaml\n  partition_router:\n    default: []\n    anyOf:\n      - \"$ref\": \"#/definitions/CustomPartitionRouter\"\n      - \"$ref\": \"#/definitions/ListPartitionRouter\"\n      - \"$ref\": \"#/definitions/SubstreamPartitionRouter\"\n      - type: array\n        items:\n          anyOf:\n            - \"$ref\": \"#/definitions/CustomPartitionRouter\"\n            - \"$ref\": \"#/definitions/ListPartitionRouter\"\n            - \"$ref\": \"#/definitions/SubstreamPartitionRouter\"`\nNotice that you can specify one or more `PartitionRouter`s on a Retriever. When multiple are defined, the result will be Cartesian product of all partitions and a request cycle will be performed for each permutation.\nListPartitionRouter\n`ListPartitionRouter` iterates over values from a given list. It is defined by\n\nThe partition values, which are the valid values for the cursor field\nThe cursor field on a record\nrequest_option: optional request option to set on outgoing request parameters\n\nSchema:\n`yaml\n  ListPartitionRouter:\n    description: Partition router that is used to retrieve records that have been partitioned according to a list of values\n    type: object\n    required:\n      - type\n      - cursor_field\n      - slice_values\n    properties:\n      type:\n        type: string\n        enum: [ListPartitionRouter]\n      cursor_field:\n        type: string\n      partition_values:\n        anyOf:\n          - type: string\n          - type: array\n            items:\n              type: string\n      request_option:\n        \"$ref\": \"#/definitions/RequestOption\"\n      $parameters:\n        type: object\n        additionalProperties: true`\nAs an example, this partition router will iterate over the 2 repositories (\"airbyte\" and \"airbyte-secret\") and will set a request_parameter on outgoing HTTP requests.\n`yaml\npartition_router:\n  type: ListPartitionRouter\n  values:\n    - \"airbyte\"\n    - \"airbyte-secret\"\n  cursor_field: \"repository\"\n  request_option:\n    type: RequestOption\n    field_name: \"repository\"\n    inject_into: \"request_parameter\"`\nSubstreamPartitionRouter\nSubstreams are streams that depend on the records of another stream\nWe might for instance want to read all the commits for a given repository (parent stream).\nSubstreams are implemented by defining their partition router as a `SubstreamPartitionRouter`.\n`SubstreamPartitionRouter` is used to route requests to fetch data that has been partitioned according to a parent stream's records . We might for instance want to read all the commits for a given repository (parent resource).\n\nwhat the parent stream is\nwhat is the key of the records in the parent stream\nwhat is the attribute on the parent record that is being used to partition the substream data\nhow to specify that attribute on an outgoing HTTP request to retrieve that set of records\n\nSchema:\n`yaml\n  SubstreamPartitionRouter:\n    description: Partition router that is used to retrieve records that have been partitioned according to records from the specified parent streams\n    type: object\n    required:\n      - type\n      - parent_stream_configs\n    properties:\n      type:\n        type: string\n        enum: [SubstreamPartitionRouter]\n      parent_stream_configs:\n        type: array\n        items:\n          \"$ref\": \"#/definitions/ParentStreamConfig\"\n      $parameters:\n        type: object\n        additionalProperties: true`\nExample:\n`yaml\npartition_router:\n  type: SubstreamPartitionRouter\n  parent_streams_configs:\n    - stream: \"#/repositories_stream\"\n      parent_key: \"id\"\n      partition_field: \"repository\"\n      request_option:\n        type: RequestOption\n        field_name: \"repository\"\n        inject_into: \"request_parameter\"`\nREST APIs often nest sub-resources in the URL path.\nIf the URL to fetch commits was \"/repositories/:id/commits\", then the `Requester`'s path would need to refer to the stream slice's value and no `request_option` would be set:\nExample:\n`yaml\nretriever:\n  <...>\n  requester:\n    <...>\n    path: \"/respositories/{{ stream_slice.repository }}/commits\"\n  partition_router:\n    type: SubstreamPartitionRouter\n    parent_streams_configs:\n      - stream: \"#/repositories_stream\"\n        parent_key: \"id\"\n        partition_field: \"repository\"`\nNested streams\nNested streams, subresources, or streams that depend on other streams can be implemented using a SubstreamPartitionRouter\nMore readings\n\nIncremental streams\nStream slices\n",
    "tag": "airbyte"
  },
  {
    "title": "Requester",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/config-based/understanding-the-yaml-file/requester.md",
    "content": "Requester\nThe `Requester` defines how to prepare HTTP requests to send to the source API.\nThere is currently only one implementation, the `HttpRequester`, which is defined by\n\nA base url: The root of the API source\nA path: The specific endpoint to fetch data from for a resource\nThe HTTP method: the HTTP method to use (GET or POST)\nA request options provider: Defines the request parameters (query parameters), headers, and request body to set on outgoing HTTP requests\nAn authenticator: Defines how to authenticate to the source\nAn error handler: Defines how to handle errors\n\nThe schema of a requester object is:\n`yaml\n  Requester:\n    type: object\n    anyOf:\n      - \"$ref\": \"#/definitions/HttpRequester\"\n  HttpRequester:\n    type: object\n    additionalProperties: true\n    required:\n      - url_base\n      - path\n    properties:\n      \"$parameters\":\n        \"$ref\": \"#/definitions/$parameters\"\n      url_base:\n        type: string\n        description: \"base url\"\n      path:\n        type: string\n        description: \"path\"\n      http_method:\n        \"$ref\": \"#/definitions/HttpMethod\"\n        default: \"GET\"\n      request_options_provider:\n        \"$ref\": \"#/definitions/RequestOptionsProvider\"\n      authenticator:\n        \"$ref\": \"#/definitions/Authenticator\"\n      error_handler:\n        \"$ref\": \"#/definitions/ErrorHandler\"\n  HttpMethod:\n    type: string\n    enum:\n      - GET\n      - POST`\nConfiguring request parameters and headers\nThe primary way to set request parameters and headers is to define them as key-value pairs using a `RequestOptionsProvider`.\nOther components, such as an `Authenticator` can also set additional request params or headers as needed.\nAdditionally, some stateful components use a `RequestOption` to configure the options and update the value. Example of such components are Paginators and Partition routers.\nMore readings",
    "tag": "airbyte"
  },
  {
    "title": "Incremental Syncs",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/config-based/understanding-the-yaml-file/incremental-syncs.md",
    "content": "Incremental Syncs\nAn incremental sync is a sync which pulls only the data that has changed since the previous sync (as opposed to all the data available in the data source).\nIncremental syncs are usually implemented using a cursor value (like a timestamp) that delineates which data was pulled and which data is new. A very common cursor value is an `updated_at` timestamp. This cursor means that records whose `updated_at` value is less than or equal than that cursor value have been synced already, and that the next sync should only export records whose `updated_at` value is greater than the cursor value.\nOn a stream, `incremental_sync` defines the connector behavior to support cursor based replication.\nWhen a stream is read incrementally, a state message will be output by the connector after reading all the records, which allows for checkpointing (link: https://docs.airbyte.com/understanding-airbyte/airbyte-protocol/#state--checkpointing). On the next incremental sync, the prior state message will be used to determine the next set of records to read.\nDatetimeBasedCursor\nThe `DatetimeBasedCursor` is used to read records from the underlying data source (e.g: an API)  according to a specified datetime range. This time range is partitioned into time windows according to the `step`. For example, if you have `start_time=2022-01-01T00:00:00`, `end_time=2022-01-05T00:00:00` and `step=P1D`, the following partitions will be created:\n| Start               | End                 |\n|---------------------|---------------------|\n| 2022-01-01T00:00:00 | 2022-01-01T23:59:59 |\n| 2022-01-02T00:00:00 | 2022-01-02T23:59:59 |\n| 2022-01-03T00:00:00 | 2022-01-03T23:59:59 |\n| 2022-01-04T00:00:00 | 2022-01-04T23:59:59 |\n| 2022-01-05T00:00:00 | 2022-01-05T00:00:00 |\nDuring the sync, records are read from the API according to these time windows and the `cursor_field` indicates where the datetime value is stored on a record. This cursor is progressed as these partitions of records are successfully transmitted to the destination.\nUpon a successful sync, the final stream state will be the datetime of the last record emitted. On the subsequent sync, the connector will fetch records whose cursor value begins on that datetime and onward.\nSchema:\n`yaml\n  DatetimeBasedCursor:\n    description: Cursor to provide incremental capabilities over datetime\n    type: object\n    required:\n      - type\n      - cursor_field\n      - end_datetime\n      - datetime_format\n      - cursor_granularity\n      - start_datetime\n      - step\n    properties:\n      type:\n        type: string\n        enum: [DatetimeBasedCursor]\n      cursor_field:\n        description: The location of the value on a record that will be used as a bookmark during sync\n        type: string\n      datetime_format:\n        description: The format of the datetime\n        type: string\n      cursor_granularity:\n        description: Smallest increment the datetime_format has (ISO 8601 duration) that is used to ensure the start of a slice does not overlap with the end of the previous one\n        type: string\n      end_datetime:\n        description: The datetime that determines the last record that should be synced\n        anyOf:\n          - type: string\n          - \"$ref\": \"#/definitions/MinMaxDatetime\"\n      start_datetime:\n        description: The datetime that determines the earliest record that should be synced\n        anyOf:\n          - type: string\n          - \"$ref\": \"#/definitions/MinMaxDatetime\"\n      step:\n        description: The size of the time window (ISO8601 duration)\n        type: string\n      end_time_option:\n        description: Request option for end time\n        \"$ref\": \"#/definitions/RequestOption\"\n      lookback_window:\n        description: How many days before start_datetime to read data for (ISO8601 duration)\n        type: string\n      start_time_option:\n        description: Request option for start time\n        \"$ref\": \"#/definitions/RequestOption\"\n      partition_field_end:\n        description: Partition start time field\n        type: string\n      partition_field_start:\n        description: Partition end time field\n        type: string\n      $parameters:\n        type: object\n        additionalProperties: true\n  MinMaxDatetime:\n    description: Compares the provided date against optional minimum or maximum times. The max_datetime serves as the ceiling and will be returned when datetime exceeds it. The min_datetime serves as the floor\n    type: object\n    required:\n      - type\n      - datetime\n    properties:\n      type:\n        type: string\n        enum: [MinMaxDatetime]\n      datetime:\n        type: string\n      datetime_format:\n        type: string\n        default: \"\"\n      max_datetime:\n        type: string\n      min_datetime:\n        type: string\n      $parameters:\n        type: object\n        additionalProperties: true`\nExample:\n`yaml\nincremental_sync:\n  type: DatetimeBasedCursor\n  start_datetime: \"2022-01-01T00:00:00.000000+0000\"\n  end_datetime: \"2022-01-05T00:00:00.000000+0000\"\n  datetime_format: \"%Y-%m-%dT%H:%M:%S.%f%z\"\n  cursor_granularity: \"PT0.000001S\"\n  step: \"P1D\"`\nwill result in the datetime partition windows in the example mentioned earlier.\nLookback Windows\nThe `DatetimeBasedCursor` also supports an optional lookback window, specifying how many days before the start_datetime to read data for.\n`yaml\nincremental_sync:\n  type: DatetimeBasedCursor\n  start_datetime: \"2022-02-01T00:00:00.000000+0000\"\n  end_datetime: \"2022-03-01T00:00:00.000000+0000\"\n  datetime_format: \"%Y-%m-%dT%H:%M:%S.%f%z\"\n  cursor_granularity: \"PT0.000001S\"\n  lookback_window: \"P31D\"\n  step: \"P1D\"`\nwill read data from `2022-01-01` to `2022-03-01`.\nThe stream partitions will be of the form `{\"start_date\": \"2021-02-01T00:00:00.000000+0000\", \"end_date\": \"2021-02-02T23:59:59.999999+0000\"}`\nThe stream partitions' field names can be customized through the `partition_field_start` and `partition_field_end` parameters.\nThe `datetime_format` can be used to specify the format of the start and end time. It is RFC3339 by default.\nThe Stream's state will be derived by reading the record's `cursor_field`.\nIf the `cursor_field` is `updated_at`, and the record is `{\"id\": 1234, \"created\": \"2021-02-02T00:00:00.000000+0000\"}`, then the state after reading that record is `\"updated_at\": \"2021-02-02T00:00:00.000000+0000\"`. [^1]\nNote that all durations are expressed as ISO 8601 durations.\nFiltering according to Cursor Field\nIf an API supports filtering data based on the cursor field, the `start_time_option` and `end_time_option` parameters can be used to configure this filtering.\nFor instance, if the API supports filtering using the request parameters `created[gte]` and `created[lte]`, then the component can specify the request parameters as\n`yaml\nincremental_sync:\n  type: DatetimeCursorBased\n  <...>\n  start_time_option:\n    type: RequestOption\n    field_name: \"created[gte]\"\n    inject_into: \"request_parameter\"\n  end_time_option:\n    type: RequestOption\n    field_name: \"created[lte]\"\n    inject_into: \"request_parameter\"`\nMore readings",
    "tag": "airbyte"
  },
  {
    "title": "Full Refresh Streams",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/cdk-python/full-refresh-stream.md",
    "content": "Full Refresh Streams\nAs mentioned in the Basic Concepts Overview, a `Stream` is the atomic unit for reading data from a Source. A stream can read data from anywhere: a relational database, an API, or even scrape a web page! (although that might be stretching the limits of what a connector should do).\nTo implement a stream, there are two minimum requirements: 1. Define the stream's schema 2. Implement the logic for reading records from the underlying data source\nDefining the stream's schema\nYour connector must describe the schema of each stream it can output using JSONSchema.\nThe simplest way to do this is to describe the schema of your streams using one `.json` file per stream. You can also dynamically generate the schema of your stream in code, or you can combine both approaches: start with a `.json` file and dynamically add properties to it.\nThe schema of a stream is the return value of `Stream.get_json_schema`.\nStatic schemas\nBy default, `Stream.get_json_schema` reads a `.json` file in the `schemas/` directory whose name is equal to the value of the `Stream.name` property. In turn `Stream.name` by default returns the name of the class in snake case. Therefore, if you have a class `class EmployeeBenefits(HttpStream)` the default behavior will look for a file called `schemas/employee_benefits.json`. You can override any of these behaviors as you need.\nImportant note: any objects referenced via `$ref` should be placed in the `shared/` directory in their own `.json` files.\nDynamic schemas\nIf you'd rather define your schema in code, override `Stream.get_json_schema` in your stream class to return a `dict` describing the schema using JSONSchema.\nDynamically modifying static schemas\nPlace a `.json` file in the `schemas` folder containing the basic schema like described in the static schemas section. Then, override `Stream.get_json_schema` to run the default behavior, edit the returned value, then return the edited value:\n`text\ndef get_json_schema(self):\n    schema = super().get_json_schema()\n    schema['dynamically_determined_property'] = \"property\"\n    return schema`\nReading records from the data source\nIf custom functionality is required for reading a stream, you may need to override `Stream.read_records`. Given some information about how the stream should be read, this method should output an iterable object containing records from the data source. We recommend using generators as they are very efficient with regards to memory requirements.\nIncremental Streams\nWe highly recommend implementing Incremental when feasible. See the incremental streams page for more information.",
    "tag": "airbyte"
  },
  {
    "title": "HTTP-API-based Connectors",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/cdk-python/http-streams.md",
    "content": "HTTP-API-based Connectors\nThe CDK offers base classes that greatly simplify writing HTTP API-based connectors. Some of the most useful features include helper functionality for:\n\nAuthentication (basic auth, Oauth2, or any custom auth method)\nPagination\nHandling rate limiting with static or dynamic backoff timing\nCaching\n\nAll these features have sane off-the-shelf defaults but are completely customizable depending on your use case. They can also be combined with other stream features described in the full refresh streams and incremental streams sections.\nOverview of HTTP Streams\nJust like any general HTTP request, the basic `HTTPStream` requires a url to perform the request, and instructions on how to parse the resulting response.\nThe full request path is broken up into two parts, the base url and the path. This makes it easy for developers to create a Source-specific base `HTTPStream` class, with the base url filled in, and individual streams for each available HTTP resource. The Stripe CDK implementation is a reification of this pattern.\nThe base url is set via the `url_base` property, while the path is set by implementing the abstract `path` function.\nThe `parse_response` function instructs the stream how to parse the API response. This returns an `Iterable`, whose elements are each later transformed into an `AirbyteRecordMessage`. API routes whose response contains a single record generally have a `parse_reponse` function that return a list of just that one response. Routes that return a list, usually have a `parse_response` function that return the received list with all elements. Pulling the data out from the response is sufficient, any deserialization is handled by the CDK.\nLastly, the `HTTPStream` must describe the schema of the records it outputs using JsonSchema. The simplest way to do this is by placing a `.json` file per stream in the `schemas` directory in the generated python module. The name of the `.json` file must match the lower snake case name of the corresponding Stream. Here are examples from the Stripe API.\nYou can also dynamically set your schema. See the schema docs for more information.\nThese four elements - the `url_base` property, the `path` function, the `parse_response` function and the schema file - are the bare minimum required to implement the `HTTPStream`, and can be seen in the same Stripe example.\nThis basic implementation gives us a Full-Refresh Airbyte Stream. We say Full-Refresh since the stream does not have state and will always indiscriminately read all data from the underlying API resource.\nAuthentication\nThe CDK supports Basic and OAuth2.0 authentication via the `TokenAuthenticator` and `Oauth2Authenticator` classes respectively. Both authentication strategies are identical in that they place the api token in the `Authorization` header. The `OAuth2Authenticator` goes an additional step further and has mechanisms to, given a refresh token, refresh the current access token. Note that the `OAuth2Authenticator` currently only supports refresh tokens and not the full OAuth2.0 loop.\nUsing either authenticator is as simple as passing the created authenticator into the relevant `HTTPStream` constructor. Here is an example from the Stripe API.\nPagination\nMost APIs, when facing a large call, tend to return the results in pages. The CDK accommodates paging via the `next_page_token` function. This function is meant to extract the next page \"token\" from the latest response. The contents of a \"token\" are completely up to the developer: it can be an ID, a page number, a partial URL etc.. The CDK will continue making requests as long as the `next_page_token` function. The CDK will continue making requests as long as the `next_page_token` continues returning non-`None` results. This can then be used in the `request_params` and other methods in `HttpStream` to page through API responses. Here is an example from the Stripe API.\nRate Limiting\nThe CDK, by default, will conduct exponential backoff on the HTTP code 429 and any 5XX exceptions, and fail after 5 tries.\nRetries are governed by the `should_retry` and the `backoff_time` methods. Override these methods to customise retry behavior. Here is an example from the Slack API.\nNote that Airbyte will always attempt to make as many requests as possible and only slow down if there are errors. It is not currently possible to specify a rate limit Airbyte should adhere to when making requests.\nStream Slicing\nWhen implementing stream slicing in an `HTTPStream` each Slice is equivalent to a HTTP request; the stream will make one request per element returned by the `stream_slices` function. The current slice being read is passed into every other method in `HttpStream` e.g: `request_params`, `request_headers`, `path`, etc.. to be injected into a request. This allows you to dynamically determine the output of the `request_params`, `path`, and other functions to read the input slice and return the appropriate value.\nNested Streams & Caching\nIt's possible to cache data from a stream onto a temporary file on disk. \nThis is especially useful when dealing with streams that depend on the results of another stream e.g: `/employees/{id}/details`. In this case, we can use caching to write the data of the parent stream to a file to use this data when the child stream synchronizes, rather than performing a full HTTP request again.\nThe caching mechanism works as follows: If the request is made for the first time, the returned value will be written to disk (all requests made by the `read_records` method will be written to the cache file). When the same request is made again, instead of making another HTTP request, the result will instead be read from disk. It is checked whether the required request is in the cache file, and if so, the data from this file is returned. However, if the check for the request's existence in the cache file fails, a new request will be made, and its result will be added to the cache file.\nCaching can be enabled by overriding the `use_cache` property of the `HttpStream` class to return `True`.\nThe caching mechanism is related to parent streams. For child streams, there is an `HttpSubStream` class inheriting from `HttpStream` and overriding the `stream_slices` method that returns a generator of all parent entries.\nTo use caching in the parent/child relationship, perform the following steps:\n1. Turn on parent stream caching by overriding the `use_cache` property.\n2. Inherit child stream class from `HttpSubStream` class.\nExample\n```python\nclass Employees(HttpStream):\n    ...\n\n\n```@property\ndef use_cache(self) -> bool:\n    return True\n```\n\n\nclass EmployeeDetails(HttpSubStream):\n    ...",
    "tag": "airbyte"
  },
  {
    "title": "Defining Stream Schemas",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/cdk-python/schemas.md",
    "content": "Defining Stream Schemas\nYour connector must describe the schema of each stream it can output using JSONSchema.\nThe simplest way to do this is to describe the schema of your streams using one `.json` file per stream. You can also dynamically generate the schema of your stream in code, or you can combine both approaches: start with a `.json` file and dynamically add properties to it.\nThe schema of a stream is the return value of `Stream.get_json_schema`.\nStatic schemas\nBy default, `Stream.get_json_schema` reads a `.json` file in the `schemas/` directory whose name is equal to the value of the `Stream.name` property. In turn `Stream.name` by default returns the name of the class in snake case. Therefore, if you have a class `class EmployeeBenefits(HttpStream)` the default behavior will look for a file called `schemas/employee_benefits.json`. You can override any of these behaviors as you need.\nImportant note: any objects referenced via `$ref` should be placed in the `shared/` directory in their own `.json` files.\nGenerating schemas from OpenAPI definitions\nIf you are implementing a connector to pull data from an API which publishes an OpenAPI/Swagger spec, you can use a tool we've provided for generating JSON schemas from the OpenAPI definition file. Detailed information can be found here.\nGenerating schemas using the output of your connector's read command\nWe also provide a tool for generating schemas using a connector's `read` command output. Detailed information can be found here.\nBackwards Compatibility\nBecause statically defined schemas explicitly define how data is represented in a destination, updates to a schema must be backwards compatible with prior versions. More information about breaking changes can be found here\nDynamic schemas\nIf you'd rather define your schema in code, override `Stream.get_json_schema` in your stream class to return a `dict` describing the schema using JSONSchema.\nDynamically modifying static schemas\nOverride `Stream.get_json_schema` to run the default behavior, edit the returned value, then return the edited value:\n`text\ndef get_json_schema(self):\n    schema = super().get_json_schema()\n    schema['dynamically_determined_property'] = \"property\"\n    return schema`\nType transformation\nIt is important to ensure output data conforms to the declared json schema. This is because the destination receiving this data to load into tables may strictly enforce schema (e.g. when data is stored in a SQL database, you can't put CHAR type into INTEGER column). In the case of changes to API output (which is almost guaranteed to happen over time) or a minor mistake in jsonschema definition, data syncs could thus break because of mismatched datatype schemas.\nTo remain robust in operation, the CDK provides a transformation ability to perform automatic object mutation to align with desired schema before outputting to the destination. All streams inherited from airbyte_cdk.sources.streams.core.Stream class have this transform configuration available. It is _disabled_ by default and can be configured per stream within a source connector.\nDefault type transformation\nHere's how you can configure the TypeTransformer:\n```python\nfrom airbyte_cdk.sources.utils.transform import TransformConfig, Transformer\nfrom airbyte_cdk.sources.streams.core import Stream\nclass MyStream(Stream):\n    ...\n    transformer = Transformer(TransformConfig.DefaultSchemaNormalization)\n    ...\n```\nIn this case default transformation will be applied. For example if you have schema like this\n`javascript\n{\"type\": \"object\", \"properties\": {\"value\": {\"type\": \"string\"}}}`\nand source API returned object with non-string type, it would be casted to string automaticaly:\n`javascript\n{\"value\": 12} -> {\"value\": \"12\"}`\nAlso it works on complex types:\n`javascript\n{\"value\": {\"unexpected_object\": \"value\"}} -> {\"value\": \"{'unexpected_object': 'value'}\"}`\nAnd objects inside array of referenced by $ref attribute.\nIf the value cannot be cast (e.g. string \"asdf\" cannot be casted to integer), the field would retain its original value. Schema type transformation support any jsonschema types, nested objects/arrays and reference types. Types described as array of more than one type (except \"null\"), types under oneOf/anyOf keyword wont be transformed.\nNote: This transformation is done by the source, not the stream itself. I.e. if you have overriden \"read_records\" method in your stream it wont affect object transformation. All transformation are done in-place by modifing output object before passing it to \"get_updated_state\" method, so \"get_updated_state\" would receive the transformed object.\nCustom schema type transformation\nDefault schema type transformation performs simple type casting. Sometimes you want to perform more sophisticated transform like making \"date-time\" field compliant to rcf3339 standard. In this case you can use custom schema type transformation:\n```python\nclass MyStream(Stream):\n    ...\n    transformer = Transformer(TransformConfig.CustomSchemaNormalization)\n    ...\n\n\n```@transformer.registerCustomTransform\ndef transform_function(original_value: Any, field_schema: Dict[str, Any]) -> Any:\n    # transformed_value = ...\n    return transformed_value\n```\n\n\n```\nWhere original_value is initial field value and field_schema is part of jsonschema describing field type. For schema\n`javascript\n{\"type\": \"object\", \"properties\": {\"value\": {\"type\": \"string\", \"format\": \"date-time\"}}}`\nfield_schema variable would be equal to\n`javascript\n{\"type\": \"string\", \"format\": \"date-time\"}`\nIn this case default transformation would be skipped and only custom transformation apply. If you want to run both default and custom transformation you can configure transdormer object by combining config flags:\n`python\ntransformer = Transformer(TransformConfig.DefaultSchemaNormalization | TransformConfig.CustomSchemaNormalization)`\nIn this case custom transformation will be applied after default type transformation function. Note that order of flags doesn't matter, default transformation will always be run before custom.\nIn some specific cases, you might want to make your custom transform not static, e.g. Formatting a field according to the connector configuration.\nTo do so, we suggest you to declare a function to generate another, a.k.a a closure:\n```python\nclass MyStream(Stream):\n    ...\n    transformer = TypeTransformer(TransformConfig.CustomSchemaNormalization)\n    ...\n    def init(self, config_based_date_format):\n        self.config_based_date_format = config_based_date_format\n        transform_function = self.get_custom_transform()\n        self.transformer.registerCustomTransform(transform_function)\n\n\n```def get_custom_transform(self):\n    def custom_transform_function(original_value, field_schema):\n        if original_value and \"format\" in field_schema and field_schema[\"format\"] == \"date\":\n            transformed_value = pendulum.from_format(original_value, self.config_based_date_format).to_date_string()\n            return transformed_value\n        return original_value\n    return custom_transform_function\n```\n\n\n```\nPerformance consideration\nTransforming each object on the fly would add some time for each object processing. This time is depends on object/schema complexity and hardware configuration.\nThere are some performance benchmarks we've done with ads_insights facebook schema (it is complex schema with objects nested inside arrays ob object and a lot of references) and example object. Here is the average transform time per single object, seconds:\n```text\nregular transform:\n0.0008423403530008121\ntransform without type casting (but value still being write to dict/array):\n0.000776215762666349\ntransform without actual value setting  (but iterating through object properties):\n0.0006788729513330812\njust traverse/validate through json schema and object fields:\n0.0006139181846665452\n```\nOn my PC (AMD Ryzen 7 5800X) it took 0.8 milliseconds per object. As you can see most time (~ 75%) is taken by jsonschema traverse/validation routine and very little (less than 10 %) by actual converting. Processing time can be reduced by skipping jsonschema type checking but it would be no warnings about possible object jsonschema inconsistency.",
    "tag": "airbyte"
  },
  {
    "title": "Python Concepts",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/cdk-python/python-concepts.md",
    "content": "Python Concepts\nThe Airbyte CDK makes use of various not-so-obvious Python concepts. You might want to revisit these concepts as you implement your connector:\nAbstract Classes ABCs (AbstractBaseClasses) and abstractmethods\nYou'll want a strong understanding of these as the central API concepts require extending and using them.\nKeyword Arguments.\nYou'll often see this referred to as `**kwargs` in the code.\nProperties\nNote that there are two ways of defining properties: statically and dynamically.\nStatically\n```text\nclass Employee(ABC):\n    @property\n    @abstractmethod\n    def job_title():\n        \"\"\" returns this employee's job title\"\"\"\nclass Pilot(Employee):\n    job_title = \"pilot\"\n```\nNotice how statically defining properties in this manner looks the same as defining variables. You can then reference this property as follows:\n`text\npilot = Pilot()\nprint(pilot.job_title) # pilot`\nDynamically\nYou can also run arbitrary code to get the value of a property. For example:\n```text\nclass Employee(ABC):\n    @property\n    @abstractmethod\n    def job_title():\n        \"\"\" returns this employee's job title\"\"\"\nclass Pilot(Employee):\n    def job_title():\n        # You can run any arbitrary code and return its result\n        return \"pilot\"\n```\nGenerators\nGenerators are basically iterators over arbitrary source data. They are handy because their syntax is extremely concise and feel just like any other list or collection when working with them in code.\nIf you see `yield` anywhere in the code -- that's a generator at work.",
    "tag": "airbyte"
  },
  {
    "title": "Connector Development Kit (Python)",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/cdk-python",
    "content": "Connector Development Kit (Python)\nThe Airbyte Python CDK is a framework for rapidly developing production-grade Airbyte connectors. The CDK currently offers helpers specific for creating Airbyte source connectors for:\n\nHTTP APIs (REST APIs, GraphQL, etc..)\nGeneric Python sources (anything not covered by the above)\nSinger Taps (Note: The CDK supports building Singer taps but Airbyte no longer access contributions of this type)\n\nThe CDK provides an improved developer experience by providing basic implementation structure and abstracting away low-level glue boilerplate.\nThis document is a general introduction to the CDK. Readers should have basic familiarity with the Airbyte Specification before proceeding.\nIf you have any issues with troubleshooting or want to learn more about the CDK from the Airbyte team, head to the Connector Development section of our Discourse forum to inquire further!\nGetting Started\nGenerate an empty connector using the code generator. First clone the Airbyte repository then from the repository root run\n`text\ncd airbyte-integrations/connector-templates/generator\n./generate.sh`\nthen follow the interactive prompt. Next, find all `TODO`s in the generated project directory -- they're accompanied by lots of comments explaining what you'll need to do in order to implement your connector. Upon completing all TODOs properly, you should have a functioning connector.\nAdditionally, you can follow this tutorial for a complete walkthrough of creating an HTTP connector using the Airbyte CDK.\nConcepts & Documentation\nBasic Concepts\nIf you want to learn more about the classes required to implement an Airbyte Source, head to our basic concepts doc.\nFull Refresh Streams\nIf you have questions or are running into issues creating your first full refresh stream, head over to our full refresh stream doc. If you have questions about implementing a `path` or `parse_response` function, this doc is for you.\nIncremental Streams\nHaving trouble figuring out how to write a `stream_slices` function or aren't sure what a `cursor_field` is? Head to our incremental stream doc.\nPractical Tips\nAirbyte recommends using the CDK template generator to develop with the CDK. The template generates created all the required scaffolding, with convenient TODOs, allowing developers to truly focus on implementing the API.\nFor tips on useful Python knowledge, see the Python Concepts page.\nYou can find a complete tutorial for implementing an HTTP source connector in this tutorial\nExample Connectors\nHTTP Connectors:\n\nExchangerates API\nStripe\nSlack\n\nSimple Python connectors using the barebones `Source` abstraction:\n\nGoogle Sheets\nMailchimp\n\nContributing\nFirst time setup\nWe assume `python` points to python >=3.9.\nSetup a virtual env:\n`text\npython -m venv .venv\nsource .venv/bin/activate\npip install -e \".[tests]\" # [tests] installs test-only dependencies`\nIteration\n\nIterate on the code locally\nRun tests via `pytest -s unit_tests`\nPerform static type checks using `mypy airbyte_cdk`. `MyPy` configuration is in `.mypy.ini`.\nThe `type_check_and_test.sh` script bundles both type checking and testing in one convenient command. Feel free to use it!\n\nDebugging\nWhile developing your connector, you can print detailed debug information during a sync by specifying the `--debug` flag. This allows you to get a better picture of what is happening during each step of your sync.\n`text\npython main.py read --config secrets/config.json --catalog sample_files/configured_catalog.json --debug`\nIn addition to preset CDK debug statements, you can also add your own statements to emit debug information specific to your connector:\n`python\nself.logger.debug(\"your debug message here\", extra={\"debug_field\": self.value})`\nTesting\nAll tests are located in the `unit_tests` directory. Run `pytest --cov=airbyte_cdk unit_tests/` to run them. This also presents a test coverage report.\nPublishing a new version to PyPi\n\nOpen a PR\nOnce it is approved and merge, an Airbyte member must run the `Publish CDK Manually` workflow using `release-type=major|manor|patch` and setting the changelog message.\n\nComing Soon\n\nFull OAuth 2.0 support (including refresh token issuing flow via UI or CLI) \nAirbyte Java HTTP CDK\nCDK for Async HTTP endpoints (request-poll-wait style endpoints)\nCDK for other protocols\nDon't see a feature you need? Create an issue and let us know how we can help!\n",
    "tag": "airbyte"
  },
  {
    "title": "Stream Slices",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/cdk-python/stream-slices.md",
    "content": "Stream Slices\nStream Slicing\nA Stream Slice is a subset of the records in a stream.\nWhen a stream is being read incrementally, Slices can be used to control when state is saved.\nWhen slicing is enabled, a state message will be output by the connector after reading every slice. Slicing is completely optional and is provided as a way for connectors to checkpoint state in a more granular way than basic interval-based state checkpointing. Slicing is typically used when reading a large amount of data or when the underlying data source imposes strict rate limits that make it difficult to re-read the same data over and over again. This being said, interval-based checkpointing is compatible with slicing with one difference: intervals are counted within a slice rather than across all records. In other words, the counter used to determine if the interval has been reached (e.g: every 10k records) resets at the beginning of every slice.\nThe relationship between records in a slice is up to the developer, but the list of slices must be yielded in ascending order, using the cursor field as context for the ordering. This is to ensure that the state can't be updated to a timestamp that is ahead of other slices yet to be processed. Slices are typically used to implement date-based checkpointing, for example to group records generated within a particular hour, day, or month etc.\nSlices can be hard-coded or generated dynamically (e.g: by making a query).\nAn important restriction imposed on slices is that they must be described with a list of `dict`s returned from the `Stream.stream_slices()` method, where each `dict` describes a slice. The `dict`s may have any schema, and are passed as input to each stream's `read_stream` method. This way, the connector can read the current slice description (the input `dict`) and use that to make queries as needed. As described above, this list of dicts must be in appropriate ascending order based on the cursor field.\nUse cases\nIf your use case requires saving state based on an interval e.g: only 10,000 records but nothing more sophisticated, then slicing is not necessary and you can instead set the `state_checkpoint_interval` property on a stream.\nThe Slack connector: time-based slicing for large datasets\nSlack is a chat platform for businesses. Collectively, a company can easily post tens or hundreds of thousands of messages in a single Slack instance per day. So when writing a connector to pull chats from Slack, it's easy to run into rate limits or for the sync to take a very long time to complete because of the large amount of data. So we want a way to frequently \"save\" which data we already read from the connector so that if there is a halfway failure, we pick up reading where we left off. In addition, the Slack API does not return messages sorted by timestamp, so we cannot use `state_checkpoint_interval`s.\nThis is a great usecase for stream slicing. The `messages` stream, which outputs one record per chat message, can slice records by time e.g: hourly. It implements this by specifying the beginning and end timestamp of each hour that it wants to pull data from. Then after all the records in a given hour (i.e: slice) have been read, the connector outputs a STATE message to indicate that state should be saved. This way, if the connector ever fails during a sync (for example if the API goes down) then at most, it will reread only one hour's worth of messages.\nSee the implementation of the Slack connector here.",
    "tag": "airbyte"
  },
  {
    "title": "Basic Concepts",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/cdk-python/basic-concepts.md",
    "content": "Basic Concepts\nThe Airbyte Specification\nAs a quick recap, the Airbyte Specification requires an Airbyte Source to support 4 distinct operations:\n\n`Spec` - The required configuration in order to interact with the underlying technical system e.g. database\n\ninformation, authentication information etc.\n\n`Check` - Validate that the provided configuration is valid with sufficient permissions for one to perform all\n\nrequired operations on the Source.\n\n`Discover` - Discover the Source's schema. This let users select what a subset of the data to sync. Useful\n\nif users require only a subset of the data.\n\n`Read` - Perform the actual syncing process. Data is read from the Source, parsed into `AirbyteRecordMessage`s\n\nand sent to the Airbyte Destination. Depending on how the Source is implemented, this sync can be incremental\nor a full-refresh.\nA core concept discussed here is the Source.\nThe Source contains one or more Streams (or Airbyte Streams). A Stream is the other concept key to understanding how Airbyte models the data syncing process. A Stream models the logical data groups that make up the larger Source. If the Source is a RDMS, each Stream is a table. In a REST API setting, each Stream corresponds to one resource within the API. e.g. a Stripe Source would have have one Stream for `Transactions`, one for `Charges` and so on.\nThe `Source` class\nAirbyte provides abstract base classes which make it much easier to perform certain categories of tasks e.g: `HttpStream` makes it easy to create HTTP API-based streams. However, if those do not satisfy your use case (for example, if you're pulling data from a relational database), you can always directly implement the Airbyte Protocol by subclassing the CDK's `Source` class.\nThe `Source` class implements the `Spec` operation by looking for a file named `spec.yaml` (or `spec.json`) in the module's root by default. This is expected to be a json schema file that specifies the required configuration. Here is an example from the Exchange Rates source.\nNote that while this is the most flexible way to implement a source connector, it is also the most toilsome as you will be required to manually manage state, input validation, correctly conforming to the Airbyte Protocol message formats, and more. We recommend using a subclass of `Source` unless you cannot fulfill your use case otherwise.\nThe `AbstractSource` Object\n`AbstractSource` is a more opinionated implementation of `Source`. It implements `Source`'s 4 methods as follows:\n`Check` delegates to the `AbstractSource`'s `check_connection` function. The function's `config` parameter contains the user-provided configuration, specified in the `spec.yaml` returned by `Spec`. `check_connection` uses this configuration to validate access and permissioning. Here is an example from the same Exchange Rates API.\nThe `Stream` Abstract Base Class\nAn `AbstractSource` also owns a set of `Stream`s. This is populated via the `AbstractSource`'s `streams` function. `Discover` and `Read` rely on this populated set.\n`Discover` returns an `AirbyteCatalog` representing all the distinct resources the underlying API supports. Here is the entrypoint for those interested in reading the code. See schemas for more information on how to declare the schema of a stream.\n`Read` creates an in-memory stream reading from each of the `AbstractSource`'s streams. Here is the entrypoint for those interested.\nAs the code examples show, the `AbstractSource` delegates to the set of `Stream`s it owns to fulfill both `Discover` and `Read`. Thus, implementing `AbstractSource`'s `streams` function is required when using the CDK.\nA summary of what we've covered so far on how to use the Airbyte CDK:\n\nA concrete implementation of the `AbstractSource` object is required.\nThis involves,\nimplementing the `check_connection`function.\nCreating the appropriate `Stream` classes and returning them in the `streams` function.\nplacing the above mentioned `spec.yaml` file in the right place.\n\nHTTP Streams\nWe've covered how the `AbstractSource` works with the `Stream` interface in order to fulfill the Airbyte Specification. Although developers are welcome to implement their own object, the CDK saves developers the hassle of doing so in the case of HTTP APIs with the HTTPStream object.",
    "tag": "airbyte"
  },
  {
    "title": "Incremental Streams",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/connector-development/cdk-python/incremental-stream.md",
    "content": "Incremental Streams\nAn incremental Stream is a stream which reads data incrementally. That is, it only reads data that was generated or updated since the last time it ran, and is thus far more efficient than a stream which reads all the source data every time it runs. If possible, developers are encouraged to implement incremental streams to reduce sync times and resource usage.\nSeveral new pieces are essential to understand how incrementality works with the CDK:\n\n`AirbyteStateMessage`\ncursor fields\n`IncrementalMixin`\n`Stream.get_updated_state` (deprecated)\n\nas well as a few other optional concepts.\n`AirbyteStateMessage`\nThe `AirbyteStateMessage` persists state between syncs, and allows a new sync to pick up from where the previous sync last finished. See the incremental sync guide for more information.\nCursor fields\nThe `cursor_field` refers to the field in the stream's output records used to determine the \"recency\" or ordering of records. An example is a `created_at` or `updated_at` field in an API or DB table.\nCursor fields can be input by the user (e.g: a user can choose to use an auto-incrementing `id` column in a DB table) or they can be defined by the source e.g: where an API defines that `updated_at` is what determines the ordering of records.\nIn the context of the CDK, setting the `Stream.cursor_field` property to any truthy value informs the framework that this stream is incremental.\n`IncrementalMixin`\nThis class mixin adds property `state` with abstract setter and getter.\nThe `state` attribute helps the CDK figure out the current state of sync at any moment (in contrast to deprecated `Stream.get_updated_state` method).\nThe setter typically deserialize state saved by CDK and initialize internal state of the stream.\nThe getter should serialize internal state of the stream. \n```python\n@property\ndef state(self) -> Mapping[str, Any]:\n   return {self.cursor_field: str(self._cursor_value)}\n@state.setter\ndef state(self, value: Mapping[str, Any]):\n   self._cursor_value = value[self.cursor_field]\n```\nThe actual logic of updating state during reading is implemented somewhere else, usually as part of `read_records` method, right after the latest record returned that matches the new state.\nTherefore, the state represents the latest checkpoint successfully achieved, and all next records should match the next state after that one.\n`python\ndef read_records(self, ...):\n   ...\n   yield record\n   yield record\n   yield record\n   self._cursor_value = max(record[self.cursor_field], self._cursor_value)\n   yield record\n   yield record\n   yield record\n   self._cursor_value = max(record[self.cursor_field], self._cursor_value)`\n`Stream.get_updated_state`\n(deprecated since 1.48.0, see `IncrementalMixin`)\nThis function helps the stream keep track of the latest state by inspecting every record output by the stream (as returned by the `Stream.read_records` method) and comparing it against the most recent state object. This allows sync to resume from where the previous sync last stopped, regardless of success or failure. This function typically compares the state object's and the latest record's cursor field, picking the latest one.\nCheckpointing state\nThere are two ways to checkpointing state (i.e: controlling the timing of when state is saved) while reading data from a connector:\n\nInterval-based checkpointing\nStream Slices\n\nInterval based checkpointing\nThis is the simplest method for checkpointing. When the interval is set to a truthy value e.g: 100, then state is persisted after every 100 records output by the connector e.g: state is saved after reading 100 records, then 200, 300, etc..\nWhile this is very simple, it requires that records are output in ascending order with regards to the cursor field. For example, if your stream outputs records in ascending order of the `updated_at` field, then this is a good fit for your usecase. But if the stream outputs records in a random order, then you cannot use this method because we can only be certain that we read records after a particular `updated_at` timestamp once all records have been fully read.\nInterval based checkpointing can be implemented by setting the `Stream.state_checkpoint_interval` property e.g:\n`text\nclass MyAmazingStream(Stream): \n  # Save the state every 100 records\n  state_checkpoint_interval = 100`\n`Stream.stream_slices`\nStream slices can be used to achieve finer grain control of when state is checkpointed.\nConceptually, a Stream Slice is a subset of the records in a stream which represent the smallest unit of data which can be re-synced. Once a full slice is read, an `AirbyteStateMessage` will be output, causing state to be saved. If a connector fails while reading the Nth slice of a stream, then the next time it retries, it will begin reading at the beginning of the Nth slice again, rather than re-read slices `1...N-1`.\nA Slice object is not typed, and the developer is free to include any information necessary to make the request. This function is called when the `Stream` is about to be read. Typically, the `stream_slices` function, via inspecting the state object, generates a Slice for every request to be made.\nAs an example, suppose an API is able to dispense data hourly. If the last sync was exactly 24 hours ago, we can either make an API call retrieving all data at once, or make 24 calls each retrieving an hour's worth of data. In the latter case, the `stream_slices` function, sees that the previous state contains yesterday's timestamp, and returns a list of 24 Slices, each with a different hourly timestamp to be used when creating request. If the stream fails halfway through (at the 12th slice), then the next time it starts reading, it will read from the beginning of the 12th slice.\nFor a more in-depth description of stream slicing, see the Stream Slices guide.\nConclusion\nIn summary, an incremental stream requires:\n\nthe `cursor_field` property\nto be inherited from `IncrementalMixin` and state methods implemented\nOptionally, the `stream_slices` function\n",
    "tag": "airbyte"
  },
  {
    "title": "December 2022",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/release_notes/december_2022.md",
    "content": "December 2022\nAirbyte v0.40.24 to v0.40.26\nThis page includes new features and improvements to the Airbyte Cloud and Airbyte Open Source platforms. \nNew features\n\nAdded throughput metrics and a progress bar to the Connection Sync History UI for Airbyte Open Source. These provide real-time information on data transfer rates and sync progress. #19193\nAdded the custom connector UI in alpha to Airbyte Cloud, which allows you to create and update custom connectors. #20483\nAdded the stream details panel to the Connection Replication UI, which allows you to display and configure streams in your connection. #19219\nAdded source-defined Cursor and Primary key fields to the stream details panel. #20366 \n\n\nAdded the UX flow for auto-detect schema changes. #19226\nAdded the auto-detect schema changes option to the Connection Replication UI, which allows you to choose whether Airbyte ignores or disables the connection when it detects a non-breaking schema change in the source. #19734\nAdded stream table configuration windows for Destination namespace and Stream name, which allow you to choose how the data is stored and edit the names and prefixes of tables in the destination. #19713\nAdded the AWS Secret Manager to Airbyte Open Source as an option for storing secrets. #19690\nAdded the Airbyte Cloud API in alpha, which allows you to programmatically control Airbyte Cloud through an API.\n\nImprovements\n\nImproved the Connection UX by preventing users from modifying an existing connection if there is a breaking change in the source schema. Now users must review changes before modifying the connection. #20276\nImproved the stream catalog index by defining `stream`. This precaution keeps all streams matching correctly and data organized consistently. #20443\nUpdated the API to support column selection configuration in Airbyte Cloud. #20259\nOngoing improvements to Low-code CDK in alpha:\nAdded `SessionTokenAuthenticator` for authentication management. #19716\nAdded the first iteration of the Configuration UI, which allows you to build connectors using forms instead of writing a YAML file. #20008\nAdded request options component to streams. You can now choose request options for streams in the connector builder. #20497\nFixed an issue where errors were not indicated properly by omitting individually touched fields in `useBuilderErrors`. #20463\nUpdated UI to match the current design, including UI text changes and the addition of the stream delete button. #20464\nUpgraded Orval and updated the connector builder OpenAPI to pull the connector manifest schema directly into the API. #20166\n\n\n\nBugs",
    "tag": "airbyte"
  },
  {
    "title": "August 2022",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/release_notes/august_2022.md",
    "content": "August 2022\nAirbyte v0.39.42-alpha to v0.40.3\nThis page includes new features and improvements to the Airbyte Cloud and Airbyte Open Source platforms.\nNew features\n\n\nAdded reserved keywords for schema names by fixing the quotation logic in normalization. #14683\n\n\nAdded documentation about the data displayed in sync log summaries. #15181\n\n\nAdded OAuth login to Airbyte Cloud, which allows you to sign in using your Google login credentials. #15414\n\n\nYou can use your Google login credentials to sign in to your Airbyte account if they share the same email address.  \n\n\nYou can create a new Airbyte account with OAuth using your Google login credentials. \n\n\nYou cannot use OAuth to log in if you are invited to join a workspace.\n\n\n\n\nImprovements\n\n\nImproved the Airbyte version naming conventions by removing the `-alpha` tag. The Airbyte platform is used successfully by thousands of users, so the `-alpha` tag is no longer necessary. #15766\n\n\nImproved the `loadBalancerIP` in the web app by making it configurable. #14992\n\n\nDatadog:\n\n\nImproved the Airbyte platform by supporting StatsD, which sends Temporal metrics to Datadog. #14842\n\n\nAdded Datadog tags to help you identify metrics between Airbyte instances. #15213 \n\n\nAdded metric client tracking to record schema validation errors. #13393\n\n\n\n\nBugs\n\n\nFixed an issue where data types did not display correctly in the UI. The correct data types are now displayed in the streams of your connections. #15558\n\n\nFixed an issue where requests would fail during a release by adding a shutdown hook to the Airbyte server. This ensures the requests will be gracefully terminated before they can fail. #15934\n\n\nHelm charts:\n\n\nFixed the deployment problems of the Helm chart with FluxCD by removing unconditional resource assignment in the chart for Temporal. #15374\n\n\nFixed the following issues in #15199:\n\n\nFixed an issue where `toyaml` was being used instead of `toYaml`, which caused Helm chart installation to fail.\n\n\nFixed incorrect `extraContainers` indentation, which caused Helm chart installation to fail if the value was supplied.\n\n\nFixed incorrect Postgres secret reference and made it more user friendly.\n\n\n\n\n\n",
    "tag": "airbyte"
  },
  {
    "title": "January 2023",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/release_notes/january_2023.md",
    "content": "January 2023\nAirbyte v0.40.27 to v0.40.32\nThis page includes new features and improvements to the Airbyte Cloud and Airbyte Open Source platforms. \nNew features\n\nAdded the Free Connector Program to Airbyte Cloud, allowing you to sync connections with alpha or beta connectors for free.\n\nImprovements\n\nImproved Airbyte Open Source by integrating Docker Compose V2. You must have Docker Compose V2 installed before upgrading to Airbyte version 0.42.0 or later. #19321\nImproved the Airbyte Cloud UI by displaying the Credits label in the sidebar and low-credit alerts on the Credits page. #20595\nImproved the Airbyte CI workflow by adding support to pull requests and limiting the CI runs to only occur on pushes to the master branch. This enhances collaboration with external contributors and reduces unnecessary runs. #21266\nImproved the connector form by using proper validation in the array section. #20725\nOngoing improvements to the Connector Builder UI in alpha:\nAdded support for substream slicers and cartesian slicers, allowing the Connector Builder to create substreams and new streams from multiple existing streams. #20861\nAdded support for in-schema specification and validation, including a manual schema option. #20862\nAdded user inputs, request options, authentication, pagination, and slicing to the Connector Builder UI. #20809\nAdded ability to convert from YAML manifest to UI form values. #21142\nImproved the Connector Builder\u2019s conversion of YAML manifest to UI form values by resolving references and options in the manifest. The Connector Builder Server API has been updated with a new endpoint for resolving the manifest, which is now utilized by the conversion function. #21898\n\n\n\nBugs\n\nFixed an issue where the checkboxes in the stream table would collapse and updated icons to match the new design. #21108\n",
    "tag": "airbyte"
  },
  {
    "title": "November 2022",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/release_notes/november_2022.md",
    "content": "November 2022\nAirbyte v0.40.18 to v0.40.23\nThis page includes new features and improvements to the Airbyte Cloud and Airbyte Open Source platforms. \nNew features\n\nAdded multi-region Cloud architecture, which allows for better data protection and for Airbyte Cloud to launch in Europe.\nAdded the low-code connector builder UI to Airbyte Open Source. Run Airbyte v0.40.19 or higher and visit `localhost:8000/connector-builder` to start building low-code connectors.\nAdded a Helm chart for deploying `airbyte-cron`. New installations of Airbyte Open Source will now deploy `airbyte-cron` by default. To disable cron, use `--set cron.enabled=false` when running a `helm install`. #18542\nAdded a progress bar estimate to syncs in Airbyte Cloud. #19814\n\nImprovements\n\nImproved the Airbyte Protocol by introducing Airbyte Protocol v1 #19846, which defines a set of well-known data types. #17486\nThese replace existing JSON Schema primitive types. \nThey provide out-of-the-box validation and enforce specific formatting on some data types, like timestamps. \nNon-primitive types, like `object`, `array`, and `oneOf`, still use raw JSON Schema types.\nThese well-known types mostly correspond with the existing Airbyte data types, aside from a few differences: \n`BinaryData` is the only new type, which is used in places that previously produced a `Base64` string.\n`TimestampWithTimezone`, `TimestampWithoutTimezone`, `TimeWithTimezone`, and `TimeWithoutTimezone` have been in use for some time, so we made them official.\n\n\n\n\n",
    "tag": "airbyte"
  },
  {
    "title": "September 2022",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/release_notes/september_2022.md",
    "content": "September 2022\nAirbyte v0.40.4 to v0.40.6\nThis page includes new features and improvements to the Airbyte Cloud and Airbyte Open Source platforms. \nNew features\n\nAdded the low-code connector development kit (early access). This low-code framework is a declarative approach based on YAML with the goal of significantly reducing the time and complexity of building and maintaining connectors. #11582\nAdded a guide for using the low-code framework. #17534\n\n\nAdded support for large schema discovery. #17394\n\nImprovements\n\nImproved `airbyte-metrics` support in the Helm chart. #16166\nImproved the visibility button behavior for the password input field. This ensures that passwords are always submitted as sensitive fields. #16011\nImproved Sync History page performance by adding the Load more button, which you can click to display previous syncs. #15938\nImproved the validation error that displays when submitting an incomplete ServiceForm. #15625\nImproved the source-defined cursor and primary key by adding a tooltip, which displays the full cursor or primary key when you hover over them. #16116\nImproved Airbyte Cloud\u2019s method of updating source and destination definitions by using `airbyte-cron` to schedule updates. This allows us to keep connectors updated as the catalog changes. #16438\nImproved the speed that workspace connections are listed. #17004\n\nBugs",
    "tag": "airbyte"
  },
  {
    "title": "July 2022",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/release_notes/july_2022.md",
    "content": "July 2022\nAirbyte v0.39.27-alpha to v0.39.41-alpha\nThis page includes new features and improvements to the Airbyte Cloud and Airbyte Open Source platforms. \nNew features\n\n\nAdded per-stream state to the Airbyte Cloud and OSS platforms. Per-stream state currently includes per-stream resets and connection states, and it lays the groundwork for auto-detecting schema changes, parallel syncs, and more.  \n\n\nThe new flow gives you the option to refresh streams when saving changes to a connection. #14634 \n\n\nPer-stream reset functionality is now available for connections with a Postgres source. Per-stream resets allow you to reset only the affected streams when saving an edited connection, instead of resetting all streams in a connection. #14634 \n\n\nFor connections with a Postgres source, the state of the connection to the source is displayed in the Connection State. #15020 \n\n\nFor Airbyte Open Source users:\n\nIf you are using the Postgres source connector, upgrade your Airbyte platform to version v0.40.0-alpha or newer and upgrade your AzureBlobStorage connector to version 0.1.6 or newer. #15008 \n\n\n\n\n\nAdded `airbyte_type` to normalization. This displays whether `timestamp` and `time` have an associated time zone. #13591 \n\n\nAirbyte is currently developing a low-code connector builder, which allows you to easily create new source and destination connectors in your workspace. #14402 #14317 #14288 #14004 \n\n\nAdded documentation about the benefits and considerations of having a single workspace vs. multiple workspaces in Airbyte Cloud. #14608\n\n\nImprovements\n\n\nImproved platform security by using Docker images from the latest version of OpenJDK (openjdk:19-slim-bullseye). #14971 \n\n\nImproved Airbyte Open Source self-hosting by refactoring and publishing Helm charts according to best practices as we prepare to formally support Helm deployments. #14794 \n\n\nImproved Airbyte Open Source by supporting the OpenTelemetry (OTEL) Collector. Airbyte Open Source now sends telemetry data to the OTEL collector, and we included a set of recommended metrics to export to OTEL when running Airbyte Open Source at scale. #12908 \n\n\nImproved the Airbyte Connector Development Kit (CDK) by enabling detailed bug logs from the command line. In addition to the preset CDK debug logs, you can also create custom debug statements and display custom debug logs in the command line. #14521 \n\n\nImproved CDK by supporting a schema generator tool. #13518 \n\n\nImproved documentation about contributing locally by adding information on formatting connectors. #14661 \n\n\nImproved Octavia CLI so you can now:\n\n\nSwitch between Airbyte instances and deploy the same configurations on multiple instances. #13070 #13748 \n\n\nEnable normalization or custom DBT transformation from YAML configurations. #10973 \n\n\nSet custom HTTP headers on requests made to the Airbyte server. You can use CLI If you have instances secured with basic access authentication or identity-aware proxy (IAP). This lays the groundwork for making the CLI compatible with Airbyte Cloud once we release the public API. #13770 \n\n\nImport existing remote resources to a local Octavia project with `octavia import`. #14291 \n\n\nUse the `get` command to get existing configurations for sources, destinations, and connections. #13254 \n\n\n\n",
    "tag": "airbyte"
  },
  {
    "title": "October 2022",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/release_notes/october_2022.md",
    "content": "October 2022\nAirbyte v0.40.13 to v0.40.17\nThis page includes new features and improvements to the Airbyte Cloud and Airbyte Open Source platforms. \nNew features\n\nAdded the low-code connector builder UI to Airbyte OSS. It includes an embedded YAML editor and significantly reduces the time and complexity of building and maintaining connectors. #17482\nAdded Datadog Real User Monitoring (RUM) support to the webapp, which helps us monitor frontend performance in Airbyte Cloud. #17821\nAdded Nginx and Basic Auth to ensure security when using Airbyte Open Source. #17694\nNow when you start the Airbyte server and go to localhost:8000, you\u2019ll be prompted to log in before accessing your Airbyte workspace.   \nYou should change the default username (airbyte) and password (password) before you deploy Airbyte. If you do not want a username or password, you can remove them by setting `BASIC_AUTH_USERNAME` and `BASIC_AUTH_PASSWORD` to empty values (\" \") in your `.env` file. \nOur CLI and docs have been updated to reflect this change.\n\n\n\nImprovements\n\nSince adding Basic Auth to Airbyte Open Source, we improved the `load_test` script to reflect this change. Now when the `load_test` script sources the `.env` file, it includes `BASIC_AUTH_USERNAME` and `BASIC_AUTH_PASSWORD` when calling the API. #18273\nImproved the Airbyte platform by updating the Apache Commons Text from 1.9 to 1.10.0 because version 1.9 was affected by CVE 2022-42889 (Text4Shell). #18273 \nWe do not intend to update older versions of Airbyte because we were not affected by the vulnerable behavior:\nOur direct usages of `commons-text` either do not use the vulnerable class or are pinned to an unaffected version.\n\n\n\n\n",
    "tag": "airbyte"
  },
  {
    "title": "Securing Airbyte access",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/archive/securing-airbyte.md",
    "content": "Securing Airbyte access\nReporting Vulnerabilities\n\u26a0\ufe0f Please do not file GitHub issues or post on our public forum for security vulnerabilities as they are public! \u26a0\ufe0f\nAirbyte takes security issues very seriously. If you have any concern around Airbyte or believe you have uncovered a vulnerability, please get in touch via the e-mail address security@airbyte.io. In the message, try to provide a description of the issue and ideally a way of reproducing it. The security team will get back to you as soon as possible.\nNote that this security address should be used only for undisclosed vulnerabilities. Dealing with fixed issues or general questions on how to use the security features should be handled regularly via the user and the dev lists. Please report any security problems to us before disclosing it publicly.\nAccess control\nAirbyte, in its open-source version, does not support RBAC to manage access to the UI.\nHowever, multiple options exist for the operators to implement access control themselves.\nTo secure access to Airbyte you have three options:\n* Networking restrictions: deploy Airbyte in a private network or use a firewall to filter which IP is allowed to access your host.\n* Put Airbyte behind a reverse proxy and handle the access control on the reverse proxy side. \n* If you deployed Airbyte on a cloud provider: \n    * GCP: use the Identity-Aware proxy service\n    * AWS: use the AWS Systems Manager Session Manager service\nNon exhaustive online resources list to set up auth on your reverse proxy:\n* Configure HTTP Basic Auth on NGINX for Airbyte\n* Kubernetes: Basic auth on a Nginx ingress controller\n* How to set up Okta SSO on an NGINX reverse proxy\n* How to enable HTTP Basic Auth on Caddy",
    "tag": "airbyte"
  },
  {
    "title": "Mongo DB",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/archive/mongodb.md",
    "content": "Mongo DB\nThe MongoDB source supports Full Refresh and Incremental sync strategies.\nResulting schema\nMongoDB does not have anything like table definition, thus we have to define column types from actual attributes and their values. Discover phase have two steps:\nStep 1. Find all unique properties\nConnector runs the map-reduce command which returns all unique document props in the collection. Map-reduce approach should be sufficient even for large clusters.\nNote\nTo work with Atlas MongoDB, a non-free tier is required, as the free tier does not support the ability to perform the mapReduce operation.\nStep 2. Determine property types\nFor each property found, connector selects 10k documents from the collection where this property is not empty. If all the selected values have the same type - connector will set appropriate type to the property. In all other cases connector will fallback to `string` type.\nFeatures\n| Feature | Supported |\n| :--- | :--- |\n| Full Refresh Sync | Yes |\n| Incremental - Append Sync | Yes |\n| Replicate Incremental Deletes | No |\n| Namespaces | No |\nFull Refresh sync\nWorks as usual full refresh sync.\nIncremental sync\nCursor field can not be nested. Currently only top level document properties are supported.\nCursor should never be blank. In case cursor is blank - the incremental sync results might be unpredictable and will totally rely on MongoDB comparison algorithm.\nOnly `datetime` and `integer` cursor types are supported. Cursor type is determined based on the cursor field name:\n\n`datetime` - if cursor field name contains a string from: `time`, `date`, `_at`, `timestamp`, `ts`\n`integer` - otherwise\n\nGetting started\nThis guide describes in details how you can configure MongoDB for integration with Airbyte.\nCreate users\nRun `mongo` shell, switch to `admin` database and create a `READ_ONLY_USER`. `READ_ONLY_USER` will be used for Airbyte integration. Please make sure that user has read-only privileges.\n`javascript\nmongo\nuse admin;\ndb.createUser({user: \"READ_ONLY_USER\", pwd: \"READ_ONLY_PASSWORD\", roles: [{role: \"read\", db: \"TARGET_DATABASE\"}]}`\nMake sure the user have appropriate access levels.\nConfigure application\nIn case your application uses MongoDB without authentication you will have to adjust code base and MongoDB config to enable MongoDB authentication. Otherwise your application might go down once MongoDB authentication will be enabled.\nEnable MongoDB authentication\nOpen `/etc/mongod.conf` and add/replace specific keys:\n```yaml\nnet:\n  bindIp: 0.0.0.0\nsecurity:\n  authorization: enabled\n```\nBinding to `0.0.0.0` will allow to connect to database from any IP address.\nThe last line will enable MongoDB security. Now only authenticated users will be able to access the database.\nConfigure firewall\nMake sure that MongoDB is accessible from external servers. Specific commands will depend on the firewall you are using (UFW/iptables/AWS/etc). Please refer to appropriate documentation.\nYour `READ_ONLY_USER` should now be ready for use with Airbyte.\nPossible configuration Parameters\n\nAuthentication Source\nHost: URL of the database\nPort: Port to use for connecting to the database\nUser: username to use when connecting\nPassword: used to authenticate the user\nReplica Set\nWhether to enable SSL\n",
    "tag": "airbyte"
  },
  {
    "title": "Platform",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/archive/changelog/platform.md",
    "content": "\ndescription: Be sure to not miss out on new features and improvements!\nPlatform\nThis is the changelog for Airbyte Platform. For our connector changelog, please visit our Connector Changelog page.\n20-12-2021 - 0.32.5\n\nAdd an endpoint that specify that the feedback have been given after the first sync.\n\n18-12-2021 - 0.32.4\n\nNo major changes to Airbyte Core.\n\n18-12-2021 - 0.32.3\n\nNo major changes to Airbyte Core.\n\n18-12-2021 - 0.32.2\n\nImprove error handling when additional sources/destinations cannot be read.\nImplement connector config dependency for OAuth consent URL.\nTreat oauthFlowInitParameters just as hidden instead of getting rid of them.\nStop using gentle close with heartbeat.\n\n17-12-2021 - 0.32.1\n\nAdd to the new connection flow form with an existing source and destination dropdown.\nImplement protocol change for OAuth outputs.\nEnhance API for use by cloud to provide per-connector billing info.\n\n11-12-2021 - 0.32.0\n\nThis is a MAJOR version update. You need to update to this version before updating to any version newer than `0.32.0`\n\n11-11-2021 - 0.31.0\n\nNo major changes to Airbyte Core.\n\n11-11-2021 - 0.30.39\n\nWe migrated our secret management to Google Secret Manager, allowing us to scale how many connectors we support.\n\n11-09-2021 - 0.30.37\n\nNo major changes to Airbyte Core.\n\n11-09-2021 - 0.30.36\n\nNo major changes to Airbyte Core.\n\n11-08-2021 - 0.30.35\n\nNo major changes to Airbyte Core.\n\n11-06-2021 - 0.30.34\n\nNo major changes to Airbyte Core.\n\n11-06-2021 - 0.30.33\n\nNo major changes to Airbyte Core.\n\n11-05-2021 - 0.30.32\n\nAirbyte Server no longer crashes from having too many open files.\n\n11-04-2021 - 0.30.31\n\nNo major changes to Airbyte Core.\n\n11-01-2021 - 0.30.25\n\nNo major changes to Airbyte Core.\n\n11-01-2021 - 0.30.24\n\nIncremental normalization is live. Basic normalization no longer runs on already normalized data, making it way faster and cheaper.\n\n11-01-2021 - 0.30.23\n\nNo major changes to Airbyte Core.\n\n10-21-2021 - 0.30.22\n\nWe now support experimental deployment of Airbyte on Macbooks with M1 chips!\n\n:::info\nThis interim patch period mostly contained stability changes for Airbyte Cloud, so we skipped from `0.30.16` to `0.30.22`.\n:::\n10-07-2021 - 0.30.16\n\nOn Kubernetes deployments, you can now configure the Airbyte Worker Pod's image pull policy.\n\n:::info\nThis interim patch period mostly contained stability changes for Airbyte Cloud, so we skipped from `0.30.2` to `0.30.16`.\n:::\n09-30-2021 - 0.30.2\n\nFixed a bug that would fail Airbyte upgrades for deployments with sync notifications.\n\n09-24-2021 - 0.29.22\n\nWe now have integration tests for SSH.\n\n09-19-2021 - 0.29.21\n\nYou can now deploy Airbyte on Kubernetes with a Helm Chart!\n\n09-16-2021 - 0.29.19\n\nFixes a breaking bug that prevents Airbyte upgrading from older versions.\n\n09-15-2021 - 0.29.18\n\nBuilding images is now optional in the CI build. \n\n09-08-2021 - 0.29.17\n\nYou can now properly cancel deployments when deploying on K8s.\n\n09-08-2021 - 0.29.16\n\nYou can now send notifications via webhook for successes and failures on Airbyte syncs.\nScheduling jobs and worker jobs are now separated, allowing for workers to be scaled horizontally.\n\n09-04-2021 - 0.29.15\n\nFixed a bug that made it possible for connector definitions to be duplicated, violating uniqueness.\n\n09-02-2021 - 0.29.14\n\nNothing of note. \n\n08-27-2021 - 0.29.13\n\nThe scheduler now waits for the server before it creates any databases.\nYou can now apply tolerations for Airbyte Pods on K8s deployments.\n\n08-23-2021 - 0.29.12\n\nSyncs now have a `max_sync_timeout` that times them out after 3 days.\nFixed Kube deploys when logging with Minio.\n\n08-20-2021 - 0.29.11\n\nNothing of note.\n\n08-20-2021 - 0.29.10\n\nMigration of Python connector template images to Alpine Docker images to reduce size.\n\n08-20-2021 - 0.29.9\n\nNothing of note.\n\n08-17-2021 - 0.29.8\n\nNothing of note.\n\n08-14-2021 - 0.29.7\n\nRe-release: Fixed errant ENV variable in `0.29.6`\n\n08-14-2021 - 0.29.6\n\nConnector pods no longer fail with edge case names for the associated Docker images.\n\n08-14-2021 - 0.29.5\n\nNothing of note.\n\n08-12-2021 - 0.29.4\n\nIntroduced implementation for date-time support in normalization.\n\n08-9-2021 - 0.29.3\n\nImporting configuration no longer removes available but unused connectors. \n\n08-6-2021 - 0.29.2\n\nFixed nil pointer exception in version migrations.\n\n07-29-2021 - 0.29.1\n\nWhen migrating, types represented in the config archive need to be a subset of the types declared in the schema.\n\n07-28-2021 - 0.29.0\n\nDeprecated `DEFAULT_WORKSPACE_ID`; default workspace no longer exists by default.\n\n07-28-2021 - 0.28.2\n\nBackend now handles workspaceId for WebBackend operations.\n\n07-26-2021 - 0.28.1\n\nK8s: Overly-sensitive logs are now silenced.\n\n07-22-2021 - 0.28.0\n\nAcceptance test dependencies fixed.\n\n07-22-2021 - 0.27.5\n\nFixed unreliable logging on Kubernetes deployments.\nIntroduced pre-commit to auto-format files on commits.\n\n07-21-2021 - 0.27.4\n\nConfig persistence is now migrated to the internal Airbyte database.\nSource connector ports now properly close when deployed on Kubernetes.\nMissing dependencies added that allow acceptance tests to run.\n\n07-15-2021 - 0.27.3\n\nFixed some minor API spec errors.\n\n07-12-2021 - 0.27.2\n\nGCP environment variable is now stubbed out to prevent noisy and harmless errors.\n\n07-8-2021 - 0.27.1\n\nNew API endpoint: List workspaces\nK8s: Server doesn't start up before Temporal is ready to operate now.\nSilent source failures caused by last patch fixed to throw exceptions.\n\n07-1-2021 - 0.27.0\n\nAirbyte now automatically upgrades on server startup!\nAirbyte will check whether your `.env` Airbyte version is compatible with the Airbyte version in the database and upgrade accordingly.\nWhen running Airbyte on K8s logs will automatically be stored in a Minio bucket unless configured otherwise.\nCDC for MySQL now handles decimal types correctly.\n\n06-21-2021 - 0.26.2\n\nFirst-Class Kubernetes support!\n\n06-16-2021 - 0.26.0\n\nCustom dbt transformations! \nYou can now configure your destination namespace at the table level when setting up a connection!  \nMigrate basic normalization settings to the sync operations.\n\n06-09-2021 - 0.24.8 / 0.25.0\n\nBugfix: Handle TINYINT(1) and BOOLEAN correctly and fix target file comparison for MySQL CDC.\nBugfix: Updating the source/destination name in the UI now works as intended.\n\n06-04-2021 - 0.24.7\n\nBugfix: Ensure that logs from threads created by replication workers are added to the log file.\n\n06-03-2021 - 0.24.5\n\nRemove hash from table names when it's not necessary for normalization outputs.\n\n06-03-2021 - 0.24.4\n\nPythonCDK: change minimum Python version to 3.7.0\n\n05-28-2021 - 0.24.3\n\nMinor fixes to documentation\nReliability updates in preparation for custom transformations  \nLimit Docker log size to 500 MB (#3702)\n\n05-26-2021 - 0.24.2\n\nFix for file names being too long in Windows deployments (#3625)\nAllow users to access the API and WebApp from the same port (#3603)\n\n05-25-2021 - 0.24.1\n\nCheckpointing for incremental syncs that will now continue where they left off even if they fail! (#3290)\n\n05-25-2021 - 0.24.0\n\nAvoid dbt runtime exception \"maximum recursion depth exceeded\" in ephemeral materialization (#3470)\n\n05-18-2021 - 0.23.0\n\nDocumentation to deploy locally on Windows is now available (#3425)\nConnector icons are now displayed in the UI\nRestart core containers if they fail automatically (#3423)\nProgress on supporting custom transformation using dbt. More updates on this soon!\n\n05-11-2021 - 0.22.3\n\nBump K8s deployment version to latest stable version, thanks to Coetzee van Staden\nAdded tutorial to deploy Airbyte on Azure VM (#3171), thanks to geekwhocodes\nProgress on checkpointing to support rate limits better\nUpgrade normalization to use dbt from docker images (#3186)\n\n05-04-2021 - 0.22.2\n\nSplit replication and normalization into separate temporal activities (#3136)\nFix normalization Nesting bug (#3110)\n\n04-27-2021 - 0.22.0\n\nReplace timeout for sources (#3031)\nFix UI issue where tables with the same name are selected together (#3032)\nFix feed handling when feeds are unavailable (#2964)\nExport whitelisted tables (#3055)\nCreate a contributor bootstrap script (#3028) (#3054), thanks to nclsbayona\n\n04-20-2021 - 0.21.0\n\nNamespace support: supported source-destination pairs will now sync data into the same namespace as the source (#2862)\nAdd \u201cRefresh Schema\u201d button (#2943)\nIn the Settings, you can now add a webhook to get notified when a sync fails\nAdd destinationSyncModes to connection form\nAdd tooltips for connection status icons\n\n04-12-2021 - 0.20.0\n\nChange Data Capture (CDC) is now supported for Postgres, thanks to @jrhizor and @cgardens. We will now expand it to MySQL and MSSQL in the coming weeks.\nWhen displaying the schema for a source, you can now search for table names, thanks to @jamakase\nBetter feedback UX when manually triggering a sync with \u201cSync now\u201d\n\n04-07-2021 - 0.19.0\n\nNew Connections page where you can see the list of all your connections and their statuses. \nNew Settings page to update your preferences.\nBugfix where very large schemas caused schema discovery to fail.\n\n03-29-2021 - 0.18.1\n\nSurface the health of each connection so that a user can spot any problems at a glance. \nAdded support for deduplicating records in the destination using a primary key using incremental dedupe -  \nA source\u2019s extraction mode (incremental, full refresh) is now decoupled from the destination\u2019s write mode -- so you can repeatedly append full refreshes to get repeated snapshots of data in your source.\nNew Upgrade all button in Admin to upgrade all your connectors at once \nNew Cancel job button in Connections Status page when a sync job is running, so you can stop never-ending processes.\n\n03-22-2021 - 0.17.2\n\nImproved the speed of get spec, check connection, and discover schema by migrating to the Temporal workflow engine.\nExposed cancellation for sync jobs in the API (will be exposed in the UI in the next week!).\nBug fix: Fix issue where migration app was OOMing.\n\n03-15-2021 - 0.17.1\n\nCreating and deleting multiple workspaces is now supported via the API. Thanks to @Samuel Gordalina for contributing this feature!\nNormalization now supports numeric types with precision greater than 32 bits\nNormalization now supports union data types\nSupport longform text inputs in the UI for cases where you need to preserve formatting on connector inputs like .pem keys\nExpose the latest available connector versions in the API\nAirflow: published a new tutorial for how to use the Airbyte operator. Thanks @Marcos Marx for writing the tutorial! \nConnector Contributions: All connectors now describe how to contribute to them without having to touch Airbyte\u2019s monorepo build system -- just work on the connector in your favorite dev setup!\n\n03-08-2021 - 0.17\n\nIntegration with Airflow is here. Thanks to @Marcos Marx, you can now run Airbyte jobs from Airflow directly. A tutorial is on the way and should be coming this week!\nAdd a prefix for tables, so that tables with the same name don't clobber each other in the destination\n\n03-01-2021 - 0.16\n\nWe made some progress to address nested tables in our normalization.\n\nPreviously, basic normalization would output nested tables as-is and append a number for duplicate tables. For example, Stripe\u2019s nested address fields go from:\n`text\n  Address\n  address_1`\nTo\n`text\n  Charges_source_owner_755_address\n  customers_shipping_c70_address`\nAfter the change, the parent tables are combined with the name of the nested table to show where the nested table originated. This is a breaking change for the consumers of nested tables. Consumers will need to update to point at the new tables.\n02-19-2021 - 0.15\n\nWe now handle nested tables with the normalization steps. Check out the video below to see how it works. \n\n{% embed url=\"https://youtu.be/I4fngMnkJzY\" caption=\"\" %}\n02-12-2021 - 0.14\n\nFront-end changes:\nDisplay Airbyte's version number\nDescribe schemas using JsonSchema\nBetter feedback on buttons\n\nBeta launch - 0.13 - Released 02/02/2021\n\nAdd connector build status dashboard\nSupport Schema Changes in Sources\nSupport Import / Export of Airbyte Data in the Admin section of the UI\nBug fixes:\nIf Airbyte is closed during a sync the running job is not marked as failed\nAirbyte should fail when deployment version doesn't match data version\nUpgrade Airbyte Version without losing existing configuration / data\n\n0.12-alpha - Released 01/20/2021\n\nAbility to skip onboarding\nMiscellaneous bug fixes:\nA long discovery request causes a timeout in the UI type/bug\nOut of Memory when replicating large table from MySQL\n\n0.11.2-alpha - Released 01/18/2021\n\nIncrease timeout for long running catalog discovery operations from 3 minutes to 30 minutes to avoid prematurely failing long-running operations \n\n0.11.1-alpha - Released 01/17/2021\nBugfixes\n\nWriting boolean columns to Redshift destination now works correctly \n\n0.11.0-alpha - Delivered 01/14/2021\nNew features\n\nAllow skipping the onboarding flow in the UI\nAdd the ability to reset a connection's schema when the underlying data source schema changes\n\nBugfixes\n\nFix UI race condition which showed config for the wrong connector when rapidly choosing between different connector \nFix a bug in MSSQL and Redshift source connectors where custom SQL types weren't being handled correctly. Pull request\nSupport incremental sync for Salesforce, Slack, and Braintree sources\nGracefully handle invalid nuemric values (e.g NaN or Infinity) in MySQL, MSSQL, and Postgtres DB sources\nFix flashing red sources/destinations fields after success submit\nFix a bug which caused getting a connector's specification to hang indefinitely if the connector docker image failed to download\n\nNew connectors\n\nTempo\nAppstore\n\n0.10.0 - delivered on 01/04/2021\n\nYou can now deploy Airbyte on Kuberbetes **(alpha version)\nSupport incremental sync for Mixpanel and HubSpot sources\nFixes on connectors:\nFixed a bug in the GitHub connector where the connector didn\u2019t verify the provided API token was granted the correct permissions\nFixed a bug in the Google Sheets connector where rate limits were not always respected\nAlpha version of Facebook marketing API v9. This connector is a native Airbyte connector (current is Singer based).\nNew source: Plaid (contributed by @tgiardina - thanks Thomas!)\n\n0.9.0 - delivered on 12/23/2020\n\nNew chat app from the web app so you can directly chat with the team for any issues you run into\nDebugging has been made easier in the UI, with checks, discover logs, and sync download logs\nSupport of Kubernetes in local. GKE will come at the next release.\nNew source: Looker **\n\n0.8.0 - delivered on 12/17/2020\n\nIncremental - Append\"\nWe now allow sources to replicate only new or modified data. This enables to avoid re-fetching data that you have already replicated from a source.\nThe delta from a sync will be appended to the existing data in the data warehouse.\nHere are all the details of this feature.\nIt has been released for 15 connectors, including Postgres, MySQL, Intercom, Zendesk, Stripe, Twilio, Marketo, Shopify, GitHub, and all the destination connectors. We will expand it to all the connectors in the next couple of weeks.\nOther features:\nImprove interface for writing python sources (should make writing new python sources easier and clearer).\nAdd support for running Standard Source Tests with files (making them easy to run for any language a source is written in)\nAdd ability to reset data for a connection.\nBug fixes:\nUpdate version of test containers we use to avoid pull issues while running tests.\nFix issue where jobs were not sorted by created at in connection detail view.\nNew sources: Intercom, Mixpanel, Jira Cloud, Zoom, Drift, Microsoft Teams\n\n0.7.0 - delivered on 12/07/2020\n\nNew destination: our own Redshift warehouse connector. You can also use this connector for Panoply.\nNew sources: 8 additional source connectors including Recurly, Twilio, Freshdesk. Greenhouse, Redshift (source), Braintree, Slack, Zendesk Support\nBug fixes\n\n0.6.0 - delivered on 11/23/2020\n\nSupport multiple destinations\nNew source: Sendgrid\nSupport basic normalization\nBug fixes\n\n0.5.0 - delivered on 11/18/2020\n\nNew sources: 10 additional source connectors, including Files (CSV, HTML, JSON...), Shopify, MSSQL, Mailchimp\n\n0.4.0 - delivered on 11/04/2020\nHere is what we are working on right now:\n\nNew destination: our own Snowflake warehouse connector\nNew sources: Facebook Ads, Google Ads.\n\n0.3.0 - delivered on 10/30/2020\n\nNew sources: Salesforce, GitHub, Google Sheets, Google Analytics, HubSpot, Rest APIs, and MySQL\nIntegration test suite for sources\nImprove build speed\n\n0.2.0 - delivered on 10/21/2020\n\na new Admin section to enable users to add their own connectors, in addition to upgrading the ones they currently use\nimprove the developer experience (DX) for contributing new connectors with additional documentation and a connector protocol\nour own BigQuery warehouse connector\nour own Postgres warehouse connector\nsimplify the process of supporting new Singer taps, ideally make it a 1-day process\n\n0.1.0 - delivered on 09/23/2020\nThis is our very first release after 2 months of work.\n\nNew sources: Stripe, Postgres\nNew destinations: BigQuery, Postgres\nOnly one destination: we only support one destination in that 1st release, but you will soon be able to add as many as you need.\nLogs & monitoring: you can now see your detailed logs\nScheduler: you now have 10 different frequency options for your recurring syncs\nDeployment: you can now deploy Airbyte via a simple Docker image, or directly on AWS and GCP\nNew website: this is the day we launch our website - airbyte.io. Let us know what you think\nNew documentation: this is the 1st day for our documentation too\nNew blog: we published a few articles on our startup journey, but also about our vision to making data integrations a commodity.\n\nStay tuned, we will have new sources and destinations very soon! Don't hesitate to subscribe to our newsletter to receive our product updates and community news.",
    "tag": "airbyte"
  },
  {
    "title": "Changelog",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/archive/changelog",
    "content": "Changelog\n1/28/2022 Summary\n\nNew Source: Chartmogul (contributyed by Titas Skreb\u0117)\nNew Source: Hellobaton (contributed by Daniel Luftspring)\nNew Source: Flexport (contributed by Juozas)\n\nNew Source: PersistIq (contributed by Wadii Zaim)\n\n\n\u2728 Postgres Source: Users can now select which schemas they wish to sync before discovery. This makes the discovery stage for large instances much more performant.\n\n\u2728 Shopify Source: Now verifies permissions on the token before accessing resources.\n\u2728 Snowflake Destination: Users now have access to an option to purge their staging data.\n\u2728 HubSpot Source: Added some more fields for the email_events stream.\n\u2728 Amazon Seller Partner Source: Added the GET_FLAT_FILE_ALL_ORDERS_DATA_BY_LAST_UPDATE_GENERAL report stream. (contributed by @ron-damon)\n\n\u2728 HubSpot Source: Added the form_submission and property_history streams.\n\n\n\ud83d\udc1b DynamoDB Destination: The parameter dynamodb_table_name is now named dynamodb_table_name_prefix to more accurately represent it.\n\n\ud83d\udc1b Intercom Source: The handling of scroll param is now fixed when it is expired.\n\ud83d\udc1b S3 + GCS Destinations: Now support arrays with unknown item type.\n\ud83d\udc1b Postgres Source: Now supports handling of the Java SQL date type.\n\ud83d\udc1b Salesforce Source: No longer fails during schema generation.\n\n1/13/2022 Summary\n\u26a0\ufe0f WARNING \u26a0\ufe0f\nSnowflake Source: Normalization with Snowflake now produces permanent tables. [If you want to continue creating transient tables, you will need to create a new transient database for Airbyte.]\n\n\u2728 GitHub Source: PR related streams now support incremental sync.\n\u2728 HubSpot Source: We now support ListMemberships in the Contacts stream.\n\n\u2728 Azure Blob Storage Destination: Now has the option to add a BufferedOutputStream to improve performance and fix writing data with over 50GB in a stream. (contributed by @bmatticus)\n\n\n\ud83d\udc1b Normalization partitioning now works as expected with FLOAT64 and BigQuery.\n\n\ud83d\udc1b Normalization now works properly with quoted and case sensitive columns.\n\ud83d\udc1b Source MSSQL: Added support for some missing data types.\n\ud83d\udc1b Snowflake Destination: Schema is now not created if it previously exists.\n\ud83d\udc1b Postgres Source: Now properly reads materialized views.\n\ud83d\udc1b Delighted Source: Pagination for survey_responses, bounces and unsubscribes streams now works as expected.\n\ud83d\udc1b Google Search Console Source: Incremental sync now works as expected.\n\ud83d\udc1b Recurly Source: Now does not load all accounts when importing account coupon redemptions.\n\ud83d\udc1b Salesforce Source: Now properly handles 400 when streams don't support query or queryAll.\n\n1/6/2022 Summary\n\nNew Source: 3PL Central (contributed by Juozas)\nNew Source: My Hours (contributed by Wisse Jelgersma)\nNew Source: Qualaroo (contributed by gunu)\n\nNew Source: SearchMetrics\n\n\n\ud83d\udc8e Salesforce Source: Now supports filtering streams at configuration, making it easier to handle large Salesforce instances.\n\n\ud83d\udc8e Snowflake Destination: Now supports byte-buffering for staged inserts.\n\ud83d\udc8e Redshift Destination: Now supports byte-buffering for staged inserts.\n\u2728 Postgres Source: Now supports all Postgres 14 types.\n\u2728 Recurly Source: Now supports incremental sync for all streams.\n\u2728 Zendesk Support Source: Added the Brands, CustomRoles, and Schedules streams.\n\u2728 Zendesk Support Source: Now uses cursor-based pagination.\n\u2728 Kustomer Source: Setup configuration is now more straightforward.\n\u2728 Hubspot Source: Now supports incremental sync on all streams where possible.\n\u2728 Facebook Marketing Source: Fixed schema for breakdowns fields.\n\u2728 Facebook Marketing Source: Added asset_feed_spec to AdCreatives stream.\n\n\u2728 Redshift Destination: Now has an option to toggle the deletion of staging data.\n\n\n\ud83d\udc1b S3 Destination: Avro and Parquet formats are now processed correctly.\n\n\ud83d\udc1b Snowflake Destination: Fixed SQL Compliation error.\n\ud83d\udc1b Kafka Source: SASL configurations no longer throw null pointer exceptions (contributed by Nitesh Kumar)\n\ud83d\udc1b Salesforce Source: Now throws a 400 for non-queryable streams.\n\ud83d\udc1b Amazon Ads Source: Polling for report generation is now much more resilient. (contributed by Juozas)\n\ud83d\udc1b Jira Source: The filters stream now works as expected.\n\ud83d\udc1b BigQuery Destination: You can now properly configure the buffer size with the part_size config field.\n\ud83d\udc1b Snowflake Destination: You can now properly configure the buffer size with the part_size config field.\n\ud83d\udc1b CockroachDB Source: Now correctly only discovers tables the user has permission to access.\n\ud83d\udc1b Stripe Source: The date and arrival_date fields are now typed correctly.\n\n12/16/2021 Summary\n\ud83c\udf89  First off... There's a brand new CDK! Menno Hamburg contributed a .NET/C# implementation for our CDK, allowing you to write HTTP API sources and Generic Dotnet sources. Thank you so much Menno, this is huge!\n\nNew Source: OpenWeather\nNew Destination: ClickHouse (contributed by @Bo)\nNew Destination: RabbitMQ (contributed by @Luis Gomez)\nNew Destination: Amazon SQS (contributed by @Alasdair Brown)\n\nNew Destination: Rockset (contributed by @Steve Baldwin)\n\n\n\u2728 Facebook Marketing Source: Updated the campaign schema with more relevant fields. (contributed by @Maxime Lavoie)\n\n\u2728 TikTok Marketing Source: Now supports the Basic Report stream.\n\u2728 MySQL Source: Now supports all MySQL 8.0 data types.\n\u2728 Klaviyo Source: Improved performance, added incremental sync support to the Global Exclusions stream.\n\u2728 Redshift Destination: You can now specify a bucket path to stage your data in before inserting.\n\u2728 Kubernetes deployments: Sidecar memory is now 25Mi, up from 6Mi to cover all usage cases.\n\n\u2728 Kubernetes deployments: The Helm chart can now set up writing logs to S3 easily. (contributed by @Valentin Nourdin)\n\n\n\ud83d\udc1b Python CDK: Now shows the stack trace of unhandled exceptions.\n\n\ud83d\udc1b Google Analytics Source: Fix data window input validation, fix date type conversion.\n\ud83d\udc1b Google Ads Source: Data from the end_date for syncs is now included in a sync.\n\ud83d\udc1b Marketo Source: Fixed issues around input type conversion and conformation to the schema.\n\ud83d\udc1b Mailchimp Source: Fixed schema conversion error causing sync failures.\n\ud83d\udc1b PayPal Transactions Source: Now reports full error message details on failure.\n\ud83d\udc1b Shopify Source: Normalization now works as expected.\n\n12/9/2021 Summary\n\u26a0\ufe0f WARNING \u26a0\ufe0f\nv0.33.0 is a minor version with breaking changes. Take the normal precautions with upgrading safely to this version.\nv0.33.0 has a bug that affects GCS logs on Kubernetes. Upgrade straight to v0.33.2 if you are running a K8s deployment of Airbyte.\n\nNew Source: Mailgun\n\n\ud83c\udf89 Snowflake Destination: You can now stage your inserts, making them much faster.\n\n\u2728 Google Ads Source: Source configuration is now more clear.\n\u2728 Google Analytics Source: Source configuration is now more clear.\n\u2728 S3 Destination: You can now write timestamps in Avro and Parquet formats.\n\u2728 BigQuery & BigQuery Denormalized Destinations: Now use byte-based buffering for batch inserts.\n\n\u2728 Iterable Source: Now has email validation on the list_users stream.\n\n\n\ud83d\udc1b Incremental normalization now works properly with empty tables.\n\n\ud83d\udc1b LinkedIn Ads Source: 429 response is now properly handled.\n\ud83d\udc1b Intercom Source: Now handles failed pagination requests with backoffs.\n\ud83d\udc1b Intercom Source: No longer drops records from the conversation stream.\n\ud83d\udc1b Google Analytics Source: 400 errors no longer get ignored with custom reports.\n\ud83d\udc1b Marketo Source: The createdAt and updatedAt fields are now formatted correctly.\n\n12/2/2021 Summary\n\ud83c\udf83 Hacktoberfest Submissions \ud83c\udf83\n\nNew Destination: Redis (contributed by @Ivica Taseski)\nNew Destination: MQTT (contributed by @Mario Molina)\nNew Destination: Google Firestore (contributed by @Adam Dobrawy)\nNew Destination: Kinesis (contributed by @Ivica Taseski)\nNew Source: Zenloop (contributed by @Alexander Batoulis)\n\nNew Source: Outreach (contributed by @Luis Gomez)\n\n\n\u2728 Zendesk Source: The chats stream now supports incremental sync and added testing for all streams.\n\n\ud83d\udc1b Monday Source: Pagination now works as expected and the schema has been fixed.\n\ud83d\udc1b Postgres Source: Views are now properly listed during schema discovery.\n\ud83d\udc1b Postgres Source: Using the money type with an amount greater than 1000 works properly now.\n\ud83d\udc1b Google Search Console Search: We now set a default end_data value.\n\ud83d\udc1b Mixpanel Source: Normalization now works as expected and streams are now displayed properly in the UI.\n\ud83d\udc1b MongoDB Source: The DATE_TIME type now uses milliseconds.\n\n11/25/2021 Summary\nHey Airbyte Community! Let's go over all the changes from v.32.5 and prior!\n\ud83c\udf83 Hacktoberfest Submissions \ud83c\udf83\n* New Source: Airtable (contributed by Tuan Nguyen).\n* New Source: Notion (contributed by Bo Lu).\n* New Source: Pardot (contributed by Tuan Nguyen).\n\n\nNew Source: Youtube analytics.\n\n\n\u2728 Source Exchange Rates: add ignore_weekends option.\n\n\u2728 Source Facebook: add the videos stream.\n\u2728 Source Freshdesk: removed the limitation in streams pagination.\n\u2728 Source Jira: add option to render fields in HTML format.\n\u2728 Source MongoDB v2: improve read performance.\n\u2728 Source Pipedrive: specify schema for \"persons\" stream.\n\u2728 Source PostgreSQL: exclude tables on which user doesn't have select privileges.\n\n\u2728 Source SurveyMonkey: improve connection check.\n\n\n\ud83d\udc1b Source Salesforce:  improve resiliency of async bulk jobs.\n\n\ud83d\udc1b Source Zendesk Support: fix missing ticket_id in ticket_comments stream.\n\ud83d\udc1b Normalization: optimize incremental normalization runtime with Snowflake.\n\nAs usual, thank you so much to our wonderful contributors this week that have made Airbyte into what it is today: Madison Swain-Bowden, Tuan Nguyen, Bo Lu, Adam Dobrawy, Christopher Wu, Luis Gomez, Ivica Taseski, Mario Molina, Ping Yee, Koji Matsumoto, Sujit Sagar, Shadab, Juozas V.(Labanoras Tech) and Serhii Chvaliuk!\n11/17/2021 Summary\nHey Airbyte Community! Let's go over all the changes from v.32.1 and prior! But first, there's an important announcement I need to make about upgrading Airbyte to v.32.1.\n\u26a0\ufe0f WARNING \u26a0\ufe0f\nUpgrading to v.32.0 is equivalent to a major version bump. If your current version is v.32.0, you must upgrade to v.32.0 first before upgrading to any later version\nKeep in mind that this upgrade requires your all of your connector Specs to be retrievable, or Airbyte will fail on startup. You can force delete your connector Specs by setting the `VERSION_0_32_0_FORCE_UPGRADE` environment variable to `true`. Steps to specifically check out v.32.0 and details around this breaking change can be found here.\nNow back to our regularly scheduled programming.\n\ud83c\udf83 Hacktoberfest Submissions \ud83c\udf83\n\nNew Destination: ScyllaDB (contributed by Ivica Taseski)\nNew Source: Azure Table Storage (contributed by geekwhocodes)\n\nNew Source: Linnworks (contributed by Juozas V.(Labanoras Tech))\n\n\n\u2728 Source MySQL: Now has basic performance tests.\n\n\n\u2728 Source Salesforce: We now automatically transform and handle incorrect data for the anyType and calculated types.\n\n\n\ud83d\udc1b IBM Db2 Source: Now handles conversion from DECFLOAT to BigDecimal correctly.\n\n\ud83d\udc1b MSSQL Source: Now handles VARBINARY correctly.\n\ud83d\udc1b CockroachDB Source: Improved parsing of various data types.\n\nAs usual, thank you so much to our wonderful contributors this week that have made Airbyte into what it is today: Achmad Syarif Hidayatullah, Tuan Nguyen, Ivica Taseski, Hai To, Juozas, gunu, Shadab, Per-Victor Persson, and Harsha Teja Kanna!\n11/11/2021 Summary\nTime to go over changes from v.30.39! And... let's get another update on Hacktoberfest.\n\ud83c\udf83 Hacktoberfest Submissions \ud83c\udf83\n\nNew Destination: Cassandra (contributed by Ivica Taseski)\nNew Destination: Pulsar (contributed by Mario Molina)\nNew Source: Confluence (contributed by Tuan Nguyen)\nNew Source: Monday (contributed by Tuan Nguyen)\nNew Source: Commerce Tools (contributed by James Wilson)\n\nNew Source: Pinterest Marketing (contributed by us!)\n\n\n\u2728 Shopify Source: Now supports the FulfillmentOrders and Fulfillments streams.\n\n\u2728 Greenhouse Source: Now supports the Demographics stream.\n\u2728 Recharge Source: Broken requests should now be re-requested with improved backoff.\n\u2728 Stripe Source: Now supports the checkout_sessions, checkout_sessions_line_item, and promotion_codes streams.\n\n\u2728 Db2 Source: Now supports SSL.\n\n\n\ud83d\udc1b We've made some updates to incremental normalization to fix some outstanding issues. Details\n\n\ud83d\udc1b Airbyte Server no longer crashes due to too many open files.\n\ud83d\udc1b MSSQL Source: Data type conversion with smalldatetime and smallmoney works correctly now.\n\ud83d\udc1b Salesforce Source: anyType fields can now be retrieved properly with the BULK API\n\ud83d\udc1b BigQuery-Denormalized Destination: Fixed JSON parsing with $ref fields.\n\nAs usual, thank you to our awesome contributors that have done awesome work during the last week: Tuan Nguyen, Harsha Teja Kanna, Aaditya S, James Wilson, Vladimir Remar, Yuhui Shi, Mario Molina, Ivica Taseski, Collin Scangarella, and haoranyu!\n11/03/2021 Summary\nIt's patch notes time. Let's go over the changes from 0.30.24 and before. But before we do, let's get a quick update on how Hacktober is going!\n\ud83c\udf83 Hacktoberfest Submissions \ud83c\udf83\n\nNew Destination: Elasticsearch (contributed by Jeremy Branham)\nNew Source: Salesloft (contributed by Pras)\nNew Source: OneSignal (contributed by Bo)\nNew Source: Strava (contributed by terencecho)\nNew Source: Lemlist (contributed by Igli Koxha)\nNew Source: Amazon SQS (contributed by Alasdair Brown)\nNew Source: Freshservices (contributed by Tuan Nguyen)\nNew Source: Freshsales (contributed by Tuan Nguyen)\nNew Source: Appsflyer (contributed by Achmad Syarif Hidayatullah)\nNew Source: Paystack (contributed by Foluso Ogunlana)\nNew Source: Sentry (contributed by koji matsumoto)\nNew Source: Retently (contributed by Subhash Gopalakrishnan)\nNew Source: Delighted! (contributed by Rodrigo Parra)\n\nwith 18 more currently in review...\n\ud83c\udf89 Incremental Normalization is here! \ud83c\udf89\n\ud83d\udc8e Basic normalization no longer runs on already normalized data, making it way faster and cheaper. :gem:\n\ud83c\udf89 Airbyte Compiles on M1 Macs!\nAirbyte developers with M1 chips in their MacBooks can now compile the project and run the server. This is a major step towards being able to fully run Airbyte on M1. (contributed by Harsha Teja Kanna)\n\n\u2728 BigQuery Destination: You can now run transformations in batches, preventing queries from hitting BigQuery limits. (contributed by Andr\u00e9s Bravo)\n\u2728 S3 Source: Memory and Performance optimizations, also some fancy new PyArrow CSV configuration options.\n\u2728 Zuora Source: Now supports Unlimited as an option for the Data Query Live API.\n\n\u2728 Clickhouse Source: Now supports SSL and connection via SSH tunneling.\n\n\n\ud83d\udc1b Oracle Source: Now handles the LONG RAW data type correctly.\n\n\ud83d\udc1b Snowflake Source: Fixed parsing of extreme values for FLOAT and NUMBER data types.\n\ud83d\udc1b Hubspot Source: No longer fails due to lengthy URI/URLs.\n\ud83d\udc1b Zendesk Source: The chats stream now pulls data past the first page.\n\ud83d\udc1b Jira Source: Normalization now works as expected.\n\nAs usual, thank you to our awesome contributors that have done awesome work during this productive spooky season: Tuan Nguyen, Achmad Syarif Hidayatullah, Christopher Wu, Andr\u00e9s Bravo, Harsha Teja Kanna, Collin Scangarella, haoranyu, koji matsumoto, Subhash Gopalakrishnan, Jeremy Branham, Rodrigo Parra, Foluso Ogunlana, EdBizarro, Gergely Lendvai, Rodeoclash, terencecho, Igli Koxha, Alasdair Brown, bbugh, Pras, Bo, Xiangxuan Liu, Hai To, s-mawjee, Mario Molina, SamyPesse, Yuhui Shi, Maciej N\u0119dza, Matt Hoag, and denis-sokolov!\n10/20/2021 Summary\nIt's patch notes time! Let's go over changes from 0.30.16! But before we do... I want to remind everyone that Airbyte Hacktoberfest is currently taking place! For every connector that is merged into our codebase, you'll get $500, so make sure to submit before the hackathon ends on November 19th.\n\n\ud83c\udf89 New Source: WooCommerce (contributed by James Wilson)\n\n\ud83c\udf89 K8s deployments: Worker image pull policy is now configurable (contributed by Mario Molina)\n\n\n\u2728 MSSQL destination: Now supports basic normalization\n\n\ud83d\udc1b LinkedIn Ads source: Analytics streams now work as expected.\n\nWe've had a lot of contributors over the last few weeks, so I'd like to thank all of them for their efforts: James Wilson, Mario Molina, Maciej N\u0119dza, Pras, Tuan Nguyen, Andr\u00e9s Bravo, Christopher Wu, gunu, Harsha Teja Kanna, Jonathan Stacks, darian, Christian Gagnon, Nicolas Moreau, Matt Hoag, Achmad Syarif Hidayatullah, s-mawjee, SamyPesse, heade, zurferr, denis-solokov, and aristidednd!\n09/29/2021 Summary\nIt's patch notes time, let's go over the changes from our new minor version, v0.30.0. As usual, bug fixes are in the thread.\n\nNew source: LinkedIn Ads\nNew source: Kafka\n\nNew source: Lever Hiring\n\n\n\ud83c\udf89 New License: Nothing changes for users of Airbyte/contributors. You just can't sell your own Airbyte Cloud!\n\n\n\ud83d\udc8e New API endpoint: You can now call connections/search in the web backend API to search sources and destinations. (contributed by Mario Molina)\n\n\ud83d\udc8e K8s: Added support for ImagePullSecrets for connector images.\n\n\ud83d\udc8e MSSQL, Oracle, MySQL sources & destinations: Now support connection via SSH (Bastion server)\n\n\n\u2728 MySQL destination: Now supports connection via TLS/SSL\n\n\u2728 BigQuery (denormalized) destination: Supports reading BigQuery types such as date by reading the format field (contributed by Nicolas Moreau)\n\u2728 Hubspot source: Added contacts associations to the deals stream.\n\u2728 GitHub source: Now supports pulling commits from user-specified branches.\n\u2728 Google Search Console source: Now accepts admin email as input when using a service account key.\n\u2728 Greenhouse source: Now identifies API streams it has access to if permissions are limited.\n\u2728 Marketo source: Now Airbyte native.\n\u2728 S3 source: Now supports any source that conforms to the S3 protocol (Non-AWS S3).\n\u2728 Shopify source: Now reports pre_tax_price on the line_items stream if you have Shopify Plus.\n\n\u2728 Stripe source: Now actually uses the mandatory start_date config field for incremental syncs.\n\n\n\ud83c\udfd7 Python CDK: Now supports passing custom headers to the requests in OAuth2, enabling token refresh calls.\n\n\ud83c\udfd7 Python CDK: Parent streams can now be configured to cache data for their child streams.\n\n\ud83c\udfd7 Python CDK: Now has a Transformer class that can cast record fields to the data type expected by the schema.\n\n\n\ud83d\udc1b Amplitude source: Fixed schema for date-time objects.\n\n\ud83d\udc1b Asana source: Schema fixed for the sections, stories, tasks, and users streams.\n\ud83d\udc1b GitHub source: Added error handling for streams not applicable to a repo. (contributed by Christopher Wu)\n\ud83d\udc1b Google Search Console source: Verifies access to sites when performing the connection check.\n\ud83d\udc1b Hubspot source: Now conforms to the V3 API, with streams such as owners reflecting the new fields.\n\ud83d\udc1b Intercom source: Fixed data type for the updated_at field. (contributed by Christian Gagnon)\n\ud83d\udc1b Iterable source: Normalization now works as expected.\n\ud83d\udc1b Pipedrive source: Schema now reflects the correct types for date/time fields.\n\ud83d\udc1b Stripe source: Incorrect timestamp formats removed for coupons and subscriptions streams.\n\ud83d\udc1b Salesforce source: You can now sync more than 10,000 records with the Bulk API.\n\ud83d\udc1b Snowflake destination: Now accepts any date-time format with normalization.\n\ud83d\udc1b Snowflake destination: Inserts are now split into batches to accommodate for large data loads.\n\nThank you to our awesome contributors. Y'all are amazing: Mario Molina, Pras, Vladimir Remar, Christopher Wu, gunu, Juliano Benvenuto Piovezan, Brian M, Justinas Lukasevicius, Jonathan Stacks, Christian Gagnon, Nicolas Moreau, aristidednd, camro, minimax75, peter-mcconnell, and sashkalife!\n09/16/2021 Summary\nNow let's get to the 0.29.19 changelog. As with last time, bug fixes are in the thread!\n\nNew Destination: Databricks \ud83c\udf89\nNew Source: Google Search Console\n\nNew Source: Close.com\n\n\n\ud83c\udfd7 Python CDK: Now supports auth workflows involving query params.\n\n\n\ud83c\udfd7 Java CDK: You can now run the connector gradle build script on Macs with M1 chips! (contributed by @Harsha Teja Kanna)\n\n\n\ud83d\udc8e Google Ads source: You can now specify user-specified queries in GAQL.\n\n\u2728 GitHub source: All streams with a parent stream use cached parent stream data when possible.\n\u2728 Shopify source: Substantial performance improvements to the incremental sync mode.\n\u2728 Stripe source: Now supports the PaymentIntents stream.\n\u2728 Pipedrive source: Now supports the Organizations stream.\n\u2728 Sendgrid source: Now supports the SingleSendStats stream.\n\u2728 Bing Ads source: Now supports the Report stream.\n\u2728 GitHub source: Now supports the Reactions stream.\n\u2728 MongoDB source: Now Airbyte native!\n\ud83d\udc1b Facebook Marketing source: Numeric values are no longer wrapped into strings.\n\ud83d\udc1b Facebook Marketing source: Fetching conversion data now works as expected. (contributed by @Manav)\n\ud83d\udc1b Keen destination: Timestamps are now parsed correctly.\n\ud83d\udc1b S3 destination: Parquet schema parsing errors are fixed.\n\ud83d\udc1b Snowflake destination: No longer syncs unnecessary tables with S3.\n\ud83d\udc1b SurveyMonkey source: Cached responses are now decoded correctly.\n\ud83d\udc1b Okta source: Incremental sync now works as expected.\n\nAlso, a quick shout out to Jinni Gu and their team who made the DynamoDB destination that we announced last week!\nAs usual, thank you to all of our contributors: Harsha Teja Kanna, Manav, Maciej N\u0119dza, mauro, Brian M, Iakov Salikov, Eliziario (Marcos Santos), coeurdestenebres, and mohammadbolt.\n09/09/2021 Summary\nWe're going over the changes from 0.29.17 and before... and there's a lot of big improvements here, so don't miss them!\nNew Source: Facebook Pages New Destination: MongoDB New Destination: DynamoDB\n\n\ud83c\udf89 You can now send notifications via webhook for successes and failures on Airbyte syncs. (This is a massive contribution by @Pras, thank you) \ud83c\udf89\n\ud83c\udf89 Scheduling jobs and worker jobs are now separated, allowing for workers to be scaled horizontally.\n\ud83c\udf89 When developing a connector, you can now preview what your spec looks like in real time with this process.\n\ud83c\udf89 Oracle destination: Now has basic normalization.\n\ud83c\udf89 Add XLSB (binary excel) support to the Files source (contributed by Muutech).\n\ud83c\udf89 You can now properly cancel K8s deployments.\n\u2728 S3 source: Support for Parquet format.\n\u2728 Github source: Branches, repositories, organization users, tags, and pull request stats streams added (contributed by @Christopher Wu).\n\u2728 BigQuery destination: Added GCS upload option.\n\u2728 Salesforce source: Now Airbyte native.\n\u2728 Redshift destination: Optimized for performance.\n\ud83c\udfd7 CDK: \ud83c\udf89 We\u2019ve released a tool to generate JSON Schemas from OpenAPI specs. This should make specifying schemas for API connectors a breeze! \ud83c\udf89\n\ud83c\udfd7 CDK: Source Acceptance Tests now verify that connectors correctly format strings which are declared as using date-time and date formats.\n\ud83c\udfd7 CDK: Add private options to help in testing: _limit and _page_size are now accepted by any CDK connector to minimze your output size for quick iteration while testing.\n\ud83d\udc1b Fixed a bug that made it possible for connector definitions to be duplicated, violating uniqueness.\n\ud83d\udc1b Pipedrive source: Output schemas no longer remove timestamp from fields.\n\ud83d\udc1b Github source: Empty repos and negative backoff values are now handled correctly.\n\ud83d\udc1b Harvest source: Normalization now works as expected.\n\ud83d\udc1b All CDC sources: Removed sleep logic which caused exceptions when loading data from high-volume sources.\n\ud83d\udc1b Slack source: Increased number of retries to tolerate flaky retry wait times on the API side.\n\ud83d\udc1b Slack source: Sync operations no longer hang indefinitely.\n\ud83d\udc1b Jira source: Now uses updated time as the cursor field for incremental sync instead of the created time.\n\ud83d\udc1b Intercom source: Fixed inconsistency between schema and output data.\n\ud83d\udc1b HubSpot source: Streams with the items property now have their schemas fixed.\n\ud83d\udc1b HubSpot source: Empty strings are no longer handled as dates, fixing the deals, companies, and contacts streams.\n\ud83d\udc1b Typeform source: Allows for multiple choices in responses now.\n\ud83d\udc1b Shopify source: The type for the amount field is now fixed in the schema.\n\ud83d\udc1b Postgres destination: \\u0000(NULL) value processing is now fixed.\n\nAs usual... thank you to our wonderful contributors this week: Pras, Christopher Wu, Brian M, yahu98, Michele Zuccala, jinnig, and luizgribeiro!\n09/01/2021 Summary\nGot the changes from 0.29.13... with some other surprises!\n\n\ud83d\udd25 There's a new way to create Airbyte sources! The team at Faros AI has created a Javascript/Typescript CDK which can be found here and in our docs here. This is absolutely awesome and give a huge thanks to Chalenge Masekera, Christopher Wu, eskrm, and Matthew Tovbin!\n\u2728 New Destination: Azure Blob Storage \u2728\n\nNew Source: Bamboo HR (contributed by @Oren Haliva) New Source: BigCommerce (contributed by @James Wilson) New Source: Trello New Source: Google Analytics V4 New Source: Amazon Ads\n\n\ud83d\udc8e Alpine Docker images are the new standard for Python connectors, so image sizes have dropped by around 100 MB!\n\u2728 You can now apply tolerations for Airbyte Pods on K8s deployments (contributed by @Pras).\n\ud83d\udc1b Shopify source: Rate limit throttling fixed.\n\ud83d\udcda We now have a doc on how to deploy Airbyte at scale. Check it out here!\n\ud83c\udfd7 Airbyte CDK: You can now ignore HTTP status errors and override retry parameters.\n\nAs usual, thank you to our awesome contributors: Oren Haliva, Pras, James Wilson, and Muutech.\n08/26/2021 Summary\nNew Source: Short.io (contributed by @Apostol Tegko)\n\n\ud83d\udc8e GitHub source: Added support for rotating through multiple API tokens!\n\u2728 Syncs are now scheduled with a 3 day timeout (contributed by @Vladimir Remar).\n\u2728 Google Ads source: Added UserLocationReport stream (contributed by @Max Krog).\n\u2728 Cart.com source: Added the order_items stream.\n\ud83d\udc1b Postgres source: Fixed out-of-memory issue with CDC interacting with large JSON blobs.\n\ud83d\udc1b Intercom source: Pagination now works as expected.\n\nAs always, thank you to our awesome community contributors this week: Apostol Tegko, Vladimir Remar, Max Krog, Pras, Marco Fontana, Troy Harvey, and damianlegawiec!\n08/20/2021 Summary\nHey Airbyte community, we got some patch notes for y'all. Here's all the changes we've pushed since the last update.\n\nNew Source: S3/Abstract Files\nNew Source: Zuora\nNew Source: Kustomer\nNew Source: Apify\nNew Source: Chargebee\nNew Source: Bing Ads\n\nNew Destination: Keen\n\n\u2728 Shopify source: The `status` property is now in the `Products` stream.\n\u2728 Amazon Seller Partner source: Added support for `GET_MERCHANT_LISTINGS_ALL_DATA` and `GET_FBA_INVENTORY_AGED_DATA` stream endpoints.\n\u2728 GitHub source: Existing streams now don't minify the user property.\n\u2728 HubSpot source: Updated user-defined custom field schema generation.\n\u2728 Zendesk source: Migrated from Singer to the Airbyte CDK.\n\u2728 Amazon Seller Partner source: Migrated to the Airbyte CDK.\n\ud83d\udc1b Shopify source: Fixed the `products` schema to be in accordance with the API.\n\ud83d\udc1b S3 source: Fixed bug where syncs could hang indefinitely.\n\nAnd as always... we'd love to shout out the awesome contributors that have helped push Airbyte forward. As a reminder, you can now see your contributions publicly reflected on our contributors page.\nThank you to Rodrigo Parra, Brian Krausz, Max Krog, Apostol Tegko, Matej Hamas, Vladimir Remar, Marco Fontana, Nicholas Bull, @mildbyte, @subhaklp, and Maciej N\u0119dza!\n07/30/2021 Summary\nFor this week's update, we got... a few new connectors this week in 0.29.0. We found that a lot of sources can pull data directly from the underlying db instance, which we naturally already supported.\n\nNew Source: PrestaShop \u2728\nNew Source: Snapchat Marketing \u2728\nNew Source: Drupal\nNew Source: Magento\nNew Source: Microsoft Dynamics AX\nNew Source: Microsoft Dynamics Customer Engagement\nNew Source: Microsoft Dynamics GP\nNew Source: Microsoft Dynamics NAV\nNew Source: Oracle PeopleSoft\nNew Source: Oracle Siebel CRM\nNew Source: SAP Business One\nNew Source: Spree Commerce\nNew Source: Sugar CRM\nNew Source: Wordpress\nNew Source: Zencart\n\ud83d\udc1b Shopify source: Fixed the products schema to be in accordance with the API\n\ud83d\udc1b BigQuery source: No longer fails with nested array data types.\n\nView the full release highlights here: Platform, Connectors\nAnd as always, thank you to our wonderful contributors: Madison Swain-Bowden, Brian Krausz, Apostol Tegko, Matej Hamas, Vladimir Remar, Oren Haliva, satishblotout, jacqueskpoty, wallies\n07/23/2021 Summary\nWhat's going on? We just released 0.28.0 and here's the main highlights.\n\nNew Destination: Google Cloud Storage \u2728\nNew Destination: Kafka \u2728 (contributed by @Mario Molina)\nNew Source: Pipedrive\nNew Source: US Census (contributed by @Daniel Mateus Pires (Earnest Research))\n\u2728 Google Ads source: Now supports Campaigns, Ads, AdGroups, and Accounts streams.\n\u2728 Stripe source: All subscription types (including expired and canceled ones) are now returned.\n\ud83d\udc1b Facebook source: Improved rate limit management\n\ud83d\udc1b Square source: The send_request method is no longer broken due to CDK changes\n\ud83d\udc1b MySQL destination: Does not fail on columns with JSON data now.\n\nView the full release highlights here: Platform, Connectors\nAnd as always, thank you to our wonderful contributors: Mario Molina, Daniel Mateus Pires (Earnest Research), gunu, Ankur Adhikari, Vladimir Remar, Madison Swain-Bowden, Maksym Pavlenok, Sam Crowder, mildbyte, avida, and gaart\n07/16/2021 Summary\nAs for our changes this week...\n\nNew Source: Zendesk Sunshine\nNew Source: Dixa\nNew Source: Typeform\n\ud83d\udc8e MySQL destination: Now supports normalization!  \n\ud83d\udc8e MSSQL source: Now supports CDC (Change Data Capture)\n\u2728 Snowflake destination: Data coming from Airbyte is now identifiable\n\ud83d\udc1b GitHub source: Now uses the correct cursor field for the IssueEvents stream\n\ud83d\udc1b Square source: The send_request method is no longer broken due to CDK changes\n\nView the full release highlights here: Platform, Connectors\nAs usual, thank you to our awesome community contributors this week: Oliver Meyer, Varun, Brian Krausz, shadabshaukat, Serhii Lazebnyi, Juliano Benvenuto Piovezan, mildbyte, and Sam Crowder!\n07/09/2021 Summary\n\nNew Source: PayPal Transaction\nNew Source: Square\nNew Source: SurveyMonkey\nNew Source: CockroachDB\nNew Source: Airbyte-Native GitHub\nNew Source: Airbyte-Native GitLab\nNew Source: Airbyte-Native Twilio\n\u2728 S3 destination: Now supports anyOf, oneOf and allOf schema fields.\n\u2728 Instagram source: Migrated to the CDK and has improved error handling.\n\u2728 Shopify source: Add support for draft orders.\n\u2728 K8s Deployments: Now support logging to GCS.\n\ud83d\udc1b GitHub source: Fixed issue with locked breaking normalization of the pull_request stream.\n\ud83d\udc1b Okta source: Fix endless loop when syncing data from logs stream.\n\ud83d\udc1b PostgreSQL source: Fixed decimal handling with CDC.\n\ud83d\udc1b Fixed random silent source failures.\n\ud83d\udcda New document on how the CDK handles schemas.\n\ud83c\udfd7\ufe0f Python CDK: Now allows setting of network adapter args on outgoing HTTP requests.\n\nView the full release highlights here: Platform, Connectors\nAs usual, thank you to our awesome community contributors this week: gunu, P.VAD, Rodrigo Parra, Mario Molina, Antonio Grass, sabifranjo, Jaime Farres, shadabshaukat, Rodrigo Menezes, dkelwa, Jonathan Duval, and Augustin Lafanech\u00e8re.\n07/01/2021 Summary\n\nNew Destination: Google PubSub\nNew Source: AWS CloudTrail\n\nThe risks and issues with upgrading Airbyte are now gone...\n\n\ud83c\udf89 Airbyte automatically upgrades versions safely at server startup \ud83c\udf89\n\ud83d\udc8e Logs on K8s are now stored in Minio by default, no S3 bucket required\n\u2728 Looker Source: Supports the Run Look output stream\n\u2728 Slack Source: is now Airbyte native!\n\ud83d\udc1b Freshdesk Source: No longer fails after 300 pages\n\ud83d\udcda New tutorial on building Java destinations\n\nStarting from next week, our weekly office hours will now become demo days! Drop by to get sneak peeks and new feature demos.\n\nWe added the #careers channel, so if you're hiring, post your job reqs there!\nWe added a #understanding-airbyte channel to mirror this section on our docs site. Ask any questions about our architecture or protocol there.\nWe added a #contributing-to-airbyte channel. A lot of people ask us about how to contribute to the project, so ask away there!\n\nView the full release highlights here: Platform, Connectors\nAs usual, thank you to our awesome community contributors this week: Harshith Mullapudi, Michael Irvine, and sabifranjo.\n06/24/2021 Summary\n\nNew Source: IBM Db2\n\ud83d\udc8e We now support Avro and JSONL output for our S3 destination! \ud83d\udc8e\n\ud83d\udc8e Brand new BigQuery destination flavor that now supports denormalized STRUCT types.\n\u2728 Looker source now supports self-hosted instances.\n\u2728 Facebook Marketing source is now migrated to the CDK, massively improving async job performance and error handling.\n\nView the full connector release notes here.\nAs usual, thank you to some of our awesome community contributors this week: Harshith Mullapudi, Tyler DeLange, Daniel Mateus Pires, EdBizarro, Tyler Schroeder, and Konrad Schlatte!\n06/18/2021 Summary\n\nNew Source: Snowflake\n\ud83d\udc8e We now support custom dbt transformations! \ud83d\udc8e\n\u2728 We now support configuring your destination namespace at the table level when setting up a connection!\n\u2728 The S3 destination now supports Minio S3 and Parquet output!\n\nView the full release notes here: Platform, Connectors\nAs usual, thank you to some of our awesome community contributors this week: Tyler DeLange, Mario Molina, Rodrigo Parra, Prashanth Patali, Christopher Wu, Itai Admi, Fred Reimer, and Konrad Schlatte!\n06/10/2021 Summary\n\nNew Destination: S3!! \nNew Sources: Harvest, Amplitude, Posthog\n\ud83d\udc1b Ensure that logs from threads created by replication workers are added to the log file.\n\ud83d\udc1b Handle TINYINT(1) and BOOLEAN correctly and fix target file comparison for MySQL CDC.\nJira source: now supports all available entities in Jira Cloud.\n\ud83d\udcda Added a troubleshooting section, a gradle cheatsheet, a reminder on what the reset button does, and a refresh on our docs best practices.\n\nConnector Development:\n\nContainerized connector code generator\nAdded JDBC source connector bootstrap template.\nAdded Java destination generator.\n\nView the full release notes highlights here: Platform, Connectors\nAs usual, thank you to some of our awesome community contributors this week (I've noticed that we've had more contributors to our docs, which we really appreciate). Ping, Harshith Mullapudi, Michael Irvine, Matheus di Paula, jacqueskpoty and P.VAD.\nOverview\nAirbyte is comprised of 2 parts:\n\nPlatform (The scheduler, workers, api, web app, and the Airbyte protocol). Here is the changelog for Platform. \nConnectors that run in Docker containers. Here is the changelog for the connectors. \n\nAirbyte Platform Releases\nProduction v. Dev Releases\nThe \"production\" version of Airbyte is the version of the app specified in `.env`. With each production release, we update the version in the `.env` file. This version will always be available for download on DockerHub. It is the version of the app that runs when a user runs `docker compose up`.\nThe \"development\" version of Airbyte is the head of master branch. It is the version of the app that runs when a user runs `./gradlew build && \nVERSION=dev docker compose up`.\nProduction Release Schedule\nScheduled Releases\nAirbyte currently releases a new minor version of the application on a weekly basis. Generally this weekly release happens on Monday or Tuesday.\nHotfixes\nAirbyte releases a new version whenever it discovers and fixes a bug that blocks any mission critical functionality.\nMission Critical\ne.g. Non-ASCII characters break the Salesforce source.\nNon-Mission Critical\ne.g. Buttons in the UI are offset.\nUnscheduled Releases\nWe will often release more frequently than the weekly cadence if we complete a feature that we know that a user is waiting on.\nDevelopment Release Schedule\nAs soon as a feature is on master, it is part of the development version of Airbyte. We merge features as soon as they are ready to go (have been code reviewed and tested). We attempt to keep the development version of the app working all the time. We are iterating quickly, however, and there may be intermittent periods where the development version is broken.\nIf there is ever a feature that is only on the development version, and you need it on the production version, please let us know. We are very happy to do ad-hoc production releases if it unblocks a specific need for one of our users.\nAirbyte Connector Releases\nEach connector is tracked with its own version. These versions are separate from the versions of Airbyte Platform. We generally will bump the version of a connector anytime we make a change to it. We rely on a large suite of tests to make sure that these changes do not cause regressions in our connectors.\nWhen we updated the version of a connector, we usually update the connector's version in Airbyte Platform as well. Keep in mind that you might not see the updated version of that connector in the production version of Airbyte Platform until after a production release of Airbyte Platform.",
    "tag": "airbyte"
  },
  {
    "title": "Connectors",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/archive/changelog/connectors.md",
    "content": "\ndescription: Do not miss the new connectors we support!\nConnectors\nYou can request new connectors directly here.\nNote: Airbyte is not built on top of Singer but is compatible with Singer's protocol. Airbyte's ambitions go beyond what Singer enables us to do, so we are building our own protocol that maintains compatibility with Singer's protocol.\nCheck out our connector roadmap to see what we're currently working on.\n1/28/2022\nNew sources:\n\nChartmogul\nHellobaton\nFlexport\nPersistIq\n\n1/6/2022\nNew sources:\n\n3PL Central\nMy Hours\nQualaroo\nSearchMetrics\n\n12/16/2021\nNew source:\n\nOpenWeather\n\nNew destinations:\n\nClickHouse\nRabbitMQ\nAmazon SQS\nRockset\n\n12/9/2021\nNew source:\n\nMailgun\n\n12/2/2021\nNew destinations:\n\nRedis\nMQTT\nGoogle Firestore\nKinesis\n\n11/25/2021\nNew sources:\n\nAirtable\nNotion\nPardot\nNotion\nYouTube Analytics\n\nNew features:\n\nExchange Rates Source: add `ignore_weekends` option.\nFacebook Source: add the videos stream.\nFreshdesk Source: removed the limitation in streams pagination.\nJira Source: add option to render fields in HTML format.\nMongoDB v2 Source: improve read performance.\nPipedrive Source: specify schema for \"persons\" stream.\nPostgreSQL Source: exclude tables on which user doesn't have select privileges.\nSurveyMonkey Source: improve connection check.\n\n11/17/2021\nNew destination:\n\nScyllaDB\n\nNew sources:\n\nAzure Table Storage\nLinnworks\n\nNew features:\n\nMySQL Source: Now has basic performance tests.\nSalesforce Source: We now automatically transform and handle incorrect data for the anyType and calculated types.\n\n11/11/2021\nNew destinations:\n\nCassandra\nPulsar\n\nNew sources:\n\nConfluence\nMonday\nCommerce Tools\nPinterest\n\nNew features:\n\nShopify Source: Now supports the FulfillmentOrders and Fulfillments streams.\nGreenhouse Source: Now supports the Demographics stream.\nRecharge Source: Broken requests should now be re-requested with improved backoff.\nStripe Source: Now supports the checkout_sessions, checkout_sessions_line_item, and promotion_codes streams.\nDb2 Source: Now supports SSL.\n\n11/3/2021\nNew destination:\n\nElasticsearch\n\nNew sources:\n\nSalesloft\nOneSignal\nStrava\nLemlist\nAmazon SQS\nFreshservices\nFreshsales\nAppsflyer\nPaystack\nSentry\nRetently\nDelighted!\n\nNew features:\n\nBigQuery Destination: You can now run transformations in batches, preventing queries from hitting BigQuery limits. (contributed by @Andr\u00e9s Bravo)\nS3 Source: Memory and Performance optimizations, also some fancy new PyArrow CSV configuration options.\nZuora Source: Now supports Unlimited as an option for the Data Query Live API.\nClickhouse Source: Now supports SSL and connection via SSH tunneling.\n\n10/20/2021\nNew source:\n\nWooCommerce\n\nNew feature:\n\nMSSQL destination: Now supports basic normalization\n\n9/29/2021\nNew sources:\n\nLinkedIn Ads\nKafka\nLever Hiring\n\nNew features:\n\nMySQL destination: Now supports connection via TLS/SSL\nBigQuery (denormalized) destination: Supports reading BigQuery types such as date by reading the format field (contributed by @Nicolas Moreau)\nHubspot source: Added contacts associations to the deals stream.\nGitHub source: Now supports pulling commits from user-specified branches.\nGoogle Search Console source: Now accepts admin email as input when using a service account key.\nGreenhouse source: Now identifies API streams it has access to if permissions are limited.\nMarketo source: Now Airbyte native.\nS3 source: Now supports any source that conforms to the S3 protocol (Non-AWS S3).\nShopify source: Now reports pre_tax_price on the line_items stream if you have Shopify Plus.\nStripe source: Now actually uses the mandatory start_date config field for incremental syncs.\n\n9/16/2021\nNew destinations:\n\nDatabricks\n\nNew sources:\n\nClose.com\nGoogle Search Console\n\nNew features:\n\nGoogle Ads source: You can now specify user-specified queries in GAQL.\nGitHub source: All streams with a parent stream use cached parent stream data when possible.\nShopify source: Substantial performance improvements to the incremental sync mode.\nStripe source: Now supports the PaymentIntents stream.\nPipedrive source: Now supports the Organizations stream.\nSendgrid source: Now supports the SingleSendStats stream.\nBing Ads source: Now supports the Report stream.\nGitHub source: Now supports the Reactions stream.\nMongoDB source: Now Airbyte native!\n\n9/9/2021\nNew source:\n\nFacebook Pages\n\nNew destinations:\n\nMongoDB\nDynamoDB\n\nNew features:\n\nS3 source: Support for Parquet format.\nGithub source: Branches, repositories, organization users, tags, and pull request stats streams added (contributed by @Christopher Wu).\nBigQuery destination: Added GCS upload option.\nSalesforce source: Now Airbyte native.\nRedshift destination: Optimized for performance.\n\nBug fixes:\n\nPipedrive source: Output schemas no longer remove timestamp from fields.\nGithub source: Empty repos and negative backoff values are now handled correctly.\nHarvest source: Normalization now works as expected.\nAll CDC sources: Removed sleep logic which caused exceptions when loading data from high-volume sources.\nSlack source: Increased number of retries to tolerate flaky retry wait times on the API side.\nSlack source: Sync operations no longer hang indefinitely.\nJira source: Now uses updated time as the cursor field for incremental sync instead of the created time.\nIntercom source: Fixed inconsistency between schema and output data.\nHubSpot source: Streams with the items property now have their schemas fixed.\nHubSpot source: Empty strings are no longer handled as dates, fixing the deals, companies, and contacts streams.\nTypeform source: Allows for multiple choices in responses now.\nShopify source: The type for the amount field is now fixed in the schema.\nPostgres destination: \\u0000(NULL) value processing is now fixed.\n\n9/1/2021\nNew sources:\n\nBamboo HR\nBigCommerce\nTrello\nGoogle Analytics V4\nAmazon Ads\n\nBug fixes:\n\nShopify source: Rate limit throttling fixed.\n\n8/26/2021\nNew source:\n\nShort.io\n\nNew features:\n\nGitHub source: Add support for rotating through multiple API tokens.\nGoogle Ads source: Added `UserLocationReport` stream.\nCart.com source: Added the `order_items` stream.\n\nBug fixes:\n\nPostgres source: Fix out-of-memory issue with CDC interacting with large JSON blobs.\nIntercom source: Pagination now works as expected.\n\n8/18/2021\nNew source:\n\nBing Ads\n\nNew destination:\n\nKeen\n\nNew features:\n\nChargebee source: Adds support for the `items`, `item prices` and `attached items` endpoints.\n\nBug fixes:\n\nQuickBooks source: Now uses the number data type for decimal fields.\nHubSpot source: Fixed `empty string` inside of the `number` and `float` datatypes.\nGitHub source: Validation fixed on non-required fields.\nBigQuery destination: Now supports processing of arrays of records properly.\nOracle destination: Fixed destination check for users without DBA role.\n\n8/9/2021\nNew sources:\n\nS3/Abstract Files\nZuora\nKustomer\nApify\nChargebee\n\nNew features:\n\nShopify source: The `status` property is now in the `Products` stream.\nAmazon Seller Partner source: Added support for `GET_MERCHANT_LISTINGS_ALL_DATA` and `GET_FBA_INVENTORY_AGED_DATA` stream endpoints.\nGitHub source: Existing streams now don't minify the `user` property.\nHubSpot source: Updated user-defined custom field schema generation.\nZendesk source: Migrated from Singer to the Airbyte CDK.\nAmazon Seller Partner source: Migrated to the Airbyte CDK.\n\nBug fixes:\n\nHubSpot source: Casting exceptions are now logged correctly.\nS3 source: Fixed bug where syncs could hang indefinitely.\nShopify source: Fixed the `products` schema to be in accordance with the API.\nPayPal Transactions source: Fixed the start date minimum to be 3 years rather than 45 days.\nGoogle Ads source: Added the `login-customer-id` setting.\nIntercom source: Rate limit corrected from 1000 requests/minute from 1000 requests/hour.\nS3 source: Fixed bug in spec to properly display the `format` field in the UI.\n\nNew CDK features:\n\nNow allows for setting request data in non-JSON formats.\n\n7/30/2021\nNew sources:\n\nPrestaShop\nSnapchat Marketing\nDrupal\nMagento\nMicrosoft Dynamics AX\nMicrosoft Dynamics Customer Engagement\nMicrosoft Dynamics GP\nMicrosoft Dynamics NAV\nOracle PeopleSoft\nOracle Siebel CRM\nSAP Business One\nSpree Commerce\nSugar CRM\nWooCommerce\nWordpress\nZencart\n\nBug fixes:\n\nShopify source: Fixed the `products` schema to be in accordance with the API.\nBigQuery source: No longer fails with `Array of Records` data types.\nBigQuery destination: Improved logging, Job IDs are now filled with location and Project IDs.\n\n7/23/2021\nNew sources:\n\nPipedrive\nUS Census\nBigQuery\n\nNew destinations:\n\nGoogle Cloud Storage\nKafka\n\nNew Features:\n\nJava Connectors: Now have config validators for check, discover, read, and write calls\nStripe source: All subscription types are returnable (including expired and canceled ones).\nMixpanel source: Migrated to the CDK.\nIntercom source: Migrated to the CDK.\nGoogle Ads source: Now supports the `Campaigns`, `Ads`, `AdGroups`, and `Accounts` streams.\n\nBug Fixes:\n\nFacebook source: Improved rate limit management\nInstagram source: Now supports old format for state and automatically updates it to the new format.\nSendgrid source: Now gracefully handles malformed responses from API.\nJira source: Fixed dbt failing to normalize schema for the labels stream.\nMySQL destination: Does not fail anymore with columns that contain JSON data.\nSlack source: Now does not fail stream slicing on reading threads.\n\n7/16/2021\n3 new sources:\n\nZendesk Sunshine\nDixa\nTypeform\n\nNew Features:\n\nMySQL destination: Now supports normalization!\nMSSQL source: Now supports CDC (Change Data Capture).\nSnowflake destination: Data coming from Airbyte is now identifiable.\nGitHub source: Now handles rate limiting.\n\nBug Fixes:\n\nGitHub source: Now uses the correct cursor field for the `IssueEvents` stream.\nSquare source: `send_request` method is no longer broken.\n\n7/08/2021\n7 new sources:\n\nPayPal Transaction\nSquare\nSurveyMonkey\nCockroachDB\nAirbyte-native GitLab\nAirbyte-native GitHub\nAirbyte-native Twilio\n\nNew Features:\n\nS3 destination: Now supports `anyOf`, `oneOf` and `allOf` schema fields.\nInstagram source: Migrated to the CDK and has improved error handling.\nSnowflake source: Now has comprehensive data type tests.\nShopify source: Change the default stream cursor field to `update_at` where possible.\nShopify source: Add support for draft orders.\nMySQL destination: Now supports normalization.\n\nConnector Development:\n\nPython CDK: Now allows setting of network adapter args on outgoing HTTP requests.\nAbstract classes for non-JDBC relational database sources.\n\nBugfixes:\n\nGitHub source: Fixed issue with `locked` breaking normalization of the pull_request stream.\nPostgreSQL source: Fixed decimal handling with CDC.\nOkta source: Fix endless loop when syncing data from logs stream.\n\n7/01/2021\nBugfixes:\n\nLooker source: Now supports the Run Look stream.\nGoogle Adwords: CI is fixed and new version is published.\nSlack source: Now Airbyte native and supports channels, channel members, messages, users, and threads streams.\nFreshdesk source: Does not fail after 300 pages anymore.\nMSSQL source: Now has comprehensive data type tests.\n\n6/24/2021\n1 new source:\n\nDb2\n\nNew features:\n\nS3 destination: supports Avro and Jsonl output!\nBigQuery destination: now supports loading JSON data as structured data.\nLooker source: Now supports self-hosted instances.\nFacebook source: is now migrated to the CDK.\n\n6/18/2021\n1 new source:\n\nSnowflake\n\nNew features:\n\nPostgres source: now has comprehensive data type tests.\nGoogle Ads source: now uses the Google Ads Query Language!\nS3 destination: supports Parquet output!\nS3 destination: supports Minio S3!\nBigQuery destination: credentials are now optional.\n\n6/10/2021\n1 new destination:\n\nS3\n\n3 new sources:\n\nHarvest\nAmplitude\nPosthog\n\nNew features:\n\nJira source: now supports all available entities in Jira Cloud.\nExchangeRatesAPI source: clearer messages around unsupported currencies.\nMySQL source: Comprehensive core extension to be more compatible with other JDBC sources.\nBigQuery destination: Add dataset location.\nShopify source: Add order risks + new attributes to orders schema for native connector\n\nBugfixes:\n\nMSSQL destination: fixed handling of unicode symbols.\n\nConnector development updates:\n\nContainerized connector code generator.\nAdded JDBC source connector bootstrap template.\nAdded Java destination generator.\n\n06/3/2021\n2 new sources:\n\nOkta\nAmazon Seller Partner\n\nNew features:\n\nMySQL CDC now only polls for 5 minutes if we haven't received any records (#3789)\nPython CDK now supports Python 3.7.X (#3692)\nFile source: now supports Azure Blob Storage (#3660)\n\nBugfixes:\n\nRecurly source: now uses type `number` instead of `integer` (#3769)\nStripe source: fix types in schema (#3744)\nStripe source: output `number` instead of `int` (#3728)\nMSSQL destination: fix issue with unicode symbols handling (#3671)\n\n05/25/2021\n4 new sources:\n\nAsana\nKlaviyo\nRecharge\nTempo\n\nProgress on connectors:\n\nCDC for MySQL is now available!\nSendgrid source: support incremental sync, as rewritten using HTTP CDK (#3445)\nGithub source bugfix: exception when parsing null date values, use `created_at` as cursor value for issue_milestones (#3314)\nSlack source bugfix: don't overwrite thread_ts in threads stream (#3483)\nFacebook Marketing source: allow configuring insights lookback window (#3396)\nFreshdesk source: fix discovery (#3591)\n\n05/18/2021\n1 new destination: MSSQL\n1 new source: ClickHouse\nProgress on connectors:\n\nShopify: make this source more resilient to timeouts (#3409)\nFreshdesk bugfix: output correct schema for various streams (#3376)\nIterable: update to use latest version of CDK (#3378)\n\n05/11/2021\n1 new destination: MySQL\n2 new sources:\n\nGoogle Search Console\nPokeAPI (talking about long tail and having fun ;))\n\nProgress on connectors:\n\nZoom: bugfix on declaring correct types to match data coming from API (#3159), thanks to vovavovavovavova\nSmartsheets: bugfix on gracefully handling empty cell values (#3337), thanks to Nathan Nowack\nStripe: fix date property name, only add connected account header when set, and set primary key (#3210), thanks to Nathan Yergler\n\n05/04/2021\n2 new sources:\n\nSmartsheets, thanks to Nathan Nowack\nZendesk Chat\n\nProgress on connectors:\n\nAppstore: bugfix private key handling in the UI (#3201)\nFacebook marketing: Wait longer (5 min) for async jobs to start (#3116), thanks to Max Krog\nStripe: support reading data from connected accounts (#3121), and 2 new streams with Refunds & Bank Accounts (#3030) (#3086)\nRedshift destination: Ignore records that are too big (instead of failing) (#2988)\nMongoDB: add supporting TLS and Replica Sets (#3111)\nHTTP sources: bugfix on handling array responses gracefully (#3008)\n\n04/27/2021\n\nZendesk Talk: fix normalization failure (#3022), thanks to yevhenii-ldv\nGithub: pull_requests stream only incremental syncs (#2886) (#3009), thanks to Zirochkaa\nCreate streaming writes to a file and manage the issuance of copy commands for the destination (#2921)\nRedshift: make Redshift part size configurable. (#3053)\nHubSpot: fix argument error in log call (#3087) (#3087) , thanks to Nathan Yergler\n\n04/20/2021\n3 new source connectors!\n\nZendesk Talk\nIterable\nQuickBooks\n\nOther progress on connectors:\n\nPostgres source/destination: add SSL option, thanks to Marcos Marx (#2757)\nGoogle sheets bugfix: handle duplicate sheet headers, thanks to Aneesh Makala (#2905)\nSource Google Adwords: support specifying the lookback window for conversions, thanks to Harshith Mullapudi (#2918)\nMongoDB improvement: speed up mongodb schema discovery, thanks to Yury Koleda (#2851)\nMySQL bugfix: parsing Mysql jdbc params, thanks to Vasily Safronov (#2891)\nCSV bugfix: discovery takes too much memory (#2089)\nA lot of work was done on improving the standard tests for the connectors, for better standardization and maintenance!\n\n04/13/2021\n\nNew connector: Oracle DB, thanks to Marcos Marx\n\n04/07/2021\n\nNew connector: Google Workspace Admin Reports (audit logs)\nBugfix in the base python connector library that caused errors to be silently skipped rather than failing the sync\nExchangeratesapi.io bugfix: to point to the updated API URL\nRedshift destination bugfix: quote keywords \u201cDATETIME\u201d and \u201cTIME\u201d when used as identifiers\nGitHub bugfix: syncs failing when a personal repository doesn\u2019t contain collaborators or team streams available\nMixpanel connector: sync at most the last 90 days of data in the annotations stream to adhere to API limits\n\n03/29/2021\n\nWe started measuring throughput of connectors. This will help us improve that point for all connectors.\nRedshift: implemented Copy strategy to improve its throughput.\nInstagram: bugfix an issue which caused media and media_insights streams to stop syncing prematurely.\nSupport NCHAR and NVCHAR types in SQL-based database sources.\nAdd the ability to specify a custom JDBC parameters for the MySQL source connector.\n\n03/22/2021\n\n2 new source connectors: Gitlab and Airbyte-native HubSpot\nDeveloping connectors now requires almost no interaction with Gradle, Airbyte\u2019s monorepo build tool. If you\u2019re building a Python connector, you never have to worry about developing outside your typical flow. See the updated documentation.\n\n03/15/2021\n\n2 new source connectors: Instagram and Google Directory\nFacebook Marketing: support of API v10\nGoogle Analytics: support incremental sync\nJira: bug fix to consistently pull all tickets\nHTTP Source: bug fix to correctly parse JSON responses consistently\n\n03/08/2021\n\n1 new source connector: MongoDB\nGoogle Analytics: Support chunked syncs to avoid sampling\nAppStore: fix bug where the catalog was displayed incorrectly\n\n03/01/2021\n\nNew native HubSpot connector with schema folder populated\nFacebook Marketing connector: add option to include deleted records\n\n02/22/2021\n\nBug fixes:\nGoogle Analytics: add the ability to sync custom reports\nApple Appstore: bug fix to correctly run incremental syncs\nExchange rates: UI now correctly validates input date pattern\nFile Source: Support JSONL (newline-delimited JSON) format\nFreshdesk: Enable controlling how many requests per minute the connector makes to avoid overclocking rate limits\n\n02/15/2021\n\n1 new destination connector: MeiliSearch\n2 new sources that support incremental append: Freshdesk and Sendgrid\nOther fixes:\nThanks to @ns-admetrics for contributing an upgrade to the Shopify source connector which now provides the landing_site field containing UTM parameters in the Orders table.\nSendgrid source connector supports most available endpoints available in the API\nFacebook Source connector now supports syncing Ad Insights data\nFreshdesk source connector now supports syncing satisfaction ratings and conversations\nMicrosoft Teams source connector now gracefully handles rate limiting\nBug fix in Slack source where the last few records in a sync were sporadically dropped\nBug fix in Google Analytics source where the last few records in sync were sporadically dropped\nIn Redshift source, support non alpha-numeric table names\nBug fix in Github Source to fix instances where syncs didn\u2019t always fail if there was an error while reading data from the API\n\n02/02/2021\n\nSources that we improved reliability for (and that became \u201ccertified\u201d):\nCertified sources: Files and Shopify\nEnhanced continuous testing for Tempo and Looker sources\nOther fixes / features:\nCorrectly handle boolean types in the File Source\nAdd docs for App Store source\nFix a bug in Snowflake destination where the connector didn\u2019t check for all needed write permissions, causing some syncs to fail\n\n01/26/2021\n\nImproved reliability with our best practices on : Google Sheets, Google Ads, Marketo, Tempo\nSupport incremental for Facebook and Google Ads\nThe Facebook connector now supports the FB marketing API v9\n\n01/19/2021\n\nOur new Connector Health Grade page\n1 new source: App Store (thanks to @Muriloo)\nFixes on connectors:\nBug fix writing boolean columns to Redshift\nBug fix where getting a connector\u2019s input configuration hung indefinitely\nStripe connector now gracefully handles rate limiting from the Stripe API\n\n01/12/2021\n\n1 new source: Tempo (thanks to @thomasvl)\nIncremental support for 3 new source connectors: Salesforce, Slack and Braintree\nFixes on connectors:\nFix a bug in MSSQL and Redshift source connectors where custom SQL types weren't being handled correctly.\nImprovement of the Snowflake connector from @hudsondba (batch size and timeout sync)\n\n01/05/2021\n\nIncremental support for 2 new source connectors: Mixpanel and HubSpot\nFixes on connectors:\nFixed a bug in the github connector where the connector didn\u2019t verify the provided API token was granted the correct permissions\nFixed a bug in the Google sheets connector where rate limits were not always respected\nAlpha version of Facebook marketing API v9. This connector is a native Airbyte connector (current is Singer based).\n\n12/30/2020\nNew sources: Plaid (contributed by tgiardina), Looker\n12/18/2020\nNew sources: Drift, Microsoft Teams\n12/10/2020\nNew sources: Intercom, Mixpanel, Jira Cloud, Zoom\n12/07/2020\nNew sources: Slack, Braintree, Zendesk Support\n12/04/2020\nNew sources: Redshift, Greenhouse New destination: Redshift\n11/30/2020\nNew sources: Freshdesk, Twilio\n11/25/2020\nNew source: Recurly\n11/23/2020\nNew source: Sendgrid\n11/18/2020\nNew source: Mailchimp\n11/13/2020\nNew source: MSSQL\n11/11/2020\nNew source: Shopify\n11/09/2020\nNew sources: Files (CSV, JSON, HTML...)\n11/04/2020\nNew sources: Facebook Ads, Google Ads, Marketo New destination: Snowflake\n10/30/2020\nNew sources: Salesforce, Google Analytics, HubSpot, GitHub, Google Sheets, Rest APIs, and MySQL\n10/21/2020\nNew destinations: we built our own connectors for BigQuery and Postgres, to ensure they are of the highest quality.\n09/23/2020",
    "tag": "airbyte"
  },
  {
    "title": "Deploying Airbyte on a Non-Standard Operating System",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/archive/faq/deploying-on-other-os.md",
    "content": "Deploying Airbyte on a Non-Standard Operating System\nCentOS 8\nFrom clean install:\n`firewall-cmd --zone=public --add-port=8000/tcp --permanent\nfirewall-cmd --zone=public --add-port=8001/tcp --permanent\nfirewall-cmd --zone=public --add-port=7233/tcp --permanent\nsystemctl restart firewalld`\nOR... if you prefer iptables:\n`iptables -A INPUT -p tcp -m tcp --dport 8000 -j ACCEPT\niptables -A INPUT -p tcp -m tcp --dport 8001 -j ACCEPT\niptables -A INPUT -p tcp -m tcp --dport 7233 -j ACCEPT\nsystemctl restart iptables`\nSetup the docker repo:\n`dnf config-manager --add-repo=https://download.docker.com/linux/centos/docker-ce.repo`\ndnf install docker-ce --nobest\nsystemctl enable --now docker\nusermod -aG docker $USER`\nYou'll need to get docker-compose separately.\n`dnf install wget git curl\ncurl -L https://github.com/docker/compose/releases/download/1.25.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose\nchmod +x /usr/local/bin/docker-compose`\nNow we can install Airbyte. In this example, we will install it under `/opt/`\n```\ncd /opt\ngit clone https://github.com/airbytehq/airbyte.git\ncd airbyte\ndocker-compose up\ndocker-compose ps",
    "tag": "airbyte"
  },
  {
    "title": "Transformation and Schemas",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/archive/faq/transformation-and-schemas.md",
    "content": "Transformation and Schemas\nWhere's the T in Airbyte\u2019s ETL tool?\nAirbyte is actually an ELT tool, and you have the freedom to use it as an EL-only tool. The transformation part is done by default, but it is optional. You can choose to receive the data in raw (JSON file for instance) in your destination.\nWe do provide normalization (if option is still on) so that data analysts / scientists / any users of the data can use it without much effort.\nWe also intend to integrate deeply with dbt to make it easier for your team to continue relying you on them, if this was what you were doing.\nHow does Airbyte handle replication when a data source changes its schema?\nAirbyte continues to sync data using the configured schema until that schema is updated. Because Airbyte treats all fields as optional, if a field is renamed or deleted in the source, that field simply will no longer be replicated, but all remaining fields will. The same is true for streams as well.\nFor now, the schema can only be updated manually in the UI (by clicking \"Update Schema\" in the settings page for the connection). When a schema is updated Airbyte will re-sync all data for that source using the new schema.\nHow does Airbyte handle namespaces (or schemas for the DB-inclined)?\nAirbyte respects source-defined namespaces when syncing data with a namespace-supported destination. See this for more details.",
    "tag": "airbyte"
  },
  {
    "title": "Getting Started",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/archive/faq/getting-started.md",
    "content": "Getting Started\nWhat do I need to get started using Airbyte?\nYou can deploy Airbyte in several ways, as documented here. Airbyte will then help you replicate data between a source and a destination. If you don\u2019t see the connector you need, you can build your connector yourself and benefit from Airbyte\u2019s optional scheduling, orchestration and monitoring modules.\nHow long does it take to set up Airbyte?\nIt depends on your source and destination. Check our setup guides to see the tasks for your source and destination. Each source and destination also has a list of prerequisites for setup. To make setup faster, get your prerequisites ready before you start to set up your connector. During the setup process, you may need to contact others (like a database administrator or AWS account owner) for help, which might slow you down. But if you have access to the connection information, it can take 2 minutes: see demo video. \nWhat data sources does Airbyte offer connectors for?\nWe already offer 100+ connectors, and will focus all our effort in ramping up the number of connectors and strengthening them. If you don\u2019t see a source you need, you can file a connector request here.\nWhere can I see my data in Airbyte?\nYou can\u2019t see your data in Airbyte, because we don\u2019t store it. The sync loads your data into your destination (data warehouse, data lake, etc.). While you can\u2019t see your data directly in Airbyte, you can check your schema and sync status on the source detail page in Airbyte.\nCan I add multiple destinations?\nSure, you can. Just go to the \"Destinations\" section and click on the top right \"+ new destination\" button. You can have multiple destinations for the same source, and multiple sources for the same destination.\nAm I limited to GUI interaction or is there a way to set up / run / interact with Airbyte programmatically?\nYou can use the API to do anything you do today from the UI. Though, word of notice, the API is in alpha and may change. You won\u2019t lose any functionality, but you may need to update your code to catch up to any backwards incompatible changes in the API.\nHow does Airbyte handle connecting to databases that are behind a firewall / NAT?\nWe don\u2019t. Airbyte is to be self-hosted in your own private cloud.\nCan I set a start time for my integration?\nHere is the link to the docs on scheduling syncs.\nCan I disable analytics in Airbyte?\nYes, you can control what's sent outside of Airbyte for analytics purposes.\nWe added the following telemetry to Airbyte to ensure the best experience for users:\n\nMeasure usage of features & connectors\nMeasure failure rate of connectors to address bugs quickly\nReach out to our users about Airbyte community updates if they opt-in\n...\n\nTo disable telemetry, modify the `.env` file and define the two following environment variables:\n```text\nTRACKING_STRATEGY=logging",
    "tag": "airbyte"
  },
  {
    "title": "Security & Data Audits",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/archive/faq/security-and-data-audits.md",
    "content": "Security & Data Audits\nHow secure is Airbyte?\nAirbyte is an open-source self-hosted solution, so let\u2019s say it is as safe as your data infrastructure. **\nIs Airbyte GDPR compliant?\nAirbyte is a self-hosted solution, so it doesn\u2019t bring any security or privacy risk to your infrastructure. We do intend to add data quality and privacy compliance features in the future, in order to give you more visibility on that topic.\nHow does Airbyte charge?\nWe don\u2019t. All connectors are all under the MIT license. If you are curious about the business model we have in mind, please check our company handbook.",
    "tag": "airbyte"
  },
  {
    "title": "Data Loading",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/archive/faq/data-loading.md",
    "content": "Data Loading\nWhy don\u2019t I see any data in my destination yet?\nIt can take a while for Airbyte to load data into your destination. Some sources have restrictive API limits which constrain how much \ndata we can sync in a given time. Large amounts of data in your source can also make the initial sync take longer. You can check your\nsync status in your connection detail page that you can access through the destination detail page or the source one.\nWhy my final tables are being recreated everytime?\nAirbyte ingests data into raw tables and applies the process of normalization if you selected it in the connection page.\nThe normalization runs a full refresh each sync and for some destinations like Snowflake, Redshift, Bigquery this may incur more\nresource consumption and more costs. You need to pay attention to the frequency that you're retrieving your data to avoid issues.\nFor example, if you create a connection to sync every 5 minutes with incremental sync on, it will only retrieve new records into the raw tables but will apply normalization\nto all the data in every sync! If you have tons of data, this may not be the right sync frequency for you.\nThere is a Github issue to implement normalization using incremental, which will reduce\ncosts and resources in your destination.\nWhat happens if a sync fails?\nYou won't lose data when a sync fails, however, no data will be added or updated in your destination.\nAirbyte will automatically attempt to replicate data 3 times. You can see and export the logs for those attempts in the connection \ndetail page. You can access this page through the Source or Destination detail page.\nYou can configure a Slack webhook to warn you when a sync fails.\nIn the future you will be able to configure other notification method (email, Sentry) and an option to create a\nGitHub issue with the logs. We\u2019re still working on it, and the purpose would be to help the community and the Airbyte team to fix the\nissue as soon as possible, especially if it is a connector issue.\nUntil Airbyte has this system in place, here is what you can do:\n\nFile a GitHub issue: go here \n  and file an issue with the detailed logs copied in the issue\u2019s description. The team will be notified about your issue and will update\n  it for any progress or comment on it.  \nFix the issue yourself: Airbyte is open source so you don\u2019t need to wait for anybody to fix your issue if it is important to you.\n  To do so, just fork the GitHub project and fix the piece of code that need fixing. If you\u2019re okay\n  with contributing your fix to the community, you can submit a pull request. We will review it ASAP.\nAsk on Slack: don\u2019t hesitate to ping the team on Slack.\n\nOnce all this is done, Airbyte resumes your sync from where it left off.\nWe truly appreciate any contribution you make to help the community. Airbyte will become the open-source standard only if everybody participates.\nCan Airbyte support 2-way sync i.e. changes from A go to B and changes from B go to A?\nAirbyte actually does not support this right now. There are some details around how we handle schema and tables names that isn't going to \nwork for you in the current iteration.\nIf you attempt to do a circular dependency between source and destination, you'll end up with the following\nA.public.table_foo writes to B.public.public_table_foo to A.public.public_public_table_foo. You won't be writing into your original table,\nwhich I think is your intention.\nWhat happens to data in the pipeline if the destination gets disconnected? Could I lose data, or wind up with duplicate data when the pipeline is reconnected?\nAirbyte is architected to prevent data loss or duplication. Airbyte will display a failure for the sync, and re-attempt it at the next syncing,\naccording to the frequency you set.\nHow frequently can Airbyte sync data?\nYou can adjust the load time to run as frequent as every hour or as infrequent as once a year using Cron expressions. \nWhy wouldn\u2019t I choose to load all of my data more frequently?\nWhile frequent data loads will give you more up-to-date data, there are a few reasons you wouldn\u2019t want to load your too frequently, including:\n\nHigher API usage may cause you to hit a limit that could impact other systems that rely on that API.\nHigher cost of loading data into your warehouse.\nMore frequent delays, resulting in increased delay notification emails. For instance, if the data source generally takes several hours to \n  update but you wanted five-minute increments, you may receive a delay notification every sync.\n\nGenerally is recommended setting the incremental loads to every hour to help limit API calls.\nIs there a way to know the estimated time to completion for the first historic sync?\nUnfortunately not yet.\nDo you support change data capture (CDC) or logical replication for databases?\nAirbyte currently supports CDC for Postgres and Mysql. Airbyte is adding support for a few other \ndatabases you can check in the roadmap.\nUsing incremental sync, is it possible to add more fields when some new columns are added to a source table, or when a new table is added?\nFor the moment, incremental sync doesn't support schema changes, so you would need to perform a full refresh whenever that happens.\nHere\u2019s a related Github issue.\nThere is a limit of how many tables one connection can handle?\nYes, for more than 6000 thousand tables could be a problem to load the information on UI.\nThere are two Github issues about this limitation: Issue #3942 \nand Issue #3943.\nHelp, Airbyte is hanging/taking a long time to discover my source's schema!\nThis usually happens for database sources that contain a lot of tables. This should resolve itself in half an hour or so.\nIf the source contains more than 6k tables, see the above question.\nThere is a known issue with Oracle databases.\nI see you support a lot of connectors \u2013 what about connectors Airbyte doesn\u2019t support yet?\nYou can either:\n\nSubmit a connector request on our Github project, and be notified once we or the community build a connector for it. \nBuild a connector yourself by forking our GitHub project and submitting a pull request. Here\n  are the instructions how to build a connector.\nAsk on Slack: don\u2019t hesitate to ping the team on Slack.\n\nWhat kind of notifications do I get?\nFor the moment, the UI will only display one kind of notification: when a sync fails, Airbyte will display the failure at the source/destination \nlevel in the list of sources/destinations, and in the connection detail page along with the logs.\nHowever, there are other types of notifications:\n\nWhen a connector that you use is no longer up to date\nWhen your connections fails\nWhen core isn't up to date\n",
    "tag": "airbyte"
  },
  {
    "title": "Singer vs Airbyte",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/archive/faq/differences-with/singer-vs-airbyte.md",
    "content": "Singer vs Airbyte\nIf you want to understand the difference between Airbyte and Singer, you might be interested in 2 articles we wrote:\n\n\u201cAirbyte vs. Singer: Why Airbyte is not built on top of Singer.\u201d\n\u201cThe State of Open-Source Data Integration and ETL,\u201d in which we list and compare all ETL-related open-source projects, including Singer and Airbyte. As a summary, here are the differences:\n\n\nSinger:\n\nSupports 96 connectors after 4 years.\nIncreasingly outdated connectors: Talend (acquirer of StitchData) seems to have stopped investing in maintaining Singer\u2019s community and connectors. As most connectors see schema changes several times a year, more and more Singer\u2019s taps and targets are not actively maintained and are becoming outdated. \nAbsence of standardization: each connector is its own open-source project. So you never know the quality of a tap or target until you have actually used it. There is no guarantee whatsoever about what you\u2019ll get.\nSinger\u2019s connectors are standalone binaries: you still need to build everything around to make them work (e.g. UI, configuration validation, state management, normalization, schema migration, monitoring, etc). \nNo full commitment to open sourcing all connectors, as some connectors are only offered by StitchData under a paid plan.  **\n\nAirbyte:\n\nOur ambition is to support 300+ connectors by the end of 2021. We already supported about 50 connectors at the end of 2020, just 5 months after its inception. \nAirbyte\u2019s connectors are usable out of the box through a UI and API, with monitoring, scheduling and orchestration. Airbyte was built on the premise that a user, whatever their background, should be able to move data in 2 minutes. Data engineers might want to use raw data and their own transformation processes, or to use Airbyte\u2019s API to include data integration in their workflows. On the other hand, analysts and data scientists might want to use normalized consolidated data in their database or data warehouses. Airbyte supports all these use cases.  \nOne platform, one project with standards: This will help consolidate the developments behind one single project, some standardization and specific data protocol that can benefit all teams and specific cases. \nConnectors can be built in the language of your choice, as Airbyte runs them as Docker containers.\nAirbyte integrates with your data stack and your needs: Airflow, Kubernetes, dbt, etc. Its normalization is optional, it gives you a basic version that works out of the box, but also allows you to use dbt to do more complicated things.\nA full commitment to the open-source MIT project with the promise not to hide some connectors behind paid walls.\n\nNote that Airbyte\u2019s data protocol is compatible with Singer\u2019s. So it is easy to migrate a Singer tap onto Airbyte.",
    "tag": "airbyte"
  },
  {
    "title": "StitchData vs Airbyte",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/archive/faq/differences-with/stitchdata-vs-airbyte.md",
    "content": "StitchData vs Airbyte\nWe wrote an article, \u201cOpen-source vs. Commercial Software: How to Solve the Data Integration Problem,\u201d in which we describe the pros and cons of StitchData\u2019s commercial approach and Airbyte\u2019s open-source approach. Don\u2019t hesitate to check it out for more detailed arguments. As a summary, here are the differences:\n\nStitchData:\n\nLimited deprecating connectors: Stitch only supports 150 connectors. Talend has stopped investing in StitchData and its connectors. And on Singer, each connector is its own open-source project. So you never know the quality of a tap or target until you have actually used it. There is no guarantee whatsoever about what you\u2019ll get.\nPricing indexed on usage: StitchData\u2019s pricing is indexed on the connectors used and the volume of data transferred. Teams always need to keep that in mind and are not free to move data without thinking about cost. \nSecurity and privacy compliance: all companies are subject to privacy compliance laws, such as GDPR, CCPA, HIPAA, etc. As a matter of fact, above a certain stage (about 100 employees) in a company, all external products need to go through a security compliance process that can take several months. \nNo moving data between internal databases: StitchData sits in the cloud, so if you have to replicate data from an internal database to another, it makes no sense to have the data move through their cloud for privacy and cost reasons. \nStitchData\u2019s Singer connectors are standalone binaries: you still need to build everything around to make them work. And it\u2019s hard to update some pre-built connectors, as they are of poor quality. \n\nAirbyte:\n\nFree, as open source, so no more pricing based on usage: learn more about our future business model (connectors will always remain open-source). \nSupporting 50+ connectors by the end of 2020 (so in only 5 months of existence). Our goal is to reach 300+ connectors by the end of 2021.\nBuilding new connectors made trivial, in the language of your choice: Airbyte makes it a lot easier to create your own connector, vs. building them yourself in-house (with Airflow or other tools). Scheduling, orchestration, and monitoring comes out of the box with Airbyte.\nMaintenance-free connectors you can use in minutes. Just authenticate your sources and warehouse, and get connectors that adapt to schema and API changes for you.\nAddressing the long tail of connectors: with the help of the community, Airbyte ambitions to support thousands of connectors. \nAdapt existing connectors to your needs: you can adapt any existing connector to address your own unique edge case.\nUsing data integration in a workflow: Airbyte\u2019s API lets engineering teams add data integration jobs into their workflow seamlessly.\nIntegrates with your data stack and your needs: Airflow, Kubernetes, dbt, etc. Its normalization is optional, it gives you a basic version that works out of the box, but also allows you to use dbt to do more complicated things.\nDebugging autonomy: if you experience any connector issue, you won\u2019t need to wait for Fivetran\u2019s customer support team to get back to you, if you can fix the issue fast yourself. \nYour data stays in your cloud. Have full control over your data, and the costs of your data transfers.\nNo more security and privacy compliance, as self-hosted and open-sourced (MIT). Any team can directly address their integration needs.\nPremium support directly on our Slack for free. Our time to resolution is about 3-4 hours in average. \n",
    "tag": "airbyte"
  },
  {
    "title": "Pipelinewise vs Airbyte",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/archive/faq/differences-with/pipelinewise-vs-airbyte.md",
    "content": "Pipelinewise vs Airbyte\nPipelineWise:\nPipelineWise is an open-source project by Transferwise that was built with the primary goal of serving their own needs. There is no business model attached to the project, and no apparent interest in growing the community.\n\nSupports 21 connectors, and only adds new ones based on the needs of the mother company, Transferwise. \nNo business model attached to the project, and no apparent interest from the company in growing the community. \nAs close to the original format as possible: PipelineWise aims to reproduce the data from the source to an Analytics-Data-Store in as close to the original format as possible. Some minor load time transformations are supported, but complex mapping and joins have to be done in the Analytics-Data-Store to extract meaning.\nManaged Schema Changes: When source data changes, PipelineWise detects the change and alters the schema in your Analytics-Data-Store automatically.\nYAML based configuration: Data pipelines are defined as YAML files, ensuring that the entire configuration is kept under version control.\nLightweight: No daemons or database setup are required.\n\nAirbyte:\nIn contrast, Airbyte is a company fully committed to the open-source project and has a business model in mind around this project.\n\nOur ambition is to support 300+ connectors by the end of 2021. We already supported about 50 connectors at the end of 2020, just 5 months after its inception.\nAirbyte\u2019s connectors are usable out of the box through a UI and API, with monitoring, scheduling and orchestration. Airbyte was built on the premise that a user, whatever their background, should be able to move data in 2 minutes. Data engineers might want to use raw data and their own transformation processes, or to use Airbyte\u2019s API to include data integration in their workflows. On the other hand, analysts and data scientists might want to use normalized consolidated data in their database or data warehouses. Airbyte supports all these use cases.  \nOne platform, one project with standards: This will help consolidate the developments behind one single project, some standardization and specific data protocol that can benefit all teams and specific cases. \nConnectors can be built in the language of your choice, as Airbyte runs them as Docker containers.\nAirbyte integrates with your data stack and your needs: Airflow, Kubernetes, dbt, etc. Its normalization is optional, it gives you a basic version that works out of the box, but also allows you to use dbt to do more complicated things.\n\nThe data protocols for both projects are compatible with Singer\u2019s. So it is easy to migrate a Singer tap or target onto Airbyte or PipelineWise.",
    "tag": "airbyte"
  },
  {
    "title": "Meltano vs Airbyte",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/archive/faq/differences-with/meltano-vs-airbyte.md",
    "content": "Meltano vs Airbyte\nWe wrote an article, \u201cThe State of Open-Source Data Integration and ETL,\u201d in which we list and compare all ETL-related open-source projects, including Meltano and Airbyte. Don\u2019t hesitate to check it out for more detailed arguments. As a summary, here are the differences:\nMeltano:\n\nMeltano is built on top of the Singer protocol, whereas Airbyte is built on top of the Airbyte protocol. Having initially created Airbyte on top of Singer, we wrote about why we didn't move forward with it here and here. Summarized, the reasons were: Singer connectors didn't always adhere to the Singer protocol, had poor standardization and visibility in terms of quality, and community governance and support was abandoned by Stitch. By contrast, we aim to make Airbyte a product that \"just works\" and always plan to maximize engagement within the Airbyte community. \nCLI-first approach: Meltano was primarily built with a command line interface in mind. In that sense, they seem to target engineers with a preference for that interface.\nIntegration with Airflow for orchestration: You can either use Meltano alone for orchestration or with Airflow; Meltano works both ways.  \nAll connectors must use Python.  \nMeltano works with any of Singer's 200+ available connectors. However, in our experience, quality has been hit or miss. \n\nAirbyte:\nIn contrast, Airbyte is a company fully committed to the open-source project and has a business model in mind around this project. Our team are data integration experts that have built more than 1,000 integrations collectively at large scale. The team now counts 20 engineers working full-time on Airbyte.\n\nAirbyte supports more than 100 connectors after only 1 year since its inception, 20% of which were built by the community. Our ambition is to support 200+ connectors by the end of 2021. \nAirbyte\u2019s connectors are usable out of the box through a UI and API, with monitoring, scheduling and orchestration. Airbyte was built on the premise that a user, whatever their background, should be able to move data in 2 minutes. Data engineers might want to use raw data and their own transformation processes, or to use Airbyte\u2019s API to include data integration in their workflows. On the other hand, analysts and data scientists might want to use normalized consolidated data in their database or data warehouses. Airbyte supports all these use cases.  \nOne platform, one project with standards: This will help consolidate the developments behind one single project, some standardization and specific data protocol that can benefit all teams and specific cases. \nNot limited by Singer\u2019s data protocol: In contrast to Meltano, Airbyte was not built on top of Singer, but its data protocol is compatible with Singer\u2019s. This means Airbyte can go beyond Singer, but Meltano will remain limited. \nConnectors can be built in the language of your choice, as Airbyte runs them as Docker containers.\nAirbyte integrates with your data stack and your needs: Airflow, Kubernetes, dbt, etc. Its normalization is optional, it gives you a basic version that works out of the box, but also allows you to use dbt to do more complicated things.\n\nOther noteworthy differences:\n\nIn terms of community, Meltano's Slack community got 430 new members in the last 6 months, while Airbyte got 800. \nThe difference in velocity in terms of feature progress is easily measurable as both are open-source projects. Meltano closes about 30 issues per month, while Airbyte closes about 120. \n",
    "tag": "airbyte"
  },
  {
    "title": "Fivetran vs Airbyte",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/archive/faq/differences-with/fivetran-vs-airbyte.md",
    "content": "Fivetran vs Airbyte\nWe wrote an article, \u201cOpen-source vs. Commercial Software: How to Solve the Data Integration Problem,\u201d in which we describe the pros and cons of Fivetran\u2019s commercial approach and Airbyte\u2019s open-source approach. Don\u2019t hesitate to check it out for more detailed arguments. As a summary, here are the differences:\n\nFivetran:\n\nLimited high-quality connectors: after 8 years in business, Fivetran supports 150 connectors. The more connectors, the more difficult it is for Fivetran to keep the same level of maintenance across all connectors. They will always have a ROI consideration to maintaining long-tailed connectors. \nPricing indexed on usage: Fivetran\u2019s pricing is indexed on the number of active rows (rows added or edited) per month. Teams always need to keep that in mind and are not free to move data without thinking about cost, as the costs can grow fast. \nSecurity and privacy compliance: all companies are subject to privacy compliance laws, such as GDPR, CCPA, HIPAA, etc. As a matter of fact, above a certain stage (about 100 employees) in a company, all external products need to go through a security compliance process that can take several months. \nNo moving data between internal databases: Fivetran sits in the cloud, so if you have to replicate data from an internal database to another, it makes no sense to have the data move through them (Fivetran) for privacy and cost reasons. \n\nAirbyte:\n\nFree, as open source, so no more pricing based on usage: learn more about our future business model (connectors will always remain open source). \nSupporting 60 connectors within 8 months from inception.  Our goal is to reach 200+ connectors by the end of 2021. \nBuilding new connectors made trivial, in the language of your choice: Airbyte makes it a lot easier to create your own connector, vs. building them yourself in-house (with Airflow or other tools). Scheduling, orchestration, and monitoring comes out of the box with Airbyte.\nAddressing the long tail of connectors: with the help of the community, Airbyte ambitions to support thousands of connectors. \nAdapt existing connectors to your needs: you can adapt any existing connector to address your own unique edge case.\nUsing data integration in a workflow: Airbyte\u2019s API lets engineering teams add data integration jobs into their workflow seamlessly. \nIntegrates with your data stack and your needs: Airflow, Kubernetes, dbt, etc. Its normalization is optional, it gives you a basic version that works out of the box, but also allows you to use dbt to do more complicated things.\nDebugging autonomy: if you experience any connector issue, you won\u2019t need to wait for Fivetran\u2019s customer support team to get back to you, if you can fix the issue fast yourself. \nNo more security and privacy compliance, as self-hosted, source-available and open-sourced (MIT). Any team can directly address their integration needs.\n\nYour data stays in your cloud. Have full control over your data, and the costs of your data transfers.",
    "tag": "airbyte"
  },
  {
    "title": "Build a Slack Activity Dashboard",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/archive/examples/build-a-slack-activity-dashboard.md",
    "content": "\ndescription: Using Airbyte and Apache Superset\nBuild a Slack Activity Dashboard\n\nThis article will show how to use Airbyte - an open-source data integration platform - and Apache Superset - an open-source data exploration platform - in order to build a Slack activity dashboard showing:\n\nTotal number of members of a Slack workspace\nThe evolution of the number of Slack workspace members\nEvolution of weekly messages\nEvolution of messages per channel\nMembers per time zone\n\nBefore we get started, let\u2019s take a high-level look at how we are going to achieve creating a Slack dashboard using Airbyte and Apache Superset.\n\nWe will use the Airbyte\u2019s Slack connector to get the data off a Slack workspace (we will be using Airbyte\u2019s own Slack workspace for this tutorial).\nWe will save the data onto a PostgreSQL database.\nFinally, using Apache Superset, we will implement the various metrics we care about.\n\nGot it? Now let\u2019s get started.\n1. Replicating Data from Slack to Postgres with Airbyte\na. Deploying Airbyte\nThere are several easy ways to deploy Airbyte, as listed here. For this tutorial, I will just use the Docker Compose method from my workstation:\n```text\nIn your workstation terminal\ngit clone https://github.com/airbytehq/airbyte.git\ncd airbyte\ndocker-compose up\n```\nThe above command will make the Airbyte app available on `localhost:8000`. Visit the URL on your favorite browser, and you should see Airbyte\u2019s dashboard (if this is your first time, you will be prompted to enter your email to get started).\nIf you haven\u2019t set Docker up, follow the instructions here to set it up on your machine.\nb. Setting Up Airbyte\u2019s Slack Source Connector\nAirbyte\u2019s Slack connector will give us access to the data. So, we are going to kick things off by setting this connector to be our data source in Airbyte\u2019s web app. I am assuming you already have Airbyte and Docker set up on your local machine. We will be using Docker to create our PostgreSQL database container later on.\nNow, let\u2019s proceed. If you already went through the onboarding, click on the \u201cnew source\u201d button at the top right of the Sources section. If you're going through the onboarding, then follow the instructions.\nYou will be requested to enter a name for the source you are about to create. You can call it \u201cslack-source\u201d. Then, in the Source Type combo box, look for \u201cSlack,\u201d and then select it. Airbyte will then present the configuration fields needed for the Slack connector. So you should be seeing something like this on the Airbyte App:\n\nThe first thing you will notice is that this connector requires a Slack token. So, we have to obtain one. If you are not a workspace admin, you will need to ask for permission.\nLet\u2019s walk through how we would get the Slack token we need.\nAssuming you are a workspace admin, open the Slack workspace and navigate to [Workspace Name] > Administration > Customize [Workspace Name]. In our case, it will be Airbyte > Administration > Customize Airbyte (as shown below):\n\nIn the new page that opens up in your browser, you will then need to navigate to Configure apps.\n\nIn the new window that opens up, click on Build in the top right corner.\n\nClick on the Create an App button.\n\nIn the modal form that follows, give your app a name - you can name it `airbyte_superset`, then select your workspace from the Development Slack Workspace.\n\nNext, click on the Create App button. You will then be presented with a screen where we are going to set permissions for our `airbyte_superset` app, by clicking on the Permissions button on this page.\n\nIn the next screen, navigate to the scope section. Then, click on the Add an OAuth Scope button. This will allow you to add permission scopes for your app. At a minimum, your app should have the following permission scopes:\n\nThen, we are going to add our created app to the workspace by clicking the Install to Workspace button.\n\nSlack will prompt you that your app is requesting permission to access your workspace of choice. Click Allow.\n\nAfter the app has been successfully installed, you will be navigated to Slack\u2019s dashboard, where you will see the Bot User OAuth Access Token.\nThis is the token you will provide back on the Airbyte page, where we dropped off to obtain this token. So make sure to copy it and keep it in a safe place.\nNow that we are done with obtaining a Slack token, let\u2019s go back to the Airbyte page we dropped off and add the token in there.\nWe will also need to provide Airbyte with `start_date`. This is the date from which we want Airbyte to start replicating data from the Slack API, and we define that in the format: `YYYY-MM-DDT00:00:00Z`.\nWe will specify ours as `2020-09-01T00:00:00Z`. We will also tell Airbyte to exclude archived channels and not include private channels, and also to join public channels, so the latter part of the form should look like this:\n\nFinally, click on the Set up source button for Airbyte to set the Slack source up.\nIf the source was set up correctly, you will be taken to the destination section of Airbyte\u2019s dashboard, where you will tell Airbyte where to store the replicated data.\nc. Setting Up Airbyte\u2019s Postgres Destination Connector\nFor our use case, we will be using PostgreSQL as the destination.\nClick the add destination button in the top right corner, then click on add a new destination.\n\nIn the next screen, Airbyte will validate the source, and then present you with a form to give your destination a name. We\u2019ll call this destination slack-destination. Then, we will select the Postgres destination type. Your screen should look like this now:\n\nGreat! We have a form to enter Postgres connection credentials, but we haven\u2019t set up a Postgres database. Let\u2019s do that!\nSince we already have Docker installed, we can spin off a Postgres container with the following command in our terminal:\n`text\ndocker run --rm --name slack-db -e POSTGRES_PASSWORD=password -p 2000:5432 -d postgres`\n(Note that the Docker compose file for Superset ships with a Postgres database, as you can see here).\nThe above command will do the following:\n\ncreate a Postgres container with the name slack-db,\nset the password to password,\nexpose the container\u2019s port 5432, as our machine\u2019s port 2000. \ncreate a database and a user, both called postgres. \n\nWith this, we can go back to the Airbyte screen and supply the information needed. Your form should look like this:\n\nThen click on the Set up destination button.\nd. Setting Up the Replication\nYou should now see the following screen:\n\nAirbyte will then fetch the schema for the data coming from the Slack API for your workspace. You should leave all boxes checked and then choose the sync frequency - this is the interval in which Airbyte will sync the data coming from your workspace. Let\u2019s set the sync interval to every 24 hours.\nThen click on the Set up connection button.\nAirbyte will now take you to the destination dashboard, where you will see the destination you just set up. Click on it to see more details about this destination.\n\nYou will see Airbyte running the very first sync. Depending on the size of the data Airbyte is replicating, it might take a while before syncing is complete.\n\nWhen it\u2019s done, you will see the Running status change to Succeeded, and the size of the data Airbyte replicated as well as the number of records being stored on the Postgres database.\n\nTo test if the sync worked, run the following in your terminal:\n`text\ndocker exec slack-source psql -U postgres -c \"SELECT * FROM public.users;\"`\nThis should output the rows in the users\u2019 table.\nTo get the count of the users\u2019 table as well, you can also run:\n`text\ndocker exec slack-db psql -U postgres -c \"SELECT count(*) FROM public.users;\"`\nNow that we have the data from the Slack workspace in our Postgres destination, we will head on to creating the Slack dashboard with Apache Superset.\n2. Setting Up Apache Superset for the Dashboards\na. Installing Apache Superset\nApache Superset, or simply Superset, is a modern data exploration and visualization platform. To get started using it, we will be cloning the Superset repo. Navigate to a destination in your terminal where you want to clone the Superset repo to and run:\n`text\ngit clone https://github.com/apache/superset.git`\nIt\u2019s recommended to check out the latest branch of Superset, so run:\n`text\ncd superset`\nAnd then run:\n`text\ngit checkout latest`\nSuperset needs you to install and build its frontend dependencies and assets. So, we will start by installing the frontend dependencies:\n`text\nnpm install`\nNote: The above command assumes you have both Node and NPM installed on your machine.\nFinally, for the frontend, we will build the assets by running:\n`text\nnpm run build`\nAfter that, go back up one directory into the Superset directory by running:\n`text\ncd..`\nThen run:\n`text\ndocker-compose up`\nThis will download the Docker images Superset needs and build containers and start services Superset needs to run locally on your machine.\nOnce that\u2019s done, you should be able to access Superset on your browser by visiting http://localhost:8088, and you should be presented with the Superset login screen.\nEnter username: admin and Password: admin to be taken to your Superset dashboard.\nGreat! You\u2019ve got Superset set up. Now let\u2019s tell Superset about our Postgres Database holding the Slack data from Airbyte.\nb. Setting Up a Postgres Database in Superset\nTo do this, on the top menu in your Superset dashboard, hover on the Data dropdown and click on Databases.\n\nIn the page that opens up, click on the + Database button in the top right corner.\n\nThen, you will be presented with a modal to add your Database Name and the connection URI.\n\nLet\u2019s call our Database `slack_db`, and then add the following URI as the connection URI:\n`text\npostgresql://postgres:password@docker.for.mac.localhost:2000/postgres`\nIf you are on a Windows Machine, yours will be:\n`text\npostgresql://postgres:password@docker.for.win.localhost:2000/postgres`\nNote: We are using `docker.for.[mac|win].localhost` in order to access the localhost of your machine, because using just localhost will point to the Docker container network and not your machine\u2019s network.\nYour Superset UI should look like this:\n\nWe will need to enable some settings on this connection. Click on the SQL LAB SETTINGS and check the following boxes:\n\nAfterwards, click on the ADD button, and you will see your database on the data page of Superset.\n\nc. Importing our dataset\nNow that you\u2019ve added the database, you will need to hover over the data menu again; now click on Datasets.\n\nThen, you will be taken to the datasets page:\n\nWe want to only see the datasets that are in our `slack_db` database, so in the Database that is currently showing All, select `slack_db` and you will see that we don\u2019t have any datasets at the moment.\n\n\nYou can fix this by clicking on the + DATASET button and adding the following datasets.\nNote: Make sure you select the public schema under the Schema dropdown.\n\nNow that we have set up Superset and given it our Slack data, let\u2019s proceed to creating the visualizations we need.\nStill remember them? Here they are again:\n\nTotal number of members of a Slack workspace\nThe evolution of the number of Slack workspace members\nEvolution of weekly messages\nEvolution of weekly threads created\nEvolution of messages per channel\nMembers per time zone\n\n3. Creating Our Dashboards with Superset\na. Total number of members of a Slack workspace\nTo get this, we will first click on the users\u2019 dataset of our `slack_db` on the Superset dashboard.\n\nNext, change untitled at the top to Number of Members.\n\nNow change the Visualization Type to Big Number, remove the Time Range filter, and add a Subheader named \u201cSlack Members.\u201d So your UI should look like this:\n\nThen, click on the RUN QUERY button, and you should now see the total number of members.\nPretty cool, right? Now let\u2019s save this chart by clicking on the SAVE button.\n\nThen, in the ADD TO DASHBOARD section, type in \u201cSlack Dashboard\u201d, click on the \u201cCreate Slack Dashboard\u201d button, and then click the Save button.\nGreat! We have successfully created our first Chart, and we also created the Dashboard. Subsequently, we will be following this flow to add the other charts to the created Slack Dashboard.\nb. Casting the ts column\nBefore we proceed with the rest of the charts for our dashboard, if you inspect the ts column on either the messages table or the threads table, you will see it\u2019s of the type `VARCHAR`. We can\u2019t really use this for our charts, so we have to cast both the messages and threads\u2019 ts column as `TIMESTAMP`. Then, we can create our charts from the results of those queries. Let\u2019s do this.\nFirst, navigate to the Data menu, and click on the Datasets link. In the list of datasets, click the Edit button for the messages table.\n\nYou\u2019re now in the Edit Dataset view. Click the Lock button to enable editing of the dataset. Then, navigate to the Columns tab, expand the ts dropdown, and then tick the Is Temporal box.\n\nPersist the changes by clicking the Save button.\nc. The evolution of the number of Slack workspace members\nIn the exploration page, let\u2019s first get the chart showing the evolution of the number of Slack members. To do this, make your settings on this page match the screenshot below:\n\nSave this chart onto the Slack Dashboard.\nd. Evolution of weekly messages posted\nNow, we will look at the evolution of weekly messages posted. Let\u2019s configure the chart settings on the same page as the previous one.\n\nRemember, your visualization will differ based on the data you have.\ne. Evolution of weekly threads created\nNow, we are finished with creating the message chart. Let's go over to the thread chart. You will recall that we will need to cast the ts column as stated earlier. So, do that and get to the exploration page, and make it match the screenshot below to achieve the required visualization:\n\nf. Evolution of messages per channel\nFor this visualization, we will need a more complex SQL query. Here\u2019s the query we used (as you can see in the screenshot below):\n`text\nSELECT CAST(m.ts as TIMESTAMP), c.name, m.text\nFROM public.messages m\nINNER JOIN public.channels c\nON m.channel_id = c_id`\n\nNext, click on EXPLORE to be taken to the exploration page; make it match the screenshot below:\n\nSave this chart to the dashboard.\ng. Members per time zone\nFinally, we will be visualizing members per time zone. To do this, instead of casting in the SQL lab as we\u2019ve previously done, we will explore another method to achieve casting by using Superset\u2019s Virtual calculated column feature. This feature allows us to write SQL queries that customize the appearance and behavior of a specific column.\nFor our use case, we will need the updated column of the users table to be a `TIMESTAMP`, in order to perform the visualization we need for Members per time zone. Let\u2019s start on clicking the edit icon on the users table in Superset.\n\nYou will be presented with a modal like so:\n\nClick on the CALCULATED COLUMNS tab:\n\nThen, click on the + ADD ITEM button, and make your settings match the screenshot below.\n\nThen, go to the exploration page and make it match the settings below:\n\nNow save this last chart, and head over to your Slack Dashboard. It should look like this:\n\nOf course, you can edit how the dashboard looks to fit what you want on it.\nConclusion\nIn this article, we looked at using Airbyte\u2019s Slack connector to get the data from a Slack workspace into a Postgres database, and then used Apache Superset to craft a dashboard of visualizations.If you have any questions about Airbyte, don\u2019t hesitate to ask questions on our Slack! If you have questions about Superset, you can join the Superset Community Slack!",
    "tag": "airbyte"
  },
  {
    "title": "Postgres Replication",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/archive/examples/postgres-replication.md",
    "content": "\ndescription: Start syncing data in minutes with Airbyte\nPostgres Replication\nLet's see how you can spin up a local instance of Airbyte and syncing data from one Postgres database to another.\nHere's a 6-minute video showing you how you can do it.\n{% embed url=\"https://www.youtube.com/watch?v=Rcpt5SVsMpk\" caption=\"\" %}\nFirst of all, make sure you have Docker and Docker Compose installed. If this isn't the case, follow the guide for the recommended approach to install Docker. \nOnce Docker is installed successfully, run the following commands:\n`text\ngit clone https://github.com/airbytehq/airbyte.git\ncd airbyte\ndocker-compose up`\nOnce you see an Airbyte banner, the UI is ready to go at http://localhost:8000/.\n1. Set up your preferences\nYou should see an onboarding page. Enter your email and continue.\n\n2. Set up your first connection\nWe support a growing list of source connectors. For now, we will start out with a Postgres source and destination.\nIf you don't have a readily available Postgres database to sync, here are some quick instructions:\nRun the following commands in a new terminal window to start backgrounded source and destination databases:\n`text\ndocker run --rm --name airbyte-source -e POSTGRES_PASSWORD=password -p 2000:5432 -d postgres\ndocker run --rm --name airbyte-destination -e POSTGRES_PASSWORD=password -p 3000:5432 -d postgres`\nAdd a table with a few rows to the source database:\n`text\ndocker exec -it airbyte-source psql -U postgres -c \"CREATE TABLE users(id SERIAL PRIMARY KEY, col1 VARCHAR(200));\"\ndocker exec -it airbyte-source psql -U postgres -c \"INSERT INTO public.users(col1) VALUES('record1');\"\ndocker exec -it airbyte-source psql -U postgres -c \"INSERT INTO public.users(col1) VALUES('record2');\"\ndocker exec -it airbyte-source psql -U postgres -c \"INSERT INTO public.users(col1) VALUES('record3');\"`\nYou now have a Postgres database ready to be replicated!\nConnect the Postgres database\nIn the UI, you will see a wizard that allows you choose the data you want to send through Airbyte.\n\nUse the name `airbyte-source` for the name and `Postgres`as the type. If you used our instructions to create a Postgres database, fill in the configuration fields as follows:\n`text\nHost: localhost\nPort: 2000\nUser: postgres\nPassword: password\nDB Name: postgres`\nClick on `Set Up Source` and the wizard should move on to allow you to configure a destination.\nWe support a growing list of data warehouses, lakes and databases. For now, use the name `airbyte-destination`, and configure the destination Postgres database:\n`text\nHost: localhost\nPort: 3000\nUser: postgres\nPassword: password\nDB Name: postgres`\nAfter adding the destination, you can choose what tables and columns you want to sync.\n\nFor this demo, we recommend leaving the defaults and selecting \"Every 5 Minutes\" as the frequency. Click `Set Up Connection` to finish setting up the sync.\n3. Check the logs of your first sync\nYou should now see a list of sources with the source you just added. Click on it to find more information about your connection. This is the page where you can update any settings about this source and how it syncs. There should be a `Completed` job under the history section. If you click on that run, it will show logs from that run.\n\nOne of biggest problems we've seen in tools like Fivetran is the lack of visibility when debugging. In Airbyte, allowing full log access and the ability to debug and fix connector problems is one of our highest priorities. We'll be working hard to make these logs accessible and understandable.\n4. Check if the syncing actually worked\nNow let's verify that this worked. Let's output the contents of the destination db:\n`text\ndocker exec airbyte-destination psql -U postgres -c \"SELECT * FROM public.users;\"`\n:::info\nDon't worry about the awkward `public_users` name for now; we are currently working on an update to allow users to configure their destination table names!\n:::\nYou should see the rows from the source database inside the destination database!\nAnd there you have it. You've taken data from one database and replicated it to another. All of the actual configuration for this replication only took place in the UI.\nThat's it! This is just the beginning of Airbyte. If you have any questions at all, please reach out to us on Slack. We\u2019re still in alpha, so if you see any rough edges or want to request a connector you need, please create an issue on our Github or leave a thumbs up on an existing issue.",
    "tag": "airbyte"
  },
  {
    "title": "Save and Search Through Your Slack History on a Free Slack Plan",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/archive/examples/slack-history.md",
    "content": "\ndescription: Using Airbyte and MeiliSearch\nSave and Search Through Your Slack History on a Free Slack Plan\n\nThe Slack free tier saves only the last 10K messages. For social Slack instances, it may be impractical to upgrade to a paid plan to retain these messages. Similarly, for an open-source project like Airbyte where we interact with our community through a public Slack instance, the cost of paying for a seat for every Slack member is prohibitive.\nHowever, searching through old messages can be really helpful. Losing that history feels like some advanced form of memory loss. What was that joke about Java 8 Streams? This contributor question sounds familiar\u2014haven't we seen it before? But you just can't remember!\nThis tutorial will show you how you can, for free, use Airbyte to save these messages (even after Slack removes access to them). It will also provide you a convenient way to search through them.\nSpecifically, we will export messages from your Slack instance into an open-source search engine called MeiliSearch. We will be focusing on getting this setup running from your local workstation. We will mention at the end how you can set up a more productionized version of this pipeline.\nWe want to make this process easy, so while we will link to some external documentation for further exploration, we will provide all the instructions you need here to get this up and running.\n1. Set Up MeiliSearch\nFirst, let's get MeiliSearch running on our workstation. MeiliSearch has extensive docs for getting started. For this tutorial, however, we will give you all the instructions you need to set up MeiliSearch using Docker.\n`text\ndocker run -it --rm \\\n  -p 7700:7700 \\\n  -v $(pwd)/data.ms:/data.ms \\\n  getmeili/meilisearch`\nThat's it!\n:::info\nMeiliSearch stores data in $(pwd)/data.ms, so if you prefer to store it somewhere else, just adjust this path.\n:::\n2. Replicate Your Slack Messages to MeiliSearch\na. Set Up Airbyte\nMake sure you have Docker and Docker Compose installed. If you haven\u2019t set Docker up, follow the instructions here to set it up on your machine. Then, run the following commands:\n`bash\ngit clone https://github.com/airbytehq/airbyte.git\ncd airbyte\ndocker-compose up`\nIf you run into any problems, feel free to check out our more extensive Getting Started FAQ for help.\nOnce you see an Airbyte banner, the UI is ready to go at http://localhost:8000/. Once you have set your user preferences, you will be brought to a page that asks you to set up a source. In the next step, we'll go over how to do that.\nb. Set Up Airbyte\u2019s Slack Source Connector\nIn the Airbyte UI, select Slack from the dropdown. We provide step-by-step instructions for setting up the Slack source in Airbyte here. These will walk you through how to complete the form on this page.\n\nBy the end of these instructions, you should have created a Slack source in the Airbyte UI. For now, just add your Slack app to a single public channel (you can add it to more channels later). Only messages from that channel will be replicated.\nThe Airbyte app will now prompt you to set up a destination. Next, we will walk through how to set up MeiliSearch.\nc. Set Up Airbyte\u2019s MeiliSearch Destination Connector\nHead back to the Airbyte UI. It should still be prompting you to set up a destination. Select \"MeiliSearch\" from the dropdown. For the `host` field, set: `http://localhost:7700`. The `api_key` can be left blank.\nd. Set Up the Replication\nOn the next page, you will be asked to select which streams of data you'd like to replicate. We recommend unchecking \"files\" and \"remote files\" since you won't really be able to search them easily in this search engine.\n\nFor frequency, we recommend every 24 hours.\n3. Search MeiliSearch\nAfter the connection has been saved, Airbyte should start replicating the data immediately. When it completes you should see the following:\n\nWhen the sync is done, you can sanity check that this is all working by making a search request to MeiliSearch. Replication can take several minutes depending on the size of your Slack instance.\n`bash\ncurl 'http://localhost:7700/indexes/messages/search' --data '{ \"q\": \"<search-term>\" }'`\nFor example, I have the following message in one of the messages that I replicated: \"welcome to airbyte\".\n```bash\ncurl 'http://localhost:7700/indexes/messages/search' --data '{ \"q\": \"welcome to\" }'\n=> {\"hits\":[{\"_ab_pk\":\"7ff9a858_6959_45e7_ad6b_16f9e0e91098\",\"channel_id\":\"C01M2UUP87P\",\"client_msg_id\":\"77022f01-3846-4b9d-a6d3-120a26b2c2ac\",\"type\":\"message\",\"text\":\"welcome to airbyte.\",\"user\":\"U01AS8LGX41\",\"ts\":\"2021-02-05T17:26:01.000000Z\",\"team\":\"T01AB4DDR2N\",\"blocks\":[{\"type\":\"rich_text\"}],\"file_ids\":[],\"thread_ts\":\"1612545961.000800\"}],\"offset\":0,\"limit\":20,\"nbHits\":2,\"exhaustiveNbHits\":false,\"processingTimeMs\":21,\"query\":\"test-72\"}\n```\n4. Search via a UI\nMaking curl requests to search your Slack History is a little clunky, so we have modified the example UI that MeiliSearch provides in their docs to search through the Slack results.\nDownload (or copy and paste) this html file to your workstation. Then, open it using a browser. You should now be able to write search terms in the search bar and get results instantly!\n\n5. \"Productionizing\" Saving Slack History\nYou can find instructions for how to host Airbyte on various cloud platforms here.\nDocumentation on how to host MeiliSearch on cloud platforms can be found here.",
    "tag": "airbyte"
  },
  {
    "title": "Visualizing the Time Spent by Your Team in Zoom Calls",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/archive/examples/zoom-activity-dashboard.md",
    "content": "\ndescription: Using Airbyte and Tableau\nVisualizing the Time Spent by Your Team in Zoom Calls\nIn this article, we will show you how you can understand how much your team leverages Zoom, or spends time in meetings, in a couple of minutes. We will be using Airbyte (an open-source data integration platform) and Tableau (a business intelligence and analytics software) for this tutorial.\nHere is what we will cover:\n\nReplicating data from Zoom to a PostgreSQL database, using Airbyte\nConnecting the PostgreSQL database to Tableau\nCreating charts in Tableau with Zoom data\n\nWe will produce the following charts in Tableau:\n\nMeetings per week in a team\nHours a team spends in meetings per week\nListing of team members with the number of meetings per week and number of hours spent in meetings, ranked\nWebinars per week in a team\nHours a team spends in webinars per week\nParticipants for all webinars in a team per week\nListing of team members with the number of webinars per week and number of hours spent in meetings, ranked \n\nLet\u2019s get started by replicating Zoom data using Airbyte.\nStep 1: Replicating Zoom data to PostgreSQL\nLaunching Airbyte\nIn order to replicate Zoom data, we will need to use Airbyte\u2019s Zoom connector. To do this, you need to start off Airbyte\u2019s web app by opening up your terminal and navigating to Airbyte and running:\n`docker-compose up`\nYou can find more details about this in the Getting Started FAQ on our Discourse Forum.\nThis will start up Airbyte on `localhost:8000`; open that address in your browser to access the Airbyte dashboard.\n\nIf you haven't gone through the onboarding yet, you will be prompted to connect a source and a destination. Then just follow the instructions. If you've gone through it, then you will see the screenshot above. In the top right corner of the Airbyte dashboard, click on the + new source button to add a new Airbyte source. In the screen to set up the new source, enter the source name (we will use airbyte-zoom) and select Zoom as source type.\nChoosing Zoom as source type will cause Airbyte to display the configuration parameters needed to set up the Zoom source.\n\nThe Zoom connector for Airbyte requires you to provide it with a Zoom JWT token. Let\u2019s take a detour and look at how to obtain one from Zoom.\nObtaining a Zoom JWT Token\nTo obtain a Zoom JWT Token, login to your Zoom account and go to the Zoom Marketplace. If this is your first time in the marketplace, you will need to agree to the Zoom\u2019s marketplace terms of use.\nOnce you are in, you need to click on the Develop dropdown and then click on Build App.\n\nClicking on Build App for the first time will display a modal for you to accept the Zoom\u2019s API license and terms of use. Do accept if you agree and you will be presented with the below screen.\n\nSelect JWT as the app you want to build and click on the Create button on the card. You will be presented with a modal to enter the app name; type in `airbyte-zoom`.\n\nNext, click on the Create button on the modal.\nYou will then be taken to the App Information page of the app you just created. Fill in the required information.\n\nAfter filling in the needed information, click on the Continue button. You will be taken to the App Credentials page. Here, click on the View JWT Token dropdown.\n\nThere you can set the expiration time of the token (we will leave the default 90 minutes), and then you click on the Copy button of the JWT Token.\nAfter copying it, click on the Continue button.\n\nYou will be taken to a screen to activate Event Subscriptions. Just leave it as is, as we won\u2019t be needing Webhooks. Click on Continue, and your app should be marked as activated.\nConnecting Zoom on Airbyte\nSo let\u2019s go back to the Airbyte web UI and provide it with the JWT token we copied from our Zoom app.\nNow click on the Set up source button. You will see the below success message when the connection is made successfully.\n\nAnd you will be taken to the page to add your destination.\nConnecting PostgreSQL on Airbyte\n\nFor our destination, we will be using a PostgreSQL database, since Tableau supports PostgreSQL as a data source. Click on the add destination button, and then in the drop down click on + add a new destination. In the page that presents itself, add the destination name and choose the Postgres destination.\n\nTo supply Airbyte with the PostgreSQL configuration parameters needed to make a PostgreSQL destination, we will spin off a PostgreSQL container with Docker using the following command in our terminal.\n`docker run --rm --name airbyte-zoom-db -e POSTGRES_PASSWORD=password -v airbyte_zoom_data:/var/lib/postgresql/data -p 2000:5432 -d postgres`\nThis will spin a docker container and persist the data we will be replicating in the PostgreSQL database in a Docker volume `airbyte_zoom_data`.\nNow, let\u2019s supply the above credentials to the Airbyte UI requiring those credentials.\n\nThen click on the Set up destination button.\nAfter the connection has been made to your PostgreSQL database successfully, Airbyte will generate the schema of the data to be replicated in your database from the Zoom source.\nLeave all the fields checked.\n\nSelect a Sync frequency of manual and then click on Set up connection.\nAfter successfully making the connection, you will see your PostgreSQL destination. Click on the Launch button to start the data replication.\n\nThen click on the airbyte-zoom-destination to see the Sync page.\n\nSyncing should take a few minutes or longer depending on the size of the data being replicated. Once Airbyte is done replicating the data, you will get a succeeded status.\nThen, you can run the following SQL command on the PostgreSQL container to confirm that the sync was done successfully.\n`docker exec airbyte-zoom-db psql -U postgres -c \"SELECT * FROM public.users;\"`\nNow that we have our Zoom data replicated successfully via Airbyte, let\u2019s move on and set up Tableau to make the various visualizations and analytics we want.\nStep 2: Connect the PostgreSQL database to Tableau\nTableau helps people and organizations to get answers from their data. It\u2019s a visual analytic platform that makes it easy to explore and manage data.\nTo get started with Tableau, you can opt in for a free trial period by providing your email and clicking the DOWNLOAD FREE TRIAL button to download the Tableau desktop app. The download should automatically detect your machine type (Windows/Mac).\nGo ahead and install Tableau on your machine. After the installation is complete, you will need to fill in some more details to activate your free trial.\nOnce your activation is successful, you will see your Tableau dashboard.\n\nOn the sidebar menu under the To a Server section, click on the More\u2026 menu. You will see a list of datasource connectors you can connect Tableau with.\n\nSelect PostgreSQL and you will be presented with a connection credentials modal.\nFill in the same details of the PostgreSQL database we used as the destination in Airbyte.\n\nNext, click on the Sign In button. If the connection was made successfully, you will see the Tableau dashboard for the database you just connected.\nNote: If you are having trouble connecting PostgreSQL with Tableau, it might be because the driver Tableau comes with for PostgreSQL might not work for newer versions of PostgreSQL. You can download the JDBC driver for PostgreSQL here and follow the setup instructions.\nNow that we have replicated our Zoom data into a PostgreSQL database using Airbyte\u2019s Zoom connector, and connected Tableau with our PostgreSQL database containing our Zoom data, let\u2019s proceed to creating the charts we need to visualize the time spent by a team in Zoom calls.\nStep 3: Create the charts on Tableau with the Zoom data\nMeetings per week in a team\nTo create this chart, we will need to use the count of the meetings and the createdAt field of the meetings table. Currently, we haven\u2019t selected a table to work on in Tableau. So you will see a prompt to Drag tables here.\n\nDrag the meetings table from the sidebar onto the space with the prompt.\nNow that we have the meetings table, we can start building out the chart by clicking on Sheet 1 at the bottom left of Tableau.\n\nAs stated earlier, we need Created At, but currently it\u2019s a String data type. Let\u2019s change that by converting it to a data time. So right click on Created At, then select `ChangeDataType` and choose Date & Time. And that\u2019s it! That field is now of type Date & Time.\n\nNext, drag Created At to Columns.\n\nCurrently, we get the Created At in YEAR, but per our requirement we want them in Weeks, so right click on the YEAR(Created At) and choose Week Number.\n\nTableau should now look like this:\n\nNow, to finish up, we need to add the meetings(Count) measure Tableau already calculated for us in the Rows section. So drag meetings(Count) onto the Columns section to complete the chart.\n\nAnd now we are done with the very first chart. Let's save the sheet and create a new Dashboard that we will add this sheet to as well as the others we will be creating.\nCurrently the sheet shows Sheet 1; right click on Sheet 1 at the bottom left and rename it to Weekly Meetings.\nTo create our Dashboard, we can right click on the sheet we just renamed and choose new Dashboard. Rename the Dashboard to Zoom Dashboard and drag the sheet into it to have something like this:\n\nNow that we have this first chart out of the way, we just need to replicate most of the process we used for this one to create the other charts. Because the steps are so similar, we will mostly be showing the finished screenshots of the charts except when we need to conform to the chart requirements.\nHours a team spends in meetings per week\nFor this chart, we need the sum of the duration spent in weekly meetings. We already have a Duration field, which is currently displaying durations in minutes. We can derive a calculated field off this field since we want the duration in hours (we just need to divide the duration field by 60).\nTo do this, right click on the Duration field and select create, then click on calculatedField. Change the name to Duration in Hours, and then the calculation should be [Duration]/60. Click ok to create the field.\nSo now we can drag the Duration in Hours and Created At fields onto your sheet like so:\n\nNote: We are adding a filter on the Duration to filter out null values. You can do this by right clicking on the SUM(Duration) pill and clicking filter, then make sure the include null values checkbox is unchecked.\nParticipants for all meetings per week\nFor this chart, we will need to have a calculated field called # of meetings attended, which will be an aggregate of the counts of rows matching a particular user's email in the `report_meeting_participants` table plotted against the Created At field of the meetings table. To get this done, right click on the User Email field. Select create and click on calculatedField, then enter the title of the field as # of meetings attended. Next, enter the below formula:\n`COUNT(IF [User Email] == [User Email] THEN [Id (Report Meeting Participants)] END)`\nThen click on apply. Finally, drag the Created At fields (make sure it\u2019s on the Weekly number) and the calculated field you just created to match the below screenshot:\n\nListing of team members with the number of meetings per week and number of hours spent in meetings, ranked.\nTo get this chart, we need to create a relationship between the meetings table and the `report_meeting_participants` table. You can do this by dragging the `report_meeting_participants` table in as a source alongside the meetings table and relate both via the meeting id. Then you will be able to create a new worksheet that looks like this:\n\nNote: To achieve the ranking, we simply use the sort menu icon on the top menu bar.\nWebinars per week in a team\nThe rest of the charts will be needing the webinars and `report_webinar_participants` tables. Similar to the number of meetings per week in a team, we will be plotting the Count of webinars against the Created At property.\n\nHours a week spends in webinars per week\nFor this chart, as for the meeting\u2019s counterpart, we will get a calculated field off the Duration field to get the Webinar Duration in Hours, and then plot Created At against the Sum of Webinar Duration in Hours, as shown in the screenshot below. Note: Make sure you create a new sheet for each of these graphs.\nParticipants for all webinars per week\nThis calculation is the same as the number of participants for all meetings per week, but instead of using the meetings and `report_meeting_participants` tables, we will use the webinars and `report_webinar_participants` tables.\nAlso, the formula will now be:\n`COUNT(IF [User Email] == [User Email] THEN [Id (Report Webinar Participants)] END)`\nBelow is the chart:\n\nListing of team members with the number of webinars per week and number of hours spent in meetings, ranked\nBelow is the chart with these specs\n\nConclusion\nIn this article, we see how we can use Airbyte to get data off the Zoom API onto a PostgreSQL database, and then use that data to create some chart visualizations in Tableau.\nYou can leverage Airbyte and Tableau to produce graphs on any collaboration tool. We just used Zoom to illustrate how it can be done. Hope this is helpful!",
    "tag": "airbyte"
  },
  {
    "title": "Airbyte Protocol Docker Interface",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/understanding-airbyte/airbyte-protocol-docker.md",
    "content": "Airbyte Protocol Docker Interface\nSummary\nThe Airbyte Protocol describes a series of structs and interfaces for building data pipelines. The Protocol article describes those interfaces in language agnostic pseudocode, this article transcribes those into docker commands. Airbyte's implementation of the protocol is all done in docker. Thus, this reference is helpful for getting a more concrete look at how the Protocol is used. It can also be used as a reference for interacting with Airbyte's implementation of the Protocol.\nSource\nPseudocode:\n`spec() -> ConnectorSpecification\ncheck(Config) -> AirbyteConnectionStatus\ndiscover(Config) -> AirbyteCatalog\nread(Config, ConfiguredAirbyteCatalog, State) -> Stream<AirbyteRecordMessage | AirbyteStateMessage>`\nDocker:\n`shell\ndocker run --rm -i <source-image-name> spec\ndocker run --rm -i <source-image-name> check --config <config-file-path>\ndocker run --rm -i <source-image-name> discover --config <config-file-path>\ndocker run --rm -i <source-image-name> read --config <config-file-path> --catalog <catalog-file-path> [--state <state-file-path>] > message_stream.json`\nThe `read` command will emit a stream records to STDOUT.\nDestination\nPseudocode:\n`spec() -> ConnectorSpecification\ncheck(Config) -> AirbyteConnectionStatus\nwrite(Config, AirbyteCatalog, Stream<AirbyteMessage>(stdin)) -> Stream<AirbyteStateMessage>`\nDocker:\n`shell\ndocker run --rm -i <destination-image-name> spec\ndocker run --rm -i <destination-image-name> check --config <config-file-path>\ncat <&0 | docker run --rm -i <destination-image-name> write --config <config-file-path> --catalog <catalog-file-path>`\nThe `write` command will consume `AirbyteMessage`s from STDIN.\nI/O:\n\nConnectors receive arguments on the command line via JSON files. `e.g. --catalog catalog.json`\nThey read `AirbyteMessage`s from STDIN. The destination `write` action is the only command that consumes `AirbyteMessage`s.\n",
    "tag": "airbyte"
  },
  {
    "title": "Operations",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/understanding-airbyte/operations.md",
    "content": "Operations\nAirbyte connections support configuring additional transformations that execute after the sync. Useful applications could be:\n\nCustomized normalization to better fit the requirements of your own business context.\nBusiness transformations from a technical data representation into a more logical and business oriented data structure. This can facilitate usage by end-users, non-technical operators, and executives looking to generate Business Intelligence dashboards and reports.\nData Quality, performance optimization, alerting and monitoring, etc.\nIntegration with other tools from your data stack (orchestration, data visualization, etc.)\n\nSupported Operations\ndbt transformations\n- git repository url:\nA url to a git repository to (shallow) clone the latest dbt project code from.\nThe project versioned in the repository is expected to:\n\nbe a valid dbt package with a `dbt_project.yml` file at its root.\nhave a `dbt_project.yml` with a \"profile\" name declared as described here.\n\nWhen using the dbt CLI, dbt checks your `profiles.yml` file for a profile with the same name. A profile contains all the details required to connect to your data warehouse. This file generally lives outside of your dbt project to avoid sensitive credentials being checked in to version control. Therefore, a `profiles.yml` will be generated according to the configured destination from the Airbyte UI.\nNote that if you prefer to use your own `profiles.yml` stored in the git repository or in the Docker image, then you can specify an override with `--profiles-dir=<path-to-my-profiles-yml>` in the dbt CLI arguments.\n- git repository branch (optional):\nThe name of the branch to use when cloning the git repository. If left empty, git will use the default branch of your repository.\n- docker image:\nA Docker image and tag to run dbt commands from. The Docker image should have `/bin/bash` and `dbt` installed for this operation type to work.\nA typical value for this field would be for example: `fishtownanalytics/dbt:1.0.0` from dbt dockerhub.\nThis field lets you configure the version of dbt that your custom dbt project requires and the loading of additional software and packages necessary for your transformations (other than your dbt `packages.yml` file).\n- dbt cli arguments\nThis operation type is aimed at running the dbt cli.\nA typical value for this field would be \"run\" and the actual command invoked would as a result be: `dbt run` in the docker container.\nOne thing to consider is that dbt allows for vast configuration of the run command, for example, allowing you to select a subset of models. You can find the dbt reference docs which describes this set of available commands and options.\nFuture Operations\n\nDocker/Script operations: Execute a generic script in a custom Docker container.\nWebhook operations: Trigger API or hooks from other providers.\nAirflow operations: To use a specialized orchestration tool that lets you schedule and manage more advanced/complex sequences of operations in your sync workflow.\n\nGoing Further\nIn the meantime, please feel free to react, comment, and share your thoughts/use cases with us. We would be glad to hear your feedback and ideas as they will help shape the next set of features and our roadmap for the future. You can head to our GitHub and participate in the corresponding issue or discussions. Thank you!",
    "tag": "airbyte"
  },
  {
    "title": "Workers & Jobs",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/understanding-airbyte/jobs.md",
    "content": "Workers & Jobs\nIn Airbyte, all interactions with connectors are run as jobs performed by a Worker. Each job has a corresponding worker:\n\nSpec worker: retrieves the specification of a connector (the inputs needed to run this connector)\nCheck connection worker: verifies that the inputs to a connector are valid and can be used to run a sync\nDiscovery worker: retrieves the schema of the source underlying a connector\nSync worker, used to sync data between a source and destination\n\nThus, there are generally 4 types of workers.\nNote: Workers here refers to Airbyte workers. Temporal, which Airbyte uses under the hood for scheduling, has its own worker concept. This distinction is important.\nJob State Machine\nJobs have the following state machine.\n\nImage Source\nWorker Responsibilities\nThe worker has the following responsibilities.\n\nHandle the process lifecycle for job-related processes. This includes starting, monitoring and shutting down processes.\nFacilitate message passing to or from various processes, if required. (more on this below).\nHandle job-relation operational work such as:\nBasic schema validation.\nReturning job output, including any error messages. (See Airbyte Specification to understand the output of each worker type.)\nTelemetry work e.g. tracking the number and size of records within a sync.\n\nConceptually, workers contain the complexity of all non-connector-related job operations. This lets each connector be as simple as possible.\nWorker Types\nThere are 2 flavors of workers: \n\nSynchronous Job Worker - Workers that interact with a single connector (e.g. spec, check, discover).\n\nThe worker extracts data from the connector and reports it to the scheduler.  It does this by listening to the connector's STDOUT.\n   These jobs are synchronous as they are part of the configuration process and need to be immediately run to provide a good user experience. These are also all lightweight operations.\n\nAsynchronous Job Worker - Workers that interact with 2 connectors (e.g. sync, reset)\n\nThe worker passes data (via record messages) from the source to the destination. It does this by listening on STDOUT of the source and writing to STDIN on the destination.\n   These jobs are asynchronous as they are often long-running resource-intensive processes. They are decoupled from the rest of the platform to simplify development and operation.\nFor more information on the schema of the messages that are passed, refer to Airbyte Specification.\nWorker-Job Architecture\nThis section will depict the worker-job architecture as discussed above. Only the 2-connector version is shown. The single connector version is the same with one side removed.\nThe source process should automatically exit after passing all of its messages. Similarly, the destination process shutdowns after receiving all records. Each process is given a shutdown grace period. The worker forces shutdown if this is exceeded.\n\nImage Source\nSee the architecture overview for more information about workers.\nDeployment Types\nUp to now, the term 'processes' has been used loosely. This section will describe this in more detail.\nAirbyte offers two deployment types. The underlying process implementations differ accordingly.\n\nThe Docker deployment - Each process is a local process backed by a Docker container. As all processes are local, process communication is per standard unix pipes.\nThe Kubernetes deployment - Each process is a backed by a Kubernetes pod. As Kubernetes does not make process-locality guarantees, Airbyte has implemented mechanisms to hide the remote process execution.\n   See this blogpost for more details.\n\nDecoupling Worker and Job Processes\nWorkers being responsible for all non-connector-related job operations means multiple jobs are operationally dependent on a single worker process.\nThere are two downsides to this:\n1. Any issues to the parent worker process affects all job processes launched by the worker.\n2. Unnecessary complexity of vertically scaling the worker process to deal with IO and processing requirements from multiple jobs.\nThis gives us a potentially brittle system component that can be operationally tricky to manage. For example, since redeploying Airbyte terminates all worker processes, all running jobs are also terminated.\nThe Container Orchestrator was introduced to solve this.\nContainer Orchestrator\nWhen enabled, workers launch the Container Orchestrator process.\nThe worker process delegates the above listed responsibilities to the orchestrator process.\nThis decoupling introduces a new need for workers to track the orchestrator's, and the job's, state. This is done via a shared Cloud Storage store.\nBrief description of how this works,\n1. Workers constantly poll the Cloud Storage location for job state.\n2. As an Orchestrator process executes, it writes status marker files to the Cloud Storage location i.e. `NOT_STARTED`, `INITIALIZING`, `RUNNING`, `SUCCESS`, `FAILURE`.\n3. If the Orchestrator process runs into issues at any point, it writes a `FAILURE`.\n4. If the Orchestrator process succeeds, it writes a job summary as part of the `SUCCESS` marker file.\nThe Cloud Storage store is treated as the source-of-truth of execution state.\nThe Container Orchestrator is only available for Airbyte Kubernetes today and automatically enabled when running the Airbyte Helm charts/Kustomize deploys.\nUsers running Airbyte Docker should be aware of the above pitfalls.\nConfiguring Workers\nDetails on configuring workers can be found here.\nWorker Parallization\nAirbyte exposes the following environment variable to change the maximum number of each type of worker allowed to run in parallel. \nTweaking these values might help you run more jobs in parallel and increase the workload of your Airbyte instance: \n* `MAX_SPEC_WORKERS`: Maximum number of Spec workers allowed to run in parallel.\n* `MAX_CHECK_WORKERS`: Maximum number of Check connection workers allowed to run in parallel.\n* `MAX_DISCOVERY_WORKERS`: Maximum number of Discovery workers allowed to run in parallel.\n* `MAX_SYNC_WORKERS`: Maximum number of Sync workers allowed to run in parallel.",
    "tag": "airbyte"
  },
  {
    "title": "Data Types in Records",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/understanding-airbyte/supported-data-types.md",
    "content": "Data Types in Records\nAirbyteRecords are required to conform to the Airbyte type system. This means that all sources must produce schemas and records within these types, and all destinations must handle records that conform to this type system.\nBecause Airbyte's interfaces are JSON-based, this type system is realized using JSON schemas. In order to work around some limitations of JSON schemas, we define our own types - see well_known_types.yaml. Sources should use `$ref` to reference these types, rather than directly defining JsonSchema entries.\nIn an older version of the protocol, we relied on an `airbyte_type` property in schemas. This has been replaced by the well-known type schemas. All \"old-style\" types map onto well-known types. For example, a legacy connector producing a field of type `{\"type\": \"string\", \"airbyte_type\": \"timestamp_with_timezone\"}` is treated as producing `{\"$ref\": \"WellKnownTypes.json#/definitions/TimestampWithTimezone\"}`.\nThis type system does not (generally) constrain values. The exception is in numeric types; `integer` and `number` fields must be representable within 64-bit primitives.\nThe types\nThis table summarizes the available types. See the Specific Types section for explanation of optional parameters.\n| Airbyte type                                                   | JSON Schema                                                              | Examples                                                                        |\n| -------------------------------------------------------------- | ------------------------------------------------------------------------ | ------------------------------------------------------------------------------- |\n| String                                                         | `{\"$ref\": \"WellKnownTypes.json#/definitions/String\"}`                     | `\"foo bar\"`                                                                     |\n| Binary data, represented as a base64 string                    | `{\"$ref\": \"WellKnownTypes.json#/definitions/BinaryData\"}`                 | `\"Zm9vIGJhcgo=\"`                                                                |\n| Boolean                                                        | `{\"$ref\": \"WellKnownTypes.json#/definitions/Boolean\"}`                    | `true` or `false`                                                               |\n| Date                                                           | `{\"$ref\": \"WellKnownTypes.json#/definitions/Date\"}`                       | `\"2021-01-23\"`, `\"2021-01-23 BC\"`                                               |\n| Timestamp with timezone                                        | `{\"$ref\": \"WellKnownTypes.json#/definitions/TimestampWithTimezone\"}`      | `\"2022-11-22T01:23:45.123456+05:00\"`, `\"2022-11-22T01:23:45Z BC\"`               |\n| Timestamp without timezone                                     | `{\"$ref\": \"WellKnownTypes.json#/definitions/TimestampWithoutTimezone\"}`   | `\"2022-11-22T01:23:45\"`, `\"2022-11-22T01:23:45.123456 BC\"`                      |\n| Time with timezone                                             | `{\"$ref\": \"WellKnownTypes.json#/definitions/TimeWithTimezone\"}`           | `\"01:23:45.123456+05:00\"`, `\"01:23:45Z\"`                                        |\n| Time without timezone                                          | `{\"$ref\": \"WellKnownTypes.json#/definitions/TimeWithoutTimezone\"}`        | `\"01:23:45.123456\"`, `\"01:23:45\"`                                               |\n| Integer                                                        | `{\"$ref\": \"WellKnownTypes.json#/definitions/Integer\"}`                    | `42`, `NaN`, `Infinity`, `-Infinity`                                            |\n| Number                                                         | `{\"$ref\": \"WellKnownTypes.json#/definitions/Number\"}`                     | `1234.56`, `NaN`, `Infinity`, `-Infinity`                                       |\n| Array                                                          | `{\"type\": \"array\"}`; optionally `items` and `additionalItems`            | `[1, 2, 3]`                                                                     |\n| Object                                                         | `{\"type\": \"object\"}`; optionally `properties` and `additionalProperties` | `{\"foo\": \"bar\"}`                                                                |\n| Union                                                          | `{\"anyOf\": [...]}` or `{\"oneOf\": [...]}`                                 |                                                                                 |\nNote that some of these may be destination-dependent. For example, different warehouses may impose different limits on string column length.\nRecord structure\nAs a reminder, sources expose a `discover` command, which returns a list of AirbyteStreams, and a `read` method, which emits a series of AirbyteRecordMessages. The type system determines what a valid `json_schema` is for an `AirbyteStream`, which in turn dictates what messages `read` is allowed to emit.\nFor example, a source could produce this `AirbyteStream` (remember that the `json_schema` must declare `\"type\": \"object\"` at the top level):\n`json\n{\n  \"name\": \"users\",\n  \"json_schema\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"username\": {\n        \"$ref\": \"WellKnownTypes.json#/definitions/String\"\n      },\n      \"age\": {\n        \"$ref\": \"WellKnownTypes.json#/definitions/Integer\"\n      },\n      \"appointments\": {\n        \"type\": \"array\",\n        \"items\": {\n          \"$ref\": \"WellKnownTypes.json#/definitions/TimestampWithTimezone\"\n        }\n      }\n    }\n  }\n}`\nAlong with this `AirbyteRecordMessage` (observe that the `data` field conforms to the `json_schema` from the stream):\n`json\n{\n  \"stream\": \"users\",\n  \"data\": {\n    \"username\": \"someone42\",\n    \"age\": 84,\n    \"appointments\": [\"2021-11-22T01:23:45+00:00\", \"2022-01-22T14:00:00+00:00\"]\n  },\n  \"emitted_at\": 1623861660\n}`\nThe top-level `object` must conform to the type system. This means that all of the fields must also conform to the type system.\nNulls\nMany sources cannot guarantee that all fields are present on all records. In these cases, sources should simply not list them as `required` fields. In most cases, sources do not need to list fields as required; by default, all fields are treated as nullable.\nUnsupported types\nAs an escape hatch, destinations which cannot handle a certain type should just fall back to treating those values as strings. For example, let's say a source discovers a stream with this schema:\n`json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"appointments\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"$ref\": \"WellKnownTypes.json#/definitions/TimestampWithTimezone\"\n      }\n    }\n  }\n}`\nAlong with records which contain data that looks like this:\n`json\n{\"appointments\": [\"2021-11-22T01:23:45+00:00\", \"2022-01-22T14:00:00+00:00\"]}`\nThe user then connects this source to a destination that cannot handle `array` fields. The destination connector should simply JSON-serialize the array back to a string when pushing data into the end platform. In other words, the destination connector should behave as though the source declared this schema:\n`json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"appointments\": {\n      \"type\": \"string\"\n    }\n  }\n}`\nAnd emitted this record:\n`json\n{\"appointments\": \"[\\\"2021-11-22T01:23:45+00:00\\\", \\\"2022-01-22T14:00:00+00:00\\\"]\"}`\nOf course, destinations are free to choose the most convenient/reasonable stringification for any given value. JSON serialization is just one possible strategy.\nSpecific types\nBoolean\nAirbyte boolean type represents one of the two values `true` or `false` and they are are lower case. Note that values that evaluate to true or false such as data type String `\"true\"` or `\"false\"` or Integer like `1` or `0` are not accepted by the Schema.\nDates and timestamps\nAirbyte has five temporal types: `date`, `timestamp_with_timezone`, `timestamp_without_timezone`, `time_with_timezone`, and `time_without_timezone`. These are represented as strings with specific `format` (either `date` or `date-time`).\nHowever, JSON schema does not have a built-in way to indicate whether a field includes timezone information. For example, given this JsonSchema:\n`json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"created_at\": {\n      \"type\": \"string\",\n      \"format\": \"date-time\"\n    }\n  }\n}`\nBoth `{\"created_at\": \"2021-11-22T01:23:45+00:00\"}` and `{\"created_at\": \"2021-11-22T01:23:45\"}` are valid records.\nThe protocol's type definitions resolve this ambiguity; sources producing timestamp-ish fields must choose either `TimestampWithTimezone` or `TimestampWithoutTimezone` (or time with/without timezone).\nAll of these must be represented as RFC 3339\u00a75.6 strings, extended with BC era support. See the type definition descriptions for specifics.\nNumeric values\nIntegers are extended to accept infinity/-infinity/NaN values. Most sources will not actually produce those values, and destinations may not fully support them.\n64-bit integers and floating-point numbers (AKA `long` and `double`) cannot represent every number in existence. Sources should use the string type if their fields may exceed `int64`/`float64` ranges.\nArrays\nArrays contain 0 or more items, which must have a defined type. These types should also conform to the type system. Arrays may require that all of their elements be the same type (`\"items\": {whatever type...}`), or they may require specific types for the first N entries (`\"items\": [{first type...}, {second type...}, ... , {Nth type...}]`, AKA tuple-type).\nTuple-typed arrays can configure the type of any additional elements using the `additionalItems` field; by default, any type is allowed. They may also pass a boolean to enable/disable additional elements, with `\"additionalItems\": true` being equivalent to `\"additionalItems\": {\"$ref\": \"WellKnownTypes.json#/definitions/String\"}` and `\"additionalItems\": false` meaning that only the tuple-defined items are allowed.\nDestinations may have a difficult time supporting tuple-typed arrays without very specific handling, and as such are permitted to somewhat loosen their requirements. For example, many Avro-based destinations simply declare an array of a union of all allowed types, rather than requiring the correct type in each position of the array.\nObjects\nAs with arrays, objects may declare `properties`, each of which should have a type which conforms to the type system. Objects may additionally accept `additionalProperties`, as `true` (any type is acceptable), a specific type (all additional properties must be of that type), or `false` (no additonal properties are allowed).\nUnions\nIn some cases, sources may want to use multiple types for the same field. For example, a user might have a property which holds either an object, or a `string` explanation of why that data is missing. This is supported with JSON schema's  `oneOf` and `anyOf` types.\nNote that JsonSchema's `allOf` combining structure is not accepted within the protocol, because all of the protocol type definitions are mutually exclusive.\nUntyped values",
    "tag": "airbyte"
  },
  {
    "title": "Namespaces",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/understanding-airbyte/namespaces.md",
    "content": "Namespaces\nHigh-Level Overview\n:::info\nThe high-level overview contains all the information you need to use Namespaces when pulling from APIs. Information past that can be read for advanced or educational purposes.\n:::\nWhen looking through our connector docs, you'll notice that some sources and destinations support \"Namespaces.\" These allow you to organize and separate your data into groups in the destination if the destination supports it. In most cases, namespaces are schemas in the database you're replicating to. If your desired destination doesn't support it, you can ignore this feature.\nNote that this is the location that both your normalized and raw data will get written to. Your raw data will show up with the prefix `_airbyte_raw_` in the namespace you define. If you don't enable basic normalization, you will only receive the raw tables.\nIf only your destination supports namespaces, you have two simple options. This is the most likely case, as all HTTP APIs currently don't support Namespaces.\n\nMirror Destination Settings - Replicate to the default namespace in the destination, which will differ based on your destination.\nCustom Format - Create a \"Custom Format\" to rename the namespace that your data will be replicated into.\n\nIf both your desired source and destination support namespaces, you're likely using a more advanced use case with a database as a source, so continue reading.\nWhat is a Namespace?\nTechnical systems often group their underlying data into namespaces with each namespace's data isolated from another namespace. This isolation allows for better organisation and flexibility, leading to better usability.\nAn example of a namespace is the RDMS's `schema` concept. Some common use cases for schemas are enforcing permissions, segregating test and production data and general data organisation.\nSyncing\nThe Airbyte Protocol supports namespaces and allows Sources to define namespaces, and Destinations to write to various namespaces.\nIf the Source does not support namespaces, the data will be replicated into the Destination's default namespace. For databases, the default namespace is the schema provided in the destination configuration.\nIf the Destination does not support namespaces, the namespace field is ignored.\nDestination namespace configuration\nAs part of the connections sync settings, it is possible to configure the namespace used by: 1. destination connectors: to store the `_airbyte_raw_*` tables. 2. basic normalization: to store the final normalized tables.\nNote that custom transformation outputs are not affected by the namespace settings from Airbyte: It is up to the configuration of the custom dbt project, and how it is written to handle its custom schemas. The default target schema for dbt in this case, will always be the destination namespace.\nAvailable options for namespace configurations are:\n- Mirror source structure\nSome sources (such as databases based on JDBC for example) are providing namespace information from which a stream has been extracted. Whenever a source is able to fill this field in the catalog.json file, the destination will try to reproduce exactly the same namespace when this configuration is set. For sources or streams where the source namespace is not known, the behavior will fall back to the \"Destination Connector settings\".\n- Destination connector settings\nAll stream will be replicated and store in the default namespace defined on the destination settings page. In the destinations, namespace refers to:\n| Destination Connector | Namespace setting |\n| :--- | :--- |\n| BigQuery | dataset |\n| MSSQL | schema |\n| MySql | database |\n| Oracle DB | schema |\n| Postgres | schema |\n| Redshift | schema |\n| Snowflake | schema |\n| S3 | path prefix |\n- Custom format\nWhen replicating multiple sources into the same destination, conflicts on tables being overwritten by syncs can occur.\nFor example, a Github source can be replicated into a \"github\" schema. But if we have multiple connections to different GitHub repositories (similar in multi-tenant scenarios):\n\nwe'd probably wish to keep the same table names (to keep consistent queries downstream)\nbut store them in different namespaces (to avoid mixing data from different \"tenants\")\n\nTo solve this, we can either:\n\nuse a specific namespace for each connection, thus this option of custom format.\nor, use prefix to stream names as described below.\n\nNote that we can use a template format string using variables that will be resolved during replication as follow:\n\n`${SOURCE_NAMESPACE}`: will be replaced by the namespace provided by the source if available\n\nExamples\nThe following table summarises how this works. We assume an example of replication configurations between a Postgres Source and Snowflake Destination (with settings of schema = \"my_schema\"):\n| Namespace Configuration | Source Namespace | Source Table Name | Destination Namespace | Destination Table Name |\n| :--- | :--- | :--- | :--- | :--- |\n| Mirror source structure | public | my_table | public | my_table |\n| Mirror source structure |  | my_table | my_schema | my_table |\n| Destination connector settings | public | my_table | my_schema | my_table |\n| Destination connector settings |  | my_table | my_schema | my_table |\n| Custom format = \"custom\" | public | my_table | custom | my_table |\n| Custom format = \"${SOURCE_NAMESPACE}\" | public | my_table | public | my_table |\n| Custom format = \"my_${SOURCE_NAMESPACE}_schema\" | public | my_table | my_public_schema | my_table |\n| Custom format = \"   \" | public | my_table | my_schema | my_table |\nRequirements\n\nBoth Source and Destination connectors need to support namespaces.\nRelevant Source and Destination connectors need to be at least version `0.3.0` or later.\nAirbyte version `0.21.0-alpha` or later.\n\nCurrent Support\nSources\n\nMSSQL\nMYSQL\nOracle DB\nPostgres\nRedshift\n\nDestination\n\nBigQuery\nMSSQL\nMySql\nOracle DB\nPostgres\nRedshift\nSnowflake\nS3\n",
    "tag": "airbyte"
  },
  {
    "title": "Airbyte Databases Data Catalog",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/understanding-airbyte/database-data-catalog.md",
    "content": "Airbyte Databases Data Catalog\nConfig Database\n\n`workspace`\nEach record represents a logical workspace for an Airbyte user. In the open-source version of the product, only one workspace is allowed.\n`actor_definition`\nEach record represents a connector that Airbyte supports, e.g. Postgres. This table represents all the connectors that is supported by the current running platform.\nThe `actor_type` column tells us whether the record represents a Source or a Destination.\nThe `spec` column is a JSON blob. The schema of this JSON blob matches the spec model in the Airbyte Protocol. Because the protocol object is JSON, this has to be a JSON blob.\nThe `release_stage` describes the certification level of the connector (e.g. Alpha, Beta, Generally Available).\nThe `docker_repository` field is the name of the docker image associated with the connector definition. `docker_image_tag` is the tag of the docker image and the version of the connector definition.\nThe `source_type` field is only used for Sources, and represents the category of the connector definition (e.g. API, Database).\nThe `resource_requirements` field sets a default resource requirement for any connector of this type. This overrides the default we set for all connector definitions, and it can be overridden by a connection-specific resource requirement. The column is a JSON blob with the schema defined in ActorDefinitionResourceRequirements.yaml\nThe `public` boolean column, describes if a connector is available to all workspaces or not. For non, `public` connector definitions, they can be provisioned to a workspace using the `actor_definition_workspace_grant` table. `custom` means that the connector is written by a user of the platform (and not packaged into the Airbyte product).\nEach record contains additional metadata and display data about a connector (e.g. `name` and `icon`), and we should add additional metadata here over time.\n`actor_definition_workspace_grant`\nEach record represents provisioning a non `public` connector definition to a workspace.\ntodo (cgardens) - should this table have a `created_at` column?\n`actor`\nEach record represents a configured connector. e.g. A Postgres connector configured to pull data from my database.\nThe `actor_type` column tells us whether the record represents a Source or a Destination.\nThe `actor_definition_id` column is a foreign key to the connector definition that this record is implementing.\nThe `configuration` column is a JSON blob. The schema of this JSON blob matches the schema specified in the `spec` column in the `connectionSpecification` field of the JSON blob. Keep in mind this schema is specific to each connector (e.g. the schema of Postgres and Salesforce are different), which is why this column has to be a JSON blob.\n`actor_catalog`\nEach record contains a catalog for an actor. The records in this table are meant to be immutable.\nThe `catalog` column is a JSON blob. The schema of this JSON blob matches the catalog model in the Airbyte Protocol. Because the protocol object is JSON, this has to be a JSON blob. The `catalog_hash` column is a 32-bit murmur3 hash ( x86 variant) of the `catalog` field to make comparisons easier.\ntodo (cgardens) - should we remove the `modified_at` column? These records should be immutable.\n`actor_catalog_fetch_event`\nEach record represents an attempt to fetch the catalog for an actor. The records in this table are meant to be immutable.\nThe `actor_id` column represents the actor that the catalog is being fetched for. The `config_hash` represents a hash (32-bit murmur3 hash - x86 variant) of the `configuration` column of that actor, at the time the attempt to fetch occurred.\nThe `catalog_id` is a foreign key to the `actor_catalog` table. It represents the catalog fetched by this attempt. We use the foreign key, because the catalogs are often large and often multiple fetch events result in retrieving the same catalog. Also understanding how often the same catalog is fetched is interesting from a product analytics point of view.\nThe `actor_version` column represents the `actor_definition` version that was in use when the fetch event happened. This column is needed, because while we can infer the `actor_definition` from the foreign key relationship with the `actor` table, we cannot do the same for the version, as that can change over time.\ntodo (cgardens) - should we remove the `modified_at` column? These records should be immutable.\n`connection`\nEach record in this table configures a connection (`source_id`, `destination_id`, and relevant configuration).\nThe `resource_requirements` field sets a default resource requirement for the connection. This overrides the default we set for all connector definitions and the default set for the connector definitions. The column is a JSON blob with the schema defined in ResourceRequirements.yaml.\nThe `source_catalog_id` column is a foreign key to the `sourc_catalog` table and represents the catalog that was used to configure the connection. This should not be confused with the `catalog` column which contains the ConfiguredCatalog for the connection.\nThe `schedule_type` column defines what type of schedule is being used. If the `type` is manual, then `schedule_data` will be null. Otherwise, `schedule_data` column is a JSON blob with the schema of StandardSync#scheduleData that defines the actual schedule. The columns `manual` and `schedule` are deprecated and should be ignored (they will be dropped soon).\nThe `namespace_type` column configures whether the namespace for the connection should use that defined by the source, the destination, or a user-defined format (`custom`). If `custom` the `namespace_format` column defines the string that will be used as the namespace.\nThe `status` column describes the activity level of the connector: `active` - current schedule is respected, `inactive` - current schedule is ignored (the connection does not run) but it could be switched back to active, and `deprecated` - the connection is permanently off (cannot be moved to active or inactive).\n`state`\nThe `state` table represents the current (last) state for a connection. For a connection with `stream` state, there will be a record per stream. For a connection with `global` state, there will be a record per stream and an additional record to store the shared (global) state. For a connection with `legacy` state, there will be one record per connection.\nIn the `stream` and `global` state cases, the `stream_name` and `namespace` columns contains the name of the stream whose state is represented by that record. For the shared state in global `stream_name` and `namespace` will be null.\nThe `state` column contains the state JSON blob. Depending on the type of the connection, the schema of the blob will be different.\n`stream` - for this type, this column is a JSON blob that is a blackbox to the platform and known only to the connector that generated it.\n`global` - for this type, this column is a JSON blob that is a blackbox to the platform and known only to the connector that generated it. This is true for both the states for each stream and the shared state.\n`legacy` - for this type, this column is a JSON blob with a top-level key called `state`. Within that `state` is a blackbox to the platform and known only to the connector that generated it.\n\n\nThe `type` column describes the type of the state of the row. type can be `STREAM`, `GLOBAL` or `LEGACY`.\nThe connection_id is a foreign key to the connection for which we are tracking state.\n`stream_reset`\nEach record in this table represents a stream in a connection that is enqueued to be reset or is currently being reset. It can be thought of as a queue. Once the stream is reset, the record is removed from the table.\n`operation`\nThe `operation` table transformations for a connection beyond the raw output produced by the destination. The two options are: `normalization`, which outputs Airbyte's basic normalization. The second is `dbt`, which allows a user to configure their own custom dbt transformation. A connection can have multiple operations (e.g. it can do `normalization` and `dbt`).\nIf the `operation` is `dbt`, then the `operator_dbt` column will be populated with a JSON blob with the schema from OperatorDbt.\nIf the `operation` is `normalization`, then the `operator_dbt` column will be populated with a JSON blob with the scehma from OperatorNormalization.\nOperations are scoped by workspace, using the `workspace_id` column.\n`connection_operation`\nThis table joins the `operation` table to the `connection` for which it is configured. \n`workspace_service_account`\nThis table is a WIP for an unfinished feature.\n`actor_oauth_parameter`\nThe name of this table is misleading. It refers to parameters to be used for any instance of an `actor_definition` (not an `actor`) within a given workspace. For OAuth, the model is that a user is provisioning access to their data to a third party tool (in this case the Airbyte Platform). Each record represents information (e.g. client id, client secret) for that third party that is getting access. \nThese parameters can be scoped by workspace. If `workspace_id` is not present, then the scope of the parameters is to the whole deployment of the platform (e.g. all workspaces).\nThe `actor_type` column tells us whether the record represents a Source or a Destination.\nThe `configuration` column is a JSON blob. The schema of this JSON blob matches the schema specified in the `spec` column in the `advanced_auth` field of the JSON blob. Keep in mind this schema is specific to each connector (e.g. the schema of Hubspot and Salesforce are different), which is why this column has to be a JSON blob.\n`secrets`\nThis table is used to store secrets in open-source versions of the platform that have not set some other secrets store. This table allows us to use the same code path for secrets handling regardless of whether an external secrets store is set or not. This table is used by default for the open-source product.\n`airbyte_configs_migrations` is metadata table used by Flyway (our database migration tool). It is not used for any application use cases.\n`airbyte_configs`\nLegacy table for config storage. Should be dropped.\n\nJobs Database\n\n`jobs`\nEach record in this table represents a job.\nThe `config_type` column captures the type of job. We only make jobs for `sync` and `reset` (we do not use them for `spec`, `check`, `discover`).\nA job represents an attempt to use a connector (or a pair of connectors). The goal of this model is to capture the input of that run. A job can have multiple attempts (see the `attempts` table). The guarantee across all attempts is that the input into each attempt will be the same.\nThat input is captured in the `config` column. This column is a JSON Blob with the schema of a JobConfig. Only `sync` and `resetConnection` are ever used in that model.\nThe other top-level fields are vestigial from when `spec`, `check`, `discover` were used in this model (we will eventually remove them).\n\n\nThe `scope` column contains the `connection_id` for the relevant connection of the job.\nContext: It is called `scope` and not `connection_id`, because, this table was originally used for `spec`, `check`, and `discover`, and in those cases the `scope` referred to the relevant actor or actor definition. At this point the scope is always a `connection_id`.\n\n\nThe `status` column contains the job status. The lifecycle of a job is explained in detail in the Jobs & Workers documentation.\n`attempts`\nEach record in this table represents an attempt.\nEach attempt belongs to a job--this is captured by the `job_id` column. All attempts for a job will run on the same input.\nThe `id` column is a unique id across all attempts while the `attempt_number` is an ascending number of the attempts for a job.\nThe output of each attempt, however, can be different. The `output` column is a JSON blob with the schema of a JobOutput. Only `sync` is used in that model. Reset jobs will also use the `sync` field, because under the hood `reset` jobs end up just doing a `sync` with special inputs. This object contains all the output info for a sync including stats on how much data was moved.\nThe other top-level fields are vestigial from when `spec`, `check`, `discover` were used in this model (we will eventually remove them).\n\n\nThe `status` column contains the attempt status. The lifecycle of a job / attempt is explained in detail in the Jobs & Workers documentation.\nIf the attempt fails, the `failure_summary` column will be populated. The column is a JSON blob with the schema of AttemptFailureReason.\nThe `log_path` column captures where logs for the attempt will be written.\n`created_at`, `started_at`, and `ended_at` track the run time.\nThe `temporal_workflow_id` column keeps track of what temporal execution is associated with the attempt.\n`airbyte_metadata`\nThis table is a key-value store for various metadata about the platform. It is used to track information about what version the platform is currently on as well as tracking the upgrade history.\nLogically it does not make a lot of sense that it is in the jobs db. It would make sense if it were either in its own dbs or in the config dbs.\nThe only two columns are `key` and `value`. It is truly just a key-value store.\n",
    "tag": "airbyte"
  },
  {
    "title": "Technical Stack",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/understanding-airbyte/tech-stack.md",
    "content": "Technical Stack\nAirbyte Core Backend\n\nJava 17\nFramework: Jersey\nAPI: OAS3\nDatabases: PostgreSQL\nUnit & E2E testing: JUnit 5\nOrchestration: Temporal\n\nConnectors\nConnectors can be written in any language. However the most common languages are:\n\nPython 3.9.0\nJava 17\n\nFrontend\n\nNode.js 16\nTypeScript\nWeb Framework/Library: React\n\nAdditional Tools\n\nCI/CD: GitHub Actions\nContainerization: Docker and Docker Compose\nLinter (Frontend): ESLint\nFormatter (Frontend): Prettier\nFormatter (Backend): Spotless\n\nFAQ\nWhy do we write most destination/database connectors in Java?\nJDBC makes writing reusable database connector frameworks fairly easy, saving us a lot of development time.\nWhy are most REST API connectors written in Python?\nMost contributors felt comfortable writing in Python, so we created a Python CDK to accelerate this development. You can write a connector from scratch in any language as long as it follows the Airbyte Specification.\nWhy did we choose to build the server with Java?\nSimply put, the team has more experience writing production Java code.\nWhy do we use Temporal for orchestration?\nTemporal solves the two major hurdles that exist in orchestrating hundreds to thousands of jobs simultaneously: scaling state management and proper queue management. Temporal solves this by offering primitives that allow serialising the jobs' current runtime memory into a DB. Since a job's entire state is stored, it's trivial to recover from failures, and it's easy to determine if a job was assigned correctly.",
    "tag": "airbyte"
  },
  {
    "title": "Json to Avro Conversion for Blob Storage Destinations",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/understanding-airbyte/json-avro-conversion.md",
    "content": "Json to Avro Conversion for Blob Storage Destinations\nWhen an Airbyte data stream is synced to the Avro or Parquet format (e.g. Parquet on S3), the source Json schema is converted to an Avro schema, then the Json object is converted to an Avro record based on the Avro schema (and further to Parquet if necessary). Because the data stream can come from any data source, the Json to Avro conversion process has the following rules and limitations.\nConversion Rules\nType Mapping\nJson schema types are mapped to Avro types as follows:\n| Json Data Type | Avro Data Type |\n| :---: | :---: |\n| string | string |\n| number | double |\n| integer | int |\n| boolean | boolean |\n| null | null |\n| object | record |\n| array | array |\nNullable Fields\nAll fields are nullable. For example, a `string` Json field will be typed as `[\"null\", \"string\"]` in Avro. This is necessary because the incoming data stream may have optional fields.\nBuilt-in Formats\nThe following built-in Json formats will be mapped to Avro logical types.\n| Json Type | Json Built-in Format | Avro Type | Avro Logical Type | Meaning |\n| --- | --- | --- | --- | --- |\n| `string` | `date` | `int` | `date` | Number of epoch days from 1970-01-01 (reference). |\n| `string` | `time` | `long` | `time-micros` | Number of microseconds after midnight (reference). |\n| `string` | `date-time` | `long` | `timestamp-micros` | Number of microseconds from `1970-01-01T00:00:00Z` (reference). |\nIn the final Avro schema, these Avro logical type fields will be a union of the logical type and string. The rationale is that the incoming Json objects may contain invalid Json built-in formats. If that's the case, and the conversion from the Json built-in format to Avro built-in format fails, the field will fall back to a string. The extra string type can cause problem for some users in the destination. We may re-evaluate this conversion rule in the future. This issue is tracked here.\nDate\nThe date logical type represents a date within the calendar, with no reference to a particular time zone or time of day.\nA date logical type annotates an Avro int, where the int stores the number of days from the unix epoch, 1 January 1970 (ISO calendar).\n`json\n{\n  \"type\": \"string\",\n  \"format\": \"date\"\n}`\nis mapped to:\n`json\n{\n  \"type\": \"int\",\n  \"logicalType\": \"date\"\n}`\nand the Avro schema is:\n`json\n{\n  \"type\": [\n    \"null\",\n    {\n      \"type\": \"int\",\n      \"logicalType\": \"date\"\n    },\n    \"string\"\n  ]\n}`\nTime (microsecond precision)\nThe time-micros logical type represents a time of day, with no reference to a particular calendar, time zone or date, with a precision of one microsecond.\nA time-micros logical type annotates an Avro long, where the long stores the number of microseconds after midnight, 00:00:00.000000.\n`json\n{\n  \"type\": \"string\",\n  \"format\": \"time\"\n}`\nis mapped to:\n`json\n{\n  \"type\": \"long\",\n  \"logicalType\": \"time-micros\"\n}`\nand the Avro schema is:\n`json\n{\n  \"type\": [\n    \"null\",\n    {\n      \"type\": \"long\",\n      \"logicalType\": \"time-micros\"\n    },\n    \"string\"\n  ]\n}`\nTimestamp (microsecond precision)\nThe timestamp-micros logical type represents an instant on the global timeline, independent of a particular time zone or calendar, with a precision of one microsecond.\nA timestamp-micros logical type annotates an Avro long, where the long stores the number of microseconds from the unix epoch, 1 January 1970 00:00:00.000000 UTC.\n`json\n{\n  \"type\": \"string\",\n  \"format\": \"date-time\"\n}`\nis mapped to:\n`json\n{\n  \"type\": \"long\",\n  \"logicalType\": \"timestamp-micros\"\n}`\nand the Avro schema is:\n`json\n{\n  \"type\": [\n    \"null\",\n    {\n      \"type\": \"long\",\n      \"logicalType\": \"timestamp-micros\"\n    },\n    \"string\"\n  ]\n}`\nCombined Restrictions\nCombined restrictions (`allOf`, `anyOf`, and `oneOf`) will be converted to type unions. The corresponding Avro schema can be less stringent. For example, the following Json schema\n`json\n{\n  \"oneOf\": [\n    {\"type\": \"string\"},\n    {\"type\": \"integer\"}\n  ]\n}`\nwill become this in Avro schema:\n`json\n{\n  \"type\": [\"null\", \"string\", \"int\"]\n}`\nKeyword `not`\nKeyword `not` is not supported, as there is no equivalent validation mechanism in Avro schema.\nFiled Name\nOnly alphanumeric characters and underscores (`/a-zA-Z0-9_/`) are allowed in a stream or field name. Any special character will be converted to an alphabet or underscore. For example, `sp\u00e9cial:character_names` will become `special_character_names`. The original names will be stored in the `doc`property in this format: `_airbyte_original_name:<original-name>`.\nField name cannot start with a number, so an underscore will be added to those field names at the beginning.\nArray Types\nFor array fields in Json schema, when the `items` property is an array, it means that each element in the array should follow its own schema sequentially. For example, the following specification means the first item in the array should be a string, and the second a number.\n`json\n{\n  \"array_field\": {\n    \"type\": \"array\",\n    \"items\": [\n      {\"type\": \"string\"},\n      {\"type\": \"number\"}\n    ]\n  }\n}`\nThis is not supported in Avro schema. As a compromise, the converter creates a union, `[\"null\", \"string\", \"number\"]`, which is less stringent:\n`json\n{\n  \"name\": \"array_field\",\n  \"type\": [\n    \"null\",\n    {\n      \"type\": \"array\",\n      \"items\": [\"null\", \"string\", \"number\"]\n    }\n  ],\n  \"default\": null\n}`\nIf the Json array has multiple object items, these objects will be recursively merged into one Avro record. For example, the following Json array expects two different objects. The first object has an `id` field, and second has an `id` and `message` field. Their `id` fields have slightly different types.\nJson schema:\n`json\n{\n  \"array_field\": {\n    \"type\": \"array\",\n    \"items\": [\n      {\n        \"type\": \"object\",\n        \"properties\": {\n          \"id\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"id_part_1\": { \"type\": \"integer\" },\n              \"id_part_2\": { \"type\": \"string\" }\n            }\n          }\n        }\n      },\n      {\n        \"type\": \"object\",\n        \"properties\": {\n          \"id\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"id_part_1\": { \"type\": \"string\" },\n              \"id_part_2\": { \"type\": \"integer\" }\n            }\n          },\n          \"message\": {\n            \"type\": \"string\"\n          }\n        }\n      }\n    ]\n  }\n}`\nJson object:\n`json\n{\n  \"array_field\": [\n    {\n      \"id\": {\n        \"id_part_1\": 1000,\n        \"id_part_2\": \"abcde\"\n      }\n    }, {\n      \"id\": {\n        \"id_part_1\": \"wxyz\",\n        \"id_part_2\": 2000\n      },\n      \"message\": \"test message\"\n    }\n  ]\n}`\nAfter conversion, the two object schemas will be merged into one. Furthermore, the fields under the `id` record, `id_part_1` and `id_part_2`, will also be merged. In this way, all possible valid elements from the Json array can be converted to Avro records.\nAvro schema:\n`json\n{\n  \"name\": \"array_field\",\n  \"type\": [\n    \"null\",\n    {\n      \"type\": \"array\",\n      \"items\": [\n        \"boolean\",\n        {\n          \"type\": \"record\",\n          \"name\": \"array_field\",\n          \"fields\": [\n            {\n              \"name\": \"id\",\n              \"type\": [\n                \"null\",\n                {\n                  \"type\": \"record\",\n                  \"name\": \"id\",\n                  \"fields\": [\n                    {\n                      \"name\": \"id_part_1\",\n                      \"type\": [\"null\", \"int\", \"string\"],\n                      \"default\": null\n                    },\n                    {\n                      \"name\": \"id_part_2\",\n                      \"type\": [\"null\", \"string\", \"int\"],\n                      \"default\": null\n                    }\n                  ]\n                }\n              ],\n              \"default\": null\n            },\n            {\n              \"name\": \"message\",\n              \"type\": [\"null\", \"string\"],\n              \"default\": null\n            }\n          ]\n        }\n      ]\n    }\n  ],\n  \"default\": null\n}`\nNote that `id_part_1` is a union of `int` and `string`, which comes from the first and second `id` definitions, respectively, in the original Json `items` specification.\nAvro object:\n`json\n{\n  \"array_field\": [\n    {\n      \"id\": {\n        \"id_part_1\": 1000,\n        \"id_part_2\": \"abcde\"\n      },\n      \"message\": null\n    },\n    {\n      \"id\": {\n        \"id_part_1\": \"wxyz\",\n        \"id_part_2\": 2000\n      },\n      \"message\": \"test message\"\n    }\n  ]\n}`\nNote that the first object in `array_field` originally does not have a `message` field. However, because its schema is merged with the second object definition, it has a null `message` field in the Avro record.\nUntyped Array\nWhen a Json array field has no `items`, the element in that array field may have any type. However, Avro requires that each array has a clear type specification. To solve this problem, the elements in the array are forced to be `string`s.\nFor example, given the following Json schema and object:\n`json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"identifier\": {\n      \"type\": \"array\"\n    }\n  }\n}`\n`json\n{\n  \"identifier\": [\"151\", 152, true, {\"id\": 153}, null]\n}`\nthe corresponding Avro schema and object will be:\n`json\n{\n  \"type\": \"record\",\n  \"fields\": [\n    {\n      \"name\": \"identifier\",\n      \"type\": [\n        \"null\",\n        {\n          \"type\": \"array\",\n          \"items\": [\"null\", \"string\"]\n        }\n      ],\n      \"default\": null\n    }\n  ]\n}`\n`json\n{\n  \"identifier\": [\"151\", \"152\", \"true\", \"{\\\"id\\\": 153}\", null]\n}`\nNote that every non-null element inside the `identifier` array field is converted to string.\nAirbyte-Specific Fields\nThree Airbyte specific fields will be added to each Avro record:\n| Field | Schema | Document |\n| :--- | :--- | :---: |\n| `_airbyte_ab_id` | `uuid` | link |\n| `_airbyte_emitted_at` | `timestamp-millis` | link |\n| `_airbyte_additional_properties` | `map` of `string` | See explanation below. |\nAdditional Properties\nA Json object can have additional properties of unknown types, which is not compatible with the Avro schema. To solve this problem during Json to Avro object conversion, we introduce a special field: `_airbyte_additional_properties` typed as a nullable `map` from `string` to `string`:\n`json\n{\n  \"name\": \"_airbyte_additional_properties\",\n  \"type\": [\"null\", {\"type\": \"map\", \"values\": \"string\"}],\n  \"default\": null\n}`\nFor example, given the following Json schema:\n`json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"username\": {\n      \"type\": [\"null\", \"string\"]\n    }\n  }\n}`\nthis Json object\n`json\n{\n  \"username\": \"admin\",\n  \"active\": true,\n  \"age\": 21,\n  \"auth\": {\n    \"auth_type\": \"ssl\",\n    \"api_key\": \"abcdefg/012345\",\n    \"admin\": false,\n    \"id\": 1000\n  }\n}`\nwill be converted to the following Avro object:\n`json\n{\n  \"username\": \"admin\",\n  \"_airbyte_additional_properties\": {\n    \"active\": \"true\",\n    \"age\": \"21\",\n    \"auth\": \"{\\\"auth_type\\\":\\\"ssl\\\",\\\"api_key\\\":\\\"abcdefg/012345\\\",\\\"admin\\\":false,\\\"id\\\":1000}\"\n  }\n}`\nNote that all fields other than the `username` is moved under `_ab_additional_properties` as serialized strings, including the original object `auth`.\nUntyped Object\nIf an `object` field has no `properties` specification, all fields within this `object` will be put into the aforementioned `_airbyte_additional_properties` field.\nFor example, given the following Json schema and object:\n`json\n{\n  \"type\": \"object\"\n}`\n`json\n{\n  \"username\": \"343-guilty-spark\",\n  \"password\": 1439,\n  \"active\": true\n}`\nthe corresponding Avro schema and record will be:\n`json\n{\n  \"type\": \"record\",\n  \"name\": \"record_without_properties\",\n  \"fields\": [\n    {\n      \"name\": \"_airbyte_additional_properties\",\n      \"type\": [\"null\", {\"type\": \"map\", \"values\": \"string\"}],\n      \"default\": null\n    }\n  ]\n}`\n`json\n{\n  \"_airbyte_additional_properties\": {\n    \"username\": \"343-guilty-spark\",\n    \"password\": \"1439\",\n    \"active\": \"true\"\n  }\n}`\nUntyped Field\nAny field without property type specification will default to a `string` field, and its value will be serialized to string.\nExample\nBased on the above rules, here is an overall example. Given the following Json schema:\n`json\n{\n  \"type\": \"object\",\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n  \"properties\": {\n    \"id\": {\n      \"type\": \"integer\"\n    },\n    \"user\": {\n      \"type\": [\"null\", \"object\"],\n      \"properties\": {\n        \"id\": {\n          \"type\": \"integer\"\n        },\n        \"field_with_sp\u00e9cial_character\": {\n          \"type\": \"integer\"\n        }\n      }\n    },\n    \"created_at\": {\n      \"type\": [\"null\", \"string\"],\n      \"format\": \"date-time\"\n    }\n  }\n}`\nIts corresponding Avro schema will be:\n`json\n{\n  \"name\": \"stream_name\",\n  \"type\": \"record\",\n  \"fields\": [\n    {\n      \"name\": \"_airbyte_ab_id\",\n      \"type\": {\n        \"type\": \"string\",\n        \"logicalType\": \"uuid\"\n      }\n    },\n    {\n      \"name\": \"_airbyte_emitted_at\",\n      \"type\": {\n        \"type\": \"long\",\n        \"logicalType\": \"timestamp-millis\"\n      }\n    },\n    {\n      \"name\": \"id\",\n      \"type\": [\"null\", \"int\"],\n      \"default\": null\n    },\n    {\n      \"name\": \"user\",\n      \"type\": [\n        \"null\",\n        {\n          \"type\": \"record\",\n          \"name\": \"user\",\n          \"fields\": [\n            {\n              \"name\": \"id\",\n              \"type\": [\"null\", \"int\"],\n              \"default\": null\n            },\n            {\n              \"name\": \"field_with_special_character\",\n              \"type\": [\"null\", \"int\"],\n              \"doc\": \"_airbyte_original_name:field_with_sp\u00e9cial_character\",\n              \"default\": null\n            },\n            {\n              \"name\": \"_airbyte_additional_properties\",\n              \"type\": [\"null\", {\"type\": \"map\", \"values\": \"string\"}],\n              \"default\": null\n            }\n          ]\n        }\n      ],\n      \"default\": null\n    },\n    {\n      \"name\": \"created_at\",\n      \"type\": [\n        \"null\",\n        {\"type\": \"long\", \"logicalType\": \"timestamp-micros\"},\n        \"string\"\n      ],\n      \"default\": null\n    },\n    {\n      \"name\": \"_airbyte_additional_properties\",\n      \"type\": [\"null\", {\"type\": \"map\", \"values\": \"string\"}],\n      \"default\": null\n    }\n  ]\n}`\nMore examples can be found in the Json to Avro conversion test cases.\nImplementation\n\nSchema conversion: JsonToAvroSchemaConverter\n",
    "tag": "airbyte"
  },
  {
    "title": "Architecture overview",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/understanding-airbyte/high-level-view.md",
    "content": "\ndescription: A high level view of Airbyte's components.\nArchitecture overview\nAirbyte is conceptually composed of two parts: platform and connectors. \nThe platform provides all the horizontal services required to configure and run data movement operations e.g: the UI, configuration API, job scheduling, logging, alerting, etc. and is structured as a set of microservices. \nConnectors are independent modules which push/pull data to/from sources and destinations. Connectors are built in accordance with the Airbyte Specification, which describes the interface with which data can be moved between a source and a destination using Airbyte. Connectors are packaged as Docker images, which allows total flexibility over the technologies used to implement them. \nA more concrete diagram can be seen below:\n\n\n`UI`: An easy-to-use graphical interface for interacting with the Airbyte API.\n`WebApp Server`: Handles connection between UI and API.\n`Config Store`: Stores all the connections information (credentials, frequency...).\n`Scheduler Store`: Stores statuses and job information for the scheduler bookkeeping.\n`Config API`: Airbyte's main control plane. All operations in Airbyte such as creating sources, destinations, connections, managing configurations, etc.. are configured and invoked from the API.\n`Scheduler`: The scheduler takes work requests from the API and sends them to the Temporal service to parallelize. It is responsible for tracking success/failure and for triggering syncs based on the configured frequency.\n`Temporal Service`: Manages the task queue and workflows for the Scheduler. \n`Worker`: The worker connects to a source connector, pulls the data and writes it to a destination.\n`Temporary Storage`: A storage that workers can use whenever they need to spill data on a disk.\n",
    "tag": "airbyte"
  },
  {
    "title": "Airbyte Protocol",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/understanding-airbyte/airbyte-protocol.md",
    "content": "Airbyte Protocol\nGoals\nThe Airbyte Protocol describes a series of standard components and all the interactions between them in order to declare an ELT pipeline. All message passing across components is done via serialized JSON messages for inter-process communication.\nThis document describes the protocol as it exists in its CURRENT form. Stay tuned for an RFC on how the protocol will evolve.\nThis document is intended to contain ALL the rules of the Airbyte Protocol in one place. Anything not contained in this document is NOT part of the Protocol. At the time of writing, there is one known exception, which is the Supported Data Types, which contains rules on data types that are part of the Protocol. That said, there are additional articles, e.g. A Beginner's Guide to the Airbyte Catalog that repackage the information in this document for different audiences.\nKey Concepts\nThere are 2 major components in the Airbyte Protocol: Source and Destination. These components are referred to as Actors. A source is an application that is described by a series of standard interfaces. This application extracts data from an underlying data store. A data store in this context refers to the tool where the data is actually stored. A data store includes: databases, APIs, anything that produces data, etc. For example, the Postgres Source is a Source that pulls from Postgres (which is a data store). A Destination is an application that is described by a series of standard interfaces that loads data into a data store.\nThe key primitives that the Protocol uses to describe data are Catalog, Configured Catalog, Stream, Configured Stream, and Field:\n\nStream - A Stream describes the schema of a resource and various metadata about how a user can interact with that resource. A resource in this context might refer to a database table, a resource in a REST API, or a data stream.\nField - A Field refers to a \"column\" in a Stream. In a database this would be a column; in a JSON object it is a field.\nCatalog - A Catalog is a list of Streams that describes the data in the data store that a Source represents.\n\nAn Actor can advertise information about itself with an Actor Specification. One of the main pieces of information the specification shares is what information is needed to configure an Actor.\nEach of these concepts is described in greater depth in their respective section.",
    "tag": "airbyte"
  },
  {
    "title": "Change Data Capture (CDC)",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/understanding-airbyte/cdc.md",
    "content": "Change Data Capture (CDC)\nWhat is log-based incremental replication?\nMany common databases support writing all record changes to log files for the purpose of replication. A consumer of these log files (such as Airbyte) can read these logs while keeping track of the current position within the logs in order to read all record changes coming from `DELETE`/`INSERT`/`UPDATE` statements.\nSyncing\nThe orchestration for syncing is similar to non-CDC database sources. After selecting a sync interval, syncs are launched regularly. We read data from the log up to the time that the sync was started. We do not treat CDC sources as infinite streaming sources. You should ensure that your schedule for running these syncs is frequent enough to consume the logs that are generated. The first time the sync is run, a snapshot of the current state of the data will be taken. This is done using `SELECT` statements and is effectively a Full Refresh. Subsequent syncs will use the logs to determine which changes took place since the last sync and update those. Airbyte keeps track of the current log position between syncs.\nA single sync might have some tables configured for Full Refresh replication and others for Incremental. If CDC is configured at the source level, all tables with Incremental selected will use CDC. All Full Refresh tables will replicate using the same process as non-CDC sources. However, these tables will still include CDC metadata columns by default.\nThe Airbyte Protocol outputs records from sources. Records from `UPDATE` statements appear the same way as records from `INSERT` statements. We support different options for how to sync this data into destinations using primary keys, so you can choose to append this data, delete in place, etc.\nWe add some metadata columns for CDC sources:\n\n`ab_cdc_lsn` (postgres and sql server sources) is the point in the log where the record was retrieved\n`ab_cdc_log_file` & `ab_cdc_log_pos` (specific to mysql source) is the file name and position in the file where the record was retrieved\n`ab_cdc_updated_at` is the timestamp for the database transaction that resulted in this record change and is present for records from `DELETE`/`INSERT`/`UPDATE` statements \n`ab_cdc_deleted_at` is the timestamp for the database transaction that resulted in this record change and is only present for records from `DELETE` statements\n\nLimitations\n\nCDC incremental is only supported for tables with primary keys. A CDC source can still choose to replicate tables without primary keys as Full Refresh or a non-CDC source can be configured for the same database to replicate the tables without primary keys using standard incremental replication.\nData must be in tables, not views.\nThe modifications you are trying to capture must be made using `DELETE`/`INSERT`/`UPDATE`. For example, changes made from `TRUNCATE`/`ALTER`  won't appear in logs and therefore in your destination.\nWe do not support schema changes automatically for CDC sources. We recommend resetting and resyncing data if you make a schema change.\nThere are database-specific limitations. See the documentation pages for individual connectors for more information.\nThe records produced by `DELETE` statements only contain primary keys. All other data fields are unset.\n\nCurrent Support\n\nPostgres (For a quick video overview of CDC on Postgres, click here)\nMySQL\nMicrosoft SQL Server / MSSQL\n\nComing Soon\n\nOracle DB\nPlease create a ticket if you need CDC support on another database!\n\nAdditional information\n\nAn overview of Airbyte\u2019s replication modes.\nUnderstanding Change Data Capture (CDC): Definition, Methods and Benefits\nExplore Airbyte's Change Data Capture (CDC) synchronization\n",
    "tag": "airbyte"
  },
  {
    "title": "Airbyte Protocol Versioning",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/understanding-airbyte/airbyte-protocol-versioning.md",
    "content": "Airbyte Protocol Versioning\nGoal\nThe goal of this document is to define our approach to protocol changes.\nWe need a compromise between frequent breaking changes that are heavy on operations and infinite backward compatibility which is a burden from a software engineering point of view.\nVersioning Scheme\nWe are using a `<MAJOR>.<MINOR>.<PATCH>` scheme for the Protocol Versioning. (see SemVer).\nWe increment the\n* MAJOR version when you make incompatible protocol changes\n* MINOR version when you add functionality in a backwards compatible manner\n* PATCH version when you make backwards compatible bug fixes\nDevelopment Guidelines\n\nWe will continue to do our best effort to avoid introducing breaking changes to the Airbyte Protocol.\nWhen introducing a new minor version of the Airbyte Protocol, new fields must come with sensible defaults for backward compatibility within the same major version, or be entirely optional.\nWhen introducing a new major version of the Airbyte Protocol, all connectors from the previous major version will continue to work.  This requires the ability to \u201ctranslate\u201d messages between 1 major version of the Airbyte Protocol.\n\nSafeguards\nTo ensure continuous operation, we have a few safeguards to prevent breaking existing configuration through protocol version incompatibilities.\nWhen upgrading Airbyte\nWhen removing support for older versions of the Protocol, there is a risk removing the support for a version that is currently used.\nTo mitigate this, as part of the pre-upgrade checks that happens in the `airbyte-bootloader`, we verify that any connector currently part of an active connection will still be supported after the upgrade.\nIf any connector fails this check, we abort the upgrade and the `airbyte-bootloader` logs contains a list of connectors to upgrade. Those connectors will need to be upgraded from the UI before the platform itself can be upgraded.\nWhen upgrading a Connector",
    "tag": "airbyte"
  },
  {
    "title": "A Beginner's Guide to the AirbyteCatalog",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/understanding-airbyte/beginners-guide-to-catalog.md",
    "content": "A Beginner's Guide to the AirbyteCatalog\nOverview\nThe goal of this article is to make the `AirbyteCatalog` approachable to someone contributing to Airbyte for the first time. If you are looking to get deeper into the details of the catalog, you can read our technical specification on it here.\nThe goal of the `AirbyteCatalog` is to describe what data is available in a source. The goal of the `ConfiguredAirbyteCatalog` is to, based on an `AirbyteCatalog`, specify how data from the source is replicated.\nContents\nThis article will illustrate how to use `AirbyteCatalog` via a series of examples. We recommend reading the Database Example first. The other examples, will refer to knowledge described in that section. After that, jump around to whichever example is most pertinent to your inquiry.\n\nPostgres Example\nAPI Example\nStatic Streams Example\nDynamic Streams Example\nNested Schema Example\n\nIn order to understand in depth how to configure incremental data replication, head over to the incremental replication docs.\nDatabase Example\nLet's jump into an example using a relational database. We will assume we have a database with the following schema:\n```sql\nCREATE TABLE \"airlines\" (\n    \"id\"   INTEGER,\n    \"name\" VARCHAR\n);\nCREATE TABLE \"pilots\" (\n    \"id\"   INTEGER,\n    \"airline_id\" INTEGER,\n    \"name\" VARCHAR\n);\n```\nAirbyteCatalog\nWe would represent this data in a catalog as follows:\n`javascript\n{\n  \"streams\": [\n    {\n      \"name\": \"airlines\",\n      \"supported_sync_modes\": [\n        \"full_refresh\",\n        \"incremental\"\n      ],\n      \"source_defined_cursor\": false,\n      \"json_schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"id\": {\n            \"type\": \"number\"\n          },\n          \"name\": {\n            \"type\": \"string\"\n          }\n        }\n      }\n    },\n    {\n      \"name\": \"pilots\",\n      \"supported_sync_modes\": [\n        \"full_refresh\",\n        \"incremental\"\n      ],\n      \"source_defined_cursor\": false,\n      \"json_schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"id\": {\n            \"type\": \"number\"\n          },\n          \"airline_id\": {\n            \"type\": \"number\"\n          },\n          \"name\": {\n            \"type\": \"string\"\n          }\n        }\n      }\n    }\n  ]\n}`\nThe catalog is structured as a list of `AirbyteStream`. In the case of a database a \"stream\" is analogous to a table. (For APIs the mapping can be a more creative; we will discuss it later in API Examples)\nLet's walk through what each field in a stream means.\n\n`name` - The name of the stream.\n`supported_sync_modes` - This field lists the type of data replication that this source supports. The possible values in this array include `FULL_REFRESH` (docs) and `INCREMENTAL` (docs).\n`source_defined_cursor` - If the stream supports `INCREMENTAL` replication, then this field signals whether the source can figure out how to detect new records on its own or not.\n`json_schema` - This field is a JsonSchema object that describes the structure of the data. Notice that each key in the `properties` object corresponds to a column name in our database table.\n\nNow we understand what data is available from this source. Next we will configure how we want to replicate that data.\nConfiguredAirbyteCatalog\nLet's say that we do not care about replicating the pilot data at all. We do want to replicate the airlines data as a `FULL_REFRESH`. Here's what our `ConfiguredAirbyteCatalog` would look like.\n`javascript\n{\n  \"streams\": [\n    {\n      \"sync_mode\": \"FULL_REFRESH\",\n      \"stream\": {\n        \"name\": \"airlines\",\n        \"supported_sync_modes\": [\n          \"full_refresh\",\n          \"incremental\"\n        ],\n        \"source_defined_cursor\": false,\n        \"json_schema\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"id\": {\n              \"type\": \"number\"\n            },\n            \"name\": {\n              \"type\": \"string\"\n            }\n          }\n        }\n      }\n    }\n  ]\n}`\nJust as with the `AirbyteCatalog` the `ConfiguredAirbyteCatalog` contains a list. This time it is a list of `ConfiguredAirbyteStream` (instead of just `AirbyteStream`).\nLet's walk through each field in the `ConfiguredAirbyteStream`:\n\n`sync_mode` - This field must be one of the values that was in `supported_sync_modes` in the `AirbyteStream` - Configures which sync mode will be used when data is replicated.\n`stream` - Hopefully this one looks familiar! This field contains an `AirbyteStream`. It should be identical to the one we saw in the `AirbyteCatalog`.\n`cursor_field` - When `sync_mode` is `INCREMENTAL` and `source_defined_cursor = false`, this field configures which field in the stream will be used to determine if a record should be replicated or not. Read more about this concept in our documentation of incremental replication.\n\nSummary of the Postgres Example\nWhen thinking about `AirbyteCatalog` and `ConfiguredAirbyteCatalog`, remember that the `AirbyteCatalog` describes what data is present in the source (and metadata around what replication configuration it can support). It is output by the `discover` method of source. It should be treated as an immutable object; if you are ever manually editing a catalog outside of a source, you've gone off the rails. The `ConfiguredAirbyteCatalog` is a mutable configuration object that specifies, for each `AirbyteStream`, how (and if) it should be replicated. The `ConfiguredAirbyteCatalog` does this by wrapping each `AirbyteStream` in an `AirbyteCatalog` inside a `ConfiguredAirbyteStream`.\nAPI Examples\nThe `AirbyteCatalog` offers the flexibility in how to model the data for an API. In the next two examples, we will model data from the same API--a stock ticker--in two different ways. In the first, the source will return a single stream called `ticker`, and in the second, the source with return a stream for each stock symbol it is configured to retrieve data for. Each stream's name will be a stock symbol.\nStatic Streams Example\nLet's imagine we want to create a basic Stock Ticker source. The goal of this source is to take in a single stock symbol and return a single stream. We will call the stream `ticker` and will contain the closing price of the stock. We will assume that you already have a rough understanding of the `AirbyteCatalog` and the `ConfiguredAirbyteCatalog` from the previous database example.\nAirbyteCatalog\nHere is what the `AirbyteCatalog` might look like.\n`javascript\n{\n  \"streams\": [\n    {\n      \"name\": \"ticker\",\n      \"supported_sync_modes\": [\n        \"full_refresh\",\n        \"incremental\"\n      ],\n      \"source_defined_cursor\": false,\n      \"json_schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"symbol\": {\n            \"type\": \"string\"\n          },\n          \"price\": {\n            \"type\": \"number\"\n          },\n          \"date\": {\n            \"type\": \"string\"\n          }\n        }\n      }\n    }\n  ]\n}`\nThis catalog looks pretty similar to the `AirbyteCatalog` that we created for the Database Example. For the data we've picked here, you can think about `ticker` as a table and then each field it returns in a record as a column, so it makes sense that these look pretty similar.\nConfiguredAirbyteCatalog\nThe `ConfiguredAirbyteCatalog` follows the same rules as we described in the Database Example. It just wraps the `AirbyteCatalog` described above.\nDynamic Streams Example\nNow let's build a stock ticker source that handles returning ticker data for multiple stocks. The name of each stream will be the stock symbol that it represents.\nAirbyteCatalog\n`javascript\n{\n  \"streams\": [\n    {\n      \"name\": \"TSLA\",\n      \"supported_sync_modes\": [\n        \"full_refresh\",\n        \"incremental\"\n      ],\n      \"source_defined_cursor\": false,\n      \"json_schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"symbol\": {\n            \"type\": \"string\"\n          },\n          \"price\": {\n            \"type\": \"number\"\n          },\n          \"date\": {\n            \"type\": \"string\"\n          }\n        }\n      }\n    },\n    {\n      \"name\": \"FB\",\n      \"supported_sync_modes\": [\n        \"full_refresh\",\n        \"incremental\"\n      ],\n      \"source_defined_cursor\": false,\n      \"json_schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"symbol\": {\n            \"type\": \"string\"\n          },\n          \"price\": {\n            \"type\": \"number\"\n          },\n          \"date\": {\n            \"type\": \"string\"\n          }\n        }\n      }\n    }\n  ]\n}`\nThis example provides another way of thinking about exposing data in a source. As a developer building a source, you can model the `AirbyteCatalog` for a source however makes most sense to the use case you are trying to fulfill.\nNested Schema Example\nOften, a data source contains \"nested\" data. In other words this is data where each record contains other objects nested inside it. Cases like this cannot be easily modeled just as tables / columns. This is why Airbyte uses JsonSchema to model the schema of its streams.\nLet's imagine we are modeling a flight object. A flight object might look like this:\n`javascript\n{\n  \"airline\": \"alaska\",\n  \"origin\": {\n    \"airport_code\": \"SFO\",\n    \"terminal\": \"2\",\n    \"gate\": \"G23\"\n  },\n  \"destination\": {\n    \"airport_code\": \"JFK\",\n    \"terminal\": \"7\",\n    \"gate\": \"1\"\n  }\n}`\nThe `AirbyteCatalog` would look like this:\n`javascript\n{\n  \"streams\": [\n    {\n      \"name\": \"flights\",\n      \"supported_sync_modes\": [\n        \"full_refresh\"\n      ],\n      \"source_defined_cursor\": false,\n      \"json_schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"airline\": {\n            \"type\": \"string\"\n          },\n          \"origin\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"airport_code\": {\n                \"type\": \"string\"\n              },\n              \"terminal\": {\n                \"type\": \"string\"\n              },\n              \"gate\": {\n                \"type\": \"string\"\n              }\n            }\n          },\n          \"destination\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"airport_code\": {\n                \"type\": \"string\"\n              },\n              \"terminal\": {\n                \"type\": \"string\"\n              },\n              \"gate\": {\n                \"type\": \"string\"\n              }\n            }\n          }\n        }\n      }\n    }\n  ]\n}`\nBecause Airbyte uses JsonSchema to model the schema of streams, it is able to handle arbitrary nesting of data in a way that a table / column based model cannot.",
    "tag": "airbyte"
  },
  {
    "title": "Basic Normalization",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/understanding-airbyte/basic-normalization.md",
    "content": "Basic Normalization\nHigh-Level Overview\n:::info\nThe high-level overview contains all the information you need to use Basic Normalization when pulling from APIs. Information past that can be read for advanced or educational purposes.\n:::\nWhen you run your first Airbyte sync without the basic normalization, you'll notice that your data gets written to your destination as one data column with a JSON blob that contains all of your data. This is the `_airbyte_raw_` table that you may have seen before. Why do we create this table? A core tenet of ELT philosophy is that data should be untouched as it moves through the E and L stages so that the raw data is always accessible. If an unmodified version of the data exists in the destination, it can be retransformed without needing to sync data again.\nIf you have Basic Normalization enabled, Airbyte automatically uses this JSON blob to create a schema and tables with your data in mind, converting it to the format of your destination. This runs after your sync and may take a long time if you have a large amount of data synced. If you don't enable Basic Normalization, you'll have to transform the JSON data from that column yourself.\nExample\nBasic Normalization uses a fixed set of rules to map a json object from a source to the types and format that are native to the destination. For example if a source emits data that looks like this:\n`javascript\n{\n  \"make\": \"alfa romeo\",\n  \"model\": \"4C coupe\",\n  \"horsepower\": \"247\"\n}`\nThe destination connectors produce the following raw table in the destination database:\n`sql\nCREATE TABLE \"_airbyte_raw_cars\" (\n    -- metadata added by airbyte\n    \"_airbyte_ab_id\" VARCHAR, -- uuid value assigned by connectors to each row of the data written in the destination.\n    \"_airbyte_emitted_at\" TIMESTAMP_WITH_TIMEZONE, -- time at which the record was emitted.\n    \"_airbyte_data\" JSONB -- data stored as a Json Blob.\n);`\nThen, basic normalization would create the following table:\n```sql\nCREATE TABLE \"cars\" (\n    \"_airbyte_ab_id\" VARCHAR,\n    \"_airbyte_emitted_at\" TIMESTAMP_WITH_TIMEZONE,\n    \"_airbyte_cars_hashid\" VARCHAR,\n    \"_airbyte_normalized_at\" TIMESTAMP_WITH_TIMEZONE,\n\n\n```-- data from source\n\"make\" VARCHAR,\n\"model\" VARCHAR,\n\"horsepower\" INTEGER\n```\n\n\n);\n```\nNormalization metadata columns\nYou'll notice that some metadata are added to keep track of important information about each record.\n- Some are introduced at the destination connector level: These are propagated by the normalization process from the raw table to the final table\n  - `_airbyte_ab_id`: uuid value assigned by connectors to each row of the data written in the destination.\n  - `_airbyte_emitted_at`: time at which the record was emitted and recorded by destination connector.\n- While other metadata columns are created at the normalization step.\n  - `_airbyte_<table_name>_hashid`: hash value assigned by airbyte normalization derived from a hash function of the record data.\n  - `_airbyte_normalized_at`: time at which the record was last normalized (useful to track when incremental transformations are performed)\nAdditional metadata columns can be added on some tables depending on the usage:\n- On the Slowly Changing Dimension (SCD) tables:\n  - `_airbyte_start_at`: equivalent to the cursor column defined on the table, denotes when the row was first seen\n  - `_airbyte_end_at`: denotes until when the row was seen with these particular values. If this column is not NULL, then the record has been updated and is no longer the most up to date one. If NULL, then the row is the latest version for the record.\n  - `_airbyte_active_row`: denotes if the row for the record is the latest version or not.\n  - `_airbyte_unique_key_scd`: hash of primary keys + cursors used to de-duplicate the scd table.\n  - On de-duplicated (and SCD) tables:\n  - `_airbyte_unique_key`: hash of primary keys used to de-duplicate the final table.\nThe normalization rules are not configurable. They are designed to pick a reasonable set of defaults to hit the 80/20 rule of data normalization. We respect that normalization is a detail-oriented problem and that with a fixed set of rules, we cannot normalize your data in such a way that covers all use cases. If this feature does not meet your normalization needs, we always put the full json blob in destination as well, so that you can parse that object however best meets your use case. We will be adding more advanced normalization functionality shortly. Airbyte is focused on the EL of ELT. If you need a really featureful tool for the transformations then, we suggest trying out dbt.\nAirbyte places the json blob version of your data in a table called `_airbyte_raw_<stream name>`. If basic normalization is turned on, it will place a separate copy of the data in a table called `<stream name>`. Under the hood, Airbyte is using dbt, which means that the data only ingresses into the data store one time. The normalization happens as a query within the datastore. This implementation avoids extra network time and costs.\nWhy does Airbyte have Basic Normalization?\nAt its core, Airbyte is geared to handle the EL (Extract Load) steps of an ELT process. These steps can also be referred in Airbyte's dialect as \"Source\" and \"Destination\".\nHowever, this is actually producing a table in the destination with a JSON blob column... For the typical analytics use case, you probably want this json blob normalized so that each field is its own column.\nSo, after EL, comes the T (transformation) and the first T step that Airbyte actually applies on top of the extracted data is called \"Normalization\".\nAirbyte runs this step before handing the final data over to other tools that will manage further transformation down the line.\nTo summarize, we can represent the ELT process in the diagram below. These are steps that happens between your \"Source Database or API\" and the final \"Replicated Tables\" with examples of implementation underneath:\n\nIn Airbyte, the current normalization option is implemented using a dbt Transformer composed of:\n\nAirbyte base-normalization python package to generate dbt SQL models files\ndbt to compile and executes the models on top of the data in the destinations that supports it.\n\nDestinations that Support Basic Normalization\n\nBigQuery\nMS Server SQL\nMySQL\nThe server must support the `WITH` keyword.\nRequire MySQL >= 8.0, or MariaDB >= 10.2.1.\nPostgres\nRedshift\nSnowflake\n\nBasic Normalization can be configured when you're creating the connection between your Connection Setup and after in the Transformation Tab.\nSelect the option: Normalized tabular data.\nRules\nTyping\nAirbyte tracks types using JsonSchema's primitive types. Here is how these types will map onto standard SQL types. Note: The names of the types may differ slightly across different destinations.\nAirbyte uses the types described in the catalog to determine the correct type for each column. It does not try to use the values themselves to infer the type.\n| JsonSchema Type | Resulting Type | Notes |\n| :--- | :--- | :--- |\n| `number` | float |  |\n| `integer` | integer |  |\n| `string` | string |  |\n| `bit` | boolean |  |\n| `boolean` | boolean |  |\n| `string` with format label `date-time`| timestamp with timezone |  |\n| `array` | new table | see nesting |\n| `object` | new table | see nesting |\nNesting\nBasic Normalization attempts to expand any nested arrays or objects it receives into separate tables in order to allow more ergonomic querying of your data.\nArrays\nBasic Normalization expands arrays into separate tables. For example if the source provides the following data:\n`javascript\n{\n  \"make\": \"alfa romeo\",\n  \"model\": \"4C coupe\",\n  \"limited_editions\": [\n    { \"name\": \"4C spider\", \"release_year\": 2013 },\n    { \"name\" : \"4C spider italia\" , \"release_year\":  2018 }\n  ]\n}`\nThe resulting normalized schema would be:\n```sql\nCREATE TABLE \"cars\" (\n    \"_airbyte_cars_hashid\" VARCHAR,\n    \"_airbyte_emitted_at\" TIMESTAMP_WITH_TIMEZONE,\n    \"_airbyte_normalized_at\" TIMESTAMP_WITH_TIMEZONE,\n\n\n```\"make\" VARCHAR,\n\"model\" VARCHAR\n```\n\n\n);\nCREATE TABLE \"limited_editions\" (\n    \"_airbyte_limited_editions_hashid\" VARCHAR,\n    \"_airbyte_cars_foreign_hashid\" VARCHAR,\n    \"_airbyte_emitted_at\" TIMESTAMP_WITH_TIMEZONE,\n    \"_airbyte_normalized_at\" TIMESTAMP_WITH_TIMEZONE,\n\n\n```\"name\" VARCHAR,\n\"release_year\" VARCHAR\n```\n\n\n);\n```\nIf the nested items in the array are not objects then they are expanded into a string field of comma separated values e.g.:\n`javascript\n{\n  \"make\": \"alfa romeo\",\n  \"model\": \"4C coupe\",\n  \"limited_editions\": [ \"4C spider\", \"4C spider italia\"]\n}`\nThe resulting normalized schema would be:\n```sql\nCREATE TABLE \"cars\" (\n    \"_airbyte_cars_hashid\" VARCHAR,\n    \"_airbyte_emitted_at\" TIMESTAMP_WITH_TIMEZONE,\n    \"_airbyte_normalized_at\" TIMESTAMP_WITH_TIMEZONE,\n\n\n```\"make\" VARCHAR,\n\"model\" VARCHAR\n```\n\n\n);\nCREATE TABLE \"limited_editions\" (\n    \"_airbyte_limited_editions_hashid\" VARCHAR,\n    \"_airbyte_cars_foreign_hashid\" VARCHAR,\n    \"_airbyte_emitted_at\" TIMESTAMP_WITH_TIMEZONE,\n    \"_airbyte_normalized_at\" TIMESTAMP_WITH_TIMEZONE,\n\n\n```\"data\" VARCHAR\n```\n\n\n);\n```\nObjects\nIn the case of a nested object e.g.:\n`javascript\n{\n  \"make\": \"alfa romeo\",\n  \"model\": \"4C coupe\",\n  \"powertrain_specs\": { \"horsepower\": 247, \"transmission\": \"6-speed\" }\n}`\nThe normalized schema would be:\n```sql\nCREATE TABLE \"cars\" (\n    \"_airbyte_cars_hashid\" VARCHAR,\n    \"_airbyte_emitted_at\" TIMESTAMP_WITH_TIMEZONE,\n    \"_airbyte_normalized_at\" TIMESTAMP_WITH_TIMEZONE,\n\n\n```\"make\" VARCHAR,\n\"model\" VARCHAR\n```\n\n\n);\nCREATE TABLE \"powertrain_specs\" (\n    \"_airbyte_powertrain_hashid\" VARCHAR,\n    \"_airbyte_cars_foreign_hashid\" VARCHAR,\n    \"_airbyte_emitted_at\" TIMESTAMP_WITH_TIMEZONE,\n    \"_airbyte_normalized_at\" TIMESTAMP_WITH_TIMEZONE,\n\n\n```\"horsepower\" INTEGER,\n\"transmission\" VARCHAR\n```\n\n\n);\n```\nNaming Collisions for un-nested objects\nWhen extracting nested objects or arrays, the Basic Normalization process needs to figure out new names for the expanded tables.\nFor example, if we had a `cars` table with a nested column `cars` containing an object whose schema is identical to the parent table.\n`javascript\n{\n  \"make\": \"alfa romeo\",\n  \"model\": \"4C coupe\",\n  \"cars\": [\n    { \"make\": \"audi\", \"model\": \"A7\" },\n    { \"make\" : \"lotus\" , \"model\":  \"elise\" }\n    { \"make\" : \"chevrolet\" , \"model\":  \"mustang\" }\n  ]\n}`\nThe expanded table would have a conflict in terms of naming since both are named `cars`. To avoid name collisions and ensure a more consistent naming scheme, Basic Normalization chooses the expanded name as follows:\n\n`cars` for the original parent table\n`cars_da3_cars` for the expanded nested columns following this naming scheme in 3 parts: `<Json path>_<Hash>_<nested column name>`\nJson path: The entire json path string with '_' characters used as delimiters to reach the table that contains the nested column name.\nHash: Hash of the entire json path to reach the nested column reduced to 3 characters. This is to make sure we have a unique name (in case part of the name gets truncated, see below)\nNested column name: name of the column being expanded into its own table.\n\nBy following this strategy, nested columns should \"never\" collide with other table names. If it does, an exception will probably be thrown either by the normalization process or by dbt that runs afterward.\n```sql\nCREATE TABLE \"cars\" (\n    \"_airbyte_cars_hashid\" VARCHAR,\n    \"_airbyte_emitted_at\" TIMESTAMP_WITH_TIMEZONE,\n    \"_airbyte_normalized_at\" TIMESTAMP_WITH_TIMEZONE,\n\n\n```\"make\" VARCHAR,\n\"model\" VARCHAR\n```\n\n\n);\nCREATE TABLE \"cars_da3_cars\" (\n    \"_airbyte_cars_hashid\" VARCHAR,\n    \"_airbyte_cars_foreign_hashid\" VARCHAR,\n    \"_airbyte_emitted_at\" TIMESTAMP_WITH_TIMEZONE,\n    \"_airbyte_normalized_at\" TIMESTAMP_WITH_TIMEZONE,\n\n\n```\"make\" VARCHAR,\n\"model\" VARCHAR\n```\n\n\n);\n```\nNaming limitations & truncation\nNote that different destinations have various naming limitations, most commonly on how long names can be. For instance, the Postgres documentation states:\n\nThe system uses no more than NAMEDATALEN-1 bytes of an identifier; longer names can be written in commands, but they will be truncated. By default, NAMEDATALEN is 64 so the maximum identifier length is 63 bytes\n\nMost modern data warehouses have name lengths limits on the longer side, so this should not affect us that often. Basic Normalization will fallback to the following rules:\n\nNo Truncate if under destination's character limits\n\nHowever, in the rare cases where these limits are reached:\n\nTruncate only the `Json path` to fit into destination's character limits\nTruncate the `Json path` to at least the 10 first characters, then truncate the nested column name starting in the middle to preserve prefix/suffix substrings intact (whenever a truncate in the middle is made, two '__' characters are also inserted to denote where it happened) to fit into destination's character limits\n\nAs an example from the hubspot source, we could have the following tables with nested columns:\n| Description | Example 1 | Example 2 |\n| :--- | :--- | :--- |\n| Original Stream Name | companies | deals |\n| Json path to the nested column | `companies/property_engagements_last_meeting_booked_campaign` | `deals/properties/engagements_last_meeting_booked_medium` |\n| Final table name of expanded nested column on BigQuery | companies_2e8_property_engagements_last_meeting_booked_campaign | deals_properties_6e6_engagements_last_meeting_booked_medium |\n| Final table name of expanded nested column on Postgres | companies_2e8_property_engag__oked_campaign | deals_prop_6e6_engagements_l__booked_medium |\nAs mentioned in the overview:\n\nAirbyte places the json blob version of your data in a table called `_airbyte_raw_<stream name>`.\nIf basic normalization is turned on, it will place a separate copy of the data in a table called `<stream name>`.\nIn certain pathological cases, basic normalization is required to generate large models with many columns and multiple intermediate transformation steps for a stream. This may break down the \"ephemeral\" materialization strategy and require the use of additional intermediate views or tables instead. As a result, you may notice additional temporary tables being generated in the destination to handle these checkpoints.\n\nUI Configurations\nTo enable basic normalization (which is optional), you can toggle it on or disable it in the \"Normalization and Transformation\" section when setting up your connection:\n\nIncremental runs\nWhen the source is configured with sync modes compatible with incremental transformations (using append on destination) such as ( full_refresh_append, incremental append or  incremental deduped history), only rows that have changed in the source are transferred over the network and written by the destination connector.\nNormalization will then try to build the normalized tables incrementally as the rows in the raw tables that have been created or updated since the last time dbt ran. As such, on each dbt run, the models get built incrementally. This limits the amount of data that needs to be transformed, vastly reducing the runtime of the transformations. This improves warehouse performance and reduces compute costs.\nBecause normalization can be either run incrementally and, or, in full refresh, a technical column `_airbyte_normalized_at` can serve to track when was the last time a record has been transformed and written by normalization.\nThis may greatly diverge from the `_airbyte_emitted_at` value as the normalized tables could be totally re-built at a latter time from the data stored in the `_airbyte_raw` tables.\nPartitioning, clustering, sorting, indexing\nNormalization produces tables that are partitioned, clustered, sorted or indexed depending on the destination engine and on the type of tables being built. The goal of these are to make read more performant, especially when running incremental updates.\nIn general, normalization needs to do lookup on the last emitted_at column to know if a record is freshly produced and need to be\nincrementally processed or not. But in certain models, such as SCD tables for example, we also need to retrieve older data to update their type 2 SCD end_date and active_row flags, thus a different partitioning scheme is used to optimize that use case.\nOn Postgres destination, an additional table suffixed with `_stg` for every stream replicated in  incremental deduped history needs to be persisted (in a different staging schema) for incremental transformations to work because of a limitation.\nExtending Basic Normalization\nNote that all the choices made by Normalization as described in this documentation page in terms of naming (and more) could be overridden by your own custom choices. To do so, you can follow the following tutorials:\n\nto build a custom SQL view with your own naming conventions\nto export, edit and run custom dbt normalization yourself\nor further, you can configure the use of a custom dbt project within Airbyte by following this guide.\n",
    "tag": "airbyte"
  },
  {
    "title": "Incremental Sync - Append",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/understanding-airbyte/connections/incremental-append.md",
    "content": "Incremental Sync - Append\nOverview\nAirbyte supports syncing data in Incremental Append mode i.e: syncing only replicate new or modified data. This prevents re-fetching data that you have already replicated from a source. If the sync is running for the first time, it is equivalent to a Full Refresh since all data will be considered as new.\nIn this flavor of incremental, records in the warehouse destination will never be deleted or mutated. A copy of each new or updated record is appended to the data in the warehouse. This means you can find multiple copies of the same record in the destination warehouse. We provide an \"at least once\" guarantee of replicating each record that is present when the sync runs.\nDefinitions\nA `cursor` is the value used to track whether a record should be replicated in an incremental sync. A common example of a `cursor` would be a timestamp from an `updated_at` column in a database table.\nA `cursor field` is the field or column in the data where that cursor can be found. Extending the above example, the `updated_at` column in the database would be the `cursor field`, while the `cursor` is the actual timestamp value used to determine if a record should be replicated.\nWe will refer to the set of records that the source identifies as being new or updated as a `delta`.\nRules\nAs mentioned above, the delta from a sync will be appended to the existing data in the data warehouse. Incremental will never delete or mutate existing records. Let's walk through a few examples.\nNewly Created Record\nAssume that `updated_at` is our `cursor_field`. Let's say the following data already exists into our data warehouse.\n| name             | deceased | updated_at |\n| :--------------- | :------- | :--------- |\n| Louis XVI        | false    | 1754       |\n| Marie Antoinette | false    | 1755       |\nIn the next sync, the delta contains the following record:\n| name       | deceased | updated_at |\n| :--------- | :------- | :--------- |\n| Louis XVII | false    | 1785       |\nAt the end of this incremental sync, the data warehouse would now contain:\n| name             | deceased | updated_at |\n| :--------------- | :------- | :--------- |\n| Louis XVI        | false    | 1754       |\n| Marie Antoinette | false    | 1755       |\n| Louis XVII       | false    | 1785       |\nUpdating a Record\nLet's assume that our warehouse contains all the data that it did at the end of the previous section. Now, unfortunately the king and queen lose their heads. Let's see that delta:\n| name             | deceased | updated_at |\n| :--------------- | :------- | :--------- |\n| Louis XVI        | true     | 1793       |\n| Marie Antoinette | true     | 1793       |\nThe output we expect to see in the warehouse is as follows:\n| name             | deceased | updated_at |\n| :--------------- | :------- | :--------- |\n| Louis XVI        | false    | 1754       |\n| Marie Antoinette | false    | 1755       |\n| Louis XVII       | false    | 1785       |\n| Louis XVI        | true     | 1793       |\n| Marie Antoinette | true     | 1793       |\nSource-Defined Cursor\nSome sources are able to determine the cursor that they use without any user input. For example, in the exchange rates source, the source knows that the date field should be used to determine the last record that was synced. In these cases, simply select the incremental option in the UI.\n\n(You can find a more technical details about the configuration data model here).\nUser-Defined Cursor\nSome sources cannot define the cursor without user input. For example, in the postgres source, the user needs to choose which column in a database table they want to use as the `cursor field`. In these cases, select the column in the sync settings dropdown that should be used as the `cursor field`.\n\n(You can find a more technical details about the configuration data model here).\nGetting the Latest Snapshot of data\nAs demonstrated in the examples above, with Incremental Append, a record which was updated in the source will be appended to the destination rather than updated in-place. This means that if data in the source uses a primary key (e.g: `user_id` in the `users` table), then the destination will end up having multiple records with the same primary key value.\nHowever, some use cases require only the latest snapshot of the data. This is available by using other flavors of sync modes such as Incremental - Deduped History instead.\nNote that in Incremental Append, the size of the data in your warehouse increases monotonically since an updated record in the source is appended to the destination rather than updated in-place.\nIf you only care about having the latest snapshot of your data, you may want to look at other sync modes that will keep smaller copies of the replicated data or you can periodically run cleanup jobs which retain only the latest instance of each record.\nInclusive Cursors\nWhen replicating data incrementally, Airbyte provides an at-least-once delivery guarantee. This means that it is acceptable for sources to re-send some data when ran incrementally. One case where this is particularly relevant is when a source's cursor is not very granular. For example, if a cursor field has the granularity of a day (but not hours, seconds, etc), then if that source is run twice in the same day, there is no way for the source to know which records that are that date were already replicated earlier that day. By convention, sources should prefer resending data if the cursor field is ambiguous.\nAdditionally, you may run into behavior where you see the same row being emitted during each sync. This will occur if your data has not changed and you attempt to run additional syncs, as the cursor field will always be greater than or equal to itself, causing it to pull the latest row multiple times until there is new data at the source.\nKnown Limitations\nDue to the use of a cursor column, if modifications to the underlying records are made without properly updating the cursor field, then the updated records won't be picked up by the Incremental sync as expected since the source connectors extract delta rows using a SQL query looking like:\n`sql\nSELECT * FROM table WHERE cursor_field >= 'last_sync_max_cursor_field_value'`\nLet's say the following data already exists into our data warehouse.\n| name             | deceased | updated_at |\n| :--------------- | :------- | :--------- |\n| Louis XVI        | false    | 1754       |\n| Marie Antoinette | false    | 1755       |\nAt the start of the next sync, the source data contains the following new record:\n| name      | deceased | updated_at |\n| :-------- | :------- | :--------- |\n| Louis XVI | true     | 1754       |\nAt the end of the second incremental sync, the data warehouse would still contain data from the first sync because the delta record did not provide a valid value for the cursor field (the cursor field is not greater than last sync's max value, `1754 < 1755`), so it is not emitted by the source as a new or modified record.\n| name             | deceased | updated_at |\n| :--------------- | :------- | :--------- |\n| Louis XVI        | false    | 1754       |\n| Marie Antoinette | false    | 1755       |\nSimilarly, if multiple modifications are made during the same day to the same records. If the frequency of the sync is not granular enough (for example, set for every 24h), then intermediate modifications to the data are not going to be detected and emitted. Only the state of data at the time the sync runs will be reflected in the destination.\nThose concerns could be solved by using a different incremental approach based on binary logs, Write-Ahead-Logs (WAL), or also called Change Data Capture (CDC).\nThe current behavior of Incremental is not able to handle source schema changes yet, for example, when a column is added, renamed or deleted from an existing table etc. It is recommended to trigger a Full refresh - Overwrite to correctly replicate the data to the destination with the new schema changes.\nIf you are not satisfied with how transformations are applied on top of the appended data, you can find more relevant SQL transformations you might need to do on your data in the Connecting EL with T using SQL (part 1/2)\nRelated information\n\nExplore Airbyte\u2019s incremental data synchronization.\n",
    "tag": "airbyte"
  },
  {
    "title": "Full Refresh - Overwrite",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/understanding-airbyte/connections/full-refresh-overwrite.md",
    "content": "Full Refresh - Overwrite\nOverview\nThe Full Refresh modes are the simplest methods that Airbyte uses to sync data, as they always retrieve all available information requested from the source, regardless of whether it has been synced before. This contrasts with Incremental sync, which does not sync data that has already been synced before.\nIn the Overwrite variant, new syncs will destroy all data in the existing destination table and then pull the new data in. Therefore, data that has been removed from the source after an old sync will be deleted in the destination table.\nExample Behavior\nOn the nth sync of a full refresh connection:\nReplace existing data with new data. The connection does not create any new tables.\ndata in the destination before the sync:\n| Languages |\n| :--- |\n| Python |\n| Java |\nnew data:\n| Languages |\n| :--- |\n| Python |\n| Java |\n| Ruby |\ndata in the destination after the sync:\n| Languages |\n| :--- |\n| Python |\n| Java |\n| Ruby |\nNote: This is how Singer target-bigquery does it.\nIn the future\nWe will consider making other flavors of full refresh configurable as first-class citizens in Airbyte. e.g. On new data, copy old data to a new table with a timestamp, and then replace the original table with the new data. As always, we will focus on adding these options in such a way that the behavior of each connector is both well documented and predictable.\nRelated information\n\nAn overview of Airbyte\u2019s replication modes.\n",
    "tag": "airbyte"
  },
  {
    "title": "Incremental Sync - Deduped History",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/understanding-airbyte/connections/incremental-deduped-history.md",
    "content": "Incremental Sync - Deduped History\nHigh-Level Context\nThis connector syncs data incrementally, which means that only new or modified data will be synced. In contrast with the Incremental Append mode, this mode updates rows that have been modified instead of adding a new version of the row with the updated data. Simply put, if you've synced a row before and it has since been updated, this mode will combine the two rows\nin the destination and use the updated data. On the other hand, the Incremental Append mode would just add a new row with the updated data.\nOverview\nAirbyte supports syncing data in Incremental Deduped History mode i.e:\n\nIncremental means syncing only replicate new or modified data. This prevents re-fetching data that you have already replicated from a source. If the sync is running for the first time, it is equivalent to a Full Refresh since all data will be considered as new.\nDeduped means that data in the final table will be unique per primary key (unlike Append modes). This is determined by sorting the data using the cursor field and keeping only the latest de-duplicated data row. In dimensional data warehouse jargon defined by Ralph Kimball, this is referred as a Slowly Changing Dimension (SCD) table of type 1.\nHistory means that an additional intermediate table is created in which data is being continuously appended to (with duplicates exactly like Append modes). With the use of primary key fields, it is identifying effective `start` and `end` dates of each row of a record. In dimensional data warehouse jargon, this is referred as a Slowly Changing Dimension (SCD) table of type 2.\n\nIn this flavor of incremental, records in the warehouse destination will never be deleted in the history tables (named with a `_scd` suffix), but might not exist in the final table. A copy of each new or updated record is appended to the history data in the warehouse. Only the `end` date column is mutated when a new version of the same record is inserted to denote effective date ranges of a row. This means you can find multiple copies of the same record in the destination warehouse. We provide an \"at least once\" guarantee of replicating each record that is present when the sync runs.\nOn the other hand, records in the final destination can potentially be deleted as they are de-duplicated. You should not find multiple copies of the same primary key as these should be unique in that table.\nDefinitions\nA `cursor` is the value used to track whether a record should be replicated in an incremental sync. A common example of a `cursor` would be a timestamp from an `updated_at` column in a database table.\nA `cursor field` is the field or column in the data where that cursor can be found. Extending the above example, the `updated_at` column in the database would be the `cursor field`, while the `cursor` is the actual timestamp value used to determine if a record should be replicated.\nWe will refer to the set of records that the source identifies as being new or updated as a `delta`.\nA `primary key` is one or multiple (called `composite primary keys`) fields or columns that is used to identify the unique entities of a table. Only one row per primary key value is permitted in a database table. In the data warehouse, just like in incremental - Append, multiple rows for the same primary key can be found in the history table. The unique records per primary key behavior is mirrored in the final table with incremental deduped sync mode. The primary key is then used to refer to the entity which values should be updated.\nRules\nAs mentioned above, the delta from a sync will be appended to the existing history data in the data warehouse. In addition, it will update the associated record in the final table. Let's walk through a few examples.\nNewly Created Record\nAssume that `updated_at` is our `cursor_field` and `name` is the `primary_key`. Let's say the following data already exists into our data warehouse.\n| name             | deceased | updated_at |\n| :--------------- | :------- | :--------- |\n| Louis XVI        | false    | 1754       |\n| Marie Antoinette | false    | 1755       |\nIn the next sync, the delta contains the following record:\n| name       | deceased | updated_at |\n| :--------- | :------- | :--------- |\n| Louis XVII | false    | 1785       |\nAt the end of this incremental sync, the data warehouse would now contain:\n| name             | deceased | updated_at |\n| :--------------- | :------- | :--------- |\n| Louis XVI        | false    | 1754       |\n| Marie Antoinette | false    | 1755       |\n| Louis XVII       | false    | 1785       |\nUpdating a Record\nLet's assume that our warehouse contains all the data that it did at the end of the previous section. Now, unfortunately the king and queen lose their heads. Let's see that delta:\n| name             | deceased | updated_at |\n| :--------------- | :------- | :--------- |\n| Louis XVI        | true     | 1793       |\n| Marie Antoinette | true     | 1793       |\nThe output we expect to see in the warehouse is as follows:\nIn the history table:\n| name             | deceased | updated_at | start_at | end_at |\n| :--------------- | :------- | :--------- | :------- | :----- |\n| Louis XVI        | false    | 1754       | 1754     | 1793   |\n| Louis XVI        | true     | 1793       | 1793     | NULL   |\n| Louis XVII       | false    | 1785       | 1785     | NULL   |\n| Marie Antoinette | false    | 1755       | 1755     | 1793   |\n| Marie Antoinette | true     | 1793       | 1793     | NULL   |\nIn the final de-duplicated table:\n| name             | deceased | updated_at |\n| :--------------- | :------- | :--------- |\n| Louis XVI        | true     | 1793       |\n| Louis XVII       | false    | 1785       |\n| Marie Antoinette | true     | 1793       |\nSource-Defined Cursor\nSome sources are able to determine the cursor that they use without any user input. For example, in the exchange rates source, the source knows that the date field should be used to determine the last record that was synced. In these cases, simply select the incremental option in the UI.\n\n(You can find a more technical details about the configuration data model here).\nUser-Defined Cursor\nSome sources cannot define the cursor without user input. For example, in the postgres source, the user needs to choose which column in a database table they want to use as the `cursor field`. In these cases, select the column in the sync settings dropdown that should be used as the `cursor field`.\n\n(You can find a more technical details about the configuration data model here).\nSource-Defined Primary key\nSome sources are able to determine the primary key that they use without any user input. For example, in the (JDBC) Database sources, primary key can be defined in the table's metadata.\nUser-Defined Primary key\nSome sources cannot define the cursor without user input or the user may want to specify their own primary key on the destination that is different from the source definitions. In these cases, select the column in the sync settings dropdown that should be used as the `primary key` or `composite primary keys`.\n\nIn this example, we selected both the `campaigns.id` and `campaigns.name` as the composite primary key of our `campaigns` table.\nNote that in Incremental Deduped History, the size of the data in your warehouse increases monotonically since an updated record in the source is appended to the destination history table rather than updated in-place as it is done with the final table. If you only care about having the latest snapshot of your data, you may want to periodically run cleanup jobs which retain only the latest instance of each record in the history tables.\nInclusive Cursors\nWhen replicating data incrementally, Airbyte provides an at-least-once delivery guarantee. This means that it is acceptable for sources to re-send some data when ran incrementally. One case where this is particularly relevant is when a source's cursor is not very granular. For example, if a cursor field has the granularity of a day (but not hours, seconds, etc), then if that source is run twice in the same day, there is no way for the source to know which records that are that date were already replicated earlier that day. By convention, sources should prefer resending data if the cursor field is ambiguous.\nKnown Limitations\nDue to the use of a cursor column, if modifications to the underlying records are made without properly updating the cursor field, then the updated records won't be picked up by the Incremental sync as expected since the source connectors extract delta rows using a SQL query looking like:\n`sql\nselect * from table where cursor_field > 'last_sync_max_cursor_field_value'`\nLet's say the following data already exists into our data warehouse.\n| name             | deceased | updated_at |\n| :--------------- | :------- | :--------- |\n| Louis XVI        | false    | 1754       |\n| Marie Antoinette | false    | 1755       |\nAt the start of the next sync, the source data contains the following new record:\n| name      | deceased | updated_at |\n| :-------- | :------- | :--------- |\n| Louis XVI | true     | 1754       |\nAt the end of the second incremental sync, the data warehouse would still contain data from the first sync because the delta record did not provide a valid value for the cursor field (the cursor field is not greater than last sync's max value, `1754 < 1755`), so it is not emitted by the source as a new or modified record.\n| name             | deceased | updated_at |\n| :--------------- | :------- | :--------- |\n| Louis XVI        | false    | 1754       |\n| Marie Antoinette | false    | 1755       |\nSimilarly, if multiple modifications are made during the same day to the same records. If the frequency of the sync is not granular enough (for example, set for every 24h), then intermediate modifications to the data are not going to be detected and emitted. Only the state of data at the time the sync runs will be reflected in the destination.\nThose concerns could be solved by using a different incremental approach based on binary logs, Write-Ahead-Logs (WAL), or also called Change Data Capture (CDC).\nThe current behavior of Incremental is not able to handle source schema changes yet, for example, when a column is added, renamed or deleted from an existing table etc. It is recommended to trigger a Full refresh - Overwrite to correctly replicate the data to the destination with the new schema changes.\nAdditionally, this sync mode is only supported for destinations where dbt/normalization is possible for the moment. The de-duplicating logic is indeed implemented as dbt models as part of a sequence of transformations applied after the Extract and Load activities (thus, an ELT approach). Nevertheless, it is theoretically possible that destinations can handle directly this logic (maybe in the future) before actually writing records to the destination (as in traditional ETL manner), but that's not the way it is implemented at this time.\nIf you are not satisfied with how transformations are applied on top of the appended data, you can find more relevant SQL transformations you might need to do on your data in the Connecting EL with T using SQL (part 1/2)\nRelated information\n\nAn overview of Airbyte\u2019s replication modes.\n",
    "tag": "airbyte"
  },
  {
    "title": "Connections and Sync Modes",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/understanding-airbyte/connections",
    "content": "Connections and Sync Modes\nA connection is a configuration for syncing data between a source and a destination. To setup a connection, a user must configure things such as:\n\nSync schedule: when to trigger a sync of the data.\nDestination Namespace and stream names: where the data will end up being written.\nA catalog selection: which streams and fields to replicate from the source\nSync mode: how streams should be replicated (read and write):\nOptional transformations: how to convert Airbyte protocol messages (raw JSON blob) data into some other data representations. \n\nSync schedules\nSync schedules are explained below. For information about catalog selections, see AirbyteCatalog & ConfiguredAirbyteCatalog.\nSyncs will be triggered by either:\n\nA manual request (i.e: clicking the \"Sync Now\" button in the UI)\nA schedule\n\nWhen a scheduled connection is first created, a sync is executed as soon as possible. After that, a sync is run once the time since the last sync (whether it was triggered manually or due to a schedule) has exceeded the schedule interval. For example, consider the following illustrative scenario:\n\nOctober 1st, 2pm, a user sets up a connection to sync data every 24 hours.\nOctober 1st, 2:01pm: sync job runs\nOctober 2nd, 2:01pm: 24 hours have passed since the last sync, so a sync is triggered.\nOctober 2nd, 5pm: The user manually triggers a sync from the UI\nOctober 3rd, 2:01pm: since the last sync was less than 24 hours ago, no sync is run\nOctober 3rd, 5:01pm: It has been more than 24 hours since the last sync, so a sync is run\n\nDestination namespace\nThe location of where a connection replication will store data is referenced as the destination namespace. The destination connectors should create and write records (for both raw and normalized tables) in the specified namespace which should be configurable in the UI via the Namespace Configuration field (or NamespaceDefinition in the API). You can read more about configuring namespaces here.\nDestination stream name\nPrefix stream name\nStream names refer to table names in a typical RDBMS. But it can also be the name of an API endpoint, etc. Similarly to the namespace, stream names can be configured to diverge from their names in the source with a \"prefix\" field. The prefix is prepended to the source stream name in the destination.\nStream-specific customization\nAll the customization of namespace and stream names described above will be equally applied to all streams selected for replication in a catalog per connection. If you need more granular customization, stream by stream, for example, or with different logic rules, then you could follow the tutorial on customizing transformations with dbt.\nSync modes\nA sync mode governs how Airbyte reads from a source and writes to a destination. Airbyte provides different sync modes to account for various use cases. To minimize confusion, a mode's behavior is reflected in its name. The easiest way to understand Airbyte's sync modes is to understand how the modes are named.\n\nThe first part of the name denotes how the source connector reads data from the source:\nIncremental: Read records added to the source since the last sync job. (The first sync using Incremental is equivalent to a Full Refresh)\nMethod 1: Using a cursor. Generally supported by all connectors whose data source allows extracting records incrementally.\nMethod 2: Using change data capture. Only supported by some sources. See CDC for more info.\n\n\nFull Refresh: Read everything in the source.\nThe second part of the sync mode name denotes how the destination connector writes data. This is not affected by how the source connector produced the data:\nOverwrite: Overwrite by first deleting existing data in the destination.\nAppend: Write by adding data to existing tables in the destination.\nDeduped History: Write by first adding data to existing tables in the destination to keep a history of changes. The final table is produced by de-duplicating the intermediate ones using a primary key.\n\nA sync mode is therefore, a combination of a source and destination mode together. The UI exposes the following options, whenever both source and destination connectors are capable to support it for the corresponding stream:\n\nFull Refresh Overwrite: Sync the whole stream and replace data in destination by overwriting it.\nFull Refresh Append: Sync the whole stream and append data in destination.\nIncremental Append: Sync new records from stream and append data in destination.\nIncremental Deduped History: Sync new records from stream and append data in destination, also provides a de-duplicated view mirroring the state of the stream in the source.\n\nOptional operations\nAirbyte basic normalization\nAs described by the Airbyte Protocol from the Airbyte Specifications, replication is composed of source connectors that are transmitting data in a JSON format. It is then written as such by the destination connectors.\nOn top of this replication, Airbyte provides the option to enable or disable an additional transformation step at the end of the sync called basic normalization. This operation is:\n\nOnly available for destinations that support dbt execution\nAutomatically generates a pipeline or DAG of dbt transformation models to convert JSON blob objects into normalized tables\nRuns and applies these dbt models to the data written in the destination\n\n:::note\nNormalizing data may cause an increase in your destination's compute cost. This cost will vary depending on the amount of data that is normalized and is not related to Airbyte credit usage.\n:::\nCustom sync operations\nFurther operations can be included in a sync on top of Airbyte basic normalization (or even to replace it completely). See operations for more details.",
    "tag": "airbyte"
  },
  {
    "title": "Full Refresh - Append",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/understanding-airbyte/connections/full-refresh-append.md",
    "content": "Full Refresh - Append\nOverview\nThe Full Refresh modes are the simplest methods that Airbyte uses to sync data, as they always retrieve all available data requested from the source, regardless of whether it has been synced before. This contrasts with Incremental sync, which does not sync data that has already been synced before.\nIn the Append variant, new syncs will take all data from the sync and append it to the destination table. Therefore, if syncing similar information multiple times, every sync will create duplicates of already existing data.\nExample Behavior\nOn the nth sync of a full refresh connection:\nAdd new data to the same table. Do not touch existing data.\ndata in the destination before the nth sync:\n| Languages |\n| :--- |\n| Python |\n| Java |\nnew data:\n| Languages |\n| :--- |\n| Python |\n| Java |\n| Ruby |\ndata in the destination after the nth sync:\n| Languages |\n| :--- |\n| Python |\n| Java |\n| Python |\n| Java |\n| Ruby |\nThis could be useful when we are interested to know about deletion of data in the source. This is possible if we also consider the date, or the batch id from which the data was written to the destination:\nnew data at the n+1th sync:\n| Languages |\n| :--- |\n| Python |\n| Ruby |\ndata in the destination after the n+1th sync:\n| Languages | batch id |\n| :--- | :--- |\n| Python | 1 |\n| Java | 1 |\n| Python | 2 |\n| Java | 2 |\n| Ruby | 2 |\n| Python | 3 |\n| Ruby | 3 |\nIn the future\nWe will consider making a better detection of deletions in the source, especially with `Incremental`, and `Change Data Capture` based sync modes for example.\nRelated information\n\nAn overview of Airbyte\u2019s replication modes.\n",
    "tag": "airbyte"
  },
  {
    "title": "On Airbyte Cloud",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/deploying-airbyte/on-cloud.md",
    "content": "On Airbyte Cloud\nOverview\nAirbyte Cloud requires no setup and can be immediately run from your web browser.\nSetup Guide\n1. Using the Airbyte Cloud invite link provided to you, click `Sign up`, and verify your email.\nIf you don't have an invite, sign up here!\n2. Click on the default workspace.\nYou will be provided 1000 credits to get your first few syncs going!\n\n3. Click on `Onboarding` and follow the directions to create your first connection. Or if you know what you're doing, just get started!\n\n4. You're done!",
    "tag": "airbyte"
  },
  {
    "title": "Deploy Airbyte on Oracle Cloud",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/deploying-airbyte/on-oci-vm.md",
    "content": "Deploy Airbyte on Oracle Cloud\nThis page guides you through deploying Airbyte Open Source on an Oracle Cloud Infrastructure (OCI) Virtual Machine (VM) Instance.\n:::info\nThese instructions have been tested on an Oracle Linux 7 instance.\n:::\nPrerequisites\nTo deploy Airbyte Open Source on Oracle cloud:\n\nCreate an OCI VM compute instance\nAllowlist a port for a CIDR range in the security list of your OCI VM Instance subnet\nConnect to the instance using a bastion port forwarding session\n\n:::caution\nFor security reasons, we strongly recommend not having a Public IP for the Instance where you are running Airbyte.\n:::\nSet up the environment\nInstall Docker and Docker Compose on the VM:\n\nInstall Docker\n\nIn the terminal connected to your OCI Instance for Airbyte, run the following commands:\n```bash\nsudo yum update -y\nsudo yum install -y docker\nsudo service docker start\nsudo usermod -a -G docker $USER\n```\n\nInstall Docker Compose\n\nIn the terminal connected to your OCI Instance for Airbyte, run the following commands:\n```bash\nsudo yum install -y docker-compose-plugin\ndocker compose version\n```\nInstall and start Airbyte\nDownload the Airbyte repository and deploy it on the VM:\n\n\nRun the following commands to clone the Airbyte repo:\n```bash\nmkdir airbyte && cd airbyte\nwget https://raw.githubusercontent.com/airbytehq/airbyte-platform/main/{.env,flags.yml,docker-compose.yaml}\n```\n\n\nRun the following commands to get Airbyte running on your OCI VM instance using Docker compose:\n```bash\nwhich docker\nsudo /usr/local/bin/docker compose up -d\n``` \n\n\nOpen up a Browser and visit port 8000 - http://localhost:8000/\n\n\nAlternatively, you can get Airbyte running on your OCI VM instance using a different approach.\n\n\nIn the terminal connected to your OCI Instance for Airbyte, run the command: \n`bash\nssh opc@bastion-host-public-ip -i <private-key-file.key> -L 8000:oci-private-instance-ip:8000`\nReplace `<private-key-file.key>` with the path to your private key.\n\n\nOn your browser, visit port 8000 port 8000\n\n\nTroubleshooting",
    "tag": "airbyte"
  },
  {
    "title": "Deploy Airbyte on GCP (Compute Engine)",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/deploying-airbyte/on-gcp-compute-engine.md",
    "content": "Deploy Airbyte on GCP (Compute Engine)\nThis page guides you through deploying Airbyte Open Source on a Google Cloud Platform (GCP) Compute Engine instance by setting up the deployment environment, installing and starting Airbyte, and connecting it to the GCP instance.\n:::info\nThe instructions have been tested on a `Debian GNU/Linux 10` VM instance.\n:::\nRequirements\n\nTo test Airbyte, we recommend an `e2.medium` instance and provision at least 30GBs of disk per node\nTo deploy Airbyte in a production environment, we recommend a `n1-standard-2` instance\n\nSet up the environment\n\nCreate a new GCP instance.\nSet variables in your local terminal:\n\n`bash\nPROJECT_ID=PROJECT_ID_WHERE_YOU_CREATED_YOUR_INSTANCE\nINSTANCE_NAME=airbyte # or any other name that you've used`\n\nInstall Google Cloud SDK and initialize the gcloud command-line tool using the following commands in your local terminal:\n\n`bash\nbrew install --cask google-cloud-sdk\ngcloud init`\n\nList all instances in your project and verify that you can see the Airbyte instance you created in step 1 in your local terminal:\n\n```bash\nVerify you can see your instance\ngcloud --project $PROJECT_ID compute instances list\n[...] # You should see the airbyte instance you just created\n```\n\nConnect to your instance in your local terminal:\n\n`bash\ngcloud --project=$PROJECT_ID beta compute ssh $INSTANCE_NAME`\n\nInstall Docker on your VM instance by following the below commands in your VM terminal:\n\n`bash\nsudo apt-get update\nsudo apt-get install -y apt-transport-https ca-certificates curl gnupg2 software-properties-common\ncurl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add --\nsudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/debian buster stable\"\nsudo apt-get update\nsudo apt-get install -y docker-ce docker-ce-cli containerd.io\nsudo usermod -a -G docker $USER`\n\nInstall `docker-compose` on your VM instance by following the below commands in your VM terminal:\n\n`bash\nsudo apt-get -y install docker-compose-plugin\ndocker compose version`\n\nClose the SSH connection on your VM instance to ensure the group modification is taken into account by following the below command in your VM terminal:\n\n`bash\nlogout`\nInstall and launch Airbyte\nTo install and launch Airbyte:\n\nIn your local terminal, connect to your Google Cloud instance:\n\n`bash\ngcloud --project=$PROJECT_ID beta compute ssh $INSTANCE_NAME`\n\nIn your VM terminal, install Airbyte:\n\n`bash\nmkdir airbyte && cd airbyte\ncurl -sOO https://raw.githubusercontent.com/airbytehq/airbyte-platform/main/{.env,flags.yml,docker-compose.yaml}\ndocker compose up -d`\nConnect to Airbyte\n:::caution\nFor security reasons, we strongly recommended not exposing Airbyte publicly.\n:::\n\nIn your local terminal, create an SSH tunnel to connect the GCP instance to Airbyte:\n\n`bash\ngcloud --project=$PROJECT_ID beta compute ssh $INSTANCE_NAME -- -L 8000:localhost:8000 -N -f`\n\nVerify the connection by visiting http://localhost:8000 in your browser.\n\nTroubleshooting",
    "tag": "airbyte"
  },
  {
    "title": "Deploy Airbyte on Plural",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/deploying-airbyte/on-plural.md",
    "content": "Deploy Airbyte on Plural\n:::tip\nIf you'd prefer to follow along with a video, check out the Plural Airbyte deployment guide video here\n:::\nGetting started\n\nCreate an account on https://app.plural.sh.  \nInstall the Plural CLI by following steps 1, 2, and 3 of the instructions here. Through this, you will also configure your cloud provider and the domain name under which your application will be deployed to.\n\nWe now need a Git repository to store your Plural configuration in. This will also contain the Helm and Terraform files that Plural will autogenerate for you.\nYou have two options:\n- Run `plural init` in any directory to let Plural initiate an OAuth workflow to create a Git repo for you.\n- Create a Git repo manually, clone it, and run `plural init` inside it.\nRunning `plural init` will configure your installation and cloud provider for the repo. \nInstalling Airbyte\nTo install Airbyte on your Plural repo, run:\n`bash\nplural bundle install airbyte $CONSOLE_BUNDLE_NAME`\nTo find the console bundle name for your cloud provider, run:\n`bash\nplural bundle list airbyte`\nFor example, this is what it looks like for AWS:\n`bash\nplural bundle install airbyte airbyte-aws`\nPlural's Airbyte distribution currently has support for AWS, GCP and Azure. Select the Cloud that best fits your infrastructure.\nThe CLI prompts you to choose whether or not you want to use Plural OIDC. OIDC allows you to login to the applications you host on Plural with your login to app.plural.sh, acting as an SSO provider.\nAfter this, run:\n`bash\nplural build\nplural deploy --commit \"deploying airbyte\"`\nAdding the Plural Console\nTo make management of your installation as simple as possible, we recommend installing the Plural Console.  The console provides tools to manage resource scaling, receiving automated upgrades, dashboards tailored to your Airbyte installation, and log aggregation. Run:\n`bash\nplural bundle install console console-aws\nplural build\nplural deploy --commit \"deploying the console too\"`\nAccessing your Airbyte Installation\nNavigate to `airbyte.SUBDOMAIN_NAME.onplural.sh` to access the Airbyte UI.\nAccessing your Console Installation\nTo monitor and manage your Airbyte installation, navigate to the Plural Console at `console.YOUR_ORGANIZATION.onplural.sh` (or whichever subdomain you chose).\nAdvanced Use Cases\nRunning with External Airflow\nIf you have an Airflow instance external to the Plural Kubernetes cluster with your Airbyte installation, you can still have Airflow manage the Airbyte installation. This happens because Basic Auth setup is required for external authentication - Plural OIDC is not sufficient here.\nIn your `context.yaml` file located at the root of your Plural installation, create a user with Basic Auth for Airbyte. Then on your Airbyte Airflow connector, use the following URL template:\n`https://username:password@airbytedomain`\nTroubleshooting\nIf you have any issues with installing Airbyte on Plural, join Plural's Discord Community.\nIf you'd like to request any new features for our Airbyte installation, open an issue or PR at https://github.com/pluralsh/plural-artifacts.\nFurther Reading",
    "tag": "airbyte"
  },
  {
    "title": "Deploy Airbyte on DigitalOcean",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/deploying-airbyte/on-digitalocean-droplet.md",
    "content": "Deploy Airbyte on DigitalOcean\nThis page guides you through deploying Airbyte Open Source on a DigitalOcean droplet by setting up the deployment environment, and installing and starting Airbyte.  \nAlternatively, you can deploy Airbyte on DigitalOcean in one click using their marketplace.\nRequirements\n\nTo test Airbyte, we recommend a $20/month droplet\nTo deploy Airbyte in a production environment, we recommend a $40/month instance\n\nSet up the Environment\nTo deploy Airbyte Open Source on DigitalOcean:\n\nCreate a DigitalOcean droplet.\nConnect to the droplet using the Droplet Console.\nTo update the available packages and install Docker, run the following command:\n\n`bash\n      sudo apt update\n      sudo apt install apt-transport-https ca-certificates curl software-properties-common\n      curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\n      sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu focal stable\"\n      sudo apt install docker-ce\n      sudo systemctl status docker\n      sudo usermod -aG docker ${USER}\n      su - ${USER}`\n\nTo install Docker-Compose, run the following command:\n\n`bash\n    sudo apt install docker-compose-plugin\n    docker compose version`\nInstall Airbyte\nTo install and start Airbyte :\n\nRun the following command:\n\n`bash\n  mkdir airbyte && cd airbyte\n  wget https://raw.githubusercontent.com/airbytehq/airbyte-platform/main/{.env,flags.yml,docker-compose.yaml}\n  docker compose up -d`\n\nVerify the connection by visiting http://localhost:8000 in your browser.\n\nTroubleshooting",
    "tag": "airbyte"
  },
  {
    "title": "Deploy Airbyte on AWS (Amazon EC2)",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/deploying-airbyte/on-aws-ec2.md",
    "content": "Deploy Airbyte on AWS (Amazon EC2)\nThis page guides you through deploying Airbyte Open Source on an Amazon EC2 instance by setting up the deployment environment, installing and starting Airbyte, and connecting it to the Amazon EC2 instance.\n:::info\nThe instructions have been tested on Amazon Linux 2 AMI (HVM).\n:::\nRequirements\n\nTo test Airbyte, we recommend a `t2.medium` instance\nTo deploy Airbyte in a production environment, we recommend a `t2.large` instance\nMake sure your Docker Desktop app is running to ensure all services run smoothly\nCreate and download an SSH key to connect to the instance\n\nSet up the environment\n\nTo connect to your instance, run the following command on your local terminal:\n\n`bash\nSSH_KEY=~/Downloads/dataline-key-airbyte.pem # the file path you downloaded the key\nINSTANCE_IP=REPLACE_WITH_YOUR_INSTANCE_IP # find your IP address in the EC2 console under the Instances tab\nchmod 400 $SSH_KEY # or ssh will complain that the key has the wrong permissions\nssh -i $SSH_KEY ec2-user@$INSTANCE_IP # connect to the aws ec2 instance AMI and the your private IP address`\n\nTo install Docker, run the following command in your SSH session on the instance terminal:\n\n`bash\nsudo yum update -y\nsudo yum install -y docker\nsudo service docker start\nsudo usermod -a -G docker $USER`\n\nTo install `docker-compose`, run the following command in your ssh session on the instance terminal:\n\n`bash\nsudo yum install -y docker-compose-plugin\ndocker compose version`\n\nTo close the SSH connection, run the following command in your SSH session on the instance terminal:\n\n`bash\nlogout`\nInstall and start Airbyte\nIn your local terminal, run the following commands:\n\nConnect to your instance:\n\n`bash\nssh -i $SSH_KEY ec2-user@$INSTANCE_IP`\n\nInstall Airbyte:\n\n`bash\nmkdir airbyte && cd airbyte\nwget https://raw.githubusercontent.com/airbytehq/airbyte-platform/main/{.env,flags.yml,docker-compose.yaml}\ndocker compose up -d # run the Docker container`\nConnect to Airbyte\n:::caution\nFor security reasons, we strongly recommend not exposing Airbyte on Internet available ports.\n:::\n\nCreate an SSH tunnel for port 8000:\n\nIf you want to use different ports, modify `API_URL` in your .env file and restart Airbyte.\nRun the following commands in your workstation terminal from the downloaded key folder:\n``` bash\nIn your workstation terminal\nSSH_KEY=~/Downloads/dataline-key-airbyte.pem\nssh -i $SSH_KEY -L 8000:localhost:8000 -N -f ec2-user@$INSTANCE_IP\n```\n\nVisit `http://localhost:8000` to verify the deployment.\n\nGet Airbyte logs in CloudWatch\nFollow this guide to get your logs from your Airbyte Docker containers in CloudWatch.\nTroubleshooting",
    "tag": "airbyte"
  },
  {
    "title": "On Restack",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/deploying-airbyte/on-restack.md",
    "content": "On Restack\nGetting Started\nTo deploy Airbyte with Restack:\n\nSign up for a Restack account.\nAdd AWS credentials with AdministratorAccess.\nOne-click cluster creation with Restack.\nDeploy Airbyte on Restack.\nStart using Airbyte.\nDeploy multiple instances of Airbyte.\n\nSign up for a Restack account\nTo Sign up for a Restack account, visit www.restack.io/signup. You can sign up with your corporate email address or your GitHub profile. You do not need a credit card to sign up.\nIf you already have an account, login to Restack at www.restack.io/login.\nAdd AWS credentials with AdministratorAccess\nTo deploy Airbyte in your own AWS infrastructure with Restack, you will need to add your credentials as the next step. \nMake sure that this account has AdministratorAccess. This is how Restack can ensure an end-to-end cluster creation and cluster management process.\n\nNavigate to Clusters in the left-hand navigation menu.\nSelect the Credentials tab.\nClick Add credential.\nGive a suitable title to your credentials for managing them later.\nEnter your AWS Access Key ID and AWS Secret Access key.\nClick Add credential.\n\nOne-click cluster creation with Restack\n:::tip\nRunning your application on a Kubernetes cluster lets you deploy, scale and monitor the application reliably. \n:::\nOnce you have added your credentials: \n1. Navigate to the Clusters tab on the same page and click on Create cluster.\n2. Give a suitable name to your cluster.\n3. Select the region you want to deploy the cluster in.\n4. Select the AWS credentials you added in the previous step.\nThe cluster creation process will start automatically. Once the cluster is ready, you will get an email on the email id connected with your account. \nCreating a cluster is a one-time process. From here you can add other open source tools or multiple instances of Airbyte in the same cluster.\nAny application you deploy in your cluster will be accessible via a free restack domain. \nContact the Restack team via chat to set a custom domain for your Airbyte instances. \nDeploy Airbyte on Restack\n\nClick Add application from the Cluster description or go to the Applications tab in the left hand side navigation.\nClick Airbyte.\nSelect the cluster you have already provisioned.\nClick Add application.\n\nStart using Airbyte\nAirbyte will be deployed on your cluster and you can access it using the link under the URL tab. \nYou can also check the workloads and volumes that are deployed within Airbyte.\nDeploy multiple instances of Airbyte\nRestack makes it easier to deploy multiple instances of Airbyte on the same or multiple clusters. ",
    "tag": "airbyte"
  },
  {
    "title": "Deploy Airbyte on Azure (Cloud Shell)",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/deploying-airbyte/on-azure-vm-cloud-shell.md",
    "content": "Deploy Airbyte on Azure (Cloud Shell)\nThis page guides you through deploying Airbyte Open Source on a Microsoft Azure VM by setting up the deployment environment, installing and starting Airbyte, and connecting it to the VM.\n:::info\nThe instructions have been tested on a standard DS1 v2 (1 vcpu, 3.5 GiB memory) Microsoft Azure VM with Ubuntu 18.04.\n:::\nSet up the environment\nInstall Docker and Docker Compose in the VM:\n\n\nCreate a new VM and generate the SSH keys to connect to the VM. You\u2019ll need the SSH keys to connect to the VM remotely later. \n\n\nTo connect to the VM, run the following command in the Azure Cloud Shell:\n`bash\nssh <admin username>@<IP address>`\nIf successfully connected to the VM, the working directory of Cloud Shell should look like this: `<admin username>@<virtual machine name>:~$`\n\n\nTo install Docker, run the following commands:\n`bash\nsudo apt-get update -y\nsudo apt-get install apt-transport-https ca-certificates curl gnupg lsb-release -y\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\necho \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\nsudo apt-get update\nsudo apt-get install docker-ce docker-ce-cli -y\nsudo usermod -a -G docker $USER`\n\n\nTo install Docker Compose, run the following command:\n`bash\nsudo apt-get install docker-compose-plugin -y`\n\n\nCheck Docker Compose version:\n`bash\ndocker compose version`\n\n\nClose the SSH connection to ensure that the group modification is considered:\n`bash\nlogout`\n\n\nReconnect to the VM:\n`bash\nssh <admin username>@<IP address>`\n\n\nInstall and start Airbyte\nDownload Airbyte and deploy it in the VM using Docker Compose:\n\n\nEnsure that you are connected to the VM:\n`bash\nssh <admin username>@<IP address>`\n\n\nCreate and use a new directory:\n`bash \nmkdir airbyte\ncd airbyte`\n\n\nDownload Airbyte from GitHub: \n`bash\nwget https://raw.githubusercontent.com/airbytehq/airbyte-platform/main/{.env,flags.yml,docker-compose.yaml}`\n\n\nTo start Airbyte, run the following command:\n`bash\nsudo docker compose up -d`\n\n\nConnect to Airbyte\nTest a remote connection to your VM locally and verify that Airbyte is up and running.\n\nIn your local machine, open a terminal. \nGo to the folder where you stored the SSH key.\n\nCreate a SSH tunnel for `port 8000` by typing the following command:\n`bash \nssh -N -L 8000:localhost:8000 -i <your SSH key file> <admin username>@<IP address>`\n\n\nOpen a web browser and navigate to `http://localhost:8000`. You will see Airbyte\u2019s landing page. \n\n\n:::caution\nFor security reasons, we strongly recommend not exposing Airbyte on Internet available ports.\n:::\nTroubleshooting",
    "tag": "airbyte"
  },
  {
    "title": "Local Deployment",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/deploying-airbyte/local-deployment.md",
    "content": "Local Deployment\n:::info\nThese instructions have been tested on MacOS, Windows 10 and Ubuntu 22.04.\n:::\nSetup & launch Airbyte\n\nInstall `Docker Engine` and the `Docker Compose plugin` on your workstation (see instructions).\nAfter Docker is installed, you can immediately get started locally by running:\n\n```bash\nclone Airbyte from GitHub\ngit clone https://github.com/airbytehq/airbyte.git\nswitch into Airbyte directory\ncd airbyte\nstart Airbyte\n./run-ab-platform.sh\n```\n\nIn your browser, just visit http://localhost:8000\nYou will be asked for a username and password. By default, that's username `airbyte` and password `password`. Once you deploy airbyte to your servers, be sure to change these:\n\n```yaml\nProxy Configuration\nSet to empty values, e.g. \"\" to disable basic auth\nBASIC_AUTH_USERNAME=your_new_username_here\nBASIC_AUTH_PASSWORD=your_new_password_here\n```\n\nStart moving some data!\n\nDeploy on Windows\nAfter installing the WSL 2 backend and Docker you should be able to run containers using Windows PowerShell. Additionally, as we note frequently, you will need `docker-compose` to build Airbyte from source. The suggested guide already installs `docker-compose` on Windows.\nSetup Guide\n1. Check out system requirements from Docker documentation.\nFollow the steps on the system requirements, and necessarily, download and install the Linux kernel update package.\n2. Install Docker Desktop on Windows.\nInstall Docker Desktop from here.\nMake sure to select the options:\n\nEnable Hyper-V Windows Features\nInstall required Windows components for WSL 2\n   when prompted. After installation, it will require to reboot your computer.\n\n3. You're done!\n`bash\ngit clone https://github.com/airbytehq/airbyte.git\ncd airbyte\nbash run-ab-platform.sh`\n\nIn your browser, just visit http://localhost:8000\nYou will be asked for a username and password. By default, that's username `airbyte` and password `password`. Once you deploy airbyte to your servers, be sure to change these.\nStart moving some data!\n\nTroubleshooting",
    "tag": "airbyte"
  },
  {
    "title": "Deploy Airbyte on Kubernetes using Helm (Beta)",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/deploying-airbyte/on-kubernetes-via-helm.md",
    "content": "Deploy Airbyte on Kubernetes using Helm (Beta)\nOverview\nAirbyte allows scaling sync workloads horizontally using Kubernetes. The core components (api server, scheduler, etc) run as deployments while the scheduler launches connector-related pods on different nodes.\nQuickstart\nIf you don't want to configure your own Kubernetes cluster and Airbyte instance, you can use the free, open-source project Plural to bring up a Kubernetes cluster and Airbyte for you. Use this guide to get started.\nAlternatively, you can deploy Airbyte on Restack to provision your Kubernetes cluster on AWS. Follow this guide to get started.\nGetting Started\nCluster Setup\nFor local testing we recommend following one of the following setup guides:\n\nDocker Desktop (Mac)\nMinikube\nNOTE: Start Minikube with at least 4gb RAM with `minikube start --memory=4000`\nKind\n\nFor testing on GKE you can create a cluster with the command line or the Cloud Console UI.\nFor testing on EKS you can install eksctl and run `eksctl create cluster` to create an EKS cluster/VPC/subnets/etc. This process should take 10-15 minutes.\nFor production, Airbyte should function on most clusters v1.19 and above. We have tested support on GKE and EKS. If you run into a problem starting Airbyte, please reach out on the `#troubleshooting` channel on our Slack or create an issue on GitHub.\nInstall `kubectl`\nIf you do not already have the CLI tool `kubectl` installed, please follow these instructions to install.\nConfigure `kubectl`\nConfigure `kubectl` to connect to your cluster by using `kubectl use-context my-cluster-name`.\nFor GKE:\n\nConfigure `gcloud` with `gcloud auth login`.\n\nOn the Google Cloud Console, the cluster page will have a `Connect` button, which will give a command to run locally that looks like\n`gcloud container clusters get-credentials $CLUSTER_NAME --zone $ZONE_NAME --project $PROJECT_NAME`.\n\n\nUse `kubectl config get-contexts` to show the contexts available.\n\nRun `kubectl config use-context $GKE_CONTEXT` to access the cluster from `kubectl`.\n\nFor EKS:\n\nConfigure your AWS CLI to connect to your project.\nInstall eksctl\nRun `eksctl utils write-kubeconfig --cluster=<CLUSTER NAME>` to make the context available to `kubectl`\nUse `kubectl config get-contexts` to show the contexts available.\nRun `kubectl config use-context <eks context>` to access the cluster with `kubectl`.\n\nInstall helm\nTo install helm simply run:\nFor MacOS:\n`brew install helm`\nFor Linux:\n\nDownload installer script `curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3`\nAssign required premissions `chmod 700 get_helm.sh`\nRun script `./get_helm.sh`\n\nAdd Helm Repository\nFrom now charts are stored in helm-repo thus there're no need to clone the repo each time you need to deploy the chart.\nTo add remote helm repo simply run: `helm repo add airbyte https://airbytehq.github.io/helm-charts`.\nWhere `airbyte` is the name of the repository that will be indexed locally.\nAfter adding the repo, perform the repo indexing process by running `helm repo update`.\nAfter this you can browse all charts uploaded to repository by running `helm search repo airbyte`\nIt'll produce the output below:\n`text\nNAME                            CHART VERSION   APP VERSION     DESCRIPTION                             \nairbyte-oss/airbyte             0.30.23         0.39.37-alpha   Helm chart to deploy airbyte            \nairbyte-oss/airbyte-bootloader  0.30.23         0.39.37-alpha   Helm chart to deploy airbyte-bootloader \nairbyte-oss/pod-sweeper         0.30.23         0.39.37-alpha   Helm chart to deploy airbyte-pod-sweeper\nairbyte-oss/server              0.30.23         0.39.37-alpha   Helm chart to deploy airbyte-server     \nairbyte-oss/temporal            0.30.23         0.39.37-alpha   Helm chart to deploy airbyte-temporal   \nairbyte-oss/webapp              0.30.23         0.39.37-alpha   Helm chart to deploy airbyte-webapp     \nairbyte-oss/worker              0.30.23         0.39.37-alpha   Helm chart to deploy airbyte-worker`\nDeploy Airbyte\nDefault deployment\nIf you don't intend to customise your deployment, you can deploy airbyte as is with default values.\nIn order to do so, run the command: \n`helm install %release_name% airbyte/airbyte`\nCustom deployment\nIn order to customize your deployment, you need to create `values.yaml` file in the local folder and populate it with default configuration override values.\n`values.yaml` example can be located in charts/airbyte folder of the Airbyte repository.\nAfter specifying your own configuration, run the following command:\n`text\nhelm install --values path/to/values.yaml %release_name% airbyte/airbyte`\nMigrate from old charts to new ones\nStarting from `0.39.37-alpha` we've revisited helm charts structure and separated all components of airbyte into their own independent charts, thus by allowing our developers to test single component without deploying airbyte as a whole and by upgrading single component at a time.\nIn most cases upgrade from older monolith chart to a new one should go without any issue, but if you've configured custom logging or specified custom configuration of DB or Logging then follow the instructions listed bellow\nMinio migration\nSince the latest release of bitnami/minio chart, they've changed the way of setting up the credentials for accessing the minio. (written mid-2022)\nGoing forward in new version you need to specify the following values in values yaml for user/password instead old one\nBefore:\n`text\nminio:\n  rootUser: airbyte-user\n  rootPassword: airbyte-password-123`\nAfter:\n```text\nminio:\n  auth:\n    rootUser: minio\n    rootPassword: minio123\n```\nBefore upgrading the chart update values.yaml as stated above and then run:\n\nGet the old rootPassword by running `export ROOT_PASSWORD=$(kubectl get secret --namespace \"default\" %release_name%-minio -o jsonpath=\"{.data.root-password}\" | base64 -d)`\nPerform upgrade of chart by running `helm upgrade %release_name% airbyte/airbyte --set auth.rootPassword=$ROOT_PASSWORD`\nIf you get an error about setting the auth.rootPassword, then you forgot to update the `values.yaml` file\n\nCustom logging and jobs configuration\nStarting from `0.39.37-alpha` if you've configured logging yourself using `logging or jobs` section of `values.yaml` file, you need to update your configuration so you can continue to use your custom logging and jobs configuration.\nSimply declare global value in `values.yaml` file and move everything related to logging and jobs under that section like in the example bellow:\n`text\nglobal:\n    logging:\n        %your_logging_options_here%\n    jobs:\n        %your_jobs_options_here%`\nAfter updating `values.yaml` simply upgrade your chart by running command: \n`shell\nhelm upgrade -f path/to/values.yaml %release_name% airbyte/airbyte`\nDatabase external secrets\nIf you're using external DB secrets, then provide them in `values.yaml` under global.database section in the following format:\n`text\n  database:\n    secretName: \"myOctaviaSecret\"\n    secretValue: \"postgresql-password\"\n    host: \"example.com\"\n    port: \"5432\"`\nAnd upgrade the chart by running: \n```shell\nhelm upgrade -f path/to/values.yaml %release_name% airbyte/airbyte",
    "tag": "airbyte"
  },
  {
    "title": "(Deprecated) Deploy Airbyte on Kubernetes using Kustomize",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/deploying-airbyte/on-kubernetes.md",
    "content": "(Deprecated) Deploy Airbyte on Kubernetes using Kustomize\n:::caution\nThis deployment method uses Kustomize and is only supported up to Airbyte version 0.40.32. For existing deployments, check out commit 21a7e102183e20d2d4998ea70c2a8fe4eac8921b to continue deploying using Kustomize. For new deployments, deploy Airbyte on Kubernetes via Helm.\n:::\nThis page guides you through deploying Airbyte Open Source on Kubernetes. \nRequirements\nTo test locally, you can use one of the following:\n\nDocker Desktop with Kubernetes enabled\nMinikube with at least 4GB RAM\nKind\n\nTo test on Google Kubernetes Engine(GKE), create a standard zonal cluster.\nTo test on  Amazon Elastic Kubernetes Service (Amazon EKS), install eksctl and create a cluster.\n:::info \nAirbyte deployment is tested on GKE and EKS with version v1.19 and above. If you run into problems, reach out on the `#airbyte-help` channel in our Slack or create an issue on GitHub.\n:::\nInstall and configure `kubectl`\nInstall `kubectl` and run the following command to configure it and connect to your cluster:\n`bash\nkubectl use-context <my-cluster-name>`\nTo configure `kubectl` in `GKE`:\n\nInitialize the `gcloud` cli.\nTo view cluster details, go to the `cluster` page in the Google Cloud Console and click `connect`. Run the following command to test cluster details: \n`gcloud container clusters get-credentials <CLUSTER_NAME> --zone <ZONE_NAME> --project <PROJECT_NAME>`.\nTo view contexts, run: `kubectl config get-contexts`.\nTo access the cluster from `kubectl` run : `kubectl config use-context <gke context>`.\n\nTo configure `kubectl` in  `EKS`:\n\nConfigure AWS CLI to connect to your project.\nInstall eksctl.\nTo Make contexts available to `kubectl`, run `eksctl utils write-kubeconfig --cluster=<CLUSTER NAME>` \nTo view available contexts,  run `kubectl config get-contexts`.\nTo access the cluster, run `kubectl config use-context <eks context>`.\n\nConfigure Logs\nDefault configuration\nAirbyte comes with a self-contained Kubernetes deployment and uses a stand-alone `Minio` deployment in both the `dev` and `stable` versions. Logs are published to the `Minio` deployment by default.\nTo send the logs to the local `Minio` deployment, make sure the specified credentials have both read and write permissions.\nCustom configuration\nAirbyte supports logging to the `Minio` layer, `S3` bucket, and `GCS` bucket.\nCustomize the `Minio` log location\nTo write to a custom location, update the following `.env` variable in the `kube/overlays/stable` directory (you will find this directory at the location you launched Airbyte)\n`bash\nS3_LOG_BUCKET=<your_minio_bucket_to_write_logs_in>\nAWS_ACCESS_KEY_ID=<your_minio_access_key>\nAWS_SECRET_ACCESS_KEY=<your_minio_secret_key>\nS3_MINIO_ENDPOINT=<endpoint_where_minio_is_deployed_at>\nS3_LOG_BUCKET_REGION=`\nSet the`S3_PATH_STYLE_ACCESS variable to`true`.\nLet the`S3_LOG_BUCKET_REGION` variable remain empty.\nConfigure the Custom `S3` Log Location\u200b\nFor the `S3` log location, create an S3 bucket with your AWS credentials.\nTo write to a custom location, update the following `.env` variable in the `kube/overlays/stable` directory (you can find this directory at the location you launched Airbyte)\n``` bash\nS3_LOG_BUCKET=\nS3_LOG_BUCKET_REGION=\nSet this to empty.\nS3_MINIO_ENDPOINT=\nSet this to empty.\nS3_PATH_STYLE_ACCESS=\n```\nReplace the following variable in`.secrets`file in the`kube/overlays/stable` directory:\n`bash\nAWS_ACCESS_KEY_ID=<your_aws_access_key_id>\nAWS_SECRET_ACCESS_KEY=<your_aws_secret_access_key>`\nConfigure the Custom GCS Log Location\u200b\nCreate a GCS bucket and  GCP credentials if you haven\u2019t already. Make sure your GCS log bucket has read/write permission.\nTo configure the custom log location:\nBase encode the GCP JSON secret with the following command:\n```bash\nThe output of this command will be a Base64 string.\n$ cat gcp.json | base64\n```\nTo populate the`gcs-log-creds`secrets with the Base64-encoded credential, take the encoded GCP JSON secret from the previous step and add it to`secret-gcs-log-creds.yaml`file as the value for`gcp.json` key. \n`bash\napiVersion: v1\nkind: Secret\nmetadata:\n name: gcs-log-creds\n namespace: default\ndata:\n gcp.json: <base64-encoded-string>`\nIn the `kube/overlays/stable` directory, update the  `GCS_LOG_BUCKET` with your GCS log bucket credentials:\n`bash\nGCS_LOG_BUCKET=<your_GCS_bucket_to_write_logs_in>`\nModify `GOOGLE_APPLICATION_CREDENTIALS` to the path to `gcp.json` in the `.secrets` file at `kube/overlays/stable` directory.\n```bash\nThe path the GCS creds are written to. Unless you know what you are doing, use the below default value.\nGOOGLE_APPLICATION_CREDENTIALS=/secrets/gcs-log-creds/gcp.json\n```\nLaunch Airbyte\nThe following commands will help you launch Airbyte:\n`bash\ngit clone https://github.com/airbytehq/airbyte.git\ncd airbyte\nkubectl apply -k kube/overlays/stable`\nTo check the pod status, run `kubectl get pods | grep airbyte`.\nIf you are on Windows, run `kubectl get pods` to the list of pods.\nRun `kubectl port-forward svc/airbyte-webapp-svc 8000:80`  to allow access to the UI/API.\nNavigate to http://localhost:8000 in your browser to verify the deployment.\nDeploy Airbyte on Kubernetes in production\nSet resource limits\n\n\nCore container pods \n\n\nTo provide different resource requirements for core pods, set resource limits in the  `kube/overlays/stable-with-resource-limits/set-resource-limits.yaml` file.\n\n\nTo launch Airbyte with new resource limits,     use the `kubectl apply -k kube/overlays/stable-with-resource-limits command.\n\n\nConnector pods\n\n\nBy default, connector pods launch without resource limits. To add resource limit, configure the `Docker resource limits` section of the `.env` file in the `kube/overlays` directory.\n\n\nVolume sizes\n\n\nTo specify different volume sizes for the persistent volume backing Airbyte, modify `kube/resources/volume-*`  files.\n\n\nIncrease job parallelism\nThe ability to run parallel jobs like getting specs, checking connections, discovering schemas and performing syncs is limited by a few factors. `Airbyte-worker-pods` picks and executes the job. Increasing the number of workers will allow more jobs to be processed.\nTo create more worker pods, increase the number of replicas for the `airbyte-worker` deployment. Refer to examples of increasing worker pods in a Kustomization patch in `airbyte/kube/overlays/dev-integration-test/kustomization.yaml` and `airbyte/kube/overlays/dev-integration-test/parallelize-worker.yaml` \nTo limit the exposed ports in `.env`  file, set the value to `TEMPORAL_WORKER_PORTS`. You can run jobs parallely at each exposed port.\nIf you do not have enough ports to communicate, the jobs might not complete or halt until ports become available.\nYou can set a limit for the maximum parallel jobs that run on the pod. Set the value to `MAX_SPEC_WORKERS`, `MAX_CHECK_WORKERS`, `MAX_DISCOVER_WORKERS`, and `MAX_SYNC_WORKERS` variables in the worker pod deployment and not in `.env` file. You can use these values to create separate worker deployments for each type of worker with different resource allocations.\nCloud Logging\nAirbyte writes logs to two different directories: The `App-logging` directory and the `job-logging` directory. App logs, server logs, and scheduler logs are written to the `app-logging` directory. Job logs are written to the `job-logging` directory. Both directories live at the top level. For example, the app logging directory may live at `s3://log-bucket/app-logging`. We recommend having a dedicated logging bucket and not using it for other purposes.\nAirbyte publishes logs every minute, so it\u2019s normal to have minute-long log delays. Cloud Storages do not support append operations. Each publisher creates its own log files, which means you will have hundreds of files in your log bucket.\nEach log file is uncompressed and named `{yyyyMMddHH24mmss}_{podname}_{UUID}`.\nTo view logs, navigate to the relevant folder and download the file for the time period you want.\nUse external databases\nYou can configure a custom database instead of a simple `postgres` container in Kubernetes. This separate instance (AWS RDS or Google Cloud SQL) should be easier and safer to maintain than Postgres on your cluster.\nCustomize Airbytes Manifests\nWe use Kustomize to allow configuration for different environments. Our shared resources are in the `kube/resources` directory. We recommend defining overlays for each environment and creating your own overlay to customize your deployments. The overlay can live in your own version control system.\nAn example of `kustomization.yaml`  file:\n```bash\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nbases: https://github.com/airbytehq/airbyte.git/kube/overlays/stable?ref=master\n```\nView Raw Manifests\nTo view manifests for a specific overlay that Kustomize applies to your Kubernetes cluster, run `kubectl kustomize kube/overlays/stable`. \nHelm Charts\nFor detailed information about Helm Charts, refer to the charts readme file.\nOperator Guide\nView API server logs\nYou can view real-time logs in `kubectl logs deployments/airbyte-server` directory and download them from the Admin Tab.\nConnector Container Logs\u200b\nAll logs can be accessed by viewing the scheduler logs. As for connector container logs, use Airbyte UI or Airbyte API to isolate them for a specific job attempt and for easier understanding. Connector pods launched by Airbyte will not relay logs directly to Kubernetes logging. You must access these logs through Airbyte.\nResize Volumes\nTo resize a volume, change the `.spec.resources.requests.storage` value. After re-applying, extend the mount(if that operation is supported for your mount type). For a production deployment, track the usage of volumes to ensure they don't run out of space.\nCopy Files in Volumes\nTo copy files, use the cp command in kubectl.\nList Files\nTo list files, run:\n`kubectl exec -it airbyte-server-6b5747df5c-bj4fx ls /tmp/workspace/8`\nRead Files\nTo read files, run:\n`kubectl exec -it airbyte-server-6b5747df5c-bj4fx cat /tmp/workspace/8/0/logs.log`\nPersistent storage on Google Kubernetes Engine(GKE) regional cluster\nRunning Airbyte on a GKE regional cluster requires enabling persistent regional storage. Start with enabling CSE driver on GKE and add `storageClassName: standard-rwo` to the volume-configs.yamll.\nSample `volume-configs.yaml` file:\n`bash\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: airbyte-volume-configs\n  labels:\n    airbyte: volume-configs\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 500Mi\n  storageClassName: standard-rwo`\nTroubleshooting\nIf you encounter any issues, reach out to our community on Slack.",
    "tag": "airbyte"
  },
  {
    "title": "On Setting up a New Connection",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/troubleshooting/new-connection.md",
    "content": "\ndescription: Common issues when trying to set up a new connection (source/destination)\nOn Setting up a New Connection\nAirbyte is stuck while loading required configuration parameters for my connector\nExample of the issue:\n\nTo load configuration parameters, Airbyte must first `docker pull` the connector's image, which may be many hundreds of megabytes. Under poor connectivity conditions, the request to pull the image may take a very long time or time out. More context on this issue can be found here. If your Internet speed is less than 30Mbps down or are running bandwidth-consuming workloads concurrently with Airbyte, you may encounter this issue. Run a speed test to verify your internet speed.\nOne workaround is to manually pull the latest version of every connector you'll use then resetting Airbyte. Note that this will remove any configured connections, sources, or destinations you currently have in Airbyte. To do this:\n\nDecide which connectors you'd like to use. For this example let's say you want the Postgres source and the Snowflake destination.\nFind the Docker image name of those connectors. Look here for sources and here for destinations. For each of the connectors you'd like to use, copy the value of the `dockerRepository` and `dockerImageTag` fields. For example, for the Postgres source this would be `airbyte/source-postgres` and e.g `0.1.6`.\nFor each of the connectors you'd like to use, from your shell run `docker pull <repository>:<tag>`, replacing `<repository>` and `<tag>` with the values copied from the step above e.g: `docker pull airbyte/source-postgres:0.1.6`.\nOnce you've finished downloading all the images, from the Airbyte repository root run `docker compose down -v` followed by `docker compose up`.\nThe issue should be resolved.\n\nIf the above workaround does not fix your problem, please report it here or in our Slack.\nConnection refused errors when connecting to a local db\nDepending on your Docker network configuration, you may not be able to connect to `localhost` or `127.0.0.1` directly.\nIf you are running into connection refused errors when running Airbyte via Docker Compose on Mac, try using `host.docker.internal` as the host. On Linux, you may have to modify `docker-compose.yml` and add a host that maps to your local machine using extra_hosts.\nI don\u2019t see a form when selecting a connector\nWe\u2019ve had that issue once. (no spinner & 500 http error). We don\u2019t know why. Resolution: try to stop airbyte (`docker compose down`) & restart (`docker compose up`)\nConnection hangs when trying to run the discovery step\nYou receive the error below when you tried to sync a database with a lot of tables (6000 or more).\n`bash\nairbyte-server   | io.grpc.StatusRuntimeException: RESOURCE_EXHAUSTED: grpc: received message larger than max (<NUMBER> vs. 4194304)`\nThere are two Github issues tracking this problem: Issue #3942 and Issue #3943\nThe workaround for this is trying to transfer the tables you really want to use to another namespace. If you need all tables you should split them into separate namespaces and try to use two connections.",
    "tag": "airbyte"
  },
  {
    "title": "On Running a Sync",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/troubleshooting/running-sync.md",
    "content": "On Running a Sync\nOne of your sync jobs is failing\nSeveral things to check:\n\nIs Airbyte updated to your latest version? You can see the latest version here. If not, please upgrade to the latest one\nIs the connector that is failing updated to the latest version? You can check the latest version available for the connectors in the yamls here. If you don't have the latest connector version, make sure you first update to the latest Airbyte version, and then go to the Admin section in the web app and put the right version in the cell for the connector. Then try again.\n\nIf the above workaround does not fix your problem, please report it here or in our Slack.\nYour incremental connection is not working\nOur current version of incremental is append. It works from a cursor field. So you need to check which cursor field you're using and if it's well populated in every record in your table.\nIf this is true, then, there are still several things to check:\n\nIs Airbyte updated to your latest version? You can see the latest version here. If not, please upgrade to the latest one\nIs the connector that is failing updated to the latest version? You can check the latest version available for the connectors in the yamls here. If you don't have the latest connector version, make sure you first update to the latest Airbyte version, and then go to the Admin section in the web app and put the right version in the cell for the connector. Then try again.\n\nIf the above workaround does not fix your problem, please report it here or in our Slack.\nAirbyte says successful sync, but some records are missing\nSeveral things to check:\n\nWhat is the name of the table you are looking at in the destination? Let's make sure you're not looking at a temporary table.\nIs the basic normalization toggle set to true at the connection settings? If it's false, you won't see columns but most probably a JSON file. So you need to switch it on true, and try again.\nIs Airbyte updated to your latest version? You can see the latest version here. If not, please upgrade to the latest one\nIs the connector that is failing updated to the latest version? You can check the latest version available for the connectors in the yamls here. If you don't have the latest connector version, make sure you first update to the latest Airbyte version, and then go to the Admin section in the web app and put the right version in the cell for the connector. Then try again.\n\nIf the above workaround does not fix your problem, please report it here or in our Slack.",
    "tag": "airbyte"
  },
  {
    "title": "Troubleshooting & FAQ",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/troubleshooting",
    "content": "Troubleshooting & FAQ\nThe troubleshooting section is aimed at collecting common issues users have to provide quick solutions. There are some sections you can find:\n\nOn Deploying\nOn Setting up a New Connection\nOn Running a Sync\nOn Upgrading\n\nIf you did not find a solution in the previous sections please head over to our online forum.\nOur online forum\nWe are driving our community support from our forum.\nOur User Success Engineering team is online to answer any question you may have about Airbyte and help you troubleshoot eventual issue you have with our Open Source version.\nThe community is also welcomed to participate in the opened topic!\nBefore posting on this forum please first check if a similar question was already answered.\nThe existing categories:\n* Troubleshooting: Support requests on issues encountered while implementing or using Airbyte.\n* Product and feedback ideas: Suggestions on how to improve Airbyte\u2019s product. Upvote to help with prioritization.\n* Connector development: Anything related to connector development.\n* Connector Job Board: Asks and offers for building custom connectors not yet supported by Airbyte.\n* Contributing to Airbyte: Anything related to contributing to the open-source repo.\n* Releases: Posts about new releases of Airbyte, including any migration instructions.\n* Q&A: Ask anything that doesn\u2019t belong to the other categories.",
    "tag": "airbyte"
  },
  {
    "title": "On Deploying",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/troubleshooting/on-deploying.md",
    "content": "\ndescription: Common issues and their workarounds when trying to deploy Airbyte\nOn Deploying\nStuck in onboarding, can\u2019t skip or do anything\nTo fully reset Airbyte, you also need to delete the docker volumes associated with Airbyte. This is where data is stored. Assuming that you are running Airbyte by running `docker compose up`, then what you need to do is:\n\nTurn off Airbyte completely: `docker compose down -v`\nTurn Airbyte back on: `docker compose up`\n\nthat should handle you getting reset to the beginning. I would be curious if we can see the logs associated with the failure you are seeing. I would say if after you reset you run into it again we can debug that.\nGit says file names are too long.\nIf you are cloning the repo, you might run into a problem where git indicates that certain filenames are too long and it therefore can't create the local file. So if you received this error after cloning the repo, run the following commands in git bash:\n`bash\ncd airbyte\ngit config core.longpaths true\ngit reset --hard HEAD`\nHowever it's worth pointing out that the `core.longpaths` option is defaulted to false for a reason, so use with caution. This git configuration is only changed within the cloned Airbyte repo, so you won't need to worry about changing this setting for other repositories. Find more details about this issue in this stack overflow question.\nInstead of cloning the repo, you can alternatively download the latest Airbyte release here. Unzip the downloaded file, access the unzipped file using PowerShell terminal, and run `docker compose up`. After this, you should see the Airbyte containers in the Docker application as in the image below.\n\nI have run `docker compose up` and can not access the interface\n\nIf you see a blank screen and not a loading icon:\n\nCheck your web browser version; Some old versions of web browsers doesn't support our current Front-end stack.\n\nIf you see a loading icon or the message `Cannot reach the server` persist:\n\nCheck if all Airbyte containers are running, executing: `docker ps`\n`text\nCONTAINER ID   IMAGE                            COMMAND                  CREATED        STATUS        PORTS                              NAMES\nf02fc709b130   airbyte/server:1.11.1-alpha      \"/bin/bash -c './wai\u2026\"   2 hours ago    Up 2 hours   8000/tcp, [...] :::8001->8001/tcp  airbyte-server\n153b2b322870   airbyte/webapp:1.11.1-alpha      \"/docker-entrypoint.\u2026\"   2 hours ago    Up 2 hours   :::8000->80/tcp                    airbyte-webapp\nb88d94652268   airbyte/db:1.11.1-alpha          \"docker-entrypoint.s\u2026\"   2 hours ago    Up 2 hours   5432/tcp                           airbyte-db\n0573681a10e0   airbyte/temporal-auto-setup:1.13.0  \"/entrypoint.sh /bin\u2026\"  2 hours ago  Up 2 hours   6933-6935/tcp, [...]               airbyte-temporal`\nYou must see 4 containers running. If you are not seeing execute the following steps:\n\n`docker compose down -v`\n`docker compose up`\n\nKeep in mind the commands above will delete ALL containers, volumes and data created by Airbyte.\nWe do not recommend this is you already deploy and have connection created.\nFirst, let's check the server logs by running `docker logs airbyte-server | grep ERROR`. \n If this command returns any output, please run `docker logs airbyte-server > airbyte-server.log`. \n This command will create a file in the current directory. We advise you to send a message on our #issues on Slack channel\nIf you don't have any server errors let's check the worker, `docker logs airbyte-worker | grep ERROR`. \n If this command returns any output, please run `docker logs airbyte-worker > airbyte-worker.log`. \n This command will create a file in the current directory. We advise you to send a message on our #issues on Slack channel\nIf there is no error printed in both cases, we recommend running: `docker restart airbyte-server airbyte-worker` \n Wait a few moments and try to access the interface again.\n`docker.errors.DockerException`: Error while fetching server API version\nIf you see the following error:\n`text\ndocker.errors.DockerException: Error while fetching server API\nversion: ('Connection aborted.', FileNotFoundError(2, 'No such file or\ndirectory'))`\nIt usually means that Docker isn't running on your machine (and a running Docker daemon is required to run Airbyte). An easy way to verify this is to run `docker ps`, which will show `Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?` if the Docker daemon is not running on your machine.\nThis happens (sometimes) on Windows system when you first install `docker`. You need to restart your machine.\nGetting a weird error related to setting up the Airbyte server when running Docker Compose -- wondering if this is because I played around with Airbyte in a past version?\nIf you are okay with losing your previous Airbyte configurations, you can run `docker compose down -v` and that should fix things then `docker compose up`.\n`unauthorized: incorrect username or password` when running `docker compose up`\nIf you see the following error:\n`bash\nERROR: Head \"https://registry-1.docker.io/v2/airbyte/init/manifests/{XXX}\": unauthorized: incorrect username or password`\nYou are most likely logged into Docker with your email address instead of your Docker ID.\nLog out of Docker by running `docker logout` and try running `docker compose up` again.\nProtocol Version errors from the bootloader when trying to upgrade\nWhen starting up Airbyte, the bootloader may fail with the following error:\n`Aborting bootloader to avoid breaking existing connection after an upgrade. Please address airbyte protocol version support issues in the connectors before retrying.`\nWe aborted the upgrade to avoid breaking existing connections due to a deprecation of protocol version.\nLooking at the `airbyte-bootloader` logs, there should be a few messages describing the change of support range of the Airbyte Protocol:\n`2022-11-21 22:07:20 INFO i.a.b.ProtocolVersionChecker(validate):81 - Detected an AirbyteProtocolVersion range change from [0.0.0:2.0.0] to [1.0.0:2.0.0]\n2022-11-21 22:07:20 WARN i.a.b.ProtocolVersionChecker(validate):98 - The following connectors need to be upgraded before being able to upgrade the platform\n2022-11-21 22:07:20 WARN j.u.s.ReferencePipeline$3$1(accept):197 - Source: d53f9084-fa6b-4a5a-976c-5b8392f4ad8a: E2E Testing: protocol version: 0.2.1`\nFrom this example, this upgrade will drop the support for the major version 0 of the Airbyte Protocol. One connector here is problematic, we have the `E2E Testing` source connector that is blocking the upgrade because it is still using protocol version 0.2.1.\nIn order to resolve this situation, all the problematic connectors must be upgraded to a version that is using a newer version of the Airbyte Protocol. In this specific example, we should target version 1 or 2, our recommendation is to always target the most recent version.",
    "tag": "airbyte"
  },
  {
    "title": "Deploy Airbyte",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/quickstart/deploy-airbyte.md",
    "content": "Deploy Airbyte\nDeploying Airbyte Open-Source just takes two steps.\n\nInstall Docker on your workstation (see instructions). Make sure you're on the latest version of `docker-compose`.\nRun the following commands in your terminal:\n\n`bash\ngit clone https://github.com/airbytehq/airbyte.git\ncd airbyte\n./run-ab-platform.sh`\nOnce you see an Airbyte banner, the UI is ready to go at http://localhost:8000! You will be asked for a username and password. By default, that's username `airbyte` and password `password`. Once you deploy airbyte to your servers, be sure to change these in your `.env` file.\nAlternatively, if you have an Airbyte Cloud invite, just follow these steps.\nFAQ\nIf you have any questions about the Airbyte Open-Source setup and deployment process, head over to our Getting Started FAQ on our Discourse that answers the following questions and more:\n\nHow long does it take to set up Airbyte?\nWhere can I see my data once I've run a sync?\nCan I set a start time for my sync?\n",
    "tag": "airbyte"
  },
  {
    "title": "Getting Started",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/quickstart/getting-started.md",
    "content": "Getting Started\nGoal\nDuring this getting started tutorial, we are going to replicate currencies closing price into a JSON file.\nStart Airbyte\nFirst of all, make sure you have Docker and Docker Compose installed. Then run the following commands:\n`text\ngit clone https://github.com/airbytehq/airbyte.git\ncd airbyte\ndocker compose -f docker-compose.yaml up`\nOnce you see an Airbyte banner, the UI is ready to go at http://localhost:8000/.\nSet up your preferences\nYou should see an onboarding page. Enter your email if you want updates about Airbyte and continue.\n\nSet up your first connection\nCreate a source\nThe source we are creating will pull data from an external API. It will replicate the closing price of currencies compared to USD since the specified start date.\nTo set it up, just follow the instructions on the screenshot below.\n:::info\nYou might have to wait ~30 seconds before the fields show up because it is the first time you're using Airbyte.\n:::\n\nCreate a destination\nThe destination we are creating is a simple JSON line file, meaning that it will contain one JSON object per line. Each objects will represent data extracted from the source.\nThe resulting files will be located in `/tmp/airbyte_local/json_data`\n:::caution\nPlease make sure that Docker Desktop has access to `/tmp` (and `/private` on a MacOS, as /tmp has a symlink that points to /private. It will not work otherwise). You allow it with \"File sharing\" in `Settings -> Resources -> File sharing -> add the one or two above folder` and hit the \"Apply & restart\" button.\n:::\nTo set it up, just follow the instructions on the screenshot below.\n:::info\nYou might have to wait ~30 seconds before the fields show up because it is the first time you're using Airbyte.\n:::\n\nCreate connection\nWhen we create the connection, we can select which data stream we want to replicate. We can also select if we want an incremental replication. The replication will run at the specified sync frequency.\nTo set it up, just follow the instructions on the screenshot below.\n\nCheck the logs of your first sync\nAfter you've completed the onboarding, you will be redirected to the source list and will see the source you just added. Click on it to find more information about it. You will now see all the destinations connected to that source. Click on it and you will see the sync history.\nFrom there, you can look at the logs, download them, force a sync and adjust the configuration of your connection.\n\nCheck the data of your first sync\nNow let's verify that this worked:\n`bash\ncat /tmp/airbyte_local/json_data/_airbyte_raw_exchange_rate.jsonl`\nYou should see one line for each day that was replicated.\nIf you have jq installed, let's look at the evolution of `EUR`.\n`bash\ncat /tmp/airbyte_local/test_json/_airbyte_raw_exchange_rate.jsonl | \njq -c '.data | {date: .date, EUR: .EUR }'`\nAnd there you have it. You've pulled data from an API directly into a file and all of the actual configuration for this replication only took place in the UI.\nThat's it!\nThis is just the beginning of using Airbyte. We support a large collection of sources and destinations. You can even contribute your own.\nIf you have any questions at all, please reach out to us on Slack. We\u2019re still in alpha, so if you see any rough edges or want to request a connector you need, please create an issue on our Github or leave a thumbs up on an existing issue.\nThank you and we hope you enjoy using Airbyte.",
    "tag": "airbyte"
  },
  {
    "title": "Add a Source",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/quickstart/add-a-source.md",
    "content": "Add a Source\nYou can either follow this tutorial from the onboarding or through the UI, where you can first navigate to the `Sources` tab on the left bar.\nOur demo source will pull data from an external API, which will pull down the information on one specified Pok\u00e9mon.\nTo set it up, just follow the instructions on the screenshot below.\n:::info\nYou might have to wait ~30 seconds before the fields show up because it is the first time you're using Airbyte.\n:::\n\nCan't find the connectors that you want? Try your hand at easily building one yourself using our Python CDK for HTTP API sources!",
    "tag": "airbyte"
  },
  {
    "title": "Set up a Connection",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/quickstart/set-up-a-connection.md",
    "content": "Set up a Connection\nWhen we create the connection, we can select which data stream we want to replicate. We can also select if we want an incremental replication, although it isn't currently offered for this source. The replication will run at the specified sync frequency.\nTo set it up, just follow the instructions on the screenshot below.\n\nCheck the logs of your first sync\nAfter you've completed the onboarding, you will be redirected to the source list and will see the source you just added. Click on it to find more information about it. You will now see all the destinations connected to that source. Click on it and you will see the sync history.\nFrom there, you can look at the logs, download them, force a sync and adjust the configuration of your connection.\n\nCheck the data of your first sync\nNow let's verify that this worked:\n`bash\ncat /tmp/airbyte_local/json_data/_airbyte_raw_pokemon.jsonl`\nYou should see a large JSON object with the response from the API, giving you a lot of information about the selected Pokemon.\nIf you have jq installed, let's look at some of the data that we have replicated about `charizard`. We'll pull its abilities and weight:\n`bash\ncat _airbyte_raw_pokemon.jsonl | \njq '._airbyte_data | {abilities: .abilities, weight: .weight}'`\nAnd there you have it. You've pulled data from an API directly into a file, with all of the actual configuration for this replication only taking place in the UI.\nNote: If you are using Airbyte on Windows with WSL2 and Docker, refer to this tutorial or this section in the local-json destination guide to locate the replicated folder and file.\nThat's it!\nThis is just the beginning of using Airbyte. We support a large collection of sources and destinations. You can even contribute your own.\nIf you have any questions at all, please reach out to us on Slack. We\u2019re still in alpha, so if you see any rough edges or want to request a connector you need, please create an issue on our Github or leave a thumbs up on an existing issue.",
    "tag": "airbyte"
  },
  {
    "title": "Connector Catalog",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations",
    "content": "Connector Catalog\nConnector Release Stages\nAirbyte uses a grading system for connectors to help you understand what to expect from a connector:\nGenerally Available: A generally available connector has been deemed ready for use in a production environment and is officially supported by Airbyte. Its documentation is considered sufficient to support widespread adoption.\nBeta: A beta connector is considered stable with no backwards incompatible changes but has not been validated by a broader group of users. We expect to find and fix a few issues and bugs in the release before it\u2019s ready for GA.\nAlpha: An alpha connector signifies a connector under development and helps Airbyte gather early feedback and issues reported by early adopters. We strongly discourage using alpha releases for production use cases and do not offer Cloud Support SLAs around these products, features, or connectors.\nFor more information about the grading system, see Product Release Stages\nSources\n\n\nDestinations\n<iframe\n    src=\"https://airbyte.metabaseapp.com/public/question/263d737e-533a-4cc2-a1cf-4c8c7eca87d6#titled=false&hide_download_button=true\"\n    frameborder=\"0\"\n    width=\"600\"\n    height=\"2000\"\n    allowtransparency",
    "tag": "airbyte"
  },
  {
    "title": "Custom or New Connector",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/custom-connectors.md",
    "content": "\ndescription: Missing a connector?\nCustom or New Connector\nIf you'd like to ask for a new connector, you can request it directly here.\nIf you'd like to build new connectors and make them part of the pool of pre-built connectors on Airbyte, first a big thank you. We invite you to check our contributing guide on building connectors.\nIf you'd like to build new connectors, or update existing ones, for your own usage, without contributing to the Airbyte codebase, read along.\nDeveloping your own connector\nIt's easy to code your own connectors on Airbyte. Here is a link to instruct on how to code new sources and destinations: building new connectors\nWhile the guides in the link above are specific to the languages used most frequently to write integrations, Airbyte connectors can be written in any language. Please reach out to us if you'd like help developing connectors in other languages.\nAdding your connectors in the UI\nThere are only 3 easy steps to do that:\n1.Get the `Docker` coordinate of a custom connector from `Dockerhub` (or any image repository that Airbyte can access).\n2.In the UI, go to the Admin section, and click on `[+ New connector]` on the top right\n\n3.We will ask you for the display name, the Docker repository name (repository + image name), tag and documentation URL for that connector.\n\nOnce this is filled, you will see your connector in the UI and your team will be able to use it, from the UI and Airbyte's API too.\nNote that this new connector could just be an updated version of an existing connector that you adapted to your specific edge case. Anything is possible!\nWhen using Airbyte on Kubernetes, the repository name must be a valid Kubernetes name. That is, it must consist of lower case alphanumeric characters or '-', and must start and end with an alphanumeric character (e.g. 'my-name',  or '123-abc'). Other names will work locally on Docker but cause an error on Kubernetes (Internal Server Error: Get Spec job failed).\nUpgrading a connector\nTo upgrade your connector version, go to the admin panel in the left hand side of the UI, find this connector in the list, and input the latest connector version.\n\nTo browse the available connector versions, simply click on the relevant link in the `Image` column to navigate to the connector's DockerHub page. From there, simply click on the `Tags` section in the top bar.",
    "tag": "airbyte"
  },
  {
    "title": "Missing an Integration?",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/missing-an-integration.md",
    "content": "Missing an Integration?\nIf you'd like to ask for a new connector, or build a new connectors and make them part of the pool of pre-built connectors on Airbyte, first a big thank you. We invite you to check our contributing guide.\nIf you'd like to build new connectors, or update existing ones, for your own usage, without contributing to the Airbyte codebase, read along.\nDeveloping your own connectors\nIt's easy to code your own integrations on Airbyte. Here are some links to instruct on how to code new sources and destinations.\n\nBuilding new connectors\n\nWhile the guides above are specific to the languages used most frequently to write integrations, Airbyte integrations can be written in any language. Please reach out to us if you'd like help developing integrations in other languages.",
    "tag": "airbyte"
  },
  {
    "title": "Kafka",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/kafka.md",
    "content": "Kafka\nOverview\nThe Airbyte Kafka destination allows you to sync data to Kafka. Each stream is written to the corresponding Kafka topic.\nPrerequisites\n\nFor Airbyte Open Source users using the Postgres source connector, upgrade your Airbyte platform to version `v0.40.0-alpha` or newer and upgrade your Kafka connector to version `0.1.10` or newer\n\nSync overview\nOutput schema\nEach stream will be output into a Kafka topic.\nCurrently, this connector only writes data with JSON format. More formats (e.g. Apache Avro) will be supported in the future.\nEach record will contain in its key the uuid assigned by Airbyte, and in the value these 3 fields:\n\n`_airbyte_ab_id`: a uuid assigned by Airbyte to each event that is processed.\n`_airbyte_emitted_at`: a timestamp representing when the event was pulled from the data source.\n`_airbyte_data`: a json blob representing with the event data.\n`_airbyte_stream`: the name of each record's stream.\n\nFeatures\n| Feature                       | Supported?(Yes/No) | Notes                                                                                        |\n| :---------------------------- | :------------------- | :------------------------------------------------------------------------------------------- |\n| Full Refresh Sync             | No                   |                                                                                              |\n| Incremental - Append Sync     | Yes                  |                                                                                              |\n| Incremental - Deduped History | No                   | As this connector does not support dbt, we don't support this sync mode on this destination. |\n| Namespaces                    | Yes                  |                                                                                              |\nGetting started\nRequirements\nTo use the Kafka destination, you'll need:\n\nA Kafka cluster 1.0 or above.\n\nSetup guide\nNetwork Access\nMake sure your Kafka brokers can be accessed by Airbyte.\nPermissions\nAirbyte should be allowed to write messages into topics, and these topics should be created before writing into Kafka or, at least, enable the configuration in the brokers `auto.create.topics.enable` (which is not recommended for production environments).\nNote that if you choose to use dynamic topic names, you will probably need to enable `auto.create.topics.enable` to avoid your connection failing if there was an update to the source connector's schema. Otherwise a hardcoded topic name may be best.\nTarget topics\nYou can determine the topics to which messages are written via the `topic_pattern` configuration parameter. Messages can be written to either a hardcoded, pre-defined topic, or dynamically written to different topics based on the namespace or stream they came from.\nTo write all messages to a single hardcoded topic, enter its name in the `topic_pattern` field e.g: setting `topic_pattern` to `my-topic-name` will write all messages from all streams and namespaces to that topic.\nTo define the output topics dynamically, you can leverage the `{namespace}` and `{stream}` pattern variables, which cause messages to be written to different topics based on the values present when producing the records. For example, setting the `topic_pattern` parameter to `airbyte_syncs/{namespace}/{stream}` means that messages from namespace `n1` and stream `s1` will get written to the topic `airbyte_syncs/n1/s1`, and messages from `s2` to `airbyte_syncs/n1/s2` etc.\nIf you define output topic dynamically, you might want to enable `auto.create.topics.enable` to avoid your connection failing if there was an update to the source connector's schema. Otherwise, you'll need to manually create topics in Kafka as they are added/updated in the source, which is the recommended option for production environments.\nNOTICE: a naming convention transformation will be applied to the target topic name using the `StandardNameTransformer` so that some special characters will be replaced.\nSetup the Kafka destination in Airbyte\nYou should now have all the requirements needed to configure Kafka as a destination in the UI. You can configure the following parameters on the Kafka destination (though many of these are optional or have default values):\n\nBootstrap servers\nTopic pattern\nTest topic\nSync producer\nSecurity protocol\nSASL JAAS config\nSASL mechanism\nClient ID\nACKs\nEnable idempotence\nCompression type\nBatch size\nLinger ms\nMax in flight requests per connection\nClient DNS lookup\nBuffer memory\nMax request size\nRetries\nSocket connection setup timeout\nSocket connection setup max timeout\nMax block ms\nRequest timeout\nDelivery timeout\nSend buffer bytes\nReceive buffer bytes\n\nMore info about this can be found in the Kafka producer configs documentation site.\nNOTE: Some configurations for SSL are not available yet.",
    "tag": "airbyte"
  },
  {
    "title": "SFTP JSON",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/sftp-json.md",
    "content": "SFTP JSON\nOverview\nThis destination writes data to a directory on an SFTP server.\nSync Overview\nOutput schema\nEach stream will be output into its own file.\nEach file will contain a collection of `json` objects which correspond directly with the data supplied by the source.\nFeatures\n| Feature                   | Supported |\n| :------------------------ | :-------- |\n| Full Refresh Sync         | Yes       |\n| Incremental - Append Sync | Yes       |\n| Namespaces                | No        |\nPerformance considerations\nThis integration will be constrained by the connection speed to the SFTP server and speed at which that server accepts writes.\nGetting Started\nThe `destination_path` can refer to any path that the associated account has write permissions to.\nThe `filename` should not have an extension in the configuration, as `.jsonl` will be added on by the connector.\nExample:\nIf `destination_path` is set to `/myfolder/files` and `filename` is set to `mydata`, the resulting file will be `/myfolder/files/mydata.jsonl`.\nThese files can then be accessed by creating an SFTP connection to the server and navigating to the `destination_path`.",
    "tag": "airbyte"
  },
  {
    "title": "Snowflake",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/snowflake.md",
    "content": "Snowflake\nSetting up the Snowflake destination connector involves setting up Snowflake entities (warehouse, database, schema, user, and role) in the Snowflake console, setting up the data loading method (internal stage, AWS S3, Google Cloud Storage bucket, or Azure Blob Storage), and configuring the Snowflake destination connector using the Airbyte UI.\nThis page describes the step-by-step process of setting up the Snowflake destination connector.\nPrerequisites\n\nA Snowflake account with the ACCOUNTADMIN role. If you don\u2019t have an account with the `ACCOUNTADMIN` role, contact your Snowflake administrator to set one up for you.\n(Optional) An AWS, Google Cloud Storage, or Azure account.\n\nNetwork policies\nBy default, Snowflake allows users to connect to the service from any computer or device IP address. A security administrator (i.e. users with the SECURITYADMIN role) or higher can create a network policy to allow or deny access to a single IP address or a list of addresses.\nIf you have any issues connecting with Airbyte Cloud please make sure that the list of IP addresses is on the allowed list\nTo determine whether a network policy is set on your account or for a specific user, execute the SHOW PARAMETERS command.\nAccount\n\n\n```    SHOW PARAMETERS LIKE 'network_policy' IN ACCOUNT;\n```\n\n\nUser\n\n\n```    SHOW PARAMETERS LIKE 'network_policy' IN USER <username>;\n```\n\n\nTo read more please check official Snowflake documentation\nSetup guide\nStep 1: Set up Airbyte-specific entities in Snowflake\nTo set up the Snowflake destination connector, you first need to create Airbyte-specific Snowflake entities (a warehouse, database, schema, user, and role) with the `OWNERSHIP` permission to write data into Snowflake, track costs pertaining to Airbyte, and control permissions at a granular level.\nYou can use the following script in a new Snowflake worksheet to create the entities:\n\nLog into your Snowflake account.\n\nEdit the following script to change the password to a more secure password and to change the names of other resources if you so desire.\nNote: Make sure you follow the Snowflake identifier requirements while renaming the resources.\n\n\n```-- set variables (these need to be uppercase)\nset airbyte_role = 'AIRBYTE_ROLE';\nset airbyte_username = 'AIRBYTE_USER';\nset airbyte_warehouse = 'AIRBYTE_WAREHOUSE';\nset airbyte_database = 'AIRBYTE_DATABASE';\nset airbyte_schema = 'AIRBYTE_SCHEMA';\n\n-- set user password\nset airbyte_password = 'password';\n\nbegin;\n\n-- create Airbyte role\nuse role securityadmin;\ncreate role if not exists identifier($airbyte_role);\ngrant role identifier($airbyte_role) to role SYSADMIN;\n\n-- create Airbyte user\ncreate user if not exists identifier($airbyte_username)\npassword = $airbyte_password\ndefault_role = $airbyte_role\ndefault_warehouse = $airbyte_warehouse;\n\ngrant role identifier($airbyte_role) to user identifier($airbyte_username);\n\n-- change role to sysadmin for warehouse / database steps\nuse role sysadmin;\n\n-- create Airbyte warehouse\ncreate warehouse if not exists identifier($airbyte_warehouse)\nwarehouse_size = xsmall\nwarehouse_type = standard\nauto_suspend = 60\nauto_resume = true\ninitially_suspended = true;\n\n-- create Airbyte database\ncreate database if not exists identifier($airbyte_database);\n\n-- grant Airbyte warehouse access\ngrant USAGE\non warehouse identifier($airbyte_warehouse)\nto role identifier($airbyte_role);\n\n-- grant Airbyte database access\ngrant OWNERSHIP\non database identifier($airbyte_database)\nto role identifier($airbyte_role);\n\ncommit;\n\nbegin;\n\nUSE DATABASE identifier($airbyte_database);\n\n-- create schema for Airbyte data\nCREATE SCHEMA IF NOT EXISTS identifier($airbyte_schema);\n\ncommit;\n\nbegin;\n\n-- grant Airbyte schema access\ngrant OWNERSHIP\non schema identifier($airbyte_schema)\nto role identifier($airbyte_role);\n\ncommit;\n```\n\n\n\n\nRun the script using the Worksheet page or Snowsight. Make sure to select the All Queries checkbox.\n\n\nStep 2: Set up a data loading method\nBy default, Airbyte uses Snowflake\u2019s Internal Stage to load data. You can also load data using an Amazon S3 bucket, a Google Cloud Storage bucket, or Azure Blob Storage.\nMake sure the database and schema have the `USAGE` privilege.\nUsing an Amazon S3 bucket\nTo use an Amazon S3 bucket, create a new Amazon S3 bucket with read/write access for Airbyte to stage data to Snowflake.\nUsing a Google Cloud Storage bucket\nTo use a Google Cloud Storage bucket:\n\nNavigate to the Google Cloud Console and create a new bucket with read/write access for Airbyte to stage data to Snowflake.\nGenerate a JSON key for your service account.\n\nEdit the following script to replace `AIRBYTE_ROLE` with the role you used for Airbyte's Snowflake configuration and `YOURBUCKETNAME` with your bucket name.\n    ```text\n    create storage INTEGRATION gcs_airbyte_integration\n      TYPE = EXTERNAL_STAGE\n      STORAGE_PROVIDER = GCS\n      ENABLED = TRUE\n      STORAGE_ALLOWED_LOCATIONS = ('gcs://YOURBUCKETNAME');\ncreate stage gcs_airbyte_stage\n  url = 'gcs://YOURBUCKETNAME'\n  storage_integration = gcs_airbyte_integration;\nGRANT USAGE ON integration gcs_airbyte_integration TO ROLE AIRBYTE_ROLE;\nGRANT USAGE ON stage gcs_airbyte_stage TO ROLE AIRBYTE_ROLE;\nDESC STORAGE INTEGRATION gcs_airbyte_integration;\n```\nThe final query should show a`STORAGE_GCP_SERVICE_ACCOUNT` property with an email as the property value. Add read/write permissions to your bucket with that email.\n\n\nNavigate to the Snowflake UI and run the script as a Snowflake account admin using the Worksheet page or Snowsight.\n\n\nUsing Azure Blob Storage\nTo use Azure Blob Storage, create a storage account and container, and provide a SAS Token to access the container. We recommend creating a dedicated container for Airbyte to stage data to Snowflake. Airbyte needs read/write access to interact with this container.\nStep 3: Set up Snowflake as a destination in Airbyte\nNavigate to the Airbyte UI to set up Snowflake as a destination. You can authenticate using username/password or OAuth 2.0:\nLogin and Password\n| Field | Description |\n|---|---|\n| Host | The host domain of the snowflake instance (must include the account, region, cloud environment, and end with snowflakecomputing.com). Example: `accountname.us-east-2.aws.snowflakecomputing.com` |\n| Role | The role you created in Step 1 for Airbyte to access Snowflake. Example: `AIRBYTE_ROLE` |\n| Warehouse | The warehouse you created in Step 1 for Airbyte to sync data into. Example: `AIRBYTE_WAREHOUSE` |\n| Database | The database you created in Step 1 for Airbyte to sync data into. Example: `AIRBYTE_DATABASE` |\n| Schema | The default schema used as the target schema for all statements issued from the connection that do not explicitly specify a schema name.  |\n| Username | The username you created in Step 1 to allow Airbyte to access the database. Example: `AIRBYTE_USER` |\n| Password | The password associated with the username. |\n| JDBC URL Params (Optional) | Additional properties to pass to the JDBC URL string when connecting to the database formatted as `key=value` pairs separated by the symbol `&`. Example: `key1=value1&key2=value2&key3=value3` |\nOAuth 2.0\nField | Description |\n|---|---|\n| Host | The host domain of the snowflake instance (must include the account, region, cloud environment, and end with snowflakecomputing.com). Example: `accountname.us-east-2.aws.snowflakecomputing.com` |\n| Role | The role you created in Step 1 for Airbyte to access Snowflake. Example: `AIRBYTE_ROLE` |\n| Warehouse | The warehouse you created in Step 1 for Airbyte to sync data into. Example: `AIRBYTE_WAREHOUSE` |\n| Database | The database you created in Step 1 for Airbyte to sync data into. Example: `AIRBYTE_DATABASE` |\n| Schema | The default schema used as the target schema for all statements issued from the connection that do not explicitly specify a schema name.  |\n| Username | The username you created in Step 1 to allow Airbyte to access the database. Example: `AIRBYTE_USER` |\n| OAuth2 | The Login name and password to obtain auth token. |\n| JDBC URL Params (Optional) | Additional properties to pass to the JDBC URL string when connecting to the database formatted as `key=value` pairs separated by the symbol `&`. Example: `key1=value1&key2=value2&key3=value3` |\nKey pair authentication\n\n\n```In order to configure key pair authentication you will need a private/public key pair.\nIf you do not have the key pair yet, you can generate one using openssl command line tool\nUse this command in order to generate an unencrypted private key file:\n\n   `openssl genrsa 2048 | openssl pkcs8 -topk8 -inform PEM -out rsa_key.p8 -nocrypt`\n\nAlternatively, use this command to generate an encrypted private key file:\n\n  `openssl genrsa 2048 | openssl pkcs8 -topk8 -inform PEM -v1 PBE-SHA1-RC4-128 -out rsa_key.p8`\n\nOnce you have your private key, you need to generate a matching public key.\nYou can do so with the following command:\n\n  `openssl rsa -in rsa_key.p8 -pubout -out rsa_key.pub`\n\nFinally, you need to add the public key to your Snowflake user account.\nYou can do so with the following SQL command in Snowflake:\n\n  `alter user <user_name> set rsa_public_key=<public_key_value>;`\n\nand replace <user_name> with your user name and <public_key_value> with your public key.\n```\n\n\nTo use AWS S3 as the cloud storage, enter the information for the S3 bucket you created in Step 2:\n| Field                          | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n|--------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| S3 Bucket Name                 | The name of the staging S3 bucket (Example: `airbyte.staging`). Airbyte will write files to this bucket and read them via statements on Snowflake.                                                                                                                                                                                                                                                                                                                                                                                                      |\n| S3 Bucket Region               | The S3 staging bucket region used.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n| S3 Key Id *                    | The Access Key ID granting access to the S3 staging bucket. Airbyte requires Read and Write permissions for the bucket.                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n| S3 Access Key *                | The corresponding secret to the S3 Key ID.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n| Stream Part Size (Optional)    | Increase this if syncing tables larger than 100GB. Files are streamed to S3 in parts. This determines the size of each part, in MBs. As S3 has a limit of 10,000 parts per file, part size affects the table size. This is 5MB by default, resulting in a default limit of 100GB tables. Note, a larger part size will result in larger memory requirements. A rule of thumb is to multiply the part size by 10 to get the memory requirement. Modify this with care. (e.g. 5)                                                                    |\n| Purge Staging Files and Tables | Determines whether to delete the staging files from S3 after completing the sync. Specifically, the connector will create CSV files named `bucketPath/namespace/streamName/syncDate_epochMillis_randomUuid.csv` containing three columns (`ab_id`, `data`, `emitted_at`). Normally these files are deleted after sync; if you want to keep them for other purposes, set `purge_staging_data` to false.                                                                                                                                                  |\n| Encryption                     | Whether files on S3 are encrypted. You probably don't need to enable this, but it can provide an additional layer of security if you are sharing your data storage with other applications. If you do use encryption, you must choose between ephemeral keys (Airbyte will automatically generate a new key for each sync, and nobody but Airbyte and Snowflake will be able to read the data on S3) or providing your own key (if you have the \"Purge staging files and tables\" option disabled, and you want to be able to decrypt the data yourself) |\n| S3 Filename pattern (Optional) | The pattern allows you to set the file-name format for the S3 staging file(s), next placeholders combinations are currently supported: {date}, {date:yyyy_MM}, {timestamp}, {timestamp:millis}, {timestamp:micros}, {part_number}, {sync_id}, {format_extension}. Please, don't use empty space and not supportable placeholders, as they won't recognized.                                                                                                                                                                                             |\nTo use a Google Cloud Storage bucket, enter the information for the bucket you created in Step 2:\n| Field | Description |\n|---|---|\n| GCP Project ID | The name of the GCP project ID for your credentials. (Example: `my-project`)  |\n| GCP Bucket Name | The name of the staging bucket. Airbyte will write files to this bucket and read them via statements on Snowflake. (Example: `airbyte-staging`)  |\n| Google Application Credentials | The contents of the JSON key file that has read/write permissions to the staging GCS bucket. You will separately need to grant bucket access to your Snowflake GCP service account. See the Google Cloud docs for more information on how to generate a JSON key for your service account.  |\nTo use Azure Blob storage, enter the information for the storage you created in Step 2:\n| Field | Description |\n|---|---|\n| Endpoint Domain Name | Leave default value `blob.core.windows.net` or map a custom domain to an Azure Blob Storage endpoint. |\n| Azure Blob Storage Account Name | The Azure storage account you created in Step 2. |\n| Azure blob storage container (Bucket) Name | The Azure blob storage container you created in Step 2. |\n| SAS Token | The SAS Token you provided in Step 2. |\nOutput schema\nAirbyte outputs each stream into its own table with the following columns in Snowflake:\n| Airbyte field | Description | Column type |\n|---|---|---|\n| _airbyte_ab_id | A UUID assigned to each processed event | VARCHAR |\n| _airbyte_emitted_at | A timestamp for when the event was pulled from the data source | TIMESTAMP WITH TIME ZONE |\n| _airbyte_data | A JSON blob with the event data. | VARIANT |\nNote: By default, Airbyte creates permanent tables. If you prefer transient tables, create a dedicated transient database for Airbyte. For more information, refer to Working with Temporary and Transient Tables\nSupported sync modes\nThe Snowflake destination supports the following sync modes:\n\nFull Refresh - Overwrite\nFull Refresh - Append\nIncremental Sync - Append\nIncremental Sync - Deduped History\n\nSnowflake tutorials\nNow that you have set up the Snowflake destination connector, check out the following Snowflake tutorials:\n\nBuild a data ingestion pipeline from Mailchimp to Snowflake\nReplicate data from a PostgreSQL database to Snowflake\nMigrate your data from Redshift to Snowflake\nOrchestrate ELT pipelines with Prefect, Airbyte and dbt\n\nTroubleshooting\n'Current role does not have permissions on the target schema'\nIf you receive an error stating `Current role does not have permissions on the target schema` make sure that the \nSnowflake destination `SCHEMA` is one that the role you've provided has permissions on. When creating a connection,\nit may allow you to select `Mirror source structure` for the `Destination namespace`, which if you have followed\nsome of our default examples and tutorials may result in the connection trying to write to a `PUBLIC` schema.\nA quick fix could be to edit your connection's 'Replication' settings from `Mirror source structure` to `Destination Default`.\nOtherwise, make sure to grant the role the required permissions in the desired namespace.",
    "tag": "airbyte"
  },
  {
    "title": "Local CSV",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/csv.md",
    "content": "Local CSV\n:::danger\nThis destination is meant to be used on a local workstation and won't work on Kubernetes\n:::\nOverview\nThis destination writes data to a directory on the local filesystem on the host running Airbyte. By default, data is written to `/tmp/airbyte_local`. To change this location, modify the `LOCAL_ROOT` environment variable for Airbyte.\n:::caution\nPlease make sure that Docker Desktop has access to `/tmp` (and `/private` on a MacOS, as /tmp has a symlink that points to /private. It will not work otherwise). You allow it with \"File sharing\" in `Settings -> Resources -> File sharing -> add the one or two above folder` and hit the \"Apply & restart\" button.\n:::\nSync Overview\nOutput schema\nEach stream will be output into its own file. Each file will contain 3 columns:\n\n`_airbyte_ab_id`: a uuid assigned by Airbyte to each event that is processed.\n`_airbyte_emitted_at`: a timestamp representing when the event was pulled from the data source.\n`_airbyte_data`: a json blob representing with the event data.\n\nFeatures\n| Feature | Supported |  |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental - Append Sync | Yes |  |\n| Incremental - Deduped History | No | As this connector does not support dbt, we don't support this sync mode on this destination. |\n| Namespaces | No |  |\nPerformance considerations\nThis integration will be constrained by the speed at which your filesystem accepts writes.\nGetting Started\nThe `destination_path` will always start with `/local` whether it is specified by the user or not. Any directory nesting within local will be mapped onto the local mount.\nBy default, the `LOCAL_ROOT` env variable in the `.env` file is set `/tmp/airbyte_local`.\nThe local mount is mounted by Docker onto `LOCAL_ROOT`. This means the `/local` is substituted by `/tmp/airbyte_local` by default.\nExample:\n\nIf `destination_path` is set to `/local/cars/models`\nthe local mount is using the `/tmp/airbyte_local` default\nthen all data will be written to `/tmp/airbyte_local/cars/models` directory.\n\nAccess Replicated Data Files\nIf your Airbyte instance is running on the same computer that you are navigating with, you can open your browser and enter file:///tmp/airbyte_local to look at the replicated data locally. If the first approach fails or if your Airbyte instance is running on a remote server, follow the following steps to access the replicated files:\n\nAccess the scheduler container using `docker exec -it airbyte-server bash`\nNavigate to the default local mount using `cd /tmp/airbyte_local`\nNavigate to the replicated file directory you specified when you created the destination, using `cd /{destination_path}`\nList files containing the replicated data using `ls`\nExecute `cat {filename}` to display the data in a particular file\n\nYou can also copy the output file to your host machine, the following command will copy the file to the current working directory you are using:\n`text\ndocker cp airbyte-server:/tmp/airbyte_local/{destination_path}/{filename}.csv .`\nNote: If you are running Airbyte on Windows with Docker backed by WSL2, you have to use similar step as above or refer to this link for an alternative approach.",
    "tag": "airbyte"
  },
  {
    "title": "AzureBlobStorage",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/azure-blob-storage.md",
    "content": "AzureBlobStorage\nOverview\nThis destination writes data to Azure Blob Storage.\nThe Airbyte Azure Blob Storage destination allows you to sync data to Azure Blob Storage. Each stream is written to its own blob under the container.\nPrerequisites\n\nFor Airbyte Open Source users using the Postgres source connector, upgrade your Airbyte platform to version `v0.40.0-alpha` or newer and upgrade your AzureBlobStorage connector to version `0.1.6` or newer\n\nSync Mode\n| Feature | Support | Notes |\n| :--- | :---: | :--- |\n| Full Refresh Sync | \u2705 | Warning: this mode deletes all previously synced data in the configured blob. |\n| Incremental - Append Sync | \u2705 | The append mode would only work for \"Append blobs\" blobs as per Azure limitations, more details https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction#blobs |\n| Incremental - Deduped History | \u274c | As this connector does not support dbt, we don't support this sync mode on this destination. |\nConfiguration\n| Parameter                                    | Type    | Notes                                                                                                                                                                     |\n|:---------------------------------------------|:-------:|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Endpoint Domain Name                         | string  | This is Azure Blob Storage endpoint domain name. Leave default value (or leave it empty if run container from command line) to use Microsoft native one.                |\n| Azure blob storage container (Bucket) Name | string  | A name of the Azure blob storage container. If not exists - will be created automatically. If leave empty, then will be created automatically airbytecontainer+timestamp. |\n| Azure Blob Storage account name              | string  | The account's name of the Azure Blob Storage.                                                                                                                             |\n| The Azure blob storage account key           | string  | Azure blob storage account key. Example: `abcdefghijklmnopqrstuvwxyz/0123456789+ABCDEFGHIJKLMNOPQRSTUVWXYZ/0123456789%++sampleKey==`.                                     |\n| Azure Blob Storage output buffer size        | integer | Azure Blob Storage output buffer size, in megabytes. Example: 5                                                                                                           |\n| Azure Blob Storage spill size                | integer | Azure Blob Storage spill size, in megabytes. Example: 500. After exceeding threshold connector will create new blob with incremented sequence number 'prefix_name'_seq+1  |\n| Format                                       | object  | Format specific configuration. See below for details.                                                                                                                     |\n\u26a0\ufe0f Please note that under \"Full Refresh Sync\" mode, data in the configured blob will be wiped out before each sync. We recommend you to provision a dedicated Azure Blob Storage Container resource for this sync to prevent unexpected data deletion from misconfiguration. \u26a0\ufe0f\nOutput Schema\nEach stream will be outputted to its dedicated Blob according to the configuration. The complete datastore of each stream includes all the output files under that Blob. You can think of the Blob as equivalent of a Table in the database world. \nIf stream replication exceeds configured threshold data will continue to be replicated in a new blob file for better read performance\n\nUnder Full Refresh Sync mode, old output files will be purged before new files are created.\nUnder Incremental - Append Sync mode, new output files will be added that only contain the new data.\n\nCSV\nLike most of the other Airbyte destination connectors, usually the output has three columns: a UUID, an emission timestamp, and the data blob. With the CSV output, it is possible to normalize (flatten) the data blob to multiple columns.\n| Column | Condition | Description |\n| :--- | :--- | :--- |\n| `_airbyte_ab_id` | Always exists | A uuid assigned by Airbyte to each processed record. |\n| `_airbyte_emitted_at` | Always exists. | A timestamp representing when the event was pulled from the data source. |\n| `_airbyte_data` | When no normalization (flattening) is needed, all data reside under this column as a json blob. |  |\n| root level fields | When root level normalization (flattening) is selected, the root level fields are expanded. |  |\nFor example, given the following json object from a source:\n`javascript\n{\n  \"user_id\": 123,\n  \"name\": {\n    \"first\": \"John\",\n    \"last\": \"Doe\"\n  }\n}`\nWith no normalization, the output CSV is:\n| `_airbyte_ab_id` | `_airbyte_emitted_at` | `_airbyte_data` |\n| :--- | :--- | :--- |\n| `26d73cde-7eb1-4e1e-b7db-a4c03b4cf206` | 1622135805000 | `{ \"user_id\": 123, name: { \"first\": \"John\", \"last\": \"Doe\" } }` |\nWith root level normalization, the output CSV is:\n| `_airbyte_ab_id` | `_airbyte_emitted_at` | `user_id` | `name` |\n| :--- | :--- | :--- | :--- |\n| `26d73cde-7eb1-4e1e-b7db-a4c03b4cf206` | 1622135805000 | 123 | `{ \"first\": \"John\", \"last\": \"Doe\" }` |\nJSON Lines (JSONL)\nJson Lines is a text format with one JSON per line. Each line has a structure as follows:\n`javascript\n{\n  \"_airbyte_ab_id\": \"<uuid>\",\n  \"_airbyte_emitted_at\": \"<timestamp-in-millis>\",\n  \"_airbyte_data\": \"<json-data-from-source>\"\n}`\nFor example, given the following two json objects from a source:\n`javascript\n[\n  {\n    \"user_id\": 123,\n    \"name\": {\n      \"first\": \"John\",\n      \"last\": \"Doe\"\n    }\n  },\n  {\n    \"user_id\": 456,\n    \"name\": {\n      \"first\": \"Jane\",\n      \"last\": \"Roe\"\n    }\n  }\n]`\nThey will be like this in the output file:\n`text\n{ \"_airbyte_ab_id\": \"26d73cde-7eb1-4e1e-b7db-a4c03b4cf206\", \"_airbyte_emitted_at\": \"1622135805000\", \"_airbyte_data\": { \"user_id\": 123, \"name\": { \"first\": \"John\", \"last\": \"Doe\" } } }\n{ \"_airbyte_ab_id\": \"0a61de1b-9cdd-4455-a739-93572c9a5f20\", \"_airbyte_emitted_at\": \"1631948170000\", \"_airbyte_data\": { \"user_id\": 456, \"name\": { \"first\": \"Jane\", \"last\": \"Roe\" } } }`\nGetting started\nRequirements\n\nCreate an AzureBlobStorage account.\nCheck if it works under https://portal.azure.com/ -> \"Storage explorer (preview)\".\n\nSetup guide\n\nFill up AzureBlobStorage info\nEndpoint Domain Name\nLeave default value (or leave it empty if run container from command line) to use Microsoft native one or use your own.\n\n\nAzure blob storage container\nIf not exists - will be created automatically. If leave empty, then will be created automatically airbytecontainer+timestamp..\n\n\nAzure Blob Storage account name\nSee this on how to create an account.\n\n\nThe Azure blob storage account key\nCorresponding key to the above user.\n\n\nFormat\nData format that will be use for a migrated data representation in blob.\n\n\nMake sure your user has access to Azure from the machine running Airbyte.\nThis depends on your networking setup.\nThe easiest way to verify if Airbyte is able to connect to your Azure blob storage container is via the check connection tool in the UI.\n",
    "tag": "airbyte"
  },
  {
    "title": "Iceberg",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/iceberg.md",
    "content": "Iceberg\nThis page guides you through the process of setting up the Iceberg destination connector.\nSync overview\nOutput schema\nThe incoming airbyte data is structured in keyspaces and tables and is partitioned and replicated across different nodes\nin the cluster. This connector maps an incoming `stream` to an Iceberg `table` and a `namespace` to an\nIceberg `database`. Fields in the airbyte message become different columns in the Iceberg tables. Each table will\ncontain the following columns.\n\n`_airbyte_ab_id`: A random generated uuid.\n`_airbyte_emitted_at`: a timestamp representing when the event was received from the data source.\n`_airbyte_data`: a json text representing the extracted data.\n\nFeatures\nThis section should contain a table with the following format:\n| Feature                       | Supported?(Yes/No) | Notes |\n| :---------------------------- | :----------------- | :---- |\n| Full Refresh Sync             | \u2705                 |       |\n| Incremental Sync              | \u2705                 |       |\n| Replicate Incremental Deletes | \u274c                 |       |\n| SSH Tunnel Support            | \u274c                 |       |\nPerformance considerations\nEvery ten thousand pieces of incoming airbyte data in a stream \u2014\u2014\u2014\u2014we call it a batch, would produce one data file(\nParquet/Avro) in an Iceberg table. This batch size can be configurabled by `Data file flushing batch size`\nproperty.\nAs the quantity of Iceberg data files grows, it causes an unnecessary amount of metadata and less efficient queries from\nfile open costs.\nIceberg provides data file compaction action to improve this case, you can read more about\ncompaction HERE.\nThis connector also provides auto compact action when stream closes, by `Auto compact data files` property. Any you can\nspecify the target size of compacted Iceberg data file.\nGetting started\nRequirements\n\nIceberg catalog : Iceberg uses `catalog` to manage tables. this connector already supports:\nHiveCatalog connects to a Hive metastore\n    to keep track of Iceberg tables.\nHadoopCatalog doesn\u2019t need\n    to connect to a Hive MetaStore, but can only be used with HDFS or similar file systems that support atomic\n    rename. For `HadoopCatalog`, this connector use Storage Config (S3 or HDFS) to manage Iceberg tables.\nJdbcCatalog uses a table in a relational database to manage\n    Iceberg tables through JDBC. So far, this connector supports PostgreSQL only.\nStorage medium means where Iceberg data files storages in. So far, this connector supports S3/S3N/S3N\n  object-storage only.\n",
    "tag": "airbyte"
  },
  {
    "title": "Weaviate",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/weaviate.md",
    "content": "Weaviate\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | No |  |\n| Incremental - Append Sync | Yes |  |\n| Incremental - Deduped History | No |  |\n| Namespaces | No |  |\n| Provide vector | Yes |  |\nOutput Schema\nEach stream will be output into its own class in Weaviate. The record fields will be stored as fields\nin the Weaviate class.\nUploading Vectors: Use the vectors configuration if you want to upload\nvectors from a source database into Weaviate. You can do this by specifying\nthe stream name and vector field name in the following format:\n`<stream_name>.<vector_field_name>, <stream_name2>.<vector_field_name>`\nFor example, if you have a table named `my_table` and the vector is stored using the column `vector` then\nyou should use the following `vectors`configuration: `my_table.vector`.\nDynamic Schema: Weaviate will automatically create a schema for the stream if no class was defined unless\nyou have disabled the Dynamic Schema feature in Weaviate. You can also create the class in Weaviate in advance\nif you need more control over the schema in Weaviate. \nIDs: If your source table has an int based id stored as field name `id` then the\nID will automatically be converted to a UUID. Weaviate only supports the ID to be a UUID.\nFor example, if the record has `id=1` then this would become a uuid of\n`00000000-0000-0000-0000-000000000001`.\nAny field name starting with an upper case letter will be converted to lower case. For example,\nif you have a field name `USD` then that field will become `uSD`. This is due to a limitation\nin Weaviate, see this issue in Weaviate.\nGetting Started\nAirbyte Cloud only supports connecting to your Weaviate Instance instance with TLS encryption and with a username and\npassword.\nGetting Started (Airbyte Open-Source)\nRequirements\nTo use the Weaviate destination, you'll need:\n\nA Weaviate cluster version 21.8.10.19 or above\n\nConfigure Network Access\nMake sure your Weaviate database can be accessed by Airbyte. If your database is within a VPC, you may need to allow access from the IP you're using to expose Airbyte.\nPermissions\nYou need a Weaviate user or use a Weaviate instance that's accessible to all\nSetup the Weaviate Destination in Airbyte\nYou should now have all the requirements needed to configure Weaviate as a destination in the UI. You'll need the following information to configure the Weaviate destination:\n\nURL for example http://localhost:8080 or https://my-wcs.semi.network\nUsername (Optional)\nPassword (Optional)\nBatch Size (Optional, defaults to 100)\nVectors a comma separated list of `<stream_name.vector_field_name>` to specify the field\nID Schema a comma separated list of `<stream_name.id_field_name>` to specify the field\n  name that contains the ID of a record\n",
    "tag": "airbyte"
  },
  {
    "title": "RabbitMQ",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/rabbitmq.md",
    "content": "RabbitMQ\nOverview\nThe RabbitMQ destination allows you to send/stream data to a RabbitMQ routing key. RabbitMQ is one of the most popular open source message brokers.\nSync overview\nOutput schema\nEach stream will be output a RabbitMQ message with properties. The message properties will be\n\n`content_type`: set as `application/json`\n`headers`: message headers, which include:\n`stream`: the name of stream where the data is coming from\n`namespace`: namespace if available from the stream\n`emitted_at`: timestamp the `AirbyteRecord` was emitted at.\n\nThe `AirbyteRecord` data will be serialized as JSON and set as the RabbitMQ message body.\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental - Append Sync | Yes |  |\n| Incremental - Deduped History | No | As this connector does not support dbt, we don't support this sync mode on this destination. |\n| Namespaces | Yes |  |\nGetting started\nRequirements\nTo use the RabbitMQ destination, you'll need:\n\nA RabbitMQ host and credentials (username/password) to publish messages, if required.\nA RabbitMQ routing key.\nRabbitMQ exchange is optional. If specified, a binding between exchange and routing key is required.\nRabbitMQ port is optional (it defaults to 5672).\nRabbitMQ virtual host is also optional.\n",
    "tag": "airbyte"
  },
  {
    "title": "Yugabytedb",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/yugabytedb.md",
    "content": "Yugabytedb\nTODO: update this doc\nSync overview\nOutput schema\nIs the output schema fixed (e.g: for an API like Stripe)? If so, point to the connector's schema (e.g: link to Stripe\u2019s documentation) or describe the schema here directly (e.g: include a diagram or paragraphs describing the schema).\nDescribe how the connector's schema is mapped to Airbyte concepts. An example description might be: \"MagicDB tables become Airbyte Streams and MagicDB columns become Airbyte Fields. In addition, an extracted_at column is appended to each row being read.\"\nData type mapping\nThis section should contain a table mapping each of the connector's data types to Airbyte types. At the moment, Airbyte uses the same types used by JSONSchema. `string`, `date-time`, `object`, `array`, `boolean`, `integer`, and `number` are the most commonly used data types.\n| Integration Type | Airbyte Type | Notes |\n| :--- | :--- | :--- |\nFeatures\nThis section should contain a table with the following format:\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync |  |  |\n| Incremental Sync |  |  |\n| Replicate Incremental Deletes |  |  |\n| For databases, WAL/Logical replication |  |  |\n| SSL connection |  |  |\n| SSH Tunnel Support |  |  |\n| (Any other source-specific features) |  |  |\nPerformance considerations\nCould this connector hurt the user's database/API/etc... or put too much strain on it in certain circumstances? For example, if there are a lot of tables or rows in a table? What is the breaking point (e.g: 100mm> records)? What can the user do to prevent this? (e.g: use a read-only replica, or schedule frequent syncs, etc..)\nGetting started\nRequirements\n\nWhat versions of this connector does this implementation support? (e.g: `postgres v3.14 and above`)\nWhat configurations, if any, are required on the connector? (e.g: `buffer_size > 1024`)\nNetwork accessibility requirements\nCredentials/authentication requirements? (e.g: A  DB user with read permissions on certain tables)\n\nSetup guide\nFor each of the above high-level requirements as appropriate, add or point to a follow-along guide. See existing source or destination guides for an example.\nFor each major cloud provider we support, also add a follow-along guide for setting up Airbyte to connect to that destination. See the Postgres destination guide for an example of what this should look like.",
    "tag": "airbyte"
  },
  {
    "title": "Chargify",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/chargify.md",
    "content": "\ndescription: >-\n  Chargify is a SaaS billing and subscription management platform which specializes in complex billing, payment collections, and business analytics.\n\nChargify\nOverview\nThe Airbyte Chargify destination allows you to stream data from any Airbyte Source into Chargify for custom analysis and multi-attribute, usage-based billing. Chargify is the leading billing and subscription management software built for the evolving needs of fast-growth companies.\nSync overview\nOutput schema\nEach replicated stream from Airbyte will output data into a corresponding event collection in Chargify. Event collections store data in JSON format. Each collection will inherit the name from the stream with all non-alphanumeric characters removed, except for `.\u2019, \u2018-\u2019, \u2018_\u2019,` and whitespace characters. When possible, the connector will try to infer the timestamp value for the record and override the special field `chargify.timestamp` with it.\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental - Append Sync | Yes |  |\n| Incremental - Deduped History | No | As this connector does not support dbt, we don't support this sync mode on this destination. |\n| Namespaces | No |  |\nGetting started\nRequirements\nTo use the Chargify destination, you'll first need to create a Chargify account (if you don\u2019t already have one).\nOnce you have a Chargify account, you can use the following credentials to set up the connector\n\nA Project ID associated with the site\nA Master API key associated with the site\n\nYou can reach out to support@chargify.com to request your Project ID and Master API key for the Airbyte destination connector.\nSee the setup guide for more information about how to get started.\nSetup guide\nChargify\nIf Business Intelligence and Events-Based Billing have not been enabled, please contact support@chargify.com.\nLogin to the Chargify application and identify which sites you want to stream data to for Events-Based Billing and Chargify Business Intelligence.\nChargify sites are simply containers for your Products, Customers, and Subscriptions. You can use Chargify with just one Site, although most customers will want two sites at a minimum \u2013 one for testing and one for production.\nReach out to support@chargify.com to obtain your Site Project ID and Site Master API key. Note: You will need keys for each site you plan to stream data to.\nAPI Key and Project ID\nThe Chargify Connector requires your `Project ID` and `Master Key` for authentication. To get them, please reach out to support@chargify.com.\nTimestamp Inference\nThe `Infer Timestamp` field lets you specify if you want the connector to infer the chargify.timestamp field based on the data from the event that occurred in the source application. This feature allows for historical data synchronization enabling you to fully leverage the power of Chargify's time series analytics. By default, this property is set to true. If toggled off, chargify.timestamp will be set to the datetime when the data was recorded by Chargify.\nSetup the Chargify destination in Airbyte\nNow, you should have all the parameters needed to configure Chargify destination.\n\nProject ID\nMaster API Key\nInfer Timestamp\n\nConnect your first source and then head to the Chargify application. You can seamlessly run custom analysis on your data and build multi-attribute, usage-based pricing models.\nIf you have any questions or want to get started, please reach out to a billing expert.",
    "tag": "airbyte"
  },
  {
    "title": "PubSub",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/pubsub.md",
    "content": "\ndescription: >-\n  Pub/Sub is an asynchronous messaging service provided by Google Cloud\n  Provider.\n\nPubSub\nOverview\nThe Airbyte Google PubSub destination allows you to send/stream data into PubSub. Pub/Sub is an asynchronous messaging service provided by Google Cloud Provider.\nPrerequisites\n\nFor Airbyte Open Source users using the Postgres source connector, upgrade your Airbyte platform to version `v0.40.0-alpha` or newer and upgrade your PubSub connector to version `0.1.6` or newer\n\nSync overview\nOutput schema\nEach stream will be output a PubSubMessage with attributes. The message attributes will be\n\n`_stream`: the name of stream where the data is coming from\n`_namespace`: namespace if available from the stream\n\nThe data will be a serialized JSON, containing the following fields\n\n`_airbyte_ab_id`: a uuid string assigned by Airbyte to each event that is processed.\n`_airbyte_emitted_at`: a long timestamp(ms) representing when the event was pulled from the data source.\n`_airbyte_data`: a json string representing source data.\n\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental - Append Sync | Yes |  |\n| Incremental - Deduped History | No | As this connector does not support dbt, we don't support this sync mode on this destination. |\n| Namespaces | Yes |  |\nGetting started\nRequirements\nTo use the PubSub destination, you'll need:\n\nA Google Cloud Project with PubSub enabled\nA PubSub Topic to which Airbyte can stream/sync your data\nA Google Cloud Service Account with the `Pub/Sub Editor` role in your GCP project\nA Service Account Key to authenticate into your Service Account\n\nSee the setup guide for more information about how to create the required resources.\nSetup guide\nGoogle cloud project\nIf you have a Google Cloud Project with PubSub enabled, skip to the \"Create a Topic\" section.\nFirst, follow along the Google Cloud instructions to Create a Project. PubSub is enabled automatically in new projects. If this is not the case for your project, find it in Marketplace and enable.\nPubSub topic for Airbyte syncs\nAirbyte needs a topic in PubSub to write the data being streamed/synced from your data sources. If you already have a Topic into which Airbyte should stream/sync data, skip this section. Otherwise, follow the Google Cloud guide for Creating a PubSub Topic to achieve this.\nService account\nIn order for Airbyte to stream/sync data into PubSub, it needs credentials for a Service Account with the `Pub/Sub Editor` role, which grants permissions to publish messages into PubSub topics. We highly recommend that this Service Account is exclusive to Airbyte for ease of permissioning and auditing. However, you can use a pre-existing Service Account if you already have one with the correct permissions.\nThe easiest way to create a Service Account is to follow GCP's guide for Creating a Service Account. Once you've created the Service Account, make sure to keep its ID handy as you will need to reference it when granting roles. Service Account IDs typically take the form `<account-name>@<project-name>.iam.gserviceaccount.com`\nThen, add the service account as a Member in your Google Cloud Project with the `Pub/Sub Editor` role. To do this, follow the instructions for Granting Access in the Google documentation. The email address of the member you are adding is the same as the Service Account ID you just created.\nAt this point you should have a service account with the `Pub/Sub Editor` project-level permission.\nService account key\nService Account Keys are used to authenticate as Google Service Accounts. For Airbyte to leverage the permissions you granted to the Service Account in the previous step, you'll need to provide its Service Account Keys. See the Google documentation for more information about Keys.\nFollow the Creating and Managing Service Account Keys guide to create a key. Airbyte currently supports JSON Keys only, so make sure you create your key in that format. As soon as you created the key, make sure to download it, as that is the only time Google will allow you to see its contents. Once you've successfully configured BigQuery as a destination in Airbyte, delete this key from your computer.\nSetup the PubSub destination in Airbyte\nYou should now have all the requirements needed to configure BigQuery as a destination in the UI. You'll need the following information to configure the BigQuery destination:\n\nProject ID: GCP project id\nTopic ID: name of pubsub topic under the project\nService Account Key: the contents of your Service Account Key JSON file\n\nOnce you've configured PubSub as a destination, delete the Service Account Key from your computer.",
    "tag": "airbyte"
  },
  {
    "title": "Postgres",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/postgres.md",
    "content": "Postgres\nThis page guides you through the process of setting up the Postgres destination connector.\nPrerequisites\nTo use the Postgres destination, you'll need:\n\nA Postgres server version 9.5 or above\n\nAirbyte Cloud only supports connecting to your Postgres instances with SSL or TLS encryption. TLS is\nused by default. Other than that, you can proceed with the open-source instructions below.\nYou'll need the following information to configure the Postgres destination:\n\nHost - The host name of the server.\nPort - The port number the server is listening on. Defaults to the PostgreSQL\u2122 standard port number (5432).\nUsername\nPassword\nDefault Schema Name - Specify the schema (or several schemas separated by commas) to be set in the search-path. These schemas will be used to resolve unqualified object names used in statements executed over this connection.\nDatabase - The database name. The default is to connect to a database with the same name as the user name.\nJDBC URL Params (optional)\n\nRefer to this guide for more details\nConfigure Network Access\nMake sure your Postgres database can be accessed by Airbyte. If your database is within a VPC, you\nmay need to allow access from the IP you're using to expose Airbyte.\nStep 1: Set up Postgres\nPermissions\nYou need a Postgres user with the following permissions:\n\ncan create tables and write rows.\ncan create schemas e.g:\n\nYou can create such a user by running:\n`CREATE USER airbyte_user WITH PASSWORD '<password>';\nGRANT CREATE, TEMPORARY ON DATABASE <database> TO airbyte_user;`\nYou can also use a pre-existing user but we highly recommend creating a dedicated user for Airbyte.\nStep 2: Set up the Postgres connector in Airbyte\nTarget Database\nYou will need to choose an existing database or create a new database that will be used to store\nsynced data from Airbyte.\nNaming Conventions\nFrom Postgres SQL Identifiers syntax:\n\nSQL identifiers and key words must begin with a letter (a-z, but also letters with diacritical\n  marks and non-Latin letters) or an underscore (_).\nSubsequent characters in an identifier or key word can be letters, underscores, digits (0-9), or\n  dollar signs ($).\n\nNote that dollar signs are not allowed in identifiers according to the SQL standard,\n  so their use might render applications less portable. The SQL standard will not define a key word\n  that contains digits or starts or ends with an underscore, so identifiers of this form are safe\n  against possible conflict with future extensions of the standard.\n\nThe system uses no more than NAMEDATALEN-1 bytes of an identifier; longer names can be written in\n  commands, but they will be truncated. By default, NAMEDATALEN is 64 so the maximum identifier\n  length is 63 bytes\nQuoted identifiers can contain any character, except the character with code zero. (To include a\n  double quote, write two double quotes.) This allows constructing table or column names that would\n  otherwise not be possible, such as ones containing spaces or ampersands. The length limitation\n  still applies.\nQuoting an identifier also makes it case-sensitive, whereas unquoted names are always folded to\n  lower case.\nIn order to make your applications portable and less error-prone, use consistent quoting with each name (either always quote it or never quote it).\n\nNote, that Airbyte Postgres destination will create tables and schemas using the Unquoted\nidentifiers when possible or fallback to Quoted Identifiers if the names are containing special\ncharacters.\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Destinations. In the top-right corner, click new destination.\nOn the Set up the destination page, enter the name for the Postgres connector\n   and select Postgres from the Destination type dropdown.\nEnter a name for your source.\nFor the Host, Port, and DB Name, enter the hostname, port number, and name for your Postgres database.\nList the Default Schemas.\n    :::note\n    The schema names are case sensitive. The 'public' schema is set by default. Multiple schemas may be used at one time. No schemas set explicitly - will sync all of existing.\n    :::\nFor User and Password, enter the username and password you created in Step 1.\nFor Airbyte Open Source, toggle the switch to connect using SSL. For Airbyte Cloud uses SSL by default.\nFor SSL Modes, select:\ndisable to disable encrypted communication between Airbyte and the source\nallow to enable encrypted communication only when required by the source\nprefer to allow unencrypted communication only when the source doesn't support encryption\nrequire to always require encryption. Note: The connection will fail if the source doesn't support encryption.\nverify-ca to always require encryption and verify that the source has a valid SSL certificate\nverify-full to always require encryption and verify the identity of the source\n\n\n\nTo customize the JDBC connection beyond common options, specify additional supported JDBC URL parameters as key-value pairs separated by the symbol & in the JDBC URL Parameters (Advanced) field.\nExample: key1=value1&key2=value2&key3=value3\nThese parameters will be added at the end of the JDBC URL that the AirByte will use to connect to your Postgres database.\nThe connector now supports `connectTimeout` and defaults to 60 seconds. Setting connectTimeout to 0 seconds will set the timeout to the longest time available.\nNote: Do not use the following keys in JDBC URL Params field as they will be overwritten by Airbyte:\n`currentSchema`, `user`, `password`, `ssl`, and `sslmode`.\n:::warning\nThis is an advanced configuration option. Users are advised to use it with caution.\n:::\n11. For SSH Tunnel Method, select:\n- No Tunnel for a direct connection to the database\n- SSH Key Authentication to use an RSA Private as your secret for establishing the SSH tunnel\n- Password Authentication to use a password as your secret for establishing the SSH tunnel\n:::warning\nSince Airbyte Cloud requires encrypted communication, select SSH Key Authentication or Password Authentication if you selected disable, allow, or prefer as the SSL Mode; otherwise, the connection will fail.\n:::\n12. Click Set up destination.\n\n\nSupported sync modes\nThe Postgres destination connector supports the\nfollowing sync modes:\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental - Append Sync | Yes |  |\n| Incremental - Deduped History | Yes |  |\n| Namespaces | Yes |  |\nSchema map\nOutput Schema\nEach stream will be mapped to a separate table in Postgres. Each table will contain 3 columns:\n\n`_airbyte_ab_id`: a uuid assigned by Airbyte to each event that is processed. The column type in\n  Postgres is `VARCHAR`.\n`_airbyte_emitted_at`: a timestamp representing when the event was pulled from the data source.\n  The column type in Postgres is `TIMESTAMP WITH TIME ZONE`.\n`_airbyte_data`: a json blob representing with the event data. The column type in Postgres\n  is `JSONB`.\n\nTutorials\nNow that you have set up the Postgres destination connector, check out the following tutorials:\n\nMigrate from mysql to postgres\nPostgres replication\n",
    "tag": "airbyte"
  },
  {
    "title": "Doris",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/doris.md",
    "content": "Doris\ndestination-doris is a destination implemented based on Apache Doris stream load, supports batch rollback, and uses http/https put request\nSync overview\nOutput schema\nEach stream will be output into its own table in Doris. Each table will contain 3 columns:\n\n`_airbyte_ab_id`: an uuid assigned by Airbyte to each event that is processed. The column type in Doris is `VARCHAR(40)`.\n`_airbyte_emitted_at`: a timestamp representing when the event was pulled from the data source. The column type in Doris is `BIGINT`.\n`_airbyte_data`: a json blob representing with the event data. The column type in Doris is `String`.\n\nFeatures\nThis section should contain a table with the following format:\n| Feature                                | Supported?(Yes/No) | Notes                    |\n| :------------------------------------- | :----------------- | :----------------------- |\n| Full Refresh Sync                      | Yes                |                          |\n| Incremental - Append Sync              | Yes                |                          |\n| Incremental - Deduped History          | No                 | it will soon be realized |\n| For databases, WAL/Logical replication | Yes                |                          |\nPerformance considerations\nBatch writes are performed. mini records may impact performance.\nImporting multiple tables will generate multiple Doris stream load transactions, which should be split as much as possible.\nGetting started\nRequirements\nTo use the Doris destination, you'll need:\n\nA Doris server version 0.14 or above\nMake sure your Doris fe http port can be accessed by Airbyte.\nMake sure your Doris database host can be accessed by Airbyte.\nMake sure your Doris user with read/write permissions on certain tables.\n\nTarget Database and tables\nYou will need to choose a database that will be used to store synced data from Airbyte.\nYou need to prepare tables that will be used to store synced data from Airbyte, and ensure the order and matching of the column names in the table as much as possible.\nSetup the access parameters\n\nHost\nHttpPort\nQueryPort\nUsername\nPassword\nDatabase\n",
    "tag": "airbyte"
  },
  {
    "title": "Keen",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/keen.md",
    "content": "\ndescription: >-\n  Keen is a fully managed event streaming and analytics platform.\n\nKeen\nOverview\nThe Airbyte Keen destination allows you to stream data from any Airbyte Source into Keen for storage, analysis, and visualization. Keen is a flexible, fully managed event streaming and analytics platform that empowers anyone to ship custom, embeddable dashboards in minutes, not months.\nPrerequisites\n\nFor Airbyte Open Source users using the Postgres source connector, upgrade your Airbyte platform to version `v0.40.0-alpha` or newer and upgrade your Keen connector to version `0.2.4` or newer\n\nSync overview\nOutput schema\nEach replicated stream from Airbyte will output data into a corresponding event collection in Keen. Event collections store data in JSON format. Each collection will inherit the name from the stream with all non-alphanumeric characters removed, except for `.\u2019, \u2018-\u2019, \u2018_\u2019,` and whitespace characters. When possible, the connector will try to infer the timestamp value for the record and override the special field `keen.timestamp` with it.\nFeatures\n| Feature                       | Supported?(Yes/No) | Notes                                                                                        |\n| :---------------------------- | :------------------- | :------------------------------------------------------------------------------------------- |\n| Full Refresh Sync             | Yes                  |                                                                                              |\n| Incremental - Append Sync     | Yes                  |                                                                                              |\n| Incremental - Deduped History | No                   | As this connector does not support dbt, we don't support this sync mode on this destination. |\n| Namespaces                    | No                   |                                                                                              |\nGetting started\nRequirements\nTo use the Keen destination, you'll first need to create a Keen account (if you don\u2019t already have one).\nOnce you have a Keen account, you can use the following credentials to set up the connector\n\nA Keen Project ID\nA Keen Master API key associated with the project\n\nSee the setup guide for more information about how to get started.\nSetup guide\nKeen Project\nIf you haven\u2019t set up a project to stream your data to:\nLogin to the Keen application and add a new project. To do this, click the \u2018Add New\u2019 link next to the Projects label on the left-hand, side ribbon. Then, give the project a name.\nYou can think of a project as a data silo. The data in a project is completely separate from data in other projects. We recommend that you create separate projects for each of your applications and separate projects for Dev and Prod environments.\nNow, head to the \u2018Access\u2019 section and grab your Project ID and Master API Key.\nIf you already have a project set up:\nHead to the \u2018Access\u2019 tab and grab your Project ID and Master API Key\nAPI Key and Project ID\nThe Keen Connector uses the Keen Kafka Inbound Cluster to stream data. It requires your `Project ID` and `Master Key` for authentication. To get them, navigate to the `Access` tab from the left-hand, side panel and check the `Project Details` section.\nImportant: This destination requires the Project's Master Key.\nTimestamp Inference\nThe `Infer Timestamp` field lets you specify if you want the connector to infer the keen.timestamp field based on the data from the event that occurred in the source application. This feature allows for historical data synchronization enabling you to fully leverage the power of Keen's time series analytics. By default, this property is set to `true`. If toggled off, `keen.timestamp` will be set to the datetime when the data was recorded by Keen.\nSetup the Keen destination in Airbyte\nNow, you should have all the parameters needed to configure Keen destination.\n\nProject ID\nMaster API Key\nInfer Timestamp\n\nConnect your first source and then head to the Keen application. You can seamlessly run custom analysis on your data and build interactive dashboards for key stakeholders.\nIf you have any questions, please reach out to us at team@keen.io and we\u2019ll be happy to help!",
    "tag": "airbyte"
  },
  {
    "title": "Scylla",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/scylla.md",
    "content": "Scylla\nPrerequisites\n\nFor Airbyte Open Source users using the Postgres source connector, upgrade your Airbyte platform to version `v0.40.0-alpha` or newer and upgrade your Scylla connector to version `0.1.3` or newer\n\nSync overview\nOutput schema\nThe incoming airbyte data is structured in keyspaces and tables and is partitioned and replicated across different nodes\nin the cluster. This connector maps an incoming `stream` to a Scylla `table` and a `namespace` to a Scylla`keyspace`.\nFields in the airbyte message become different columns in the Scylla tables. Each table will contain the following\ncolumns.\n\n`_airbyte_ab_id`: A random uuid generated to be used as a partition key.\n`_airbyte_emitted_at`: a timestamp representing when the event was received from the data source.\n`_airbyte_data`: a json text representing the extracted data.\n\nFeatures\n| Feature | Support | Notes |\n| :--- | :---: | :--- |\n| Full Refresh Sync | \u2705 | Warning: this mode deletes all previously synced data in the configured DynamoDB table. |\n| Incremental - Append Sync | \u2705 |  |\n| Incremental - Deduped History | \u274c | As this connector does not support dbt, we don't support this sync mode on this destination. |\n| Namespaces | \u2705 | Namespace will be used as part of the table name. |\nPerformance considerations\nScylla is highly performant and is designed to handle large amounts of data by using different nodes in the cluster in\norder to perform write operations. As long as you have enough nodes in your cluster the database can scale infinitely\nand handle any amount of data from the connector.\nGetting started\nRequirements\n\nDriver compatibility: NA\nConfiguration\nKeyspace [default keyspace to use when writing data]\nUsername [authentication username]\nPassword [authentication password]\nAddress [cluster address]\nPort [default: 9042]\nReplication [optional] [default: 1]\n\n\n\nSetup guide",
    "tag": "airbyte"
  },
  {
    "title": "Redis",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/redis.md",
    "content": "Redis\nSync overview\nOutput schema\nThe incoming Airbyte data is structured depending on the target Redis cache/data type.\nThis connector maps an incoming data from a namespace and stream to a unique Redis key.\nFor the hash implementation as a Redis data type the keys and the hashes are structured in the following way:\nkey:\n\n\n```namespace:stream:id\n```\n\n\nhash:\n\n`_airbyte_ab_id`: Sequential id for a given key generated by using the INCR Redis command.\n`_airbyte_emitted_at`: a timestamp representing when the event was received from the data source.\n`_airbyte_data`: a json text/object representing the data that was received from the data source.\n\nFeatures\n| Feature                       | Support| Notes                                                                          |\n|:------------------------------| :-----:|:-------------------------------------------------------------------------------|\n| Full Refresh Sync             | \u2705     | Existing keys in the Redis cache are deleted and replaced with the new keys.   |\n| Incremental - Append Sync     | \u2705     | New keys are inserted in the same keyspace without touching the existing keys. |\n| Incremental - Deduped History | \u274c     |                                                                                |\n| Namespaces                    | \u2705     | Namespaces will be used to determine the correct Redis key.                    |\n| SSH Tunnel Connection         | \u2705     |                                                                                |\n| SSL connection                | \u2705     |                                                                                |\nPerformance considerations\nAs long as you have the necessary memory capacity for your cache, Redis should be able to handle even millions of records without any issues since the data is stored in-memory with the option to \nsave snapshots periodically on disk.\nGetting started\nRequirements\n\nThe connector is fully compatible with redis 2.8.x, 3.x.x and above\nConfiguration\nhost: Hostname or address of the Redis server where to connect.\nport: Port of the Redis server where to connect.\nusername: Username for authenticating with the Redis server.\npassword: Password for authenticating with the Redis server.\ncache_type: Redis cache/data type to use when storing the incoming messages. i.e hash,set,list,stream,etc.\n\n\nSSL toggle the switch to connect using SSL\nFor SSL Modes, select:\ndisable to disable encrypted communication between Airbyte and the source\nverify-full to always require encryption and verify the identity of the source\n\n\n\nSetup guide\nTODO: more info, screenshots?, etc...",
    "tag": "airbyte"
  },
  {
    "title": "Rockset",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/rockset.md",
    "content": "Rockset\nPrerequisites\n\nFor Airbyte Open Source users using the Postgres source connector, upgrade your Airbyte platform to version `v0.40.0-alpha` or newer and upgrade your Rockset connector to version `0.1.4` or newer\n\nFeatures\n| Feature                       | Support |\n| :---------------------------- | :-----: |\n| Full Refresh Sync             |   \u2705    |\n| Incremental - Append Sync     |   \u2705    |\n| Incremental - Deduped History |   \u274c    |\n| Namespaces                    |   \u274c    |\nTroubleshooting\nConfiguration\n| Parameter  |  Type  | Notes                                                            |\n| :--------- | :----: | :--------------------------------------------------------------- |\n| api_key    | string | rockset api key                                                  |\n| api_server | string | api URL to rockset, specifying http protocol                     |\n| workspace  | string | workspace under which rockset collections will be added/modified |\nGetting Started (Airbyte Open-Source / Airbyte Cloud)\nRequirements\n\nRockset api key with appropriate read and write credentials\n",
    "tag": "airbyte"
  },
  {
    "title": "Local JSON",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/local-json.md",
    "content": "Local JSON\n:::danger\nThis destination is meant to be used on a local workstation and won't work on Kubernetes\n:::\nOverview\nThis destination writes data to a directory on the local filesystem on the host running Airbyte. By default, data is written to `/tmp/airbyte_local`. To change this location, modify the `LOCAL_ROOT` environment variable for Airbyte.\nSync Overview\nOutput schema\nEach stream will be output into its own file. Each file will a collections of `json` objects containing 3 fields:\n\n`_airbyte_ab_id`: a uuid assigned by Airbyte to each event that is processed.\n`_airbyte_emitted_at`: a timestamp representing when the event was pulled from the data source.\n`_airbyte_data`: a json blob representing with the extracted data.\n\nFeatures\n| Feature | Supported |  |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental - Append Sync | Yes |  |\n| Incremental - Deduped History | No | As this connector does not support dbt, we don't support this sync mode on this destination. |\n| Namespaces | No |  |\nPerformance considerations\nThis integration will be constrained by the speed at which your filesystem accepts writes.\nGetting Started\nThe `destination_path` will always start with `/local` whether it is specified by the user or not. Any directory nesting within local will be mapped onto the local mount.\nBy default, the `LOCAL_ROOT` env variable in the `.env` file is set `/tmp/airbyte_local`.\nThe local mount is mounted by Docker onto `LOCAL_ROOT`. This means the `/local` is substituted by `/tmp/airbyte_local` by default.\n:::caution\nPlease make sure that Docker Desktop has access to `/tmp` (and `/private` on a MacOS, as /tmp has a symlink that points to /private. It will not work otherwise). You allow it with \"File sharing\" in `Settings -> Resources -> File sharing -> add the one or two above folder` and hit the \"Apply & restart\" button.\n:::\nExample:\n\nIf `destination_path` is set to `/local/cars/models`\nthe local mount is using the `/tmp/airbyte_local` default\nthen all data will be written to `/tmp/airbyte_local/cars/models` directory.\n\nAccess Replicated Data Files\nIf your Airbyte instance is running on the same computer that you are navigating with, you can open your browser and enter file:///tmp/airbyte_local to look at the replicated data locally. If the first approach fails or if your Airbyte instance is running on a remote server, follow the following steps to access the replicated files:\n\nAccess the scheduler container using `docker exec -it airbyte-server bash`\nNavigate to the default local mount using `cd /tmp/airbyte_local`\nNavigate to the replicated file directory you specified when you created the destination, using `cd /{destination_path}`\nList files containing the replicated data using `ls`\nExecute `cat {filename}` to display the data in a particular file\n\nYou can also copy the output file to your host machine, the following command will copy the file to the current working directory you are using:\n`text\ndocker cp airbyte-server:/tmp/airbyte_local/{destination_path}/{filename}.jsonl .`\nNote: If you are running Airbyte on Windows with Docker backed by WSL2, you have to use similar step as above or refer to this link for an alternative approach.",
    "tag": "airbyte"
  },
  {
    "title": "R2",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/r2.md",
    "content": "R2\nThis page guides you through the process of setting up the R2 destination connector.\nPrerequisites\nList of required fields:\n* Account ID\n* Access Key ID\n* Secret Access Key\n* R2 Bucket Name\n* R2 Bucket Path\n\nAllow connections from Airbyte server to your Cloudflare R2 bucket\n\nStep 1: Set up R2\nSign in to your Cloudflare account.\nPurchase R2 this\nUse an existing or create new Access Key ID and Secret Access Key.\nPrepare R2 bucket that will be used as destination, see this\nto create an S3 bucket, or you can create bucket via R2 module of dashboard.\nStep 2: Set up the R2 destination connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Destinations. In the top-right corner, click + new destination.\nOn the destination setup page, select R2 from the Destination type dropdown and enter a name for this connector.\nConfigure fields:\nAccount Id\nSee this to copy your Account ID.\n\n\nAccess Key Id\nSee this on how to generate an access key.\n\n\nSecret Access Key\nCorresponding key to the above key id.\n\n\nR2 Bucket Name\nSee this to create an R2 bucket or you can create bucket via R2 module of dashboard.\n\n\nR2 Bucket Path\nSubdirectory under the above bucket to sync the data into.\n\n\nR2 Path Format\nAdditional string format on how to store data under R2 Bucket Path. Default value is `${NAMESPACE}/${STREAM_NAME}/${YEAR}_${MONTH}_${DAY}\n   _${EPOCH}_`.\n\n\nR2 Filename pattern\nThe pattern allows you to set the file-name format for the R2 staging file(s), next placeholders combinations are currently supported: \n  {date}, {date:yyyy_MM}, {timestamp}, {timestamp:millis}, {timestamp:micros}, {part_number}, {sync_id}, {format_extension}. Please, don't use empty space and not supportable placeholders, as they won't recognized.\n\n\n\n\nClick `Set up destination`.\n\nFor Airbyte OSS:\n\nGo to local Airbyte page.\nIn the left navigation bar, click Destinations. In the top-right corner, click + new destination.\nOn the destination setup page, select R2 from the Destination type dropdown and enter a name for this connector.\n\nConfigure fields:\n\nAccount Id\nSee this to copy your Account ID.\n\n\nAccess Key Id\nSee this on how to generate an access key.\n\n\nSecret Access Key\nCorresponding key to the above key id.\n\n\nMake sure your R2 bucket is accessible from the machine running Airbyte.\nThis depends on your networking setup.\nThe easiest way to verify if Airbyte is able to connect to your R2 bucket is via the check connection tool in the UI.\nR2 Bucket Name\nSee this to create an R2 bucket or you can create bucket via R2 module of dashboard.\n\n\nR2 Bucket Path\nSubdirectory under the above bucket to sync the data into.\n\n\nR2 Path Format\nAdditional string format on how to store data under R2 Bucket Path. Default value is `${NAMESPACE}/${STREAM_NAME}/${YEAR}_${MONTH}_${DAY}\n   _${EPOCH}_`.\n\n\nR2 Filename pattern\nThe pattern allows you to set the file-name format for the R2 staging file(s), next placeholders combinations are currently supported:\n  {date}, {date:yyyy_MM}, {timestamp}, {timestamp:millis}, {timestamp:micros}, {part_number}, {sync_id}, {format_extension}. Please, don't use empty space and not supportable placeholders, as they won't recognized.\n\n\n\n\n\nClick `Set up destination`.\n\n\nThe full path of the output data with the default S3 Path Format `${NAMESPACE}/${STREAM_NAME}/${YEAR}_${MONTH}_${DAY}_${EPOCH}_` is:\n`text\n<bucket-name>/<source-namespace-if-exists>/<stream-name>/<upload-date>_<epoch>_<partition-id>.<format-extension>`\nFor example:\n`text\ntesting_bucket/data_output_path/public/users/2021_01_01_1234567890_0.csv.gz\n\u2191              \u2191                \u2191      \u2191     \u2191          \u2191          \u2191 \u2191\n|              |                |      |     |          |          | format extension\n|              |                |      |     |          |          unique incremental part id\n|              |                |      |     |          milliseconds since epoch\n|              |                |      |     upload date in YYYY_MM_DD\n|              |                |      stream name\n|              |                source namespace (if it exists)\n|              bucket path\nbucket name`\nThe rationales behind this naming pattern are:\n\nEach stream has its own directory.\nThe data output files can be sorted by upload time.\nThe upload time composes of a date part and millis part so that it is both readable and unique.\n\nBut it is possible to further customize by using the available variables to format the bucket path:\n- `${NAMESPACE}`: Namespace where the stream comes from or configured by the connection namespace fields.\n- `${STREAM_NAME}`: Name of the stream\n- `${YEAR}`: Year in which the sync was writing the output data in.\n- `${MONTH}`: Month in which the sync was writing the output data in.\n- `${DAY}`: Day in which the sync was writing the output data in.\n- `${HOUR}`: Hour in which the sync was writing the output data in.\n- `${MINUTE}` : Minute in which the sync was writing the output data in.\n- `${SECOND}`: Second in which the sync was writing the output data in.\n- `${MILLISECOND}`: Millisecond in which the sync was writing the output data in.\n- `${EPOCH}`: Milliseconds since Epoch in which the sync was writing the output data in.\n- `${UUID}`: random uuid string\nNote:\n- Multiple `/` characters in the R2 path are collapsed into a single `/` character.\n- If the output bucket contains too many files, the part id variable is using a `UUID` instead. It uses sequential ID otherwise.\nPlease note that the stream name may contain a prefix, if it is configured on the connection.\nA data sync may create multiple files as the output files can be partitioned by size (targeting a size of 200MB compressed or lower) .\nSupported sync modes\n| Feature | Support | Notes |\n| :--- | :---: | :--- |\n| Full Refresh Sync | \u2705 | Warning: this mode deletes all previously synced data in the configured bucket path. |\n| Incremental - Append Sync | \u2705 |  |\n| Incremental - Deduped History | \u274c | As this connector does not support dbt, we don't support this sync mode on this destination. |\n| Namespaces | \u274c | Setting a specific bucket path is equivalent to having separate namespaces. |\nThe Airbyte R2 destination allows you to sync data to Cloudflare R2. Each stream is written to its own directory under the bucket.\n\u26a0\ufe0f Please note that under \"Full Refresh Sync\" mode, data in the configured bucket and path will be wiped out before each sync. We recommend you to provision a dedicated R2 resource for this sync to prevent unexpected data deletion from misconfiguration. \u26a0\ufe0f\nSupported Output schema\nEach stream will be outputted to its dedicated directory according to the configuration. The complete datastore of each stream includes all the output files under that directory. You can think of the directory as equivalent of a Table in the database world.\n\nUnder Full Refresh Sync mode, old output files will be purged before new files are created.\nUnder Incremental - Append Sync mode, new output files will be added that only contain the new data.\n\nAvro\nApache Avro serializes data in a compact binary format. Currently, the Airbyte R2 Avro connector always uses thebinary encoding, and assumes that all data records follow the same schema.\nConfiguration\nHere is the available compression codecs:\n\nNo compression\n`deflate`\nCompression level\nRange `[0, 9]`. Default to 0.\nLevel 0: no compression & fastest.\nLevel 9: best compression & slowest.\n\n\n\n\n`bzip2`\n`xz`\nCompression level\nRange `[0, 9]`. Default to 6.\nLevel 0-3 are fast with medium compression.\nLevel 4-6 are fairly slow with high compression.\nLevel 7-9 are like level 6 but use bigger dictionaries and have higher memory requirements. Unless the uncompressed size of the file exceeds 8 MiB, 16 MiB, or 32 MiB, it is waste of memory to use the presets 7, 8, or 9, respectively.\n\n\n\n\n`zstandard`\nCompression level\nRange `[-5, 22]`. Default to 3.\nNegative levels are 'fast' modes akin to `lz4` or `snappy`.\nLevels above 9 are generally for archival purposes.\nLevels above 18 use a lot of memory.\n\n\nInclude checksum\nIf set to `true`, a checksum will be included in each data block.\n\n\n\n\n`snappy`\n\nData schema\nUnder the hood, an Airbyte data stream in JSON schema is first converted to an Avro schema, then the JSON object is converted to an Avro record. Because the data stream can come from any data source, the JSON to Avro conversion process has arbitrary rules and limitations. Learn more about how source data is converted to Avro and the current limitations here.\nCSV\nLike most of the other Airbyte destination connectors, usually the output has three columns: a UUID, an emission timestamp, and the data blob. With the CSV output, it is possible to normalize (flatten) the data blob to multiple columns.\n| Column | Condition | Description |\n| :--- | :--- | :--- |\n| `_airbyte_ab_id` | Always exists | A uuid assigned by Airbyte to each processed record. |\n| `_airbyte_emitted_at` | Always exists. | A timestamp representing when the event was pulled from the data source. |\n| `_airbyte_data` | When no normalization (flattening) is needed, all data reside under this column as a json blob. |  |\n| root level fields | When root level normalization (flattening) is selected, the root level fields are expanded. |  |\nFor example, given the following json object from a source:\n`json\n{\n  \"user_id\": 123,\n  \"name\": {\n    \"first\": \"John\",\n    \"last\": \"Doe\"\n  }\n}`\nWith no normalization, the output CSV is:\n| `_airbyte_ab_id` | `_airbyte_emitted_at` | `_airbyte_data` |\n| :--- | :--- | :--- |\n| `26d73cde-7eb1-4e1e-b7db-a4c03b4cf206` | 1622135805000 | `{ \"user_id\": 123, name: { \"first\": \"John\", \"last\": \"Doe\" } }` |\nWith root level normalization, the output CSV is:\n| `_airbyte_ab_id` | `_airbyte_emitted_at` | `user_id` | `name` |\n| :--- | :--- | :--- | :--- |\n| `26d73cde-7eb1-4e1e-b7db-a4c03b4cf206` | 1622135805000 | 123 | `{ \"first\": \"John\", \"last\": \"Doe\" }` |\nOutput files can be compressed. The default option is GZIP compression. If compression is selected, the output filename will have an extra extension (GZIP: `.csv.gz`).\nJSON Lines (JSONL)\nJSON Lines is a text format with one JSON per line. Each line has a structure as follows:\n`json\n{\n  \"_airbyte_ab_id\": \"<uuid>\",\n  \"_airbyte_emitted_at\": \"<timestamp-in-millis>\",\n  \"_airbyte_data\": \"<json-data-from-source>\"\n}`\nFor example, given the following two json objects from a source:\n`json\n[\n  {\n    \"user_id\": 123,\n    \"name\": {\n      \"first\": \"John\",\n      \"last\": \"Doe\"\n    }\n  },\n  {\n    \"user_id\": 456,\n    \"name\": {\n      \"first\": \"Jane\",\n      \"last\": \"Roe\"\n    }\n  }\n]`\nThey will be like this in the output file:\n`text\n{ \"_airbyte_ab_id\": \"26d73cde-7eb1-4e1e-b7db-a4c03b4cf206\", \"_airbyte_emitted_at\": \"1622135805000\", \"_airbyte_data\": { \"user_id\": 123, \"name\": { \"first\": \"John\", \"last\": \"Doe\" } } }\n{ \"_airbyte_ab_id\": \"0a61de1b-9cdd-4455-a739-93572c9a5f20\", \"_airbyte_emitted_at\": \"1631948170000\", \"_airbyte_data\": { \"user_id\": 456, \"name\": { \"first\": \"Jane\", \"last\": \"Roe\" } } }`\nOutput files can be compressed. The default option is GZIP compression. If compression is selected, the output filename will have an extra extension (GZIP: `.jsonl.gz`).\nParquet\nConfiguration\nThe following configuration is available to configure the Parquet output:\n| Parameter | Type | Default | Description |\n| :--- | :---: | :---: | :--- |\n| `compression_codec` | enum | `UNCOMPRESSED` | Compression algorithm. Available candidates are: `UNCOMPRESSED`, `SNAPPY`, `GZIP`, `LZO`, `BROTLI`, `LZ4`, and `ZSTD`. |\n| `block_size_mb` | integer | 128 (MB) | Block size (row group size) in MB. This is the size of a row group being buffered in memory. It limits the memory usage when writing. Larger values will improve the IO when reading, but consume more memory when writing. |\n| `max_padding_size_mb` | integer | 8 (MB) | Max padding size in MB. This is the maximum size allowed as padding to align row groups. This is also the minimum size of a row group. |\n| `page_size_kb` | integer | 1024 (KB) | Page size in KB. The page size is for compression. A block is composed of pages. A page is the smallest unit that must be read fully to access a single record. If this value is too small, the compression will deteriorate. |\n| `dictionary_page_size_kb` | integer | 1024 (KB) | Dictionary Page Size in KB. There is one dictionary page per column per row group when dictionary encoding is used. The dictionary page size works like the page size but for dictionary. |\n| `dictionary_encoding` | boolean | `true` | Dictionary encoding. This parameter controls whether dictionary encoding is turned on. |\nThese parameters are related to the `ParquetOutputFormat`. See the Java doc for more details. Also see Parquet documentation for their recommended configurations (512 - 1024 MB block size, 8 KB page size).\nData schema\nUnder the hood, an Airbyte data stream in JSON schema is first converted to an Avro schema, then the JSON object is converted to an Avro record, and finally the Avro record is outputted to the Parquet format. Because the data stream can come from any data source, the JSON to Avro conversion process has arbitrary rules and limitations. Learn more about how source data is converted to Avro and the current limitations here.",
    "tag": "airbyte"
  },
  {
    "title": "Redpanda",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/redpanda.md",
    "content": "Redpanda\nThe Airbyte Redpanda destination connector allows you to sync data to Redpada. Each stream is written to the corresponding Redpanda topic.\nSync overview\nOutput schema\nEach stream will be output into a Redpanda topic.\nThe Redpanda topic will be created with the following format `{namespace}_{stream}`\nCurrently, this connector only writes data with JSON format. More formats (e.g. Apache Avro) will be supported in the future.\nEach record will contain in its key the uuid assigned by Airbyte, and in the value these 3 fields:\n\n`_airbyte_ab_id`: a uuid assigned by Airbyte to each event that is processed.\n`_airbyte_emitted_at`: a timestamp representing when the event was pulled from the data source.\n`_airbyte_data`: a json blob representing with the event data.\n\nData type mapping\n| Integration Type | Airbyte Type | Notes |\n| :--------------- | :----------- | :---- |\nFeatures\nThis section should contain a table with the following format:\n| Feature                       | Supported?(Yes/No) | Notes                                                                                        |\n| :---------------------------- | :------------------- | :------------------------------------------------------------------------------------------- |\n| Full Refresh Sync             | No                   |                                                                                              |\n| Incremental - Append Sync     | Yes                  |                                                                                              |\n| Incremental - Deduped History | No                   | As this connector does not support dbt, we don't support this sync mode on this destination. |\n| Namespaces                    | Yes                  |                                                                                              |\nPerformance considerations\nGranted you have enough Redpanda nodes/partitions the cluster should be able to handle any type of load you throw at it from the connector.\nGetting started\nRequirements\n\nThe connector should be able to create topics using the AdminClient\nConfiguration options\nBootstrap servers\nBuffer Memory\nCompression Type\nBatch Size\nRetries\nNumber of topic partitions\nTopic replication factor\nSocket Connection Setup Timeout\nSocket Connection Setup Max Timeout\n\nMore info about this can be found in the Redpanda producer configs documentation site.\nNOTE: Configurations for SSL are not available yet.",
    "tag": "airbyte"
  },
  {
    "title": "Mariadb Columnstore",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/mariadb-columnstore.md",
    "content": "Mariadb Columnstore\nSync overview\nOutput schema\nEach stream will be output into its own table in MariaDB ColumnStore. Each table will contain 3 columns:\n\n`_airbyte_ab_id`: a uuid assigned by Airbyte to each event that is processed. The column type in MariaDB ColumnStore is VARCHAR(256).\n`_airbyte_emitted_at`: a timestamp representing when the event was pulled from the data source. The column type in MariaDB ColumnStore is TIMESTAMP.\n`_airbyte_data`: a json blob representing with the event data. The column type in MariaDB ColumnStore is LONGTEXT.\n\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | Yes |  |\n| Replicate Incremental Deletes | Yes |  |\n| SSL connection | No |  |\n| SSH Tunnel Support | Yes |  |\nPerformance considerations\nCould this connector hurt the user's database/API/etc... or put too much strain on it in certain circumstances? For example, if there are a lot of tables or rows in a table? What is the breaking point (e.g: 100mm> records)? What can the user do to prevent this? (e.g: use a read-only replica, or schedule frequent syncs, etc..)\nGetting started\nRequirements\nTo use the MariaDB ColumnStore destination, you'll need to sync data to MariaDB ColumnStore 5.5.3 or above.\nNetwork Access\nMake sure your MariaDB ColumnStore database can be accessed by Airbyte. If your database is within a VPC, you may need to allow access from the IP you're using to expose Airbyte.\nPermissions\nYou need a MariaDB user with `CREATE, INSERT, SELECT, DROP` permissions. We highly recommend creating an Airbyte-specific user for this purpose.\nTarget Database\nMariaDB ColumnStore doesn't differentiate between a database and schema. A database is essentially a schema where all the tables live in. You will need to choose an existing database or create a new database. This will act as a default database/schema where the tables will be created if the source doesn't provide a namespace.\nSetup the MariaDB ColumnStore destination in Airbyte\nBefore setting up MariaDB ColumnStore destination in Airbyte, you need to set the local_infile system variable to true. You can do this by running the query `SET GLOBAL local_infile = true` . This is required cause Airbyte uses `LOAD DATA LOCAL INFILE` to load data into table.\nYou should now have all the requirements needed to configure MariaDB ColumnStore as a destination in the UI. You'll need the following information to configure the MariaDB ColumnStore destination:\n\nHost\nPort\nUsername\nPassword\nDatabase\n\nConnection via SSH Tunnel\nAirbyte has the ability to connect to a MariaDB instance via an SSH Tunnel. The reason you might want to do this because it is not possible (or against security policy) to connect to the database directly (e.g. it does not have a public IP address).\nWhen using an SSH tunnel, you are configuring Airbyte to connect to an intermediate server (a.k.a. a bastion sever) that does have direct access to the database. Airbyte connects to the bastion and then asks the bastion to connect directly to the server.\nUsing this feature requires additional configuration, when creating the destination. We will talk through what each piece of configuration means.\n\nConfigure all fields for the destination as you normally would, except `SSH Tunnel Method`.\n`SSH Tunnel Method` defaults to `No Tunnel` (meaning a direct connection). If you want to use an SSH Tunnel choose `SSH Key Authentication` or `Password Authentication`.\nChoose `Key Authentication` if you will be using an RSA private key as your secret for establishing the SSH Tunnel (see below for more information on generating this key).\nChoose `Password Authentication` if you will be using a password as your secret for establishing the SSH Tunnel.\n`SSH Tunnel Jump Server Host` refers to the intermediate (bastion) server that Airbyte will connect to. This should be a hostname or an IP Address.\n`SSH Connection Port` is the port on the bastion server with which to make the SSH connection. The default port for SSH connections is `22`, so unless you have explicitly changed something, go with the default.\n`SSH Login Username` is the username that Airbyte should use when connection to the bastion server. This is NOT the MySQl username.\nIf you are using `Password Authentication`, then `SSH Login Username` should be set to the password of the User from the previous step. If you are using `SSH Key Authentication` leave this blank. Again, this is not the MariaDB password, but the password for the OS-user that Airbyte is using to perform commands on the bastion.\nIf you are using `SSH Key Authentication`, then `SSH Private Key` should be set to the RSA Private Key that you are using to create the SSH connection. This should be the full contents of the key file starting with `-----BEGIN RSA PRIVATE KEY-----` and ending with `-----END RSA PRIVATE KEY-----`.\n",
    "tag": "airbyte"
  },
  {
    "title": "MySQL",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/mysql.md",
    "content": "MySQL\nThere are two flavors of connectors for this destination:\n1. destination-mysql connector. Supports both SSL and non SSL connections.\n2. destination-mysql-strict-encrypt connector. Pretty same as connector above, but supports SSL connections only.\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental - Append Sync | Yes |  |\n| Incremental - Deduped History | No |  |\n| Namespaces | Yes |  |\n| SSH Tunnel Connection | Yes |  |\nOutput Schema\nEach stream will be output into its own table in MySQL. Each table will contain 3 columns:\n\n`_airbyte_ab_id`: a uuid assigned by Airbyte to each event that is processed. The column type in MySQL is `VARCHAR(256)`.\n`_airbyte_emitted_at`: a timestamp representing when the event was pulled from the data source. The column type in MySQL is `TIMESTAMP(6)`.\n`_airbyte_data`: a json blob representing with the event data. The column type in MySQL is `JSON`.\n\nGetting Started (Airbyte Cloud)\nAirbyte Cloud only supports connecting to your MySQL instance with TLS encryption. Other than that, you can proceed with the open-source instructions below.\nGetting Started (Airbyte Open-Source)\nRequirements\nTo use the MySQL destination, you'll need:\n\nTo sync data to MySQL with normalization MySQL database 8.0.0 or above\nTo sync data to MySQL without normalization you'll need MySQL 5.0 or above.\n\nTroubleshooting\nSome users reported that they could not connect to Amazon RDS MySQL or MariaDB. This can be diagnosed with the error message: `Cannot create a PoolableConnectionFactory`.\nTo solve this issue add `enabledTLSProtocols=TLSv1.2` in the JDBC parameters.\nNetwork Access\nMake sure your MySQL database can be accessed by Airbyte. If your database is within a VPC, you may need to allow access from the IP you're using to expose Airbyte.\nPermissions\nYou need a MySQL user with `CREATE, INSERT, SELECT, DROP` permissions. We highly recommend creating an Airbyte-specific user for this purpose.\nTarget Database\nMySQL doesn't differentiate between a database and schema. A database is essentially a schema where all the tables live in. You will need to choose an existing database or create a new database. This will act as a default database/schema where the tables will be created if the source doesn't provide a namespace.\nSetup the MySQL destination in Airbyte\nBefore setting up MySQL destination in Airbyte, you need to set the local_infile system variable to true. You can do this by running the query `SET GLOBAL local_infile = true` with a user with SYSTEM_VARIABLES_ADMIN permission. This is required cause Airbyte uses `LOAD DATA LOCAL INFILE` to load data into table.\nYou should now have all the requirements needed to configure MySQL as a destination in the UI. You'll need the following information to configure the MySQL destination:\n\nHost\nPort\nUsername\nPassword\nDatabase\njdbc_url_params (Optional)\n\nDefault JDBC URL Parameters\nThe following JDBC URL parameters are set by Airbyte and cannot be overridden by the `jdbc_url_params` field:\n\n`useSSL=true` (unless `ssl` is set to false)\n`requireSSL=true` (unless `ssl` is set to false)\n`verifyServerCertificate=false` (unless `ssl` is set to false)\n`zeroDateTimeBehavior=convertToNull`\n\nKnown Limitations\nNote that MySQL documentation discusses identifiers case sensitivity using the `lower_case_table_names` system variable. One of their recommendations is:\n`text\n\"It is best to adopt a consistent convention, such as always creating and referring to databases and tables using lowercase names.\n This convention is recommended for maximum portability and ease of use.\"`\nSource: MySQL docs\nAs a result, Airbyte MySQL destination forces all identifier (table, schema and columns) names to be lowercase.\nConnection via SSH Tunnel\nAirbyte has the ability to connect to a MySQl instance via an SSH Tunnel. The reason you might want to do this because it is not possible (or against security policy) to connect to the database directly (e.g. it does not have a public IP address).\nWhen using an SSH tunnel, you are configuring Airbyte to connect to an intermediate server (a.k.a. a bastion sever) that does have direct access to the database. Airbyte connects to the bastion and then asks the bastion to connect directly to the server.\nUsing this feature requires additional configuration, when creating the destination. We will talk through what each piece of configuration means.\n\nConfigure all fields for the destination as you normally would, except `SSH Tunnel Method`.\n`SSH Tunnel Method` defaults to `No Tunnel` (meaning a direct connection). If you want to use an SSH Tunnel choose `SSH Key Authentication` or `Password Authentication`.\nChoose `Key Authentication` if you will be using an RSA private key as your secret for establishing the SSH Tunnel (see below for more information on generating this key).\nChoose `Password Authentication` if you will be using a password as your secret for establishing the SSH Tunnel.\n\n:::warning\n    Since Airbyte Cloud requires encrypted communication, select SSH Key Authentication or Password Authentication if you selected preferred as the SSL Mode; otherwise, the connection will fail.\n   :::\n\n`SSH Tunnel Jump Server Host` refers to the intermediate (bastion) server that Airbyte will connect to. This should be a hostname or an IP Address.\n`SSH Connection Port` is the port on the bastion server with which to make the SSH connection. The default port for SSH connections is `22`, so unless you have explicitly changed something, go with the default.\n`SSH Login Username` is the username that Airbyte should use when connection to the bastion server. This is NOT the MySQl username.\nIf you are using `Password Authentication`, then `SSH Login Username` should be set to the password of the User from the previous step. If you are using `SSH Key Authentication` leave this blank. Again, this is not the MySQl password, but the password for the OS-user that Airbyte is using to perform commands on the bastion.\nIf you are using `SSH Key Authentication`, then `SSH Private Key` should be set to the RSA Private Key that you are using to create the SSH connection. This should be the full contents of the key file starting with `-----BEGIN RSA PRIVATE KEY-----` and ending with `-----END RSA PRIVATE KEY-----`.\n",
    "tag": "airbyte"
  },
  {
    "title": "Typesense",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/typesense.md",
    "content": "Typesense\nOverview\nThe Airbyte Typesense destination allows you to sync data to Airbyte.Typesense is a modern, privacy-friendly, open source search engine built from the ground up using cutting-edge search algorithms, that take advantage of the latest advances in hardware capabilities.\nSync overview\nUsing overwrite sync, the auto schema detection is used and all the fields in a document are automatically indexed for searching and filtering\nWith append mode, you have to create the collection first and can use pre-defined schema that gives you fine-grained control over your document fields.\nOutput schema\nEach stream will be output into its own collection in Typesense. If an id column is not provided, it will be generated.\nFeatures\n| Feature                       | Supported?(Yes/No) | Notes                                                                                        |\n| :---------------------------- | :------------------- | :------------------------------------------------------------------------------------------- |\n| Full Refresh Sync             | Yes                  |                                                                                              |\n| Incremental - Append Sync     | Yes                  |                                                                                              |\n| Incremental - Deduped History | No                   | As this connector does not support dbt, we don't support this sync mode on this destination. |\n| Namespaces                    | No                   |                                                                                              |\nGetting started\nRequirements\nTo use the Typesense destination, you'll need an existing Typesense instance. You can learn about how to create one in the Typesense docs.\nSetup guide\nThe setup only requires two fields. First is the `host` which is the address at which Typesense can be reached. The second piece of information is the API key.",
    "tag": "airbyte"
  },
  {
    "title": "Exasol",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/exasol.md",
    "content": "Exasol\nExasol is the in-memory database built for analytics.\nSync overview\nOutput schema\nEach Airbyte Stream becomes an Exasol table and each Airbyte Field becomes an Exasol column. Each Exasol table created by Airbyte will contain 3 columns:\n\n`_AIRBYTE_AB_ID`: a uuid assigned by Airbyte to each event that is processed. The column type in Exasol is `VARCHAR(64)`.\n`_AIRBYTE_DATA`: a json blob representing with the event data. The column type in Exasol is `VARCHAR(2000000)`.\n`_AIRBYTE_EMITTED_AT`: a timestamp representing when the event was pulled from the data source. The column type in Exasol is `TIMESTAMP`.\n\nFeatures\nThe Exasol destination supports the following features:\n| Feature                       | Supported? (Yes/No) | Notes |\n| :---------------------------- | :------------------ | :---- |\n| Full Refresh Sync             | Yes                 |       |\n| Incremental - Append Sync     | Yes                 |       |\n| Incremental - Deduped History | No                  |       |\n| Normalization                 | No                  |       |\n| Namespaces                    | Yes                 |       |\n| SSL connection                | Yes                 | TLS   |\n| SSH Tunnel Support            | No                  |       |\nLimitations\nMaximum data size two million characters\nExasol does not have a special data type for storing data of arbitrary length or JSON. That's why this connector uses type `VARCHAR(2000000)` for storing Airbyte data.\nGetting started\nRequirements\nTo use the Exasol destination, you'll need Exasol database version 7.1 or above.\nNetwork Access\nMake sure your Exasol database can be accessed by Airbyte. If your database is within a VPC, you may need to allow access from the IP you're using to expose Airbyte.\nPermissions\nAs Airbyte namespaces allow to store data into different schemas, there are different scenarios requiring different permissions assigned to the user account. The following table describes 4 scenarios regarding the login user and the destination user.\n| Login user   | Destination user   | Required permissions                                          | Comment                                                                    |\n| :----------- | :----------------- | :------------------------------------------------------------ | :------------------------------------------------------------------------- |\n| DBA User     | Any user           | -                                                             |                                                                            |\n| Regular user | Same user as login | Create, drop and write table, create session                  |                                                                            |\n| Regular user | Any existing user  | Create, drop and write ANY table, create session              | Grants can be provided on a system level by DBA or by target user directly |\n| Regular user | Not existing user  | Create, drop and write ANY table, create user, create session | Grants should be provided on a system level by DBA                         |\nWe highly recommend creating an Airbyte-specific user for this purpose.\nSetup guide\nYou should now have all the requirements needed to configure Exasol as a destination in the UI. You'll need the following information to configure the Exasol destination:\n\nHost\nPort\nFingerprint of the Exasol server's TLS certificate (if the database uses a self-signed certificate)\nUsername\nPassword\n",
    "tag": "airbyte"
  },
  {
    "title": "Oracle DB",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/oracle.md",
    "content": "Oracle DB\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental - Append Sync | Yes |  |\n| Incremental - Deduped History | Yes |  |\n| Namespaces | Yes |  |\n| Basic Normalization | Yes | Doesn't support for nested json yet |\n| SSH Tunnel Connection | Yes |  |\n| Encryption | Yes | Support Native Network Encryption (NNE) as well as TLS using SSL cert |\nOutput Schema\nBy default, each stream will be output into its own table in Oracle. Each table will contain 3 columns:\n\n`_AIRBYTE_AB_ID`: a uuid assigned by Airbyte to each event that is processed. The column type in Oracle is `VARCHAR(64)`.\n`_AIRBYTE_EMITTED_AT`: a timestamp representing when the event was pulled from the data source. The column type in Oracle is `TIMESTAMP WITH TIME ZONE`.\n`_AIRBYTE_DATA`: a json blob representing with the event data. The column type in Oracles is `NCLOB`.\n\nEnabling normalization will also create normalized, strongly typed tables.\nGetting Started (Airbyte Cloud)\nThe Oracle connector is currently in Alpha on Airbyte Cloud. Only TLS encrypted connections to your DB can be made from Airbyte Cloud. Other than that, follow the open-source instructions below.\nGetting Started (Airbyte Open-Source)\nRequirements\nTo use the Oracle destination, you'll need:\n\nAn Oracle server version 18 or above\nIt's possible to use Oracle 12+ but you need to configure the table name length to 120 chars.\n\nNetwork Access\nMake sure your Oracle database can be accessed by Airbyte. If your database is within a VPC, you may need to allow access from the IP you're using to expose Airbyte.\nPermissions\nAs Airbyte namespaces allows us to store data into different schemas, we have different scenarios and list of required permissions:\n| Login user | Destination user | Required permissions | Comment |\n| :--- | :--- | :--- | :--- |\n| DBA User | Any user | - |  |\n| Regular user | Same user as login | Create, drop and write table, create session |  |\n| Regular user | Any existing user | Create, drop and write ANY table, create session | Grants can be provided on a system level by DBA or by target user directly |\n| Regular user | Not existing user | Create, drop and write ANY table, create user, create session | Grants should be provided on a system level by DBA |\nWe highly recommend creating an Airbyte-specific user for this purpose.\nSetup the Oracle destination in Airbyte\nYou should now have all the requirements needed to configure Oracle as a destination in the UI. You'll need the following information to configure the Oracle destination:\n\nHost\nPort\nUsername\nPassword\nDatabase\nConnection via SSH Tunnel\n\nAirbyte has the ability to connect to a Oracle instance via an SSH Tunnel. The reason you might want to do this because it is not possible (or against security policy) to connect to the database directly (e.g. it does not have a public IP address).\nWhen using an SSH tunnel, you are configuring Airbyte to connect to an intermediate server (a.k.a. a bastion sever) that does have direct access to the database. Airbyte connects to the bastion and then asks the bastion to connect directly to the server.\nUsing this feature requires additional configuration, when creating the source. We will talk through what each piece of configuration means.\n\nConfigure all fields for the source as you normally would, except `SSH Tunnel Method`.\n`SSH Tunnel Method` defaults to `No Tunnel` (meaning a direct connection). If you want to use an SSH Tunnel choose `SSH Key Authentication` or `Password Authentication`.\nChoose `Key Authentication` if you will be using an RSA private key as your secret for establishing the SSH Tunnel (see below for more information on generating this key).\nChoose `Password Authentication` if you will be using a password as your secret for establishing the SSH Tunnel.\n\n\n`SSH Tunnel Jump Server Host` refers to the intermediate (bastion) server that Airbyte will connect to. This should be a hostname or an IP Address.\n`SSH Connection Port` is the port on the bastion server with which to make the SSH connection. The default port for SSH connections is `22`, so unless you have explicitly changed something, go with the default.\n`SSH Login Username` is the username that Airbyte should use when connection to the bastion server. This is NOT the Oracle username.\nIf you are using `Password Authentication`, then `SSH Login Username` should be set to the password of the User from the previous step. If you are using `SSH Key Authentication` leave this blank. Again, this is not the Oracle password, but the password for the OS-user that Airbyte is using to perform commands on the bastion.\nIf you are using `SSH Key Authentication`, then `SSH Private Key` should be set to the RSA Private Key that you are using to create the SSH connection. This should be the full contents of the key file starting with `-----BEGIN RSA PRIVATE KEY-----` and ending with `-----END RSA PRIVATE KEY-----`.\n\nEncryption Options\nAirbyte has the ability to connect to the Oracle source with 3 network connectivity options:\n\n`Unencrypted` the connection will be made using the TCP protocol. In this case, all data over the network will be transmitted in unencrypted form.\n`Native network encryption` gives you the ability to encrypt database connections, without the configuration overhead of TCP / IP and SSL / TLS and without the need to open and listen on different ports. In this case, the SQLNET.ENCRYPTION_CLIENT\n   option will always be set as REQUIRED by default: The client or server will only accept encrypted traffic, but the user has the opportunity to choose an `Encryption algorithm` according to the security policies he needs.\n`TLS Encrypted` (verify certificate) - if this option is selected, data transfer will be transfered using the TLS protocol, taking into account the handshake procedure and certificate verification. To use this option, insert the content of the certificate issued by the server into the `SSL PEM file` field\n",
    "tag": "airbyte"
  },
  {
    "title": "Databricks Lakehouse",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/databricks.md",
    "content": "Databricks Lakehouse\nOverview\nThis destination syncs data to Delta Lake on Databricks Lakehouse. Each stream is written to its own delta-table.\nThis connector requires a JDBC driver to connect to the Databricks cluster. By using the driver and the connector, you must agree to the JDBC ODBC driver license. This means that you can only use this connector to connect third party applications to Apache Spark SQL within a Databricks offering using the ODBC and/or JDBC protocols.\nCurrently, this connector requires 30+MB of memory for each stream. When syncing multiple streams, it may run into an out-of-memory error if the allocated memory is too small. This performance bottleneck is tracked in this issue. Once this issue is resolved, the connector should be able to sync an almost infinite number of streams with less than 500MB of memory.\nSync Mode\n| Feature | Support | Notes |\n| :--- | :---: | :--- |\n| Full Refresh Sync | \u2705 | Warning: this mode deletes all previously synced data in the configured bucket path. |\n| Incremental - Append Sync | \u2705 |  |\n| Incremental - Deduped History | \u274c |  |\n| Namespaces | \u2705 |  |\nData Source\nDatabricks Delta Lake supports various cloud storage as the data source. Currently, only Amazon S3 and Azure Blob Storage are supported by this connector.\nConfiguration\n| Category            | Parameter             |  Type   | Notes                                                                                                                                                                                                                                                                                                                                                       |\n|:--------------------|:----------------------|:-------:|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Databricks          | Server Hostname       | string  | Required. Example: `abc-12345678-wxyz.cloud.databricks.com`. See documentation. Please note that this is the server for the Databricks Cluster. It is different from the SQL Endpoint Cluster.                                             |\n|                     | HTTP Path             | string  | Required. Example: `sql/protocolvx/o/1234567489/0000-1111111-abcd90`. See documentation.                                                                                                                                                   |\n|                     | Port                  | string  | Optional. Default to \"443\". See documentation.                                                                                                                                                                                             |\n|                     | Personal Access Token | string  | Required. Example: `dapi0123456789abcdefghij0123456789AB`. See documentation.                                                                                                                                                                                                  |\n| General             | Database schema       | string  | Optional. Default to \"public\". Each data stream will be written to a table under this database schema.                                                                                                                                                                                                                                                      |\n|                     | Purge Staging Data    | boolean | The connector creates staging files and tables on S3 or Azure. By default, they will be purged when the data sync is complete. Set it to `false` for debugging purposes.                                                                                                                                                                                    |\n| Data Source - S3    | Bucket Name           | string  | Name of the bucket to sync data into.                                                                                                                                                                                                                                                                                                                       |\n|                     | Bucket Path           | string  | Subdirectory under the above bucket to sync the data into.                                                                                                                                                                                                                                                                                                  |\n|                     | Region                | string  | See documentation for all region codes.                                                                                                                                                                                             |\n|                     | Access Key ID         | string  | AWS/Minio credential.                                                                                                                                                                                                                                                                                                                                       |\n|                     | Secret Access Key     | string  | AWS/Minio credential.                                                                                                                                                                                                                                                                                                                                       |\n|                     | S3 Filename pattern   | string  | The pattern allows you to set the file-name format for the S3 staging file(s), next placeholders combinations are currently supported: {date}, {date:yyyy_MM}, {timestamp}, {timestamp:millis}, {timestamp:micros}, {part_number}, {sync_id}, {format_extension}. Please, don't use empty space and not supportable placeholders, as they won't recognized. |\n| Data Source - Azure | Account Name          | string  | Name of the account to sync data into.                                                                                                                                                                                                                                                                                                                      |\n|                     | Container Name        | string  | Container under the above account to sync the data into.                                                                                                                                                                                                                                                                                                    |\n|                     | SAS token             | string  | Shared-access signature token for the above account.                                                                                                                                                                                                                                                                                                        |\n|                     | Endpoint domain name  | string  | Usually blob.core.windows.net.                                                                                                                                                                                                                                                                                                                              |\n\u26a0\ufe0f Please note that under \"Full Refresh Sync\" mode, data in the configured bucket and path will be wiped out before each sync. We recommend you provision a dedicated S3 or Azure resource for this sync to prevent unexpected data deletion from misconfiguration. \u26a0\ufe0f\nStaging Files (Delta Format)\nS3\nData streams are first written as staging delta-table (Parquet + Transaction Log) files on S3, and then loaded into Databricks delta-tables. All the staging files will be deleted after the sync is done. For debugging purposes, here is the full path for a staging file:\n`text\ns3://<bucket-name>/<bucket-path>/<uuid>/<stream-name>`\nFor example:\n`text\ns3://testing_bucket/data_output_path/98c450be-5b1c-422d-b8b5-6ca9903727d9/users/_delta_log\n     \u2191              \u2191                \u2191                                    \u2191     \u2191\n     |              |                |                                    |     transaction log\n     |              |                |                                    stream name\n     |              |                database schema\n     |              bucket path\n     bucket name`\nAzure\nSimilarly, streams are first written to a staging location, but the Azure option uses CSV format. A staging table is created from the CSV files.\nUnmanaged Spark SQL Table\nCurrently, all streams are synced into unmanaged Spark SQL tables. See documentation for details. In summary, you have full control of the location of the data underlying an unmanaged table. In S3, the full path of each data stream is:\n`text\ns3://<bucket-name>/<bucket-path>/<database-schema>/<stream-name>`\nFor example:\n`text\ns3://testing_bucket/data_output_path/public/users\n     \u2191              \u2191                \u2191      \u2191\n     |              |                |      stream name\n     |              |                database schema\n     |              bucket path\n     bucket name`\nIn Azure, the full path of each data stream is:\n`text\nabfss://<container-name>@<account-name>.dfs.core.windows.net/<database-schema>/<stream-name>`\nPlease keep these data directories on S3/Azure. Otherwise, the corresponding tables will have no data in Databricks.\nOutput Schema\nEach table will have the following columns:\n| Column | Type | Notes |\n| :--- | :---: | :--- |\n| `_airbyte_ab_id` | string | UUID. |\n| `_airbyte_emitted_at` | timestamp | Data emission timestamp. |\n| Data fields from the source stream | various | All fields in the staging files will be expanded in the table. |\nUnder the hood, an Airbyte data stream in Json schema is first converted to an Avro schema, then the Json object is converted to an Avro record, and finally the Avro record is outputted to the Parquet format. Because the data stream can come from any data source, the Json to Avro conversion process has arbitrary rules and limitations. Learn more about how source data is converted to Avro and the current limitations here.\nGetting started\nRequirements\n\nCredentials for a Databricks cluster. See documentation.\nCredentials for an S3 bucket or Azure container. See documentation.\nGrant the Databricks cluster full access to the S3 bucket or Azure container. Or mount it as Databricks File System (DBFS). See documentation.\n\nRelated tutorial\nSuppose you are interested in learning more about the Databricks connector or details on how the Delta Lake tables are created. You may want to consult the tutorial on How to Load Data into Delta Lake on Databricks Lakehouse.",
    "tag": "airbyte"
  },
  {
    "title": "Pulsar",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/pulsar.md",
    "content": "Pulsar\nOverview\nThe Airbyte Pulsar destination allows you to sync data to Pulsar. Each stream is written to the corresponding Pulsar topic.\nPrerequisites\n\nFor Airbyte Open Source users using the Postgres source connector, upgrade your Airbyte platform to version `v0.40.0-alpha` or newer and upgrade your Pulsar connector to version `0.1.3` or newer\n\nSync overview\nOutput schema\nEach stream will be output into a Pulsar topic.\nCurrently, this connector only writes data with JSON format. More formats (e.g. Apache Avro) will be supported in the future.\nEach record will contain in its key the uuid assigned by Airbyte, and in the value these 3 fields:\n\n`_airbyte_ab_id`: a uuid assigned by Airbyte to each event that is processed.\n`_airbyte_emitted_at`: a timestamp representing when the event was pulled from the data source.\n`_airbyte_data`: a json blob representing with the event data encoded in base64 .\n`_airbyte_stream`: the name of each record's stream.\n\nFeatures\n| Feature                       | Supported?(Yes/No) | Notes                                                                                        |\n| :---------------------------- | :------------------- | :------------------------------------------------------------------------------------------- |\n| Full Refresh Sync             | No                   |                                                                                              |\n| Incremental - Append Sync     | Yes                  |                                                                                              |\n| Incremental - Deduped History | No                   | As this connector does not support dbt, we don't support this sync mode on this destination. |\n| Namespaces                    | Yes                  |                                                                                              |\nGetting started\nRequirements\nTo use the Pulsar destination, you'll need:\n\nA Pulsar cluster 2.8 or above.\n\nSetup guide\nNetwork Access\nMake sure your Pulsar brokers can be accessed by Airbyte.\nPermissions\nAirbyte should be allowed to write messages into topics, and these topics should be created before writing into Pulsar or, at least, enable the configuration in the brokers `allowAutoTopicCreation` (which is not recommended for production environments).\nNote that if you choose to use dynamic topic names, you will probably need to enable `allowAutoTopicCreation` to avoid your connection failing if there was an update to the source connector's schema. Otherwise a hardcoded topic name may be best.\nAlso, notice that the messages will be sent to topics based on the configured Pulsar `topic_tenant` and `topic_namespace` configs with their `topic_type`.\nTarget topics\nYou can determine the topics to which messages are written via the `topic_pattern` configuration parameter in its corresponding Pulsar `topic_tenant`-`topic_namespace`. Messages can be written to either a hardcoded, pre-defined topic, or dynamically written to different topics based on the namespace or stream they came from.\nTo write all messages to a single hardcoded topic, enter its name in the `topic_pattern` field e.g: setting `topic_pattern` to `my-topic-name` will write all messages from all streams and namespaces to that topic.\nTo define the output topics dynamically, you can leverage the `{namespace}` and `{stream}` pattern variables, which cause messages to be written to different topics based on the values present when producing the records. For example, setting the `topic_pattern` parameter to `airbyte_syncs/{namespace}/{stream}` means that messages from namespace `n1` and stream `s1` will get written to the topic `airbyte_syncs/n1/s1`, and messages from `s2` to `airbyte_syncs/n1/s2` etc.\nIf you define output topic dynamically, you might want to enable `allowAutoTopicCreation` to avoid your connection failing if there was an update to the source connector's schema. Otherwise, you'll need to manually create topics in Pulsar as they are added/updated in the source, which is the recommended option for production environments.\nNOTICE: a naming convention transformation will be applied to the target topic name using the `StandardNameTransformer` so that some special characters will be replaced.\nSetup the Pulsar destination in Airbyte\nYou should now have all the requirements needed to configure Pulsar as a destination in the UI. You can configure the following parameters on the Pulsar destination (though many of these are optional or have default values):\n\nPulsar brokers\nUse TLS\nTopic type\nTopic tenant\nTopic namespace\nTopic pattern\nTest topic\nProducer name\nSync producer\nCompression type\nMessage send timeout\nMax pending messages\nMax pending messages across partitions\nEnable batching\nBatching max messages\nBatching max publish delay\nBlock if queue is full\n\nMore info about this can be found in the Pulsar producer configs documentation site.",
    "tag": "airbyte"
  },
  {
    "title": "Teradata",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/teradata.md",
    "content": "Teradata\nThis page guides you through the process of setting up the Teradata destination connector.\nPrerequisites\nTo use the Teradata destination connector, you'll need:\n\n\nAccess to a Teradata Vantage instance\nNote: If you need a new instance of Vantage, you can install a free version called Vantage Express in the cloud on Google Cloud, Azure, and AWS. You can also run Vantage Express on your local machine using VMware, VirtualBox, or UTM.\n\n\nYou'll need the following information to configure the Teradata destination:\n\nHost - The host name of the Teradata Vantage instance.\nUsername\nPassword\nDefault Schema Name - Specify the schema (or several schemas separated by commas) to be set in the search-path. These schemas will be used to resolve unqualified object names used in statements executed over this connection.\nJDBC URL Params (optional)\n\nRefer to this guide for more details\nSync overview\nOutput schema\nEach stream will be output into its own table in Teradata. Each table will contain 3 columns:\n\n`_airbyte_ab_id`: a uuid assigned by Airbyte to each event that is processed. The column type in Teradata is `VARCHAR(256)`.\n`_airbyte_emitted_at`: a timestamp representing when the event was pulled from the data source. The column type in Teradata is `TIMESTAMP(6)`.\n`_airbyte_data`: a json blob representing with the event data. The column type in Teradata is `JSON`.\n\nFeatures\nThe Teradata destination connector supports the\nfollowing sync modes:\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental - Append Sync | Yes |  |\n| Incremental - Deduped History | No |  |\n| Namespaces | Yes |  |\nPerformance considerations\nGetting started\nRequirements\nYou need a Teradata user with the following permissions:\n\ncan create tables and write permission.\ncan create schemas e.g:\n\nYou can create such a user by running:\n```\nCREATE USER airbyte_user  as perm=10e6, PASSWORD=;\nGRANT ALL on dbc to airbyte_user;\n```\nYou can also use a pre-existing user but we highly recommend creating a dedicated user for Airbyte.\nSetup guide\nSet up the Teradata Destination connector\n\nLog into your Airbyte Open Source account.\nClick Destinations and then click + New destination.\nOn the Set up the destination page, select Teradata from the Destination type dropdown.\nEnter the Name for the Teradata connector.\nFor Host, enter the host domain of the Teradata instance\nFor Default Schema, enter the Default Schema name. The default value is public.\nFor User and Password, enter the database username and password.\nTo customize the JDBC connection beyond common options, specify additional supported JDBC URL parameters as key-value pairs separated by the symbol & in the JDBC URL Params field.\n\nExample: key1=value1&key2=value2&key3=value3\nThese parameters will be added at the end of the JDBC URL that the AirByte will use to connect to your Teradata database.",
    "tag": "airbyte"
  },
  {
    "title": "S3",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/s3.md",
    "content": "S3\nThis page guides you through the process of setting up the S3 destination connector.\nPrerequisites\nList of required fields:\n* Access Key ID\n* Secret Access Key\n* S3 Bucket Name\n* S3 Bucket Path\n* S3 Bucket Region\n\nAllow connections from Airbyte server to your AWS S3/ Minio S3 cluster (if they exist in separate VPCs).\nAn S3 bucket with credentials or an instance profile with read/write permissions configured for the host (ec2, eks).\nEnforce encryption of data in transit\n\nSetup guide\nStep 1: Set up S3\nSign in to your AWS account.\nUse an existing or create new Access Key ID and Secret Access Key.\nPrepare S3 bucket that will be used as destination, see this to create an S3 bucket.\nNOTE: If the S3 cluster is not configured to use TLS, the connection to Amazon S3 silently reverts to an unencrypted connection. Airbyte recommends all connections be configured to use TLS/SSL as support for AWS's shared responsibility model\nStep 2: Set up the S3 destination connector in Airbyte\n\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Destinations. In the top-right corner, click + new destination.\nOn the destination setup page, select S3 from the Destination type dropdown and enter a name for this connector.\nConfigure fields:\nAccess Key Id\nSee this on how to generate an access key.\nWe recommend creating an Airbyte-specific user. This user will require read and write permissions to objects in the bucket.\n\n\nSecret Access Key\nCorresponding key to the above key id.\n\n\nS3 Bucket Name\nSee this to create an S3 bucket.\n\n\nS3 Bucket Path\nSubdirectory under the above bucket to sync the data into.\n\n\nS3 Bucket Region\nSee here for all region codes.\n\n\nS3 Path Format\nAdditional string format on how to store data under S3 Bucket Path. Default value is `${NAMESPACE}/${STREAM_NAME}/${YEAR}_${MONTH}_${DAY}_${EPOCH}_`.\n\n\nS3 Endpoint\nLeave empty if using AWS S3, fill in S3 URL if using Minio S3.\n\n\nS3 Filename pattern\nThe pattern allows you to set the file-name format for the S3 staging file(s), next placeholders combinations are currently supported: {date}, {date:yyyy_MM}, {timestamp}, {timestamp:millis}, {timestamp:micros}, {part_number}, {sync_id}, {format_extension}. Please, don't use empty space and not supportable placeholders, as they won't be recognized.\n\n\nClick `Set up destination`.\n\n\n\nFor Airbyte Open Source:\n\nGo to local Airbyte page.\nIn the left navigation bar, click Destinations. In the top-right corner, click + new destination.\nOn the destination setup page, select S3 from the Destination type dropdown and enter a name for this connector.\nConfigure fields:\nAccess Key Id\nSee this on how to generate an access key.\nSee this on how to create a instanceprofile.\nWe recommend creating an Airbyte-specific user. This user will require read and write permissions to objects in the staging bucket.\nIf the Access Key and Secret Access Key are not provided, the authentication will rely on the instanceprofile.\n\n\nSecret Access Key\nCorresponding key to the above key id.\n\n\nMake sure your S3 bucket is accessible from the machine running Airbyte.\nThis depends on your networking setup.\nYou can check AWS S3 documentation with a tutorial on how to properly configure your S3's access here.\nIf you use instance profile authentication, make sure the role has permission to read/write on the bucket.\nThe easiest way to verify if Airbyte is able to connect to your S3 bucket is via the check connection tool in the UI.\n\n\nS3 Bucket Name\nSee this to create an S3 bucket.\n\n\nS3 Bucket Path\nSubdirectory under the above bucket to sync the data into.\n\n\nS3 Bucket Region\nSee here for all region codes.\n\n\nS3 Path Format\nAdditional string format on how to store data under S3 Bucket Path. Default value is `${NAMESPACE}/${STREAM_NAME}/${YEAR}_${MONTH}_${DAY}_${EPOCH}_`.\n\n\nS3 Endpoint\nLeave empty if using AWS S3, fill in S3 URL if using Minio S3.\n\n\n\n\nS3 Filename pattern\n        * The pattern allows you to set the file-name format for the S3 staging file(s), next placeholders combinations are currently supported: {date}, {date:yyyy_MM}, {timestamp}, {timestamp:millis}, {timestamp:micros}, {part_number}, {sync_id}, {format_extension}. Please, don't use empty space and not supportable placeholders, as they won't recognized.\n\n\n\nClick `Set up destination`.\n\nIn order for everything to work correctly, it is also necessary that the user whose \"S3 Key Id\" and \"S3 Access Key\" are used have access to both the bucket and its contents. Minimum required Policies to use:\n`json\n{\n  \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n             \"Action\": [\n                \"s3:PutObject\",\n                \"s3:GetObject\",\n                \"s3:DeleteObject\",\n                \"s3:PutObjectAcl\",\n                \"s3:ListBucket\",\n                \"s3:ListBucketMultipartUploads\",\n                \"s3:AbortMultipartUpload\",\n                \"s3:GetBucketLocation\"\n             ],\n            \"Resource\": [\n                \"arn:aws:s3:::YOUR_BUCKET_NAME/*\",\n                \"arn:aws:s3:::YOUR_BUCKET_NAME\"\n            ]\n        }\n    ]\n}`\nThe full path of the output data with the default S3 Path Format `${NAMESPACE}/${STREAM_NAME}/${YEAR}_${MONTH}_${DAY}_${EPOCH}_` is:\n`text\n<bucket-name>/<source-namespace-if-exists>/<stream-name>/<upload-date>_<epoch>_<partition-id>.<format-extension>`\nFor example:\n`text\ntesting_bucket/data_output_path/public/users/2021_01_01_1234567890_0.csv.gz\n\u2191              \u2191                \u2191      \u2191     \u2191          \u2191          \u2191 \u2191\n|              |                |      |     |          |          | format extension\n|              |                |      |     |          |          unique incremental part id\n|              |                |      |     |          milliseconds since epoch\n|              |                |      |     upload date in YYYY_MM_DD\n|              |                |      stream name\n|              |                source namespace (if it exists)\n|              bucket path\nbucket name`\nThe rationales behind this naming pattern are:\n\nEach stream has its own directory.\nThe data output files can be sorted by upload time.\nThe upload time composes of a date part and millis part so that it is both readable and unique.\n\nBut it is possible to further customize by using the available variables to format the bucket path:\n- `${NAMESPACE}`: Namespace where the stream comes from or configured by the connection namespace fields.\n- `${STREAM_NAME}`: Name of the stream\n- `${YEAR}`: Year in which the sync was writing the output data in.\n- `${MONTH}`: Month in which the sync was writing the output data in.\n- `${DAY}`: Day in which the sync was writing the output data in.\n- `${HOUR}`: Hour in which the sync was writing the output data in.\n- `${MINUTE}` : Minute in which the sync was writing the output data in.\n- `${SECOND}`: Second in which the sync was writing the output data in.\n- `${MILLISECOND}`: Millisecond in which the sync was writing the output data in.\n- `${EPOCH}`: Milliseconds since Epoch in which the sync was writing the output data in.\n- `${UUID}`: random uuid string\nNote:\n- Multiple `/` characters in the S3 path are collapsed into a single `/` character.\n- If the output bucket contains too many files, the part id variable is using a `UUID` instead. It uses sequential ID otherwise.\nPlease note that the stream name may contain a prefix, if it is configured on the connection.\nA data sync may create multiple files as the output files can be partitioned by size (targeting a size of 200MB compressed or lower) .\nSupported sync modes\n| Feature | Support | Notes |\n| :--- | :---: | :--- |\n| Full Refresh Sync | \u2705 | Warning: this mode deletes all previously synced data in the configured bucket path. |\n| Incremental - Append Sync | \u2705 |  |\n| Incremental - Deduped History | \u274c | As this connector does not support dbt, we don't support this sync mode on this destination. |\n| Namespaces | \u274c | Setting a specific bucket path is equivalent to having separate namespaces. |\nThe Airbyte S3 destination allows you to sync data to AWS S3 or Minio S3. Each stream is written to its own directory under the bucket.\n\u26a0\ufe0f Please note that under \"Full Refresh Sync\" mode, data in the configured bucket and path will be wiped out before each sync. We recommend you to provision a dedicated S3 resource for this sync to prevent unexpected data deletion from misconfiguration. \u26a0\ufe0f\nSupported Output schema\nEach stream will be outputted to its dedicated directory according to the configuration. The complete datastore of each stream includes all the output files under that directory. You can think of the directory as equivalent of a Table in the database world.\n\nUnder Full Refresh Sync mode, old output files will be purged before new files are created.\nUnder Incremental - Append Sync mode, new output files will be added that only contain the new data.\n\nAvro\nApache Avro serializes data in a compact binary format. Currently, the Airbyte S3 Avro connector always uses the binary encoding, and assumes that all data records follow the same schema.\nConfiguration\nHere is the available compression codecs:\n\nNo compression\n`deflate`\nCompression level\nRange `[0, 9]`. Default to 0.\nLevel 0: no compression & fastest.\nLevel 9: best compression & slowest.\n\n\n`bzip2`\n`xz`\nCompression level\nRange `[0, 9]`. Default to 6.\nLevel 0-3 are fast with medium compression.\nLevel 4-6 are fairly slow with high compression.\nLevel 7-9 are like level 6 but use bigger dictionaries and have higher memory requirements. Unless the uncompressed size of the file exceeds 8 MiB, 16 MiB, or 32 MiB, it is waste of memory to use the presets 7, 8, or 9, respectively.\n\n\n`zstandard`\nCompression level\nRange `[-5, 22]`. Default to 3.\nNegative levels are 'fast' modes akin to `lz4` or `snappy`.\nLevels above 9 are generally for archival purposes.\nLevels above 18 use a lot of memory.\n\n\nInclude checksum\nIf set to `true`, a checksum will be included in each data block.\n\n\n`snappy`\n\nData schema\nUnder the hood, an Airbyte data stream in JSON schema is first converted to an Avro schema, then the JSON object is converted to an Avro record. Because the data stream can come from any data source, the JSON to Avro conversion process has arbitrary rules and limitations. Learn more about how source data is converted to Avro and the current limitations here.\nCSV\nLike most of the other Airbyte destination connectors, usually the output has three columns: a UUID, an emission timestamp, and the data blob. With the CSV output, it is possible to normalize (flatten) the data blob to multiple columns.\n| Column | Condition | Description |\n| :--- | :--- | :--- |\n| `_airbyte_ab_id` | Always exists | A uuid assigned by Airbyte to each processed record. |\n| `_airbyte_emitted_at` | Always exists. | A timestamp representing when the event was pulled from the data source. |\n| `_airbyte_data` | When no normalization (flattening) is needed, all data reside under this column as a json blob. |  |\n| root level fields | When root level normalization (flattening) is selected, the root level fields are expanded. |  |\nFor example, given the following json object from a source:\n`json\n{\n  \"user_id\": 123,\n  \"name\": {\n    \"first\": \"John\",\n    \"last\": \"Doe\"\n  }\n}`\nWith no normalization, the output CSV is:\n| `_airbyte_ab_id` | `_airbyte_emitted_at` | `_airbyte_data` |\n| :--- | :--- | :--- |\n| `26d73cde-7eb1-4e1e-b7db-a4c03b4cf206` | 1622135805000 | `{ \"user_id\": 123, name: { \"first\": \"John\", \"last\": \"Doe\" } }` |\nWith root level normalization, the output CSV is:\n| `_airbyte_ab_id` | `_airbyte_emitted_at` | `user_id` | `name` |\n| :--- | :--- | :--- | :--- |\n| `26d73cde-7eb1-4e1e-b7db-a4c03b4cf206` | 1622135805000 | 123 | `{ \"first\": \"John\", \"last\": \"Doe\" }` |\nOutput files can be compressed. The default option is GZIP compression. If compression is selected, the output filename will have an extra extension (GZIP: `.csv.gz`).\nJSON Lines (JSONL)\nJSON Lines is a text format with one JSON per line. Each line has a structure as follows:\n`json\n{\n  \"_airbyte_ab_id\": \"<uuid>\",\n  \"_airbyte_emitted_at\": \"<timestamp-in-millis>\",\n  \"_airbyte_data\": \"<json-data-from-source>\"\n}`\nFor example, given the following two json objects from a source:\n`json\n[\n  {\n    \"user_id\": 123,\n    \"name\": {\n      \"first\": \"John\",\n      \"last\": \"Doe\"\n    }\n  },\n  {\n    \"user_id\": 456,\n    \"name\": {\n      \"first\": \"Jane\",\n      \"last\": \"Roe\"\n    }\n  }\n]`\nThey will be like this in the output file:\n`text\n{ \"_airbyte_ab_id\": \"26d73cde-7eb1-4e1e-b7db-a4c03b4cf206\", \"_airbyte_emitted_at\": \"1622135805000\", \"_airbyte_data\": { \"user_id\": 123, \"name\": { \"first\": \"John\", \"last\": \"Doe\" } } }\n{ \"_airbyte_ab_id\": \"0a61de1b-9cdd-4455-a739-93572c9a5f20\", \"_airbyte_emitted_at\": \"1631948170000\", \"_airbyte_data\": { \"user_id\": 456, \"name\": { \"first\": \"Jane\", \"last\": \"Roe\" } } }`\nOutput files can be compressed. The default option is GZIP compression. If compression is selected, the output filename will have an extra extension (GZIP: `.jsonl.gz`).\nParquet\nConfiguration\nThe following configuration is available to configure the Parquet output:\n| Parameter | Type | Default | Description |\n| :--- | :---: | :---: | :--- |\n| `compression_codec` | enum | `UNCOMPRESSED` | Compression algorithm. Available candidates are: `UNCOMPRESSED`, `SNAPPY`, `GZIP`, `LZO`, `BROTLI`, `LZ4`, and `ZSTD`. |\n| `block_size_mb` | integer | 128 (MB) | Block size (row group size) in MB. This is the size of a row group being buffered in memory. It limits the memory usage when writing. Larger values will improve the IO when reading, but consume more memory when writing. |\n| `max_padding_size_mb` | integer | 8 (MB) | Max padding size in MB. This is the maximum size allowed as padding to align row groups. This is also the minimum size of a row group. |\n| `page_size_kb` | integer | 1024 (KB) | Page size in KB. The page size is for compression. A block is composed of pages. A page is the smallest unit that must be read fully to access a single record. If this value is too small, the compression will deteriorate. |\n| `dictionary_page_size_kb` | integer | 1024 (KB) | Dictionary Page Size in KB. There is one dictionary page per column per row group when dictionary encoding is used. The dictionary page size works like the page size but for dictionary. |\n| `dictionary_encoding` | boolean | `true` | Dictionary encoding. This parameter controls whether dictionary encoding is turned on. |\nThese parameters are related to the `ParquetOutputFormat`. See the Java doc for more details. Also see Parquet documentation for their recommended configurations (512 - 1024 MB block size, 8 KB page size).\nData schema\nUnder the hood, an Airbyte data stream in JSON schema is first converted to an Avro schema, then the JSON object is converted to an Avro record, and finally the Avro record is outputted to the Parquet format. Because the data stream can come from any data source, the JSON to Avro conversion process has arbitrary rules and limitations. Learn more about how source data is converted to Avro and the current limitations here.\nIn order for everything to work correctly, it is also necessary that the user whose \"S3 Key Id\" and \"S3 Access Key\" are used have access to both the bucket and its contents. Policies to use:\n`json\n{\n  \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:*\",\n            \"Resource\": [\n                \"arn:aws:s3:::YOUR_BUCKET_NAME/*\",\n                \"arn:aws:s3:::YOUR_BUCKET_NAME\"\n            ]\n            }\n    ]\n}`",
    "tag": "airbyte"
  },
  {
    "title": "DuckDB ",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/duckdb.md",
    "content": "DuckDB\n:::danger\nThis destination is meant to be used on a local workstation and won't work on Kubernetes\n:::\nOverview\nDuckDB is an in-process SQL OLAP database management system and this destination is meant to use locally if you have multiple smaller sources such as GitHub repos, some social media and local CSVs or files you want to run analytics workloads on.\nThis destination writes data to a file on the local filesystem on the host running Airbyte. By default, data is written to `/tmp/airbyte_local`. To change this location, modify the `LOCAL_ROOT` environment variable for Airbyte.\nSync Overview\nOutput schema\nIf you set Normalization, source data will be normalized to a tabular form. Let's say you have a source such as GitHub with nested JSONs; the Normalization ensures you end up with tables and columns. Suppose you have a many-to-many relationship between the users and commits. Normalization will create separate tables for it. The end state is the third normal form (3NF).\nEach table will contain 3 columns:\n\n`_airbyte_ab_id`: a uuid assigned by Airbyte to each event that is processed.\n`_airbyte_emitted_at`: a timestamp representing when the event was pulled from the data source.\n`_airbyte_data`: a json blob representing with the event data.\n\nFeatures\n| Feature | Supported |  |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental - Append Sync | Yes |  |\n| Incremental - Deduped History | No |  |\n| Namespaces | No |  |\nPerformance consideration\nThis integration will be constrained by the speed at which your filesystem accepts writes.\nGetting Started\nThe `destination_path` will always start with `/local` whether it is specified by the user or not. Any directory nesting within local will be mapped onto the local mount.\nBy default, the `LOCAL_ROOT` env variable in the `.env` file is set `/tmp/airbyte_local`.\nThe local mount is mounted by Docker onto `LOCAL_ROOT`. This means the `/local` is substituted by `/tmp/airbyte_local` by default.\n:::caution\nPlease make sure that Docker Desktop has access to `/tmp` (and `/private` on a MacOS, as /tmp has a symlink that points to /private. It will not work otherwise). You allow it with \"File sharing\" in `Settings -> Resources -> File sharing -> add the one or two above folder` and hit the \"Apply & restart\" button.\n:::\nExample:\n\nIf `destination_path` is set to `/local/destination.duckdb`\nthe local mount is using the `/tmp/airbyte_local` default\nthen all data will be written to `/tmp/airbyte_local/destination.duckdb`.\n\nAccess Replicated Data Files\nIf your Airbyte instance is running on the same computer that you are navigating with, you can open your browser and enter file:///tmp/airbyte_local to look at the replicated data locally. If the first approach fails or if your Airbyte instance is running on a remote server, follow the following steps to access the replicated files:\n\nAccess the scheduler container using `docker exec -it airbyte-server bash`\nNavigate to the default local mount using `cd /tmp/airbyte_local`\nNavigate to the replicated file directory you specified when you created the destination, using `cd /{destination_path}`\nExecute `duckdb {filename}` to access the data in a particular database file.\n\nYou can also copy the output file to your host machine, the following command will copy the file to the current working directory you are using:\n`text\ndocker cp airbyte-server:/tmp/airbyte_local/{destination_path} .`\nNote: If you are running Airbyte on Windows with Docker backed by WSL2, you have to use similar step as above or refer to this link for an alternative approach.",
    "tag": "airbyte"
  },
  {
    "title": "Google Sheets",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/google-sheets.md",
    "content": "Google Sheets\nThe Google Sheets Destination is configured to push data to a single Google Sheets spreadsheet with multiple Worksheets as streams. To replicate data to multiple spreadsheets, you can create multiple instances of the Google Sheets Destination in your Airbyte instance.\nThis page guides you through the process of setting up the Google Sheets destination connector.\nPrerequisites\n\nGoogle Account\nGoogle Spreadsheet URL\n\nStep 1: Set up Google Sheets\nGoogle Account\nIf you don't have a Google Account\nVisit the Google Support and create your Google Account.\nGoogle Sheets (Google Spreadsheets)\n\nOnce you acquire your Google Account, simply open the Google Support to create the fresh empty Google to be used as a destination for your data replication, or if already have one - follow the next step.\nYou will need the link of the Spreadsheet you'd like to sync. To get it, click Share button in the top right corner of Google Sheets interface, and then click Copy Link in the dialog that pops up.\n   These two steps are highlighted in the screenshot below:\n\n\nStep 2: Set up the Google Sheets destination connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Destinations. In the top-right corner, click + new destination.\nOn the source setup page, select Google Sheets from the Source type dropdown and enter a name for this connector.\nSelect `Sign in with Google`.\nLog in and Authorize to the Google account and click `Set up source`.\n\nFor Airbyte Open Source:\nAt this moment the `Google Sheets Destination` works only with Airbyte Cloud.\nOutput schema\nEach worksheet in the selected spreadsheet will be the output as a separate source-connector stream. The data is coerced to string before the output to the spreadsheet. The nested data inside of the source connector data is normalized to the `first-level-nesting` and represented as string, this produces nested lists and objects to be a string rather than normal lists and objects, the further data processing is required if you need to analyze the data.\nAirbyte only supports replicating `Grid Sheets`, which means the text raw data only could be replicated to the target spreadsheet. See the Google Sheets API docs for more info on all available sheet types.\nNote:\n\nThe output columns are ordered alphabetically. The output columns should not be reordered manually after the sync, this could cause the data corruption for all next syncs.\n\nThe underlying process of record normalization is applied to avoid data corruption during the write process. This handles two scenarios:\n\n\nUnderSetting - when record has less keys (columns) than catalog declares\n\nOverSetting - when record has more keys (columns) than catalog declares\n\n```\nEXAMPLE:\n\nUnderSetting:\nCatalog:\nhas 3 entities:\n    [ 'id', 'key1', 'key2' ]\n                ^\n\n\nInput record:\nmissing 1 entity, compare to catalog\n    { 'id': 123,    'key2': 'value' }\n                    ^\n\n\nResult:\n'key1' has been added to the record, because it was declared in catalog, to keep the data structure.\n    {'id': 123, 'key1': '', {'key2': 'value'} }\n                    ^\n\n\n\n\nOverSetting:\nCatalog:\nhas 3 entities:\n    [ 'id', 'key1', 'key2',   ]\n                            ^\n\n\nInput record:\ndoesn't have entity 'key1'\nhas 1 more enitity, compare to catalog 'key3'\n    { 'id': 123,     ,'key2': 'value', 'key3': 'value' }\n                    ^                      ^\n\n\nResult:\n'key1' was added, because it expected be the part of the record, to keep the data structure\n'key3' was dropped, because it was not declared in catalog, to keep the data structure\n    { 'id': 123, 'key1': '', 'key2': 'value',   }\n                    ^                          ^\n```\n\n\n\n\n\nData type mapping\n| Integration Type | Airbyte Type |\n| :--------------- | :----------- |\n| Any Type         | `string`     |\nFeatures & Supported sync modes\n| Feature                        | Supported?(Yes/No) |\n| :----------------------------- | :------------------- |\n| Ful-Refresh Overwrite          | Yes                  |\n| Ful-Refresh Append             | Yes                  |\n| Incremental Append             | Yes                  |\n| Incremental Append-Deduplicate | Yes                  |\nRate Limiting & Performance Considerations\nAt the time of writing, the Google API rate limit is 100 requests per 100 seconds per user and 500 requests per 100 seconds per project. Airbyte batches requests to the API in order to efficiently pull data and respects these rate limits. It is recommended that you use the same service user (see the \"Creating a service user\" section below for more information on how to create one) for no more than 3 instances of the Google Sheets Destination to ensure high transfer speeds.\nPlease be aware of the Google Spreadsheet limitations before you configure your airbyte data replication using Destination Google Sheets\nGoogle Sheets Limitations\nDuring the upload process and from the data storage perspective there are some limitations that should be considered beforehand as determined by Google here:\n\nMaximum of 10 Million Cells\n\nA Google Sheets document can have a maximum of 10 million cells. These can be in a single worksheet or in multiple sheets.\nIn case you already have the 10 million limit reached in fewer columns, it will not allow you to add more columns (and vice versa, i.e., if 10 million cells limit is reached with a certain number of rows, it will not allow more rows).\n\nMaximum of 18,278 Columns\n\nAt max, you can have 18,278 columns in Google Sheets in a worksheet.\n\nUp to 200 Worksheets in a Spreadsheet\n\nYou cannot create more than 200 worksheets within single spreadsheet.\nFuture improvements:\n\nHandle multiple spreadsheets to split big amount of data into parts, once the main spreadsheet is full and cannot be extended more, due to limitations.\n",
    "tag": "airbyte"
  },
  {
    "title": "Databend",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/databend.md",
    "content": "Databend\nThis page guides you through the process of setting up the Databend destination connector.\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental - Append Sync | Yes |  |\nOutput Schema\nEach stream will be output into its own table in Databend. Each table will contain 3 columns:\n\n`_airbyte_ab_id`: a uuid assigned by Airbyte to each event that is processed. The column type in Databend is `VARCHAR`.\n`_airbyte_emitted_at`: a timestamp representing when the event was pulled from the data source. The column type in Databend is `TIMESTAMP`.\n`_airbyte_data`: a json blob representing with the event data. The column type in Databend is `VARVHAR`.\n\nGetting Started (Airbyte Cloud)\nComing soon...\nGetting Started (Airbyte Open-Source)\nYou can follow the Connecting to a Warehouse docs to get the user, password, host etc.\nOr you can create such a user by running:\n`GRANT CREATE ON * TO airbyte_user;`\nMake sure the Databend user with the following permissions:\n\ncan create tables and write rows.\ncan create databases e.g:\n\nYou can also use a pre-existing user but we highly recommend creating a dedicated user for Airbyte.\nTarget Database\nYou will need to choose an existing database or create a new database that will be used to store synced data from Airbyte.\nSetup the Databend Destination in Airbyte\nYou should now have all the requirements needed to configure Databend as a destination in the UI. You'll need the following information to configure the Databend destination:\n\nHost\nPort\nUsername\nPassword\nDatabase\n",
    "tag": "airbyte"
  },
  {
    "title": "DynamoDB",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/dynamodb.md",
    "content": "DynamoDB\nThis destination writes data to AWS DynamoDB.\nThe Airbyte DynamoDB destination allows you to sync data to AWS DynamoDB. Each stream is written to its own table under the DynamoDB.\nPrerequisites\n\nFor Airbyte Open Source users using the Postgres source connector, upgrade your Airbyte platform to version `v0.40.0-alpha` or newer and upgrade your DynamoDB connector to version `0.1.5` or newer\n\nSync overview\nOutput schema\nEach stream will be output into its own DynamoDB table. Each table will a collections of `json` objects containing 4 fields:\n\n`_airbyte_ab_id`: a uuid assigned by Airbyte to each event that is processed.\n`_airbyte_emitted_at`: a timestamp representing when the event was pulled from the data source.\n`_airbyte_data`: a json blob representing with the extracted data.\n`sync_time`: a timestamp representing when the sync up task be triggered.\n\nFeatures\n| Feature | Support | Notes |\n| :--- | :---: | :--- |\n| Full Refresh Sync | \u2705 | Warning: this mode deletes all previously synced data in the configured DynamoDB table. |\n| Incremental - Append Sync | \u2705 |  |\n| Incremental - Deduped History | \u274c | As this connector does not support dbt, we don't support this sync mode on this destination. |\n| Namespaces | \u2705 | Namespace will be used as part of the table name. |\nPerformance considerations\nThis connector by default uses 10 capacity units for both Read and Write in DynamoDB tables. Please provision more capacity units in the DynamoDB console when there are performance constraints.\nGetting started\nRequirements\n\nAllow connections from Airbyte server to your AWS DynamoDB tables (if they exist in separate VPCs).\nThe credentials for AWS DynamoDB (for the COPY strategy).\n\nSetup guide\n\nFill up DynamoDB info\nDynamoDB Endpoint\nLeave empty if using AWS DynamoDB, fill in endpoint URL if using customized endpoint.\n\n\nDynamoDB Table Name\nThe name prefix of the DynamoDB table to store the extracted data. The table name is \\\\.\n\n\nDynamoDB Region\nThe region of the DynamoDB.\n\n\nAccess Key Id\nSee this on how to generate an access key.\nWe recommend creating an Airbyte-specific user. This user will require read and write permissions to the DynamoDB table.\n\n\nSecret Access Key\nCorresponding key to the above key id.\n\n\nMake sure your DynamoDB tables are accessible from the machine running Airbyte.\nThis depends on your networking setup.\nYou can check AWS DynamoDB documentation with a tutorial on how to properly configure your DynamoDB's access here.\nThe easiest way to verify if Airbyte is able to connect to your DynamoDB tables is via the check connection tool in the UI.\n",
    "tag": "airbyte"
  },
  {
    "title": "MSSQL",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/mssql.md",
    "content": "MSSQL\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental - Append Sync | Yes |  |\n| Incremental - Deduped History | No | As this connector does not support dbt, we don't support this sync mode on this destination. |\n| Namespaces | Yes |  |\nOutput Schema\nEach stream will be output into its own table in SQL Server. Each table will contain 3 columns:\n\n`_airbyte_ab_id`: a uuid assigned by Airbyte to each event that is processed. The column type in SQL Server is `VARCHAR(64)`.\n`_airbyte_emitted_at`: a timestamp representing when the event was pulled from the data source. The column type in SQL Server is `DATETIMEOFFSET(7)`.\n`_airbyte_data`: a JSON blob representing with the event data. The column type in SQL Server is `NVARCHAR(MAX)`.\n\nMicrosoft SQL Server specifics or why NVARCHAR type is used here:\n\nNVARCHAR is Unicode - 2 bytes per character, therefore max. of 1 billion characters; will handle East Asian, Arabic, Hebrew, Cyrillic etc. characters just fine.\nVARCHAR is non-Unicode - 1 byte per character, max. capacity is 2 billion characters, but limited to the character set you're SQL Server is using, basically - no support for those languages mentioned before\n\nGetting Started (Airbyte Cloud)\nAirbyte Cloud only supports connecting to your MSSQL instance with TLS encryption. Other than that, you can proceed with the open-source instructions below.\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental - Append Sync | Yes |  |\n| Incremental - Deduped History | Yes |  |\n| Namespaces | Yes |  |\nGetting Started (Airbyte Open-Source)\nRequirements\nTo use the SQL Server destination, you'll need:\nMS SQL Server: `Azure SQL Database`, `Azure Synapse Analytics`, `Azure SQL Managed Instance`, `SQL Server 2019`, `SQL Server 2017`, `SQL Server 2016`, `SQL Server 2014`, `SQL Server 2012`, or `PDW 2008R2 AU34`.\nNormalization Requirements\nTo sync with normalization you'll need to use MS SQL Server of the following versions: `SQL Server 2019`, `SQL Server 2017`, `SQL Server 2016`, `SQL Server 2014`. The work of normalization on `SQL Server 2012` and bellow are not guaranteed.\nSetup guide\n\nMS SQL Server: `Azure SQL Database`, `Azure Synapse Analytics`, `Azure SQL Managed Instance`, `SQL Server 2019`, `SQL Server 2017`, `SQL Server 2016`, `SQL Server 2014`, `SQL Server 2012`, or `PDW 2008R2 AU34`.\n\nNetwork Access\nMake sure your SQL Server database can be accessed by Airbyte. If your database is within a VPC, you may need to allow access from the IP you're using to expose Airbyte.\nPermissions\nYou need a user configured in SQL Server that can create tables and write rows. We highly recommend creating an Airbyte-specific user for this purpose.\nIn order to allow for normalization, please grant ALTER permissions for the user configured. \nTarget Database\nYou will need to choose an existing database or create a new database that will be used to store synced data from Airbyte.\nSSL configuration (optional)\nAirbyte supports a SSL-encrypted connection to the database. If you want to use SSL to securely access your database, ensure that the server is configured to use an SSL certificate.\nSetup the MSSQL destination in Airbyte\nYou should now have all the requirements needed to configure SQL Server as a destination in the UI. You'll need the following information to configure the MSSQL destination:\n\nHost\nPort\nUsername\nPassword\nSchema\nDatabase\nThis database needs to exist within the schema provided.\nSSL Method:\nThe SSL configuration supports three modes: Unencrypted, Encrypted (trust server certificate), and Encrypted (verify certificate).\nUnencrypted: Do not use SSL encryption on the database connection\nEncrypted (trust server certificate): Use SSL encryption without verifying the server's certificate.  This is useful for self-signed certificates in testing scenarios, but should not be used in production.\nEncrypted (verify certificate): Use the server's SSL certificate, after standard certificate verification.\n\n\nHost Name In Certificate (optional): When using certificate verification, this property can be set to specify an expected name for added security.  If this value is present, and the server's certificate's host name does not match it, certificate verification will fail.\n\nConnection via SSH Tunnel\nAirbyte has the ability to connect to the MS SQL Server instance via an SSH Tunnel. The reason you might want to do this because it is not possible (or against security policy) to connect to the database directly (e.g. it does not have a public IP address).\nWhen using an SSH tunnel, you are configuring Airbyte to connect to an intermediate server (a.k.a. a bastion sever) that have direct access to the database. Airbyte connects to the bastion and then asks the bastion to connect directly to the server.\nUsing this feature requires additional configuration, when creating the source. We will talk through what each piece of configuration means.\n\nConfigure all fields for the source as you normally would, except `SSH Tunnel Method`.\n`SSH Tunnel Method` defaults to `No Tunnel` (meaning a direct connection). If you want to use an SSH Tunnel choose `SSH Key Authentication` or `Password Authentication`.\nChoose `Key Authentication` if you will be using an RSA private key as your secret for establishing the SSH Tunnel (see below for more information on generating this key).\nChoose `Password Authentication` if you will be using a password as your secret for establishing the SSH Tunnel.\n`SSH Tunnel Jump Server Host` refers to the intermediate (bastion) server that Airbyte will connect to. This should be a hostname or an IP Address.\n`SSH Connection Port` is the port on the bastion server with which to make the SSH connection. The default port for SSH connections is `22`,\n\nso unless you have explicitly changed something, go with the default.\n\n`SSH Login Username` is the username that Airbyte should use when connection to the bastion server. This is NOT the MS SQL Server username.\nIf you are using `Password Authentication`, then `SSH Login Username` should be set to the password of the User from the previous step.\n\nIf you are using `SSH Key Authentication` leave this blank. Again, this is not the MS SQL Server password, but the password for the OS-user that\nAirbyte is using to perform commands on the bastion.\n\nIf you are using `SSH Key Authentication`, then `SSH Private Key` should be set to the RSA Private Key that you are using to create the SSH connection.\n\nThis should be the full contents of the key file starting with `-----BEGIN RSA PRIVATE KEY-----` and ending with `-----END RSA PRIVATE KEY-----`.",
    "tag": "airbyte"
  },
  {
    "title": "ClickHouse",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/clickhouse.md",
    "content": "ClickHouse\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental - Append Sync | Yes |  |\n| Incremental - Deduped History | Yes |  |\n| Namespaces | Yes |  |\nOutput Schema\nEach stream will be output into its own table in ClickHouse. Each table will contain 3 columns:\n\n`_airbyte_ab_id`: a uuid assigned by Airbyte to each event that is processed. The column type in ClickHouse is `String`.\n`_airbyte_emitted_at`: a timestamp representing when the event was pulled from the data source. The column type in ClickHouse is `DateTime64`.\n`_airbyte_data`: a json blob representing with the event data. The column type in ClickHouse is `String`.\n\nGetting Started (Airbyte Cloud)\nAirbyte Cloud only supports connecting to your ClickHouse instance with SSL or TLS encryption, which is supported by ClickHouse JDBC driver.\nGetting Started (Airbyte Open-Source)\nRequirements\nTo use the ClickHouse destination, you'll need:\n\nA ClickHouse server version 21.8.10.19 or above\n\nConfigure Network Access\nMake sure your ClickHouse database can be accessed by Airbyte. If your database is within a VPC, you may need to allow access from the IP you're using to expose Airbyte.\nPermissions\nYou need a ClickHouse user with the following permissions:\n\ncan create tables and write rows.\ncan create databases e.g:\n\nYou can create such a user by running:\n`GRANT CREATE ON * TO airbyte_user;`\nYou can also use a pre-existing user but we highly recommend creating a dedicated user for Airbyte.\nTarget Database\nYou will need to choose an existing database or create a new database that will be used to store synced data from Airbyte.\nSetup the ClickHouse Destination in Airbyte\nYou should now have all the requirements needed to configure ClickHouse as a destination in the UI. You'll need the following information to configure the ClickHouse destination:\n\nHost\nPort\nUsername\nPassword\nDatabase\nJdbc_url_params\n\nNaming Conventions\nFrom ClickHouse SQL Identifiers syntax:\n\nSQL identifiers and key words must begin with a letter (a-z, but also letters with diacritical marks and non-Latin letters) or an underscore (_).\nSubsequent characters in an identifier or key word can be letters, underscores, digits (0-9).\nIdentifiers can be quoted or non-quoted. The latter is preferred.\nIf you want to use identifiers the same as keywords or you want to use other symbols in identifiers, quote it using double quotes or backticks, for example, \"id\", `id`.\nIf you want to write portable applications you are advised to always quote a particular name or never quote it.\n\nTherefore, Airbyte ClickHouse destination will create tables and schemas using the Unquoted identifiers when possible or fallback to Quoted Identifiers if the names are containing special characters.",
    "tag": "airbyte"
  },
  {
    "title": "TiDB",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/tidb.md",
    "content": "TiDB\nTiDB (/\u2019ta\u026adi\u02d0bi:/, \"Ti\" stands for Titanium) is an open-source, distributed, NewSQL database that supports Hybrid Transactional and Analytical Processing (HTAP) workloads. It is MySQL compatible and features horizontal scalability, strong consistency, and high availability. Now, everyone can take a free dev trial on TiDB Cloud.\nThis page guides you through the process of setting up the TiDB destination connector.\nFeatures\n| Feature                       | Supported?(Yes/No) | Notes |\n|:------------------------------|:---------------------|:------|\n| Full Refresh Sync             | Yes                  |       |\n| Incremental - Append Sync     | Yes                  |       |\n| Incremental - Deduped History | Yes                  |       |\n| Namespaces                    | Yes                  |       |\n| SSH Tunnel Connection         | Yes                  |       |\nOutput Schema\nEach stream will be output into its own table in TiDB. Each table will contain 3 columns:\n\n`_airbyte_ab_id`: a uuid assigned by Airbyte to each event that is processed. The column type in TiDB is `VARCHAR(256)`.\n`_airbyte_emitted_at`: a timestamp representing when the event was pulled from the data source. The column type in TiDB is `TIMESTAMP(6)`.\n`_airbyte_data`: a json blob representing with the event data. The column type in TiDB is `JSON`.\n\nGetting Started\nRequirements\nTo use the TiDB destination, you'll need:\n\nTo sync data to TiDB with normalization you should have a TiDB database v5.4.0 or above.\n\nNetwork Access\nMake sure your TiDB database can be accessed by Airbyte. If your database is within a VPC, you may need to allow access from the IP you're using to expose Airbyte.\nPermissions\nYou need a user with `CREATE, INSERT, SELECT, DROP, CREATE VIEW, ALTER` permissions. We highly recommend creating an Airbyte-specific user for this purpose.\nTo create a dedicated database user, run the following commands against your database:\n`sql\nCREATE USER 'airbyte'@'%' IDENTIFIED BY 'your_password_here';`\nThen give it access to the relevant database:\n`sql\nGRANT CREATE, INSERT, SELECT, DROP, CREATE VIEW, ALTER ON <database name>.* TO 'airbyte'@'%';`\nTarget Database\nTiDB doesn't differentiate between a database and schema. A database is essentially a schema where all the tables live in. You will need to choose an existing database or create a new database. This will act as a default database/schema where the tables will be created if the source doesn't provide a namespace.\nSetup the TiDB destination in Airbyte\nConfig the following information in the TiDB destination:\n\nHost\nPort\nUsername\nPassword\nDatabase\njdbc_url_params (Optional)\n\nNote: When connecting to TiDB Cloud with TLS enabled, you need to specify TLS protocol, such as `enabledTLSProtocols=TLSv1.2` or `enabledTLSProtocols=TLSv1.3` in the JDBC parameters.\nDefault JDBC URL Parameters\n\n`useSSL=false` (unless `ssl` is set to true)\n`requireSSL=false` (unless `ssl` is set to true)\n`verifyServerCertificate=false` (unless `ssl` is set to true)\n\nKnown Limitations\nTiDB destination forces all identifier (table, schema and columns) names to be lowercase.\nConnection via SSH Tunnel\nAirbyte has the ability to connect to a TiDB instance via an SSH Tunnel. The reason you might want to do this because it is not possible (or against security policy) to connect to the database directly (e.g. it does not have a public IP address).\nWhen using an SSH tunnel, you are configuring Airbyte to connect to an intermediate server (a.k.a. a bastion sever) that does have direct access to the database. Airbyte connects to the bastion and then asks the bastion to connect directly to the server.\nUsing this feature requires additional configuration, when creating the destination. We will talk through what each piece of configuration means.\n\nConfigure all fields for the destination as you normally would, except `SSH Tunnel Method`.\n`SSH Tunnel Method` defaults to `No Tunnel` (meaning a direct connection). If you want to use an SSH Tunnel choose `SSH Key Authentication` or `Password Authentication`.\nChoose `Key Authentication` if you will be using an RSA private key as your secret for establishing the SSH Tunnel (see below for more information on generating this key).\nChoose `Password Authentication` if you will be using a password as your secret for establishing the SSH Tunnel.\n\n\n`SSH Tunnel Jump Server Host` refers to the intermediate (bastion) server that Airbyte will connect to. This should be a hostname or an IP Address.\n`SSH Connection Port` is the port on the bastion server with which to make the SSH connection. The default port for SSH connections is `22`, so unless you have explicitly changed something, go with the default.\n`SSH Login Username` is the username that Airbyte should use when connection to the bastion server. This is NOT the TiDB username.\nIf you are using `Password Authentication`, then `SSH Login Username` should be set to the password of the User from the previous step. If you are using `SSH Key Authentication` leave this blank. Again, this is not the TiDB password, but the password for the OS-user that Airbyte is using to perform commands on the bastion.\nIf you are using `SSH Key Authentication`, then `SSH Private Key` should be set to the RSA Private Key that you are using to create the SSH connection. This should be the full contents of the key file starting with `-----BEGIN RSA PRIVATE KEY-----` and ending with `-----END RSA PRIVATE KEY-----`.\n",
    "tag": "airbyte"
  },
  {
    "title": "Google Cloud Storage (GCS)",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/gcs.md",
    "content": "Google Cloud Storage (GCS)\nOverview\nThis destination writes data to GCS bucket.\nThe Airbyte GCS destination allows you to sync data to cloud storage buckets. Each stream is written to its own directory under the bucket.\nSync overview\nFeatures\n| Feature | Support | Notes |\n| :--- | :---: | :--- |\n| Full Refresh Sync | \u2705 | Warning: this mode deletes all previously synced data in the configured bucket path. |\n| Incremental - Append Sync | \u2705 |  |\n| Incremental - Deduped History | \u274c | As this connector does not support dbt, we don't support this sync mode on this destination. |\n| Namespaces | \u274c | Setting a specific bucket path is equivalent to having separate namespaces. |\nConfiguration\n| Parameter | Type | Notes |\n| :--- | :---: | :--- |\n| GCS Bucket Name | string | Name of the bucket to sync data into. |\n| GCS Bucket Path | string | Subdirectory under the above bucket to sync the data into. |\n| GCS Region | string | See here for all region codes. |\n| HMAC Key Access ID | string | HMAC key access ID . The access ID for the GCS bucket. When linked to a service account, this ID is 61 characters long; when linked to a user account, it is 24 characters long. See HMAC key for details. |\n| HMAC Key Secret | string | The corresponding secret for the access ID. It is a 40-character base-64 encoded string. |\n| Format | object | Format specific configuration. See below for details. |\n| Part Size | integer | Arg to configure a block size. Max allowed blocks by GCS = 10,000, i.e. max stream size = blockSize * 10,000 blocks. |\nCurrently, only the HMAC key is supported. More credential types will be added in the future, please submit an issue with your request.\nAdditionally, your bucket must be encrypted using a Google-managed encryption key (this is the default setting when creating a new bucket). We currently do not support buckets using customer-managed encryption keys (CMEK). You can view this setting under the \"Configuration\" tab of your GCS bucket, in the `Encryption type` row.\n\u26a0\ufe0f Please note that under \"Full Refresh Sync\" mode, data in the configured bucket and path will be wiped out before each sync. We recommend you to provision a dedicated S3 resource for this sync to prevent unexpected data deletion from misconfiguration. \u26a0\ufe0f\nThe full path of the output data is:\n`text\n<bucket-name>/<sorce-namespace-if-exists>/<stream-name>/<upload-date>-<upload-mills>-<partition-id>.<format-extension>`\nFor example:\n`text\ntesting_bucket/data_output_path/public/users/2021_01_01_1609541171643_0.csv.gz\n\u2191              \u2191                \u2191      \u2191     \u2191          \u2191             \u2191 \u2191\n|              |                |      |     |          |             | format extension\n|              |                |      |     |          |             partition id\n|              |                |      |     |          upload time in millis\n|              |                |      |     upload date in YYYY-MM-DD\n|              |                |      stream name\n|              |                source namespace (if it exists)\n|              bucket path\nbucket name`\nPlease note that the stream name may contain a prefix, if it is configured on the connection.\nThe rationales behind this naming pattern are: 1. Each stream has its own directory. 2. The data output files can be sorted by upload time. 3. The upload time composes of a date part and millis part so that it is both readable and unique.\nA data sync may create multiple files as the output files can be partitioned by size (targeting a size of 200MB compressed or lower) .\nOutput Schema\nEach stream will be outputted to its dedicated directory according to the configuration. The complete datastore of each stream includes all the output files under that directory. You can think of the directory as equivalent of a Table in the database world.\n\nUnder Full Refresh Sync mode, old output files will be purged before new files are created.\nUnder Incremental - Append Sync mode, new output files will be added that only contain the new data.\n\nAvro\nApache Avro serializes data in a compact binary format. Currently, the Airbyte S3 Avro connector always uses the binary encoding, and assumes that all data records follow the same schema.\nConfiguration\nHere is the available compression codecs:\n\nNo compression\n`deflate`\nCompression level\nRange `[0, 9]`. Default to 0.\nLevel 0: no compression & fastest.\nLevel 9: best compression & slowest.\n\n\n`bzip2`\n`xz`\nCompression level\nRange `[0, 9]`. Default to 6.\nLevel 0-3 are fast with medium compression.\nLevel 4-6 are fairly slow with high compression.\nLevel 7-9 are like level 6 but use bigger dictionaries and have higher memory requirements. Unless the uncompressed size of the file exceeds 8 MiB, 16 MiB, or 32 MiB, it is waste of memory to use the presets 7, 8, or 9, respectively.\n\n\n`zstandard`\nCompression level\nRange `[-5, 22]`. Default to 3.\nNegative levels are 'fast' modes akin to `lz4` or `snappy`.\nLevels above 9 are generally for archival purposes.\nLevels above 18 use a lot of memory.\n\n\nInclude checksum\nIf set to `true`, a checksum will be included in each data block.\n\n\n`snappy`\n\nData schema\nUnder the hood, an Airbyte data stream in Json schema is first converted to an Avro schema, then the Json object is converted to an Avro record. Because the data stream can come from any data source, the Json to Avro conversion process has arbitrary rules and limitations. Learn more about how source data is converted to Avro and the current limitations here.\nCSV\nLike most of the other Airbyte destination connectors, usually the output has three columns: a UUID, an emission timestamp, and the data blob. With the CSV output, it is possible to normalize (flatten) the data blob to multiple columns.\n| Column | Condition | Description |\n| :--- | :--- | :--- |\n| `_airbyte_ab_id` | Always exists | A uuid assigned by Airbyte to each processed record. |\n| `_airbyte_emitted_at` | Always exists. | A timestamp representing when the event was pulled from the data source. |\n| `_airbyte_data` | When no normalization (flattening) is needed, all data reside under this column as a json blob. |  |\n| root level fields | When root level normalization (flattening) is selected, the root level fields are expanded. |  |\nFor example, given the following json object from a source:\n`json\n{\n  \"user_id\": 123,\n  \"name\": {\n    \"first\": \"John\",\n    \"last\": \"Doe\"\n  }\n}`\nWith no normalization, the output CSV is:\n| `_airbyte_ab_id` | `_airbyte_emitted_at` | `_airbyte_data` |\n| :--- | :--- | :--- |\n| `26d73cde-7eb1-4e1e-b7db-a4c03b4cf206` | 1622135805000 | `{ \"user_id\": 123, name: { \"first\": \"John\", \"last\": \"Doe\" } }` |\nWith root level normalization, the output CSV is:\n| `_airbyte_ab_id` | `_airbyte_emitted_at` | `user_id` | `name` |\n| :--- | :--- | :--- | :--- |\n| `26d73cde-7eb1-4e1e-b7db-a4c03b4cf206` | 1622135805000 | 123 | `{ \"first\": \"John\", \"last\": \"Doe\" }` |\nOutput files can be compressed. The default option is GZIP compression. If compression is selected, the output filename will have an extra extension (GZIP: `.csv.gz`).\nJSON Lines (JSONL)\nJson Lines is a text format with one JSON per line. Each line has a structure as follows:\n`json\n{\n  \"_airbyte_ab_id\": \"<uuid>\",\n  \"_airbyte_emitted_at\": \"<timestamp-in-millis>\",\n  \"_airbyte_data\": \"<json-data-from-source>\"\n}`\nFor example, given the following two json objects from a source:\n`json\n[\n  {\n    \"user_id\": 123,\n    \"name\": {\n      \"first\": \"John\",\n      \"last\": \"Doe\"\n    }\n  },\n  {\n    \"user_id\": 456,\n    \"name\": {\n      \"first\": \"Jane\",\n      \"last\": \"Roe\"\n    }\n  }\n]`\nThey will be like this in the output file:\n`text\n{ \"_airbyte_ab_id\": \"26d73cde-7eb1-4e1e-b7db-a4c03b4cf206\", \"_airbyte_emitted_at\": \"1622135805000\", \"_airbyte_data\": { \"user_id\": 123, \"name\": { \"first\": \"John\", \"last\": \"Doe\" } } }\n{ \"_airbyte_ab_id\": \"0a61de1b-9cdd-4455-a739-93572c9a5f20\", \"_airbyte_emitted_at\": \"1631948170000\", \"_airbyte_data\": { \"user_id\": 456, \"name\": { \"first\": \"Jane\", \"last\": \"Roe\" } } }`\nOutput files can be compressed. The default option is GZIP compression. If compression is selected, the output filename will have an extra extension (GZIP: `.jsonl.gz`).\nParquet\nConfiguration\nThe following configuration is available to configure the Parquet output:\n| Parameter | Type | Default | Description |\n| :--- | :---: | :---: | :--- |\n| `compression_codec` | enum | `UNCOMPRESSED` | Compression algorithm. Available candidates are: `UNCOMPRESSED`, `SNAPPY`, `GZIP`, `LZO`, `BROTLI`, `LZ4`, and `ZSTD`. |\n| `block_size_mb` | integer | 128 (MB) | Block size (row group size) in MB. This is the size of a row group being buffered in memory. It limits the memory usage when writing. Larger values will improve the IO when reading, but consume more memory when writing. |\n| `max_padding_size_mb` | integer | 8 (MB) | Max padding size in MB. This is the maximum size allowed as padding to align row groups. This is also the minimum size of a row group. |\n| `page_size_kb` | integer | 1024 (KB) | Page size in KB. The page size is for compression. A block is composed of pages. A page is the smallest unit that must be read fully to access a single record. If this value is too small, the compression will deteriorate. |\n| `dictionary_page_size_kb` | integer | 1024 (KB) | Dictionary Page Size in KB. There is one dictionary page per column per row group when dictionary encoding is used. The dictionary page size works like the page size but for dictionary. |\n| `dictionary_encoding` | boolean | `true` | Dictionary encoding. This parameter controls whether dictionary encoding is turned on. |\nThese parameters are related to the `ParquetOutputFormat`. See the Java doc for more details. Also see Parquet documentation for their recommended configurations (512 - 1024 MB block size, 8 KB page size).\nData schema\nUnder the hood, an Airbyte data stream in Json schema is first converted to an Avro schema, then the Json object is converted to an Avro record, and finally the Avro record is outputted to the Parquet format. Because the data stream can come from any data source, the Json to Avro conversion process has arbitrary rules and limitations. Learn more about how source data is converted to Avro and the current limitations here.\nGetting started\nRequirements\n\nAllow connections from Airbyte server to your GCS cluster (if they exist in separate VPCs).\nAn GCP bucket with credentials (for the COPY strategy).\n\nSetup guide\n\nFill up GCS info\nGCS Bucket Name\nSee this for instructions on how to create a GCS bucket. The bucket cannot have a retention policy. Set Protection Tools to none or Object versioning.\n\n\nGCS Bucket Region\nHMAC Key Access ID\nSee this on how to generate an access key. For more information on hmac keys please reference the GCP docs\nWe recommend creating an Airbyte-specific user or service account. This user or account will require the following permissions for the bucket:\n  `storage.multipartUploads.abort\n  storage.multipartUploads.create\n  storage.objects.create\n  storage.objects.delete\n  storage.objects.get\n  storage.objects.list`\n  You can set those by going to the permissions tab in the GCS bucket and adding the appropriate the email address of the service account or user and adding the aforementioned permissions.\n\n\nSecret Access Key\nCorresponding key to the above access ID.\n\n\nMake sure your GCS bucket is accessible from the machine running Airbyte. This depends on your networking setup. The easiest way to verify if Airbyte is able to connect to your GCS bucket is via the check connection tool in the UI.\n",
    "tag": "airbyte"
  },
  {
    "title": "Redshift",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/redshift.md",
    "content": "Redshift\nThis page guides you through the process of setting up the Redshift destination connector.\nPrerequisites\nThe Airbyte Redshift destination allows you to sync data to Redshift.\nThis Redshift destination connector has two replication strategies:\n\nINSERT: Replicates data via SQL INSERT queries. This is built on top of the destination-jdbc code base and is configured to rely on JDBC 4.2 standard drivers provided by Amazon via Mulesoft here as described in Redshift documentation here. Not recommended for production workloads as this does not scale well.\n\nFor INSERT strategy:\n* Host\n* Port\n* Username\n* Password\n* Schema\n* Database\n    * This database needs to exist within the cluster provided.\n* JDBC URL Params (optional)\n\nCOPY: Replicates data by first uploading data to an S3 bucket and issuing a COPY command. This is the recommended loading approach described by Redshift best practices. Requires an S3 bucket and credentials.\n\nAirbyte automatically picks an approach depending on the given configuration - if S3 configuration is present, Airbyte will use the COPY strategy and vice versa.\nFor COPY strategy:\n\nS3 Bucket Name\nSee this to create an S3 bucket.\n\n\nS3 Bucket Region\nPlace the S3 bucket and the Redshift cluster in the same region to save on networking costs.\n\n\nAccess Key Id\nSee this on how to generate an access key.\nWe recommend creating an Airbyte-specific user. This user will require read and write permissions to objects in the staging bucket.\n\n\nSecret Access Key\nCorresponding key to the above key id.\n\n\nPart Size\nAffects the size limit of an individual Redshift table. Optional. Increase this if syncing tables larger than 100GB. Files are streamed to S3 in parts. This determines the size of each part, in MBs. As S3 has a limit of 10,000 parts per file, part size affects the table size. This is 10MB by default, resulting in a default table limit of 100GB. Note, a larger part size will result in larger memory requirements. A rule of thumb is to multiply the part size by 10 to get the memory requirement. Modify this with care.\n\n\nS3 Filename pattern\nThe pattern allows you to set the file-name format for the S3 staging file(s), next placeholders combinations are currently supported: {date}, {date:yyyy_MM}, {timestamp}, {timestamp:millis}, {timestamp:micros}, {part_number}, {sync_id}, {format_extension}. Please, don't use empty space and not supportable placeholders, as they won't recognized.\n\n\n\nOptional parameters:\n* Bucket Path\n    * The directory within the S3 bucket to place the staging data. For example, if you set this to `yourFavoriteSubdirectory`, we will place the staging data inside `s3://yourBucket/yourFavoriteSubdirectory`. If not provided, defaults to the root directory.\n* Purge Staging Data\n    * Whether to delete the staging files from S3 after completing the sync. Specifically, the connector will create CSV files named `bucketPath/namespace/streamName/syncDate_epochMillis_randomUuid.csv` containing three columns (`ab_id`, `data`, `emitted_at`). Normally these files are deleted after the `COPY` command completes; if you want to keep them for other purposes, set `purge_staging_data` to `false`.\nStep 1: Set up Redshift\n\nLog in to AWS Management console.\n   If you don't have a AWS account already, you\u2019ll need to create one in order to use the API.\nGo to the AWS Redshift service\nCreate and activate AWS Redshift cluster if you don't have one ready\n(Optional) Allow connections from Airbyte to your Redshift cluster (if they exist in separate VPCs)\n(Optional) Create a staging S3 bucket (for the COPY strategy).\n\nStep 2: Set up the destination connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Destinations. In the top-right corner, click + new destination.\nOn the destination setup page, select Redshift from the Destination type dropdown and enter a name for this connector.\nFill in all the required fields to use the INSERT or COPY strategy\nClick `Set up destination`.\n\nFor Airbyte Open Source:\n\nGo to local Airbyte page.\nIn the left navigation bar, click Destinations. In the top-right corner, click + new destination.\nOn the destination setup page, select Redshift from the Destination type dropdown and enter a name for this connector.\nFill in all the required fields to use the INSERT or COPY strategy\nClick `Set up destination`.\n\nSupported sync modes\nThe Redshift destination connector supports the following sync modes:\n- Full Refresh\n- Incremental - Append Sync\n- Incremental - Deduped History\nPerformance considerations\nSynchronization performance depends on the amount of data to be transferred.\nCluster scaling issues can be resolved directly using the cluster settings in the AWS Redshift console\nConnector-specific features & highlights\nNotes about Redshift Naming Conventions\nFrom Redshift Names & Identifiers:\nStandard Identifiers\n\nBegin with an ASCII single-byte alphabetic character or underscore character, or a UTF-8 multibyte character two to four bytes long.\nSubsequent characters can be ASCII single-byte alphanumeric characters, underscores, or dollar signs, or UTF-8 multibyte characters two to four bytes long.\nBe between 1 and 127 bytes in length, not including quotation marks for delimited identifiers.\nContain no quotation marks and no spaces.\n\nDelimited Identifiers\nDelimited identifiers (also known as quoted identifiers) begin and end with double quotation marks (\"). If you use a delimited identifier, you must use the double quotation marks for every reference to that object. The identifier can contain any standard UTF-8 printable characters other than the double quotation mark itself. Therefore, you can create column or table names that include otherwise illegal characters, such as spaces or the percent symbol. ASCII letters in delimited identifiers are case-insensitive and are folded to lowercase. To use a double quotation mark in a string, you must precede it with another double quotation mark character.\nTherefore, Airbyte Redshift destination will create tables and schemas using the Unquoted identifiers when possible or fallback to Quoted Identifiers if the names are containing special characters.\nData Size Limitations\nRedshift specifies a maximum limit of 1MB (and 65535 bytes for any VARCHAR fields within the JSON record) to store the raw JSON record data. Thus, when a row is too big to fit, the Redshift destination fails to load such data and currently ignores that record.\nSee docs for SUPER and SUPER limitations\nEncryption\nAll Redshift connections are encrypted using SSL\nOutput schema\nEach stream will be output into its own raw table in Redshift. Each table will contain 3 columns:\n\n`_airbyte_ab_id`: a uuid assigned by Airbyte to each event that is processed. The column type in Redshift is `VARCHAR`.\n`_airbyte_emitted_at`: a timestamp representing when the event was pulled from the data source. The column type in Redshift is `TIMESTAMP WITH TIME ZONE`.\n`_airbyte_data`: a json blob representing with the event data. The column type in Redshift is `VARCHAR` but can be be parsed with JSON functions.\n\nData type mapping\n| Redshift Type | Airbyte Type | Notes |\n| :--- | :--- | :--- |\n| `boolean` | `boolean` | |\n| `int` | `integer` | |\n| `float` | `number` | |\n| `varchar` | `string` | |\n| `date/varchar` | `date` | |\n| `time/varchar` | `time` | |\n| `timestamptz/varchar` | `timestamp_with_timezone` | |\n| `varchar` | `array` | |\n| `varchar` | `object` | |",
    "tag": "airbyte"
  },
  {
    "title": "Firebolt",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/firebolt.md",
    "content": "Firebolt\nThis page guides you through the process of setting up the Firebolt destination connector.\nPrerequisites\nThis Firebolt destination connector has two replication strategies:\n\n\nSQL: Replicates data via SQL INSERT queries. This leverages Firebolt SDK to execute queries directly on Firebolt Engines. Not recommended for production workloads as this does not scale well.\n\n\nS3: Replicates data by first uploading data to an S3 bucket, creating an External Table and writing into a final Fact Table. This is the recommended loading approach. Requires an S3 bucket and credentials in addition to Firebolt credentials.\n\n\nFor SQL strategy:\n* Host\n* Username\n* Password\n* Database\n* Engine (optional)\nAirbyte automatically picks an approach depending on the given configuration - if S3 configuration is present, Airbyte will use the S3 strategy.\nFor S3 strategy:\n\nUsername\nPassword\nDatabase\nS3 Bucket Name\nSee this to create an S3 bucket.\n\n\nS3 Bucket Region\nCreate the S3 bucket on the same region as the Firebolt database.\n\n\nAccess Key Id\nSee this on how to generate an access key.\nWe recommend creating an Airbyte-specific user. This user will require read, write and delete permissions to objects in the staging bucket.\n\n\nSecret Access Key\nCorresponding key to the above key id.\n\n\nHost (optional)\nFirebolt backend URL. Can be left blank for most usecases.\n\n\nEngine (optional)\nIf connecting to a non-default engine you should specify its name or url here.\n\n\n\nSetup guide\n\nCreate a Firebolt account following the guide\nFollow the getting started tutorial to setup a database.\nCreate a General Purpose (read-write) engine as described in here\n(Optional) Create a staging S3 bucket (for the S3 strategy).\n(Optional) Create an IAM with programmatic access to read, write and delete objects from an S3 bucket.\n\nSupported sync modes\nThe Firebolt destination connector supports the following sync modes:\n- Full Refresh\n- Incremental - Append Sync\nConnector-specific features & highlights\nOutput schema\nEach stream will be output into its own raw Fact table in Firebolt. Each table will contain 3 columns:\n\n`_airbyte_ab_id`: a uuid assigned by Airbyte to each event that is processed. The column type in Firebolt is `VARCHAR`.\n`_airbyte_emitted_at`: a timestamp representing when the event was pulled from the data source. The column type in Firebolt is `TIMESTAMP`.\n`_airbyte_data`: a json blob representing the event data. The column type in Firebolt is `VARCHAR` but can be be parsed with JSON functions.\n",
    "tag": "airbyte"
  },
  {
    "title": "Cassandra",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/cassandra.md",
    "content": "Cassandra\nPrerequisites\n\nFor Airbyte Open Source users using the Postgres source connector, upgrade your Airbyte platform to version `v0.40.0-alpha` or newer and upgrade your Cassandra connector to version `0.1.3` or newer\n\nSync overview\nOutput schema\nThe incoming airbyte data is structured in keyspaces and tables and is partitioned and replicated across different nodes\nin the cluster. This connector maps an incoming `stream` to a Cassandra `table` and a `namespace` to a\nCassandra`keyspace`. Fields in the airbyte message become different columns in the Cassandra tables. Each table will\ncontain the following columns.\n\n`_airbyte_ab_id`: A random uuid generator to be used as a partition key.\n`_airbyte_emitted_at`: a timestamp representing when the event was received from the data source.\n`_airbyte_data`: a json text representing the extracted data.\n\nFeatures\n| Feature                       | Support | Notes                                                                                        |\n| :---------------------------- | :-----: | :------------------------------------------------------------------------------------------- |\n| Full Refresh Sync             |   \u2705    | Warning: this mode deletes all previously synced data in the configured DynamoDB table.      |\n| Incremental - Append Sync     |   \u2705    |                                                                                              |\n| Incremental - Deduped History |   \u274c    | As this connector does not support dbt, we don't support this sync mode on this destination. |\n| Namespaces                    |   \u2705    | Namespace will be used as part of the table name.                                            |\nPerformance considerations\nCassandra is designed to handle large amounts of data by using different nodes in the cluster in order to perform write\noperations. As long as you have enough nodes in the cluster the database can scale infinitely and handle any amount of\ndata from the connector.\nGetting started\nRequirements\n\nThe driver is compatible with Cassandra >= 2.1\nConfiguration\nKeyspace [default keyspace to use when writing data]\nUsername [authentication username]\nPassword [authentication password]\nAddress [cluster address]\nPort [default: 9042]\nDatacenter [optional] [default: datacenter1]\nReplication [optional] [default: 1]\n",
    "tag": "airbyte"
  },
  {
    "title": "Streamr",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/streamr.md",
    "content": "Streamr\nFeatures\n| Feature                       | Support | Notes                                                                                        |\n| :---------------------------- | :-----: | :------------------------------------------------------------------------------------------- |\n| Full Refresh Sync             |   \u274c    | Warning: this mode deletes all previously synced data in the configured bucket path.         |\n| Incremental - Append Sync     |   \u2705    |                                                                                              |\n| Incremental - Deduped History |   \u274c    | As this connector does not support dbt, we don't support this sync mode on this destination. |\n| Namespaces                    |   \u274c    | Setting a specific bucket path is equivalent to having separate namespaces.                  |\nThe Streamr destination allows you to sync data to Streamr - The decentralized\nreal\u2011time data network.\nHow to use: https://github.com/devmate-cloud/streamr-airbyte-connectors\nTroubleshooting\nCheck out common troubleshooting issues for the Streamr destination connector\nConfiguration\n| Parameter  |  Type  | Notes                      |\n| :--------- | :----: | :------------------------- |\n| privateKey | string | You private key on Streamr |\n| streamId   | string | Your full Stream ID        |\nOutput Schema\nAll json data is output at Streamr\nData schema\nAny json data schema will work",
    "tag": "airbyte"
  },
  {
    "title": "Elasticsearch",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/elasticsearch.md",
    "content": "Elasticsearch\nSync overview\nOutput schema\nElasticsearch is a Lucene based search engine that's a type of NoSql storage.\nDocuments are created in an `index`, similar to a `table`in a relation database.\nThe output schema matches the input schema of a source. \nEach source `stream` becomes a destination `index`.\nFor example, in with a relational database source -\nThe DB table name is mapped to the destination index. \nThe DB table columns become fields in the destination document.\nEach row becomes a document in the destination index.  \nData type mapping\nSee Elastic documentation for detailed information about the field types\nThis section should contain a table mapping each of the connector's data types to Airbyte types. At the moment, Airbyte uses the same types used by JSONSchema. `string`, `date-time`, `object`, `array`, `boolean`, `integer`, and `number` are the most commonly used data types.\n| Integration Type | Airbyte Type | Notes |\n| :--- | :--- | :--- |\n| text | string | more info\n| date | date-time | more info\n| object | object | more info\n| array | array | more info\n| boolean | boolean | more info\n| numeric | integer | more info\n| numeric | number | more info\nFeatures\nThis section should contain a table with the following format:\n| Feature | Supported?(Yes/No) | Notes |\n| :--- |:-------------------| :--- |\n| Full Refresh Sync | yes                |  |\n| Incremental Sync | yes                |  |\n| Replicate Incremental Deletes | no                 |  |\n| SSL connection | yes                |  |\n| SSH Tunnel Support | yes                |  |\nPerformance considerations\nBatch/bulk writes are performed. Large records may impact performance.\nThe connector should be enhanced to support variable batch sizes.\nGetting started\nRequirements\n\nElasticsearch >= 7.x\nConfiguration \nEndpoint URL [ex. https://elasticsearch.savantly.net:9423]\nUsername [optional] (basic auth)\nPassword [optional] (basic auth)\nCA certificate [optional]\nApi key ID [optional]\nApi key secret [optional]\nIf authentication is used, the user should have permission to create an index if it doesn't exist, and/or be able to `create` documents\n\nCA certificate\nCa certificate may be fetched from the Elasticsearch server from /usr/share/elasticsearch/config/certs/http_ca.crt\nFetching example from dockerized Elasticsearch:\n`docker cp es01:/usr/share/elasticsearch/config/certs/http_ca.crt .` where es01 is a container's name. For more details please visit https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html\nSetup guide\nEnter the endpoint URL, select authentication method, and whether to use 'upsert' method when indexing new documents. \nConnection via SSH Tunnel\nAirbyte has the ability to connect to an Elastic instance via an SSH Tunnel.\nThe reason you might want to do this because it is not possible (or against security policy) to connect to your Elastic instance directly (e.g. it does not have a public IP address).\nWhen using an SSH tunnel, you are configuring Airbyte to connect to an intermediate server (a.k.a. a bastion sever) that does have direct access to the Elastic instance.\nAirbyte connects to the bastion and then asks the bastion to connect directly to the server.\nUsing this feature requires additional configuration, when creating the source. We will talk through what each piece of configuration means.\n\nConfigure all fields for the source as you normally would, except `SSH Tunnel Method`.\n`SSH Tunnel Method` defaults to `No Tunnel` (meaning a direct connection). If you want to use an SSH Tunnel choose `SSH Key Authentication` or `Password Authentication`.\nChoose `Key Authentication` if you will be using an RSA private key as your secret for establishing the SSH Tunnel (see below for more information on generating this key).\nChoose `Password Authentication` if you will be using a password as your secret for establishing the SSH Tunnel.\n\n\n`SSH Tunnel Jump Server Host` refers to the intermediate (bastion) server that Airbyte will connect to. This should be a hostname or an IP Address.\n`SSH Connection Port` is the port on the bastion server with which to make the SSH connection. The default port for SSH connections is `22`, so unless you have explicitly changed something, go with the default.\n`SSH Login Username` is the username that Airbyte should use when connection to the bastion server. This is NOT the TiDB username.\nIf you are using `Password Authentication`, then `SSH Login Username` should be set to the password of the User from the previous step. If you are using `SSH Key Authentication` TiDB password, but the password for the OS-user that Airbyte is using to perform commands on the bastion.\nIf you are using `SSH Key Authentication`, then `SSH Private Key` should be set to the RSA Private Key that you are using to create the SSH connection. This should be the full contents of the key file starting with `-----BEGIN RSA PRIVATE KEY-----` and ending with `-----END RSA PRIVATE KEY-----`.\n",
    "tag": "airbyte"
  },
  {
    "title": "AWS Datalake",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/aws-datalake.md",
    "content": "AWS Datalake\nThis page contains the setup guide and reference information for the AWS Datalake destination connector.\nThe AWS Datalake destination connector allows you to sync data to AWS. It will write data as JSON files in S3 and \nwill make it available through a Lake Formation Governed Table in the Glue Data Catalog so that the data is available throughout other AWS services such as Athena, Glue jobs, EMR, Redshift, etc.\nPrerequisites\nTo use this destination connector, you will need:\n* An AWS account\n* An S3 bucket where the data will be written\n* An AWS Lake Formation database where tables will be created (one per stream)\n* AWS credentials in the form of either the pair Access key ID / Secret key ID or a role with the following permissions:\n\n\n```* Writing objects in the S3 bucket\n* Updating of the Lake Formation database\n```\n\n\nPlease check the Setup guide below if you need guidance creating those.\nSetup guide\nYou should now have all the requirements needed to configure AWS Datalake as a destination in the UI. You'll need the\nfollowing information to configure the destination:\n\nAws Account Id : The account ID of your AWS account. You will find the instructions to setup a new AWS account here.\nAws Region : The region in which your resources are deployed\nAuthentication mode : The AWS Datalake connector lets you authenticate with either a user or a role. In both case, you will have to make sure\nthat appropriate policies are in place. Select \"ROLE\" if you are using a role, \"USER\" if using a user with Access key / Secret Access key.\nTarget Role Arn : The name of the role, if \"Authentication mode\" was \"ROLE\". You will find the instructions to create a new role here.\nAccess Key Id : The Access Key ID of the user if \"Authentication mode\" was \"USER\". You will find the instructions to create a new user here. Make sure to select \"Programmatic Access\" so that you get secret access keys.\nSecret Access Key : The Secret Access Key ID of the user if \"Authentication mode\" was \"USER\"\nS3 Bucket Name : The bucket in which the data will be written. You will find the instructions to create a new S3 bucket here.\nTarget S3 Bucket Prefix : A prefix to prepend to the file name when writing to the bucket\nDatabase : The database in which the tables will be created. You will find the instructions to create a new Lakeformation Database here.\n\nAssigning proper permissions\nThe policy used by the user or the role must have access to the following services:\n\nAWS Lake Formation\nAWS Glue\nAWS S3\n\nYou can use the AWS policy generator to help you generate an appropriate policy.\nPlease also make sure that the role or user you will use has appropriate permissions on the database in AWS Lakeformation. You will find more information about Lake Formation permissions in the AWS Lake Formation Developer Guide.\nSupported sync modes\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental - Append Sync | Yes |  |\n| Namespaces | No |  |\nData type map\nThe Glue tables will be created with schema information provided by the source, i.e : You will find the same columns\nand types in the destination table as in the source except for the following types which will be translated for compatibility with the Glue Data Catalog:\n|Type in the source| Type in the destination|\n| :--- | :--- |\n| number | float |\n| integer | int |",
    "tag": "airbyte"
  },
  {
    "title": "Kinesis",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/kinesis.md",
    "content": "Kinesis\nPrerequisites\n\nFor Airbyte Open Source users using the Postgres source connector, upgrade your Airbyte platform to version `v0.40.0-alpha` or newer and upgrade your Kinesis connector to version `0.1.4` or newer\n\nSync overview\nOutput schema\nThe incoming Airbyte data is structured in a Json format and is sent across diferent stream shards determined by the partition key.\nThis connector maps an incoming data from a namespace and stream to a unique Kinesis stream. The Kinesis record which is sent to the stream is consisted of the following Json fields\n\n`_airbyte_ab_id`: Random UUID generated to be used as a partition key for sending data to different shards.\n`_airbyte_emitted_at`: a timestamp representing when the event was received from the data source.\n`_airbyte_data`: a json text/object representing the data that was received from the data source.\n\nFeatures\n| Feature                       | Support | Notes                                                                             |\n| :---------------------------- | :-----: | :-------------------------------------------------------------------------------- |\n| Full Refresh Sync             |   \u274c    |                                                                                   |\n| Incremental - Append Sync     |   \u2705    | Incoming messages are streamed/appended to a Kinesis stream as they are received. |\n| Incremental - Deduped History |   \u274c    |                                                                                   |\n| Namespaces                    |   \u2705    | Namespaces will be used to determine the Kinesis stream name.                     |\nPerformance considerations\nAlthough Kinesis is designed to handle large amounts of real-time data by scaling streams with shards, you should be aware of the following Kinesis Quotas and Limits.\nThe connector buffer size should also be tweaked according to your data size and freguency\nGetting started\nRequirements\n\nThe connector is compatible with the latest Kinesis service version at the time of this writing.\nConfiguration\nEndpoint: Aws Kinesis endpoint to connect to. Default endpoint if not provided\nRegion: Aws Kinesis region to connect to. Default region if not provided.\nshardCount: The number of shards with which the stream should be created. The amount of shards affects the throughput of your stream.\naccessKey: Access key credential for authenticating with the service.\nprivateKey: Private key credential for authenticating with the service.\nbufferSize: Buffer size used to increase throughput by sending data in a single request.\n\nSetup guide",
    "tag": "airbyte"
  },
  {
    "title": "BigQuery",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/bigquery.md",
    "content": "BigQuery\nSetting up the BigQuery destination connector involves setting up the data loading method (BigQuery Standard method and Google Cloud Storage bucket) and configuring the BigQuery destination connector using the Airbyte UI.\nThis page guides you through setting up the BigQuery destination connector.\nPrerequisites\n\nFor Airbyte Open Source users using the Postgres source connector, upgrade your Airbyte platform to version `v0.40.0-alpha` or newer and upgrade your BigQuery connector to version `1.1.14` or newer\nA Google Cloud project with BigQuery enabled\n\nA BigQuery dataset to sync data to.\nNote: Queries written in BigQuery can only reference datasets in the same physical location. If you plan on combining the data that Airbyte syncs with data from other datasets in your queries, create the datasets in the same location on Google Cloud. For more information, read Introduction to Datasets\n\n\n(Required for Airbyte Cloud; Optional for Airbyte Open Source) A Google Cloud Service Account with the BigQuery User and BigQuery Data Editor roles and the Service Account Key in JSON format.\n\n\nConnector modes\nWhile setting up the connector, you can configure it in the following modes:\n\nBigQuery: Produces a normalized output by storing the JSON blob data in `_airbyte_raw_*` tables and then transforming and normalizing the data into separate tables, potentially `exploding` nested streams into their own tables if basic normalization is configured.\nBigQuery (Denormalized): Leverages BigQuery capabilities with Structured and Repeated fields to produce a single \"big\" table per stream. Airbyte does not support normalization for this option at this time.\n\nSetup guide\nStep 1: Set up a data loading method\nAlthough you can load data using BigQuery's INSERTS, we highly recommend using a Google Cloud Storage bucket not only for performance and cost but reliability since larger datasets are prone to more failures when using standard inserts.\n(Recommended) Using a Google Cloud Storage bucket\nTo use a Google Cloud Storage bucket:\n\nCreate a Cloud Storage bucket with the Protection Tools set to `none` or `Object versioning`. Make sure the bucket does not have a retention policy.\nCreate an HMAC key and access ID.\nGrant the Storage Object Admin role to the Google Cloud Service Account.\nMake sure your Cloud Storage bucket is accessible from the machine running Airbyte. The easiest way to verify if Airbyte is able to connect to your bucket is via the check connection tool in the UI.\n\nYour bucket must be encrypted using a Google-managed encryption key (this is the default setting when creating a new bucket). We currently do not support buckets using customer-managed encryption keys (CMEK). You can view this setting under the \"Configuration\" tab of your GCS bucket, in the `Encryption type` row.\nUsing `INSERT`\nYou can use BigQuery's INSERT statement to upload data directly from your source to BigQuery. While this is faster to set up initially, we strongly recommend not using this option for anything other than a quick demo. Due to the Google BigQuery SDK client limitations, using `INSERT` is 10x slower than using a Google Cloud Storage bucket, and you may see some failures for big datasets and slow sources (For example, if reading from a source takes more than 10-12 hours). For more details, refer to https://github.com/airbytehq/airbyte/issues/3549\nStep 2: Set up the BigQuery connector\n\nLog into your Airbyte Cloud or Airbyte Open Source account.\nClick Destinations and then click + New destination.\nOn the Set up the destination page, select BigQuery or BigQuery (denormalized typed struct) from the Destination type dropdown depending on whether you want to set up the connector in BigQuery or BigQuery (Denormalized) mode.\nEnter the name for the BigQuery connector.\nFor Project ID, enter your Google Cloud project ID.\nFor Dataset Location, select the location of your BigQuery dataset.\n    :::warning\n    You cannot change the location later.\n    :::\nFor Default Dataset ID, enter the BigQuery Dataset ID.\nFor Loading Method, select Standard Inserts or GCS Staging.\n    :::tip\n    We recommend using the GCS Staging option.\n    :::\nFor Service Account Key JSON (Required for cloud, optional for open-source), enter the Google Cloud Service Account Key in JSON format.\n\nFor Transformation Query Run Type (Optional), select interactive to have BigQuery run interactive query jobs or batch to have BigQuery run batch queries.\n:::note\nInteractive queries are executed as soon as possible and count towards daily concurrent quotas and limits, while batch queries are executed as soon as idle resources are available in the BigQuery shared resource pool. If BigQuery hasn't started the query within 24 hours, BigQuery changes the job priority to interactive. Batch queries don't count towards your concurrent rate limit, making it easier to start many queries at once.\n:::\n\n\nFor Google BigQuery Client Chunk Size (Optional), use the default value of 15 MiB. Later, if you see networking or memory management problems with the sync (specifically on the destination), try decreasing the chunk size. In that case, the sync will be slower but more likely to succeed.\n\n\nSupported sync modes\nThe BigQuery destination connector supports the following sync modes:\n\nFull Refresh Sync\nIncremental - Append Sync\nIncremental - Deduped History\n\nOutput schema\nAirbyte outputs each stream into its own table in BigQuery. Each table contains three columns:\n\n`_airbyte_ab_id`: A UUID assigned by Airbyte to each event that is processed. The column type in BigQuery is `String`.\n`_airbyte_emitted_at`: A timestamp representing when the event was pulled from the data source. The column type in BigQuery is `Timestamp`.\n`_airbyte_data`: A JSON blob representing the event data. The column type in BigQuery is `String`.\n\nThe output tables in BigQuery are partitioned and clustered by the Time-unit column `_airbyte_emitted_at` at a daily granularity. Partitions boundaries are based on UTC time.\nThis is useful to limit the number of partitions scanned when querying these partitioned tables, by using a predicate filter (a `WHERE` clause). Filters on the partitioning column are used to prune the partitions and reduce the query cost. (The parameter Require partition filter is not enabled by Airbyte, but you may toggle it by updating the produced tables.)\nBigQuery Naming Conventions\nFollow BigQuery Datasets Naming conventions.\nAirbyte converts any invalid characters into `_` characters when writing data. However, since datasets that begin with `_` are hidden on the BigQuery Explorer panel, Airbyte prepends the namespace with `n` for converted namespaces.\nData type map\n| Airbyte type                        | BigQuery type | BigQuery denormalized type |\n|:------------------------------------|:--------------|:---------------------------|\n| DATE                                | DATE          | DATE                       |\n| STRING (BASE64)                     | STRING        | STRING                     |\n| NUMBER                              | FLOAT         | NUMBER                     |\n| OBJECT                              | STRING        | RECORD                     |\n| STRING                              | STRING        | STRING                     |\n| BOOLEAN                             | BOOLEAN       | BOOLEAN                    |\n| INTEGER                             | INTEGER       | INTEGER                    |\n| STRING (BIG_NUMBER)                 | STRING        | STRING                     |\n| STRING (BIG_INTEGER)                | STRING        | STRING                     |\n| ARRAY                               | REPEATED      | REPEATED                   |\n| STRING (TIMESTAMP_WITH_TIMEZONE)    | TIMESTAMP     | DATETIME                   |\n| STRING (TIMESTAMP_WITHOUT_TIMEZONE) | TIMESTAMP     | DATETIME                   |\nTroubleshooting permission issues\nThe service account does not have the proper permissions.\n\nMake sure the BigQuery service account has `BigQuery User` and `BigQuery Data Editor` roles or equivalent permissions as those two roles.\nIf the GCS staging mode is selected, ensure the BigQuery service account has the right permissions to the GCS bucket and path or the `Cloud Storage Admin` role, which includes a superset of the required permissions.\n\nThe HMAC key is wrong.\n\nMake sure the HMAC key is created for the BigQuery service account, and the service account has permission to access the GCS bucket and path.\n\nTutorials\nNow that you have set up the BigQuery destination connector, check out the following BigQuery tutorials:\n\nExport Google Analytics data to BigQuery\nLoad data from Facebook Ads to BigQuery\nReplicate Salesforce data to BigQuery\nPartition and cluster BigQuery tables with Airbyte and dbt\n",
    "tag": "airbyte"
  },
  {
    "title": "End-to-End Testing Destination",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/e2e-test.md",
    "content": "End-to-End Testing Destination\nThis destination is for testing of Airbyte connections. It can be set up as a source message logger, a `/dev/null`, or to mimic specific behaviors (e.g. exception during the sync). Please use it with discretion. This destination may log your data, and expose sensitive information.\nFeatures\n| Feature | Supported  | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes | |\n| Incremental Sync | Yes | |\n| Replicate Incremental Deletes | No | |\n| SSL connection | No | |\n| SSH Tunnel Support | No | |\nMode\nSilent (`/dev/null`)\nThis is the only mode allowed on Airbyte Cloud.\nThis mode works as `/dev/null`. It does nothing about any data from the source connector. This is usually only useful for performance testing of the source connector.\nLogging\nThis mode logs the data from the source connector. It will log at most 1,000 data entries.\nThere are the different logging modes to choose from:\n| Mode | Notes | Parameters |\n| :--- | :--- | :--- |\n| First N entries  | Log the first N number of data entries for each data stream. | N: how many entries to log. |\n| Every N-th entry | Log every N-th entry for each data stream. When N=1, it will log every entry. When N=2, it will log every other entry. Etc. | N: the N-th entry to log. Max entry count: max number of entries to log. |\n| Random sampling | Log a random percentage of the entries for each data stream. | Sampling ratio: a number in range of `[0, 1]`. Optional seed: default to system epoch time. Max entry count: max number of entries to log. |\nThrottling\nThis mode mimics a slow data sync. You can specify the time (in millisecond) of delay between each message from the source is processed.\nFailing\nThis mode throws an exception after receiving a configurable number of messages.",
    "tag": "airbyte"
  },
  {
    "title": "Sqlite",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/sqlite.md",
    "content": "Sqlite\n:::danger\nThis destination is meant to be used on a local workstation and won't work on Kubernetes\n:::\nOverview\nThis destination writes data to a file on the local filesystem on the host running Airbyte. By default, data is written to `/tmp/airbyte_local`. To change this location, modify the `LOCAL_ROOT` environment variable for Airbyte.\n:::caution\nPlease make sure that Docker Desktop has access to `/tmp` (and `/private` on a MacOS, as /tmp has a symlink that points to /private. It will not work otherwise). You allow it with \"File sharing\" in `Settings -> Resources -> File sharing -> add the one or two above folder` and hit the \"Apply & restart\" button.\n:::\nSync Overview\nOutput schema\nEach stream will be output into its own table `_airbyte_raw_{stream_name}`. Each table will contain 3 columns:\n\n`_airbyte_ab_id`: a uuid assigned by Airbyte to each event that is processed.\n`_airbyte_emitted_at`: a timestamp representing when the event was pulled from the data source.\n`_airbyte_data`: a json blob representing with the event data.\n\nFeatures\n| Feature | Supported |  |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental - Append Sync | Yes |  |\n| Incremental - Deduped History | No | As this connector does not support dbt, we don't support this sync mode on this destination. |\n| Namespaces | No |  |\nPerformance considerations\nThis integration will be constrained by the speed at which your filesystem accepts writes.\nGetting Started\nThe `destination_path` will always start with `/local` whether it is specified by the user or not. Any directory nesting within local will be mapped onto the local mount.\nBy default, the `LOCAL_ROOT` env variable in the `.env` file is set `/tmp/airbyte_local`.\nThe local mount is mounted by Docker onto `LOCAL_ROOT`. This means the `/local` is substituted by `/tmp/airbyte_local` by default.\nExample:\n\nIf `destination_path` is set to `/local/sqlite.db`\nthe local mount is using the `/tmp/airbyte_local` default\nthen all data will be written to `/tmp/airbyte_local/sqlite.db`.\n\nAccess Replicated Data Files\nIf your Airbyte instance is running on the same computer that you are navigating with, you can open your browser and enter file:///tmp/airbyte_local to look at the replicated data locally. If the first approach fails or if your Airbyte instance is running on a remote server, follow the following steps to access the replicated files:\n\nAccess the scheduler container using `docker exec -it airbyte-server bash`\nNavigate to the default local mount using `cd /tmp/airbyte_local`\nNavigate to the replicated file directory you specified when you created the destination, using `cd /{destination_path}`\nExecute `sqlite3 {filename}` to access the data in a particular database file.\n\nYou can also copy the output file to your host machine, the following command will copy the file to the current working directory you are using:\n`text\ndocker cp airbyte-server:/tmp/airbyte_local/{destination_path} .`\nNote: If you are running Airbyte on Windows with Docker backed by WSL2, you have to use similar step as above or refer to this link for an alternative approach.",
    "tag": "airbyte"
  },
  {
    "title": "MQTT",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/mqtt.md",
    "content": "MQTT\nOverview\nThe Airbyte MQTT destination allows you to sync data to any MQTT system compliance with version 3.1.X. Each stream is written to the corresponding MQTT topic.\nPrerequisites\n\nFor Airbyte Open Source users using the Postgres source connector, upgrade your Airbyte platform to version `v0.40.0-alpha` or newer and upgrade your MQTT connector to the latest version\n\nSync overview\nOutput schema\nEach stream will be output into a MQTT topic.\nThis connector writes data with JSON format (in bytes).\nEach record will contain in its payload these 4 fields:\n\n`_airbyte_ab_id`: an uuid assigned by Airbyte to each event that is processed.\n`_airbyte_emitted_at`:  a timestamp representing when the event was pulled from the data source.\n`_airbyte_data`: a json blob representing with the event data.\n`_airbyte_stream`: the name of each record's stream.\n\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | No |  |\n| Incremental - Append Sync | Yes |  |\n| Incremental - Deduped History | No | As this connector does not support dbt, we don't support this sync mode on this destination. |\n| Namespaces | Yes |  |\nGetting started\nRequirements\nTo use the MQTT destination, you'll need:\n\nA MQTT broker implementing MQTT protocol version 3.1.X.\n\nSetup guide\nNetwork Access\nMake sure your MQTT broker can be accessed by Airbyte.\nPermissions\nAirbyte should be allowed to write messages into topics. Based on the MQTT broker you have deployed, check if you'll need some specific permissions.\nTarget topics\nYou can determine the topics to which messages are written via the `topic_pattern` configuration parameter. Messages can be written to either a hardcoded, pre-defined topic, or dynamically written to different topics based on the namespace or stream they came from.\nTo write all messages to a single hardcoded topic, enter its name in the `topic_pattern` field e.g: setting `topic_pattern` to `path1/path2/my-topic-name` will write all messages from all streams and namespaces to that topic.\nTo define the output topics dynamically, you can leverage the `{namespace}` and `{stream}` pattern variables, which cause messages to be written to different topics based on the values present when producing the records. For example, setting the `topic_pattern` parameter to `airbyte_syncs/{namespace}/{stream}` means that messages from namespace `n1` and stream `s1` will get written to the topic `airbyte_syncs/n1/s1`, and messages from `s2` to `airbyte_syncs/n1/s2` etc.\nSetup the MQTT destination in Airbyte\nYou should now have all the requirements needed to configure MQTT as a destination in the UI. You can configure the following parameters on the MQTT destination (though many of these are optional or have default values):\n\nMQTT broker host\nMQTT broker port\nUse TLS\nUsername\nPassword\nTopic pattern\nTest topic\nClient ID\nSync publisher\nConnect timeout\nAutomatic reconnect\nClean session\nMessage retained\nMessage QoS\n\nMore info about this can be found in the OASIS MQTT standard site.\nNOTE: MQTT version 5 is not supported yet.",
    "tag": "airbyte"
  },
  {
    "title": "Amazon SQS",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/amazon-sqs.md",
    "content": "Amazon SQS\nOverview\nThe Airbyte SQS destination allows you to sync data to Amazon SQS. It currently supports sending all streams to a single Queue.\nSync overview\nOutput schema\nAll streams will be output into a single SQS Queue.\nAmazon SQS messages can only contain JSON, XML or text, and this connector supports writing messages in all three formats. See the Writing Text or XML messages section for more info.\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | No |  |\n| Incremental - Append Sync | Yes |  |\n| Incremental - Deduped History | No |  |\n| Namespaces | No |  |\nGetting started\nRequirements\n\nAWS IAM Access Key\nAWS IAM Secret Key\nAWS SQS Queue\n\nPermissions\nIf the target SQS Queue is not public, you will need the following permissions on the Queue:\n\n`sqs:SendMessage` \n\nProperties\nRequired properties are 'Queue URL' and 'AWS Region' as noted in bold below.\n\nQueue URL (STRING)\nThe full AWS endpoint URL of the queue e.g. https://sqs.eu-west-1.amazonaws.com/1234567890/example-queue-url\nAWS Region (STRING)\nThe region code for the SQS Queue e.g. eu-west-1\nMessage Delay (INT)\nTime in seconds that this message should be hidden from consumers.\nSee the AWS SQS documentation for more detail.\nAWS IAM Access Key ID (STRING)\nThe Access Key for the IAM User with permissions on this Queue\nPermission `sqs:SendMessage` is required\nAWS IAM Secret Key (STRING)\nThe Secret Key for the IAM User with permissions on this Queue\nMessage Body Key (STRING)\nRather than sending the entire Record as the Message Body, use this property to reference a Key in the Record to use as the message body. The value of this property should be the Key name in the input Record. The key must be at the top level of the Record, nested Keys are not supported.\nMessage Group Id (STRING)\nWhen using a FIFO queue, this property is required. \nSee the AWS SQS documentation for more detail.\n\nSetup guide\n\nCreate IAM Keys\nCreate SQS Queue\n\nUsing the Message Body Key\nThis property allows you to reference a Key within the input Record as using that properties Value as the SQS Message Body.\nFor example, with the input Record:\n`{\n  \"parent_with_child\": {\n    \"child\": \"child_value\"\n  },\n  \"parent\": \"parent_value\"\n}`\nTo send only the `parent_with_child` object, we can set `Message Body Key` to `parent_with_child`. Giving an output SQS Message of:\n`{\n  \"child\": \"child_value\"\n}`\nWriting Text or XML messages\nTo output Text or XML, the data must be contained within a String field in the input data, and then referenced by setting the `Message Body Key` property.\nFor example, with an input Record as:\n`{\n  \"my_xml_field\": \"<something>value</something>\"\n}`\nTo send a pure XML message, you would set the `Message Body Key` to `my_xml_field`.\nThe output SQS message would contain:\n`<something>value</something>`",
    "tag": "airbyte"
  },
  {
    "title": "S3-Glue",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/s3-glue.md",
    "content": "S3-Glue\nThis page guides you through the process of setting up the S3 destination connector with Glue.\nPrerequisites\nList of required fields:\n\nAccess Key ID\nSecret Access Key\nS3 Bucket Name\nS3 Bucket Path\nS3 Bucket Region\nGlue database\n\nGlue serialization library\n\n\nAllow connections from Airbyte server to your AWS S3/ Minio S3 cluster (if they exist in separate VPCs).\n\nAn S3 bucket with credentials or an instance profile with read/write permissions configured for the host (ec2, eks).\nEnforce encryption of data in transit\nAllow permissions to access the AWS Glue service from the Airbyte connector\n\nStep 1: Set up S3\nSign in to your AWS account.\nUse an existing or create new Access Key ID and Secret Access Key.\nPrepare S3 bucket that will be used as destination, see this to create an S3 bucket.\nNOTE: If the S3 cluster is not configured to use TLS, the connection to Amazon S3 silently reverts to an unencrypted connection. Airbyte recommends all connections be configured to use TLS/SSL as support for AWS's shared responsibility model\nStep 2: Set up Glue\nSign in to your AWS account.\nUse an existing or create new Access Key ID and Secret Access Key.\nPrepare the Glue database that will be used as destination, see this to create a Glue database\nStep 3: Set up the S3-Glue destination connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Destinations. In the top-right corner, click + new destination.\nOn the destination setup page, select S3 from the Destination type dropdown and enter a name for this connector.\nConfigure fields:\nAccess Key Id\nSee this on how to generate an access key.\nWe recommend creating an Airbyte-specific user. This user will require read and write permissions to objects in the bucket.\n\n\nSecret Access Key\nCorresponding key to the above key id.\n\n\nS3 Bucket Name\nSee this to create an S3 bucket.\n\n\nS3 Bucket Path\nSubdirectory under the above bucket to sync the data into.\n\n\nS3 Bucket Region\nSee here for all region codes.\n\n\nS3 Path Format\nAdditional string format on how to store data under S3 Bucket Path. Default value is `${NAMESPACE}/${STREAM_NAME}/${YEAR}_${MONTH}_${DAY}_${EPOCH}_`.\n\n\nS3 Endpoint\nLeave empty if using AWS S3, fill in S3 URL if using Minio S3.\n\n\nS3 Filename pattern\nThe pattern allows you to set the file-name format for the S3 staging file(s), next placeholders combinations are currently supported: {date}, {date:yyyy_MM}, {timestamp}, {timestamp:millis}, {timestamp:micros}, {part_number}, {sync_id}, {format_extension}. Please, don't use empty space and not supportable placeholders, as they won't recognized.\n\n\nGlue database\nThe Glue database name that was previously created through the management console or the cli.\n\n\nGlue serialization library\nThe library that your query engine will use for reading and writing data in your lake\n\n\nClick `Set up destination`.\n\nFor Airbyte Open Source:\n\nGo to local Airbyte page.\nIn the left navigation bar, click Destinations. In the top-right corner, click + new destination.\nOn the destination setup page, select S3 from the Destination type dropdown and enter a name for this connector.\n\nConfigure fields:\n\n\nAccess Key Id\n\nSee this on how to generate an access key.\nSee this on how to create a instanceprofile.\nWe recommend creating an Airbyte-specific user. This user will require read and write permissions to objects in the staging bucket.\nIf the Access Key and Secret Access Key are not provided, the authentication will rely on the instanceprofile.\n\n\nSecret Access Key\nCorresponding key to the above key id.\n\n\nMake sure your S3 bucket is accessible from the machine running Airbyte.\nThis depends on your networking setup.\nYou can check AWS S3 documentation with a tutorial on how to properly configure your S3's access here.\nIf you use instance profile authentication, make sure the role has permission to read/write on the bucket.\nThe easiest way to verify if Airbyte is able to connect to your S3 bucket is via the check connection tool in the UI.\n\n\nS3 Bucket Name\nSee this to create an S3 bucket.\n\n\nS3 Bucket Path\nSubdirectory under the above bucket to sync the data into.\n\n\nS3 Bucket Region\nSee here for all region codes.\n\n\nS3 Path Format\nAdditional string format on how to store data under S3 Bucket Path. Default value is `${NAMESPACE}/${STREAM_NAME}/${YEAR}_${MONTH}_${DAY}_${EPOCH}_`.\n\n\nS3 Endpoint\nLeave empty if using AWS S3, fill in S3 URL if using Minio S3.\n\n\nS3 Filename pattern\nThe pattern allows you to set the file-name format for the S3 staging file(s), next placeholders combinations are currently supported: {date}, {date:yyyy_MM}, {timestamp}, {timestamp:millis}, {timestamp:micros}, {part_number}, {sync_id}, {format_extension}. Please, don't use empty space and not supportable placeholders, as they won't recognized.\n\n\nGlue database\nThe Glue database name that was previously created through the management console or the cli.\n\n\n\nGlue serialization library\n\nThe library that your query engine will use for reading and writing data in your lake\n\n\n\nClick `Set up destination`.\n\n\nIn order for everything to work correctly, it is also necessary that the user whose \"S3 Key Id\" and \"S3 Access Key\" are used have access to both the bucket and its contents. Policies to use:\n`json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": \"s3:*\",\n      \"Resource\": [\n        \"arn:aws:s3:::YOUR_BUCKET_NAME/*\",\n        \"arn:aws:s3:::YOUR_BUCKET_NAME\"\n      ]\n    }\n  ]\n}`\nFor setting up the necessary Glue policies see this and this\nThe full path of the output data with the default S3 Path Format `${NAMESPACE}/${STREAM_NAME}/${YEAR}_${MONTH}_${DAY}_${EPOCH}_` is:\n`text\n<bucket-name>/<source-namespace-if-exists>/<stream-name>/<upload-date>_<epoch>_<partition-id>.<format-extension>`\nFor example:\n`text\ntesting_bucket/data_output_path/public/users/2021_01_01_1234567890_0.csv.gz\n\u2191              \u2191                \u2191      \u2191     \u2191          \u2191          \u2191 \u2191\n|              |                |      |     |          |          | format extension\n|              |                |      |     |          |          unique incremental part id\n|              |                |      |     |          milliseconds since epoch\n|              |                |      |     upload date in YYYY_MM_DD\n|              |                |      stream name\n|              |                source namespace (if it exists)\n|              bucket path\nbucket name`\nThe rationales behind this naming pattern are:\n\nEach stream has its own directory.\nThe data output files can be sorted by upload time.\nThe upload time composes of a date part and millis part so that it is both readable and unique.\n\nBut it is possible to further customize by using the available variables to format the bucket path:\n\n`${NAMESPACE}`: Namespace where the stream comes from or configured by the connection namespace fields.\n`${STREAM_NAME}`: Name of the stream\n`${YEAR}`: Year in which the sync was writing the output data in.\n`${MONTH}`: Month in which the sync was writing the output data in.\n`${DAY}`: Day in which the sync was writing the output data in.\n`${HOUR}`: Hour in which the sync was writing the output data in.\n`${MINUTE}` : Minute in which the sync was writing the output data in.\n`${SECOND}`: Second in which the sync was writing the output data in.\n`${MILLISECOND}`: Millisecond in which the sync was writing the output data in.\n`${EPOCH}`: Milliseconds since Epoch in which the sync was writing the output data in.\n`${UUID}`: random uuid string\n\nNote:\n\nMultiple `/` characters in the S3 path are collapsed into a single `/` character.\nIf the output bucket contains too many files, the part id variable is using a `UUID` instead. It uses sequential ID otherwise.\n\nPlease note that the stream name may contain a prefix, if it is configured on the connection.\nA data sync may create multiple files as the output files can be partitioned by size (targeting a size of 200MB compressed or lower) .\nSupported sync modes\n| Feature                       | Support | Notes                                                                                        |\n| :---------------------------- | :-----: | :------------------------------------------------------------------------------------------- |\n| Full Refresh Sync             |   \u2705    | Warning: this mode deletes all previously synced data in the configured bucket path.         |\n| Incremental - Append Sync     |   \u2705    |                                                                                              |\n| Incremental - Deduped History |   \u274c    | As this connector does not support dbt, we don't support this sync mode on this destination. |\n| Namespaces                    |   \u274c    | Setting a specific bucket path is equivalent to having separate namespaces.                  |\nThe Airbyte S3 destination allows you to sync data to AWS S3 or Minio S3. Each stream is written to its own directory under the bucket.\n\u26a0\ufe0f Please note that under \"Full Refresh Sync\" mode, data in the configured bucket and path will be wiped out before each sync. We recommend you to provision a dedicated S3 resource for this sync to prevent unexpected data deletion from misconfiguration. \u26a0\ufe0f\nSupported Output schema\nEach stream will be outputted to its dedicated directory according to the configuration. The complete datastore of each stream includes all the output files under that directory. You can think of the directory as equivalent of a Table in the database world.\n\nUnder Full Refresh Sync mode, old output files will be purged before new files are created.\nUnder Incremental - Append Sync mode, new output files will be added that only contain the new data.\n\nJSON Lines (JSONL)\nJSON Lines is a text format with one JSON per line. Each line has a structure as follows:\n`json\n{\n  \"_airbyte_ab_id\": \"<uuid>\",\n  \"_airbyte_emitted_at\": \"<timestamp-in-millis>\",\n  \"_airbyte_data\": \"<json-data-from-source><optional>\"\n}`\nFor example, given the following two json objects from a source:\n`json\n[\n  {\n    \"user_id\": 123,\n    \"name\": {\n      \"first\": \"John\",\n      \"last\": \"Doe\"\n    }\n  },\n  {\n    \"user_id\": 456,\n    \"name\": {\n      \"first\": \"Jane\",\n      \"last\": \"Roe\"\n    }\n  }\n]`\ndepending on whether you want to flatten your data or not (available as a configuration option)\nThe json objects can have the following formats:\n`text\n{ \"_airbyte_ab_id\": \"26d73cde-7eb1-4e1e-b7db-a4c03b4cf206\", \"_airbyte_emitted_at\": \"1622135805000\", \"_airbyte_data\": { \"user_id\": 123, \"name\": { \"first\": \"John\", \"last\": \"Doe\" } } }\n{ \"_airbyte_ab_id\": \"0a61de1b-9cdd-4455-a739-93572c9a5f20\", \"_airbyte_emitted_at\": \"1631948170000\", \"_airbyte_data\": { \"user_id\": 456, \"name\": { \"first\": \"Jane\", \"last\": \"Roe\" } } }`\n`text\n{ \"_airbyte_ab_id\": \"26d73cde-7eb1-4e1e-b7db-a4c03b4cf206\", \"_airbyte_emitted_at\": \"1622135805000\", \"user_id\": 123, \"name\": { \"first\": \"John\", \"last\": \"Doe\" } }\n{ \"_airbyte_ab_id\": \"0a61de1b-9cdd-4455-a739-93572c9a5f20\", \"_airbyte_emitted_at\": \"1631948170000\", \"user_id\": 456, \"name\": { \"first\": \"Jane\", \"last\": \"Roe\" } }`\nOutput files can be compressed. The default option is GZIP compression. If compression is selected, the output filename will have an extra extension (GZIP: `.jsonl.gz`).",
    "tag": "airbyte"
  },
  {
    "title": "MongoDB",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/mongodb.md",
    "content": "MongoDB\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental - Append Sync | Yes |  |\n| Incremental - Deduped History | No | As this connector does not support dbt, we don't support this sync mode on this destination. |\n| Namespaces | Yes |  |\nPrerequisites\n\nFor Airbyte Open Source users using the Postgres source connector, upgrade your Airbyte platform to version `v0.40.0-alpha` or newer and upgrade your MongoDB connector to version `0.1.6` or newer\n\nOutput Schema for `destination-mongodb`\nEach stream will be output into its own collection in MongoDB. Each collection will contain 3 fields:\n\n`_id`: an identifier assigned to each document that is processed. The filed type in MongoDB is `String`.\n`_airbyte_emitted_at`: a timestamp representing when the event was pulled from the data source. The field type in MongoDB is `Timestamp`.\n`_airbyte_data`: a json blob representing with the event data. The field type in MongoDB is `Object`.\n\nGetting Started (Airbyte Cloud)\nAirbyte Cloud only supports connecting to your MongoDB instance with TLS encryption. Other than that, you can proceed with the open-source instructions below.\nGetting Started (Airbyte Open-Source)\nRequirements\nTo use the MongoDB destination, you'll need:\n\nA MongoDB server\n\nPermissions\nYou need a MongoDB user that can create collections and write documents. We highly recommend creating an Airbyte-specific user for this purpose.\nTarget Database\nYou will need to choose an existing database or create a new database that will be used to store synced data from Airbyte.\nSetup the MongoDB destination in Airbyte\nYou should now have all the requirements needed to configure MongoDB as a destination in the UI. You'll need the following information to configure the MongoDB destination:\n\nStandalone MongoDb instance\nHost: URL of the database\nPort: Port to use for connecting to the database\nTLS: indicates whether to create encrypted connection\nReplica Set\nServer addresses: the members of a replica set\nReplica Set: A replica set name\nMongoDb Atlas Cluster\nCluster URL: URL of a cluster to connect to\nDatabase\nUsername\nPassword\n\nFor more information regarding configuration parameters, please see MongoDb Documentation.\nConnection via SSH Tunnel\nAirbyte has the ability to connect to an MongoDB instance via an SSH Tunnel.\nThe reason you might want to do this because it is not possible (or against security policy) to connect to your MongoDB instance directly (e.g. it does not have a public IP address).\nWhen using an SSH tunnel, you are configuring Airbyte to connect to an intermediate server (a.k.a. a bastion sever) that does have direct access to the MongoDB instance.\nAirbyte connects to the bastion and then asks the bastion to connect directly to the server.\nUsing this feature requires additional configuration, when creating the source. We will talk through what each piece of configuration means.\n\nConfigure all fields for the source as you normally would, except `SSH Tunnel Method`.\n`SSH Tunnel Method` defaults to `No Tunnel` (meaning a direct connection). If you want to use an SSH Tunnel choose `SSH Key Authentication` or `Password Authentication`.\nChoose `Key Authentication` if you will be using an RSA private key as your secret for establishing the SSH Tunnel (see below for more information on generating this key).\nChoose `Password Authentication` if you will be using a password as your secret for establishing the SSH Tunnel.\n\n\n`SSH Tunnel Jump Server Host` refers to the intermediate (bastion) server that Airbyte will connect to. This should be a hostname or an IP Address.\n`SSH Connection Port` is the port on the bastion server with which to make the SSH connection. The default port for SSH connections is `22`, so unless you have explicitly changed something, go with the default.\n`SSH Login Username` is the username that Airbyte should use when connection to the bastion server. This is NOT the TiDB username.\nIf you are using `Password Authentication`, then `SSH Login Username` should be set to the password of the User from the previous step. If you are using `SSH Key Authentication` TiDB password, but the password for the OS-user that Airbyte is using to perform commands on the bastion.\nIf you are using `SSH Key Authentication`, then `SSH Private Key` should be set to the RSA Private Key that you are using to create the SSH connection. This should be the full contents of the key file starting with `-----BEGIN RSA PRIVATE KEY-----` and ending with `-----END RSA PRIVATE KEY-----`.\n\nNaming Conventions\nThe following information comes from the MongoDB Limits and Thresholds documentation.\nDatabase Name Case Sensitivity\nSince database names are case insensitive in MongoDB, database names cannot differ only by the case of the characters.\nRestrictions on Database Names for Windows\nFor MongoDB deployments running on Windows, database names cannot contain any of the following characters: /. \"$<>:\\|?\nAlso database names cannot contain the null character.\nRestrictions on Database Names for Unix and Linux Systems\nFor MongoDB deployments running on Unix and Linux systems, database names cannot contain any of the following characters: /. \"$\nAlso database names cannot contain the null character.\nLength of Database Names\nDatabase names cannot be empty and must have fewer than 64 characters.\nRestriction on Collection Names\nCollection names should begin with an underscore or a letter character, and cannot:\n\ncontain the $.\nbe an empty string (e.g. \"\").\ncontain the null character.\nbegin with the system. prefix. (Reserved for internal use.)\n",
    "tag": "airbyte"
  },
  {
    "title": "MeiliSearch",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/destinations/meilisearch.md",
    "content": "MeiliSearch\nOverview\nThe Airbyte MeilSearch destination allows you to sync data to MeiliSearch. MeiliSearch is a search engine that makes it easy for a non-developer to search through data. It does not require any SQL.\nSync overview\nOutput schema\nEach stream will be output into its own index in MeiliSearch. Each table will be named after the stream with all non-alpha numeric characters removed. Each table will contain one column per top-levelfield in a stream. In addition, it will contain a table called `_ab_pk`. This column is used internally by Airbyte to prevent records from getting overwritten and can be ignored.\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental - Append Sync | Yes |  |\n| Incremental - Deduped History | No | As this connector does not support dbt, we don't support this sync mode on this destination. |\n| Namespaces | No |  |\nGetting started\nRequirements\nTo use the MeiliSearch destination, you'll need an existing MeiliSearch instance. You can learn about how to create one in the MeiliSearch docs.\nSetup guide\nThe setup only requires two fields. First is the `host` which is the address at which MeiliSearch can be reached. If running on a localhost by default it will be on `http://localhost:7700`. Note that you must include the protocol. The second piece of information is the API key. If no API key is set for your MeiliSearch instance, then this field can be left blank. If it is set, you can find the value for your API by following these instructions. in the MeiliSearch docs.",
    "tag": "airbyte"
  },
  {
    "title": "Getting Started: Destination Redshift",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/getting-started/destination-redshift.md",
    "content": "Getting Started: Destination Redshift\nRequirements\n\nActive Redshift cluster\nAllow connections from Airbyte to your Redshift cluster (if they exist in separate VPCs)\nA staging S3 bucket with credentials (for the COPY strategy).\n\nSetup guide\n1. Make sure your cluster is active and accessible from the machine running Airbyte\nThis is dependent on your networking setup. The easiest way to verify if Airbyte is able to connect to your Redshift cluster is via the check connection tool in the UI. You can check AWS Redshift documentation with a tutorial on how to properly configure your cluster's access here\n2. Fill up connection info\nNext is to provide the necessary information on how to connect to your cluster such as the `host` whcih is part of the connection string or Endpoint accessible here without the `port` and `database` name (it typically includes the cluster-id, region and end with `.redshift.amazonaws.com`).\nYou should have all the requirements needed to configure Redshift as a destination in the UI. You'll need the following information to configure the destination:\n\nHost\nPort\nUsername\nPassword\nSchema\nDatabase\nThis database needs to exist within the cluster provided.\n\n\n\n2a. Fill up S3 info (for COPY strategy)\nProvide the required S3 info.\n\nS3 Bucket Name\nSee this to create an S3 bucket.\n\n\nS3 Bucket Region\nPlace the S3 bucket and the Redshift cluster in the same region to save on networking costs.\n\n\nAccess Key Id\nSee this on how to generate an access key.\nWe recommend creating an Airbyte-specific user. This user will require read and write permissions to objects in the staging bucket.\n\n\nSecret Access Key\nCorresponding key to the above key id.\n\n\nPart Size\nAffects the size limit of an individual Redshift table. Optional. Increase this if syncing tables larger than 100GB. Files are streamed to S3 in parts. This determines the size of each part, in MBs. As S3 has a limit of 10,000 parts per file, part size affects the table size. This is 10MB by default, resulting in a default table limit of 100GB. Note, a larger part size will result in larger memory requirements. A rule of thumb is to multiply the part size by 10 to get the memory requirement. Modify this with care.\n\n\n\nOptional parameters:\n* Bucket Path\n  * The directory within the S3 bucket to place the staging data. For example, if you set this to `yourFavoriteSubdirectory`, staging data will be placed inside `s3://yourBucket/yourFavoriteSubdirectory`. If not provided, defaults to the root directory.\nNotes about Redshift Naming Conventions\nFrom Redshift Names & Identifiers:\nStandard Identifiers\n\nBegin with an ASCII single-byte alphabetic character or underscore character, or a UTF-8 multibyte character two to four bytes long.\nSubsequent characters can be ASCII single-byte alphanumeric characters, underscores, or dollar signs, or UTF-8 multibyte characters two to four bytes long.\nBe between 1 and 127 bytes in length, not including quotation marks for delimited identifiers.\nContain no quotation marks and no spaces.\n\nDelimited Identifiers\nDelimited identifiers (also known as quoted identifiers) begin and end with double quotation marks (\"). If you use a delimited identifier, you must use the double quotation marks for every reference to that object. The identifier can contain any standard UTF-8 printable characters other than the double quotation mark itself. Therefore, you can create column or table names that include otherwise illegal characters, such as spaces or the percent symbol. ASCII letters in delimited identifiers are case-insensitive and are folded to lowercase. To use a double quotation mark in a string, you must precede it with another double quotation mark character.\nTherefore, Airbyte Redshift destination will create tables and schemas using the Unquoted identifiers when possible or fallback to Quoted Identifiers if the names are containing special characters.\nData Size Limitations\nRedshift specifies a maximum limit of 65535 bytes to store the raw JSON record data. Thus, when a row is too big to fit, the Redshift destination fails to load such data and currently ignores that record.",
    "tag": "airbyte"
  },
  {
    "title": "Getting Started: Source Facebook Marketing",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/getting-started/source-facebook-marketing.md",
    "content": "Getting Started: Source Facebook Marketing\nRequirements\nGoogle Ads Account with an approved Developer Token (note: In order to get API access to Google Ads, you must have a \"manager\" account. This must be created separately from your standard account. You can find more information about this distinction in the google ads docs.)\n\ndeveloper_token\nclient_id\nclient_secret\nrefresh_token\nstart_date\ncustomer_id\n\nSetup guide\nThis guide will provide information as if starting from scratch. Please skip over any steps you have already completed.\n\nCreate an Google Ads Account. Here are Google's instruction on how to create one.\nCreate an Google Ads MANAGER Account. Here are Google's instruction on how to create one.\nYou should now have two Google Ads accounts: a normal account and a manager account. Link the Manager account to the normal account following Google's documentation.\nApply for a developer token (make sure you follow our instructions) on your Manager account.  This token allows you to access your data from the Google Ads API. Here are Google's instructions. The docs are a little unclear on this point, but you will not be able to access your data via the Google Ads API until this token is approved. You cannot use a test developer token, it has to be at least a basic developer token. It usually takes Google 24 hours to respond to these applications. This developer token is the value you will use in the `developer_token` field.\nFetch your `client_id`, `client_secret`, and `refresh_token`. Google provides instructions on how to do this.\nSelect your `customer_id`. The `customer_is` refer to the id of each of your Google Ads accounts. This is the 10 digit number in the top corner of the page when you are in google ads ui. The source will only pull data from the accounts for which you provide an id. If you are having trouble finding it, check out Google's instructions.\n\nWow! That was a lot of steps. We are working on making the OAuth flow for all of our connectors simpler (allowing you to skip needing to get a `developer_token` and a `refresh_token` which are the most painful / time-consuming steps in this walkthrough).\nHow to apply for the developer token\nGoogle is very picky about which software and which use case can get access to a developer token. The Airbyte team has worked with the Google Ads team to whitelist Airbyte and make sure you can get one (see issue 1981 for more information).\nWhen you apply for a token, you need to mention:\n\nWhy you need the token (eg: want to run some internal analytics...)\nThat you will be using the Airbyte Open Source project\nThat you have full access to the code base (because we're open source)\nThat you have full access to the server running the code (because you're self-hosting Airbyte)\n\nIf for any reason the request gets denied, let us know and we will be able to unblock you.\nUnderstanding Google Ads Query Language",
    "tag": "airbyte"
  },
  {
    "title": "CallRail",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/callrail.md",
    "content": "CallRail\nOverview\nThe CailRail source supports Full Refresh and Incremental syncs. \nOutput schema\nThis Source is capable of syncing the following core Streams:\n\nCalls\nCompanies\nText Messages\nUsers\n\nFeatures\n| Feature | Supported? |\n| :--- |:-----------|\n| Full Refresh Sync | Yes        |\n| Incremental - Append Sync | Yes        |\n| Incremental - Dedupe Sync | Yes        |\n| SSL connection | No         |\n| Namespaces | No         |\nGetting started\nRequirements\n\nCallRail Account\nCallRail API Token\n",
    "tag": "airbyte"
  },
  {
    "title": "The Guardian API",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/the-guardian-api.md",
    "content": "The Guardian API\nOverview\nThe Guardian API source can sync data from the The Guardian\nRequirements\nTo access the API, you will need to sign up for an API key, which should be sent with every request. Visit this link to register for an API key.\nThe following (optional) parameters can be provided to the connector :-\n\n`q` (query)\nThe `q` (query) parameter filters the results to only those that include that search term. The `q` parameter supports `AND`, `OR` and `NOT` operators. For example, let's see if the Guardian has any content on political debates: `https://content.guardianapis.com/search?q=debates`\nHere the q parameter filters the results to only those that include that search term. In this case, there are many results, so we might want to filter down the response to something more meaningful, specifically looking for political content published in 2014, for example: `https://content.guardianapis.com/search?q=debate&tag=politics/politics&from-date=2014-01-01&api-key=test`\n\n`tag`\nA tag is a piece of data that is used to categorise content. All Guardian content is manually categorised using these tags, of which there are more than 50,000. Use this parameter to filter results by showing only the ones matching the entered tag. See here for a list of all tags, and here for the tags endpoint documentation.\n\n`section`\nUse this to filter the results by a particular section. See here for a list of all sections, and here for the sections endpoint documentation.\n\n`order-by`\nUse this to sort the results. The three available sorting options are - newest, oldest, relevance. For enabling incremental syncs set order-by to oldest.\n\n`start_date`\nUse this to set the minimum date (YYYY-MM-DD) of the results. Results older than the start_date will not be shown.\n\n`end_date`\nUse this to set the maximum date (YYYY-MM-DD) of the results. Results newer than the end_date will not be shown.\nDefault is set to the current date (today) for incremental syncs.\n\nOutput schema\nEach content item (news article) has the following structure:-\n`yaml\n{\n    \"id\": \"string\",\n    \"type\": \"string\"\n    \"sectionId\": \"string\"\n    \"sectionName\": \"string\"\n    \"webPublicationDate\": \"string\"\n    \"webTitle\": \"string\"\n    \"webUrl\": \"string\"\n    \"apiUrl\": \"string\"\n    \"isHosted\": \"boolean\"\n    \"pillarId\": \"string\"\n    \"pillarName\": \"string\"\n}`\nThe source is capable of syncing the content stream.\nSetup guide\nStep 1: Set up the The Guardian API connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, select The Guardian API from the Source type dropdown.\nEnter your api_key (mandatory) and any other optional parameters as per your requirements.\nClick Set up source.\n\nFor Airbyte OSS:\n\nNavigate to the Airbyte Open Source dashboard.\nSet the name for your source (The Guardian API).\nEnter your api_key (mandatory) and any other optional parameters as per your requirements.\nClick Set up source.\n\nSupported sync modes\nThe Guardian API source connector supports the following sync modes:\n| Feature           | Supported? |\n| :---------------- | :--------- |\n| Full Refresh Sync | Yes        |\n| Incremental Sync  | No         |\n| Namespaces        | No         |\nPerformance considerations\nThe key that you are assigned is rate-limited and as such any applications that depend on making large numbers of requests on a polling basis are likely to exceed their daily quota and thus be prevented from making further requests until the next period begins.",
    "tag": "airbyte"
  },
  {
    "title": "Kafka",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/kafka.md",
    "content": "Kafka\nThis page guides you through the process of setting up the Kafka source connector.\nSet up guide\nStep 1: Set up Kafka\nTo use the Kafka source connector, you'll need:\n\nA Kafka cluster 1.0 or above\nAirbyte user should be allowed to read messages from topics, and these topics should be created before reading from Kafka.\n\nStep 2: Setup the Kafka source in Airbyte\nYou'll need the following information to configure the Kafka source:\n\nGroup ID - The Group ID is how you distinguish different consumer groups. (e.g. group.id)\nProtocol - The Protocol used to communicate with brokers.\nClient ID - An ID string to pass to the server when making requests. The purpose of this is to be able to track the source of requests beyond just ip/port by allowing a logical application name to be included in server-side request logging. (e.g. airbyte-consumer)\nTest Topic - The Topic to test in case the Airbyte can consume messages. (e.g. test.topic)\nSubscription Method - You can choose to manually assign a list of partitions, or subscribe to all topics matching specified pattern to get dynamically assigned partitions.\nList of topic\nBootstrap Servers - A list of host/port pairs to use for establishing the initial connection to the Kafka cluster.\nSchema Registry - Host/port to connect schema registry server. Note: It supports for AVRO format only.\n\nFor Airbyte Open Source:\n\nGo to the Airbyte UI and in the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the Kafka connector and select Kafka from the Source type dropdown.\nFollow the Setup the Kafka source in Airbyte\n\nSupported sync modes\nThe Kafka source connector supports the following sync modes:\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental - Append Sync | Yes |  |\n| Namespaces | No |  |\nSupported Format\nJSON - Json value messages. It does not support schema registry now.\nAVRO - deserialize Using confluent API. Please refer (https://docs.confluent.io/platform/current/schema-registry/serdes-develop/serdes-avro.html)",
    "tag": "airbyte"
  },
  {
    "title": "Secoda API",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/secoda.md",
    "content": "Secoda API\nSync overview\nThis source can sync data from the Secoda API. At present this connector only supports full refresh syncs meaning that each time you use the connector it will sync all available records from scratch. Please use cautiously if you expect your API to have a lot of records.\nThis Source Supports the Following Streams\n\ncollections\ntables\nterms\n\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | No |  |\nPerformance considerations\nGetting started\nRequirements\n\nAPI Access\n",
    "tag": "airbyte"
  },
  {
    "title": "Snowflake",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/snowflake.md",
    "content": "Snowflake\nOverview\nThe Snowflake source allows you to sync data from Snowflake. It supports both Full Refresh and Incremental syncs. You can choose if this connector will copy only the new or updated data, or all rows in the tables and columns you set up for replication, every time a sync is run.\nThis Snowflake source connector is built on top of the source-jdbc code base and is configured to rely on JDBC  3.13.22 Snowflake driver as described in Snowflake documentation.\nResulting schema\nThe Snowflake source does not alter the schema present in your warehouse. Depending on the destination connected to this source, however, the result schema may be altered. See the destination's documentation for more details.\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental - Append Sync | Yes |  |\n| Namespaces | Yes |  |\nGetting started\nRequirements\n\nYou'll need the following information to configure the Snowflake source:\nHost\nRole\nWarehouse\nDatabase\nSchema\nUsername\nPassword\nJDBC URL Params (Optional)\nCreate a dedicated read-only Airbyte user and role with access to all schemas needed for replication.\n\nSetup guide\n1. Additional information about Snowflake connection parameters could be found here.\n2. Create a dedicated read-only user with access to the relevant schemas (Recommended but optional)\nThis step is optional but highly recommended to allow for better permission control and auditing. Alternatively, you can use Airbyte with an existing user in your database.\nTo create a dedicated database user, run the following commands against your database:\n```sql\n-- set variables (these need to be uppercase)\nSET AIRBYTE_ROLE = 'AIRBYTE_ROLE';\nSET AIRBYTE_USERNAME = 'AIRBYTE_USER';\n-- set user password\nSET AIRBYTE_PASSWORD = '-password-';\nBEGIN;\n-- create Airbyte role\nCREATE ROLE IF NOT EXISTS $AIRBYTE_ROLE;\n-- create Airbyte user\nCREATE USER IF NOT EXISTS $AIRBYTE_USERNAME\nPASSWORD = $AIRBYTE_PASSWORD\nDEFAULT_ROLE = $AIRBYTE_ROLE\nDEFAULT_WAREHOUSE= $AIRBYTE_WAREHOUSE;\n-- grant Airbyte schema access\nGRANT OWNERSHIP ON SCHEMA $AIRBYTE_SCHEMA TO ROLE $AIRBYTE_ROLE;\nCOMMIT;\n```\nYou can limit this grant down to specific schemas instead of the whole database. Note that to replicate data from multiple Snowflake databases, you can re-run the command above to grant access to all the relevant schemas, but you'll need to set up multiple sources connecting to the same db on multiple schemas.\nYour database user should now be ready for use with Airbyte.\nAuthentication\nThere are 2 way ways of oauth supported: login\\pass and oauth2.\nLogin and Password\n| Field | Description                                                                                                                                                                                       |\n|---|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Host | The host domain of the snowflake instance (must include the account, region, cloud environment, and end with snowflakecomputing.com). Example: `accountname.us-east-2.aws.snowflakecomputing.com` |\n| Role | The role you created in Step 1 for Airbyte to access Snowflake. Example: `AIRBYTE_ROLE`                                                                                                           |\n| Warehouse | The warehouse you created in Step 1 for Airbyte to sync data into. Example: `AIRBYTE_WAREHOUSE`                                                                                                   |\n| Database | The database you created in Step 1 for Airbyte to sync data into. Example: `AIRBYTE_DATABASE`                                                                                                     |\n| Schema | The schema whose tables this replication is targeting. If no schema is specified, all tables with permission will be presented regardless of their schema.                                        |\n| Username | The username you created in Step 2 to allow Airbyte to access the database. Example: `AIRBYTE_USER`                                                                                               |\n| Password | The password associated with the username.                                                                                                                                                        |\n| JDBC URL Params (Optional) | Additional properties to pass to the JDBC URL string when connecting to the database formatted as `key=value` pairs separated by the symbol `&`. Example: `key1=value1&key2=value2&key3=value3`   |\nOAuth 2.0\nField | Description                                                                                                                                                                                       |\n|---|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Host | The host domain of the snowflake instance (must include the account, region, cloud environment, and end with snowflakecomputing.com). Example: `accountname.us-east-2.aws.snowflakecomputing.com` |\n| Role | The role you created in Step 1 for Airbyte to access Snowflake. Example: `AIRBYTE_ROLE`                                                                                                           |\n| Warehouse | The warehouse you created in Step 1 for Airbyte to sync data into. Example: `AIRBYTE_WAREHOUSE`                                                                                                   |\n| Database | The database you created in Step 1 for Airbyte to sync data into. Example: `AIRBYTE_DATABASE`                                                                                                     |\n| Schema | The schema whose tables this replication is targeting. If no schema is specified, all tables with permission will be presented regardless of their schema.                                        |\n| OAuth2 | The Login name and password to obtain auth token.                                                                                                                                                 |\n| JDBC URL Params (Optional) | Additional properties to pass to the JDBC URL string when connecting to the database formatted as `key=value` pairs separated by the symbol `&`. Example: `key1=value1&key2=value2&key3=value3`   |\nNetwork policies\nBy default, Snowflake allows users to connect to the service from any computer or device IP address. A security administrator (i.e. users with the SECURITYADMIN role) or higher can create a network policy to allow or deny access to a single IP address or a list of addresses.\nIf you have any issues connecting with Airbyte Cloud please make sure that the list of IP addresses is on the allowed list\nTo determine whether a network policy is set on your account or for a specific user, execute the SHOW PARAMETERS command.\nAccount\n\n\n```    SHOW PARAMETERS LIKE 'network_policy' IN ACCOUNT;\n```\n\n\nUser\n\n\n```    SHOW PARAMETERS LIKE 'network_policy' IN USER <username>;\n```\n\n\nTo read more please check official Snowflake documentation",
    "tag": "airbyte"
  },
  {
    "title": "Commercetools",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/commercetools.md",
    "content": "Commercetools\nSync overview\nThe Commercetools source supports both Full Refresh and Incremental syncs. You can choose if this connector will copy only the new or updated data, or all rows in the tables and columns you set up for replication, every time a sync is run.\nThis source can sync data for the Commercetools API.\nThis Source Connector is based on a Airbyte CDK.\nOutput schema\nThis Source is capable of syncing the following core Streams:\n\nCustomers\nOrders\nProducts\nDiscountCodes\nPayments\n\nData type mapping\n| Integration Type | Airbyte Type | Notes |\n| :--- | :--- | :--- |\n| `string` | `string` |  |\n| `number` | `number` |  |\n| `array` | `array` |  |\n| `object` | `object` |  |\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental - Append Sync | Yes |  |\n| Namespaces | No |  |\nPerformance considerations\nCommercetools has some rate limit restrictions.\nGetting started\n\nCreate an API Client in the admin interface\nDecide scopes for the API client. Airbyte only needs read-level access.\nNote: The UI will show all possible data sources and will show errors when syncing if it doesn't have permissions to access a resource.\n\n\nThe `projectKey` of the store, the generated `client_id` and `client_secret` are required for the integration\nYou're ready to set up Commercetools in Airbyte!\n",
    "tag": "airbyte"
  },
  {
    "title": "Delighted",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/delighted.md",
    "content": "Delighted\nThis page contains the setup guide and reference information for the Delighted source connector.\nPrerequisites\nTo set up the Delighted source connector, you'll need the Delighted API key.\nSet up the Delighted connector in Airbyte\n\nLog into your Airbyte Cloud account.\nClick Sources and then click + New source.\nOn the Set up the source page, enter the name for the Delighted connector and select Delighted from the Source type dropdown.\nFor Since, enter the date in a Unix Timestamp format. The data added on and after this date will be replicated.\nFor API Key, enter your Delighted API Key.\nClick Set up source.\n\nSupported sync modes\nThe Delighted source connector supports the following  sync modes:\n\nFull Refresh - Overwrite\nFull Refresh - Append\nIncremental - Append\nIncremental - Deduped History\n\nSupported Streams\nThis Source is capable of syncing the following core Streams:\n\nSurvey Responses\nPeople\nBounced People\nUnsubscribed People\n",
    "tag": "airbyte"
  },
  {
    "title": "SpaceX-API",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/spacex-api.md",
    "content": "SpaceX-API\nThis page contains the setup guide and reference information for the SpaceX-API source connector.\nPrerequisites\nNo prerequisites, but a dummy api_key is required as it enhances security in future build. Please check the available routes at SpaceX Routes.\nSetup guide\nStep 1: Set up SpaceX connection\n\nHave a dummy API key (Example: 12345)\nA specific id (If specific info is needed)\n\nStep 2: Set up the SpaceX-API connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the SpaceX-API connector and select Spacex-API from the Source type dropdown.\nEnter your `api_key`.\nEnter your `id` if needed. (Optional)\nClick Set up source.\n\nFor Airbyte OSS:\n\nNavigate to the Airbyte Open Source dashboard.\nSet the name for your source.\nEnter your `api_key`.\nEnter your `id` if needed. (Optional)\nClick Set up source.\n\nSupported sync modes\nThe SpaceX-API source connector supports the following sync modes:\n| Feature                       | Supported? |\n| :---------------------------- | :--------- |\n| Full Refresh Sync             | Yes        |\n| Incremental Sync              | No         |\n| Replicate Incremental Deletes | No         |\n| SSL connection                | Yes        |\n| Namespaces                    | No         |\nSupported Streams\n\nLaunches\nCapsules\nCompany\nCrew\nCores\nDragons\nHistory\nLandpads\nPayloads\nRoadster\nRockets\nShips\nStarlink\n\nAPI method example\nGET https://api.spacexdata.com/v5/launches/latest\nPerformance considerations\nSpaceX's API reference has both v4 an v5 for launches. The connector as default uses V4 as it has minimal bugs.",
    "tag": "airbyte"
  },
  {
    "title": "Jenkins",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/jenkins.md",
    "content": "Jenkins\nOverview\nThe Jenkins source is maintained by Faros\nAI.\nPlease file any support requests on that repo to minimize response time from the\nmaintainers. The source supports both Full Refresh and Incremental syncs. You\ncan choose if this source will copy only the new or updated data, or all rows\nin the tables and columns you set up for replication, every time a sync is run.\nOutput schema\nSeveral output streams are available from this source:\n\nBuilds (Incremental)\nJobs\n\nIn the above links, replace `your.jenkins.url` with the url of your Jenkins\ninstance, and replace any environment variables with an existing Jenkins job or\nbuild id.\nIf there are more endpoints you'd like Faros AI to support, please create an\nissue.\nFeatures\n| Feature | Supported? |\n| :--- | :--- |\n| Full Refresh Sync | Yes |\n| Incremental Sync | Yes |\n| SSL connection | Yes |\n| Namespaces | No |\nPerformance considerations\nThe Jenkins source should not run into Jenkins API limitations under normal\nusage. Please create an\nissue if you see any\nrate limit issues that are not automatically retried successfully.\nGetting started\nRequirements\n\nJenkins Server\nJenkins User\nJenkins API Token\n\nSetup guide\nLogin to your Jenkins server in your browser and go to\n`https://your.jenkins.url/me/configure` to generate your API token.",
    "tag": "airbyte"
  },
  {
    "title": "Facebook Pages",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/facebook-pages.md",
    "content": "Facebook Pages\nThis page contains the setup guide and reference information for the Facebook Pages source connector.\nPrerequisites\nTo set up the Facebook Pages source connector with Airbyte, you'll need to create your Facebook Application and use both long-lived Page access token and Facebook Page ID.\nSetup guide\nStep 1: Set up Facebook Pages\n\nCreate Facebook Developer Account. Follow instruction to create one.\nCreate Facebook App. Choose \"Company\" as the purpose of the app. Fill out the remaining fields to create your app, then follow along the \"Connect a User Page\" section.\nConnect a User Page. Choose your app at `Meta App` field. Choose your Page at `User or Page` field. Add next permission:\npages_read_engagement\npages_read_user_content \npages_show_list\nread_insights\nClick Generate Access Token and follow instructions.\n\nAfter all the steps, it should look something like this\n\n\nGenerate Long-Lived User Access Token.\nGenerate Long-Lived Page Token.\n\nStep 2: Set up the Facebook Pages connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click + New source.\nOn the Set up the source page, enter the name for the Facebook Pages connector and select Facebook Pages from the Source type dropdown.\nFill in Page Access Token with Long-Lived Page Token\nFill in Page ID (if you have a page URL such as `https://www.facebook.com/Test-1111111111`, the ID would be`Test-1111111111`)\n\nFor Airbyte OSS:\n\nNavigate to the Airbyte Open Source dashboard.\nSet the name for your source. \nOn the Set up the source page, enter the name for the Facebook Pages connector and select Facebook Pages from the Source type dropdown.\nFill in Page Access Token with Long-Lived Page Token\nFill in Page ID (if you have a page URL such as `https://www.facebook.com/Test-1111111111`, the ID would be`Test-1111111111`)\n\nSupported sync modes\nThe Facebook Pages source connector supports the following sync modes:\n* Full Refresh - Overwrite\n* Full Refresh - Append\nSupported Streams\n\nPage\nPost\nPage Insights\nPost Insights\n\nData type map\n| Integration Type | Airbyte Type | Notes |\n|:-----------------|:-------------|:------|\n| `string`         | `string`     |       |\n| `number`         | `number`     |       |\n| `array`          | `array`      |       |\n| `object`         | `object`     |       |\nPerformance considerations\nFacebook heavily throttles API tokens generated from Facebook Apps by default, making it infeasible to use such a token for syncs with Airbyte. To be able to use this connector without your syncs taking days due to rate limiting follow the instructions in the Setup Guide below to access better rate limits.\nSee Facebook's documentation on rate limiting for more information on requesting a quota upgrade.",
    "tag": "airbyte"
  },
  {
    "title": "Harvest",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/harvest.md",
    "content": "Harvest\nThis page contains the setup guide and reference information for the Harvest source connector.\nPrerequisites\nTo set up the Harvest source connector, you'll need the Harvest Account ID and API key.\nSetup guide\n\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud.\nClick Sources and then click + New source.\nOn the Set up the source page, select Harvest from the Source type dropdown.\nEnter the name for the Harvest connector.\nEnter your Harvest Account ID.\nFor Start Date, enter the date in YYYY-MM-DDTHH:mm:ssZ format. The data added on and after this date will be replicated.\nFor Authentication mechanism, select Authenticate via Harvest (OAuth) from the dropdown and click Authenticate your Harvest account. Log in and authorize your Harvest account.\nClick Set up source.\n\n\n\nFor Airbyte Open Source:\n\nNavigate to the Airbyte Open Source dashboard.\nClick Sources and then click + New source.\nOn the Set up the source page, select Harvest from the Source type dropdown.\nEnter the name for the Harvest connector.\nEnter your Harvest Account ID.\nFor Start Date, enter the date in YYYY-MM-DDTHH:mm:ssZ format. The data added on and after this date will be replicated.\nFor Authentication mechanism, select Authenticate with Personal Access Token from the dropdown. Enter your Personal Access Token.\nClick Set up source.\n\n\nSupported sync modes\nThe Harvest source connector supports the following sync modes:\n\nFull Refresh - Overwrite\nFull Refresh - Append\nIncremental - Append\nIncremental - Deduped History\n\nSupported Streams\n\nClient Contacts (Incremental)\nClients (Incremental)\nCompany\nInvoice Messages (Incremental)\nInvoice Payments (Incremental)\nInvoices (Incremental)\nInvoice Item Categories (Incremental)\nEstimate Messages (Incremental)\nEstimates (Incremental)\nEstimate Item Categories (Incremental)\nExpenses (Incremental)\nExpense Categories (Incremental)\nTasks (Incremental)\nTime Entries (Incremental)\nProject User Assignments (Incremental)\nProject Task Assignments (Incremental)\nProjects (Incremental)\nRoles (Incremental)\nUser Billable Rates\nUser Cost Rates\nUser Project Assignments (Incremental)\nExpense Reports\nUninvoiced Report\nTime Reports\nProject Budget Report\n\nPerformance considerations\nThe connector is restricted by the Harvest rate limits.",
    "tag": "airbyte"
  },
  {
    "title": "Zenefits",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/zenefits.md",
    "content": "Zenefits\nThis page contains the setup guide and reference information for the Zenefits source connector.\nPrerequisites\n\nA Zenefits token\n\nSet up Zenefits as a source in Airbyte\nFor Airbyte OSS\nTo set up Zenefits as a source in Airbyte Cloud:\n\nIn the left navigation bar, click Sources. In the top-right corner, click + New source.\nOn the Set up the source page, select Zenefits from the Source type dropdown.\nFor Name, enter a name for the Zenefits connector.\nFor Token, enter the token you have got from Authentication\nClick Set up source.\n\nSupported sync modes\nThe Zenefits source connector supports the following sync modes:\n\nFull Refresh - Overwrite\n\nSupported Streams\nYou can replicate the following tables using the Zenefits connector:\n\nPeople\nEmployments\nVacation_requests\nVacation_types\nTime_durations\nDepartments\nLocations\nLabor_groups\nLabor_group_types\nCustom_fields\nCustom_field_values\n\nData type mapping\n| Integration Type | Airbyte Type |\n| :--------------: | :----------: |\n|      string      |    string    |\n|      number      |    number    |\n|      array       |    array     |\n|      object      |    object    |",
    "tag": "airbyte"
  },
  {
    "title": "Recreation.gov API",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/recreation.md",
    "content": "Recreation.gov API\nSync overview\nRecreation Information Database - RIDB\nRIDB is a part of the Recreation One Stop (R1S) program, \nwhich oversees the operation of Recreation.gov -- a user-friendly, web-based \nresource to citizens, offering a single point of access to information about \nrecreational opportunities nationwide. The website represents an authoritative \nsource of information and services for millions of visitors to federal lands, \nhistoric sites, museums, waterways and other activities and destinations.\nThis source retrieves data from the Recreation API.\nOutput schema\nThis source is capable of syncing the following streams:\n\nActivities\nCampsites\nEvents\nFacilities\nFacility Addresses\nLinks\nMedia\nOrganizations\nPermit Entrances\nRecreation Areas\nRecreation Area Addresses\nTours\n\nFeatures\n| Feature           | Supported? (Yes/No) | Notes |\n|:------------------|:----------------------|:------|\n| Full Refresh Sync | Yes                   |       |\n| Incremental Sync  | No                    |       |\nPerformance considerations\nThe Recreation API has a rate limit of 50 requests per minute.\nGetting started\nRequirements\n\nA Recreation API key. You can get one by signing up here.\nFind your key by signing in to the API docs portal and clicking on your name in the top right corner.\n\nSetup guide\nThe following fields are required fields for the connector to work:\n\n`api_key`: Your Recreation API key.\n",
    "tag": "airbyte"
  },
  {
    "title": "Gutendex",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/gutendex.md",
    "content": "Gutendex\nOverview\nThe Gutendex source can sync data from the Gutendex API\nRequirements\nGutendex requires no access token/API key to make requests.\nThe following (optional) parameters can be provided to the connector :-\n\n`author_year_start` and `author_year_end`\nUse these to find books with at least one author alive in a given range of years. They must have positive (CE) or negative (BCE) integer values. \nFor example, `/books?author_year_start=1800&author_year_end=1899` gives books with authors alive in the 19th Century.\n\n`copyright`\nUse this to find books with a certain copyright status: true for books with existing copyrights, false for books in the public domain in the USA, or null for books with no available copyright information.\n\n`languages`\nUse this to find books in any of a list of languages. They must be comma-separated, two-character language codes. For example, `/books?languages=en` gives books in English, and `/books?languages=fr,fi` gives books in either French or Finnish or both.\n\n`search`\nUse this to search author names and book titles with given words. They must be separated by a space (i.e. %20 in URL-encoded format) and are case-insensitive. For example, `/books?search=dickens%20great` includes Great Expectations by Charles Dickens.\n\n`sort`\nUse this to sort books: ascending for Project Gutenberg ID numbers from lowest to highest, descending for IDs highest to lowest, or popular (the default) for most popular to least popular by number of downloads.\n\n`topic`\nUse this to search for a case-insensitive key-phrase in books' bookshelves or subjects. For example, `/books?topic=children` gives books on the \"Children's Literature\" bookshelf, with the subject \"Sick children -- Fiction\", and so on.\n\nOutput schema\nLists of book information in the Project Gutenberg database are queried using the API at /books (e.g. gutendex.com/books). Book data will be returned in the format:-\n\n\n```{\n    \"count\": <number>,\n    \"next\": <string or null>,\n    \"previous\": <string or null>,\n    \"results\": <array of Books>\n}\n```\n\n\nwhere `results` is an array of 0-32 book objects, next and previous are URLs to the next and previous pages of results, and count in the total number of books for the query on all pages combined.\nBy default, books are ordered by popularity, determined by their numbers of downloads from Project Gutenberg.\nThe source is capable of syncing the results stream.\nSetup guide\nStep 1: Set up the Gutendex connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, select Gutendex from the Source type dropdown.\nClick Set up source.\n\nFor Airbyte OSS:\n\nNavigate to the Airbyte Open Source dashboard.\nSet the name for your source (Gutendex).\nClick Set up source.\n\nSupported sync modes\nThe Gutendex source connector supports the following sync modes:\n| Feature           | Supported? |\n| :---------------- | :--------- |\n| Full Refresh Sync | Yes        |\n| Incremental Sync  | No         |\n| Namespaces        | No         |\nPerformance considerations\nThere is no published rate limit. However, since this data updates infrequently, it is recommended to set the update cadence to 24hr or higher.",
    "tag": "airbyte"
  },
  {
    "title": "Looker",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/looker.md",
    "content": "Looker\nOverview\nThe Looker source supports Full Refresh syncs. That is, every time a sync is run, Airbyte will copy all rows in the tables and columns you set up for replication into the destination in a new table.\nOutput schema\nSeveral output streams are available from this source:\n\nColor Collections\nConnections\nContent Metadata\nContent Metadata Access\nDashboards\nDashboard Elements \nDashboard Filters \nDashboard Layouts \nDatagroups \nFolders \nGroups \nHomepages \nIntegration Hubs \nIntegrations \nLookml Dashboards \nLookml Models \nLooks \nRun Look\nProjects \nProject Files \nGit Branches \nQuery History \nRoles \nModel Sets \nPermission Sets \nPermissions \nRole Groups \nScheduled Plans \nSpaces \nUser Attributes \nUser Attribute Group Value \nUser Login Lockouts \nUsers \nUser Attribute Values \nUser Sessions \nVersions \nWorkspaces \n\nIf there are more endpoints you'd like Airbyte to support, please create an issue.\nFeatures\n| Feature | Supported? |\n| :--- | :--- |\n| Full Refresh Sync | Yes |\n| Incremental Sync | Coming soon |\n| Replicate Incremental Deletes | Coming soon |\n| SSL connection | Yes |\n| Namespaces | No |\nPerformance considerations\nThe Looker connector should not run into Looker API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.\nGetting started\nRequirements\n\nClient Id\nClient Secret\nDomain\n\nSetup guide\nPlease read the \"API3 Key\" section in Looker's information for users docs for instructions on how to generate Client Id and Client Secret.",
    "tag": "airbyte"
  },
  {
    "title": "TMDb",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/tmdb.md",
    "content": "TMDb\nThis page contains the setup guide and reference information for the Tmdb source connector.\nPrerequisites\nApi key and movie ID is mandate for this connector to work, It could be generated using a free account at TMDb. Visit: https://www.themoviedb.org/settings/api\nJust pass the generated API key and Movie ID for establishing the connection.\nSetup guide\nStep 1: Set up TMDb connection\n\nGenerate an API key (Example: 12345)\nGive a Movie ID, Query, Language (Target Movie, Query for search, Language filter)\n\nStep 2: Set up the TMDb connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the Google-webfonts connector and select TMDb from the Source type dropdown.\nEnter your `api_key`.\nEnter params `movie_id, query, language` (if needed).\nClick Set up source.\n\nFor Airbyte OSS:\n\nNavigate to the Airbyte Open Source dashboard.\nSet the name for your source.\nEnter your `api_key`.\nEnter params `movie_id, query, language` (if needed).\nClick Set up source.\n\nSupported sync modes\nThe Google-webfonts source connector supports the following sync modes:\n| Feature                       | Supported? |\n| :---------------------------- | :--------- |\n| Full Refresh Sync             | Yes        |\n| Incremental Sync              | No         |\n| Replicate Incremental Deletes | No         |\n| SSL connection                | Yes        |\n| Namespaces                    | No         |\nSupported Streams\n\nCertification_movie\nCertification_tv\nChanges_movie\nChanges_tv\nChanges_person\nMovies_alternative_titles\nMovies_changes\nMovies_credits\nMovies_details\nMovies_external_ids\nMovies_images\nMovies_keywords\nMovies_latest\nMovies_lists\nMovies_now_playing\nMovies_popular\nMovies_recommentations\nMovies_releases_dates\nMovies_reviews\nMovies_similar_movies\nMovies_top_rated\nMovies_translations\nMovies_upcoming\nMovies_videos\nMovies_watch_providers\nTrending\nSearch_collections\nSearch_companies\nSearch_keywords\nSearch_movies\nSearch_multi\nSearch_people\nSearch_tv_shows\n\nAPI method example\nGET https://api.themoviedb.org/3/movie/{movie_id}/alternative_titles?api_key={api_key}\nPerformance considerations\nTMDb's API reference has v3 at present and v4 is at development. The connector as default uses v3.",
    "tag": "airbyte"
  },
  {
    "title": "Freshservice",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/freshservice.md",
    "content": "Freshservice\nOverview\nThe Freshservice supports full refresh syncs. You can choose if this connector will copy only the new or updated data, or all rows in the tables and columns you set up for replication, every time a sync is run.\nOutput schema\nSeveral output streams are available from this source:\n\nTickets (Incremental)\nProblems (Incremental)\nChanges (Incremental)\nReleases (Incremental)\nRequesters\nAgents\nLocations\nProducts\nVendors\nAssets\nPurchaseOrders\nSoftware\n\nIf there are more endpoints you'd like Airbyte to support, please create an issue.\nFeatures\n| Feature | Supported? |\n| :--- | :--- |\n| Full Refresh Sync | Yes |\n| Incremental Sync | Yes |\n| SSL connection | No |\n| Namespaces | No |\nPerformance considerations\nThe Freshservice connector should not run into Freshservice API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.\nGetting started\nRequirements\n\nFreshservice Account\nFreshservice API Key\nFreshservice domain name\nReplciation Start Date\n\nSetup guide\nPlease read How to find your API key.",
    "tag": "airbyte"
  },
  {
    "title": "Jira",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/jira.md",
    "content": "Jira\nThis page contains the setup guide and reference information for the Jira source connector.\nPrerequisites\n\nAPI Token\nDomain\nEmail\n\nSetup guide\nStep 1: Set up Jira\n\nTo get access to the Jira API you need to create an API token, please follow the instructions in this documentation.\n\nStep 2: Set up the Jira connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click + new source.\nOn the source setup page, select Jira from the Source type dropdown and enter a name for this connector.\nEnter the API Token that you have created.\nEnter the Domain for your Jira account, e.g. `airbyteio.atlassian.net`.\nEnter the Email for your Jira account.\nEnter the list of Projects (Optional) for which you need to replicate data, or leave it empty if you want to replicate data for all projects.\nEnter the Start Date (Optional) from which you'd like to replicate data for Jira in the format YYYY-MM-DDTHH:MM:SSZ. All data generated after this date will be replicated. Note that it will be used only in the following streams: BoardIssues, IssueComments, IssueProperties, IssueRemoteLinks, IssueVotes, IssueWatchers, IssueWorklogs, Issues, PullRequests, SprintIssues.\nToggle Expand Issue Changelog allows you to get a list of recent updates to every issue in the Issues stream.\nToggle Render Issue Fields allows returning field values rendered in HTML format in the Issues stream.\nToggle Enable Experimental Streams enables experimental PullRequests stream.\n\nSupported sync modes\nThe Jira source connector supports the following sync modes:\n\nFull Refresh - Overwrite\nFull Refresh - Append\nIncremental - Append\nIncremental - Deduped History\n\nTroubleshooting\nCheck out common troubleshooting issues for the Jira connector on our Discourse here.\nSupported Streams\nThis connector outputs the following full refresh streams:\n\nApplication roles\nAvatars\nBoards\nDashboards\nFilters\nFilter sharing\nGroups\nIssue fields\nIssue field configurations\nIssue custom field contexts\nIssue link types\nIssue navigator settings\nIssue notification schemes\nIssue priorities\nIssue properties\nIssue remote links\nIssue resolutions\nIssue security schemes\nIssue type schemes\nIssue type screen schemes\nIssue votes\nIssue watchers\nJira settings\nLabels\nPermissions\nPermission schemes\nProjects\nProject avatars\nProject categories\nProject components\nProject email\nProject permission schemes\nProject types\nProject versions\nScreens\nScreen tabs\nScreen tab fields\nScreen schemes\nSprints\nTime tracking\nUsers\nUsersGroupsDetailed\nWorkflows\nWorkflow schemes\nWorkflow statuses\nWorkflow status categories\n\nThis connector outputs the following incremental streams:\n\nBoard issues\nIssue comments\nIssue worklogs\nIssues\nSprint issues\n\nIf there are more endpoints you'd like Airbyte to support, please create an issue.\nExperimental Tables\nThe following tables depend on undocumented internal Jira API endpoints and are\ntherefore subject to stop working if those endpoints undergo major changes.\nWhile they will not cause a sync to fail, they may not be able to pull any data.\nUse the \"Enable Experimental Streams\" option when setting up the source to allow\nor disallow these tables to be selected when configuring a connection.\n\nPull Requests (currently only GitHub PRs are supported)\n\nTroubleshooting\nCheck out common troubleshooting issues for the Jira connector on our Discourse here.\nRate Limiting & Performance\nThe Jira connector should not run into Jira API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.",
    "tag": "airbyte"
  },
  {
    "title": "Instagram",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/instagram.md",
    "content": "Instagram\nThis page contains the setup guide and reference information for the Instagram source connector.\nPrerequisites\n\nMeta for Developers account\nInstagram business account to your Facebook page\nInstagram Graph API to your Facebook app\nFacebook API access token\nFacebook ad account ID number (you'll use this to configure Instagram as a source in Airbyte)\n\nSetup Guide\nStep 1: Set up Instagram\u200b\nGenerate access tokens with the following permissions:\n* instagram_basic\n* instagram_manage_insights\n* pages_show_list\n* pages_read_engagement\n* Instagram Public Content Access\nStep 2: Set up the Instagram connector in Airbyte\n\nFor Airbyte Cloud:\n\nLog in to your Airbyte Cloud account.\nClick Sources and then click + New source.\nOn the Set up the source page, select Instagram from the Source type dropdown.\nEnter a name for your source.\nClick Authenticate your Instagram account.\nLog in and authorize the Instagram account.\nEnter the Start Date in YYYY-MM-DDTHH:mm:ssZ format. All data generated after this date will be replicated. If this field is blank, Airbyte will replicate all data.\nClick Set up source.\n\n\n\nFor Airbyte Open Source:\n\nLog in to your Airbyte Open Source account.\nClick Sources and then click + New source.\nOn the Set up the source page, select Instagram from the Source type dropdown.\nEnter a name for your source.\nClick Authenticate your Instagram account.\nLog in and authorize the Instagram account.\nEnter the Start Date in YYYY-MM-DDTHH:mm:ssZ format. All data generated after this date will be replicated. If this field is blank, Airbyte will replicate all data.\nPaste the access tokens from Step 1.\nClick Set up source.\n\n\nSupported sync modes\nThe Instagram source connector supports the following sync modes:\n* Full Refresh - Overwrite\n* Full Refresh - Append\n* Incremental - Append\n* Incremental - Deduped History\n:::note\nIncremental sync modes are only available for the User Insights stream.\n:::\nSupported Streams\nThe Instagram source connector supports the following streams. For more information, see the Instagram Graph API and Instagram Insights API documentation.\n\nUser\nUser Insights\nMedia\nMedia Insights\nStories\nStory Insights\n\nRate Limiting and Performance Considerations\nInstagram limits the number of requests that can be made at a time, but the Instagram connector gracefully handles rate limiting. See Facebook's documentation on rate limiting for more information.\nData type map\nAirbyteRecords are required to conform to the Airbyte type system. This means that all sources must produce schemas and records within these types and all destinations must handle records that conform to this type system.\n| Integration Type | Airbyte Type |\n| :--------------- | :----------- |\n| `string`         | `string`     |\n| `number`         | `number`     |\n| `array`          | `array`      |\n| `object`         | `object`     |",
    "tag": "airbyte"
  },
  {
    "title": "Polygon Stock API",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/polygon-stock-api.md",
    "content": "Polygon Stock API\nSync overview\nThis source can give information about stocks data available on \nPolygonStocksApi. It currently only supports Full Refresh\nsyncs.\nOutput schema\nThis source is capable of syncing the following streams:\n\n`stock_api`\n\nFeatures\n| Feature           | Supported? (Yes/No) | Notes                                                   |\n|:------------------|:----------------------|:--------------------------------------------------------|\n| Full Refresh Sync | Yes                   |                                                         |\n| Incremental Sync  | No                    |                                                         |\nPerformance considerations\nPolygon Stocks API allows only 5 API Calls/Minute on the free plan. Use of this connector\nmay require a paid plan based upon your requirements.\nGetting started\nRequirements\n\nObtain an API key from PolygonStocksApi.\nFind out the exchange symbol of the stock also known as Ticker Symbol of the stock you can google it out and find it (E.x. : Exchange symbol for Microsoft is MSFT)\nChoose and verify other options you required for fetching the stock details. here.\n\nSetup guide\nThe following fields are required fields for the connector to work:\n- `apiKey`: Your Polygon Stocks API key.\n- `stocksTicker`: The ticker symbol of the `stock/equity`.\n- `multiplier`: The size of the timespan multiplier.\n- `timespan`: The \n- `from`: The start of the aggregate time window. Either a date with the format YYYY-MM-DD or a millisecond timestamp.\n- `to`: The end of the aggregate time window. Either a date with the format YYYY-MM-DD or a millisecond timestamp.\n- (optional) `adjusted`: determines whether or not the results are adjusted for splits. By default, results are adjusted and set to true. Set this to false to get results that are NOT adjusted for splits.\n- (optional) `sort`: Sort the results by timestamp. asc will return results in ascending order (oldest at the top), desc will return results in descending order (newest at the top).\n- (optional) `limit`: Limits the number of base aggregates queried to create the aggregate results. Max 50000 and Default 5000. Read more about how limit is used to calculate aggregate results in our article on Aggregate Data API Improvements Find-more.",
    "tag": "airbyte"
  },
  {
    "title": "Zuora",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/zuora.md",
    "content": "Zuora\nSync overview\nThe Zuora source supports both Full Refresh and Incremental syncs. You can choose if this connector will copy only the new or updated data, or all rows in the tables and columns you set up for replication, every time a sync is run.\nAirbyte uses REST API to fetch data from Zuora. The REST API accepts ZOQL (Zuora Object Query Language), a SQL-like language, to export the data.\nThis Source Connector is based on a Airbyte CDK.\nOutput schema\nThis Source is capable of syncing:\n\nstandard objects available in Zuora account\ncustom objects manually added by user, available in Zuora Account\ncustom fields in both standard and custom objects, available in Zuora Account\n\nThe discovering of Zuora Account objects schema may take a while, if you add the connection for the first time, and/or you need to refresh your list of available streams. Please take your time to wait and don't cancel this operation, usually it takes up to 5-10 min, depending on number of objects available in Zuora Account.\nNote:\nSome of the Zuora Objects may not be available for sync due to limitations of Zuora Supscription Plan, Permissions. For details refer to the Availability of Data Source Objects section in the Zuora documentation.\nData type mapping\n| Integration Type | Airbyte Type | Notes |\n| :--- | :--- | :--- |\n| `decimal(22,9)` | `number` | float number |\n| `decimal` | `number` | float number |\n| `float` | `number` | float number |\n| `double` | `number` | float number |\n| `integer` | `number` |  |\n| `int` | `number` |  |\n| `bigint` | `number` |  |\n| `smallint` | `number` |  |\n| `timestamp` | `number` | number representation of the unix timestamp |\n| `date` | `string` |  |\n| `datetime` | `string` |  |\n| `timestamp with time zone` | `string` |  |\n| `picklist` | `string` |  |\n| `text` | `string` |  |\n| `varchar` | `string` |  |\n| `zoql` | `object` |  |\n| `binary` | `object` |  |\n| `json` | `object` |  |\n| `xml` | `object` |  |\n| `blob` | `object` |  |\n| `list` | `array` |  |\n| `array` | `array` |  |\n| `boolean` | `boolean` |  |\n| `bool` | `boolean` |  |\nAny other data type not listed in the table above will be treated as `string`.\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Overwrite Sync | Yes |  |\n| Full Refresh Append Sync | Yes |  |\n| Incremental - Append Sync | Yes |  |\n| Incremental - Append + Deduplication Sync | Yes |  |\n| Namespaces | No |  |\nSupported Environments for Zuora\n| Environment | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Production | Yes | Select from exising options while setup |\n| Sandbox | Yes | Select from exising options while setup |\nSupported Data Query options\n| Option | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| LIVE | Yes | Run data queries against Zuora live transactional databases |\n| UNLIMITED | Yes | Run data queries against an optimized, replicated database at 12 hours freshness for high volume extraction use cases (Early Adoption, additionall access required, contact Zuora Support in order to request this feature enabled for your account beforehand.) |\nList of Supported Environments for Zuora\nProduction\n| Environment | Endpoint |\n| :--- | :--- |\n| US Production | rest.zuora.com |\n| US Cloud Production | rest.na.zuora.com |\n| EU Production | rest.eu.zuora.com |\nSandbox\n| Environment | Endpoint |\n| :--- | :--- |\n| US API Sandbox | rest.apisandbox.zuora.com |\n| US Cloud API Sandbox | rest.sandbox.na.zuora.com |\n| US Central Sandbox | rest.test.zuora.com |\n| EU API Sandbox | rest.sandbox.eu.zuora.com |\n| EU Central Sandbox | rest.test.eu.zuora.com |\nOther\n| Environment | Endpoint |\n| :--- | :--- |\n| US Performance Test | rest.pt1.zuora.com |\nFor more information about available environments, please visit this page\nPerformance considerations\nIf you experience the long time for sync operation, please consider:\n\nto increase the `window_in_days` parameter inside Zuora source configuration\nuse the smaller date range by tuning `start_date` parameter.\n\nNote\nUsually, the very first sync operation for all of the objects inside Zuora account takes up to 25-45-60 min, the more data you have, the more time you'll need.\nGetting started\nCreate an API user role\n\nLog in to your `Zuora acccount`.\nIn the top right corner of the Zuora dashboard, select `Settings` > `Administration Settings`.\nSelect `Manage User Roles`.\nSelect `Add new role` to create a new role, and fill in neccessary information up to the form.\n\nAssign the role to a user\n\nFrom the `administration` page, click `Manage Users`.\nClick `add single user`.\nCreate a user and assign it to the role you created in `Create an API user role` section.\nYou should receive an email with activation instructions. Follow them to activate your API user.\n\nFor more information visit Create an API User page\nCreate Client ID and Client Secret\n\nFrom the `administration` page, click `Manage Users`.\nClick on User Name of the target user.\nEnter a client name and description and click `create`.\nA pop-up will open with your Client ID and Client Secret.\n\nMake a note of your Client ID and Client Secret because they will never be shown again. You will need them to configure Airbyte Zuora Connector.\n\nYou're ready to set up Zuora connector in Airbyte, using created `Client ID` and `Client Secret`!\n",
    "tag": "airbyte"
  },
  {
    "title": "Close.com",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/close-com.md",
    "content": "Close.com\nPrerequisites\n\nClose.com Account\nClose.com API Key\n\nVisit the Close.com API Keys page in the Close.com dashboard to access the secret key for your account. Secret key will be prefixed with `api_`.\nSee this guide if you need to create a new one.\nWe recommend creating a restricted key specifically for Airbyte access. This will allow you to control which resources Airbyte should be able to access. For ease of use, we recommend using read permissions for all resources and configuring which resource to replicate in the Airbyte UI.\nSetup guide\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the Close.com connector and select Close.com from the Source type dropdown.\nFill in the API Key and Start date fields and click Set up source.\n\nSupported sync modes\nThe Close.com source supports both Full Refresh and Incremental syncs. You can choose if this connector will copy only the new or updated data, or all rows in the tables and columns you set up for replication, every time a sync is run.\nSupported Streams\nThis Source is capable of syncing the following core Streams:\n\nLeads (Incremental)\nCreated Activities (Incremental)\nOpportunity Status Change Activities (Incremental)\nNote Activities (Incremental)\nMeeting Activities (Incremental)\nCall Activities (Incremental)\nEmail Activities (Incremental)\nEmail Thread Activities (Incremental)\nLead Status Change Activities (Incremental)\nSMS Activities (Incremental)\nTask Completed Activities (Incremental)\nLead Tasks (Incremental)\nIncoming Email Tasks (Incremental)\nEmail Followup Tasks (Incremental)\nMissed Call Tasks (Incremental)\nAnswered Detached Call Tasks (Incremental)\nVoicemail Tasks (Incremental)\nOpportunity Due Tasks (Incremental)\nIncoming SMS Tasks (Incremental)\nEvents (Incremental)\nLead Custom Fields\nContact Custom Fields\nOpportunity Custom Fields \nActivity Custom Fields \nUsers \nContacts \nOpportunities (Incremental)\nRoles \nLead Statuses \nOpportunity Statuses \nPipelines \nEmail Templates \nGoogle Connected Accounts \nCustom Email Connected Accounts \nZoom Connected Accounts \nSend As \nEmail Sequences \nDialer \nSmart Views \nEmail Bulk Actions \nSequence Subscription Bulk Actions \nDelete Bulk Actions \nEdit Bulk Actions \nIntegration Links \nCustom Activities \n\nNotes\nLeads, Events Incremental streams use `date_updated` field as a cursor. All other Incremental streams use `date_created` field for the same purpose.\n`SendAs` stream requires payment.\nData type mapping\nThe Close.com API uses the same JSONSchema types that Airbyte uses internally (`string`, `date-time`, `object`, `array`, `boolean`, `integer`, and `number`), so no type conversions happen as part of this source.\nPerformance considerations\nThe Close.com Connector has rate limit. There are 60 RPS for Organizations. You can find detailed info here.",
    "tag": "airbyte"
  },
  {
    "title": "SearchMetrics",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/search-metrics.md",
    "content": "SearchMetrics\nOverview\nThe SearchMetrics source supports both Full Refresh and Incremental syncs. You can choose if this connector will copy only the new or updated data, or all rows in the tables and columns you set up for replication, every time a sync is run.\nThis Source Connector is based on a Airbyte CDK.\nOutput schema\nSeveral output streams are available from this source:\n\nProjects (Full table)\nBenchmarkRankingsS7 (Full table)\nCompetitorRankingsS7 (Full table)\nDistributionKeywordsS7 (Full table)\nKeywordPotentialsS7 (Full table)\nListCompetitors (Full table)\nListCompetitorsRelevancy (Full table)\nListLosersS7 (Full table)\nListMarketShareS7 (Incremental)\nListPositionSpreadHistoricS7 (Incremental)\nListRankingsDomain (Full table)\nListRankingsHistoricS7 (Full table)\nListSeoVisibilityCountry (Full table)\nListSeoVisibilityHistoricS7 (Incremental)\nListSerpSpreadS7 (Full table)\nListWinnersS7 (Full table)\nSeoVisibilityValueS7 (Full table)\nSerpSpreadValueS7 (Full table)\nTagPotentialsS7 (Full table)\nTags (Full table)\nUrlRankingsS7 (Full table)\n\nIf there are more endpoints you'd like Airbyte to support, please create an issue.\nFeatures\n| Feature | Supported? |\n| :--- | :--- |\n| Full Refresh Sync | Yes |\n| Incremental - Append Sync | Yes |\n| SSL connection | Yes |\n| Namespaces | No |\nThe SearchMetrics connector should not run into SearchMetrics API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.\nGetting started\nRequirements\n\nSearchMetrics Client Secret\nSearchMetrics API Key\n\nSetup guide\nPlease read How to get your API Key and Client Secret .",
    "tag": "airbyte"
  },
  {
    "title": "Snapchat Marketing",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/snapchat-marketing.md",
    "content": "Snapchat Marketing\nThis page guides you through the process of setting up the Snapchat Marketing source connector.\nPrerequisites\n\nFor Airbyte Cloud:\n\nA Snapchat Marketing account with permission to access data from accounts you want to sync\n\n\n\nFor Airbyte Open Source:\n\nclient_id\nclient_secret\nrefresh_token\nstart_date\n\n\nSetup guide\nStep 1: Set up Snapchat\n\nSet up Snapchat Business account\n\n\nFor Airbyte Open Source:\n\nActivate Access to the Snapchat Marketing API \nAdd the OAuth2 app:\nAdding the OAuth2 app requires the `redirect_url` parameter.\nIf you have the API endpoint that will handle next OAuth process - write it to this parameter.\nIf not - just use some valid url. Here's the discussion about it: Snapchat Redirect URL - Clarity in documentation please\n\n\nsave Client ID and Client Secret\nGet refresh token using OAuth2 authentication workflow:\nOpen the authorize link in a browser: https://accounts.snapchat.com/login/oauth2/authorize?response_type=code&client_id={client_id}&redirect_uri={redirect_uri}&scope=snapchat-marketing-api&state=wmKkg0TWgppW8PTBZ20sldUmF7hwvU\nLogin & Authorize via UI\nLocate \"code\" query parameter in the redirect\nExchange code for access token + refresh token\n      `text\n      curl -X POST \\  \n      -d \"code={one_time_use_code}\" \\  \n      -d \"client_id={client_id}\" \\  \n      -d \"client_secret={client_secret}\"  \\  \n      -d \"grant_type=authorization_code\"  \\  \n      -d \"redirect_uri=redirect_uri\"  \n      https://accounts.snapchat.com/login/oauth2/access_token`\nYou will receive the API key and refresh token in response. Use this refresh token in the connector specifications.\nThe useful link to Authentication process is here\n\n\nStep 2: Set up the source connector in Airbyte\n\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click + new source.\nOn the source setup page, select Snapchat Marketing from the Source type dropdown and enter a name for this connector.\nlick `Authenticate your account`.\nLog in and Authorize to the Snapchat account\nChoose required Start date\nclick `Set up source`.\n\n\n\nFor Airbyte Open Source:\n\nGo to local Airbyte page.\nIn the left navigation bar, click Sources. In the top-right corner, click + new source.\nOn the source setup page, select Snapchat Marketing from the Source type dropdown and enter a name for this connector.\nAdd Client ID, Client Secret, Refresh Token\nChoose required Start date\nClick `Set up source`.\n\n\nSupported streams and sync modes\n| Stream                  | Incremental | Key                                 |\n| :---------------------- | :---------- | ----------------------------------- |\n| Adaccounts              | Yes         | \"id\"                                |\n| Ads                     | Yes         | \"id\"                                |\n| Adsquads                | Yes         | \"id\"                                |\n| Campaigns               | Yes         | \"id\"                                |\n| Creatives               | Yes         | \"id\"                                |\n| Media                   | Yes         | \"id\"                                |\n| Organizations           | No          | \"id\"                                |\n| Segments                | Yes         | \"id\"                                |\n| AdaccountsStatsHourly   | Yes         | [\"id\", \"granularity\", \"start_time\"] |\n| AdaccountsStatsDaily    | Yes         | [\"id\", \"granularity\", \"start_time\"] |\n| AdaccountsStatsLifetime | No          | [\"id\", \"granularity\"]               |\n| AdsStatsHourly          | Yes         | [\"id\", \"granularity\", \"start_time\"] |\n| AdsStatsDaily           | Yes         | [\"id\", \"granularity\", \"start_time\"] |\n| AdsStatsLifetime        | No          | [\"id\", \"granularity\"]               |\n| AdsquadsStatsHourly     | Yes         | [\"id\", \"granularity\", \"start_time\"] |\n| AdsquadsStatsDaily      | Yes         | [\"id\", \"granularity\", \"start_time\"] |\n| AdsquadsStatsLifetime   | No          | [\"id\", \"granularity\"]               |\n| CampaignsStatsHourly    | Yes         | [\"id\", \"granularity\", \"start_time\"] |\n| CampaignsStatsDaily     | Yes         | [\"id\", \"granularity\", \"start_time\"] |\n| CampaignsStatsLifetime  | No          | [\"id\", \"granularity\"]               |\nPerformance considerations\nHourly streams can be slowly because they generate a lot of records.\nSnapchat Marketing API has limitations to 1000 items per page.",
    "tag": "airbyte"
  },
  {
    "title": "Instatus",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/instatus.md",
    "content": "Instatus\nThis page contains the setup guide and reference information for the Instatus source connector.\nPrerequisites\nTo set up Metabase you need:\n  * `api_key` - Requests to Instatus API must provide an API token.\nSetup guide\nStep 1: Set up Instatus account\nStep 2: Generate an API key\nYou can get your API key from User settings\nMake sure that you are an owner of the pages you want to sync because if you are not this data will be skipped.\nStep 2: Set up the Instatus connector in Airbyte\nSupported sync modes\nThe Instatus source connector supports the following sync modes:\n\nFull Refresh - Overwrite\n\nSupported Streams\n\nStatus pages\nComponents\nIncidents\nIncident updates\nMaintenances\nMaintenance updates\nTemplates\nTeam\nSubscribers\nMetrics\nUser\nPublic data\n\nTutorials\nData type mapping\n| Integration Type    | Airbyte Type | Notes |\n|:--------------------|:-------------|:------|\n| `string`            | `string`     |       |\n| `integer`, `number` | `number`     |       |\n| `array`             | `array`      |       |\n| `object`            | `object`     |       |\nFeatures\n| Feature           | Supported?(Yes/No) | Notes |\n|:------------------|:---------------------|:------|\n| Full Refresh Sync | Yes                  |       |\n| Incremental Sync  | No                   |       |\n| SSL connection    | Yes                  |\n| Namespaces        | No                   |       |",
    "tag": "airbyte"
  },
  {
    "title": "Chargify",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/chargify.md",
    "content": "Chargify\nOverview\nThe Chargify source supports Full Refresh syncs for Customers and Subscriptions endpoints.\nAvailable streams\nSeveral output streams are available from this source:\n\nCustomers\nSubscriptions\n\nIf there are more streams you'd like Airbyte to support, please create an issue.\nFeatures\n| Feature | Supported? |\n| :--- | :--- |\n| Full Refresh Sync | Yes |\n| Incremental Sync | No |\n| Replicate Incremental Deletes | No |\n| SSL connection | Yes |\n| Namespaces | No |\nPerformance considerations\nThe Chargify connector should not run into Chargify API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.\nGetting started\nRequirements\n\nChargify API Key\nChargify domain\n\nSetup guide\nPlease follow the Chargify documentation for generating an API key.",
    "tag": "airbyte"
  },
  {
    "title": "Sendgrid",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/sendgrid.md",
    "content": "Sendgrid\nThis page contains the setup guide and reference information for the Sendgrid source connector.\nPrerequisites\n\nAPI Key\n\nSetup guide\nStep 1: Set up Sendgrid\n\nSendgrid Account\nCreate Sendgrid API Key with the following permissions:\nRead-only access to all resources\nFull access to marketing resources\n\nStep 2: Set up the Sendgrid connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the Sendgrid connector and select Sendgrid from the Source type dropdown.\nEnter your `apikey`.\nEnter your `start_time`. \nClick Set up source.\n\nFor Airbyte OSS:\n\nNavigate to the Airbyte Open Source dashboard.\nSet the name for your source. \nEnter your `apikey`.\nEnter your `start_time`. \nClick Set up source.\n\nSupported sync modes\nThe Sendgrid source connector supports the following sync modes:\n\nFull Refresh - Overwrite\nFull Refresh - Append\nIncremental - Append\n\nSupported Streams\n\nCampaigns \nLists \nContacts \nStats automations \nSegments \nSingle Sends \nTemplates \nGlobal suppression (Incremental)\nSuppression groups\nSuppression group members \nBlocks (Incremental)\nBounces (Incremental)\nInvalid emails (Incremental)\nSpam reports\n\nConnector-specific features & highlights, if any\nWe recommend creating a key specifically for Airbyte access. This will allow you to control which resources Airbyte should be able to access. The API key should be read-only on all resources except Marketing, where it needs Full Access.\nSendgrid provides two different kinds of marketing campaigns, \"legacy marketing campaigns\" and \"new marketing campaigns\". Legacy marketing campaigns are not supported by this source connector. \nIf you are seeing a `403 FORBIDDEN error message for https://api.sendgrid.com/v3/marketing/campaigns`, it might be because your SendGrid account uses legacy marketing campaigns.\nPerformance considerations\nThe connector is restricted by normal Sendgrid requests limitation.",
    "tag": "airbyte"
  },
  {
    "title": "Glassfrog",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/glassfrog.md",
    "content": "Glassfrog\nSync overview\nThe Glassfrog source supports only Full Refresh syncs. This source can sync data for the Glassfrog API.\nThis Source Connector is based on the Airbyte CDK.\nOutput schema\nThis Source is capable of syncing the following Streams:\n\nAssignments\nChecklist items\nCircles\nCustom fields\nMetrics\nPeople\nProjects\nRoles\n\nData type mapping\n| Integration Type | Airbyte Type | Notes |\n| :--- | :--- | :--- |\n| `string` | `string` |  |\n| `number` | `number` |  |\n| `array` | `array` |  |\n| `object` | `object` |  |\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental - Append Sync | No |  |\n| Namespaces | No |  |\nGetting started\n\nSign in at `app.glassfrog.com`.\nGo to `Profile & Settings`.\nIn the API tab, enter the label for your new API key (e.g. `Airbyte`) and clik on the button `Create new API Key`.\nUse the created secret key to configure your source!\n",
    "tag": "airbyte"
  },
  {
    "title": "Appstore",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/appstore-singer.md",
    "content": "Appstore\nSync overview\nThis source can sync data for the Appstore API. It supports only Incremental syncs. The Appstore API is available for many types of services. Currently, this API supports syncing Sales and Trends reports. If you'd like to sync data from other endpoints, please create an issue on Github.\nThis Source Connector is based on a Singer Tap.\nOutput schema\nThis Source is capable of syncing the following \"Sales and Trends\" Streams:\n\nSALES\nSUBSCRIPTION\nSUBSCRIPTION_EVENT\nSUBSCRIBER\n\nNote that depending on the credentials you enter, you may only be able to sync some of these reports. For example, if your app does not offer subscriptions, then it is not possible to sync subscription related reports.\nData type mapping\n| Integration Type | Airbyte Type | Notes |\n| :--- | :--- | :--- |\n| `string` | `string` |  |\n| `int`, `float`, `number` | `number` |  |\n| `date` | `date` |  |\n| `datetime` | `datetime` |  |\n| `array` | `array` |  |\n| `object` | `object` |  |\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | no |  |\n| Incremental Sync | yes |  |\n| Namespaces | No |  |\nPerformance considerations\nThe connector is restricted by normal Appstore requests limitation.\nThe Appstore connector should not run into Appstore API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.\nOne issue that can happen is the API not having the data available for the period requested, either because you're trying to request data older than 365 days or the today's and yesterday's data was not yet made available to be requested.\nGetting started\nRequirements\n\nKey ID\nPrivate Key The contents of the private API key file, which is in the P8 format and should start with `-----BEGIN PRIVATE KEY-----` and end with `-----END PRIVATE KEY-----`.\nIssuer ID\nVendor ID Go to \"Sales and Trends\", then choose \"Reports\" from the drop-down menu in the top left. On the next screen, there'll be a drop-down menu for \"Vendor\". Your name and ID will be shown there. Use the numeric Vendor ID.\nStart Date (The date that will be used in the first sync. Apple only allows to go back 365 days from today.) Example: `2020-11-16T00:00:00Z`\n\nSetup guide\nGenerate/Find all requirements using this external article.",
    "tag": "airbyte"
  },
  {
    "title": "Lever Hiring",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/lever-hiring.md",
    "content": "Lever Hiring\nSync overview\nThe Lever Hiring source supports both Full Refresh and Incremental syncs. You can choose if this connector will copy only the new or updated data, or all rows in the tables and columns you set up for replication, every time a sync is run.\nThis source can sync data for the Lever Hiring API.\nOutput schema\nThis Source is capable of syncing the following core Streams:\n\nApplications\nInterviews\nNotes\nOffers\nOpportunities\nReferrals\nUsers\n\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental - Append Sync | Yes |  |\n| SSL connection | Yes |  |\n| Namespaces | No |  |\nPerformance considerations\nThe Lever Hiring connector should not run into Lever Hiring API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.\nGetting started\nRequirements\n\nLever Hiring Client Id\nLever Hiring Client Secret\nLever Hiring Refresh Token\n",
    "tag": "airbyte"
  },
  {
    "title": "Postgres",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/postgres.md",
    "content": "Postgres\nThis page contains the setup guide and reference information for the Postgres source connector for CDC and non-CDC workflows.\n:::note\nThe Postgres source performs best on small databases (under 100GB).\n:::\nWhen to use Postgres with CDC\nConfigure Postgres with CDC if:\n\nYou need a record of deletions\nYour table has a primary key but doesn't have a reasonable cursor field for incremental syncing (`updated_at`). CDC allows you to sync your table incrementally\n\nIf your goal is to maintain a snapshot of your table in the destination but the limitations prevent you from using CDC, consider using non-CDC incremental sync and occasionally reset the data and re-sync.\nIf your dataset is small and you just want a snapshot of your table in the destination, consider using Full Refresh replication for your table instead of CDC.\nPrerequisites\n\nFor Airbyte Open Source users, upgrade your Airbyte platform to version `v0.40.0-alpha` or newer\nUse Postgres v9.3.x or above for non-CDC workflows and Postgres v10 or above for CDC workflows\nFor Airbyte Cloud (and optionally for Airbyte Open Source), ensure SSL is enabled in your environment\n\nSetup guide\nStep 1: (Optional) Create a dedicated read-only user\nWe recommend creating a dedicated read-only user for better permission control and auditing. Alternatively, you can use an existing Postgres user in your database.\nTo create a dedicated user, run the following command:\n`CREATE USER <user_name> PASSWORD 'your_password_here';`\nGrant access to the relevant schema:\n`GRANT USAGE ON SCHEMA <schema_name> TO <user_name>`\n:::note\nTo replicate data from multiple Postgres schemas, re-run the command to grant access to all the relevant schemas. Note that you'll need to set up multiple Airbyte sources connecting to the same Postgres database on multiple schemas.\n:::\nGrant the user read-only access to the relevant tables:\n`GRANT SELECT ON ALL TABLES IN SCHEMA <schema_name> TO <user_name>;`\nAllow user to see tables created in the future:\n`ALTER DEFAULT PRIVILEGES IN SCHEMA <schema_name> GRANT SELECT ON TABLES TO <user_name>;`\nAdditionally, if you plan to configure CDC for the Postgres source connector, grant `REPLICATION` permissions to the user:\n`ALTER USER <user_name> REPLICATION;`\nSyncing a subset of columns\u200b\nCurrently, there is no way to sync a subset of columns using the Postgres source connector:\n\nWhen setting up a connection, you can only choose which tables to sync, but not columns.\nIf the user can only access a subset of columns, the connection check will pass. However, the data sync will fail with a permission denied exception.\n\nThe workaround for partial table syncing is to create a view on the specific columns, and grant the user read access to that view:\n`CREATE VIEW <view_name> as SELECT <columns> FROM <table>;`\n`GRANT SELECT ON TABLE <view_name> IN SCHEMA <schema_name> to <user_name>;`\nNote: The workaround works only for non-CDC setups since CDC requires data to be in tables and not views.\nThis issue is tracked in #9771.\nStep 2: Set up the Postgres connector in Airbyte\n\nLog into your Airbyte Cloud or Airbyte Open Source account.\nClick Sources and then click + New source.\nOn the Set up the source page, select Postgres from the Source type dropdown.\nEnter a name for your source.\nFor the Host, Port, and DB Name, enter the hostname, port number, and name for your Postgres database.\nList the Schemas you want to sync.\n   :::note\n   The schema names are case sensitive. The 'public' schema is set by default. Multiple schemas may be used at one time. No schemas set explicitly - will sync all of existing.\n   :::\nFor User and Password, enter the username and password you created in Step 1.\nTo customize the JDBC connection beyond common options, specify additional supported JDBC URL parameters as key-value pairs separated by the symbol & in the JDBC URL Parameters (Advanced) field.\n\nExample: key1=value1&key2=value2&key3=value3\nThese parameters will be added at the end of the JDBC URL that the AirByte will use to connect to your Postgres database.\nThe connector now supports `connectTimeout` and defaults to 60 seconds. Setting connectTimeout to 0 seconds will set the timeout to the longest time available.\nNote: Do not use the following keys in JDBC URL Params field as they will be overwritten by Airbyte:\n   `currentSchema`, `user`, `password`, `ssl`, and `sslmode`.\n:::warning\n   This is an advanced configuration option. Users are advised to use it with caution.\n   :::\n\nFor Airbyte Open Source, toggle the switch to connect using SSL. For Airbyte Cloud uses SSL by default.\nFor SSL Modes, select:\ndisable to disable encrypted communication between Airbyte and the source\nallow to enable encrypted communication only when required by the source\nprefer to allow unencrypted communication only when the source doesn't support encryption\nrequire to always require encryption. Note: The connection will fail if the source doesn't support encryption.\nverify-ca to always require encryption and verify that the source has a valid SSL certificate\nverify-full to always require encryption and verify the identity of the source\n\n\nFor Replication Method, select Standard or Logical CDC from the dropdown. Refer to Configuring Postgres connector with Change Data Capture (CDC) for more information.\n\nFor SSH Tunnel Method, select:\n\nNo Tunnel for a direct connection to the database\nSSH Key Authentication to use an RSA Private as your secret for establishing the SSH tunnel\nPassword Authentication to use a password as your secret for establishing the SSH tunnel\n\n:::warning\nSince Airbyte Cloud requires encrypted communication, select SSH Key Authentication or Password Authentication if you selected disable, allow, or prefer as the SSL Mode; otherwise, the connection will fail.\n:::\n\n\nRefer to Connect via SSH Tunnel for more information. 13. Click Set up source.\nConnect via SSH Tunnel\u200b\nYou can connect to a Postgres instance via an SSH tunnel.\nWhen using an SSH tunnel, you are configuring Airbyte to connect to an intermediate server (also called a bastion or a jump server) that has direct access to the database. Airbyte connects to the bastion and then asks the bastion to connect directly to the server.\nTo connect to a Postgres instance via an SSH tunnel:\n\nWhile setting up the Postgres source connector, from the SSH tunnel dropdown, select:\nSSH Key Authentication to use a private as your secret for establishing the SSH tunnel\nPassword Authentication to use a password as your secret for establishing the SSH Tunnel\nFor SSH Tunnel Jump Server Host, enter the hostname or IP address for the intermediate (bastion) server that Airbyte will connect to.\nFor SSH Connection Port, enter the port on the bastion server. The default port for SSH connections is 22.\nFor SSH Login Username, enter the username to use when connecting to the bastion server. Note: This is the operating system username and not the Postgres username.\nFor authentication:\nIf you selected SSH Key Authentication, set the SSH Private Key to the private Key that you are using to create the SSH connection.\nIf you selected Password Authentication, enter the password for the operating system user to connect to the bastion server. Note: This is the operating system password and not the Postgres password.\n\nGenerating a private Key\u200b\nThe connector supports any SSH compatible key format such as RSA or Ed25519. To generate an RSA key, for example, run:\n`ssh-keygen -t rsa -m PEM -f myuser_rsa`\nThe command produces the private key in PEM format and the public key remains in the standard format used by the `authorized_keys` file on your bastion server. Add the public key to your bastion host to the user you want to use with Airbyte. The private key is provided via copy-and-paste to the Airbyte connector configuration screen to allow it to log into the bastion server.\nConfiguring Postgres connector with Change Data Capture (CDC)\nAirbyte uses logical replication of the Postgres write-ahead log (WAL) to incrementally capture deletes using a replication plugin. To learn more how Airbyte implements CDC, refer to Change Data Capture (CDC)\nCDC Considerations\n\nIncremental sync is only supported for tables with primary keys. For tables without primary keys, use Full Refresh sync.\nData must be in tables and not views. If you require data synchronization from a view, you would need to create a new connection with `Standard` as `Replication Method`.\nThe modifications you want to capture must be made using `DELETE`/`INSERT`/`UPDATE`. For example, changes made using `TRUNCATE`/`ALTER` will not appear in logs and therefore in your destination.\nSchema changes are not supported automatically for CDC sources. Reset and resync data if you make a schema change.\nThe records produced by `DELETE` statements only contain primary keys. All other data fields are unset.\nLog-based replication only works for master instances of Postgres.  CDC cannot be run from a read-replica of your primary database.\nUsing logical replication increases disk space used on the database server. The additional data is stored until it is consumed.\nSet frequent syncs for CDC to ensure that the data doesn't fill up your disk space.\nIf you stop syncing a CDC-configured Postgres instance with Airbyte, delete the replication slot. Otherwise, it may fill up your disk space.\n\nSetting up CDC for Postgres\u200b\nAirbyte requires a replication slot configured only for its use. Only one source should be configured that uses this replication slot. See Setting up CDC for Postgres for instructions.\nStep 1: Enable logical replication\u200b\nTo enable logical replication on bare metal, VMs (EC2/GCE/etc), or Docker, configure the following parameters in the postgresql.conf file for your Postgres database:\n| Parameter             | Description                                                                    | Set value to                                                                                                                       |\n|-----------------------|--------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------|\n| wal_level             | Type of coding used within the Postgres write-ahead log                        | logical                                                                                                                            |\n| max_wal_senders       | The maximum number of processes used for handling WAL changes                  | Min: 1                                                                                                                             |\n| max_replication_slots | The maximum number of replication slots that are allowed to stream WAL changes | 1 (if Airbyte is the only service reading subscribing to WAL changes. More than 1 if other services are also reading from the WAL) |\nTo enable logical replication on AWS Postgres RDS or Aurora\u200b:\n\nGo to the Configuration tab for your DB cluster.\nFind your cluster parameter group. Either edit the parameters for this group or create a copy of this parameter group to edit. If you create a copy, change your cluster's parameter group before restarting.\nWithin the parameter group page, search for `rds.logical_replication`. Select this row and click Edit parameters. Set this value to 1.\nWait for a maintenance window to automatically restart the instance or restart it manually.\n\nTo enable logical replication on Azure Database for Postgres\u200b:\nChange the replication mode of your Postgres DB on Azure to `logical` using the Replication menu of your PostgreSQL instance in the Azure Portal. Alternatively, use the Azure CLI to run the following command:\n`az postgres server configuration set --resource-group group --server-name server --name azure.replication_support --value logical`\n`az postgres server restart --resource-group group --name server`\nStep 2: Select a replication plugin\u200b\nWe currently support two plugins :\n1. pgoutput plugin (the standard logical decoding plugin in Postgres).\n2. wal2json plugin. Please note that the wal2json plugin has been deprecated and we will remove support for it in the near future. We strongly advice against using this plugin.\nStep 3: Create replication slot\u200b\nTo create a replication slot called `airbyte_slot` using pgoutput, run:\n`SELECT pg_create_logical_replication_slot('airbyte_slot', 'pgoutput');`\nTo create a replication slot called `airbyte_slot` using wal2json, run:\n`SELECT pg_create_logical_replication_slot('airbyte_slot', 'wal2json');`\nStep 4: Create publications and replication identities for tables\u200b\nFor each table you want to replicate with CDC, add the replication identity (the method of distinguishing between rows) first:\nTo use primary keys to distinguish between rows for tables that don't have a large amount of data per row, run:\n`ALTER TABLE tbl1 REPLICA IDENTITY DEFAULT;`\nIn case your tables use data types that support TOAST and have very large field values, use:\n`ALTER TABLE tbl1 REPLICA IDENTITY FULL;`\nAfter setting the replication identity, run:\n`CREATE PUBLICATION airbyte_publication FOR TABLE <tbl1, tbl2, tbl3>;``\nThe publication name is customizable. Refer to the Postgres docs if you need to add or remove tables from your publication in the future.\n:::note\nYou must add the replication identity before creating the publication. Otherwise, `ALTER`/`UPDATE`/`DELETE` statements may fail if Postgres cannot determine how to uniquely identify rows.\nAlso, the publication should include all the tables and only the tables that need to be synced. Otherwise, data from these tables may not be replicated correctly.\n:::\n:::warning\nThe Airbyte UI currently allows selecting any tables for CDC. If a table is selected that is not part of the publication, it will not be replicated even though it is selected. If a table is part of the publication but does not have a replication identity, that replication identity will be created automatically on the first run if the Airbyte user has the necessary permissions.\n:::\nStep 5: [Optional] Set up initial waiting time\n:::warning\nThis is an advanced feature. Use it if absolutely necessary.\n:::\nThe Postgres connector may need some time to start processing the data in the CDC mode in the following scenarios:\n\nWhen the connection is set up for the first time and a snapshot is needed\nWhen the connector has a lot of change logs to process\n\nThe connector waits for the default initial wait time of 5 minutes (300 seconds). Setting the parameter to a longer duration will result in slower syncs, while setting it to a shorter duration may cause the connector to not have enough time to create the initial snapshot or read through the change logs. The valid range is 120 seconds to 1200 seconds.\nIf you know there are database changes to be synced, but the connector cannot read those changes, the root cause may be insufficient waiting time. In that case, you can increase the waiting time (example: set to 600 seconds) to test if it is indeed the root cause. On the other hand, if you know there are no database changes, you can decrease the wait time to speed up the zero record syncs.\nStep 6: Set up the Postgres source connector\nIn Step 2 of the connector setup guide, enter the replication slot and publication you just created.\nSupported sync modes\nThe Postgres source connector supports the following sync modes:\n\nFull Refresh - Overwrite\nFull Refresh - Append\nIncremental Sync - Append\nIncremental Sync - Deduped History\n\nSupported cursors\n\n`TIMESTAMP`\n`TIMESTAMP_WITH_TIMEZONE`\n`TIME`\n`TIME_WITH_TIMEZONE`\n`DATE`\n`BIT`\n`BOOLEAN`\n`TINYINT/SMALLINT`\n`INTEGER`\n`BIGINT`\n`FLOAT/DOUBLE`\n`REAL`\n`NUMERIC/DECIMAL`\n`CHAR/NCHAR/NVARCHAR/VARCHAR/LONGVARCHAR`\n`BINARY/BLOB`\n\nData type mapping\nAccording to Postgres documentation, Postgres data types are mapped to the following data types when synchronizing data. You can check the test values examples here. If you can't find the data type you are looking for or have any problems feel free to add a new test!\n| Postgres Type                         | Resulting Type | Notes                                                                                                                                                 |\n|---------------------------------------|----------------|-------------------------------------------------------------------------------------------------------------------------------------------------------|\n| `bigint`                              | number         |                                                                                                                                                       |\n| `bigserial`, `serial8`                | number         |                                                                                                                                                       |\n| `bit`                                 | string         | Fixed-length bit string (e.g. \"0100\").                                                                                                                |\n| `bit varying`, `varbit`               | string         | Variable-length bit string (e.g. \"0100\").                                                                                                             |\n| `boolean`, `bool`                     | boolean        |                                                                                                                                                       |\n| `box`                                 | string         |                                                                                                                                                       |\n| `bytea`                               | string         | Variable length binary string with hex output format prefixed with \"\\x\" (e.g. \"\\x6b707a\").                                                            |\n| `character`, `char`                   | string         |                                                                                                                                                       |\n| `character varying`, `varchar`        | string         |                                                                                                                                                       |\n| `cidr`                                | string         |                                                                                                                                                       |\n| `circle`                              | string         |                                                                                                                                                       |\n| `date`                                | string         | Parsed as ISO8601 date time at midnight. CDC mode doesn't support era indicators. Issue: #14590  |\n| `double precision`, `float`, `float8` | number         | `Infinity`, `-Infinity`, and `NaN` are not supported and converted to `null`. Issue: #8902.       |\n| `hstore`                              | string         |                                                                                                                                                       |\n| `inet`                                | string         |                                                                                                                                                       |\n| `integer`, `int`, `int4`              | number         |                                                                                                                                                       |\n| `interval`                            | string         |                                                                                                                                                       |\n| `json`                                | string         |                                                                                                                                                       |\n| `jsonb`                               | string         |                                                                                                                                                       |\n| `line`                                | string         |                                                                                                                                                       |\n| `lseg`                                | string         |                                                                                                                                                       |\n| `macaddr`                             | string         |                                                                                                                                                       |\n| `macaddr8`                            | string         |                                                                                                                                                       |\n| `money`                               | number         |                                                                                                                                                       |\n| `numeric`, `decimal`                  | number         | `Infinity`, `-Infinity`, and `NaN` are not supported and converted to `null`. Issue: #8902.       |\n| `path`                                | string         |                                                                                                                                                       |\n| `pg_lsn`                              | string         |                                                                                                                                                       |\n| `point`                               | string         |                                                                                                                                                       |\n| `polygon`                             | string         |                                                                                                                                                       |\n| `real`, `float4`                      | number         |                                                                                                                                                       |\n| `smallint`, `int2`                    | number         |                                                                                                                                                       |\n| `smallserial`, `serial2`              | number         |                                                                                                                                                       |\n| `serial`, `serial4`                   | number         |                                                                                                                                                       |\n| `text`                                | string         |                                                                                                                                                       |\n| `time`                                | string         | Parsed as a time string without a time-zone in the ISO-8601 calendar system.                                                                          |\n| `timetz`                              | string         | Parsed as a time string with time-zone in the ISO-8601 calendar system.                                                                               |\n| `timestamp`                           | string         | Parsed as a date-time string without a time-zone in the ISO-8601 calendar system.                                                                     |\n| `timestamptz`                         | string         | Parsed as a date-time string with time-zone in the ISO-8601 calendar system.                                                                          |\n| `tsquery`                             | string         |                                                                                                                                                       |\n| `tsvector`                            | string         |                                                                                                                                                       |\n| `uuid`                                | string         |                                                                                                                                                       |\n| `xml`                                 | string         |                                                                                                                                                       |\n| `enum`                                | string         |                                                                                                                                                       |\n| `tsrange`                             | string         |                                                                                                                                                       |\n| `array`                               | array          | E.g. \"[\\\"10001\\\",\\\"10002\\\",\\\"10003\\\",\\\"10004\\\"]\".                                                                                                     |\n| composite type                        | string         |                                                                                                                                                       |\nLimitations\n\nThe Postgres source connector currently does not handle schemas larger than 4MB.\nThe Postgres source connector does not alter the schema present in your database. Depending on the destination connected to this source, however, the schema may be altered. See the destination's documentation for more details.\nThe following two schema evolution actions are currently supported:\nAdding/removing tables without resetting the entire connection at the destination\n    Caveat: In the CDC mode, adding a new table to a connection may become a temporary bottleneck. When a new table is added, the next sync job takes a full snapshot of the new table before it proceeds to handle any changes.\nResetting a single table within the connection without resetting the rest of the destination tables in that connection\nChanging a column data type or removing a column might break connections.\n\nTroubleshooting\nSync data from Postgres hot standby server\nWhen the connector is reading from a Postgres replica that is configured as a Hot Standby, any update from the primary server will terminate queries on the replica after a certain amount of time, default to 30 seconds. This default waiting time is not enough to sync any meaning amount of data. See the `Handling Query Conflicts` section in the Postgres documentation for detailed explanation.\nHere is the typical exception:\n`Caused by: org.postgresql.util.PSQLException: FATAL: terminating connection due to conflict with recovery\n    Detail: User query might have needed to see row versions that must be removed.\n    Hint: In a moment you should be able to reconnect to the database and repeat your command.`\nPossible solutions include:\n\n[Recommended] Set hot_standby_feedback to `true` on the replica server. This parameter will prevent the primary server from deleting the write-ahead logs when the replica is busy serving user queries. However, the downside is that the write-ahead log will increase in size.\n[Recommended] Sync data when there is no update running in the primary server, or sync data from the primary server.\n[Not Recommended] Increase max_standby_archive_delay and max_standby_streaming_delay to be larger than the amount of time needed to complete the data sync. However, it is usually hard to tell how much time it will take to sync all the data. This approach is not very practical.\n\nUnder CDC incremental mode, there are still full refresh syncs\nNormally under the CDC mode, the Postgres source will first run a full refresh sync to read the snapshot of all the existing data, and all subsequent runs will only be incremental syncs reading from the write-ahead logs (WAL). However, occasionally, you may see full refresh syncs after the initial run. When this happens, you will see the following log:\n\nSaved offset is before Replication slot's confirmed_flush_lsn, Airbyte will trigger sync from scratch\n\nThe root causes is that the WALs needed for the incremental sync has been removed by Postgres. This can occur under the following scenarios:\n\nWhen there are lots of database updates resulting in more WAL files than allowed in the `pg_wal` directory, Postgres will purge or archive the WAL files. This scenario is preventable. Possible solutions include:\nSync the data source more frequently. The downside is that more computation resources will be consumed, leading to a higher Airbyte bill.\nSet a higher `wal_keep_size`. If no unit is provided, it is in megabytes, and the default is `0`. See detailed documentation here. The downside of this approach is that more disk space will be needed.\nWhen the Postgres connector successfully reads the WAL and acknowledges it to Postgres, but the destination connector fails to consume the data, the Postgres connector will try to read the same WAL again, which may have been removed by Postgres, since the WAL record is already acknowledged. This scenario is rare, because it can happen, and currently there is no way to prevent it. The correct behavior is to perform a full refresh.\n",
    "tag": "airbyte"
  },
  {
    "title": "Workramp",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/workramp.md",
    "content": "Workramp\nSync overview\nThe Workramp source supports both Full Refresh only.\nThis source can sync data for the Workramp API.\nOutput schema\nThis Source is capable of syncing the following core Streams:\n\nawarded_certifications\ncertifications\npaths_users\nregistrations\nusers\ntrainings\n\nFeatures\n| Feature                   | Supported?(Yes/No) | Notes |\n| :------------------------ | :------------------- | :---- |\n| Full Refresh Sync         | Yes                  |       |\n| Incremental - Append Sync | No                   |       |\n| Namespaces                | No                   |       |\nPerformance considerations\nThe Workramp connector should not run into Workramp API limitations under normal usage.\nRequirements\n\nWorkramp API ke. See the Workramp docs for information on how to obtain an API key.\n",
    "tag": "airbyte"
  },
  {
    "title": "Dixa",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/dixa.md",
    "content": "Dixa\nSync overview\nThis source can sync data for the Dixa conversation_export API. It supports both Full Refresh and Incremental syncs. You can choose if this connector will copy only the new or updated data, or all rows in the tables and columns you set up for replication, every time a sync is run.\nOutput schema\nThis Source is capable of syncing the following Streams:\n\nConversation export\n\nData type mapping\n| Integration Type | Airbyte Type | Notes |\n| :--------------- | :----------- | :---- |\n| `string`         | `string`     |       |\n| `int`            | `integer`    |       |\n| `timestamp`      | `integer`    |       |\n| `array`          | `array`      |       |\nFeatures\n| Feature           | Supported?(Yes/No) | Notes |\n| :---------------- | :------------------- | :---- |\n| Full Refresh Sync | Yes                  |       |\n| Incremental Sync  | Yes                  |       |\n| Namespaces        | No                   |       |\nPerformance considerations\nThe connector is limited by standard Dixa conversation_export API limits. It should not run into limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.\nWhen using the connector, keep in mind that increasing the `batch_size` parameter will decrease the number of requests sent to the API, but increase the response and processing time.\nGetting started\nRequirements\n\nDixa API token\n\nSetup guide\n\nGenerate an API token using the Dixa documentation.\nDefine a `start_timestamp`: the connector will pull records with `updated_at >= start_timestamp`\nDefine a `batch_size`: this represents the number of days which will be batched in a single request.\n\nKeep the performance consideration above in mind",
    "tag": "airbyte"
  },
  {
    "title": "Pocket",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/pocket.md",
    "content": "Pocket\nOverview\nThe Pocket source connector only supports full refresh syncs\nOutput schema\nA single output stream is available from this source:\n\nRetrieve\n\nFeatures\n| Feature           | Supported? |\n|:------------------|:-----------|\n| Full Refresh Sync | Yes        |\n| Incremental Sync  | No         |\nPerformance considerations\nFor more info on rate limiting, please refer to Pocket Docs > Rate Limits\nGetting started\nRequirements\n\nConsumer Key\nAccess Token\n\nSetup Guide\nIn order to obtain the Consumer Key and Access Token, please follow the official Pocket Authentication docs.\nIt's nevertheless, very recommended to follow this guide by James Mackenzie, which is summarized below:\n\nCreate an App in the Pocket Developer Portal, give it Retrieve permissions and get your Consumer Key.\nObtain a Request Token. To do so, you need to issue a POST request to get a temporary Request Token. You can execute the command below:\n`sh\ncurl --insecure -X POST -H 'Content-Type: application/json' -H 'X-Accept: application/json' \\\n    https://getpocket.com/v3/oauth/request  -d '{\"consumer_key\":\"REPLACE-ME\",\"redirect_uri\":\"http://www.google.com\"}'`\nVisit the following website from your browser, and authorize the app: `https://getpocket.com/auth/authorize?request_token=REPLACE-ME&redirect_uri=http://www.google.com`\nConvert your Request Token Into a Pocket Access Token. To do so, you can execute the following command:\n`sh\ncurl --insecure -X POST -H 'Content-Type: application/json' -H 'X-Accept: application/json' \\\n    https://getpocket.com/v3/oauth/authorize  -d '{\"consumer_key\":\"REPLACE-ME\",\"code\":\"REQUEST-TOKEN\"}'`\n",
    "tag": "airbyte"
  },
  {
    "title": "Azure Table Storage",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/azure-table.md",
    "content": "Azure Table Storage\nOverview\nThe Azure table storage supports Full Refresh and Incremental syncs. You can choose which tables you want to replicate.\nOutput schema\nThis Source have generic schema for all streams.\nAzure Table storage is a service that stores non-relational structured data (also known as structured NoSQL data). There is no efficient way to read schema for the given table. We use `data` property to have all the properties for any given row. \n\ndata - This property contain all values\nadditionalProperties - This property denotes that all the values are in `data` property.\n\n`{\n    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"data\": {\n            \"type\": \"object\"\n        },\n        \"additionalProperties\": {\n            \"type\": \"boolean\"\n        }\n    }\n}`\nData type mapping\nAzure Table Storage uses different property types and Airbyte uses internally (`string`, `date-time`, `object`, `array`, `boolean`, `integer`, and `number`). We don't apply any explicit data type mappings.\nFeatures\n| Feature                   | Supported? |\n| :------------------------ | :--------- |\n| Full Refresh Sync         | Yes        |\n| Incremental - Append Sync | Yes        |\n| Incremental - Dedupe Sync | No         |\n| SSL connection            | Yes        |\n| Namespaces                | No         |\nPerformance considerations\nThe Azure table storage connector should not run into API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.\nGetting started\nRequirements\n\nAzure Storage Account\nAzure Storage Account Key\nAzure Storage Endpoint Suffix\n\nSetup guide\nVisit the Azure Portal. Go to your storage account, you can find :\n - Azure Storage Account - under the overview tab\n - Azure Storage Account Key - under the Access keys tab\n - Azure Storage Endpoint Suffix - under the Enpoint tab\nWe recommend creating a restricted key specifically for Airbyte access. This will allow you to control which resources Airbyte should be able to access. However, shared access key authentication is not supported by this connector yet.",
    "tag": "airbyte"
  },
  {
    "title": "PrestaShop",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/prestashop.md",
    "content": "PrestaShop\nThis page contains the setup guide and reference information for the PrestaShop source connector.\nPrerequisites\n\nAccess Key\nShop URL\nStart date\n\nSetup guide\nStep 1: Set up PrestaShop\n\nBy default, the webservice feature is disabled on PrestaShop and needs to be switched on.\nTo get access to the PrestaShop API you need to create an access key, please follow the instructions in this documentation.\n\nStep 2: Set up the PrestaShop connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click + new source.\nOn the source setup page, select PrestaShop from the Source type dropdown and enter a name for this connector.\nEnter the Access Key that you obtained.\nEnter the Shop URL without trailing slash, for example, https://my.prestashop.com, only HTTPS urls are supported.\nEnter Start date in YYYY-MM-DD format. The data added on and after this date will be replicated.\n\nSupported sync modes\nThe PrestaShop source connector supports the following  sync modes:\n\nFull Refresh - Overwrite\nFull Refresh - Append\nIncremental - Append\nIncremental - Deduped History\n\nSupported Streams\nThis connector outputs the following full refresh streams:\n\nCarriers\nCombinations\nContacts\nContent Management System\nCountries\nCurrencies\nDeliveries\nEmployees\nGuests\nImage Types\nLanguages\nOrder Details\nOrder States\nPrice Ranges\nProduct Customization Fields\nProduct Feature Values\nProduct Features\nProduct Option Values\nProduct Suppliers\nShopGroups\nShopUrls\nShops\nSpecific Price Rules\nSpecific Prices\nStates\nStock Availables\nTags\nTax Rules\nTaxes\nTranslated Configurations\nWeight Ranges\nZones\n\nThis connector outputs the following incremental streams:\n\nAddresses\nCart Rules\nCarts\nCategories\nConfigurations\nCustomer Messages\nCustomer Threads\nCustomers\nGroups\nManufacturers\nMessages\nOrder Carriers\nOrder Histories\nOrder Invoices\nOrder Payments\nOrder Slip\nOrders\nProducts\nStock Movement Reasons\nStock Movements\nStores\nSuppliers\nTax Rule Groups\n\nIf there are more endpoints you'd like Airbyte to support, please create an issue.",
    "tag": "airbyte"
  },
  {
    "title": "Faker",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/faker.md",
    "content": "Faker\nSync overview\nThe Sample Data (Faker) source generates sample data using the python mimesis package.\nOutput schema\nThis source will generate an \"e-commerce-like\" dataset with users, products, and purchases. Here's what is produced at a Postgres destination connected to this source:\n```sql\nCREATE TABLE \"public\".\"users\" (\n    \"id\" float8,\n    \"age\" int8,\n    \"name\" text,\n    \"email\" text,\n    \"title\" text,\n    \"gender\" text,\n    \"height\" text,\n    \"weight\" int8,\n    \"language\" text,\n    \"telephone\" text,\n    \"blood_type\" text,\n    \"created_at\" timestamptz,\n    \"occupation\" text,\n    \"updated_at\" timestamptz,\n    \"nationality\" text,\n    \"academic_degree\" text,\n    -- \"_airbyte_ab_id\" varchar,\n    -- \"_airbyte_emitted_at\" timestamptz,\n    -- \"_airbyte_normalized_at\" timestamptz,\n    -- \"_airbyte_dev_users_hashid\" text,\n    -- \"_airbyte_unique_key\" text\n);\nCREATE TABLE \"public\".\"products\" (\n    \"id\" float8,\n    \"make\" text,\n    \"year\" float8,\n    \"model\" text,\n    \"price\" float8,\n    \"created_at\" timestamptz,\n    -- \"_airbyte_ab_id\" varchar,\n    -- \"_airbyte_emitted_at\" timestamptz,\n    -- \"_airbyte_normalized_at\" timestamptz,\n    -- \"_airbyte_dev_products_hashid\" text,\n    -- \"_airbyte_unique_key\" text\n);\nCREATE TABLE \"public\".\"purchases\" (\n    \"id\" float8,\n    \"user_id\" float8,\n    \"product_id\" float8,\n    \"returned_at\" timestamptz,\n    \"purchased_at\" timestamptz,\n    \"added_to_cart_at\" timestamptz,\n    -- \"_airbyte_ab_id\" varchar,\n    -- \"_airbyte_emitted_at\" timestamptz,\n    -- \"_airbyte_normalized_at\" timestamptz,\n    -- \"_airbyte_dev_purchases_hashid\" text,\n    -- \"_airbyte_unique_key\" text\n);\n```\nFeatures\n| Feature           | Supported?(Yes/No) | Notes |\n| :---------------- | :------------------- | :---- |\n| Full Refresh Sync | Yes                  |       |\n| Incremental Sync  | Yes                  |       |\n| Namespaces        | No                   |       |\nOf note, if you choose `Incremental Sync`, state will be maintained between syncs, and once you hit `count` records, no new records will be added.\nYou can choose a specific `seed` (integer) as an option for this connector which will guarantee that the same fake records are generated each time. Otherwise, random data will be created on each subsequent sync.\nRequirements\nNone!",
    "tag": "airbyte"
  },
  {
    "title": "Convex",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/convex.md",
    "content": "Convex\nThis page contains the setup guide and reference information for the Convex source connector.\nGet started with Convex at the Convex website.\nSee your data on the Convex dashboard.\nOverview\nThe Convex connector supports Full Refresh, Incremental Append, and Incremental Dedupe with deletes.\nOutput schema\nThis source syncs each Convex table as a separate stream.\nCheck out the list of your tables on the Convex dashboard in the \"Data\" view.\nTypes not directly supported by JSON are encoded as described in the\nJSONSchema\nfor the stream.\nFor example, the Javascript value `new Set([\"a\", \"b\"])` is encoded as `{\"$set\": [\"a\", \"b\"]}`, as described by the JSONSchema\n`{\"type\": \"object\", \"description\": \"Set\", \"properties\": {\"$set\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}}}`.\nEvery record includes the client-defined fields for the table, for example a `\"messages\"` table may contain fields for `\"author\"` and `\"body\"`.\nAdditionally, each document has system fields:\n\n`_id` uniquely identifies the document. It is not changed by `.patch` or `.replace` operations.\n`_creationTime` records a timestamp in milliseconds when the document was initially created. It is not changed by `.patch` or `.replace` operations.\n`_ts` records a timestamp in nanoseconds when the document was last modified. It can be used for ordering operations in Incremental Append mode, and is automatically used in Incremental Dedupe mode.\n`_deleted` identifies whether the document was deleted. It can be used to filter deleted documents in Incremental Append mode, and is automatically used to remove documents in Incremental Dedupe mode.\n\nFeatures\n| Feature | Supported? |\n| :--- | :--- |\n| Full Refresh Sync | Yes |\n| Incremental - Append Sync | Yes |\n| Incremental - Dedupe Sync | Yes |\n| Replicate Incremental Deletes | Yes |\n| Change Data Capture           | Yes |\n| Namespaces | No |\nPerformance considerations\nThe Convex connector syncs all documents from the historical log.\nIf you see performance issues due to syncing unnecessary old versions of documents,\nplease reach out to Convex support.\nGetting started\nRequirements\n\nConvex Account\nConvex Project\nDeploy key\n\nSetup guide\nContact Convex via email or Discord to request Airbyte support for your account.\nOn the Convex dashboard, navigate to the project that you want to sync.\nNote only \"Production\" deployments should be synced.\nIn the Data tab, you should see the tables and a sample of the data that will be synced.\n\nNavigate to the Settings tab.\nCopy the \"Deployment URL\" from the settings page to the `deployment_url` field in Airbyte.\nClick \"Generate a deploy key\".\nCopy the generated deploy key into the `access_key` field in Airbyte.\n",
    "tag": "airbyte"
  },
  {
    "title": "Pipedrive",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/pipedrive.md",
    "content": "Pipedrive\nThis page contains the setup guide and reference information for the Pipedrive connector.\nPrerequisites\n\nA Pipedrive account;\nAn `API token`;\nA `client_id`, `client_secret`, and `refresh_token`. \n\nSetup guide\nThe Pipedrive connector accepts two authentication flows:\nVia API Token Authentication\nStep 1 - Enable API Token:\nIf you don't see API next to the `Your companies` section, it's due to the permission sets handled by the company's admin. The company's admin can give you access to your API token by enabling it for you from the Settings in Pipedrive web app.\nFor more information, access enabling API for company users.\nStep 2 - Find the API Token:\nYou can get the API Token manually from the Pipedrive web app by going to account name (on the top right) > Company settings > Personal preferences > API.\nSee How to find the API Token for detailed information.\nVia OAuth\nStep 1 - Register a Pipedrive app:\nPipedrive allows integrations with its API through registered apps. So, to authenticate Airbyte, first you need to create a Pipedrive private app in the marketplace. Follow these instructions to register your integration.\nStep 2 - Follow the Oauth Authorization flow:\nWith the registered app, you can follow the authorization flow to obtain the `client_id`, `client_secret`, and `refresh_token` secrets. Pipedrive has documentation about it: https://pipedrive.readme.io/docs/marketplace-oauth-authorization.\nStep 3 - Configure Airbyte:\nNow you can fill the fields Client ID, Client Secret, and Refresh Token. Your Pipedrive connector is set up to work with the OAuth authentication.\nSupported sync modes\nThe Pipedrive connector supports the following sync modes:\n| Feature                       | Supported? |\n| :---------------------------- | :--------- |\n| Full Refresh Sync             | Yes        |\n| Incremental Sync              | Yes        |\n| Replicate Incremental Deletes | No         |\n| SSL connection                | Yes        |\n| Namespaces                    | No         |\nSupported Streams\nApart from `Fields` streams, all other streams support incremental.\n\n\nActivities\n\n\nActivityFields\n\n\nDealFields\n\n\nDeals\n\n\nLeads\n\n\nOrganizationFields\n\n\nOrganizations\n\n\nPersonFields\n\n\nPersons\n\n\nPipelines\n\n\nStages\n\n\nUsers\n\n\nPerformance considerations\nThe Pipedrive connector will gracefully handle rate limits. For more information, see the Pipedrive docs for rate limitations.",
    "tag": "airbyte"
  },
  {
    "title": "Recurly",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/recurly.md",
    "content": "Recurly\nOverview\nThe Recurly source supports Full Refresh as well as Incremental syncs. \nFull Refresh sync means every time a sync is run, Airbyte will copy all rows in the tables and columns you set up for replication into the destination in a new table.\nIncremental syn means only changed resources are copied from Recurly. For the first run, it will be a Full Refresh sync.\nOutput schema\nSeveral output streams are available from this source:\n\nAccounts\nAccount Notes\nAccount Coupon Redemptions\nAdd Ons\nBilling Infos\nCoupons\nUnique Coupons\nCredit Payments\nAutomated Exports\nInvoices\nMeasured Units\nLine Items\nPlans\nShipping Addresses\nShipping Methods\nSubscriptions\nSubscription Changes\nTransactions\n\nIf there are more endpoints you'd like Airbyte to support, please create an issue.\nFeatures\n| Feature | Supported? |\n| :--- | :--- |\n| Full Refresh Sync | Yes |\n| Incremental Sync | Yes |\n| Replicate Incremental Deletes | Coming soon |\n| SSL connection | Yes |\n| Namespaces | No |\nPerformance considerations\nThe Recurly connector should not run into Recurly API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.\nGetting started\nRequirements\n\nRecurly Account\nRecurly API Key\n\nSetup guide\nGenerate a API key using the Recurly documentation\nWe recommend creating a restricted, read-only key specifically for Airbyte access. This will allow you to control which resources Airbyte should be able to access.",
    "tag": "airbyte"
  },
  {
    "title": "OneSignal",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/onesignal.md",
    "content": "OneSignal\nSync overview\nThis source can sync data for the OneSignal API. It supports both Full Refresh and Incremental syncs. You can choose if this connector will copy only the new or updated data, or all rows in the tables and columns you set up for replication, every time a sync is run.\nOutput schema\nThis Source is capable of syncing the following core Streams:\n\nApps\nDevices (Incremental)\nNotifications (Incremental)\nOutcomes\n\nThe `Outcomes` stream requires `outcome_names` parameter to filter out outcomes, see the API docs for more details.\nData type mapping\n| Integration Type | Airbyte Type | Notes |\n| :--- | :--- | :--- |\n| `string` | `string` |  |\n| `integer` | `integer` |  |\n| `number` | `number` |  |\n| `array` | `array` |  |\n| `object` | `object` |  |\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | Yes |  |\n| Namespaces | No |  |\nPerformance considerations\nThe connector is restricted by normal OneSignal rate limits.\nThe OneSignal connector should not run into OneSignal API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.\nGetting started\nRequirements\n\nOneSignal account\nOneSignal user auth Key\n\nSetup guide\nPlease register on OneSignal and follow this docs to get your user auth key.",
    "tag": "airbyte"
  },
  {
    "title": "TVMaze Schedule",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/tvmaze-schedule.md",
    "content": "TVMaze Schedule\nSync overview\nThis source retrieves historical and future TV scheduling data using the\nTVMaze schedule API.\nOutput schema\nThis source is capable of syncing the following streams:\n\n`domestic`\n`web`\n`future`\n\nFeatures\n| Feature           | Supported? (Yes/No) | Notes |\n|:------------------|:----------------------|:------|\n| Full Refresh Sync | Yes                   |       |\n| Incremental Sync  | No                    |       |\nPerformance considerations\nTVMaze has a rate limit of 20 requests per 10 seconds. This source should not\nrun into this limit.\nGetting started\nRequirements\n\nChoose a start date for your sync. This may be in the future.\nChoose an ISO 3166-1 country code for domestic schedule syncs.\n\nSetup guide\nThe following fields are required fields for the connector to work:\n\n`start_date`: The start date to pull `history` data from.\n(optional) `end_date`: The end date to pull `history` data until.\n`domestic_schedule_country_code`: The ISO 3166-1 country code to pull domestic\n  schedule data for.\n(optional) `web_schedule_country_code`: The ISO 3166-1 country code to pull\n  web schedule data for. Can be left blank for all countries and global\n  channels, or set to 'global' for only global channels.\n",
    "tag": "airbyte"
  },
  {
    "title": "Pok\u00e9API",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/pokeapi.md",
    "content": "Pok\u00e9API\nTutorials\nThe Pok\u00e9API is primarly used as a tutorial and educational resource, as it requires zero dependencies. Learn how Airbyte and this connector works with these tutorials:\n\nAirbyte Quickstart: An Introduction to Deploying and Syncing\nAirbyte CDK Speedrun: A Quick Primer on Building Source Connectors\nHow to Build ETL Sources in Under 30 Minutes: A Video Tutorial\n\nFeatures\n| Feature                       | Supported? |\n| :---------------------------- | :--------- |\n| Full Refresh Sync             | Yes        |\n| Incremental - Append Sync     | No         |\n| Replicate Incremental Deletes | No         |\n| SSL connection                | No         |\n| Namespaces                    | No         |\nThis source uses the fully open Pok\u00e9API to serve and retrieve information about Pok\u00e9mon. This connector should be primarily used for educational purposes or for getting a trial source up and running without needing any dependencies. As this API is fully open and is not rate-limited, no authentication or rate-limiting is performed, so you can use this connector right out of the box without any further configuration.\nOutput Schema\nCurrently, only one output stream is available from this source, which is the Pok\u00e9mon output stream. This schema is defined here.\nRate Limiting & Performance Considerations (Airbyte Open-Source)\nAccording to the API's fair use policy, please make sure to cache resources retrieved from the Pok\u00e9API wherever possible. That said, the Pok\u00e9API does not perform rate limiting.\nData Type Mapping\nThe Pok\u00e9API uses the same JSONSchema types that Airbyte uses internally (`string`, `date-time`, `object`, `array`, `boolean`, `integer`, and `number`), so no type conversions happen as part of this source.",
    "tag": "airbyte"
  },
  {
    "title": "CockroachDB",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/cockroachdb.md",
    "content": "CockroachDB\nOverview\nThe CockroachDB source supports both Full Refresh and Incremental syncs. You can choose if this connector will copy only the new or updated data, or all rows in the tables and columns you set up for replication, every time a sync is run.\nResulting schema\nThe CockroachDb source does not alter the schema present in your database. Depending on the destination connected to this source, however, the schema may be altered. See the destination's documentation for more details.\nData type mapping\nCockroachDb data types are mapped to the following data types when synchronizing data:\n| CockroachDb Type | Resulting Type | Notes |\n| :--- | :--- | :--- |\n| `bigint` | integer |  |\n| `bit` | boolean |  |\n| `boolean` | boolean |  |\n| `character` | string |  |\n| `character varying` | string |  |\n| `date` | string |  |\n| `double precision` | string |  |\n| `enum` | number |  |\n| `inet` | string |  |\n| `int` | integer |  |\n| `json` | string |  |\n| `jsonb` | string |  |\n| `numeric` | number |  |\n| `smallint` | integer |  |\n| `text` | string |  |\n| `time with timezone` | string | may be written as a native date type depending on the destination |\n| `time without timezone` | string | may be written as a native date type depending on the destination |\n| `timestamp with timezone` | string | may be written as a native date type depending on the destination |\n| `timestamp without timezone` | string | may be written as a native date type depending on the destination |\n| `uuid` | string |  |\nNote: arrays for all the above types as well as custom types are supported, although they may be de-nested depending on the destination.\nFeatures\n| Feature | Supported | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | Yes |  |\n| Change Data Capture | No |  |\n| SSL Support | Yes |  |\nGetting started\nRequirements\n\nCockroachDb `v1.15.x` or above\nAllow connections from Airbyte to your CockroachDb database (if they exist in separate VPCs)\nCreate a dedicated read-only Airbyte user with access to all tables needed for replication\n\nSetup guide\n1. Make sure your database is accessible from the machine running Airbyte\nThis is dependent on your networking setup. The easiest way to verify if Airbyte is able to connect to your CockroachDb instance is via the check connection tool in the UI.\n2. Create a dedicated read-only user with access to the relevant tables (Recommended but optional)\nThis step is optional but highly recommended to allow for better permission control and auditing. Alternatively, you can use Airbyte with an existing user in your database.\nTo create a dedicated database user, run the following commands against your database:\n`sql\nCREATE USER airbyte PASSWORD 'your_password_here';`\nThen give it access to the relevant schema:\n`sql\nGRANT USAGE ON SCHEMA <schema_name> TO airbyte`\nNote that to replicate data from multiple CockroachDb schemas, you can re-run the command above to grant access to all the relevant schemas, but you'll need to set up multiple sources connecting to the same db on multiple schemas.\nNext, grant the user read-only access to the relevant tables. The simplest way is to grant read access to all tables in the schema as follows:\n```sql\nGRANT SELECT ON ALL TABLES IN SCHEMA  TO airbyte;\nAllow airbyte user to see tables created in the future\nALTER DEFAULT PRIVILEGES IN SCHEMA  GRANT SELECT ON TABLES TO airbyte;\n```\n3. That's it!\nYour database user should now be ready for use with Airbyte.",
    "tag": "airbyte"
  },
  {
    "title": "Tempo",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/tempo.md",
    "content": "Tempo\nThis page contains the setup guide and reference information for the Tempo source connector.\nPrerequisites\n\nAPI Token\n\nSetup guide\nStep 1: Set up Tempo\nSource Tempo is designed to interact with the data your permissions give you access to. To do so, you will need to generate a Tempo OAuth 2.0 token for an individual user.\nGo to Tempo > Settings, scroll down to Data Access and select API integration.\nStep 2: Set up the Tempo connector in Airbyte\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the Tempo connector and select Tempo from the Source type dropdown.\nEnter your API token that you obtained from Tempo.\nClick Set up source.\n\nSupported sync modes\nThe Tempo source connector supports the following  sync modes:\n\nFull Refresh - Overwrite\nFull Refresh - Append\nIncremental - Append\nIncremental - Deduped History\n\nSupported Streams\nThis connector outputs the following streams:\n\nAccounts\nCustomers\nWorklogs\nWorkload Schemes\n\nIf there are more endpoints you'd like Airbyte to support, please create an issue.",
    "tag": "airbyte"
  },
  {
    "title": "Magento",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/magento.md",
    "content": "Magento\nMagento is an open source eCommerce Platform.\nSync overview\nMagento runs on MySQL. You can use Airbyte to sync your Magento instance by connecting to the underlying database using the MySQL connector.\n:::info\nReach out to your service representative or system admin to find the parameters required to connect to the underlying database\n:::\nOutput schema\nThe output schema is described in the Magento docs. See the MySQL connector for more info on general rules followed by the MySQL connector when moving data.",
    "tag": "airbyte"
  },
  {
    "title": "ActiveCampaign",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/activecampaign.md",
    "content": "ActiveCampaign\nSync overview\nThis source can sync data from the ActiveCampaign API. At present this connector only supports full refresh syncs meaning that each time you use the connector it will sync all available records from scratch. Please use cautiously if you expect your API to have a lot of records.\nThis Source Supports the Following Streams\n\ncampaigns\ncontacts\nlists\ndeals\nsegments\nforms\n\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | No |  |\nPerformance considerations\nThe connector has a rate limit of 5 requests per second per account.\nGetting started\nRequirements\n\nActiveCampaign account\nActiveCampaign API Key\n",
    "tag": "airbyte"
  },
  {
    "title": "Senseforce",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/senseforce.md",
    "content": "Senseforce\nThis page guides you through the process of setting up the Senseforce source connector.\nSync overview\nPrerequisites\n\nA Senseforce Dataset to export\nYour Senseforce API Access Token\nYour Senseforce Backend URL\nYour Senseforce Dataset ID\n\nCreating a Senseforce Dataset to Export\nThe Senseforce Airbyte connector allows to export custom datasets built bei Senseforce users. Follow these steps to configure a dataset which can be exported with the Airbyte connector: \n1. Create a new, empty dataset as documented here\n2. Add at least the following columns (these columns are Senseforce system columns and available for all of your custom data models/event schemas): \n   1. Metadata -> Timestamp\n   2. Metadata -> Thing\n   3. Metadata -> Id\n3. Add any other column of your event schemas you want to export\n4. Enter a descriptive Name and a Description and save the dataset\n5. Note the ID of the dataset (the GUID at the end of the URL path of your dataset in your browser URL bar)\n\nTip: For most exports it is recommended to have the Timestamp column in first place. The Airbyte connector automatically orders in ascending direction. If the Timestamp column is not in the first position, incremental syncs might not work properly.\nIMPORTANT: The Timestamp, Thing and Id column are mandatory for the Connector to work as intended. While it still works without eg. the \"Id\", functionality might be impaired if one of these 3 columns is missing. Make sure to not rename these columns - keep them at their default names.\n\nSet up the Senseforce source connector\n\nLog into your Airbyte Cloud or Airbyte Open Source account.\nClick Sources and then click + New source. \nOn the Set up the source page, select Senseforce from the Source type dropdown.\nEnter a name for your source.\nFor API Access Token, enter your Senseforce API Access Token.\nFor Senseforce backend URL, enter your Senseforce Backend URL.\nFor Dataset ID, enter your Senseforce Dataset ID.\n\nWe recommend creating an api access token specifically for Airbyte to control which resources Airbyte can access. For good operations, we recommend to create a separate Airbyte User as well as a separate Senseforce Airbyte Group. Share the dataset with this group and grant Dataset Read, Event Schema Read and Machine Master Data Read permissions.  \n\nFor The first day (in UTC) when to read data from, enter the day in YYYY-MM-DD format. The data added on and after this day will be replicated.\nClick Set up source.\n\nSupported sync modes\nThe Senseforce source connector supports the following sync modes:\n\nFull Refresh\nIncremental\n\n\nNOTE: The Senseforce Airbyte connector uses the Timestamp column to determine, which data were already read. Data inserted AFTER a finished sync, with timestamps less than already synced ones, are not considered for the next sync anymore.\nIf this behavior does not fit your use case, follow the next section\n\nUsing Inserted Timestamp instead of Data Timestamp for incremental modes\n\nRename your \"Timestamp\" column to \"Timestamp_data\"\nAdd the Metadata -> Inserted column to your dataset.\nMove the newly added \"Inserted\" column to position 1.\nRename the \"Inserted\" column to \"Timestamp\".\n\nNow the inserted timestamp will be used for creating the Airbyte cursor. Note that this method results in slower syncs, as the Senseforce queries to generate the Datasets are slower than if you use the data timestamp.\nSupported Streams\nThe Senseforce source connector supports the following streams:\n- Senseforce Datasets\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | Yes |  |\nPerformance considerations\nSenseforce utilizes an undocumented rate limit which - under normal use - should not be triggered, even with huge datasets.\nCreate an issue if you see any rate limit issues that are not automatically retried successfully.",
    "tag": "airbyte"
  },
  {
    "title": "Twitter API",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/twitter.md",
    "content": "Twitter API\nAPI Documentation link here\nOverview\nThe Twitter API source supports full refresh syncs\nOutput schema\nBelow output stream is available from this source:\n*recent_search_tweets.\nFeatures\n| Feature           | Supported? |\n|:------------------|:-----------|\n| Full Refresh Sync | Yes        |\n| Incremental Sync  | No         |\nPerformance considerations\nRate limiting is mentioned in the API docuemntation\nGetting started\nRequirements\n\nTwitter API Key.\n\nConnect using `API Key`:\n\nGenerate an API Key as described here.\nUse the generated `API Key` in the Airbyte connection.\n",
    "tag": "airbyte"
  },
  {
    "title": "Mailersend",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/mailersend.md",
    "content": "Mailersend\nSync overview\nThis source can sync data from the Mailersend. At present this connector only supports full refresh syncs meaning that each time you use the connector it will sync all available records from scratch.\nThis Source Supports the Following Streams\n\nactivity\n\nFeatures\n| Feature           | Supported?(Yes/No) | Notes |\n| :---------------- | :------------------- | :---- |\n| Full Refresh Sync | Yes                  |       |\n| Incremental Sync  | No                   |       |\nPerformance considerations\nMailerSend has a default rate limit of 60 requests per minute on general API endpoints.\nGetting started",
    "tag": "airbyte"
  },
  {
    "title": "My Hours",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/my-hours.md",
    "content": "My Hours\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | No |  |\nThis source syncs data from the My Hours API.\nSupported Tables\nThis source allows you to synchronize the following data tables:\n\nTime logs\nClients\nProjects\nTeam members\nTags\n\nGetting started\nRequirements\nIn order to use the My Hours API you need to provide the credentials to an admin My Hours account.\nPerformance Considerations (Airbyte Open-Source)\nDepending on the amount of team members and time logs the source provides a property to change the pagination size for the time logs query. Typically a pagination of 30 days is a correct balance between reliability and speed. But if you have a big amount of monthly entries you might want to change this value to a lower value.",
    "tag": "airbyte"
  },
  {
    "title": "SurveySparrow",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/survey-sparrow.md",
    "content": "SurveySparrow\nThis page guides you through the process of setting up the SurveySparrow source connector.\nPrerequisites\nFor Airbyte Open Source:\n\nAccess Token\n\nSetup guide\nStep 1: Set up SurveySparrow\nPlease read this docs.\nIn order to get access token, follow these steps:\n\nLogin to your surveysparrow account and go to Settings \u2192 Apps & Integrations\nCreate a Private App\nEnter Name, Description, select scope and generate the access token\nCopy and keep the access token in a safe place (Access token will be displayed only once and you may need to re-generate if you misplaced)\nSave your app and you are good to start developing your private app\n\nStep 2: Set up the source connector in Airbyte\nFor Airbyte Open Source:\n\nGo to local Airbyte page\nIn the left navigation bar, click Sources. In the top-right corner, click + new source\nOn the source setup page, select SurveySparrow from the Source type dropdown and enter a name for this connector\nAdd Access Token\nSelect whether SurveySparrow account location is EU-based\nAdd Survey ID (optional)\nClick `Set up source`\n\nSupported streams and sync modes\n\nContacts\nContactLists\nQuestions\nResponses\nRoles\nSurveys\nSurveyFolders\nUsers\n",
    "tag": "airbyte"
  },
  {
    "title": "Zenloop",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/zenloop.md",
    "content": "Zenloop\nThis page contains the setup guide and reference information for the Zenloop source connector.\nPrerequisites\n\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud.\nClick Sources and then click + New source.\nOn the Set up the source page, select Zenloop from the Source type dropdown.\nEnter the name for the Zenloop connector.\nEnter your API token\nFor Date from, enter the date in YYYY-MM-DDTHH:mm:ssZ format. The data added on and after this date will be replicated. \nEnter your Survey ID. Zenloop Survey ID. Can be found here. Leave empty to pull answers from all surveys. (Optional)\nEnter your Survey Group ID. Zenloop Survey Group ID. Can be found by pulling All Survey Groups via SurveyGroups stream. Leave empty to pull answers from all survey groups. (Optional)\nClick Set up source.\n\n\n\nFor Airbyte Open Source:\n\nNavigate to the Airbyte Open Source dashboard.\nClick Sources and then click + New source.\nOn the Set up the source page, select Zenloop from the Source type dropdown.\nEnter the name for the Zenloop connector.\nEnter your API token\nFor Date from, enter the date in YYYY-MM-DDTHH:mm:ssZ format. The data added on and after this date will be replicated. \nEnter your Survey ID. Zenloop Survey ID. Can be found here. Leave empty to pull answers from all surveys. (Optional)\nEnter your Survey Group ID. Zenloop Survey Group ID. Can be found by pulling All Survey Groups via SurveyGroups stream. Leave empty to pull answers from all survey groups. (Optional)\nClick Set up source.\n\n\nSupported sync modes\nThe Zenloop source connector supports the following sync modes:\n| Feature           | Supported?(Yes/No) |\n| :---------------- | :------------------- |\n| Full Refresh Sync | Yes                  |\n| Incremental Sync  | Yes                  |\n| Namespaces        | No                   | \nSupported Streams\nThis Source is capable of syncing the following core Streams:\n\nAnswers (Incremental)\nSurveys\nAnswersSurveyGroup (Incremental)\nSurveyGroups\nProperties\n\nThe `Answers`, `AnswersSurveyGroup` and `Properties` stream respectively have an optional survey_id parameter that can be set by filling the `public_hash_id` field of the connector configuration. If not provided answers for all surveys (groups) will be pulled.\nPerformance considerations\nThe Zenloop connector should not run into Zenloop API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.\nData type map\n| Integration Type | Airbyte Type |\n| :--------------- | :----------- |\n| `string`         | `string`     |\n| `integer`        | `integer`    |\n| `number`         | `number`     |\n| `array`          | `array`      |\n| `object`         | `object`     |",
    "tag": "airbyte"
  },
  {
    "title": "Coinmarketcap API",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/coinmarketcap.md",
    "content": "Coinmarketcap API\nSync overview\nThis source can sync data from the Coinmarketcap API. At present this connector only supports full refresh syncs meaning that each time you use the connector it will sync all available records from scratch. Please use cautiously if you expect your API to have a lot of records.\nThis Source Supports the Following Streams\n\ncategories\nlisting\nquotes\nfiat\nexchange\n\nFeatures\n| Feature           | Supported?(Yes/No) | Notes |\n| :---------------- | :------------------- | :---- |\n| Full Refresh Sync | Yes                  |       |\n| Incremental Sync  | No                   |       |\nPerformance considerations\nCoinmarketcap APIs are under rate limits for the number of API calls allowed per API keys per second. If you reach a rate limit, API will return a 429 HTTP error code. See here\nGetting started\nRequirements\n\nAPI token\nData Type:\nlatest\nhistorical\n",
    "tag": "airbyte"
  },
  {
    "title": "Twilio Taskrouter",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/twilio-taskrouter.md",
    "content": "Twilio Taskrouter\nThis page contains the setup guide and reference information for the Twilio Taskrouter source connector.\nPrerequisites\nTwilio Taskroute HTTP requests to the REST API are protected with HTTP Basic authentication. You will use your Twilio Account SID as the username and your Auth Token as the password for HTTP Basic authentication. You will also have to use your Workspace ID as the sid if you wish to access a particular workspace or it's subresources.\nYou can find your Account SID and Auth Token on your dashboard by scrolling down the console once you have created a Twilio account.\nYou can find Taskrouter in the Explore Products page under Solutions in the list. Click on it to expand a panel in the sidebar and click on Workspaces to be taken to your created workspaces. Each workspace has it's unique ID as SID with which you can access the streams.\nSee docs for more details.\nSetup guide\nStep 1: Set up the Twilio Takrouter connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the Twilio connector and select Twilio Taskrouter from the  type dropdown.\nEnter your `account_sid`.\nEnter your `auth_token`.\nClick Set up source.\n\nFor Airbyte OSS:\n\nNavigate to the Airbyte Open Source dashboard.\nSet the name for your source.\nEnter your `account_sid`.\nEnter your `auth_token`.\nClick Set up source.\n\nSupported sync modes\nThe Twilio source connector supports the following sync modes:\n| Feature                       | Supported? |\n| :---------------------------- | :--------- |\n| Full Refresh Sync             | Yes        |\n| Incremental Sync              | No         |\n| Replicate Incremental Deletes | No         |\n| SSL connection                | Yes        |\nSupported Streams\n\nWorkspaces\nAll Workspaces\nWorkers\n\nPerformance considerations\nThe Twilio Taskrouter connector will gracefully handle rate limits.\nFor more information, see the Twilio docs for rate limitations.",
    "tag": "airbyte"
  },
  {
    "title": "Partnerstack",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/partnerstack.md",
    "content": "Partnerstack\nSync overview\nThe Partnerstack source supports both Full Refresh only.\nThis source can sync data for the Partnerstack API.\nOutput schema\nThis Source is capable of syncing the following core Streams:\n\nCustomers\nDeals\nGroups\nLeads\nPartnerships\nRewards\nTransactions\n\nFeatures\n| Feature                   | Supported?(Yes/No) | Notes |\n| :------------------------ | :------------------- | :---- |\n| Full Refresh Sync         | Yes                  |       |\n| Incremental - Append Sync | No                   |       |\n| Namespaces                | No                   |       |\nPerformance considerations\nThe Partnerstack connector should not run into Partnerstack API limitations under normal usage.\nRequirements\n\nPartnerstack API keys. See the Partnerstack docs for information on how to obtain the API keys.\n",
    "tag": "airbyte"
  },
  {
    "title": "Db2",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/db2.md",
    "content": "Db2\nOverview\nThe IBM Db2 source allows you to sync data from Db2. It supports both Full Refresh and Incremental syncs. You can choose if this connector will copy only the new or updated data, or all rows in the tables and columns you set up for replication, every time a sync is run.\nThis IBM Db2 source connector is built on top of the IBM Data Server Driver for JDBC and SQLJ. It is a pure-Java driver (Type 4) that supports the JDBC 4 specification as described in IBM Db2 documentation.\nResulting schema\nThe IBM Db2 source does not alter the schema present in your warehouse. Depending on the destination connected to this source, however, the result schema may be altered. See the destination's documentation for more details.\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental - Append Sync | Yes |  |\n| Namespaces | Yes |  |\nGetting started\nRequirements\n\nYou'll need the following information to configure the IBM Db2 source:\nHost\nPort\nDatabase\nUsername\nPassword\nCreate a dedicated read-only Airbyte user and role with access to all schemas needed for replication.\n\nSetup guide\n1. Specify port, host and name of the database.\n2. Create a dedicated read-only user with access to the relevant schemas (Recommended but optional)\nThis step is optional but highly recommended allowing for better permission control and auditing. Alternatively, you can use Airbyte with an existing user in your database.\nPlease create a dedicated database user and run the following commands against your database:\n```sql\n-- create Airbyte role\nCREATE ROLE 'AIRBYTE_ROLE';\n-- grant Airbyte database access\nGRANT CONNECT ON 'DATABASE' TO ROLE 'AIRBYTE_ROLE'\nGRANT ROLE 'AIRBYTE_ROLE' TO USER 'AIRBYTE_USER'\n```\nYour database user should now be ready for use with Airbyte.\n3. Create SSL connection.\nTo set up an SSL connection, you need to use a client certificate. Add it to the \"SSL PEM file\" field and the connector will automatically add it to the secret keystore.\nYou can also enter your own password for the keystore, but if you don't, the password will be generated automatically.",
    "tag": "airbyte"
  },
  {
    "title": "ClickUp API",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/click-up.md",
    "content": "ClickUp API\nSync overview\nThis source can sync data from ClickUp API. Currently, this connector only supports full refresh syncs. That is, every time a sync is run, all the records are fetched from the source.\nOutput schema\nThis source is capable of syncing the following streams:\n\nuser\nteams\nspaces\nfolders\nlists\ntasks\n\nFeatures\n| Feature           | Supported? (Yes/No) | Notes                                                   |\n|:------------------|:----------------------|:--------------------------------------------------------|\n| Full Refresh Sync | Yes                   |                                                         |\n| Incremental Sync  | No                    |                                                         |\nPerformance considerations\nThe ClickUp API enforces request rate limits per token. The rate limits are depending on your workplace plan. See here.\nGetting started\nRequirements\n\nGenerate an API key from ClickUp. See here.\n\nSetup guide\nThe following fields are required fields for the connector to work:\n\n`api_token`: Your ClickUp API Token.\n\nHere are some optional fields for different streams:\n\n\n`team_id`: Your team ID in your ClickUp workspace. It is required for `space` stream.\n\n\n`space_id`: Your space ID in your ClickUp workspace. It is required for `folder` stream.\n\n\n`folder_id`: Your folder ID in your ClickUp space. It is required for `list` stream.\n\n\n`list_id`: Your list ID in your folder of space. It is required for `task` stream.\n\n",
    "tag": "airbyte"
  },
  {
    "title": "Oracle Siebel CRM",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/oracle-siebel-crm.md",
    "content": "Oracle Siebel CRM\nOracle Siebel CRM is a Customer Relationship Management platform.\nSync overview\nOracle Siebel CRM can run on the Oracle, MSSQL, or IBM DB2 databases. You can use Airbyte to sync your Oracle Siebel CRM instance by connecting to the underlying database using the appropriate Airbyte connector:\n\nDB2\nMSSQL\nOracle\n\n:::info\nReach out to your service representative or system admin to find the parameters required to connect to the underlying database\n:::\nOutput schema\nTo understand your Oracle Siebel CRM database schema, see the Organization Setup Overview docs documentation. Otherwise, the schema will be loaded according to the rules of the underlying database's connector.",
    "tag": "airbyte"
  },
  {
    "title": "Robert Koch-Institut Covid",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/rki-covid.md",
    "content": "Robert Koch-Institut Covid\nSync overview\nThis source can sync data for the Robert Koch-Institut Covid API. It supports both Full Refresh and Incremental syncs. You can choose if this connector will copy only the new or updated data, or all rows in the tables and columns you set up for replication, every time a sync is run.\nOutput schema\nThis Source is capable of syncing the following core Streams (only for Germany cases):\n\nGermany\nGermany by age and groups\nGermany cases by days\nGermany incidences by days\nGermany deaths by days\nGermany recovered by days\nGermany frozen-incidence by days\nGermany hospitalization by days\n\nData type mapping\n| Integration Type | Airbyte Type | Notes |\n| :--- | :--- | :--- |\n| `string` | `string` |  |\n| `integer` | `integer` |  |\n| `number` | `number` |  |\n| `array` | `array` |  |\n| `object` | `object` |  |\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | Yes |  |\n| Namespaces | No |  |\nPerformance considerations\nThe RKI Covid connector should not run into RKI Covid API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.\nGetting started\nRequirements\n\nStart Date \n\nSetup guide\nSelect start date",
    "tag": "airbyte"
  },
  {
    "title": "Files (CSV, JSON, Excel, Feather, Parquet)",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/file.md",
    "content": "Files (CSV, JSON, Excel, Feather, Parquet)\nFeatures\n| Feature                                  | Supported? |\n| ---------------------------------------- | ---------- |\n| Full Refresh Sync                        | Yes        |\n| Incremental Sync                         | No         |\n| Replicate Incremental Deletes            | No         |\n| Replicate Folders (multiple Files)       | No         |\n| Replicate Glob Patterns (multiple Files) | No         |\nThis source produces a single table for the target file as it replicates only one file at a time for the moment. Note that you should provide the `dataset_name` which dictates how the table will be identified in the destination (since `URL` can be made of complex characters).\nStorage Providers\n| Storage Providers      | Supported?                                      |\n| ---------------------- | ----------------------------------------------- |\n| HTTPS                  | Yes                                             |\n| Google Cloud Storage   | Yes                                             |\n| Amazon Web Services S3 | Yes                                             |\n| SFTP                   | Yes                                             |\n| SSH / SCP              | Yes                                             |\n| local filesystem       | Local use only (inaccessible for Airbyte Cloud) |\nFile / Stream Compression\n| Compression | Supported? |\n| ----------- | ---------- |\n| Gzip        | Yes        |\n| Zip         | No         |\n| Bzip2       | No         |\n| Lzma        | No         |\n| Xz          | No         |\n| Snappy      | No         |\nFile Formats\n| Format                | Supported? |\n| --------------------- | ---------- |\n| CSV                   | Yes        |\n| JSON                  | Yes        |\n| HTML                  | No         |\n| XML                   | No         |\n| Excel                 | Yes        |\n| Excel Binary Workbook | Yes        |\n| Feather               | Yes        |\n| Parquet               | Yes        |\n| Pickle                | No         |\n| YAML                  | Yes        |\nThis connector does not support syncing unstructured data files such as raw text, audio, or videos.\nGetting Started\n\nFor Airbyte Cloud:\nSetup through Airbyte Cloud will be exactly the same as the open-source setup, except for the fact that local files are disabled.\n\n\nFor Airbyte Open Source:\n\nOnce the File Source is selected, you should define both the storage provider along its URL and format of the file.\nDepending on the provider choice and privacy of the data, you will have to configure more options.\n\n\nProvider Specific Information\n\nIn case of Google Drive, it is necesary to use the Download URL, the format for that is `https://drive.google.com/uc?export=download&id=[DRIVE_FILE_ID]` where `[DRIVE_FILE_ID]` is the string found in the Share URL here `https://drive.google.com/file/d/[DRIVE_FILE_ID]/view?usp=sharing`\nIn case of GCS, it is necessary to provide the content of the service account keyfile to access private buckets. See settings of BigQuery Destination\nIn case of AWS S3, the pair of `aws_access_key_id` and `aws_secret_access_key` is necessary to access private S3 buckets.\nIn case of AzBlob, it is necessary to provide the `storage_account` in which the blob you want to access resides. Either `sas_token` (info) or `shared_key` (info) is necessary to access private blobs.\nIn case of a locally stored file on a Windows OS, it's necessary to change the values for `LOCAL_ROOT`, `LOCAL_DOCKER_MOUNT` and `HACK_LOCAL_ROOT_PARENT` in the `.env` file to an existing absolute path on your machine (colons in the path need to be replaced with a double forward slash, //). `LOCAL_ROOT` & `LOCAL_DOCKER_MOUNT` should be the same value, and `HACK_LOCAL_ROOT_PARENT` should be the parent directory of the other two.\n\nReader Options\nThe Reader in charge of loading the file format is currently based on Pandas IO Tools. It is possible to customize how to load the file into a Pandas DataFrame as part of this Source Connector. This is doable in the `reader_options` that should be in JSON format and depends on the chosen file format. See pandas' documentation, depending on the format:\nFor example, if the format `CSV` is selected, then options from the read_csv functions are available.\n\nIt is therefore possible to customize the `delimiter` (or `sep`) to in case of tab separated files.\nHeader line can be ignored with `header=0` and customized with `names`\netc\n\nWe would therefore provide in the `reader_options` the following json:\n`{ \"sep\" : \"\\t\", \"header\" : 0, \"names\": \"column1, column2\"}`\nIn case you select `JSON` format, then options from the read_json reader are available.\nFor example, you can use the `{\"orient\" : \"records\"}` to change how orientation of data is loaded (if data is `[{column -> value}, \u2026 , {column -> value}]`)\nIf you need to read Excel Binary Workbook, please specify `excel_binary` format in `File Format` select.\nChanging data types of source columns\nNormally, Airbyte tries to infer the data type from the source, but you can use `reader_options` to force specific data types. If you input `{\"dtype\":\"string\"}`, all columns will be forced to be parsed as strings. If you only want a specific column to be parsed as a string, simply use `{\"dtype\" : {\"column name\": \"string\"}}`.\nExamples\nHere are a list of examples of possible file inputs:\n| Dataset Name      | Storage | URL                                                                                                                                                        | Reader Impl        | Service Account                                                | Description                                                                                                                                                                                                           |\n| ----------------- | ------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------ | -------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| epidemiology      | HTTPS   | https://storage.googleapis.com/covid19-open-data/v2/latest/epidemiology.csv |                    |                                                                | COVID-19 Public dataset on BigQuery |\n| hr_and_financials | GCS     | gs://airbyte-vault/financial.csv                                                                                                                           | smart_open or gcfs | {\"type\": \"service_account\", \"private_key_id\": \"XXXXXXXX\", ...} | data from a private bucket, a service account is necessary                                                                                                                                                            |\n| landsat_index     | GCS     | gcp-public-data-landsat/index.csv.gz                                                                                                                       | smart_open         |                                                                | Using smart_open, we don't need to specify the compression (note the gs:// is optional too, same for other providers)                                                                                                 |\nExamples with reader options:\n| Dataset Name  | Storage | URL                                             | Reader Impl | Reader Options                | Description                                                                                                                                      |\n| ------------- | ------- | ----------------------------------------------- | ----------- | ----------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------ |\n| landsat_index | GCS     | gs://gcp-public-data-landsat/index.csv.gz       | GCFS        | {\"compression\": \"gzip\"}       | Additional reader options to specify a compression option to `read_csv`                                                                          |\n| GDELT         | S3      | s3://gdelt-open-data/events/20190914.export.csv |             | {\"sep\": \"\\t\", \"header\": null} | Here is TSV data separated by tabs without header row from AWS Open Data                                 |\n| server_logs   | local   | /local/logs.log                                 |             | {\"sep\": \";\"}                  | After making sure a local text file exists at `/tmp/airbyte_local/logs.log` with logs file from some server that are delimited by ';' delimiters |\nExample for SFTP:\n| Dataset Name | Storage | User | Password | Host            | URL                     | Reader Options                                                          | Description                                                                                                                       |\n| ------------ | ------- | ---- | -------- | --------------- | ----------------------- | ----------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |\n| Test Rebext  | SFTP    | demo | password | test.rebext.net | /pub/example/readme.txt | {\"sep\": \"\\r\\n\", \"header\": null, \"names\": [\"text\"], \"engine\": \"python\"} | We use `python` engine for `read_csv` in order to handle delimiter of more than 1 character while providing our own column names. |\nPlease see (or add) more at `airbyte-integrations/connectors/source-file/integration_tests/integration_source_test.py` for further usages examples.\nPerformance Considerations and Notes\nIn order to read large files from a remote location, this connector uses the smart_open library. However, it is possible to switch to either GCSFS or S3FS implementations as it is natively supported by the `pandas` library. This choice is made possible through the optional `reader_impl` parameter.\n\nNote that for local filesystem, the file probably have to be stored somewhere in the `/tmp/airbyte_local` folder with the same limitations as the CSV Destination so the `URL` should also starts with `/local/`.\nPlease make sure that Docker Desktop has access to `/tmp` (and `/private` on a MacOS, as /tmp has a symlink that points to /private. It will not work otherwise). You allow it with \"File sharing\" in `Settings -> Resources -> File sharing -> add the one or two above folder` and hit the \"Apply & restart\" button.\nThe JSON implementation needs to be tweaked in order to produce more complex catalog and is still in an experimental state: Simple JSON schemas should work at this point but may not be well handled when there are multiple layers of nesting.\n",
    "tag": "airbyte"
  },
  {
    "title": "Marketo",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/marketo.md",
    "content": "Marketo\nThis page contains the setup guide and reference information for the Marketo source connector.\nPrerequisites\n\n(Optional) Whitelist Airbyte's IP address if needed\nAn API-only Marketo User Role\nAn Airbyte Marketo API-only user\nA Marketo API Custom Service\nMarketo Client ID & Client Secret\nMarketo Base URL\n\nSetup guide\nStep 1: Set up Marketo\nStep 1.1: (Optional) whitelist Airbyte's IP address\nIf you don't have IP Restriction enabled in Marketo, skip this step.\nIf you have IP Restriction enabled in Marketo, you'll need to whitelist the IP address of the machine running your Airbyte instance. To obtain your IP address, run `curl ifconfig.io` from the node running Airbyte. You might need to enlist an engineer to help with this. Copy the IP address returned and keep it on hand.\nOnce you have the IP address, whitelist it by following the Marketo documentation for allowlisting IP addresses for API based access.\nStep 1.2: Create an API-only Marketo User Role\nFollow the Marketo documentation for creating an API-only Marketo User Role.\nStep 1.3: Create an Airbyte Marketo API-only user\nFollow the Marketo documentation to create an API only user\nStep 1.4: Create a Marketo API custom service\nFollow the Marketo documentation for creating a custom service for use with a REST API.\nMake sure to follow the \"Credentials for API Access\" section in the Marketo docs to generate a Client ID and Client Secret. Once generated, copy those credentials and keep them handy for use in the Airbyte UI later.\nStep 1.5: Obtain your Endpoint and Identity URLs provided by Marketo\nFollow the Marketo documentation for obtaining your base URL. Specifically, copy your Endpoint without \"/rest\" and keep them handy for use in the Airbyte UI.\nWe're almost there! Armed with your Endpoint & Identity URLs and your Client ID and Secret, head over to the Airbyte UI to setup Marketo as a source.\nStep 2: Set up the Marketo connector in Airbyte\n\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the Marketo connector and select Marketo from the Source type dropdown.\nEnter the start date, domain URL, client ID and secret\nSubmit the form\n\n\n\nFor Airbyte Open Source:\n\nNavigate to the Airbyte Open Source dashboard\nSet the name for your source\nEnter the start date\nEnter the domain URL\nEnter client ID and secret\nClick Set up source\n\n\nSupported sync modes\nThe Marketo source connector supports the following sync modes:\n - Full Refresh | Overwrite\n - Full Refresh | Append\n - Incremental  | Append\n - Incremental  | Deduped\nSupported Streams\nThis connector can be used to sync the following tables from Marketo:\n\nactivities_X where X is an activity type contains information about lead activities of the type X. For example, activities_send_email contains information about lead activities related to the activity type `send_email`. See the Marketo docs for a detailed explanation of what each column means.\nactivity_types. Contains metadata about activity types. See the Marketo docs for a detailed explanation of columns.\ncampaigns. Contains info about your Marketo campaigns. Marketo docs.\nleads. Contains info about your Marketo leads. Marketo docs.\nlists. Contains info about your Marketo static lists. Marketo docs.\nprograms. Contains info about your Marketo programs. Marketo docs.\n\nPerformance considerations\nBy default, Marketo caps all accounts to 50,000 API calls per day.\nBy default, this connector caps itself to 40,000 API calls per day. But you can also customize the maximum number of API calls this source connector makes per day to Marketo (which may be helpful if you have for example other applications which are also hitting the Marketo API). If this source connector reaches the maximum number you configured, it will not replicate any data until the next day.\nIf the 50,000 limit is too stringent, contact Marketo support for a quota increase.\nData type map\n| Integration Type | Airbyte Type | Notes                                                                           |\n|:-----------------|:-------------|:--------------------------------------------------------------------------------|\n| `array`          | `array`      | primitive arrays are converted into arrays of the types described in this table |\n| `int`, `long`    | `number`     |                                                                                 |\n| `object`         | `object`     |                                                                                 |\n| `string`         | `string`     | ``                                                                            |\n| Namespaces       | No           |                                                                                 |",
    "tag": "airbyte"
  },
  {
    "title": "Slack",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/slack.md",
    "content": "Slack\nThis page contains the setup guide and reference information for the Slack source connector.\nPrerequisites\nYou can no longer create \"Legacy\" API Keys, but if you already have one, you can use it with this source. Fill it into the API key section.\nWe recommend creating a restricted, read-only key specifically for Airbyte access. This will allow you to control which resources Airbyte should be able to access.\nNote that refresh token are entirely optional for Slack and are not required to use Airbyte. You can learn more about refresh tokens here.\nSetup guide\nStep 1: Set up Slack\n:::info\nIf you are using an \"legacy\" Slack API, skip to the Airbyte Open Source additional setup steps section below.\n:::\nIn order to pull data out of your Slack instance, you need to create a Slack App. This may sound daunting, but it is actually pretty straight forward. Slack supplies documentation on how to build apps. Feel free to follow that if you want to do something fancy. We'll describe the steps we followed to creat the Slack App for this tutorial.\n:::info\nThis tutorial assumes that you are an administrator on your slack instance. If you are not, you will need to coordinate with your administrator on the steps that require setting permissions for your app.\n:::\n\nGo to the apps page\nClick \"Create New App\"\nIt will request a name and the slack instance you want to create the app for. Make sure you select the instance form which you want to pull data.\nCompleting that form will take you to the \"Basic Information\" page for your app.\nNow we need to grant the correct permissions to the app. (This is the part that requires you to be an administrator). Go to \"Permissions\". Then under \"Bot Token Scopes\" click on \"Add an OAuth Scope\". We will now need to add the following scopes:\n\n`text\n    channels:history\n    channels:join\n    channels:read\n    files:read\n    groups:read\n    links:read\n    reactions:read\n    remote_files:read\n    team:read\n    usergroups:read\n    users.profile:read\n    users:read`\nThis may look daunting, but the search functionality in the dropdown should make this part go pretty quick.\n\nScroll to the top of the page and click \"Install to Workspace\". This will generate a \"Bot User OAuth Access Token\". We will need this in a moment.\nNow go to your slack instance. For any public channel go to info => more => add apps. In the search bar search for the name of your app. (If using the desktop version of slack, you may need to restart Slack for it to pick up the new app). Airbyte will only replicate messages from channels that the Slack bot has been added to.\n\n\n\nIn Airbyte, create a Slack source. The \"Bot User OAuth Access Token\" from the earlier should be used as the token.\nYou can now pull data from your slack instance!\n\n\nAirbyte Open Source additional setup steps\nYou can no longer create \"Legacy\" API Keys, but if you already have one, you can use it with this source. Fill it into the API key section.\nWe recommend creating a restricted, read-only key specifically for Airbyte access. This will allow you to control which resources Airbyte should be able to access.\n\nStep 2: Set up the Slack connector in Airbyte\n\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the Slack connector and select Slack from the Source type dropdown.\nSelect `Authenticate your account` and log in and Authorize to the Slack account.\nEnter your `start_date`.\nEnter your `lookback_window`.\nEnter your `join_channels`.\nEnter your `channel_filter`.\nClick Set up source.\n\n\n\nFor Airbyte Open Source:\n\nNavigate to the Airbyte Open Source dashboard.\nSet the name for your source.\nEnter your `start_date`.\nEnter your `lookback_window`.\nEnter your `join_channels`.\nEnter your `channel_filter`.\nEnter your `api_token`.\nClick Set up source.\n\n\nSupported sync modes\nThe Slack source connector supports the following sync modes:\n| Feature           | Supported? |\n| :---------------- | :--------- |\n| Full Refresh Sync | Yes        |\n| Incremental Sync  | Yes        |\n| Namespaces        | No         |\nSupported Streams\n\nChannels (Conversations)\nChannel Members (Conversation Members)\nMessages (Conversation History) It will only replicate messages from non-archive, public channels that the Slack App is a member of.\nUsers\nThreads (Conversation Replies)\nUser Groups\nFiles\nRemote Files\n\nPerformance considerations\nThe connector is restricted by normal Slack requests limitation.\nIt is recommended to sync required channels only, this can be done by specifying config variable `channel_filter` in settings.\nData type map\n| Integration Type | Airbyte Type |\n| :--------------- | :----------- |\n| `string`         | `string`     |\n| `number`         | `number`     |\n| `array`          | `array`      |\n| `object`         | `object`     |",
    "tag": "airbyte"
  },
  {
    "title": "Chartmogul",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/chartmogul.md",
    "content": "Chartmogul\nThis page contains the setup guide and reference information for the Chartmogul source connector.\nPrerequisites\n\nAPI key\nStart date\nInterval\n\nSetup guide\nStep 1: Set up Chartmogul\n\nTo get access to the Chartmogul API you need to create an API key, please follow the instructions in this documentation.\n\nStep 2: Set up the Chartmogul connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click + new source.\nOn the source setup page, select Chartmogul from the Source type dropdown and enter a name for this connector.\nEnter the API key that you obtained.\nEnter Start date - UTC date and time in the format 2017-01-25T00:00:00Z. The data added on and after this date will be replicated.\nEnter the Interval - day, week, month, quarter for `CustomerCount` stream.\n\nSupported sync modes\nThe Chartmogul source connector supports the following  sync modes:\n\nFull Refresh - Overwrite\nFull Refresh - Append\n\nSupported Streams\nThis connector outputs the following full refresh streams:\n\nActivities\nCustomerCount\nCustomers\n\nNotes\nThe Start date will only apply to the `Activities` stream. The `Customers` endpoint does not provide a way to filter by the creation or update dates.\nPerformance considerations\nThe Chartmogul connector should not run into Chartmogul API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.",
    "tag": "airbyte"
  },
  {
    "title": "Datadog",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/datadog.md",
    "content": "Datadog\nThis is a setup guide for the Datadog source connector which collects data from its API.\nPrerequisites\nAn API key is required as well as an API application key. See the Datadog API and Application Keys section for more information.\nSetup guide\nStep 1: Set up the Datadog connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the Datadog connector and select Datadog from the Source type dropdown.\nEnter your `api_key` - Datadog API key.\nEnter your `application_key` - Datadog application key.\nEnter your `query` - Optional. Type your query to filter records when collecting data from Logs and AuditLogs stream.\nEnter your `limit` - Number of records to collect per request.\nEnter your `start_date` - Optional. Start date to filter records when collecting data from Logs and AuditLogs stream.\nEnter your `end_date` - Optional. End date to filter records when collecting data from Logs and AuditLogs stream.\nClick Set up source.\n\nFor Airbyte OSS:\n\nNavigate to the Airbyte Open Source dashboard.\nSet the name for your source. \nEnter your `api_key` - Datadog API key.\nEnter your `application_key` - Datadog application key.\nEnter your `query` - Optional. Type your query to filter records when collecting data from Logs and AuditLogs stream.\nEnter your `limit` - Number of records to collect per request.\nEnter your `start_date` - Optional. Start date to filter records when collecting data from Logs and AuditLogs stream.\nEnter your `end_date` - Optional. End date to filter records when collecting data from Logs and AuditLogs stream.\nClick Set up source.\n\nSupported sync modes\nThe Datadog source connector supports the following sync modes:\n| Feature           | Supported? |\n| :---------------- |:-----------|\n| Full Refresh Sync | Yes        |\n| Incremental Sync  | Yes        |\n| SSL connection    | Yes        |\n| Namespaces        | No         |\nSupported Streams\n\nAuditLogs\nDashboards\nDowntimes\nIncidentTeams\nIncidents\nLogs\nMetrics\nSyntheticTests\nUsers\n",
    "tag": "airbyte"
  },
  {
    "title": "Freshsales",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/freshsales.md",
    "content": "Freshsales\nOverview\nThe Freshsales supports full refresh syncs.\nOutput schema\nSeveral output streams are available from this source:\n\nContacts\nAccounts\nOpen Deals\nWon Deals\nLost Deals\nOpen Tasks\nCompleted Tasks\nPast appointments\nUpcoming appointments\n\nIf there are more endpoints you'd like Airbyte to support, please create an issue.\nFeatures\n| Feature           | Supported? |\n|:------------------|:-----------|\n| Full Refresh Sync | Yes        |\n| Incremental Sync  | No         |\n| SSL connection    | No         |\n| Namespaces        | No         |\nPerformance considerations\nThe Freshsales connector should not run into Freshsales API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.\nGetting started\nRequirements\n\nFreshsales Account\nFreshsales API Key\nFreshsales domain name\n\nSetup guide\nPlease read How to find your API key.",
    "tag": "airbyte"
  },
  {
    "title": "Bing Ads",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/bing-ads.md",
    "content": "Bing Ads\nThis page contains the setup guide and reference information for the Bing Ads source connector.\nSetup guide\nStep 1: Set up Bing Ads\n\nRegister your application in the Azure portal.\nRequest user consent to get the authorization code.\nUse the authorization code to get a refresh token.\n\n:::note\nThe refresh token expires in 90 days. Repeat the authorization process to get a new refresh token. The full authentication process described here.\nPlease be sure to authenticate with the email (personal or work) that you used to sign in to the Bing ads/Microsoft ads platform.\n:::\n\nGet your Microsoft developer token.\nIf your OAuth app has a custom tenant and you cannot use Microsoft\u2019s recommended common tenant, use the custom tenant in the Tenant ID field when you set up the connector.\n\n:::info\nThe tenant is used in the authentication URL, for example: `https://login.microsoftonline.com/<tenant>/oauth2/v2.0/authorize`\n:::\nStep 2: Set up the source connector in Airbyte\n\nFor Airbyte Cloud:\n1. Log in to your Airbyte Cloud account.\n2. Click Sources and then click + New source.\n3. On the Set up the source page, select Bing Ads from the Source type dropdown.\n4. Enter a name for your source.\n5. For Tenant ID, enter the custom tenant or use the common tenant.\n6. Add the developer token from Step 1.\n7. For Replication Start Date, enter the date in YYYY-MM-DD format. The data added on and after this date will be replicated. If this field is blank, Airbyte will replicate all data.\n8. Click Authenticate your Bing Ads account.\n9. Log in and authorize the Bing Ads account.\n10. Click Set up source.  \n\n\nFor Airbyte Open Source:\n1. Log in to your Airbyte Open Source account.\n2. Click Sources and then click + New source.\n3. On the Set up the source page, select Bing Ads from the Source type dropdown.\n4. Enter a name for your source.\n5. For Tenant ID, enter the custom tenant or use the common tenant.\n6. Enter the Client ID, Client Secret, Refresh Token, and Developer Token from Step 1.\n7. For Replication Start Date, enter the date in YYYY-MM-DD format. The data added on and after this date will be replicated. If this field is blank, Airbyte will replicate all data.\n8. Click Set up source.\n\nSupported sync modes\nThe Bing Ads source connector supports the following sync modes:\n* Full Refresh - Overwrite\n* Full Refresh - Append\n* Incremental - Append\n* Incremental - Deduped History\nSupported Streams\nThe Bing Ads source connector supports the following streams. For more information, see the Bing Ads API.\nBasic streams\n\naccounts\nad_groups\nads\ncampaigns\n\nReport Streams\n\naccount_performance_report_hourly\naccount_performance_report_daily\naccount_performance_report_weekly\naccount_performance_report_monthly\nad_group_performance_report_hourly\nad_group_performance_report_daily\nad_group_performance_report_weekly\nad_group_performance_report_monthly\nad_performance_report_hourly\nad_performance_report_daily\nad_performance_report_weekly\nad_performance_report_monthly\nbudget_summary_report\ncampaign_performance_report_hourly\ncampaign_performance_report_daily\ncampaign_performance_report_weekly\ncampaign_performance_report_monthly\nkeyword_performance_report_hourly\nkeyword_performance_report_daily\nkeyword_performance_report_weekly\nkeyword_performance_report_monthly\n\nReport aggregation\nAll reports synced by this connector can be aggregated using hourly, daily, weekly, or monthly time windows.\nFor example, if you select a report with daily aggregation, the report will contain a row for each day for the duration of the report. Each row will indicate the number of impressions recorded on that day.\nA report's aggregation window is indicated in its name. For example, `account_performance_report_hourly` is the Account Performance Reported aggregated using an hourly window.\nPerformance considerations\nThe Bing Ads API limits the number of requests for all Microsoft Advertising clients. You can find detailed info here.",
    "tag": "airbyte"
  },
  {
    "title": "Zendesk Sunshine",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/zendesk-sunshine.md",
    "content": "Zendesk Sunshine\nSync overview\nThe Zendesk Chat source supports Full Refresh and Incremental syncs.\nThis source can sync data for the Zendesk Sunshine API.\nOutput schema\nThis Source is capable of syncing the following core Streams:\n\nObjectTypes\nObjectRecords\nRelationshipTypes\nRelationshipRecords\nObjectTypePolicies\nJobs \n\nThis stream is currently not available because it stores data temporary.\n\nLimits\n\nData type mapping\n| Integration Type | Airbyte Type | Notes |\n| :--- | :--- | :--- |\n| `string` | `string` |  |\n| `number` | `number` |  |\n| `array` | `array` |  |\n| `object` | `object` |  |\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | Yes |  |\nPerformance considerations\nThe connector is restricted by normal Zendesk requests limitation\nThe Zendesk connector should not run into Zendesk API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.\nGetting started\nRequirements\n\nZendesk Sunshine API Token\n\nOR\n* Zendesk Sunshine oauth2.0 application (client_id, client_secret, access_token)\nSetup guide\nPlease follow this guide\nGenerate an API Token or oauth2.0 Access token as described in here\nWe recommend creating a restricted, read-only key specifically for Airbyte access. This will allow you to control which resources Airbyte should be able to access.",
    "tag": "airbyte"
  },
  {
    "title": "News API",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/news-api.md",
    "content": "News API\nSync overview\nThis source retrieves news stories from the News API.\nIt can retrieve news stories all news stories found within the parameters\nchosen, or just top headlines.\nOutput schema\nThis source is capable of syncing the following streams:\n\n`everything`\n`top_headlines`\n\nFeatures\n| Feature           | Supported? (Yes/No) | Notes |\n|:------------------|:----------------------|:------|\n| Full Refresh Sync | Yes                   |       |\n| Incremental Sync  | No                    |       |\nPerformance considerations\nThe News API free tier only allows 100 requests per day, and only up to 100\nresults per request. It is not recommended to attempt to use this source with\na free tier API key.\nGetting started\nRequirements\n\nA News API key. You can get one here. It is\n   highly recommended to use a paid tier key.\n\nSetup guide\nThe following fields are required fields for the connector to work:\n\n`api_key`: Your News API key.\n(optional) `search_query`: A search query to filter the results by. For more\n  information on constructing a search query, see the\n  News API documentation.\n(optional) `search_in`: Fields to search in. Possible values are `title`,\n  `description` and `content`.\n(optional) `sources`: Sources to search in. For a list of sources, see the\n  News API documentation.\n(optional) `domains`: Domains to search in.\n(optional) `exclude_domains`: Domains to exclude from the search.\n(optional) `start_date`: The start date to search from.\n(optional) `end_date`: The end date to search to.\n(optional) `language`: The language to search in.\n`country`: The country you want headlines for.\n`category`: The category you want headlines for.\n`sort_by`: How to sort the results.\n",
    "tag": "airbyte"
  },
  {
    "title": "Oracle Peoplesoft",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/oracle-peoplesoft.md",
    "content": "Oracle Peoplesoft\nOracle PeopleSoft is a Human Resource, Financial, Supply Chain, Customer Relationship, and Enterprise Performance Management System.\nSync overview\nOracle PeopleSoft can run on the Oracle, MSSQL, or IBM DB2 databases. You can use Airbyte to sync your Oracle PeopleSoft instance by connecting to the underlying database using the appropriate Airbyte connector:\n\nDB2\nMSSQL\nOracle\n\n:::info\nReach out to your service representative or system admin to find the parameters required to connect to the underlying database\n:::\nOutput schema\nThe schema will be loaded according to the rules of the underlying database's connector. Oracle provides ERD diagrams but they are behind a paywall. Contact your Oracle rep to gain access.",
    "tag": "airbyte"
  },
  {
    "title": "Newsdata API",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/newsdata.md",
    "content": "Newsdata API\nSync overview\nThis source retrieves the latests news from the Newsdata API.\nOutput schema\nThis source is capable of syncing the following streams:\n\n`latest`\n`sources` \nNOTE: `category`, `language` and `country` input parameters only accept a single value, not multiple like `latest` stream. \n  Thus, if several values are supplied, the first one will be the one to be used.\n\nIf there are more endpoints you'd like Airbyte to support, please create an issue.\nFeatures\n| Feature           | Supported? | Notes |\n|:------------------|------------|:------|\n| Full Refresh Sync | Yes        |       |\n| Incremental Sync  | No         |       |\nPerformance considerations\nThe News API free tier only allows 200 requests per day, and only up to 10\nnews per request.\nThe free tier does not allow to perform advanced search queries.\nGetting started\nRequirements\n\nA Newsdata API key. You can get one here.\n\nSetup guide\nThe following fields are required fields for the connector to work:\n\n`api_key`: Your Newsdata API key.\n",
    "tag": "airbyte"
  },
  {
    "title": "Apify Dataset",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/apify-dataset.md",
    "content": "\ndescription: Web scraping and automation platform.\nApify Dataset\nOverview\nApify is a web scraping and web automation platform providing both ready-made and custom solutions, an open-source SDK for web scraping, proxies, and many other tools to help you build and run web automation jobs at scale.\nThe results of a scraping job are usually stored in Apify Dataset. This Airbyte connector allows you to automatically sync the contents of a dataset to your chosen destination using Airbyte.\nTo sync data from a dataset, all you need to know is its ID. You will find it in Apify console under storages.\nRunning Airbyte sync from Apify webhook\nWhen your Apify job (aka actor run) finishes, it can trigger an Airbyte sync by calling the Airbyte API manual connection trigger (`POST /v1/connections/sync`). The API can be called from Apify webhook which is executed when your Apify run finishes.\n\nOutput schema\nSince the dataset items do not have strongly typed schema, they are synced as objects, without any assumption on their content.\nFeatures\n| Feature | Supported? |\n| :--- | :--- |\n| Full Refresh Sync | Yes |\n| Incremental Sync | No |\nPerformance considerations\nThe Apify dataset connector uses Apify Python Client under the hood and should handle any API limitations under normal usage.\nGetting started\nRequirements\n\nApify dataset ID\n\nChangelog\n| Version | Date | Pull Request | Subject |\n| :--- | :--- | :--- | :--- |\n| 0.1.11 | 2022-04-27 | 12397 | No changes. Used connector to test publish workflow changes. |\n| 0.1.9 | 2022-04-05 | PR#11712 | No changes from 0.1.4. Used connector to test publish workflow changes. |\n| 0.1.4 | 2021-12-23 | PR#8434 | Update fields in source-connectors specifications |\n| 0.1.2 | 2021-11-08 | PR#7499 | Remove base-python dependencies |\n| 0.1.0 | 2021-07-29 | PR#5069 | Initial version of the connector |",
    "tag": "airbyte"
  },
  {
    "title": "Intercom",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/intercom.md",
    "content": "Intercom\nThis page guides you through the process of setting up the Intercom source connector.\nSet up the Intercom connector\n\nLog into your Airbyte Cloud or Airbyte Open Source account.\nClick Sources and then click + New source. \nOn the Set up the source page, select Intercom from the Source type dropdown.\nEnter a name for your source.\nFor Start date, enter the date in YYYY-MM-DDTHH:mm:ssZ format. The data added on and after this date will be replicated. If this field is blank, Airbyte will replicate all data.\nFor Airbyte Cloud, click Authenticate your Intercom account to sign in with Intercom and authorize your account. \n   For Airbyte Open Source, enter your Access Token to authenticate your account.\nClick Set up source.\n\nSupported sync modes\nThe Intercom source connector supports the following sync modes:\n\nFull Refresh\nIncremental\n\nSupported Streams\nThe Intercom source connector supports the following streams:\n\nAdmins (Full table)\nCompanies (Incremental)\nCompany Segments (Incremental)\nConversations (Incremental)\nConversation Parts (Incremental)\nData Attributes (Full table)\nCustomer Attributes (Full table)\nCompany Attributes (Full table)\nContacts (Incremental)\nSegments (Incremental)\nTags (Full table)\nTeams (Full table)\n\nPerformance considerations\nThe connector is restricted by normal Intercom requests limitation.\nThe Intercom connector should not run into Intercom API limitations under normal usage. Create an issue if you see any rate limit issues that are not automatically retried successfully.",
    "tag": "airbyte"
  },
  {
    "title": "Twilio",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/twilio.md",
    "content": "Twilio\nThis page contains the setup guide and reference information for the Twilio source connector.\nPrerequisites\nTwilio HTTP requests to the REST API are protected with HTTP Basic authentication. In short, you will use your Twilio Account SID as the username and your Auth Token as the password for HTTP Basic authentication.\nYou can find your Account SID and Auth Token on your dashboard.\nSee docs for more details.\nSetup guide\n\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the Twilio connector and select Twilio from the  type dropdown.\nEnter your `account_sid`.\nEnter your `auth_token`.\nEnter your `start_date`.\nEnter your `lookback_window`.\nClick Set up source.\n\n\n\nFor Airbyte Open Source:\n\nNavigate to the Airbyte Open Source dashboard.\nSet the name for your source.\nEnter your `account_sid`.\nEnter your `auth_token`.\nEnter your `start_date`.\nEnter your `lookback_window`.\nClick Set up source.\n\n\nSupported sync modes\nThe Twilio source connector supports the following sync modes:\n| Feature                       | Supported? |\n| :---------------------------- | :--------- |\n| Full Refresh Sync             | Yes        |\n| Incremental Sync              | Yes        |\n| Replicate Incremental Deletes | No         |\n| SSL connection                | Yes        |\n| Namespaces                    | No         |\nSupported Streams\n\nAccounts\nAddresses\nAlerts (Incremental)\nApplications\nAvailable Phone Number Countries (Incremental)\nAvailable Phone Numbers Local (Incremental)\nAvailable Phone Numbers Mobile (Incremental)\nAvailable Phone Numbers Toll Free (Incremental)\nCalls (Incremental)\nConference Participants (Incremental)\nConferences (Incremental)\nDependent Phone Numbers (Incremental)\nIncoming Phone Numbers (Incremental)\nKeys\nMessage Media (Incremental)\nMessages (Incremental)\nOutgoing Caller Ids\nQueues\nRecordings (Incremental)\nTranscriptions\nUsage Records (Incremental)\nUsage Triggers\n\nPerformance considerations\nThe Twilio connector will gracefully handle rate limits.\nFor more information, see the Twilio docs for rate limitations.",
    "tag": "airbyte"
  },
  {
    "title": "RD Station Marketing",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/rd-station-marketing.md",
    "content": "RD Station Marketing\nRD Station Marketing is the leading Marketing Automation tool in Latin America. It is a software application that helps your company carry out better campaigns, nurture Leads, generate qualified business opportunities and achieve more results. From social media to email, Landing Pages, Pop-ups, even Automations and Analytics.\nPrerequisites\n\nAn RD Station account\nA callback URL to receive the first account credential (can be done using localhost)\n`client_id` and `client_secret` credentials. Access this link to register a new application and start the authentication flow. \n\nAirbyte Open Source\n\nStart Date\nClient Id\nClient Secret\nRefresh token\n\nSupported sync modes\nThe RD Station Marketing source connector supports the following sync modes:\n - Full Refresh\n - Incremental (for analytics endpoints)\nSupported Streams\n\nconversions (analytics endpoint)\nemails (analytics endpoint)\nfunnel (analytics endpoint)\nworkflow_emails_statistics (analytics endpoint)\nemails\nembeddables\nfields\nlanding_pages\npopups\nsegmentations\nworkflows\n\nPerformance considerations\nEach endpoint has its own performance limitations, which also consider the account plan. For more informations, visit the page API request limit.",
    "tag": "airbyte"
  },
  {
    "title": "Auth0",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/auth0.md",
    "content": "Auth0\nAuth0 is a flexible, drop-in solution to add authentication and authorization services to your applications.\nThe source connector fetches data from Auth0 Management API\nPrerequisites\n\nYou own an Auth0 account, free or paid.\nFollow the Setup guide to authorize Airbyte to read data from your account.\n\nSetup guide\nStep 1: Set up an Auth0 account\n\nIt's free to sign up an account in Auth0.\nConfirm your Email.\n\nStep 2.1: Get an Access Tokens for Testing\n\nIn Auth0, go to the Api Explorer tab of your Auth0 Management API. A token is automatically generated and displayed there.\nClick Copy Token.\nIn Airbyte, choose OAuth2 Access Token under the Authentication Method menu, Paste the token to the text box of OAuth2 Access Token\nClick Save to test the connectivity.\nMore details can be found from this documentation.\n\nStep 2.2: Create a new app for OAuth2\n\nTo make scheduled frequent calls for a production environment, you have setup an OAuth2 integration so that Airbyte can generate the access token automatically.\nIn Auth0, go to Dashboard > Applications > Applications.\nCreate a new application, name it Airbyte. Choose the application type Machine to Machine Applications\nSelect the Management API V2, this is the api you want call from Airbyte.\nEach M2M app that accesses an API must be granted a set of permissions (or scopes). Here, we only need permissions starting with `read` (e.g. read:users). Under the API doc, each api will list the required scopes.\nMore details can be found from this documentation.\n\nSupported sync modes\nThe Auth0 source connector supports the following sync modes:\n - Full Refresh\n - Incremental\nSupported Streams\n\nUsers\n\nPerformance considerations\nThe connector is restricted by Auth0 rate limits.",
    "tag": "airbyte"
  },
  {
    "title": "YouTube Analytics",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/youtube-analytics.md",
    "content": "YouTube Analytics\nThis page contains the setup guide and reference information for the YouTube Analytics source connector.\nPrerequisites\nYouTube does not start to generate a report until you create a reporting job for that report. \nAirbyte creates a reporting job for your report or uses current reporting job if it's already exists. \nThe report will be available within 48 hours of creating the reporting job and will be for the day that the job was scheduled. \nFor example, if you schedule a job on September 1, 2015, then the report for September 1, 2015, will be ready on September 3, 2015. \nThe report for September 2, 2015, will be posted on September 4, 2015, and so forth. \nYoutube also generates historical data reports covering the 30-day period prior to when you created the job. Airbyte syncs all available historical data too.\nSetup guide\nStep 1: Set up YouTube Analytics\n\nGo to the YouTube Reporting API dashboard in the project for your service user. Enable the API for your account.\nUse your Google account and authorize over Google's OAuth 2.0 on connection setup. Please make sure to grant the following authorization scope: `https://www.googleapis.com/auth/yt-analytics.readonly`.\n\nStep 2: Set up the YouTube Analytics connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the YouTube Analytics connector and select YouTube Analytics from the Source type dropdown.\nSelect `Authenticate your account`.\nLog in and Authorize to the Instagram account and click `Set up source`.\n\nFor Airbyte OSS:\n\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the YouTube Analytics connector and select YouTube Analytics from the Source type dropdown.\nSelect `client_id`\nSelect `client_secret`\nSelect `refresh_token`\nClick `Set up source`.\n\nSupported sync modes\nThe YouTube Analytics source connector supports the following sync modes:\n| Feature | Supported? |\n| :--- | :--- |\n| Full Refresh Sync | Yes |\n| Incremental Sync | Yes |\n| SSL connection | Yes |\n| Channel Reports | Yes |\n| Content Owner Reports | Coming soon |\n| YouTube Data API | Coming soon |\nSupported Streams\n\nchannel_annotations_a1\nchannel_basic_a2\nchannel_cards_a1\nchannel_combined_a2\nchannel_demographics_a1\nchannel_device_os_a2\nchannel_end_screens_a1\nchannel_playback_location_a2\nchannel_province_a2\nchannel_sharing_service_a1\nchannel_subtitles_a2\nchannel_traffic_source_a2\nplaylist_basic_a1\nplaylist_combined_a1\nplaylist_device_os_a1\nplaylist_playback_location_a1\nplaylist_province_a1\nplaylist_traffic_source_a1\n\nPerformance considerations\n\nFree requests per day: 20,000\nFree requests per 100 seconds: 100\nFree requests per minute: 60\n\nQuota usage is not an issue because data is retrieved once and then filtered, sorted, and queried within the application.",
    "tag": "airbyte"
  },
  {
    "title": "AWS CloudTrail",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/aws-cloudtrail.md",
    "content": "AWS CloudTrail\nOverview\nThe AWS CloudTrail source supports both Full Refresh and Incremental syncs. You can choose if this connector will copy only the new or updated data, or all rows in the tables and columns you set up for replication, every time a sync is run.\nThis Source Connector is based on a Boto3 CloudTrail.\nOutput schema\nThis Source is capable of syncing the following core Streams:\n\nManagement Events\n\nInsight events are not supported right now. Only Management events are available.\nData type mapping\n| Integration Type | Airbyte Type | Notes |\n| :--------------- | :----------- | :---- |\n| `string`         | `string`     |       |\n| `number`         | `integer`    |       |\n| `array`          | `array`      |       |\n| `object`         | `object`     |       |\nFeatures\n| Feature           | Supported?(Yes/No) | Notes |\n| :---------------- | :------------------- | :---- |\n| Full Refresh Sync | Yes                  |       |\n| Incremental Sync  | Yes                  |       |\n| Namespaces        | No                   |       |\nPerformance considerations\nThe rate of lookup requests for `events` stream is limited to two per second, per account, per region. This connector gracefully retries when encountering a throttling error. However if the errors continue repeatedly after multiple retries (for example if you setup many instances of this connector using the same account and region), the connector sync will fail.\nGetting started\nRequirements\n\nAWS Access key ID\nAWS Secret access key\nAWS region name\n\nSetup guide\nPlease, follow this steps to get your AWS access key and secret.",
    "tag": "airbyte"
  },
  {
    "title": "Aha API",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/aha.md",
    "content": "Aha API\nAPI Documentation link here\nOverview\nThe Aha API source supports full refresh syncs\nOutput schema\nTwo output streams are available from this source:\nfeatures.\nproducts.\nFeatures\n| Feature           | Supported? |\n|:------------------|:-----------|\n| Full Refresh Sync | Yes        |\n| Incremental Sync  | No         |\nPerformance considerations\nRate Limiting information is updated here.\nGetting started\nRequirements\n\nAha API Key.\n\nConnect using `API Key`:\n\nGenerate an API Key as described here.\nUse the generated `API Key` in the Airbyte connection.\n",
    "tag": "airbyte"
  },
  {
    "title": "Recruitee",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/recruitee.md",
    "content": "Recruitee\nThis page contains the setup guide and reference information for the Recruitee source connector.\nYou can find more information about the Recruitee REST API here.\nPrerequisites\nYou can find your Company ID and find or create an API key within Recruitee.\nSetup guide\nStep 1: Set up the Recruitee connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the Recruitee connector and select Recruitee from the Source type dropdown.\nEnter your `company_id` - Recruitee Company ID.\nEnter your `api_key` - Recruitee API key.\nClick Set up source.\n\nFor Airbyte OSS:\n\nNavigate to the Airbyte Open Source dashboard.\nSet the name for your source. \nEnter your `company_id` - Recruitee Company ID.\nEnter your `api_key` - Recruitee API key.\nClick Set up source.\n\nSupported sync modes\nThe Recruitee source connector supports the following sync modes:\n| Feature           | Supported? |\n| :---------------- | :--------- |\n| Full Refresh Sync | Yes        |\n| Incremental Sync  | No         |\n| SSL connection    | Yes        |\n| Namespaces        | No         |\nSupported Streams\n\nCandidates\nOffers\nDepartments\n",
    "tag": "airbyte"
  },
  {
    "title": "Pexels-API",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/pexels-api.md",
    "content": "Pexels-API\nThis page contains the setup guide and reference information for the Pexels-API source connector.\nPrerequisites\nApi key is mandate for this connector to work, It could be generated by a gmail account for free at https://www.pexels.com/api/new/.\nJust pass the generated API key and optional parameters for establishing the connection.\nSetup guide\nStep 1: Set up Pexels-API connection\n\nGenerate an API key (Example: 12345)\nParams (If specific info is needed)\nAvailable params\nquery: Ocean, Tigers, Pears, etc. Default is people\norientation: landscape, portrait or square. Default is landscape\nsize: large, medium, small. Default is large\ncolor: red, orange, yellow, green, turquoise, blue, violet, pink, brown, black, gray, white or any hexidecimal color code.\nlocale: en-US, pt-BR, es-ES, ca-ES, de-DE, it-IT, fr-FR, sv-SE, id-ID, pl-PL, ja-JP, zh-TW, zh-CN, ko-KR, th-TH, nl-NL, hu-HU, vi-VN, cs-CZ, da-DK, fi-FI, uk-UA, el-GR, ro-RO, nb-NO, sk-SK, tr-TR, ru-RU. Default is en-US\n\nStep 2: Set up the Pexels-APIs connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the Pexels-API connector and select Pexels-API from the Source type dropdown.\nEnter your `api_key`.\nEnter the params configuration if needed. Supported params are: query, orientation, size, color, locale, collection_id \\\n   video_id, photo_id\nClick Set up source.\n\nFor Airbyte OSS:\n\nNavigate to the Airbyte Open Source dashboard.\nSet the name for your source.\nEnter your `api_key`.\nEnter the params configuration if needed. Supported params are: query, orientation, size, color, locale, collection_id \\\n   video_id, photo_id\nClick Set up source.\n\nSupported sync modes\nThe Pexels-API source connector supports the following sync modes:\n| Feature                       | Supported? |\n| :---------------------------- | :--------- |\n| Full Refresh Sync             | Yes        |\n| Incremental Sync              | No         |\n| Replicate Incremental Deletes | No         |\n| SSL connection                | Yes        |\n| Namespaces                    | No         |\nSupported Streams\n\nphotos_search\nphotos_curated\nvideos_search\nvideos_popular\ncollection_featured\n\nAPI method example\nGET https://api.pexels.com/v1/curated?per_page=1\nPerformance considerations\nPexels-API's [API reference]https://www.pexels.com/api/documentation) has v1 at present and v2 is at development. The connector as default uses v1.",
    "tag": "airbyte"
  },
  {
    "title": "Toggl API",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/toggl.md",
    "content": "Toggl API\nSync overview\nThis source can sync data from the Toggl API. At present this connector only supports full refresh syncs meaning that each time you use the connector it will sync all available records from scratch. Please use cautiously if you expect your API to have a lot of records.\nThis Source Supports the Following Streams\n\ntime_entries\norganizations\norganizations_users\norganizations_groups\nworkspace\nworkspace_clients\nworkspace_tasks\n\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | No |  |\nPerformance considerations\nToggl APIs are under rate limits for the number of API calls allowed per API keys per second. If you reach a rate limit, API will return a 429 HTTP error code. See here\nGetting started\nRequirements\n\nAPI token\n",
    "tag": "airbyte"
  },
  {
    "title": "Alpha Vantage",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/alpha-vantage.md",
    "content": "Alpha Vantage\nSync overview\nThis source retrieves time series data from the free\nAlpha Vantage API. It supports intraday, daily,\nweekly and monthly time series data.\nOutput schema\nThis source is capable of syncing the following streams:\n\n`time_series_intraday`\n`time_series_daily`\n`time_series_daily_adjusted` (premium only)\n`time_series_weekly`\n`time_series_weekly_adjusted`\n`time_series_monthly`\n`time_series_monthly_adjusted`\n\nFeatures\n| Feature           | Supported? (Yes/No) | Notes                                                   |\n|:------------------|:----------------------|:--------------------------------------------------------|\n| Full Refresh Sync | Yes                   |                                                         |\n| Incremental Sync  | No                    |                                                         |\n| API Environments  | Yes                   | Both sandbox and production environments are supported. |\nPerformance considerations\nSince a single API call returns the full history of a time series if\nconfigured, it is recommended to use `Full Refresh` with `Overwrite` to avoid \nstoring duplicate data.\nAlso, the data returned can be quite large.\nGetting started\nRequirements\n\nObtain an API key from Alpha Vantage.\n\nSetup guide\nThe following fields are required fields for the connector to work:\n\n`api_key`: Your Alpha Vantage API key.\n`symbol`: The symbol of the time series to retrieve, with exchange code if\n  applicable. For example, `MSFT` or `TSCO.LON`.\n(optional) `interval`: The time-series data point interval. Defaults to 1 minute.\n(optional) `Adjusted?`: Whether the `intraday` endpoint should return adjusted\n  data. Defaults to `false`.\n(optional) `outputsize`: The size of the time series to retrieve. Defaults to\n  `compact`, which returns the last 100 data points. `full` returns the full\n  history.\n",
    "tag": "airbyte"
  },
  {
    "title": "MySQL",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/mysql.md",
    "content": "MySQL\nFeatures\n| Feature                       | Supported | Notes                             |\n| :---------------------------- | :-------- | :-------------------------------- |\n| Full Refresh Sync             | Yes       |                                   |\n| Incremental - Append Sync     | Yes       |                                   |\n| Replicate Incremental Deletes | Yes       |                                   |\n| CDC                           | Yes       |                                   |\n| SSL Support                   | Yes       |                                   |\n| SSH Tunnel Connection         | Yes       |                                   |\n| Namespaces                    | Yes       | Enabled by default                |\n| Arrays                        | Yes       | Byte arrays are not supported yet |\nThe MySQL source does not alter the schema present in your database. Depending on the destination connected to this source, however, the schema may be altered. See the destination's documentation for more details.\nTroubleshooting\nThere may be problems with mapping values in MySQL's datetime field to other relational data stores. MySQL permits zero values for date/time instead of NULL which may not be accepted by other data stores. To work around this problem, you can pass the following key value pair in the JDBC connector of the source setting `zerodatetimebehavior=Converttonull`.\nSome users reported that they could not connect to Amazon RDS MySQL or MariaDB. This can be diagnosed with the error message: `Cannot create a PoolableConnectionFactory`.\nTo solve this issue add `enabledTLSProtocols=TLSv1.2` in the JDBC parameters.\nAnother error that users have reported when trying to connect to Amazon RDS MySQL is `Error: HikariPool-1 - Connection is not available, request timed out after 30001ms.`. Many times this is can be due to the VPC not allowing public traffic, however, we recommend going through this AWS troubleshooting checklist to the correct permissions/settings have been granted to allow connection to your database.\nGetting Started (Airbyte Cloud)\nOn Airbyte Cloud, only TLS connections to your MySQL instance are supported. Other than that, you can proceed with the open-source instructions below.\nGetting Started (Airbyte Open-Source)\nRequirements\n\nMySQL Server `8.0`, `5.7`, or `5.6`.\nCreate a dedicated read-only Airbyte user with access to all tables needed for replication.\n\n1. Make sure your database is accessible from the machine running Airbyte\nThis is dependent on your networking setup. The easiest way to verify if Airbyte is able to connect to your MySQL instance is via the check connection tool in the UI.\n2. Create a dedicated read-only user with access to the relevant tables (Recommended but optional)\nThis step is optional but highly recommended to allow for better permission control and auditing. Alternatively, you can use Airbyte with an existing user in your database.\nTo create a dedicated database user, run the following commands against your database:\n`sql\nCREATE USER 'airbyte'@'%' IDENTIFIED BY 'your_password_here';`\nThe right set of permissions differ between the `STANDARD` and `CDC` replication method. For `STANDARD` replication method, only `SELECT` permission is required.\n`sql\nGRANT SELECT ON <database name>.* TO 'airbyte'@'%';`\nFor `CDC` replication method, `SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT` permissions are required.\n`sql\nGRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'airbyte'@'%';`\nYour database user should now be ready for use with Airbyte.\n3. Set up CDC\nFor `STANDARD` replication method this is not applicable. If you select the `CDC` replication method then only this is required. Please read the section on CDC below for more information.\n4. That's it!\nYour database user should now be ready for use with Airbyte.\nChange Data Capture (CDC)\n\nIf you need a record of deletions and can accept the limitations posted below, you should be able to use CDC for MySQL.\nIf your data set is small, and you just want snapshot of your table in the destination, consider using Full Refresh replication for your table instead of CDC.\nIf the limitations prevent you from using CDC and your goal is to maintain a snapshot of your table in the destination, consider using non-CDC incremental and occasionally reset the data and re-sync.\nIf your table has a primary key but doesn't have a reasonable cursor field for incremental syncing (i.e. `updated_at`), CDC allows you to sync your table incrementally.\n\nCDC Limitations\n\nMake sure to read our CDC docs to see limitations that impact all databases using CDC replication.\nOur CDC implementation uses at least once delivery for all change records.\n\n1. Enable binary logging\nYou must enable binary logging for MySQL replication. The binary logs record transaction updates for replication tools to propagate changes. You can configure your MySQL server configuration file with the following properties, which are described in below:\n`text\nserver-id                  = 223344\nlog_bin                    = mysql-bin\nbinlog_format              = ROW\nbinlog_row_image           = FULL\nbinlog_expire_log_seconds  = 864000`\n\nserver-id : The value for the server-id must be unique for each server and replication client in the MySQL cluster. The `server-id` should be a non-zero value. If the `server-id` is already set to a non-zero value, you don't need to make any change. You can set the `server-id` to any value between 1 and 4294967295. For more information refer mysql doc\nlog_bin : The value of log_bin is the base name of the sequence of binlog files. If the `log_bin` is already set, you don't need to make any change. For more information refer mysql doc\nbinlog_format : The `binlog_format` must be set to `ROW`. For more information refer mysql doc\nbinlog_row_image : The `binlog_row_image` must be set to `FULL`. It determines how row images are written to the binary log. For more information refer mysql doc\nbinlog_expire_log_seconds : This is the number of seconds for automatic binlog file removal. We recommend 864000 seconds (10 days) so that in case of a failure in sync or if the sync is paused, we still have some bandwidth to start from the last point in incremental sync. We also recommend setting frequent syncs for CDC.\n\n2. Enable GTIDs (Optional)\nGlobal transaction identifiers (GTIDs) uniquely identify transactions that occur on a server within a cluster. Though not required for a Airbyte MySQL connector, using GTIDs simplifies replication and enables you to more easily confirm if primary and replica servers are consistent. For more information refer mysql doc\n\nEnable gtid_mode : Boolean that specifies whether GTID mode of the MySQL server is enabled or not. Enable it via `mysql> gtid_mode=ON`\nEnable enforce_gtid_consistency : Boolean that specifies whether the server enforces GTID consistency by allowing the execution of statements that can be logged in a transactionally safe manner. Required when using GTIDs. Enable it via `mysql> enforce_gtid_consistency=ON`\n\n3. Set up initial waiting time(Optional)\n:::warning\nThis is an advanced feature. Use it if absolutely necessary.\n:::\nThe MySQl connector may need some time to start processing the data in the CDC mode in the following scenarios:\n\nWhen the connection is set up for the first time and a snapshot is needed\nWhen the connector has a lot of change logs to process\n\nThe connector waits for the default initial wait time of 5 minutes (300 seconds). Setting the parameter to a longer duration will result in slower syncs, while setting it to a shorter duration may cause the connector to not have enough time to create the initial snapshot or read through the change logs. The valid range is 300 seconds to 1200 seconds.\nIf you know there are database changes to be synced, but the connector cannot read those changes, the root cause may be insufficient waiting time. In that case, you can increase the waiting time (example: set to 600 seconds) to test if it is indeed the root cause. On the other hand, if you know there are no database changes, you can decrease the wait time to speed up the zero record syncs.\n4. Set up server timezone(Optional)\n:::warning\nThis is an advanced feature. Use it if absolutely necessary.\n:::\nIn CDC mode, the MySQl connector may need a timezone configured if the existing MySQL database been set up with a system timezone that is not recognized by the IANA Timezone Database.\nIn this case, you can configure the server timezone to the equivalent IANA timezone compliant timezone. (e.g. CEST -> Europe/Berlin).\nNote\nWhen a sync runs for the first time using CDC, Airbyte performs an initial consistent snapshot of your database. Airbyte doesn't acquire any table locks (for tables defined with MyISAM engine, the tables would still be locked) while creating the snapshot to allow writes by other database clients. But in order for the sync to work without any error/unexpected behaviour, it is assumed that no schema changes are happening while the snapshot is running.\nConnection via SSH Tunnel\nAirbyte has the ability to connect to a MySQl instance via an SSH Tunnel. The reason you might want to do this because it is not possible (or against security policy) to connect to the database directly (e.g. it does not have a public IP address).\nWhen using an SSH tunnel, you are configuring Airbyte to connect to an intermediate server (a.k.a. a bastion sever) that does have direct access to the database. Airbyte connects to the bastion and then asks the bastion to connect directly to the server.\nUsing this feature requires additional configuration, when creating the source. We will talk through what each piece of configuration means.\n\nConfigure all fields for the source as you normally would, except `SSH Tunnel Method`.\n\n`SSH Tunnel Method` defaults to `No Tunnel` (meaning a direct connection). If you want to use an SSH Tunnel choose `SSH Key Authentication` or `Password Authentication`.\n\n\nChoose `Key Authentication` if you will be using an RSA private key as your secret for establishing the SSH Tunnel (see below for more information on generating this key).\n\nChoose `Password Authentication` if you will be using a password as your secret for establishing the SSH Tunnel.\n\n:::warning\n   Since Airbyte Cloud requires encrypted communication, select SSH Key Authentication or Password Authentication if you selected preferred as the SSL Mode; otherwise, the connection will fail.\n   :::\n\n`SSH Tunnel Jump Server Host` refers to the intermediate (bastion) server that Airbyte will connect to. This should be a hostname or an IP Address.\n`SSH Connection Port` is the port on the bastion server with which to make the SSH connection. The default port for SSH connections is `22`, so unless you have explicitly changed something, go with the default.\n`SSH Login Username` is the username that Airbyte should use when connection to the bastion server. This is NOT the MySQl username.\nIf you are using `Password Authentication`, then `SSH Login Username` should be set to the password of the User from the previous step. If you are using `SSH Key Authentication` leave this blank. Again, this is not the MySQl password, but the password for the OS-user that Airbyte is using to perform commands on the bastion.\nIf you are using `SSH Key Authentication`, then `SSH Private Key` should be set to the RSA Private Key that you are using to create the SSH connection. This should be the full contents of the key file starting with `-----BEGIN RSA PRIVATE KEY-----` and ending with `-----END RSA PRIVATE KEY-----`.\n\nGenerating an SSH Key Pair\nThe connector expects an RSA key in PEM format. To generate this key:\n`text\nssh-keygen -t rsa -m PEM -f myuser_rsa`\nThis produces the private key in pem format, and the public key remains in the standard format used by the `authorized_keys` file on your bastion host. The public key should be added to your bastion host to whichever user you want to use with Airbyte. The private key is provided via copy-and-paste to the Airbyte connector configuration screen, so it may log in to the bastion.\nData Type Mapping\nMySQL data types are mapped to the following data types when synchronizing data. You can check the test values examples here. If you can't find the data type you are looking for or have any problems feel free to add a new test!\n| MySQL Type                                | Resulting Type         | Notes                                                                                                          |\n| :---------------------------------------- | :--------------------- | :------------------------------------------------------------------------------------------------------------- |\n| `bit(1)`                                  | boolean                |                                                                                                                |\n| `bit(>1)`                                 | base64 binary string   |                                                                                                                |\n| `boolean`                                 | boolean                |                                                                                                                |\n| `tinyint(1)`                              | boolean                |                                                                                                                |\n| `tinyint(>1)`                             | number                 |                                                                                                                |\n| `tinyint(>=1) unsigned`                   | number                 |                                                                                                                |\n| `smallint`                                | number                 |                                                                                                                |\n| `mediumint`                               | number                 |                                                                                                                |\n| `int`                                     | number                 |                                                                                                                |\n| `bigint`                                  | number                 |                                                                                                                |\n| `float`                                   | number                 |                                                                                                                |\n| `double`                                  | number                 |                                                                                                                |\n| `decimal`                                 | number                 |                                                                                                                |\n| `binary`                                  | string                 |                                                                                                                |\n| `blob`                                    | string                 |                                                                                                                |\n| `date`                                    | string                 | ISO 8601 date string. ZERO-DATE value will be converted to NULL. If column is mandatory, convert to EPOCH.     |\n| `datetime`, `timestamp`                   | string                 | ISO 8601 datetime string. ZERO-DATE value will be converted to NULL. If column is mandatory, convert to EPOCH. |\n| `time`                                    | string                 | ISO 8601 time string. Values are in range between 00:00:00 and 23:59:59.                                       |\n| `year`                                    | year string            | Doc                                                       |\n| `char`, `varchar` with non-binary charset | string                 |                                                                                                                |\n| `char`, `varchar` with binary charset     | base64 binary string   |                                                                                                                |\n| `tinyblob`                                | base64 binary string   |                                                                                                                |\n| `blob`                                    | base64 binary string   |                                                                                                                |\n| `mediumblob`                              | base64 binary string   |                                                                                                                |\n| `longblob`                                | base64 binary string   |                                                                                                                |\n| `binary`                                  | base64 binary string   |                                                                                                                |\n| `varbinary`                               | base64 binary string   |                                                                                                                |\n| `tinytext`                                | string                 |                                                                                                                |\n| `text`                                    | string                 |                                                                                                                |\n| `mediumtext`                              | string                 |                                                                                                                |\n| `longtext`                                | string                 |                                                                                                                |\n| `json`                                    | serialized json string | E.g. `{\"a\": 10, \"b\": 15}`                                                                                      |\n| `enum`                                    | string                 |                                                                                                                |\n| `set`                                     | string                 | E.g. `blue,green,yellow`                                                                                       |\n| `geometry`                                | base64 binary string   |                                                                                                                |\nIf you do not see a type in this list, assume that it is coerced into a string. We are happy to take feedback on preferred mappings.\nUpgrading from 0.6.8 and older versions to 0.6.9 and later versions\nThere is a backwards incompatible spec change between MySQL Source connector versions 0.6.8 and 0.6.9. As part of that spec change\n`replication_method` configuration parameter was changed to `object` from `string`.\nIn MySQL source connector versions 0.6.8 and older, `replication_method` configuration parameter was saved in the configuration database as follows:\n`\"replication_method\": \"STANDARD\"`\nStarting with version 0.6.9, `replication_method` configuration parameter is saved as follows:\n`\"replication_method\": {\n    \"method\": \"STANDARD\"\n}`\nAfter upgrading MySQL Source connector from 0.6.8 or older version to 0.6.9 or newer version you need to fix source configurations in the `actor` table\nin Airbyte database. To do so, you need to run the following two SQL queries. Follow the instructions in Airbyte documentation to\nrun SQL queries on Airbyte database.\nIf you have connections with MySQL Source using Standard replication method, run this SQL:\n`sql\nupdate public.actor set configuration =jsonb_set(configuration, '{replication_method}', '{\"method\": \"STANDARD\"}', true)\nWHERE actor_definition_id ='435bb9a5-7887-4809-aa58-28c27df0d7ad' AND (configuration->>'replication_method' = 'STANDARD');`\nIf you have connections with MySQL Source using Logicai Replication (CDC) method, run this SQL:\n`sql\nupdate public.actor set configuration =jsonb_set(configuration, '{replication_method}', '{\"method\": \"CDC\"}', true)\nWHERE actor_definition_id ='435bb9a5-7887-4809-aa58-28c27df0d7ad' AND (configuration->>'replication_method' = 'CDC');`",
    "tag": "airbyte"
  },
  {
    "title": "Talkdesk Explore",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/talkdesk-explore.md",
    "content": "Talkdesk Explore\nOverview\nTalkdesk is a software for contact center operations.\nThe Talkdesk Explore connector uses the Talkdesk Explore API to fetch data from usage reports.\nOutput schema\nThe connector supports both Full Refresh and Incremental on the following streams:\n\nCalls Report\nUser Status Report\nStudio Flow Execution Report\nContacts Report\nRing Attempts Report\n\nNote on report generation\nTo request data from one of the endpoints, first you need to generate a report. This is done by a POST request where the payload is the report specifications. Then, the response will be a report ID that you need to use in a GET request to obtain the report's data.\nThis process is further explained here: Executing a Report\nFeatures\n| Feature | Supported? |\n| :--- | :--- |\n| Full Refresh Sync | Yes |\n| Incremental - Append Sync | Yes |\n| Incremental - Dedupe Sync | No |\n| SSL connection | Yes |\nPerformance considerations\nThe Explore API has an account-based quota limit of 15 simultaneous reports (executing + enqueued). If this limit is exceeded, the user will receive a 429 (too many requests) response.\nGetting started\nRequirements\n\nTalkdesk account\nTalkdesk API key (`Client Credentials` auth method)\n\nSetup guide\nPlease refer to the getting started with the API guide.",
    "tag": "airbyte"
  },
  {
    "title": "Wrike ",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/wrike.md",
    "content": "Wrike\nThis page guides you through the process of setting up the Wrike source connector.\nPrerequisites\n\nYour Wrike Permanent Access Token\n\nSet up the Wrike source connector\n\nLog into your Airbyte Cloud or Airbyte OSS account.\nClick Sources and then click + New source. \nOn the Set up the source page, select Wrike from the Source type dropdown.\nEnter a name for your source.\n\nFor Permanent Access Token, enter your Wrike Permanent Access Token.\nPermissions granted to the permanent token are equal to the permissions of the user who generates the token.\n\n\nFor Wrike Instance (hostname), add the hostname of the Wrike instance you are currently using. This could be `www.wrike.com`, `app-us2.wrike.com`, or anything similar.\n\nFor Start date for comments, enter the date in `YYYY-MM-DDTHH:mm:ssZ` format. The comments added on and after this date will be replicated. If this field is blank, Airbyte will replicate comments from the last seven days.\nClick Set up source.\n\nSupported sync modes\nThe Wrike source connector supports on full sync refresh.\nSupported Streams\nThe Wrike source connector supports the following streams:\n\nTasks(Full Refresh)\nCustomfields(Full Refresh)\nComments(Full Refresh)\nContacts(Full Refresh)\nFolders(Full Refresh)\n\nData type mapping\nCurrencies are number and the date is a string.\nPerformance considerations\nThe Wrike connector should not run into Wrike API limitations under normal usage. Create an issue if you see any rate limit issues that are not automatically retried successfully.",
    "tag": "airbyte"
  },
  {
    "title": "Sonar Cloud API",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/sonar-cloud.md",
    "content": "Sonar Cloud API\nSync overview\nThis source can sync data from the Sonar cloud API. At present this connector only supports full refresh syncs meaning that each time you use the connector it will sync all available records from scratch. Please use cautiously if you expect your API to have a lot of records.\nThis Source Supports the Following Streams\n\ncomponents\nissues\nmetrics\n\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | No |  |\nGetting started\nRequirements\n\nSonar cloud User Token\n",
    "tag": "airbyte"
  },
  {
    "title": "Plaid",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/plaid.md",
    "content": "Plaid\nOverview\nThe Plaid source supports Full Refresh syncs. It currently only supports pulling from the balances endpoint. It will soon support other data streams (e.g. transactions).\nOutput schema\nOutput streams:\n\nBalance\n\nFeatures\n| Feature                       | Supported?  |\n| :---------------------------- | :---------- |\n| Full Refresh Sync             | Yes         |\n| Incremental - Append Sync     | Coming soon |\n| Replicate Incremental Deletes | Coming soon |\n| SSL connection                | Yes         |\n| Namespaces                    | No          |\nPerformance considerations\nThe Plaid connector should not run into Stripe API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.\nGetting started\nRequirements\n\nPlaid Account (with client_id and API key)\nAccess Token\n\nSetup guide for Sandbox\nThis guide will walk through how to create the credentials you need to run this source in the sandbox of a new plaid account. For production use, consider using the Link functionality that Plaid provides here\n\nCreate a Plaid account Go to the plaid website and click \"Get API Keys\". Follow the instructions to create an account.\nGet Client id and API key Go to the keys page where you will find the client id and your Sandbox API Key (in the UI this key is just called \"Sandbox\").\n\nCreate an Access Token First you have to create a public token key and then you can create an access token.\n\n\nCreate public key Make this API call described in plaid docs\n`bash\n  curl --location --request POST 'https://sandbox.plaid.com/sandbox/public_token/create' \\\n      --header 'Content-Type: application/json;charset=UTF-16' \\\n      --data-raw '{\n          \"client_id\": \"<your-client-id>\",\n          \"secret\": \"<your-sandbox-api-key>\",\n          \"institution_id\": \"ins_43\",\n          \"initial_products\": [\"auth\", \"transactions\"]\n      }'`\n\n\nExchange public key for access token Make this API call described in plaid docs. The public token used in this request, is the token returned in the response of the previous request. This request will return an `access_token`, which is the last field we need to generate for the config for this source!\n`bash\ncurl --location --request POST 'https://sandbox.plaid.com/item/public_token/exchange' \\\n  --header 'Content-Type: application/json;charset=UTF-16' \\\n  --data-raw '{\n      \"client_id\": \"<your-client-id>\",\n      \"secret\": \"<your-sandbox-api-key>\",\n      \"public_token\": \"<public-token-returned-by-previous-request>\"\n  }'`\n\n\nWe should now have everything we need to configure this source in the UI.\n\n",
    "tag": "airbyte"
  },
  {
    "title": "Google Ads",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/google-ads.md",
    "content": "Google Ads\nThis page contains the setup guide and reference information for the Google Ads source connector.\nPrerequisites\n\nA Google Ads Account linked to a Google Ads Manager account\n\n\n\n(For Airbyte Open Source) A developer token\n\n\nSetup guide\nStep 1: (For Airbyte Open Source) Apply for a developer token\n:::note\nYou'll need to create a Google Ads Manager account since Google Ads accounts cannot generate a developer token.\n:::\nTo set up the Google Ads source connector with Airbyte Open Source, you'll need a developer token. This token allows you to access your data from the Google Ads API. However, Google is selective about which software and use cases can get a developer token. The Airbyte team has worked with the Google Ads team to allowlist Airbyte and make sure you can get a developer token (see issue 1981 for more information).\nFollow Google's instructions to apply for the token. Note that you will not be able to access your data via the Google Ads API until this token is approved. You cannot use a test developer token; it has to be at least a basic developer token. It usually takes Google 24 hours to respond to these applications.\nWhen you apply for a token, make sure to mention:\n\nWhy you need the token (example: Want to run some internal analytics)\nThat you will be using the Airbyte Open Source project\nThat you have full access to the code base (because we're open source)\nThat you have full access to the server running the code (because you're self-hosting Airbyte)\n\nStep 2: Set up the Google Ads connector in Airbyte\n\nFor Airbyte Cloud:\nTo set up Google Ads as a source in Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nClick Sources and then click + New source.\nOn the Set up the source page, select Google Ads from the Source type dropdown.\nEnter a Name for your source.\nClick Sign in with Google to authenticate your Google Ads account.\nEnter a comma-separated list of the Customer ID(s) for your account.\nEnter the Start Date in YYYY-MM-DD format. The data added on and after this date will be replicated. If this field is blank, Airbyte will replicate all data.\n(Optional) Enter a custom GAQL query.\n(Optional) If the access to your account is through a Google Ads Manager account, enter the Login Customer ID for Managed Accounts of the Google Ads Manager account.\n(Optional) Enter a Conversion Window.\n(Optional) Enter the End Date in YYYY-MM-DD format. The data added after this date will not be replicated.\nClick Set up source.\n\n\n\nFor Airbyte Open Source:\nTo set up Google Ads as a source in Airbyte Open Source:\n\nLog into your Airbyte Open Source account.\nClick Sources and then click + New source.\nOn the Set up the source page, select Google Ads from the Source type dropdown.\nEnter a Name for your source.\nEnter the Developer Token.\nTo authenticate your Google account via OAuth, enter your Google application's Client ID, Client Secret, Refresh Token, and optionally, the Access Token.\nEnter a comma-separated list of the Customer ID(s) for your account.\nEnter the Start Date in YYYY-MM-DD format. The data added on and after this date will be replicated. If this field is blank, Airbyte will replicate all data.\n(Optional) Enter a custom GAQL query.\n(Optional) If the access to your account is through a Google Ads Manager account, enter the Login Customer ID for Managed Accounts of the Google Ads Manager account.\n(Optional) Enter a Conversion Window.\n(Optional) Enter the End Date in YYYY-MM-DD format. The data added after this date will not be replicated.\nClick Set up source.\n\n\nSupported sync modes\nThe Google Ads source connector supports the following sync modes:\n\nFull Refresh - Overwrite\nFull Refresh - Append\nIncremental Sync - Append\nIncremental Sync - Deduped History\n\nImportant note:\n\n\n```Usage of Conversion Window may lead to duplicates in Incremental Sync, \nbecause connector is forced to read data in the given range (Last Sync - Conversion window)\n```\n\n\nSupported Streams\nThe Google Ads source connector can sync the following tables. It can also sync custom queries using GAQL.\nMain Tables\n\naccounts\nad_group_ads\nad_group_ad_labels\nad_groups\nad_group_labels\ncampaign_labels\nclick_view\nkeyword\ngeographic\n\nNote that `ad_groups`, `ad_group_ads`, and `campaigns` contain a `labels` field, which should be joined against their respective `*_labels` streams if you want to view the actual labels. For example, the `ad_groups` stream contains an `ad_group.labels` field, which you would join against the `ad_group_labels` stream's `label.resource_name` field.\nReport Tables\n\ncampaigns\naccount_performance_report\nad_group_ad_report\ndisplay_keyword_report\ndisplay_topics_report\nshopping_performance_report\nuser_location_report\n\n:::note\nDue to Google Ads API constraints, the `click_view` stream retrieves data one day at a time and can only retrieve data newer than 90 days ago. Also, metrics cannot be requested for a Google Ads Manager account. Report streams are only available when pulling data from a non-manager account.\n:::\nFor incremental streams, data is synced up to the previous day using your Google Ads account time zone since Google Ads can filter data only by date without time. Also, some reports cannot load data real-time due to Google Ads limitations.\nCustom Query: Understanding Google Ads Query Language\n:::warning\nAdditional streams for Google Ads are dynamically created based on the specified Custom GAQL Queries. For an existing Google Ads source, when you are updating or removing Custom GAQL Queries, you should also ensure that any connections syncing to these streams are either disabled or have had their source schema refreshed.\n:::\nThe Google Ads Query Language can query the Google Ads API. Check out Google Ads Query Language and the query builder. You can add these as custom queries when configuring the Google Ads source.\nEach custom query in the input configuration must work for all the customer account IDs. Otherwise, the customer ID will be skipped for every query that fails the validation test. For example, if your query contains `metrics` fields in the `select` clause, it will not be executed against manager accounts.\nFollow Google's guidance on Selectability between segments and metrics when editing custom queries or default stream schemas (which will also be turned into GAQL queries by the connector). Fields like `segments.keyword.info.text`, `segments.keyword.info.match_type`, `segments.keyword.ad_group_criterion` in the `SELECT` clause tell the query to only get the rows of data that have keywords and remove any row that is not associated with a keyword. This is often unobvious and undesired behavior and can lead to missing data records. If you need this field in the stream, add a new stream instead of editing the existing ones.\nPerformance considerations\nThis source is constrained by the Google Ads API limits\nDue to a limitation in the Google Ads API which does not allow getting performance data at a granularity level smaller than a day, the Google Ads connector usually pulls data up until the previous day. For example, if the sync runs on Wednesday at 5 PM, then data up until Tuesday midnight is pulled. Data for Wednesday is exported only if a sync runs after Wednesday (for example, 12:01 AM on Thursday) and so on. This avoids syncing partial performance data, only to have to resync it again once the full day's data has been recorded by Google. For example, without this functionality, a sync which runs on Wednesday at 5 PM would get ads performance data for Wednesday between 12:01 AM - 5 PM on Wednesday, then it would need to run again at the end of the day to get all of Wednesday's data.",
    "tag": "airbyte"
  },
  {
    "title": "Copper",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/copper.md",
    "content": "Copper\nThis page contains the setup guide and reference information for the Copper source connector.\nPrerequisites\nThis Copper source uses the Copper API.\nSetup guide\nStep 1: Set up Copper\nFor Airbyte OSS:\n\nNavigate to the Airbyte Open Source dashboard\nEnter a name for your source\nEnter your Copper `api_key`. This can be generated by logging into Copper -> Settings -> Integrations -> API Keys\nEnter your Copper `user_email`. The email used to login to Copper\nClick Set up source\n\nSupported sync modes\nThe Copper source connector supports the following sync modes:\n| Feature                       | Supported? |\n| :---------------------------- | :--------- |\n| Full Refresh Sync             | Yes        |\n| Incremental - Append Sync     | No         |\n| Replicate Incremental Deletes | No         |\n| SSL connection                | Yes        |\n| Namespaces                    | No         |\nSupported Streams\n\nPeople\nCompanies\nProjects\n",
    "tag": "airbyte"
  },
  {
    "title": "Oracle Netsuite",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/netsuite.md",
    "content": "Oracle Netsuite\nOne unified business management suite, encompassing ERP/Financials, CRM and ecommerce for more than 31,000 customers.\nThis connector implements the SuiteTalk REST Web Services and uses REST API to fetch the customers data.\nPrerequisites\n\nOracle NetSuite account\nAllowed access to all Account permissions options\n\nAirbyte OSS and Airbyte Cloud\n\nRealm (Account ID)\nConsumer Key\nConsumer Secret\nToken ID\nToken Secret\n\nSetup guide\nStep 1: Create NetSuite account\n\nCreate account on Oracle NetSuite\nConfirm your Email\n\nStep 2: Setup NetSuite account\nStep 2.1: Obtain Realm info\n\nLogin into your NetSuite account\nGo to Setup \u00bb Company \u00bb Company Information\nCopy your Account ID (Realm). It should look like 1234567 for the `Production` env. or 1234567_SB2 - for a `Sandbox`\n\nStep 2.2: Enable features\n\nGo to Setup \u00bb Company \u00bb Enable Features\nClick on SuiteCloud tab\nScroll down to SuiteScript section\nEnable checkbox for `CLIENT SUITESCRIPT` and `SERVER SUITESCRIPT`\nScroll down to Manage Authentication section\nEnable checkbox `TOKEN-BASED AUTHENTICATION`\nScroll down to SuiteTalk (Web Services)\nEnable checkbox `REST WEB SERVISES`\nSave the changes\n\nStep 2.3: Create Integration (obtain Consumer Key and Consumer Secret)\n\nGo to Setup \u00bb Integration \u00bb Manage Integrations \u00bb New\nFill the Name field (we recommend to put `airbyte-rest-integration` for a name)\nMake sure the State is `enabled`\nEnable checkbox `Token-Based Authentication` in Authentication section\nSave changes\nAfter that, Consumer Key and Consumer Secret will be showed once (copy them to the safe place)\n\nStep 2.4: Setup Role\n\nGo to Setup \u00bb Users/Roles \u00bb Manage Roles \u00bb New\nFill the Name field (we recommend to put `airbyte-integration-role` for a name)\nScroll down to Permissions tab\n(REQUIRED) Click on `Transactions` and manually `add` all the dropdown entities with either `full` or `view` access level.\n(REQUIRED) Click on `Reports` and manually `add` all the dropdown entities with either `full` or `view` access level.\n(REQUIRED) Click on `Lists` and manually `add` all the dropdown entities with either `full` or `view` access level.\n(REQUIRED) Click on `Setup` and manually `add` all the dropdown entities with either `full` or `view` access level.\nMake sure you've done all `REQUIRED` steps correctly, to avoid sync issues in the future.\nPlease edit these params again when you `rename` or `customise` any `Object` in Netsuite for `airbyte-integration-role` to reflect such changes.\n\nStep 2.5: Setup User\n\nGo to Setup \u00bb Users/Roles \u00bb Manage Users\nIn column `Name` click on the user\u2019s name you want to give access to the `airbyte-integration-role`\nThen click on Edit button under the user\u2019s name\nScroll down to Access tab at the bottom\nSelect from dropdown list the `airbyte-integration-role` role which you created in step 2.4\nSave changes\n\nStep 2.6: Create Access Token for role\n\nGo to Setup \u00bb Users/Roles \u00bb Access Tokens \u00bb New\nSelect an Application Name\nUnder User select the user you assigned the `airbyte-integration-role` in the step 2.4\nInside Role select the one you gave to the user in the step 2.5\nUnder Token Name you can give a descriptive name to the Token you are creating (we recommend to put `airbyte-rest-integration-token` for a name)\nSave changes\nAfter that, Token ID and Token Secret will be showed once (copy them to the safe place)\n\nStep 2.7: Summary\nYou have copied next parameters\n* Realm (Account ID)\n* Consumer Key\n* Consumer Secret\n* Token ID\n* Token Secret\nAlso you have properly Configured Account with Correct Permissions and Access Token for User and Role you've created early.\nStep 3: Set up the source connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click + new source.\nOn the source setup page, select NetSuite from the Source type dropdown and enter a name for this connector.\nAdd Realm\nAdd Consumer Key\nAdd Consumer Secret\nAdd Token ID\nAdd Token Secret\nClick `Set up source`.\n\nFor Airbyte OSS:\n\nGo to local Airbyte page.\nIn the left navigation bar, click Sources. In the top-right corner, click + new source.\nOn the source setup page, select NetSuite from the Source type dropdown and enter a name for this connector.\nAdd Realm\nAdd Consumer Key\nAdd Consumer Secret\nAdd Token ID\nAdd Token Secret\nClick `Set up source`\n\nSupported sync modes\nThe NetSuite source connector supports the following sync modes:\n - Full Refresh\n - Incremental\nSupported Streams\n\nStreams are generated based on `ROLE` and `USER` access to them as well as `Account` settings, make sure you're using the correct role assigned in our case `airbyte-integration-role` or any other custom `ROLE` granted to the Access Token, having the access to the NetSuite objects for data sync, please refer to the Setup guide > Step 2.4 and Setup guide > Step 2.5\n\nPerformance considerations\nThe connector is restricted by Netsuite Concurrency Limit per Integration.",
    "tag": "airbyte"
  },
  {
    "title": "Wikipedia Pageviews",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/wikipedia-pageviews.md",
    "content": "Wikipedia Pageviews\nThis page contains the setup guide and reference information for the Wikipedia Pageviews source connector.\nPrerequisites\nNone\nSetup guide\nStep 1: Set up the Courier connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the Courier connector and select Wikipedia Pageviews from the Source type dropdown.\nEnter your parameters.\nClick Set up source.\n\nFor Airbyte OSS:\n\nNavigate to the Airbyte Open Source dashboard.\nSet the name for your source.\nEnter your parameters.\nClick Set up source.\n\nSupported sync modes\nThe Wikipedia Pageviews source connector supports the following sync modes:\n| Feature                       | Supported? |\n| :---------------------------- | :--------- |\n| Full Refresh Sync             | Yes        |\n| Incremental Sync              | No         |\n| Replicate Incremental Deletes | No         |\n| SSL connection                | Yes        |\n| Namespaces                    | No         |\nSupported Streams\n\nper-article\ntop\n\nPerformance considerations\n100 req/s per endpoint.",
    "tag": "airbyte"
  },
  {
    "title": "Punk-API",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/punk-api.md",
    "content": "Punk-API\nThis page contains the setup guide and reference information for the Punk-API source connector.\nPrerequisites\nApi key is not required for this connector to work,But a dummy key need to be passed to enhance in next versions. Example:123\nSetup guide\nStep 1: Set up Punk-API connection\n\nPass a dummy API key (Example: 12345)\nParams (Optional ID)\n\nStep 2: Set up the Punk-API connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the Punk-API connector and select Punk-API from the Source type dropdown.\nEnter your dummy `api_key`.\nEnter the params configuration if needed: ID (Optional)\nClick Set up source.\n\nFor Airbyte OSS:\n\nNavigate to the Airbyte Open Source dashboard.\nSet the name for your source.\nEnter your dummy `api_key`.\nEnter the params configuration if needed: ID (Optional)\nClick Set up source.\n\nSupported sync modes\nThe Punk-API source connector supports the following sync modes:\n| Feature                       | Supported? |\n| :---------------------------- | :--------- |\n| Full Refresh Sync             | Yes        |\n| Incremental Sync              | No         |\n| Replicate Incremental Deletes | No         |\n| SSL connection                | Yes        |\n| Namespaces                    | No         |\nSupported Streams\n\nBeers\nBeers_with_ID\n\nAPI method example\nGET https://api.punkapi.com/v2/beers\nPerformance considerations\nPunk API's API reference has v2 at present and v1 as depricated. The connector as default uses v2.",
    "tag": "airbyte"
  },
  {
    "title": "GitHub",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/github.md",
    "content": "GitHub\nThis page contains the setup guide and reference information for the GitHub source connector.\nPrerequisites\n\nStart date\nGitHub Repositories\nBranch (Optional)\nPage size for large streams (Optional)\n\n\nFor Airbyte Cloud:\n\nPersonal Access Token (see Permissions and scopes)\nOAuth\n\n\n\nFor Airbyte Open Source:\n\nPersonal Access Token (see Permissions and scopes)\n\n\nSetup guide\nStep 1: Set up GitHub\nCreate a GitHub Account.\n\nAirbyte Open Source additional setup steps\nLog into GitHub and then generate a personal access token. To load balance your API quota consumption across multiple API tokens, input multiple tokens separated with `,`.\n\n\nStep 2: Set up the GitHub connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click + new source.\nOn the source setup page, select GitHub from the Source type dropdown and enter a name for this connector.\nClick `Authenticate your GitHub account` by selecting Oauth or Personal Access Token for Authentication.\nLog in and Authorize to the GitHub account.\nStart date - The date from which you'd like to replicate data for streams: `comments`, `commit_comment_reactions`, `commit_comments`, `commits`, `deployments`, `events`, `issue_comment_reactions`, `issue_events`, `issue_milestones`, `issue_reactions`, `issues`, `project_cards`, `project_columns`, `projects`, `pull_request_comment_reactions`, `pull_requests`, `pull_requeststats`, `releases`, `review_comments`, `reviews`, `stargazers`, `workflow_runs`, `workflows`.\nGitHub Repositories - Space-delimited list of GitHub organizations/repositories, e.g. `airbytehq/airbyte` for single repository, `airbytehq/airbyte airbytehq/another-repo` for multiple repositories. If you want to specify the organization to receive data from all its repositories, then you should specify it according to the following example: `airbytehq/*`.\nBranch (Optional) - Space-delimited list of GitHub repository branches to pull commits for, e.g. `airbytehq/airbyte/master`. If no branches are specified for a repository, the default branch will be pulled. (e.g. `airbytehq/airbyte/master airbytehq/airbyte/my-branch`).\nPage size for large streams (Optional) - The GitHub connector contains several streams with a large load. The page size of such streams depends on the size of your repository. Recommended to specify values between 10 and 30.\n\n\n\nFor Airbyte Open Source:\n\nAuthenticate with Personal Access Token.\n\n\nSupported sync modes\nThe GitHub source connector supports the following sync modes:\n| Feature                       | Supported?  |\n| :---------------------------- | :---------- |\n| Full Refresh Sync             | Yes         |\n| Incremental - Append Sync     | Yes         |\n| Replicate Incremental Deletes | Coming soon |\n| SSL connection                | Yes         |\n| Namespaces                    | No          |\nSupported Streams\nThis connector outputs the following full refresh streams:\n\nAssignees\nBranches\nCollaborators\nIssue labels\nOrganizations\nPull request commits\nTags\nTeamMembers\nTeamMemberships\nTeams\nUsers\n\nThis connector outputs the following incremental streams:\n\nComments\nCommit comment reactions\nCommit comments\nCommits\nDeployments\nEvents\nIssue comment reactions\nIssue events\nIssue milestones\nIssue reactions\nIssues\nProject cards\nProject columns\nProjects\nPull request comment reactions\nPull request stats\nPull requests\nReleases\nRepositories\nReview comments\nReviews\nStargazers\nWorkflowRuns\nWorkflows\n\nNotes\n\n\nOnly 4 streams (`comments`, `commits`, `issues` and `review comments`) from the above 24 incremental streams are pure incremental meaning that they:\n\n\nread only new records;\n\n\noutput only new records.\n\n\nStreams `workflow_runs` and `worflow_jobs` is almost pure incremental:\n\n\nread new records and some portion of old records (in past 30 days) docs;\n\nthe `workflow_jobs` depends on the `workflow_runs` to read the data, so they both follow the same logic docs;\n\noutput only new records.\n\n\nOther 19 incremental streams are also incremental but with one difference, they:\n\n\nread all records;\n\n\noutput only new records.\n     Please, consider this behaviour when using those 19 incremental streams because it may affect you API call limits.\n\n\nWe are passing few parameters (`since`, `sort` and `direction`) to GitHub in order to filter records and sometimes for large streams specifying very distant `start_date` in the past may result in keep on getting error from GitHub instead of records (respective `WARN` log message will be outputted). In this case Specifying more recent `start_date` may help.\n   The \"Start date\" configuration option does not apply to the streams below, because the GitHub API does not include dates which can be used for filtering:\n\n\n`assignees`\n\n`branches`\n`collaborators`\n`issue_labels`\n`organizations`\n`pull_request_commits`\n`pull_request_stats`\n`repositories`\n`tags`\n`teams`\n`users`\n\nPermissions and scopes\nIf you use OAuth authentication method, the oauth2.0 application requests the next list of scopes: repo, read:org, read:repo_hook, read:user, read:discussion, workflow. For personal access token it need to manually select needed scopes.\nYour token should have at least the `repo` scope. Depending on which streams you want to sync, the user generating the token needs more permissions:\n\nFor syncing Collaborators, the user which generates the personal access token must be a collaborator. To become a collaborator, they must be invited by an owner. If there are no collaborators, no records will be synced. Read more about access permissions here.\nSyncing Teams is only available to authenticated members of a team's organization. Personal user accounts and repositories belonging to them don't have access to Teams features. In this case no records will be synced.\nTo sync the Projects stream, the repository must have the Projects feature enabled.\n\nPerformance considerations\nThe GitHub connector should not run into GitHub API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.",
    "tag": "airbyte"
  },
  {
    "title": "Yandex Metrica",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/yandex-metrica.md",
    "content": "Yandex Metrica\nThis page guides you through the process of setting up the Yandex Metrica source connector.\nPrerequisites\n\nCounter ID\nOAuth2 Token\n\nSetup Yandex Metrica\n\nCreate a Yandex Metrica account if you don't already have one.\nHead to Management page and add new tag or choose an existing one.\nAt the top of the dasboard you will see 8 digit number to the right of your website name. This is your Counter ID.\nCreate a new app or choose an existing one from My apps page.\nWhich platform is the app required for?: Web services\nCallback URL: https://oauth.yandex.com/verification_code\nWhat data do you need?: Yandex.Metrica. Read permission will suffice.\nChoose your app from the list.\nTo create your API key you will need to grab your ClientID,\nNow to get the API key craft a GET request to an endpoint https://oauth.yandex.com/authorizE?response_type=token&client_id=\\<Your Client ID>\nYou will receive a response with your API key. Save it.\n\nSupported sync modes\nThe Yandex Metrica source connector supports the following sync modes:\n\nFull Refresh\nIncremental\nAfter the first sync the connector will set the state for next sync. The start date will be set to last syncs end date. The end date will be set to 1 day before today.\n\nSupported Streams\n\nViews (Incremental).\nSessions (Incremental).\n\nNotes\n\nWe recommend syncing data once a day. Because of the Yandex Metrica API limitation it is only possible to extract records up to yesterdays date. Todays records will only be available tomorrow.\nBecause of the way API works some syncs may take a long time to finish. Timeout period is 2 hours.\n",
    "tag": "airbyte"
  },
  {
    "title": "Breezometer",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/breezometer.md",
    "content": "Breezometer\nBreezometer connector lets you request environment information like air quality, pollen forecast, current and forecasted weather and wildfires for a specific location.\nPrerequisites\n\nA Breezometer\nAn `api_key`, that can be found on your Breezometer account home page.\n\nSupported sync modes\nThe Breezometer connector supports full sync refresh.\nAirbyte Open Source\n\nAPI Key\nLatitude\nLongitude\nDays to Forecast\nHours to Forecast\nHistoric Hours\nRadius\n\nSupported Streams\n\nAir Quality - Current\nAir Quality - Forecast\nAir Quality - Historical\nPollen - Forecast\nWeather - Current\nWeather - Forecast\nWildfire - Burnt Area\nWildfire - Locate\n",
    "tag": "airbyte"
  },
  {
    "title": "Square",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/square.md",
    "content": "Square\nThis page contains the setup guide and reference information for the Square source connector.\nPrerequisites\nTo set up the Square source connector with Airbyte, you'll need to create your Square Application and use Personal token or Oauth access token.\nSetup guide\nStep 1: Set up Square\n\nCreate Square Application\nObtain Personal token or Oauth access token.\n\nStep 2: Set up the Square connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click + New source.\nOn the Set up the source page, enter the name for the Square connector and select Square from the Source type dropdown.\nChoose authentication method:\nApi-Key\nFill in API key token with \"Access token\" from Square Application settings page (Credentials on the left)\n\n\nOauth authentication\nFill in Client ID and Client secret with data from Square Application settings page (Oauth on the left)\nFill in refresh token with one obtained during the authentication process\n\n\nChoose if your account is sandbox\nChoose start date\nChoose if you would like to include Deleted objects (for streams: Items, Categories, Discounts, Taxes)\n\nFor Airbyte OSS:\n\nNavigate to the Airbyte Open Source dashboard.\nSet the name for your source. \nOn the Set up the source page, enter the name for the Square connector and select Square from the Source type dropdown.\nChoose authentication method:\nApi-Key\nFill in API key token with \"Access token\" from Square Application settings page (Credentials on the left)\n\n\nOauth authentication\nFill in Client ID and Client secret with data from Square Application settings page (Oauth on the left)\nFill in refresh token with one obtained during the authentication process\n\n\nChoose if your account is sandbox\nChoose start date\nChoose if you would like to include Deleted objects (for streams: Items, Categories, Discounts, Taxes)\n\nSupported sync modes\nThe Square source connector supports the following  sync modes:\n* Full Refresh - Overwrite\n* Full Refresh - Append\n* Incremental - Append\n* Incremental - Deduped History\nSupported Streams\n\nItems (Incremental)\nCategories (Incremental)\nDiscounts (Incremental)\nTaxes (Incremental)\nModifierLists (Incremental)\nPayments (Incremental)\nRefunds (Incremental)\nLocations \nTeam Members\nList Team Member Wages \nCustomers \nShifts \nOrders \n\nConnector-specific features & highlights\nUseful links:\n\nSquare API Explorer\nSquare API Docs\nSquare Developer Dashboard\n\nPerformance considerations (if any)\nNo defined API rate limits were found in Square documentation however considering this information it has 10 QPS limits. The connector doesn't handle rate limits exceptions, but no errors were raised during testing.\nExponential Backoff strategy recommended.\nData type map\n| Integration Type | Airbyte Type | Notes |\n|:-----------------|:-------------|:------|\n| `string`         | `string`     |       |\n| `integer`        | `integer`    |       |\n| `array`          | `array`      |       |\n| `object`         | `object`     |       |\n| `boolean`        | `boolean`    |       |",
    "tag": "airbyte"
  },
  {
    "title": "PyPI",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/pypi.md",
    "content": "PyPI\nThis page guides you through the process of setting up the PyPI source connector.\nSetup guide\nGet package name from PyPI\nThis is the name given in `pip install package_name` box. For example, `airbyte-cdk` is the package name for airbyte-cdk.\nOptianlly, provide a version name. If not provided, the release stream, containing data for particular version, cannot be used. The project stream is as same as release stream but contains data for all versions.\nSupported streams and sync modes\n\nProject\nRelease\nStats\n\nPerformance considerations\nDue to the heavy caching and CDN use, there is currently no rate limiting of PyPI APIs at the edge.\nIn addition, PyPI reserves the right to temporarily or permanently prohibit a consumer based on irresponsible activity.\nTry not to make a lot of requests (thousands) in a short amount of time (minutes). Generally PyPI can handle it, but it\u2019s preferred to make requests in serial over a longer amount of time if possible.",
    "tag": "airbyte"
  },
  {
    "title": "SurveyCTO",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/surveycto.md",
    "content": "SurveyCTO\nThis page guides you through the process of setting up the SurveyCTO source connector.\nPrerequisites\n\nServer Name `The name of the ServerCTO server`\nYour SurveCTO `Username`\nYour SurveyCTO `Password`\nForm ID `Unique Identifier for one of your forms`\nStart Date `Start Date default`\n\nHow to setup a SurveyCTO Account\n\ncreate the account\ncreate your form\npublish your form\ngive your user an API consumer permission to the existing role or create a user with that role and permission.\n\nSet up the SurveyCTO source connection\n\nLog into your Airbyte Cloud or Airbyte Open Source account.\nClick Sources and then click + New source.\nOn the Set up the source page, select Survey CTO from the Source type dropdown.\nEnter a name for your source.\nEnter a Server name for your SurveyCTO account. \nEnter a Username for SurveyCTO account.\nEnter a Password for SurveyCTO account.\nForm ID's (We can multiple forms id here to pull from) \nStart Date (This can be pass to pull the data from particular date)\nClick Set up source.\n\nSupported sync modes\nThe Commcare source connector supports the following sync modes:\n\nFull Refresh - Overwrite\nFull Refresh - Append\nIncremental Sync - Append\n(Recommended) Incremental Sync - Deduped History\n\nSupported Streams\nThe Commcare source connector supports the following streams:\n\nForms\n",
    "tag": "airbyte"
  },
  {
    "title": "Klarna",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/klarna.md",
    "content": "Klarna\nThis page contains the setup guide and reference information for the Klarna source connector.\nPrerequisites\nThe Klarna Settlements API is used to get the payouts and transactions for a Klarna account.\nSetup guide\nStep 1: Set up Klarna\nIn order to get an `Username (UID)` and `Password` please go to this page here you should find Merchant Portal button. Using this button you could log in to your production / playground in proper region. After registration / login you may find and create `Username (UID)` and `Password` in settings tab.\n:::note\nKlarna Source Connector does not support OAuth at this time due to limitations outside of control.\n:::\nStep 2: Set up the Klarna connector in Airbyte\nFor Airbyte Open Source:\n\nNavigate to the Airbyte Open Source dashboard\nSet the name for your source\nChoose if your account is sandbox\nEnter your username\nEnter your password\nEnter the date you want your sync to start from\nClick Set up source\n\nSupported sync modes\nThe Klarna source connector supports the following sync modes:\n| Feature                   | Supported? |\n| :------------------------ |:-----------|\n| Full Refresh Sync         | Yes        |\n| Incremental - Append Sync | No         |\nSupported Streams\nThis Source is capable of syncing the following Klarna Settlements Streams:\n\nPayouts\nTransactions\n\nPerformance considerations\nKlarna API has rate limiting\nProduction environments: the API rate limit is 20 create-sessions per second on average measured over a 1-minute period. For the other operations, the API limit is 200 requests per second on average, measured over a 1 minute period\nPlayground environments: the API rate limit is one quarter (1/4th) of the rate limits of production environments.\nConnector will handle an issue with rate limiting as Klarna returns 429 status code when limits are reached",
    "tag": "airbyte"
  },
  {
    "title": "Oracle DB",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/oracle.md",
    "content": "Oracle DB\nFeatures\n| Feature | Supported | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental - Append Sync | Yes |  |\n| Replicate Incremental Deletes | Coming soon |  |\n| Logical Replication (WAL) | Coming soon |  |\n| SSL Support | Coming soon |  |\n| SSH Tunnel Connection | Yes |  |\n| LogMiner | Coming soon |  |\n| Flashback | Coming soon |  |\n| Namespaces | Yes | Enabled by default |\nThe Oracle source does not alter the schema present in your database. Depending on the destination connected to this source, however, the schema may be altered. See the destination's documentation for more details.\nGetting Started (Airbyte Cloud)\nOn Airbyte Cloud, only TLS connections to your Oracle instance are supported. Other than that, you can proceed with the open-source instructions below.\nGetting Started (Airbyte Open-Source)\nRequirements\n\nOracle `11g` or above\nAllow connections from Airbyte to your Oracle database (if they exist in separate VPCs)\nCreate a dedicated read-only Airbyte user with access to all tables needed for replication\n\n1. Make sure your database is accessible from the machine running Airbyte\nThis is dependent on your networking setup. The easiest way to verify if Airbyte is able to connect to your Oracle instance is via the check connection tool in the UI.\n2. Create a dedicated read-only user with access to the relevant tables (Recommended but optional)\nThis step is optional but highly recommended to allow for better permission control and auditing. Alternatively, you can use Airbyte with an existing user in your database.\nTo create a dedicated database user, run the following commands against your database:\n`sql\nCREATE USER airbyte IDENTIFIED BY <your_password_here>;\nGRANT CREATE SESSION TO airbyte;`\nNext, grant the user read-only access to the relevant tables. The simplest way is to grant read access to all tables in the schema as follows:\n`sql\nGRANT SELECT ANY TABLE TO airbyte;`\nOr you can be more granular:\n`sql\nGRANT SELECT ON \"<schema_a>\".\"<table_1>\" TO airbyte;\nGRANT SELECT ON \"<schema_b>\".\"<table_2>\" TO airbyte;`\nYour database user should now be ready for use with Airbyte.\n3. Include the schemas Airbyte should look at when configuring the Airbyte Oracle Source.\nCase sensitive. Defaults to the upper-cased user if empty. If the user does not have access to the configured schemas, no tables will be discovered.\nConnection via SSH Tunnel\nAirbyte has the ability to connect to a Oracle instance via an SSH Tunnel. The reason you might want to do this because it is not possible (or against security policy) to connect to the database directly (e.g. it does not have a public IP address).\nWhen using an SSH tunnel, you are configuring Airbyte to connect to an intermediate server (a.k.a. a bastion sever) that does have direct access to the database. Airbyte connects to the bastion and then asks the bastion to connect directly to the server.\nUsing this feature requires additional configuration, when creating the source. We will talk through what each piece of configuration means.\n\nConfigure all fields for the source as you normally would, except `SSH Tunnel Method`.\n`SSH Tunnel Method` defaults to `No Tunnel` (meaning a direct connection). If you want to use an SSH Tunnel choose `SSH Key Authentication` or `Password Authentication`.\nChoose `Key Authentication` if you will be using an RSA private key as your secret for establishing the SSH Tunnel (see below for more information on generating this key).\nChoose `Password Authentication` if you will be using a password as your secret for establishing the SSH Tunnel.\n`SSH Tunnel Jump Server Host` refers to the intermediate (bastion) server that Airbyte will connect to. This should be a hostname or an IP Address.\n`SSH Connection Port` is the port on the bastion server with which to make the SSH connection. The default port for SSH connections is `22`, so unless you have explicitly changed something, go with the default.\n`SSH Login Username` is the username that Airbyte should use when connection to the bastion server. This is NOT the Oracle username.\nIf you are using `Password Authentication`, then `SSH Login Username` should be set to the password of the User from the previous step. If you are using `SSH Key Authentication` leave this blank. Again, this is not the Oracle password, but the password for the OS-user that Airbyte is using to perform commands on the bastion.\nIf you are using `SSH Key Authentication`, then `SSH Private Key` should be set to the RSA Private Key that you are using to create the SSH connection. This should be the full contents of the key file starting with `-----BEGIN RSA PRIVATE KEY-----` and ending with `-----END RSA PRIVATE KEY-----`.\n\nGenerating an SSH Key Pair\nThe connector expects an RSA key in PEM format. To generate this key:\n`text\nssh-keygen -t rsa -m PEM -f myuser_rsa`\nThis produces the private key in pem format, and the public key remains in the standard format used by the `authorized_keys` file on your bastion host. The public key should be added to your bastion host to whichever user you want to use with Airbyte. The private key is provided via copy-and-paste to the Airbyte connector configuration screen, so it may log in to the bastion.\nData Type Mapping\nOracle data types are mapped to the following data types when synchronizing data. You can check the test values examples here. If you can't find the data type you are looking for or have any problems feel free to add a new test!\n| Oracle Type | Resulting Type | Notes |\n| :--- | :--- | :--- |\n| `binary_double` | number |  |\n| `binary_float` | number |  |\n| `blob` | string |  |\n| `char` | string |  |\n| `char(3 char)` | string |  |\n| `clob` | string |  |\n| `date` | string |  |\n| `decimal` | number |  |\n| `float` | number |  |\n| `float(5)` | number |  |\n| `integer` | number |  |\n| `interval year to month` | string |  |\n| `long raw` | string |  |\n| `number` | number |  |\n| `number(6, 2)` | number |  |\n| `nvarchar(3)` | string |  |\n| `raw` | string |  |\n| `timestamp` | string |  |\n| `timestamp with local time zone` | string |  |\n| `timestamp with time zone` | string |  |\n| `varchar2` | string |  |\n| `varchar2(256)` | string |  |\n| `xmltype` | string |  |\nIf you do not see a type in this list, assume that it is coerced into a string. We are happy to take feedback on preferred mappings.\nEncryption Options\nAirbyte has the ability to connect to the Oracle source with 3 network connectivity options:\n1.`Unencrypted` the connection will be made using the TCP protocol. In this case, all data over the network will be transmitted in unencrypted form. 2.`Native network encryption` gives you the ability to encrypt database connections, without the configuration overhead of TCP / IP and SSL / TLS and without the need to open and listen on different ports. In this case, the SQLNET.ENCRYPTION_CLIENT option will always be set as REQUIRED by default: The client or server will only accept encrypted traffic, but the user has the opportunity to choose an `Encryption algorithm` according to the security policies he needs. 3.`TLS Encrypted` (verify certificate) - if this option is selected, data transfer will be transfered using the TLS protocol, taking into account the handshake procedure and certificate verification. To use this option, insert the content of the certificate issued by the server into the `SSL PEM file` field",
    "tag": "airbyte"
  },
  {
    "title": "SFTP",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/sftp.md",
    "content": "SFTP\nThis page contains the setup guide and reference information for the SFTP source connector.\nPrerequisites\n\nThe Server with SFTP connection type support\nThe Server host\nThe Server port\nUsername-Password/Public Key Access Rights\n\nSetup guide\nStep 1: Set up SFTP\n\nUse your username/password credential to connect the server.\nAlternatively generate Public Key Access\n\nThe following simple steps are required to set up public key authentication:\nKey pair is created (typically by the user). This is typically done with ssh-keygen.\nPrivate key stays with the user (and only there), while the public key is sent to the server. Typically with the ssh-copy-id utility.\nServer stores the public key (and \"marks\" it as authorized).\nServer will now allow access to anyone who can prove they have the corresponding private key.\nStep 2: Set up the SFTP connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click `Sources`. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the SFTP connector and select SFTP from the Source type dropdown.\nEnter your `User Name`, `Host Address`, `Port`\nChoose the `Authentication` type `Password Authentication` or `Key Authentication`\nType `File type` (temporary comma separated)\nEnter `Folder Path` (Optional) to specify server folder for sync\nEnter `File Pattern` (Optional). e.g. `log-([0-9]{4})([0-9]{2})([0-9]{2})`. Write your own regex \nClick on `Check Connection` to finish configuring the Amplitude source.\n\nSupported sync modes\nThe SFTP source connector supports the following sync modes:\n| Feature                       | Support | Notes                                                                                |\n|:------------------------------|:-------:|:-------------------------------------------------------------------------------------|\n| Full Refresh - Overwrite      |    \u2705    | Warning: this mode deletes all previously synced data in the configured bucket path. |\n| Full Refresh - Append Sync    |    \u274c    |                                                                                      |\n| Incremental - Append          |    \u274c    |                                                                                      |\n| Incremental - Deduped History |    \u274c    |                                                                                      |\n| Namespaces                    |    \u274c    |                                                                                      |\nSupported Streams\nThis source provides a single stream per file with a dynamic schema. The current supported type file: `.csv` and `.json`\nMore formats (e.g. Apache Avro) will be supported in the future.",
    "tag": "airbyte"
  },
  {
    "title": "PagerDuty",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/pagerduty.md",
    "content": "PagerDuty\nOverview\nThe PagerDuty source is maintained by Faros\nAI.\nPlease file any support requests on that repo to minimize response time from the\nmaintainers. The source supports both Full Refresh and Incremental syncs. You\ncan choose if this source will copy only the new or updated data, or all rows in\nthe tables and columns you set up for replication, every time a sync is run.\nOutput schema\nSeveral output streams are available from this source:\n\nIncidents (Incremental)\nIncident Log Entries (Incremental)\nPriorities\nUsers\n\nIf there are more endpoints you'd like Faros AI to support, please create an\nissue.\nFeatures\n| Feature | Supported? |\n| :--- | :--- |\n| Full Refresh Sync | Yes |\n| Incremental Sync | Yes |\n| SSL connection | Yes |\n| Namespaces | No |\nPerformance considerations\nThe PagerDuty source should not run into PagerDuty API limitations under normal\nusage.  Please create an\nissue if you see any\nrate limit issues that are not automatically retried successfully.\nGetting started\nRequirements\n\nPagerDuty API Key\n\nPlease follow the their documentation for generating a PagerDuty API\nKey.",
    "tag": "airbyte"
  },
  {
    "title": "getLago API",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/getlago.md",
    "content": "getLago API\nSync overview\nThis source can sync data from the getLago API. At present this connector only supports full refresh syncs meaning that each time you use the connector it will sync all available records from scratch. Please use cautiously if you expect your API to have a lot of records.\nThis Source Supports the Following Streams\n\nbillable_metrics\nplans\ncoupons\nadd_ons\ninvoices\ncustomers\nsubscriptions\n\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | No |  |\nGetting started\nRequirements\n\ngetLago API KEY\n",
    "tag": "airbyte"
  },
  {
    "title": "Vantage API",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/vantage.md",
    "content": "Vantage API\nSync overview\nThis source can sync data from the Vantage API. At present this connector only supports full refresh syncs meaning that each time you use the connector it will sync all available records from scratch. Please use cautiously if you expect your API to have a lot of records.\nThis Source Supports the Following Streams\n\nProviders: Providers are the highest level API Primitive. A Provider represents either cloud infrastructure provider or a cloud service provider. Some examples of Providers include AWS, GCP or Azure. Providers offer many Services, which is documented below.\nServices: Services are what Providers offer to their customers. A Service is always tied to a Provider. Some examples of Services are EC2 or S3 from a Provider of AWS. A Service has one or more Products offered, which is documented below.\nProducts: Products are what Services ultimately price on. Using the example of a Provider of 'AWS' and a Service of 'EC2', Products would be the individual EC2 Instance Types available such as 'm5d.16xlarge' or 'c5.xlarge'. A Product has one or more Prices, which is documented below.\nReports\n\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | No |  |\nPerformance considerations\nVantage APIs are under rate limits for the number of API calls allowed per API keys per second. If you reach a rate limit, API will return a 429 HTTP error code. See here\nGetting started\nRequirements\n\nVantage Access token\n",
    "tag": "airbyte"
  },
  {
    "title": "LinkedIn Ads",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/linkedin-ads.md",
    "content": "LinkedIn Ads\nThis page guides you through the process of setting up the LinkedIn Ads source connector.\nPrerequisites\n\nFor Airbyte Cloud:\n\nThe LinkedIn Ads account with permission to access data from accounts you want to sync.\n\n\n\nFor Airbyte Open Source:\n\nThe LinkedIn Ads account with permission to access data from accounts you want to sync.\nAuthentication Options:\nOAuth2.0:\n`Client ID` from your `Developer Application`\n`Client Secret` from your `Developer Application`\n`Refresh Token` obtained from successful authorization with `Client ID` + `Client Secret`\n\n\nAccess Token:\n`Access Token` obtained from successful authorization with `Client ID` + `Client Secret`\n\n\n\n\nStep 1: Set up LinkedIn Ads\n\nLogin to LinkedIn with a developer account.\n\nClick the Create App icon on the center of the page or use here. Fill in the required fields:  \n\nFor App Name, enter a name.\nFor LinkedIn Page, enter your company's name or LinkedIn Company Page URL.\nFor Privacy policy URL, enter the link to your company's privacy policy.\nFor App logo, upload your company's logo.\nFor Legal Agreement, select I have read and agree to these terms.\nClick Create App, on the bottom right of the screen. LinkedIn redirects you to a page showing the details of your application.\n\n\n\nVerify your app. You can verify your app using the following steps:\n\n\nTo display the settings page, click the Settings tab. On the App Settings section, click Verify under Company. A popup window will be displayed. To generate the verification URL, click on Generate URL, then copy and send the URL to the Page Admin (this may be you). Click on I'm done.\nIf you are the administrator of your Page, simply run the URL in a new tab (if not, an administrator will have to do the next step). Click on Verify. Finally, Refresh the tab of app creation, the app should now be associated with your Page.\n\n\nTo display the Products page, click the Product tab. For Marketing Developer Platform click on the Request access. A popup window will be displayed. Review and Select I have read and agree to these terms. Finally, click Request access. \n\n\nTo authorize your application, click the Auth tab. The authentication page is displayed. Copy the client_id and client_secret (for later steps). For Oauth 2.0 settings, Provide a redirect_uri (for later steps).\n\n\nClick and review the Analytics tab. This page shows the daily application and user/member limits with the percent used for each resource endpoint.\n\n\n\n\n(Optional for Airbyte Cloud) Authorize your app. In case your authorization expires:\n\n\nThe authorization token `lasts 60-days before expiring`. The connector app will need to be reauthorized when the authorization token expires.\n   Create an Authorization URL with the following steps:\n\n\nReplace the highlighted parameters `YOUR_CLIENT_ID` and `YOUR_REDIRECT_URI` in the URL (`https://www.linkedin.com/oauth/v2/authorization?response_type=code&client_id=YOUR_CLIENT_ID&redirect_uri=YOUR_REDIRECT_URI&scope=r_emailaddress,r_liteprofile,r_ads,r_ads_reporting,r_organization_social`) from the scope obtain below.\n\n\nSet up permissions for the following scopes `r_emailaddress,r_liteprofile,r_ads,r_ads_reporting,r_organization_social`. For OAuth2.0, copy the `Client ID`, and `Client Secret` from your `Developer Application`. And copy the `Refresh Token` obtained from successful authorization with `Client ID` + `Client Secret`\n\n\nEnter the modified `URL` in the browser. You will be redirected.\n\n\nTo authorize the app, click Allow.\n\n\nCopy the `code` parameter listed in the redirect URL in the Browser header URL.\n\n\n(Optional for Airbyte Cloud) Run the following curl command using `Terminal` or `Command line` with the parameters replaced to return your `access_token`. The `access_token` expires in 2-months.\n\n\n`text\n    curl -0 -v -X POST https://www.linkedin.com/oauth/v2/accessToken\\\n    -H \"Accept: application/json\"\\\n    -H \"application/x-www-form-urlencoded\"\\\n    -d \"grant_type=authorization_code\"\\\n    -d \"code=YOUR_CODE\"\\\n    -d \"client_id=YOUR_CLIENT_ID\"\\\n    -d \"client_secret=YOUR_CLIENT_SECRET\"\\\n    -d \"redirect_uri=YOUR_REDIRECT_URI\"`\n\n(Optional for Airbyte Cloud) Use the `access_token`. Same as the approach in  `Step 5` to authorize LinkedIn Ads connector.\n\nNotes:\nThe API user account should be assigned the following permissions for the API endpoints:\nEndpoints such as: `Accounts`, `Account Users`, `Ad Direct Sponsored Contents`, `Campaign Groups`, `Campaigns`, and `Creatives` requires the following permissions set:\n\n`r_ads`: read ads (Recommended), `rw_ads`: read-write ads\n  Endpoints such as: `Ad Analytics by Campaign`, and `Ad Analytics by Creatives` requires the following permissions set:\n`r_ads_reporting`: read ads reporting\n  The complete set of permissions is as follows:\n`r_emailaddress,r_liteprofile,r_ads,r_ads_reporting,r_organization_social`\n\nThe API user account should be assigned one of the following roles:\n\nACCOUNT_BILLING_ADMIN\nACCOUNT_MANAGER\nCAMPAIGN_MANAGER\nCREATIVE_MANAGER\nVIEWER (Recommended)\n\nTo edit these roles, sign in to Campaign Manager and follow these instructions.\nStep 2: Set up the source connector in Airbyte\n\nFor Airbyte Cloud:\n\nLogin to your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click + new source.\nOn the source setup page, select LinkedIn Ads from the Source type dropdown and enter a name for this connector.\nAdd `Start Date` - the starting point for your data replication.\nAdd your `Account IDs (Optional)` if required.\nClick Authenticate your account.\nLogin and Authorize the LinkedIn Ads account\nClick Set up source.\n\n\nFor Airbyte Open Source:\n\nGo to the local Airbyte page.\nIn the left navigation bar, click Sources. In the top-right corner, click + new source.\nOn the Set up the source page, enter the name for the connector and select LinkedIn Ads from the Source type dropdown.\nAdd `Start Date` - the starting point for your data replication.\nAdd your `Account IDs (Optional)` if required.\nChoose between Authentication Options:\nFor OAuth2.0: Copy and paste info (Client ID, Client Secret) from your LinkedIn Ads developer application, and obtain the Refresh Token using Set up LinkedIn Ads  guide steps and paste it into the corresponding field.\nFor Access Token: Obtain the Access Token using Set up LinkedIn Ads  guide steps and paste it into the corresponding field.\n\n\nClick Set up source.\n\n\nSupported Streams and Sync Modes\nThis Source is capable of syncing the following data as streams:\n\nAccounts\nAccount Users\nCampaign Groups\nCampaigns\nCreatives\nAd Direct Sponsored Contents\nAd Analytics by Campaign\nAd Analytics by Creative\n\n| Sync Mode                                 | Supported?(Yes/No) |\n| :---------------------------------------- | :------------------- |\n| Full Refresh Overwrite Sync               | Yes                  |\n| Full Refresh Append Sync                  | Yes                  |\n| Incremental - Append Sync                 | Yes                  |\n| Incremental - Append + Deduplication Sync | Yes                  |\nNOTE:\nThe `Ad Direct Sponsored Contents` stream includes information about VIDEO ADS, as well as `SINGLE IMAGE ADS` and other directly sponsored ads your account might have.\nFor Analytics Streams such as `Ad Analytics by Campaign` and `Ad Analytics by Creative`, the `pivot` column name is renamed to `_pivot` to handle the data normalization correctly and avoid name conflicts with certain destinations.\nData type mapping\n| Integration Type | Airbyte Type | Notes                      |\n| :--------------- | :----------- | :------------------------- |\n| `number`         | `number`     | float number               |\n| `integer`        | `integer`    | whole number               |\n| `date`           | `string`     | FORMAT YYYY-MM-DD          |\n| `datetime`       | `string`     | FORMAT YYYY-MM-DDThh:mm: ss |\n| `array`          | `array`      |                            |\n| `boolean`        | `boolean`    | True/False                 |\n| `string`         | `string`     |                            |\nPerformance considerations\nLinkedIn Ads has Official Rate Limits for API Usage, more information here. Rate limited requests will receive a 429 response. These limits reset at midnight UTC every day. In rare cases, LinkedIn may also return a 429 response as part of infrastructure protection. API service will return to normal automatically. In such cases, you will receive the following error message:\n`text\n\"Caught retriable error '<some_error> or null' after <some_number> tries. Waiting <some_number> seconds then retrying...\"`\nThis is expected when the connector hits the 429 - Rate Limit Exceeded HTTP Error. If the maximum available API requests capacity is reached, you will have the following message:\n`text\n\"Max try rate limit exceeded...\"`\nAfter 5 unsuccessful attempts - the connector will stop the sync operation. In such cases check your Rate Limits on this page > Choose your app > Analytics.",
    "tag": "airbyte"
  },
  {
    "title": "Notion",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/notion.md",
    "content": "Notion\nThis page contains the setup guide and reference information for the Notion source connector.\nSetup guide\u200b\nStep 1: Set up Notion\u200b\n\nCreate a new integration on the My integrations page.\n\n:::note\nYou must be the owner of a Notion workspace to create a new integration.\n:::\n\nFill out the form. Make sure to check Read content and check any other capabilities you want to authorize.\nClick Submit.\nIn the Integration type section, select either Internal integration (token authorization) or Public integration (OAuth2.0 authentication).\nCheck the capabilities you want to authorize.\nIf you select Public integration, fill out the fields in the OAuth Domain & URIs section.\nClick Save changes.\nCopy the Internal Access Token if you are using the internal integration, or copy the `access_token`, `client_id`, and `client_secret` if you are using the public integration.\n\nStep 2: Set up the Notion connector in Airbyte\n\nFor Airbyte Cloud:\n\nLog in to your Airbyte Cloud account.\nClick Sources and then click + New source.\nOn the Set up the source page, select Notion from the Source type dropdown.\nEnter a name for your source.\nChoose the method of authentication:\nIf you select Access Token, paste the access token from Step 8.\nIf you select OAuth2.0 authorization, click Authenticate your Notion account.\nLog in and Authorize the Notion account. Select the permissions you want to allow Airbyte.\n\n\n\n\nEnter the Start Date in YYYY-MM-DDTHH:mm:ssZ format. All data generated after this date will be replicated. If this field is blank, Airbyte will replicate all data.\nClick Set up source.\n\n\n\nFor Airbyte Open Source:\n\nLog in to your Airbyte Open Source account.\nClick Sources and then click + New source.\nOn the Set up the source page, select Notion from the Source type dropdown.\nEnter a name for your source.\nChoose the method of authentication:\nIf you select Access Token, paste the access token from Step 8.\nIf you select OAuth2.0 authorization, paste the client ID, access token, and client secret from Step 8.\n\n\nEnter the Start Date in YYYY-MM-DDTHH:mm:ssZ format. All data generated after this date will be replicated. If this field is blank, Airbyte will replicate all data.\nClick Set up source.\n\n\nSupported sync modes\nThe Notion source connector supports the following sync modes:\n* Full Refresh - Overwrite\n* Full Refresh - Append\n* Incremental - Append (partially)\n* Incremental - Deduped History\nSupported Streams\nThe Notion source connector supports the following streams. For more information, see the Notion API.\n\nblocks\ndatabases\npages\nusers\n\n:::note\nThe users stream does not support Incremental - Append sync mode.\n:::\nPerformance considerations\nThe connector is restricted by Notion request limits. The Notion connector should not run into Notion API limitations under normal usage. Create an issue if you encounter any rate limit issues that are not automatically retried successfully.",
    "tag": "airbyte"
  },
  {
    "title": "Braze",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/braze.md",
    "content": "Braze\nThis page contains the setup guide and reference information for the Braze source connector.\nPrerequisites\nIt is required to have an account on Braze to provide us with `URL` and `Rest API Key` during set up.\n- `Rest API Key` could be found on Braze Dashboard -> Developer Console tab -> API Settings -> Rest API Keys\n- `URL` could be found on Braze Dashboard -> Manage Settings -> Settings tab -> `Your App name` -> SDK Endpoint\nSet up the Braze connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the Braze connector and select Braze from the Source type dropdown.\nFill in your `URL`, `Rest API Key` and `Start date` and then click Set up source.\n\nSupported sync modes\nThe Braze source connector supports the following  sync modes:\n\nFull Refresh | Overwrite\nIncremental Sync | Append\n\nSupported Streams\n\ncampaigns\ncampaigns_analytics\ncanvases\ncanvases_analytics\nevents\nevents_analytics\nkpi_daily_new_users\nkpi_daily_active_users\nkpi_daily_app_uninstalls\ncards\ncards_analytics\nsegments\nsegments_analytics\n\nPerformance considerations\nRate limits differ depending on stream.\nRate limits table: https://www.braze.com/docs/api/api_limits/#rate-limits-by-request-type ",
    "tag": "airbyte"
  },
  {
    "title": "SAP Business One",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/sap-business-one.md",
    "content": "SAP Business One\nSAP Business One is an Enterprise Resource Planning (ERP) system.\nSync overview\nSAP Business One can run on the MSSQL or SAP HANA databases. If your instance is deployed on MSSQL, you can use Airbyte to sync your SAP Business One instance by using the MSSQL connector.\n:::info\nReach out to your service representative or system admin to find the parameters required to connect to the underlying database\n:::\nOutput schema\nThe schema will be loaded according to the rules of the underlying database's connector and the data available in your B1 instance.",
    "tag": "airbyte"
  },
  {
    "title": "HTTP Request",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/http-request.md",
    "content": "HTTP Request\n:::caution\nThis connector is graveyarded and will not be receiving any updates from the Airbyte team. Its functionalities have been replaced by the Airbyte CDK, which allows you to create source connectors for any HTTP API.\n:::\nOverview\nThis connector allows you to generally connect to any HTTP API. In order to use this connector, you must manually bring it in as a custom connector. The steps to do this can be found here. \nWhere do I find the Docker image?\nThe Docker image for the HTTP Request connector image can be found at our DockerHub here. \nWhy was this connector graveyarded?\nWe found that there are lots of cases in which using a general connector leads to poor user experience, as there are countless edge cases for different API structures, different authentication policies, and varied approaches to rate-limiting. We believe that enabling users to more easily",
    "tag": "airbyte"
  },
  {
    "title": "Trello",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/trello.md",
    "content": "Trello\nOverview\nThe Trello source supports both Full Refresh and Incremental syncs. You can choose if this connector will copy only the new or updated data, or all rows in the tables and columns you set up for replication, every time a sync is run.\nThis Source Connector is based on a Airbyte CDK.\nOutput schema\nSeveral output streams are available from this source:\n\nBoards (Full table)\nActions (Incremental)\nCards (Full table)\nChecklists (Full table)\nLists (Full table)\nUsers (Full table)\n\nIf there are more endpoints you'd like Airbyte to support, please create an issue.\nFeatures\n| Feature | Supported? |\n| :--- | :--- |\n| Full Refresh Sync | Yes |\n| Incremental - Append Sync | Yes |\n| SSL connection | Yes |\n| Namespaces | No |\nPerformance considerations\nThe connector is restricted by normal Trello requests limitation.\nThe Trello connector should not run into Trello API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.\nGetting started\nRequirements\n\nTrello API Token\nTrello API Key\n\nSetup guide\n\nPlease read How to get your APIs Token and Key or you can log in to Trello and visit Developer API Keys.",
    "tag": "airbyte"
  },
  {
    "title": "Stripe",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/stripe.md",
    "content": ":::warning\nStripe API Restriction: Access to events endpoint is guaranteed only for the last 30 days. Using the full-refresh-overwrite sync from Airbyte will delete data older than 30 days from your target destination.\n:::\nStripe\nThis page guides you through the process of setting up the Stripe source connector.\nPrerequisites\n\nYour Stripe Account ID\nYour Stripe Secret Key\n\nSet up the Stripe source connector\n\nLog into your Airbyte Cloud or Airbyte Open Source account.\nClick Sources and then click + New source. \nOn the Set up the source page, select Stripe from the Source type dropdown.\nEnter a name for your source.\nFor Account ID, enter your Stripe Account ID.\nFor Secret Key, enter your Stripe Secret Key\n\nWe recommend creating a secret key specifically for Airbyte to control which resources Airbyte can access. For ease of use, we recommend granting read permission to all resources and configuring which resource to replicate in the Airbyte UI. You can also use the API keys for the test mode to try out the Stripe integration with Airbyte.\n\nFor Replication start date, enter the date in `YYYY-MM-DDTHH:mm:ssZ` format. The data added on and after this date will be replicated.\nFor Lookback Window in days (Optional), select the number of days the value in days prior to the start date that you to sync your data with. If your data is updated after setting up this connector, you can use the this option to reload data from the past N days. Example: If the Replication start date is set to `2021-01-01T00:00:00Z`, then:\nIf you leave the Lookback Window in days parameter to its the default value of 0, Airbyte will sync data from the Replication start date `2021-01-01T00:00:00Z`\nIf the Lookback Window in days value is set to 1, Airbyte will consider the Replication start date to be `2020-12-31T00:00:00Z`\nIf the Lookback Window in days value is set to 7, Airbyte will sync data from `2020-12-25T00:00:00Z`\nClick Set up source.\n\nSupported sync modes\nThe Stripe source connector supports the following sync modes:\n\nFull Refresh\nIncremental\n\n:::note\nSince the Stripe API does not allow querying objects which were updated since the last sync, the Stripe connector uses the `created` field to query for new data in your Stripe account.\n:::\nSupported Streams\nThe Stripe source connector supports the following streams:\n\nBalance Transactions (Incremental)\nBank accounts\nCharges (Incremental)\nThe `amount` column defaults to the smallest currency unit. (See charge object for more details)\nCheckout Sessions\nCheckout Sessions Line Items\nCoupons (Incremental)\nCustomer Balance Transactions\nCustomers (Incremental)\nThis endpoint does not include deleted customers \nDisputes (Incremental)\nEvents (Incremental)\nThe Stripe API does not guarantee access to events older than 30 days, so this stream will only pull events created from the 30 days prior to the initial sync and not from the Replication start date.\nInvoice Items (Incremental)\nInvoice Line Items\nInvoices (Incremental)\nPaymentIntents (Incremental)\nPayouts (Incremental)\nPromotion Code (Incremental)\nPlans (Incremental)\nProducts (Incremental)\nRefunds (Incremental)\nSubscription Items\nSubscriptions (Incremental)\nTransfers (Incremental)\n\nData type mapping\nThe Stripe API uses the same JSONSchema types that Airbyte uses internally (`string`, `date-time`, `object`, `array`, `boolean`, `integer`, and `number`), so no type conversions are performed for the Stripe connector.\nPerformance considerations\nThe Stripe connector should not run into Stripe API limitations under normal usage. Create an issue if you see any rate limit issues that are not automatically retried successfully.",
    "tag": "airbyte"
  },
  {
    "title": "S3",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/s3.md",
    "content": "S3\nThis page contains the setup guide and reference information for the Amazon S3 source connector.\nPrerequisites\nDefine file pattern, see the Path Patterns section\nSetup guide\nStep 1: Set up Amazon S3\n\nIf syncing from a private bucket, the credentials you use for the connection must have have both `read` and `list` access on the S3 bucket. `list` is required to discover files based on the provided pattern(s).\n\nStep 2: Set up the Amazon S3 connector in Airbyte\n\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click . In the top-right corner, click +new source/destination.\nOn the Set up the source/destination page, enter the name for the `connector name` connector and select connector name from the `Source/Destination` type dropdown.\nSet `dataset` appropriately. This will be the name of the table in the destination.\nIf your bucket contains only files containing data for this table, use `**` as path_pattern. See the Path Patterns section for more specific pattern matching.\nLeave schema as `{}` to automatically infer it from the file(s). For details on providing a schema, see the User Schema section.\nFill in the fields within the provider box appropriately. If your bucket is not public, add credentials with sufficient permissions under `aws_access_key_id` and `aws_secret_access_key`.\nChoose the format corresponding to the format of your files and fill in fields as required. If unsure about values, try out the defaults and come back if needed. Find details on these settings here.\n\n\n\nFor Airbyte Open Source:\n\nCreate a new S3 source with a suitable name. Since each S3 source maps to just a single table, it may be worth including that in the name.\nSet `dataset` appropriately. This will be the name of the table in the destination.\nIf your bucket contains only files containing data for this table, use `**` as path_pattern. See the Path Patterns section for more specific pattern matching.\nLeave schema as `{}` to automatically infer it from the file(s). For details on providing a schema, see the User Schema section.\nFill in the fields within the provider box appropriately. If your bucket is not public, add credentials with sufficient permissions under `aws_access_key_id` and `aws_secret_access_key`.\nChoose the format corresponding to the format of your files and fill in fields as required. If unsure about values, try out the defaults and come back if needed. Find details on these settings here.\n\n\nSupported sync modes\nThe Amazon S3 source connector supports the following sync modes:\n| Feature                                        | Supported? |\n| :--------------------------------------------- | :--------- |\n| Full Refresh Sync                              | Yes        |\n| Incremental Sync                               | Yes        |\n| Replicate Incremental Deletes                  | No         |\n| Replicate Multiple Files (pattern matching)  | Yes        |\n| Replicate Multiple Streams (distinct tables) | No         |\n| Namespaces                                     | No         |\nFile Compressions\n| Compression | Supported? |\n| :---------- | :--------- |\n| Gzip        | Yes        |\n| Zip         | No         |\n| Bzip2       | Yes        |\n| Lzma        | No         |\n| Xz          | No         |\n| Snappy      | No         |\nPlease let us know any specific compressions you'd like to see support for next!\nPath Patterns\n(tl;dr -> path pattern syntax using wcmatch.glob. GLOBSTAR and SPLIT flags are enabled.)\nThis connector can sync multiple files by using glob-style patterns, rather than requiring a specific path for every file. This enables:\n\nReferencing many files with just one pattern, e.g. `**` would indicate every file in the bucket.\nReferencing future files that don't exist yet (and therefore don't have a specific path).\n\nYou must provide a path pattern. You can also provide many patterns split with \\| for more complex directory layouts.\nEach path pattern is a reference from the root of the bucket, so don't include the bucket name in the pattern(s).\nSome example patterns:\n\n`**` : match everything.\n`**/*.csv` : match all files with specific extension.\n`myFolder/**/*.csv` : match all csv files anywhere under myFolder.\n`*/**` : match everything at least one folder deep.\n`*/*/*/**` : match everything at least three folders deep.\n`**/file.*|**/file` : match every file called \"file\" with any extension (or no extension).\n`x/*/y/*` : match all files that sit in folder x -> any folder -> folder y.\n`**/prefix*.csv` : match all csv files with specific prefix.\n`**/prefix*.parquet` : match all parquet files with specific prefix.\n\nLet's look at a specific example, matching the following bucket layout:\n`text\nmyBucket\n    -> log_files\n    -> some_table_files\n        -> part1.csv\n        -> part2.csv\n    -> images\n    -> more_table_files\n        -> part3.csv\n    -> extras\n        -> misc\n            -> another_part1.csv`\nWe want to pick up part1.csv, part2.csv and part3.csv (excluding another_part1.csv for now). We could do this a few different ways:\n\nWe could pick up every csv file called \"partX\" with the single pattern `**/part*.csv`.\nTo be a bit more robust, we could use the dual pattern `some_table_files/*.csv|more_table_files/*.csv` to pick up relevant files only from those exact folders.\nWe could achieve the above in a single pattern by using the pattern `*table_files/*.csv`. This could however cause problems in the future if new unexpected folders started being created.\nWe can also recursively wildcard, so adding the pattern `extras/**/*.csv` would pick up any csv files nested in folders below \"extras\", such as \"extras/misc/another_part1.csv\".\n\nAs you can probably tell, there are many ways to achieve the same goal with path patterns. We recommend using a pattern that ensures clarity and is robust against future additions to the directory structure.\nUser Schema\nProviding a schema allows for more control over the output of this stream. Without a provided schema, columns and datatypes will be inferred from each file and a superset schema created. This will probably be fine in most cases but there may be situations you want to enforce a schema instead, e.g.:\n\nYou only care about a specific known subset of the columns. The other columns would all still be included, but packed into the `_ab_additional_properties` map.\nYour initial dataset is quite small (in terms of number of records), and you think the automatic type inference from this sample might not be representative of the data in the future.\nYou want to purposely define types for every column.\nYou know the names of columns that will be added to future data and want to include these in the core schema as columns rather than have them appear in the `_ab_additional_properties` map.\n\nOr any other reason! The schema must be provided as valid JSON as a map of `{\"column\": \"datatype\"}` where each datatype is one of:\n\nstring\nnumber\ninteger\nobject\narray\nboolean\nnull\n\nFor example:\n\n{\"id\": \"integer\", \"location\": \"string\", \"longitude\": \"number\", \"latitude\": \"number\"}\n{\"username\": \"string\", \"friends\": \"array\", \"information\": \"object\"}\n\nS3 Provider Settings\n\n`bucket` : name of the bucket your files are in\n`aws_access_key_id` : one half of the required credentials for accessing a private bucket.\n`aws_secret_access_key` : other half of the required credentials for accessing a private bucket.\n`path_prefix` : an optional string that limits the files returned by AWS when listing files to only that those starting with this prefix. This is different to path_pattern as it gets pushed down to the API call made to S3 rather than filtered in Airbyte and it does not accept pattern-style symbols (like wildcards `*`). We recommend using this if your bucket has many folders and files that are unrelated to this stream and all the relevant files will always sit under this chosen prefix.\nTogether with `path_pattern`, there are multiple ways to specify the files to sync. For example, all the following configs are equivalent:\n`path_prefix` = `<empty>`, `path_pattern` = `path1/path2/myFolder/**/*`.\n`path_prefix` = `path1/`, `path_pattern` = `path2/myFolder/**/*.csv`.\n`path_prefix` = `path1/path2/` and `path_pattern` = `myFolder/**/*.csv`\n`path_prefix` = `path1/path2/myFolder/`, `path_pattern` = `**/*.csv`. This is the most efficient one because the directories are filtered earlier in the S3 API call. However, the difference in efficiency is usually negligible.\n\n\nThe rationale of having both `path_prefix` and `path_pattern` is to accommodate as many use cases as possible. If you found them confusing, feel free to ignore `path_prefix` and just set the `path_pattern`.\n`endpoint` : optional parameter that allow using of non Amazon S3 compatible services. Leave it blank for using default Amazon serivce.\n`use_ssl` : Allows using custom servers that configured to use plain http. Ignored in case of using Amazon service.\n`verify_ssl_cert` : Skip ssl validity check in case of using custom servers with self signed certificates. Ignored in case of using Amazon service.\n\nFile Format Settings\nThe Reader in charge of loading the file format is currently based on PyArrow (Apache Arrow).\nNote that all files within one stream must adhere to the same read options for every provided format.\nCSV\nSince CSV files are effectively plain text, providing specific reader options is often required for correct parsing of the files. These settings are applied when a CSV is created or exported so please ensure that this process happens consistently over time.\n\n`delimiter` : Even though CSV is an acronymn for Comma Separated Values, it is used more generally as a term for flat file data that may or may not be comma separated. The delimiter field lets you specify which character acts as the separator.\n`quote_char` : In some cases, data values may contain instances of reserved characters (like a comma, if that's the delimiter). CSVs can allow this behaviour by wrapping a value in defined quote characters so that on read it can parse it correctly.\n`escape_char` : An escape character can be used to prefix a reserved character and allow correct parsing.\n`encoding` : Some data may use a different character set (typically when different alphabets are involved). See the list of allowable encodings here.\n`double_quote` : Whether two quotes in a quoted CSV value denote a single quote in the data.\n`newlines_in_values` : Sometimes referred to as `multiline`. In most cases, newline characters signal the end of a row in a CSV, however text data may contain newline characters within it. Setting this to True allows correct parsing in this case.\n`block_size` : This is the number of bytes to process in memory at a time while reading files. The default value here is usually fine but if your table is particularly wide (lots of columns / data in fields is large) then raising this might solve failures on detecting schema. Since this defines how much data to read into memory, raising this too high could cause Out Of Memory issues so use with caution.\n\n`additional_reader_options` : This allows for editing the less commonly required CSV ConvertOptions. The value must be a valid JSON string, e.g.:\n`text\n{\"timestamp_parsers\": [\"%m/%d/%Y %H:%M\", \"%Y/%m/%d %H:%M\"], \"strings_can_be_null\": true, \"null_values\": [\"NA\", \"NULL\"]}`\n* `advanced_options` : This allows for editing the less commonly required CSV ReadOptions. The value must be a valid JSON string. One use case for this is when your CSV has no header, or you want to use custom column names, you can specify `column_names` using this option.\n`test\n{\"column_names\": [\"column1\", \"column2\", \"column3\"]}`\n\n\nParquet\nApache Parquet file is a column-oriented data storage format of the Apache Hadoop ecosystem. It provides efficient data compression and encoding schemes with enhanced performance to handle complex data in bulk. For now, the solution involves iterating through individual files at the abstract level thus partitioned parquet datasets are unsupported. The following settings are available:\n\n`buffer_size` : If positive, perform read buffering when deserializing individual column chunks. Otherwise IO calls are unbuffered.\n`columns` : If not None, only these columns will be read from the file.\n`batch_size` : Maximum number of records per batch. Batches may be smaller if there aren\u2019t enough rows in the file.\n\nYou can find details on here.\nAvro\nThe avro parser uses fastavro. Currently, no additional options are supported.\nJsonl\nThe Jsonl parser uses pyarrow hence,only the line-delimited JSON format is supported.For more detailed info, please refer to the [docs] (https://arrow.apache.org/docs/python/generated/pyarrow.json.read_json.html)",
    "tag": "airbyte"
  },
  {
    "title": "Vittally",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/vitally.md",
    "content": "Vittally\nSync overview\nThe Vitally source supports both Full Refresh only.\nThis source can sync data for the Vitally API.\nOutput schema\nThis Source is capable of syncing the following core Streams:\n\nAccounts\nAdmins\nConversations\nNotes\nNPS Responses\nTasks\nUsers\n\nFeatures\n| Feature                   | Supported?(Yes/No) | Notes |\n| :------------------------ | :------------------- | :---- |\n| Full Refresh Sync         | Yes                  |       |\n| Incremental - Append Sync | No                   |       |\n| Namespaces                | No                   |       |\nPerformance considerations\nThe Vitally connector should not run into Vitally API limitations under normal usage.\nRequirements\n\nVitaly API key. See the Vitally docs for information on how to obtain an API key.\n",
    "tag": "airbyte"
  },
  {
    "title": "Smartsheets",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/smartsheets.md",
    "content": "Smartsheets\nThis page guides you through the process of setting up the Smartsheets source connector.\nPrerequisites\nTo configure the Smartsheet Source for syncs, you'll need the following:\n\nA Smartsheets API access token - generated by a Smartsheets user with at least read access\nThe ID of the spreadsheet you'd like to sync\n\nStep 1: Set up Smartsheets\nObtain a Smartsheets API access token\nYou can generate an API key for your account from a session of your Smartsheet webapp by clicking:\n\nAccount (top-right icon)\nApps & Integrations\nAPI Access\nGenerate new access token\n\nFor questions on advanced authorization flows, refer to this.\nPrepare the spreadsheet ID of your Smartsheet\nYou'll also need the ID of the Spreadsheet you'd like to sync. Unlike Google Sheets, this ID is not found in the URL. You can find the required spreadsheet ID from your Smartsheet app session by going to:\n\nFile\nProperties\n\nStep 2: Set up the Smartsheets connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the Smartsheets connector and select Smartsheets from the Source type dropdown.\nAuthenticate via OAuth2.0 using the API access token from Prerequisites\nEnter the start date and the ID of the spreadsheet you want to sync\nSubmit the form\n\nFor Airbyte Open Source:\n1. Navigate to the Airbyte Open Source dashboard\n2. Set the name for your source\n3. Enter the API access token from Prerequisites\n4. Enter the ID of the spreadsheet you want to sync\n5. Enter a start sync date\n6. Click Set up source\nSupported sync modes\nThe Smartsheets source connector supports the following sync modes:\n - Full Refresh | Overwrite\n - Full Refresh | Append\n - Incremental  | Append\n - Incremental  | Deduped\nPerformance considerations\nAt the time of writing, the Smartsheets API rate limit is 300 requests per minute per API access token.\nSupported streams\nThis source provides a single stream per spreadsheet with a dynamic schema, depending on your spreadsheet structure.\nFor example, having a spreadsheet `Customers`, the connector would introduce a stream with the same name and properties typed according to Data type map (see below).\nImportant highlights\nThe Smartsheet Source is written to pull data from a single Smartsheet spreadsheet. Unlike Google Sheets, Smartsheets only allows one sheet per Smartsheet - so a given Airbyte connector instance can sync only one sheet at a time. To replicate multiple spreadsheets, you can create multiple instances of the Smartsheet Source in Airbyte, reusing the API token for all your sheets that you need to sync.\nNote: Column headers must contain only alphanumeric characters or `_` , as specified in the Airbyte Protocol.\nData type map\nThe data type mapping adopted by this connector is based on the Smartsheet documentation.\nNOTE: For any column datatypes interpreted by Smartsheets beside `DATE` and `DATETIME`, this connector's source schema generation assumes a `string` type, in which case the `format` field is not required by Airbyte.\n| Integration Type | Airbyte Type | Airbyte Format       |\n|:-----------------|:-------------|:---------------------|\n| `TEXT_NUMBER`    | `string`     |                      |\n| `DATE`           | `string`     | `format: date`       |\n| `DATETIME`       | `string`     | `format: date-time`  |\n| `anything else`  | `string`     |                      |\nThe remaining column datatypes supported by Smartsheets are more complex types (e.g. Predecessor, Dropdown List) and are not supported by this connector beyond its `string` representation.",
    "tag": "airbyte"
  },
  {
    "title": "Google Sheets",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/google-sheets.md",
    "content": "Google Sheets\nThis page guides you through the process of setting up the Google Sheets source connector.\n:::info\nThe Google Sheets source connector pulls data from a single Google Sheets spreadsheet. To replicate multiple spreadsheets, set up multiple Google Sheets source connectors in your Airbyte instance.\n:::\nSetup guide\n\nFor Airbyte Cloud:\nTo set up Google Sheets as a source in Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click + New source.\nOn the Set up the source page, select Google Sheets from the Source type dropdown.\nEnter a name for the Google Sheets connector.\nAuthenticate your Google account via OAuth or Service Account Key Authentication.\n(Recommended) To authenticate your Google account via OAuth, click Sign in with Google and complete the authentication workflow.\nTo authenticate your Google account via Service Account Key Authentication, enter your Google Cloud service account key in JSON format. Make sure the Service Account has the Project Viewer permission. If your spreadsheet is viewable by anyone with its link, no further action is needed. If not, give your Service account access to your spreadsheet.\n\n\nFor Spreadsheet Link, enter the link to the Google spreadsheet. To get the link, go to the Google spreadsheet you want to sync, click Share in the top right corner, and click Copy Link.\nFor Row Batch Size, define the number of records you want the Google API to fetch at a time. The default value is 200.\n\n\n\nFor Airbyte Open Source:\nTo set up Google Sheets as a source in Airbyte Open Source:\n\n\nEnable the Google Cloud Platform APIs for your personal or organization account.\n:::info\nThe connector only finds the spreadsheet you want to replicate; it does not access any of your other files in Google Drive.\n:::\n\n\nGo to the Airbyte UI and in the left navigation bar, click Sources. In the top-right corner, click + New source.\n\nOn the Set up the source page, select Google Sheets from the Source type dropdown.\nEnter a name for the Google Sheets connector.\nAuthenticate your Google account via OAuth or Service Account Key Authentication:\nTo authenticate your Google account via OAuth, enter your Google application's client ID, client secret, and refresh token.\nTo authenticate your Google account via Service Account Key Authentication, enter your Google Cloud service account key in JSON format. Make sure the Service Account has the Project Viewer permission. If your spreadsheet is viewable by anyone with its link, no further action is needed. If not, give your Service account access to your spreadsheet.\n\n\nFor Spreadsheet Link, enter the link to the Google spreadsheet. To get the link, go to the Google spreadsheet you want to sync, click Share in the top right corner, and click Copy Link.\n\n\nOutput schema\nEach sheet in the selected spreadsheet is synced as a separate stream. Each selected column in the sheet is synced as a string field.\nNote: Sheet names and column headers must contain only alphanumeric characters or `_`, as specified in the Airbyte Protocol. For example, if your sheet or column header is named `the data`, rename it to `the_data`. This restriction does not apply to non-header cell values.\nAirbyte only supports replicating Grid sheets.\nSupported sync modes\nThe Google Sheets source connector supports the following sync modes:\n\nFull Refresh - Overwrite\nFull Refresh - Append\n\nData type mapping\n| Integration Type | Airbyte Type | Notes |\n| :--------------- | :----------- | :---- |\n| any type         | `string`     |       |\nPerformance consideration\nThe Google API rate limit is 100 requests per 100 seconds per user and 500 requests per 100 seconds per project. Airbyte batches requests to the API in order to efficiently pull data and respects these rate limits. We recommended not using the same service user for more than 3 instances of the Google Sheets source connector to ensure high transfer speeds.",
    "tag": "airbyte"
  },
  {
    "title": "Freshdesk",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/freshdesk.md",
    "content": "Freshdesk\nThis page guides you through the process of setting up the Freshdesk source connector.\nPrerequisites\nTo set up the Freshdesk source connector, you'll need the Freshdesk domain URL and the API key.\nSet up the Freshdesk connector in Airbyte\n\nLog into your Airbyte Cloud account or navigate to the Airbyte Open Source dashboard.\nClick Sources and then click + New source.\nOn the Set up the source page, select Freshdesk from the Source type dropdown.\nEnter the name for the Freshdesk connector.\nFor Domain, enter your Freshdesk domain URL.\nFor API Key, enter your Freshdesk API key.\nFor Start Date, enter the date in YYYY-MM-DDTHH:mm:ssZ format. The data added on and after this date will be replicated.\nFor Requests per minute, enter the number of requests per minute that this source allowed to use. The Freshdesk rate limit is 50 requests per minute per app per account.\nClick Set up source.\n\nSupported sync modes\n\nFull Refresh - Overwrite\nFull Refresh - Append\nIncremental - Append\nIncremental - Deduped History\n\nSupported Streams\nSeveral output streams are available from this source:\n\nAgents\nBusiness Hours\nCanned Responses\nCanned Response Folders\nCompanies\nContacts (Native Incremental Sync)\nConversations\nDiscussion Categories\nDiscussion Comments\nDiscussion Forums\nDiscussion Topics\nEmail Configs\nEmail Mailboxes\nGroups\nProducts\nRoles\nSatisfaction Ratings\nScenario Automations\nSettings\nSkills\nSLA Policies\nSolution Articles\nSolution Categories\nSolution Folders\nSurveys\nTickets (Native Incremental Sync)\nTicket Fields\nTime Entries\n\nPerformance considerations\nThe Freshdesk connector should not run into Freshdesk API limitations under normal usage. Create an issue if you encounter any rate limit issues that are not automatically retried successfully.\nIf you don't use the start date Freshdesk will retrieve only the last 30 days. More information here.",
    "tag": "airbyte"
  },
  {
    "title": "Sentry",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/sentry.md",
    "content": "Sentry\nThis page contains the setup guide and reference information for the Sentry source connector.\nPrerequisites\nTo set up the Sentry source connector, you'll need the Sentry project name, authentication token, and organization.\nSet up the Sentry connector in Airbyte\n\nLog into your Airbyte Cloud account or navigate to the Airbyte Open Source dashboard.\nClick Sources and then click + New source.\nOn the Set up the source page, select Sentry from the Source type dropdown.\nEnter the name for the Sentry connector.\nFor Project, enter the name of the Sentry project you want to sync.\nFor Host Name, enter the host name of your self-hosted Sentry API Server. If your server isn't self-hosted, leave the field blank.\nFor Authentication Tokens, enter the Sentry authentication token.\nFor Organization, enter the Sentry Organization the groups belong to.\nClick Set up source.\n\nSupported sync modes\nThe Sentry source connector supports the following sync modes:\n\nFull Refresh - Overwrite\nFull Refresh - Append\nIncremental - Append\nIncremental - Deduped History\n\nSupported Streams\n\nEvents\nIssues\n\nData type map\n| Integration Type    | Airbyte Type |\n| :------------------ | :----------- |\n| `string`            | `string`     |\n| `integer`, `number` | `number`     |\n| `array`             | `array`      |\n| `object`            | `object`     |",
    "tag": "airbyte"
  },
  {
    "title": "Freshcaller",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/freshcaller.md",
    "content": "Freshcaller\nOverview\nThe Freshcaller source supports full refresh and incremental sync. Depending on your needs, one could choose appropriate sync mode - `full refresh` replicates all records every time a sync happens where as `incremental` replicates net-new records since the last successful sync.\nOutput schema\nThe following endpoints are supported from this source:\n\nUsers\nTeams\nCalls\nCall Metrics\n\nIf there are more endpoints you'd like Airbyte to support, please create an issue.\nFeatures\n| Feature | Supported? |\n| :--- | :--- |\n| Full Refresh Sync | Yes |\n| Incremental Sync | Yes |\n| SSL connection | Yes |\n| Namespaces | No |\nPerformance considerations\nThe Freshcaller connector should not run into Freshcaller API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.\nGetting started\nRequirements\n\nFreshcaller Account\nFreshcaller API Key\n\nSetup guide\nPlease read How to find your API key.",
    "tag": "airbyte"
  },
  {
    "title": "Visma e-conomic",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/visma-economic.md",
    "content": "Visma e-conomic\nSync overview\nThis source collects data from Visma e-conomic.\nAt the moment the source only implements full refresh, meaning you will sync all records with every new sync.\nPrerequisites\n\nYour Visma e-conomic Agreement Grant Token\nYour Visma e-conomic App Secret Token\n\nThis page guides you through the different ways of connecting to the api.\nIn sort your options are: \n* Developer agreement\n* Create a free sandbox account, valid for 14 days.\n* Demo tokens:  `app_secret_token=demo` and `agreement_grant_token=demo`\nSet up the Visma e-conomic source connector\n\nLog into your Airbyte Cloud or Airbyte Open Source account.\nClick Sources and then click + New source.\nOn the Set up the source page, select Stripe from the Source type dropdown.\nEnter a name for your source.\nEnter Agreement Grant Token.\nEnter Secret Key.\n\nThis Source Supports the Following Streams\n\naccounts\ncustomers\ninvoices booked\ninvoices booked document\ninvoices paid\ninvoices total\nproducts\n\nFor more information about the api see the E-conomic REST API Documentation.\nSync models\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | No |  |",
    "tag": "airbyte"
  },
  {
    "title": "Bamboo HR",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/bamboo-hr.md",
    "content": "Bamboo HR\nOverview\nThe BambooHr source supports Full Refresh sync. You can choose if this connector will overwrite the old records or duplicate old ones.\nOutput schema\nThis connector outputs the following stream:\n\nCustom Reports\n\nFeatures\n| Feature | Supported? |\n| :--- | :--- |\n| Full Refresh Sync | Yes |\n| Incremental - Append Sync | No |\n| SSL connection | Yes |\n| Namespaces | No |\nPerformance considerations\nBambooHR has the rate limits, but the connector should not run into API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.\nGetting started\nRequirements\n\nBambooHr Account\nBambooHr Api key\n\nBamboo HR\nThis page contains the setup guide and reference information for the Bamboo HR source connector.\nPrerequisites\n\nBambooHr Account\nBambooHr Api key\n\nSetup guide\nStep 1: Set up the Bamboo HR connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the Bamboo HR connector and select Bamboo HR from the Source type dropdown.\nEnter your `subdomain`\nEnter your `api_key`\nEnter your `custom_reports_fields` if need\nChoose `custom_reports_include_default_fields` flag value\nClick Set up source\n\nFor Airbyte OSS:\n\nNavigate to the Airbyte Open Source dashboard\nSet the name for your source \nEnter your `subdomain`\nEnter your `api_key`\nEnter your `custom_reports_fields` if need\nChoose `custom_reports_include_default_fields` flag value\nClick Set up source\n\nSupported sync modes\nThe Bamboo HR source connector supports the following sync modes:\n| Feature | Supported? |\n| :--- | :--- |\n| Full Refresh Sync | Yes |\n| Incremental - Append Sync | No |\n| SSL connection | Yes |\n| Namespaces | No |\nSupported Streams\n\nCustom Reports\n\nPerformance considerations\nBambooHR has the rate limits, but the connector should not run into API limitations under normal usage. \nPlease create an issue if you see any rate limit issues that are not automatically retried successfully.",
    "tag": "airbyte"
  },
  {
    "title": "Recharge",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/recharge.md",
    "content": "Recharge\nThis source can sync data for the Recharge API.\nThis page guides you through the process of setting up the Recharge source connector.\nPrerequisites\n\nA Recharge account with permission to access data from accounts you want to sync.\nRecharge API Token\n\nSetup guide\nStep 1: Set up Recharge\nPlease read How to generate your API token.\nStep 2: Set up the source connector in Airbyte\n\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click + new source.\nOn the source setup page, select Recharge from the Source type dropdown and enter a name for this connector.\nChoose required `Start date`\nEnter your `Access Token`.\nclick `Set up source`.\n\n\n\nFor Airbyte Open Source:\n\nGo to local Airbyte page.\nIn the left navigation bar, click Sources. In the top-right corner, click + new source.\nOn the source setup page, select Recharge from the Source type dropdown and enter a name for this connector.\nChoose required `Start date`\nEnter your `Access Token` generated from `Step 1`.\nclick `Set up source`.\n\n\nSupported sync modes\nThe Recharge supports full refresh and incremental sync.\n| Feature           | Supported? |\n| :---------------- | :--------- |\n| Full Refresh Sync | Yes        |\n| Incremental Sync  | Yes        |\n| SSL connection    | Yes        |\nSupported Streams\nSeveral output streams are available from this source:\n\nAddresses (Incremental sync)\nCharges (Incremental sync)\nCollections\nCustomers (Incremental sync)\nDiscounts (Incremental sync)\nMetafields\nOnetimes (Incremental sync)\nOrders (Incremental sync)\nProducts\nShop\nSubscriptions (Incremental sync)\n\nIf there are more endpoints you'd like Airbyte to support, please create an issue.\nPerformance considerations\nThe Recharge connector should gracefully handle Recharge API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.",
    "tag": "airbyte"
  },
  {
    "title": "Microsoft Dynamics NAV",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/microsoft-dynamics-nav.md",
    "content": "Microsoft Dynamics NAV\nMS Dynamics NAV is a business management solution for small and mid-sized organizations that automates and streamlines business processes and helps you manage your business.\nSync overview\nMS Dynamics NAV runs on the MSSQL database. You can use the MSSQL connector to sync your MS Dynamics NAV instance by connecting to the underlying database.\n:::info\nReach out to your service representative or system admin to find the parameters required to connect to the underlying database\n:::\nOutput schema\nTo understand your MS Dynamics NAV database schema, see the Microsoft docs. Otherwise, the schema will be loaded according to the rules of MSSQL connector.",
    "tag": "airbyte"
  },
  {
    "title": "Klaviyo",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/klaviyo.md",
    "content": "Klaviyo\nThis page contains the setup guide and reference information for the Klaviyo source connector.\nPrerequisites\nTo set up the Klaviyo source connector, you'll need the Klaviyo Private API key.\nSet up the Klaviyo connector in Airbyte\n\nLog into your Airbyte Cloud or navigate to the Airbyte Open Source dashboard.\nClick Sources and then click + New source.\nOn the Set up the source page, select Klaviyo from the Source type dropdown.\nEnter the name for the Klaviyo connector.\nFor Api Key, enter the Klaviyo Private API key.\nFor Start Date, enter the date in YYYY-MM-DD format. The data added on and after this date will be replicated. \nClick Set up source.\n\nSupported sync modes\nThe Klaviyo source connector supports the following sync modes:\n\nFull Refresh - Overwrite\nFull Refresh - Append\nIncremental - Append\nIncremental - Deduped History\n\nSupported Streams\n\nCampaigns\nEvents\nGlobalExclusions\nLists\nMetrics\nFlows\n\nPerformance considerations\nThe connector is restricted by Klaviyo requests limitation.\nThe Klaviyo connector should not run into Klaviyo API limitations under normal usage. Create an issue if you encounter any rate limit issues that are not automatically retried successfully.\nData type map\n| Integration Type | Airbyte Type | Notes |\n|:-----------------|:-------------|:------|\n| `string`         | `string`     |       |\n| `number`         | `number`     |       |\n| `array`          | `array`      |       |\n| `object`         | `object`     |       |",
    "tag": "airbyte"
  },
  {
    "title": "Dynamodb",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/dynamodb.md",
    "content": "Dynamodb\nThe Dynamodb source allows you to sync data from Dynamodb. The source supports Full Refresh and Incremental sync strategies.\nResulting schema\nDynamodb doesn't have table schemas. The discover phase has three steps:\nStep 1. Retrieve items\nThe connector scans the table with a scan limit of 1k and if the data set size is > 1MB it will initiate another\nscan with the same limit until it has >= 1k items.\nStep 2. Combining attributes\nAfter retrieving the items it will combine all the different top level attributes found in the retrieved items. The implementation\nassumes that the same attribute present in different items has the same type and possibly nested attributes values.\nStep 3. Determine property types\nFor each item attribute found the connector determines its type by calling AttributeValue.type(), depending on the received type it will map the\nattribute to one of the supported Airbyte types in the schema.\nFeatures\n| Feature | Supported |\n| :--- | :--- |\n| Full Refresh Sync | Yes |\n| Incremental - Append Sync | Yes |\n| Replicate Incremental Deletes | No |\n| Namespaces | No |\nFull Refresh sync\nWorks as usual full refresh sync.\nIncremental sync\nCursor field can't be nested, and it needs to be top level attribute in the item.\nCursor should never be blank. and it needs to be either a string or integer type - the incremental sync results might be unpredictable and will totally rely on Dynamodb comparison algorithm.\nOnly `ISO 8601` and `epoch` cursor types are supported. Cursor type is determined based on the property type present in the previously generated schema:\n\n`ISO 8601` - if cursor type is string\n`epoch` - if cursor type is integer\n\nGetting started\nThis guide describes in details how you can configure the connector to connect with Dynamodb.\n\u0421onfiguration Parameters\n\nendpoint: aws endpoint of the dynamodb instance\nregion: the region code of the dynamodb instance\naccess_key_id: the access key for the IAM user with the required permissions\nsecret_access_key: the secret key for the IAM user with the required permissions\n",
    "tag": "airbyte"
  },
  {
    "title": "Google Workspace Admin Reports",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/google-workspace-admin-reports.md",
    "content": "Google Workspace Admin Reports\nOverview\nThis source supports Full Refresh syncs. It uses the Reports API to gain insights on content management with Google Drive activity reports and Audit administrator actions.\nOutput schema\nThis Source is capable of syncing the following Streams:\n\nadmin\ndrive\nlogins\nmobile\noauth_tokens\n\nData type mapping\n| Integration Type | Airbyte Type | Notes |\n| :--- | :--- | :--- |\n| `string` | `string` |  |\n| `number` | `number` |  |\n| `array` | `array` |  |\n| `object` | `object` |  |\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | No |  |\n| SSL connection | Yes |  |\n| Namespaces | No |  |\nPerformance considerations\nThis connector attempts to back off gracefully when it hits Reports API's rate limits. To find more information about limits, see Reports API Limits and Quotas documentation.\nGetting started\nRequirements\n\nCredentials to a Google Service Account with delegated Domain Wide Authority\nEmail address of the workspace admin which created the Service Account\n\nCreate a Service Account with delegated domain wide authority\nFollow the Google Documentation for performing Domain Wide Delegation of Authority to create a Service account with delegated domain wide authority. This account must be created by an administrator of the Google Workspace. Please make sure to grant the following OAuth scopes to the service user:\n\n`https://www.googleapis.com/auth/admin.reports.audit.readonly`\n`https://www.googleapis.com/auth/admin.reports.usage.readonly`\n\nAt the end of this process, you should have JSON credentials to this Google Service Account.\nYou should now be ready to use the Google Workspace Admin Reports API connector in Airbyte.",
    "tag": "airbyte"
  },
  {
    "title": "Prerequisite",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/hubspot.inapp.md",
    "content": "Prerequisite\n\nAccess to your HubSpot account\n\nSetup guide\n\nFor Start date, enter the date in `YYYY-MM-DDTHH:mm:ssZ` format. The data added on and after this date will be replicated. If this field is blank, Airbyte will replicate all data.\nYou can use OAuth or an API key to authenticate your HubSpot account. We recommend using OAuth for Airbyte Cloud and an API key for Airbyte Open Source.\nTo authenticate using OAuth for Airbyte Cloud, ensure you have set the appropriate scopes for HubSpot and then click Authenticate your HubSpot account to sign in with HubSpot and authorize your account.\nTo authenticate using an API key for Airbyte Open Source, select API key from the Authentication dropdown and enter the API key for your HubSpot account. Check the performance considerations before using an API key.\nClick Set up source.\n",
    "tag": "airbyte"
  },
  {
    "title": "Dremio",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/dremio.md",
    "content": "Dremio\nOverview\nThe Dremio source supports Full Refresh sync. You can choose if this connector will copy only the new or updated data, or all rows in the tables and columns you set up for replication, every time a sync is run.\nThis Source Connector is based on a Airbyte CDK.\nOutput schema\nSeveral output streams are available from this source:\n\nCatalogs (Full table)\n\nIf there are more endpoints you'd like Airbyte to support, please create an issue.\nFeatures\n| Feature | Supported? |\n| :--- | :--- |\n| Full Refresh Sync | Yes |\n| Incremental - Append Sync | No |\n| SSL connection | Yes |\n| Namespaces | No |\nGetting started\nRequirements\n\nAPI Key\nBase URL\n\nSetup guide\nConnector needs a self-hosted instance of Dremio, this way you can access the Dremio REST API on which this source is based. Please refer to Dremio Deployment Models document, or take a look at Dremio OSS for reference.\nPlease read How to get your APIs credentials.",
    "tag": "airbyte"
  },
  {
    "title": "Microsoft SQL Server (MSSQL)",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/mssql.md",
    "content": "Microsoft SQL Server (MSSQL)\nFeatures\n| Feature | Supported | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync - Append | Yes |  |\n| Replicate Incremental Deletes | Yes |  |\n| CDC (Change Data Capture) | Yes |  |\n| SSL Support | Yes |  |\n| SSH Tunnel Connection | Yes |  |\n| Namespaces | Yes | Enabled by default |\nThe MSSQL source does not alter the schema present in your database. Depending on the destination connected to this source, however, the schema may be altered. See the destination's documentation for more details.\nTroubleshooting\nYou may run into an issue where the connector provides wrong values for some data types. See discussion on unexpected behaviour for certain datatypes.\nNote: Currently hierarchyid and sql_variant are not processed in CDC migration type (not supported by debezium). For more details please check\nthis ticket\nGetting Started (Airbyte Cloud)\nOn Airbyte Cloud, only TLS connections to your MSSQL instance are supported in source configuration. Other than that, you can proceed with the open-source instructions below.\nGetting Started (Airbyte Open-Source)\nRequirements\n\nMSSQL Server `Azure SQL Database`, `Azure Synapse Analytics`, `Azure SQL Managed Instance`, `SQL Server 2019`, `SQL Server 2017`, `SQL Server 2016`, `SQL Server 2014`, `SQL Server 2012`, `PDW 2008R2 AU34`.\nCreate a dedicated read-only Airbyte user with access to all tables needed for replication\nIf you want to use CDC, please see the relevant section below for further setup requirements\n\n1. Make sure your database is accessible from the machine running Airbyte\nThis is dependent on your networking setup. The easiest way to verify if Airbyte is able to connect to your MSSQL instance is via the check connection tool in the UI.\n2. Create a dedicated read-only user with access to the relevant tables (Recommended but optional)\nThis step is optional but highly recommended to allow for better permission control and auditing. Alternatively, you can use Airbyte with an existing user in your database.\nComing soon: suggestions on how to create this user.\n3. Your database user should now be ready for use with Airbyte!\nChange Data Capture (CDC)\nWe use SQL Server's change data capture feature to capture row-level `INSERT`, `UPDATE` and `DELETE` operations that occur on cdc-enabled tables.\nSome extra setup requiring at least db_owner permissions on the database(s) you intend to sync from will be required (detailed below).\nPlease read the CDC docs for an overview of how Airbyte approaches CDC.\nShould I use CDC for MSSQL?\n\nIf you need a record of deletions and can accept the limitations posted below, CDC is the way to go!\nIf your data set is small and/or you just want a snapshot of your table in the destination, consider using Full Refresh replication for your table instead of CDC.\nIf the limitations below prevent you from using CDC and your goal is to maintain a snapshot of your table in the destination, consider using non-CDC incremental and occasionally reset the data and re-sync.\nIf your table has a primary key but doesn't have a reasonable cursor field for incremental syncing (i.e. `updated_at`), CDC allows you to sync your table incrementally.\n\nCDC Config\n| Parameter | Type | Default | Description |\n| :--- | :---: | :---: | :--- |\n| Data to Sync | Enum: `Existing and New`, `New Changes Only` | `Existing and New` | What data should be synced under the CDC. `Existing and New` will read existing data as a snapshot, and sync new changes through CDC. `New Changes Only` will skip the initial snapshot, and only sync new changes through CDC. See documentation here for details. Under the hood, this parameter sets the `snapshot.mode` in Debezium. |\n| Snapshot Isolation Level | Enum: `Snapshot`, `Read Committed` | `Snapshot` | Mode to control which transaction isolation level is used and how long the connector locks tables that are designated for capture. If you don't know which one to choose, just use the default one. See documentation here for details. Under the hood, this parameter sets the `snapshot.isolation.mode` in Debezium. |\nCDC Limitations\n\nMake sure to read our CDC docs to see limitations that impact all databases using CDC replication.\nThere are some critical issues regarding certain datatypes. Please find detailed info in this Github issue.\nCDC is only available for SQL Server 2016 Service Pack 1 (SP1) and later.\ndb_owner (or higher) permissions are required to perform the neccessary setup for CDC.\nIf you set `Initial Snapshot Isolation Level` to `Snapshot`, you must enable snapshot isolation mode on the database(s) you want to sync. This is used for retrieving an initial snapshot without locking tables.\nFor SQL Server Always On read-only replica, only `Snapshot` initial snapshot isolation level is supported.\nOn Linux, CDC is not supported on versions earlier than SQL Server 2017 CU18 (SQL Server 2019 is supported).\nChange data capture cannot be enabled on tables with a clustered columnstore index. (It can be enabled on tables with a non-clustered columnstore index).\nThe SQL Server CDC feature processes changes that occur in user-created tables only. You cannot enable CDC on the SQL Server master database.\nUsing variables with partition switching on databases or tables with change data capture (CDC) is not supported for the `ALTER TABLE` ... `SWITCH TO` ... `PARTITION` ... statement\nOur implementation has not been tested with managed instances, such as Azure SQL Database (we welcome any feedback from users who try this!)\nIf you do want to try this, CDC can only be enabled on Azure SQL databases tiers above Standard 3 (S3+). Basic, S0, S1 and S2 tiers are not supported for CDC.\nOur CDC implementation uses at least once delivery for all change records.\nRead more on CDC limitations in the Microsoft docs.\n\nSetting up CDC for MSSQL\n1. Enable CDC on database and tables\nMS SQL Server provides some built-in stored procedures to enable CDC.\n\nTo enable CDC, a SQL Server administrator with the necessary privileges (db_owner or sysadmin) must first run a query to enable CDC at the database level.\n\n`text\n  USE {database name}\n  GO\n  EXEC sys.sp_cdc_enable_db\n  GO`\n\nThe administrator must then enable CDC for each table that you want to capture. Here's an example:\n\n```text\n  USE {database name}\n  GO\nEXEC sys.sp_cdc_enable_table\n  @source_schema = N'{schema name}',\n  @source_name   = N'{table name}',\n  @role_name     = N'{role name}',  [1]\n  @filegroup_name = N'{fiilegroup name}', [2]\n  @supports_net_changes = 0 [3]\n  GO\n  ```\n\n[1] Specifies a role which will gain `SELECT` permission on the captured columns of the source table. We suggest putting a value here so you can use this role in the next step but you can also set the value of @role_name to `NULL` to allow only sysadmin and db_owner to have access. Be sure that the credentials used to connect to the source in Airbyte align with this role so that Airbyte can access the cdc tables.\n[2] Specifies the filegroup where SQL Server places the change table. We recommend creating a separate filegroup for CDC but you can leave this parameter out to use the default filegroup.\n\n[3] If 0, only the support functions to query for all changes are generated. If 1, the functions that are needed to query for net changes are also generated. If supports_net_changes is set to 1, index_name must be specified, or the source table must have a defined primary key.\n\n\n(For more details on parameters, see the Microsoft doc page for this stored procedure).\n\nIf you have many tables to enable CDC on and would like to avoid having to run this query one-by-one for every table, this script might help!\n\nFor further detail, see the Microsoft docs on enabling and disabling CDC.\n2. Enable snapshot isolation\n\nWhen a sync runs for the first time using CDC, Airbyte performs an initial consistent snapshot of your database. To avoid acquiring table locks, Airbyte uses snapshot isolation, allowing simultaneous writes by other database clients. This must be enabled on the database like so:\n\n`text\n  ALTER DATABASE {database name}\n    SET ALLOW_SNAPSHOT_ISOLATION ON;`\n3. Create a user and grant appropriate permissions\n\nRather than use sysadmin or db_owner credentials, we recommend creating a new user with the relevant CDC access for use with Airbyte. First let's create the login and user and add to the db_datareader role:\n\n`text\n  USE {database name};\n  CREATE LOGIN {user name}\n    WITH PASSWORD = '{password}';\n  CREATE USER {user name} FOR LOGIN {user name};\n  EXEC sp_addrolemember 'db_datareader', '{user name}';`\n\n\nAdd the user to the role specified earlier when enabling cdc on the table(s):\n`text\nEXEC sp_addrolemember '{role name}', '{user name}';`\n\n\nThis should be enough access, but if you run into problems, try also directly granting the user `SELECT` access on the cdc schema:\n`text\nUSE {database name};\nGRANT SELECT ON SCHEMA :: [cdc] TO {user name};`\n\n\nIf feasible, granting this user 'VIEW SERVER STATE' permissions will allow Airbyte to check whether or not the SQL Server Agent is running. This is preferred as it ensures syncs will fail if the CDC tables are not being updated by the Agent in the source database.\n`text\nUSE master;\nGRANT VIEW SERVER STATE TO {user name};`\n\n\n4. Extend the retention period of CDC data\n\nIn SQL Server, by default, only three days of data are retained in the change tables. Unless you are running very frequent syncs, we suggest increasing this retention so that in case of a failure in sync or if the sync is paused, there is still some bandwidth to start from the last point in incremental sync.\nThese settings can be changed using the stored procedure sys.sp_cdc_change_job as below:\n\n`text\n  -- we recommend 14400 minutes (10 days) as retention period\n  EXEC sp_cdc_change_job @job_type='cleanup', @retention = {minutes}`\n\nAfter making this change, a restart of the cleanup job is required:\n\n```text\n  EXEC sys.sp_cdc_stop_job @job_type = 'cleanup';\nEXEC sys.sp_cdc_start_job @job_type = 'cleanup';\n```\n5. Ensure the SQL Server Agent is running\n\nMSSQL uses the SQL Server Agent\n\nto run the jobs necessary\nfor CDC. It is therefore vital that the Agent is operational in order for to CDC to work effectively. You can check\nthe status of the SQL Server Agent as follows:\n`text\n  EXEC xp_servicecontrol 'QueryState', N'SQLServerAGENT';`\n\nIf you see something other than 'Running.' please follow\n\nthe Microsoft docs\nto start the service.\nConnection to MSSQL via an SSH Tunnel\nAirbyte has the ability to connect to a MSSQL instance via an SSH Tunnel. The reason you might want to do this because it is not possible (or against security policy) to connect to the database directly (e.g. it does not have a public IP address).\nWhen using an SSH tunnel, you are configuring Airbyte to connect to an intermediate server (a.k.a. a bastion sever) that does have direct access to the database. Airbyte connects to the bastion and then asks the bastion to connect directly to the server.\nUsing this feature requires additional configuration, when creating the source. We will talk through what each piece of configuration means.\n\nConfigure all fields for the source as you normally would, except `SSH Tunnel Method`.\n`SSH Tunnel Method` defaults to `No Tunnel` (meaning a direct connection). If you want to use an\n\nSSH Tunnel choose `SSH Key Authentication` or `Password Authentication`.\n\n\nChoose `Key Authentication` if you will be using an RSA private key as your secret for\nestablishing the SSH Tunnel (see below for more information on generating this key).\n\n\nChoose `Password Authentication` if you will be using a password as your secret for establishing\nthe SSH Tunnel.\n\n\n`SSH Tunnel Jump Server Host` refers to the intermediate (bastion) server that Airbyte will connect to. This should\n\n\nbe a hostname or an IP Address.\n\n`SSH Connection Port` is the port on the bastion server with which to make the SSH connection. The default port for\n\nSSH connections is `22`, so unless you have explicitly changed something, go with the default.\n\n`SSH Login Username` is the username that Airbyte should use when connection to the bastion server. This is NOT the\n\nMSSQL username.\n\nIf you are using `Password Authentication`, then `SSH Login Username` should be set to the\n\npassword of the User from the previous step. If you are using `SSH Key Authentication` leave this\nblank. Again, this is not the MSSQL password, but the password for the OS-user that Airbyte is\nusing to perform commands on the bastion.\n\nIf you are using `SSH Key Authentication`, then `SSH Private Key` should be set to the RSA\n\nprivate Key that you are using to create the SSH connection. This should be the full contents of\nthe key file starting with `-----BEGIN RSA PRIVATE KEY-----` and ending\nwith `-----END RSA PRIVATE KEY-----`.\nGenerating an SSH Key Pair\nThe connector expects an RSA key in PEM format. To generate this key:\n`text\nssh-keygen -t rsa -m PEM -f myuser_rsa`\nThis produces the private key in pem format, and the public key remains in the standard format used by the `authorized_keys` file on your bastion host. The public key should be added to your bastion host to whichever user you want to use with Airbyte. The private key is provided via copy-and-paste to the Airbyte connector configuration screen, so it may log in to the bastion.\nData type mapping\nMSSQL data types are mapped to the following data types when synchronizing data. You can check the test values examples here. If you can't find the data type you are looking for or have any problems feel free to add a new test!\n| MSSQL Type | Resulting Type | Notes |\n| :--- | :--- | :--- |\n| `bigint` | number |  |\n| `binary` | string |  |\n| `bit` | boolean |  |\n| `char` | string |  |\n| `date` | number |  |\n| `datetime` | string |  |\n| `datetime2` | string |  |\n| `datetimeoffset` | string |  |\n| `decimal` | number |  |\n| `int` | number |  |\n| `float` | number |  |\n| `geography` | string |  |\n| `geometry` | string |  |\n| `money` | number |  |\n| `numeric` | number |  |\n| `ntext` | string |  |\n| `nvarchar` | string |  |\n| `nvarchar(max)` | string |  |\n| `real` | number |  |\n| `smalldatetime` | string |  |\n| `smallint` | number |  |\n| `smallmoney` | number |  |\n| `sql_variant` | string |  |\n| `uniqueidentifier` | string |  |\n| `text` | string |  |\n| `time` | string |  |\n| `tinyint` | number |  |\n| `varbinary` | string |  |\n| `varchar` | string |  |\n| `varchar(max) COLLATE Latin1_General_100_CI_AI_SC_UTF8` | string |  |\n| `xml` | string |  |\nIf you do not see a type in this list, assume that it is coerced into a string. We are happy to take feedback on preferred mappings.\nUpgrading from 0.4.17 and older versions to 0.4.18 and newer versions\nThere is a backwards incompatible spec change between Microsoft SQL Source connector versions 0.4.17 and 0.4.18. As part of that spec change\n`replication_method` configuration parameter was changed to `object` from `string`.\nIn Microsoft SQL source connector versions 0.4.17 and older, `replication_method` configuration parameter was saved in the configuration database as follows:\n`\"replication_method\": \"STANDARD\"`\nStarting with version 0.4.18, `replication_method` configuration parameter is saved as follows:\n`\"replication_method\": {\n    \"method\": \"STANDARD\"\n}`\nAfter upgrading Microsoft SQL Source connector from 0.4.17 or older version to 0.4.18 or newer version you need to fix source configurations in the `actor` table\nin Airbyte database. To do so, you need to run two SQL queries. Follow the instructions in Airbyte documentation to\nrun SQL queries on Airbyte database.\nIf you have connections with Microsoft SQL Source using Standard replication method, run this SQL:\n`sql\nupdate public.actor set configuration =jsonb_set(configuration, '{replication_method}', '{\"method\": \"STANDARD\"}', true)  \nWHERE actor_definition_id ='b5ea17b1-f170-46dc-bc31-cc744ca984c1' AND (configuration->>'replication_method' = 'STANDARD');`\nIf you have connections with Microsoft SQL Source using Logicai Replication (CDC) method,  run this SQL:\n`sql\nupdate public.actor set configuration =jsonb_set(configuration, '{replication_method}', '{\"method\": \"CDC\"}', true)  \nWHERE actor_definition_id ='b5ea17b1-f170-46dc-bc31-cc744ca984c1' AND (configuration->>'replication_method' = 'CDC');`",
    "tag": "airbyte"
  },
  {
    "title": "Google Directory",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/google-directory.md",
    "content": "Google Directory\nOverview\nThe Directory source supports Full Refresh syncs. It uses Google Directory API.\nOutput schema\nThis Source is capable of syncing the following Streams:\n\nusers\ngroups\ngroup members\n\nData type mapping\n| Integration Type | Airbyte Type | Notes |\n| :--- | :--- | :--- |\n| `string` | `string` |  |\n| `number` | `number` |  |\n| `array` | `array` |  |\n| `object` | `object` |  |\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | No |  |\n| Replicate Incremental Deletes | Coming soon |  |\n| SSL connection | Yes |  |\n| Namespaces | No |  |\nPerformance considerations\nThis connector attempts to back off gracefully when it hits Directory API's rate limits. To find more information about limits, see Google Directory's Limits and Quotas documentation.\nGetting Started (Airbyte Cloud)\n\nClick `OAuth2.0 authorization` then `Authenticate your Google Directory account`.\nYou're done.\n\nGetting Started (Airbyte Open-Source)\nGoogle APIs use the OAuth 2.0 protocol for authentication and authorization. This connector supports Web server application and Service accounts scenarios. Therefore, there are 2 options of setting up authorization for this source:\n\nUse your Google account and authorize over Google's OAuth on connection setup. Select \"Default OAuth2.0 authorization\" from dropdown list.\nCreate service account specifically for Airbyte.\n\nService account requirements\n\nCredentials to a Google Service Account with delegated Domain Wide Authority\nEmail address of the workspace admin which created the Service Account\n\nCreate a Service Account with delegated domain wide authority\nFollow the Google Documentation for performing Domain Wide Delegation of Authority to create a Service account with delegated domain wide authority. This account must be created by an administrator of the Google Workspace. Please make sure to grant the following OAuth scopes to the service user:\n\n`https://www.googleapis.com/auth/admin.directory.user.readonly`\n`https://www.googleapis.com/auth/admin.directory.group.readonly`\n\nAt the end of this process, you should have JSON credentials to this Google Service Account.\nYou should now be ready to use the Google Directory connector in Airbyte.",
    "tag": "airbyte"
  },
  {
    "title": "ClickHouse",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/clickhouse.md",
    "content": "ClickHouse\nOverview\nThe ClickHouse source supports both Full Refresh and Incremental syncs. You can choose if this connector will copy only the new or updated data, or all rows in the tables and columns you set up for replication, every time a sync is run.\nThis Clickhouse source connector is built on top of the source-jdbc code base and is configured to rely on JDBC v0.3.1 standard drivers provided by ClickHouse here as described in ClickHouse documentation here.\nResulting schema\nThe ClickHouse source does not alter the schema present in your warehouse. Depending on the destination connected to this source, however, the schema may be altered. See the destination's documentation for more details.\nFeatures\n| Feature | Supported | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | Yes |  |\n| Replicate Incremental Deletes | Coming soon |  |\n| Logical Replication (WAL) | Coming soon |  |\n| SSL Support | Yes |  |\n| SSH Tunnel Connection | Yes |  |\n| Namespaces | Yes | Enabled by default |\nGetting started\nRequirements\n\nClickHouse Server `21.3.10.1` or later.\nCreate a dedicated read-only Airbyte user with access to all tables needed for replication\n\nSetup guide\n1. Make sure your database is accessible from the machine running Airbyte\nThis is dependent on your networking setup. The easiest way to verify if Airbyte is able to connect to your ClickHouse instance is via the check connection tool in the UI.\n2. Create a dedicated read-only user with access to the relevant tables (Recommended but optional)\nThis step is optional but highly recommended to allow for better permission control and auditing. Alternatively, you can use Airbyte with an existing user in your database.\nTo create a dedicated database user, run the following commands against your database:\n`sql\nCREATE USER 'airbyte'@'%' IDENTIFIED BY 'your_password_here';`\nThen give it access to the relevant schema:\n`sql\nGRANT SELECT ON <database name>.* TO 'airbyte'@'%';`\nYou can limit this grant down to specific tables instead of the whole database. Note that to replicate data from multiple ClickHouse databases, you can re-run the command above to grant access to all the relevant schemas, but you'll need to set up multiple sources connecting to the same db on multiple schemas.\nYour database user should now be ready for use with Airbyte.\nConnection via SSH Tunnel\nAirbyte has the ability to connect to a Clickhouse instance via an SSH Tunnel. The reason you might want to do this because it is not possible (or against security policy) to connect to the database directly (e.g. it does not have a public IP address).\nWhen using an SSH tunnel, you are configuring Airbyte to connect to an intermediate server (a.k.a. a bastion sever) that does have direct access to the database. Airbyte connects to the bastion and then asks the bastion to connect directly to the server.\nUsing this feature requires additional configuration, when creating the source. We will talk through what each piece of configuration means.\n\nConfigure all fields for the source as you normally would, except `SSH Tunnel Method`.\n`SSH Tunnel Method` defaults to `No Tunnel` (meaning a direct connection). If you want to use an SSH Tunnel choose `SSH Key Authentication` or `Password Authentication`.\nChoose `Key Authentication` if you will be using an RSA private key as your secret for establishing the SSH Tunnel (see below for more information on generating this key).\nChoose `Password Authentication` if you will be using a password as your secret for establishing the SSH Tunnel.\n`SSH Tunnel Jump Server Host` refers to the intermediate (bastion) server that Airbyte will connect to. This should be a hostname or an IP Address.\n`SSH Connection Port` is the port on the bastion server with which to make the SSH connection. The default port for SSH connections is `22`, so unless you have explicitly changed something, go with the default.\n`SSH Login Username` is the username that Airbyte should use when connection to the bastion server. This is NOT the Clickhouse username.\nIf you are using `Password Authentication`, then `SSH Login Username` should be set to the password of the User from the previous step. If you are using `SSH Key Authentication` leave this blank. Again, this is not the Clickhouse password, but the password for the OS-user that Airbyte is using to perform commands on the bastion.\nIf you are using `SSH Key Authentication`, then `SSH Private Key` should be set to the RSA Private Key that you are using to create the SSH connection. This should be the full contents of the key file starting with `-----BEGIN RSA PRIVATE KEY-----` and ending with `-----END RSA PRIVATE KEY-----`.\n",
    "tag": "airbyte"
  },
  {
    "title": "Shopify",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/shopify.md",
    "content": "\ndescription: >-\n  Shopify is a proprietary e-commerce platform for online stores and retail point-of-sale systems.\n\nShopify\n:::note\nOur Shopify Source Connector does not support OAuth at this time due to limitations outside of our control. If OAuth for Shopify is critical to your business, please reach out to us to discuss how we may be able to partner on this effort.\n:::\nSync overview\nThe Shopify source supports both Full Refresh and Incremental syncs. You can choose if this connector will copy only the new or updated data, or all rows in the tables and columns you set up for replication, every time a sync is run.\nThis source can sync data for the Shopify REST API and the Shopify GraphQl API.\nThis Source Connector is based on a Airbyte CDK.\nTroubleshooting\nCheck out common troubleshooting issues for the Shopify source connector on our Discourse here.\nOutput schema\nThis Source is capable of syncing the following core Streams:\n\nAbandoned Checkouts\nCollects\nCustom Collections\nCustomers\nDraft Orders\nDiscount Codes\nMetafields\nOrders\nOrders Refunds\nOrders Risks\nProducts\nProducts (GraphQL)\nTransactions\nBalance Transactions\nPages\nPrice Rules\nLocations\nInventoryItems\nInventoryLevels\nFulfillment Orders\nFulfillments\nShop\n\nNOTE\nFor better experience with `Incremental Refresh` the following is recommended:\n\n`Order Refunds`, `Order Risks`, `Transactions` should be synced along with `Orders` stream.\n`Discount Codes` should be synced along with `Price Rules` stream.\n\nIf child streams are synced alone from the parent stream - the full sync will take place, and the records are filtered out afterwards.\nData type mapping\n| Integration Type | Airbyte Type |\n| :--- | :--- |\n| `string` | `string` |\n| `number` | `number` |\n| `array` | `array` |\n| `object` | `object` |\n| `boolean` | `boolean` |\nFeatures\n| Feature | Supported?(Yes/No) |\n| :--- | :--- |\n| Full Refresh Sync | Yes |\n| Incremental - Append Sync | Yes |\n| Namespaces | No |\nGetting started\nThis connector support both: `OAuth 2.0` and `API PASSWORD` (for private applications) athentication methods.\nConnect using `API PASSWORD` option\n\nGo to `https://YOURSTORE.myshopify.com/admin/apps/private`\nEnable private development if it isn't enabled.\nCreate a private application.\nSelect the resources you want to allow access to. Airbyte only needs read-level access.\nNote: The UI will show all possible data sources and will show errors when syncing if it doesn't have permissions to access a resource.\nThe password under the `Admin API` section is what you'll use as the `API PASSWORD` for the integration.\nYou're ready to set up Shopify in Airbyte!\n\nOutput Streams Schemas\nThis Source is capable of syncing the following core Streams:\n\nArticles\nBlogs\nAbandoned Checkouts\nCollects\nCollections\nCustom Collections\nSmart Collections\nCustomers\nDraft Orders\nDiscount Codes\nMetafields\nOrders\nOrders Refunds\nOrders Risks\nProducts\nProducts (GraphQL)\nProduct Images\nProduct Variants\nTransactions\nTender Transactions\nPages\nPrice Rules\nLocations\nInventoryItems\nInventoryLevels\nFulfillment Orders\nFulfillments\nShop\n\nNotes\nFor better experience with `Incremental Refresh` the following is recommended:\n\n`Order Refunds`, `Order Risks`, `Transactions` should be synced along with `Orders` stream.\n`Discount Codes` should be synced along with `Price Rules` stream.\n\nIf child streams are synced alone from the parent stream - the full sync will take place, and the records are filtered out afterwards.\nPerformance considerations\nShopify has some rate limit restrictions. Typically, there should not be issues with throttling or exceeding the rate limits but in some edge cases, user can receive the warning message as follows:\n`text\n\"Caught retryable error '<some_error> or null' after <some_number> tries. Waiting <some_number> seconds then retrying...\"`\nThis is expected when the connector hits the 429 - Rate Limit Exceeded HTTP Error. With given error message the sync operation is still goes on, but will require more time to finish.",
    "tag": "airbyte"
  },
  {
    "title": "Asana",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/asana.md",
    "content": "Asana\nThis page contains the setup guide and reference information for the Asana source connector.\nPrerequisites\nPlease follow these steps to obtain Personal Access Token for your account.\nSetup guide\nStep 1: Set up the Asana connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nSet the name for your source\nEnter your `personal_access_token`\nClick Set up source\n\n\u26a0\ufe0f For the moment, oAuth login is disabled for Asana on Airbyte Cloud.\nFor Airbyte OSS:\n\nNavigate to the Airbyte Open Source dashboard\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nSet the name for your source\nEnter your `personal_access_token`\nClick Set up source\n\nSupported sync modes\nThe Asana source connector supports the following sync modes:\n| Feature           | Supported? |\n| :---------------- | :--------- |\n| Full Refresh Sync | Yes        |\n| Incremental Sync  | No         |\n| Namespaces        | No         |\nSupported Streams\n\nCustom fields\nProjects\nSections\nStories\nTags\nTasks\nTeams\nTeam Memberships\nUsers\nWorkspaces\n\nPerformance considerations\nThe connector is restricted by normal Asana requests limitation.\nData type map\n| Integration Type         | Airbyte Type |\n| :----------------------- | :----------- |\n| `string`                 | `string`     |\n| `int`, `float`, `number` | `number`     |\n| `date`                   | `date`       |\n| `datetime`               | `datetime`   |\n| `array`                  | `array`      |\n| `object`                 | `object`     |",
    "tag": "airbyte"
  },
  {
    "title": "VictorOps",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/victorops.md",
    "content": "VictorOps\nOverview\nThe VictorOps (now named Splunk On-Call) source is maintained by Faros\nAI.\nPlease file any support requests on that repo to minimize response time from the\nmaintainers. The source supports both Full Refresh and Incremental syncs. You\ncan choose if this source will copy only the new or updated data, or all rows in\nthe tables and columns you set up for replication, every time a sync is run.\nOutput schema\nSeveral output streams are available from this source:\n\nIncidents (Incremental)\nTeams\nUsers\n\nIf there are more endpoints you'd like Faros AI to support, please create an\nissue.\nFeatures\n| Feature | Supported? |\n| :--- | :--- |\n| Full Refresh Sync | Yes |\n| Incremental Sync | Yes |\n| SSL connection | Yes |\n| Namespaces | No |\nPerformance considerations\nThe VictorOps source should not run into VictorOps API limitations under normal\nusage, however your VictorOps account may be limited to a total number of API\ncalls per month.  Please create an\nissue if you see any\nrate limit issues that are not automatically retried successfully.\nGetting started\nRequirements\n\nVictorOps API ID\nVictorOps API Key\n\nPlease follow the their documentation for generating a VictorOps API\nKey.",
    "tag": "airbyte"
  },
  {
    "title": "TiDB",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/tidb.md",
    "content": "TiDB\nOverview\nTiDB (/\u2019ta\u026adi\u02d0bi:/, \"Ti\" stands for Titanium) is an open-source, distributed, NewSQL database that supports Hybrid Transactional and Analytical Processing (HTAP) workloads. It is MySQL compatible and features horizontal scalability, strong consistency, and high availability. TiDB can be deployed on-premise or in-cloud.\nThe TiDB source supports both Full Refresh and Incremental syncs. You can choose if this connector will copy only the new or updated data, or all rows in the tables and columns you set up for replication, every time a sync is run.\nResulting schema\nThe TiDB source does not alter the schema present in your database. Depending on the destination connected to this source, however, the schema may be altered. See the destination's documentation for more details.\nFeatures\n| Feature                       | Supported | Notes |\n| :---------------------------- | :-------- | :---- |\n| Full Refresh Sync             | Yes       |       |\n| Incremental - Append Sync     | Yes       |       |\n| Replicate Incremental Deletes | Yes       |       |\n| Change Data Capture           | No        |       |\n| SSL Support                   | Yes       |       |\n| SSH Tunnel Connection         | Yes       |       |\nGetting started\nRequirements\n\nTiDB `v4.0` or above\nAllow connections from Airbyte to your TiDB database (if they exist in separate VPCs)\n(Optional) Create a dedicated read-only Airbyte user with access to all tables needed for replication\n\nNote: When connecting to TiDB Cloud with TLS enabled, you need to specify TLS protocol, such as `enabledTLSProtocols=TLSv1.2` or `enabledTLSProtocols=TLSv1.3` in the JDBC parameters.\nSetup guide\n1. Make sure your database is accessible from the machine running Airbyte\nThis is dependent on your networking setup. The easiest way to verify if Airbyte is able to connect to your TiDB instance is via the check connection tool in the UI.\n2. Create a dedicated read-only user with access to the relevant tables (Recommended but optional)\nThis step is optional but highly recommended to allow for better permission control and auditing. Alternatively, you can use Airbyte with an existing user in your database.\nTo create a dedicated database user, run the following commands against your database:\n`sql\nCREATE USER 'airbyte'@'%' IDENTIFIED BY 'your_password_here';`\nThen give it access to the relevant database:\n`sql\nGRANT SELECT ON <database name>.* TO 'airbyte'@'%';`\n3. That's it!\nYour database user should now be ready for use with Airbyte.\nConnection via SSH Tunnel\nAirbyte has the ability to connect to a TiDB instance via an SSH Tunnel. The reason you might want to do this because it is not possible (or against security policy) to connect to the database directly (e.g. it does not have a public IP address).\nWhen using an SSH tunnel, you are configuring Airbyte to connect to an intermediate server (a.k.a. a bastion sever) that does have direct access to the database. Airbyte connects to the bastion and then asks the bastion to connect directly to the server.\nUsing this feature requires additional configuration, when creating the source. We will talk through what each piece of configuration means.\n\nConfigure all fields for the source as you normally would, except `SSH Tunnel Method`.\n`SSH Tunnel Method` defaults to `No Tunnel` (meaning a direct connection). If you want to use an SSH Tunnel choose `SSH Key Authentication` or `Password Authentication`.\nChoose `Key Authentication` if you will be using an RSA private key as your secret for establishing the SSH Tunnel (see below for more information on generating this key).\nChoose `Password Authentication` if you will be using a password as your secret for establishing the SSH Tunnel.\n\n\n`SSH Tunnel Jump Server Host` refers to the intermediate (bastion) server that Airbyte will connect to. This should be a hostname or an IP Address.\n`SSH Connection Port` is the port on the bastion server with which to make the SSH connection. The default port for SSH connections is `22`, so unless you have explicitly changed something, go with the default.\n`SSH Login Username` is the username that Airbyte should use when connection to the bastion server. This is NOT the TiDB username.\nIf you are using `Password Authentication`, then `SSH Login Username` should be set to the password of the User from the previous step. If you are using `SSH Key Authentication` TiDB password, but the password for the OS-user that Airbyte is using to perform commands on the bastion.\nIf you are using `SSH Key Authentication`, then `SSH Private Key` should be set to the RSA Private Key that you are using to create the SSH connection. This should be the full contents of the key file starting with `-----BEGIN RSA PRIVATE KEY-----` and ending with `-----END RSA PRIVATE KEY-----`.\n\nData type mapping\nTiDB data types are mapped to the following data types when synchronizing data:\n| TiDB Type                                 | Resulting Type         | Notes                                                        |\n| :---------------------------------------- |:-----------------------| :----------------------------------------------------------- |\n| `bit(1)`                                  | boolean                |                                                              |\n| `bit(>1)`                                 | base64 binary string   |                                                              |\n| `boolean`                                 | boolean                |                                                              |\n| `tinyint(1)`                              | boolean                |                                                              |\n| `tinyint`                                 | number                 |                                                              |\n| `smallint`                                | number                 |                                                              |\n| `mediumint`                               | number                 |                                                              |\n| `int`                                     | number                 |                                                              |\n| `bigint`                                  | number                 |                                                              |\n| `float`                                   | number                 |                                                              |\n| `double`                                  | number                 |                                                              |\n| `decimal`                                 | number                 |                                                              |\n| `binary`                                  | base64 binary string   |                                                              |\n| `blob`                                    | base64 binary string   |                                                              |\n| `date`                                    | string                 | ISO 8601 date string. ZERO-DATE value will be converted to NULL. If column is mandatory, convert to EPOCH. |\n| `datetime`, `timestamp`                   | string                 | ISO 8601 datetime string. ZERO-DATE value will be converted to NULL. If column is mandatory, convert to EPOCH. |\n| `time`                                    | string                 | ISO 8601 time string. Values are in range between 00:00:00 and 23:59:59. |\n| `year`                                    | year string            | Doc     |\n| `char`, `varchar` with non-binary charset | string                 |                                                              |\n| `char`, `varchar` with binary charset     | base64 binary string   |                                                              |\n| `tinyblob`                                | base64 binary string   |                                                              |\n| `blob`                                    | base64 binary string   |                                                              |\n| `mediumblob`                              | base64 binary string   |                                                              |\n| `longblob`                                | base64 binary string   |                                                              |\n| `binary`                                  | base64 binary string   |                                                              |\n| `varbinary`                               | base64 binary string   |                                                              |\n| `tinytext`                                | string                 |                                                              |\n| `text`                                    | string                 |                                                              |\n| `mediumtext`                              | string                 |                                                              |\n| `longtext`                                | string                 |                                                              |\n| `json`                                    | serialized json string | E.g. `{\"a\": 10, \"b\": 15}`                                    |\n| `enum`                                    | string                 |                                                              |\n| `set`                                     | string                 | E.g. `blue,green,yellow`                                     |\nNote: arrays for all the above types as well as custom types are supported, although they may be de-nested depending on the destination.\nExternal resources\nNow that you have set up the TiDB source connector, check out the following TiDB tutorial:\n\nUsing Airbyte to Migrate Data from TiDB Cloud to Snowflake\n",
    "tag": "airbyte"
  },
  {
    "title": "Zendesk Support",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/zendesk-support.md",
    "content": "Zendesk Support\nThis page guides you through setting up the Zendesk Support source connector.\nPrerequisites\n\nLocate your Zendesk subdomain found in your account URL. For example, if your account URL is `https://{MY_SUBDOMAIN}.zendesk.com/`, then `MY_SUBDOMAIN` is your subdomain.\n(For Airbyte Open Source) Find the email address associated with your Zendesk account. Also, generate an API token for the account.\n\nSet up the Zendesk Support source connector\n\nLog into your Airbyte Cloud or Airbyte Open Source account.\nClick Sources and then click + New source. \nOn the Set up the source page, select Zendesk Support from the Source type dropdown.\nEnter a name for your source.\nFor Subdomain, enter your Zendesk subdomain.\nFor Start date, enter the date in `YYYY-MM-DDTHH:mm:ssZ` format. The data added on and after this date will be replicated. If this field is blank, Airbyte will replicate all data.\nYou can use OAuth or an API key to authenticate your Zendesk Support account. We recommend using OAuth for Airbyte Cloud and an API key for Airbyte Open Source.\nTo authenticate using OAuth for Airbyte Cloud, click Authenticate your Zendesk Support account to sign in with Zendesk Support and authorize your account. \nTo authenticate using an API key for Airbyte Open Source, select API key from the Authentication dropdown and enter your API key. Enter the Email associated with your Zendesk Support account.   \n\n\nClick Set up source.\n\nSupported sync modes\nThe Zendesk Support source connector supports the following sync modes:\n - Full Refresh - overwrite\n - Full Refresh - append\n - Incremental - append\nSupported streams\nThe Zendesk Support source connector supports the following streams:\n\nBrands\nCustom Roles\nGroups\nGroup Memberships\nMacros\nOrganizations\nSatisfaction Ratings\nSchedules\nSLA Policies\nTags\nTickets\nTicket Audits\nTicket Comments\nTicket Fields\nTicket Forms\nTicket Metrics\nTicket Metric Events\nUsers\n\nPerformance considerations\nThe connector is restricted by normal Zendesk requests limitation.\nThe Zendesk connector ideally should not run into Zendesk API limitations under normal usage. Create an issue if you see any rate limit issues that are not automatically retried successfully.",
    "tag": "airbyte"
  },
  {
    "title": "Typeform",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/typeform.md",
    "content": "Typeform\nThis page guides you through the process of setting up the Typeform source connector.\nPrerequisites\n\ntoken - The Typeform API key token.\nstart_date - Date to start fetching Responses stream data from.\nform_ids (Optional) - List of Form Ids to sync. If not passed - sync all account`s forms.\n\nSetup guide\nStep 1: Set up Typeform\nTo get the API token for your application follow this steps\n\nLog in to your account at Typeform.\nIn the upper-right corner, in the drop-down menu next to your profile photo, click My Account.\nIn the left menu, click Personal tokens.\nClick Generate a new token.\nIn the Token name field, type a name for the token to help you identify it.\nChoose needed scopes (API actions this token can perform - or permissions it has). See here for more details on scopes.\nClick Generate token.\n\nStep 2: Set up the source connector in Airbyte\n\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click + new source.\nOn the source setup page, select Typeform from the Source type dropdown and enter a name for this connector.\nFill-in 'API Token' and 'Start Date'\nclick `Set up source`.\n\n\n\nFor Airbyte Open Source:\n\nGo to local Airbyte page.\nIn the left navigation bar, click Sources. In the top-right corner, click + new source.\nOn the Set up the source page, enter the name for the connector and select Tiktok Marketing from the Source type dropdown.\nFill-in 'API Token' and 'Start Date'\nclick `Set up source`.\n\n\nSupported streams and sync modes\n| Stream     | Key         | Incremental | API Link                                                                    |\n|:-----------|-------------|:------------|-----------------------------------------------------------------------------|\n| Forms      | id          | No          | https://developer.typeform.com/create/reference/retrieve-form/              |\n| Responses  | response_id | Yes         | https://developer.typeform.com/responses/reference/retrieve-responses       |\n| Webhooks   | id          | No          | https://developer.typeform.com/webhooks/reference/retrieve-webhooks/        |\n| Workspaces | id          | No          | https://developer.typeform.com/create/reference/retrieve-workspaces/        |\n| Images     | id          | No          | https://developer.typeform.com/create/reference/retrieve-images-collection/ |\n| Themes     | id          | No          | https://developer.typeform.com/create/reference/retrieve-themes/            |\nPerformance considerations\nTypeform API page size limit per source:\n\nForms - 200\nResponses - 1000\n\nConnector performs additional API call to fetch all possible `form ids` on an account using retrieve forms endpoint\nAPI rate limits (2 requests per second): https://developer.typeform.com/get-started/#rate-limits",
    "tag": "airbyte"
  },
  {
    "title": "Orbit",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/orbit.md",
    "content": "Orbit\nSync overview\nThis source can sync data for the Orbit API. It currently only supports Full Refresh syncs. \nOutput schema\nThis Source is capable of syncing the following core Streams:\n\nMembers \nWorkspaces\n\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | No |  |\n| Namespaces | No |  |\n| Pagination | Yes |  |\nPerformance considerations / Rate Limiting\nThe Orbit API is rate limited at 120 requests per IP per minute as stated here.\nPlease create an issue if you see any rate limit issues that are not automatically retried successfully.\nGetting started\nRequirements\n\nOrbit API key - This can either be a workspace-tied key or a general personal key.\n\nSetup guide\nThe Orbit API Key should be available to you immediately as an Orbit user.\n\nHead to app.orbit.love and login to your account.\nGo to the Settings tab on the right sidebar.\nNavigate to API Tokens.\nClick New API Token in the top right if one doesn't already exist.\n",
    "tag": "airbyte"
  },
  {
    "title": "Okta",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/okta.md",
    "content": "Okta\nOkta is the complete identity solution for all your apps and people that\u2019s universal, reliable, and easy\nPrerequisites\n\nCreated Okta account with added application on Add Application Page page. (change okta-domain to you'r domain received after complete registration)\n\nAirbyte Open Source\n\nName\nOkta-Domain\nStart Date\nPersonal Api Token (look here to find it)\n\nAirbyte Cloud\n\nName\nStart Date\nClient ID (received when application was added).\nClient Secret (received when application was added).\nRefresh Token (received when application was added)\n\nSetup guide\nStep 1: Set up Okta\n\nCreate account on Okta by following link signup\nConfirm your Email\nChoose authorization method (Application or SMS)\nAdd application in your Dashboard\n\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click + new source.\nOn the source setup page, select Okta from the Source type dropdown and enter a name for this connector.\nAdd Name\nAdd Okta-Domain\nAdd Start date (defaults to 7 days if no date is included)\nChoose the method of authentication\nIf you select Token authentication - fill the field  Personal Api Token \nIf you select OAuth2.0 authorization - fill the fields Client ID, Client Secret, Refresh Token\nClick `Set up source`.\n\nFor Airbyte Open Source:\n\nGo to local Airbyte page.\nUse API token from requirements and Okta domain. \nGo to local Airbyte page.\nIn the left navigation bar, click Sources. In the top-right corner, click + new source. \nOn the Set up the source page select Okta from the Source type dropdown. \nAdd Name\nAdd Okta-Domain\nAdd Start date\nPaste all data to required fields fill the fields Client ID, Client Secret, Refresh Token\nClick `Set up source`.\n\nSupported sync modes\nThe Okta source connector supports the following sync modes:\n - Full Refresh\n - Incremental\nSupported Streams\n\nUsers\nUser Role Assignments\nGroups\nGroup Members\nGroup Role Assignments\nSystem Log\nCustom Roles\nPermissions\nResource Sets\n\nPerformance considerations\nThe connector is restricted by normal Okta requests limitation.",
    "tag": "airbyte"
  },
  {
    "title": "Mailjet - SMS API",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/mailjet-sms.md",
    "content": "Mailjet - SMS API\nSync overview\nThis source can sync data from the Mailjet SMS API. At present this connector only supports full refresh syncs meaning that each time you use the connector it will sync all available records from scratch. Please use cautiously if you expect your API to have a lot of records.\nThis Source Supports the Following Streams\n\nSMS\n\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | No |  |\nPerformance considerations\nMailjet APIs are under rate limits for the number of API calls allowed per API keys per second. If you reach a rate limit, API will return a 429 HTTP error code. See here\nGetting started\nRequirements\n\nMailjet SMS TOKEN\n",
    "tag": "airbyte"
  },
  {
    "title": "Orb",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/orb.md",
    "content": "Orb\nOverview\nThe Orb source supports both Full Refresh and Incremental syncs. For incremental syncs, this source\nwill only read and output new records based on their `created_at` timestamp.\nOutput schema\nThis Source is capable of syncing the following core resources, each of which has a separate Stream. Note that all of the streams are incremental:\n\nSubscriptions\nPlans\nCustomers\nCredits Ledger Entries\nSubscription Usage\n\nAs a caveat, the Credits Ledger Entries must read all Customers for an incremental sync, but will only incrementally return new ledger entries for each customers.\nSimilarily, the Subscription Usage stream must read all Subscriptions for an incremental sync (and all Plans if using the optional `subscription_usage_grouping_key`), but will only incrementally return new usage entries for each subscription.\nNote on Incremental Syncs\nThe Orb API does not allow querying objects based on an `updated_at` time. Therefore, this connector uses the `created_at` field (or the `timeframe_start` field in the Subscription Usage stream) to query for new data since the last sync.\nIn order to capture data that has been updated after creation, please run a periodic Full Refresh.\nFeatures\n| Feature | Supported? |\n| :--- | :--- |\n| Full Refresh Sync | Yes |\n| Incremental - Append Sync | Yes |\n| Incremental - Dedupe Sync | Yes |\n| SSL connection | Yes |\nPerformance considerations\nThe Orb connector should not run into Orb API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.\nGetting started\nRequirements\n\nOrb Account\nOrb API Key\n\nSetup guide\nPlease reach out to the Orb team at team@withorb.com to request\nan Orb Account and API Key.",
    "tag": "airbyte"
  },
  {
    "title": "Mongo DB",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/mongodb-v2.md",
    "content": "Mongo DB\nThe MongoDB source allows to sync data from MongoDb. Source supports Full Refresh and Incremental sync strategies.\nResulting schema\nMongoDB does not have anything like table definition, thus we have to define column types from actual attributes and their values. Discover phase have two steps:\nStep 1. Find all unique properties\nConnector select 10k documents to collect all distinct field.\nStep 2. Determine property types\nFor each property found, connector determines its type, if all the selected values have the same type - connector will set appropriate type to the property. In all other cases connector will fallback to `string` type.\nFeatures\n| Feature | Supported |\n| :--- | :--- |\n| Full Refresh Sync | Yes |\n| Incremental - Append Sync | Yes |\n| Replicate Incremental Deletes | No |\n| Namespaces | No |\nFull Refresh sync\nWorks as usual full refresh sync.\nIncremental sync\nCursor field can not be nested. Currently only top level document properties are supported.\nCursor should never be blank. In case cursor is blank - the incremental sync results might be unpredictable and will totally rely on MongoDB comparison algorithm.\nOnly `datetime` and `number` cursor types are supported. Cursor type is determined based on the cursor field name:\n\n`datetime` - if cursor field name contains a string from: `time`, `date`, `_at`, `timestamp`, `ts`\n`number` - otherwise\n\nGetting started\nThis guide describes in details how you can configure MongoDB for integration with Airbyte.\nCreate users\nRun `mongo` shell, switch to `admin` database and create a `READ_ONLY_USER`. `READ_ONLY_USER` will be used for Airbyte integration. Please make sure that user has read-only privileges.\n`javascript\nmongo\nuse admin;\ndb.createUser({user: \"READ_ONLY_USER\", pwd: \"READ_ONLY_PASSWORD\", roles: [{role: \"read\", db: \"TARGET_DATABASE\"}]})`\nMake sure the user have appropriate access levels, a user with higher access levels may throw an exception.\nEnable MongoDB authentication\nOpen `/etc/mongod.conf` and add/replace specific keys:\n```yaml\nnet:\n  bindIp: 0.0.0.0\nsecurity:\n  authorization: enabled\n```\nBinding to `0.0.0.0` will allow to connect to database from any IP address.\nThe last line will enable MongoDB security. Now only authenticated users will be able to access the database.\nConfigure firewall\nMake sure that MongoDB is accessible from external servers. Specific commands will depend on the firewall you are using (UFW/iptables/AWS/etc). Please refer to appropriate documentation.\nYour `READ_ONLY_USER` should now be ready for use with Airbyte.\nTLS/SSL on a Connection\nIt is recommended to use encrypted connection. Connection with TLS/SSL security protocol for MongoDb Atlas Cluster and Replica Set instances is enabled by default. To enable TSL/SSL connection with Standalone MongoDb instance, please refer to MongoDb Documentation.\n\u0421onfiguration Parameters\n\nDatabase: database name\nAuthentication Source: specifies the database that the supplied credentials should be validated against. Defaults to `admin`.\nUser: username to use when connecting\nPassword: used to authenticate the user\nStandalone MongoDb instance\nHost: URL of the database\nPort: Port to use for connecting to the database\nTLS: indicates whether to create encrypted connection\nReplica Set\nServer addresses: the members of a replica set\nReplica Set: A replica set name\nMongoDb Atlas Cluster\nCluster URL: URL of a cluster to connect to\n\nFor more information regarding configuration parameters, please see MongoDb Documentation.",
    "tag": "airbyte"
  },
  {
    "title": "SmartEngage",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/smartengage.md",
    "content": "SmartEngage\nSync overview\nThis source can sync data from the SmartEngage API. At present this connector only supports full refresh syncs meaning that each time you use the connector it will sync all available records from scratch. Please use cautiously if you expect your API to have a lot of records.\nThis Source Supports the Following Streams\n\navatars\ntags\ncustom_fields\nsequences\n\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | No |  |\nGetting started\nRequirements\n\nSmartEngage API Key\n",
    "tag": "airbyte"
  },
  {
    "title": "Babelforce",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/babelforce.md",
    "content": "Babelforce\nOverview\nThe Babelforce source supports Full Refresh as well as Incremental syncs. \nFull Refresh sync means every time a sync is run, Airbyte will copy all rows in the tables and columns you set up for replication into the destination in a new table.\nIncremental syn means only changed resources are copied from Babelformce. For the first run, it will be a Full Refresh sync.\nOutput schema\nSeveral output streams are available from this source:\n\nCalls\n\nIf there are more endpoints you'd like Airbyte to support, please create an issue.\nFeatures\n| Feature | Supported? |\n| :--- | :--- |\n| Full Refresh Sync | Yes |\n| Incremental Sync | Yes |\n| Replicate Incremental Deletes | Coming soon |\n| SSL connection | Yes |\n| Namespaces | No |\nPerformance considerations\nThere are no performance consideration in the current version.\nGetting started\nRequirements\n\nRegion/environment as listed in the `Regions & environments` section here\nBabelforce access key ID \nBabelforce access token\n(Optional) start date from when the import starts in epoch Unix timestamp\n\nSetup guide\nGenerate a API access key ID and token using the Babelforce documentation",
    "tag": "airbyte"
  },
  {
    "title": "Zoom",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/zoom.md",
    "content": "Zoom\nOverview\nThe following connector allows airbyte users to fetch various meetings & webinar data points from the Zoom source. This connector is built entirely using the low-code CDK.\nPlease note that currently, it only supports Full Refresh syncs. That is, every time a sync is run, Airbyte will copy all rows in the tables and columns you set up for replication into the destination in a new table.\nOutput schema\nCurrently this source supports the following output streams/endpoints from Zoom:\n\nUsers\nMeetings\nMeeting Registrants\nMeeting Polls\nMeeting Poll Results\nMeeting Questions\nWebinars\nWebinar Panelists\nWebinar Registrants\nWebinar Absentees\nWebinar Polls\nWebinar Poll Results\nWebinar Questions\nWebinar Tracking Sources\nWebinar Q&A Results\nReport Meetings\nReport Meeting Participants\nReport Webinars\nReport Webinar Participants\n\nIf there are more endpoints you'd like Airbyte to support, please create an issue.\nFeatures\n| Feature | Supported? |\n| :--- | :--- |\n| Full Refresh Sync | Yes |\n| Incremental Sync | Coming soon |\n| Replicate Incremental Deletes | Coming soon |\n| SSL connection | Yes |\n| Namespaces | No |\nPerformance considerations\nMost of the endpoints this connector access is restricted by standard Zoom requests limitation, with a few exceptions. For more info, please check zoom API documentation. We\u2019ve added appropriate retries if we hit the rate-limiting threshold.\nPlease create an issue if you see any rate limit issues that are not automatically retried successfully.\nGetting started\nRequirements\n\nZoom JWT Token\n\nSetup guide\nPlease read How to generate your JWT Token.",
    "tag": "airbyte"
  },
  {
    "title": "Lemlist",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/lemlist.md",
    "content": "Lemlist\nSync overview\nThe Lemlist source supports Full Refresh syncs only.\nThis source can sync data for the Lemlist API.\nOutput schema\nThis Source is capable of syncing the following core Streams:\n\nTeam `api.lemlist.com/api/team`\nCampaigns `api.lemlist.com/api/campaigns`\nActivities `api.lemlist.com/api/activities`\nUnsubscribes `api.lemlist.com/api/unsubscribes`\n\nFeatures\n| Feature                   | Supported?(Yes/No) | Notes |\n| :------------------------ | :------------------- | :---- |\n| Full Refresh Sync         | Yes                  |       |\n| Incremental - Append Sync | No                   |       |\n| Namespaces                | No                   |       |\nPerformance considerations\nThe Lemlist connector should not run into Lemlist API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.\nGetting started\nRequirements\n\nLemlist API key\n",
    "tag": "airbyte"
  },
  {
    "title": "Pivotal Tracker",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/pivotal-tracker.md",
    "content": "Pivotal Tracker\nOverview\nThe Pivotal Tracker source supports Full Refresh syncs. It supports pulling from :\n\nActivity\nEpics\nLabels\nProject Membership\nProjects\nReleases\nStories\n\nOutput schema\nOutput streams:\n\nActivity\nEpics\nLabels\nProject Membership\nProjects\nReleases\nStories\n\nFeatures\n| Feature                       | Supported?  |\n| :---------------------------- | :---------- |\n| Full Refresh Sync             | Yes         |\n| Incremental - Append Sync     | Coming soon |\n| Replicate Incremental Deletes | Coming soon |\n| SSL connection                | Yes         |\n| Namespaces                    | No          |\nPerformance considerations\nThe Pivotal Trakcer connector should not run into Stripe API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.\nGetting started\nRequirements\n\nPivotal Trakcer API Token\n\nSetup guide to create the API Token\nAccess your profile here go down and click in Create New Token.\nUse this to pull data from Pivotal Tracker.",
    "tag": "airbyte"
  },
  {
    "title": "Amazon Ads",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/amazon-ads.md",
    "content": "Amazon Ads\nThis page contains the setup guide and reference information for the Amazon Ads source connector.\nPrerequisites\n\nClient ID\nClient Secret\nRefresh Token\nRegion\nStart Date (Optional)\nProfile IDs (Optional)\n\nSetup guide\nStep 1: Set up Amazon Ads\nCreate an Amazon user with access to Amazon Ads account.\n\nFor Airbyte Open Source:\nTo use the Amazon Ads API, you must first complete the onboarding process. The onboarding process has several steps and may take several days to complete. After completing all steps you will have to get Amazon client application `Client ID`, `Client Secret` and `Refresh Token`.\n\nStep 2: Set up the Amazon Ads connector in Airbyte\n\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click + new source.\nOn the source setup page, select Amazon Ads from the Source type dropdown and enter a name for this connector.\nClick `Authenticate your Amazon Ads account`.\nLog in and Authorize to the Amazon account.\nSelect Region to pull data from North America (NA), Europe (EU), Far East (FE). See docs for more details.\nStart Date (Optional) is used for generating reports starting from the specified start date. Should be in YYYY-MM-DD format and not more than 60 days in the past. If not specified today's date is used. The date is treated in the timezone of the processed profile.\nProfile IDs (Optional) you want to fetch data for. See docs for more details.\nClick `Set up source`.\n\n\n\nFor Airbyte Open Source:\n\nClient ID of your Amazon Ads developer application. See onboarding process for more details.\nClient Secret of your Amazon Ads developer application. See onboarding process for more details.\nRefresh Token. See onboarding process for more details.\n\n\nSupported sync modes\nThe Amazon Ads source connector supports the following sync modes:\n - Full Refresh\n - Incremental\nSupported Streams\nThis source is capable of syncing the following streams:\n\nProfiles\nSponsored Brands Campaigns\nSponsored Brands Ad groups\nSponsored Brands Keywords\nSponsored Display Campaigns\nSponsored Display Ad groups\nSponsored Display Product Ads\nSponsored Display Targetings\nSponsored Products Campaigns\nSponsored Products Ad groups\nSponsored Products Keywords\nSponsored Products Negative keywords\nSponsored Products Ads\nSponsored Products Targetings\nBrands Reports\nBrand Video Reports\nDisplay Reports\nProducts Reports\nAttribution Reports\n\nConnector-specific features and highlights\nAll the reports are generated relative to the target profile' timezone.\nPerformance considerations\nInformation about expected report generation waiting time you may find here.\nData type mapping\n| Integration Type         | Airbyte Type |\n| :----------------------- | :----------- |\n| `string`                 | `string`     |\n| `int`, `float`, `number` | `number`     |\n| `date`                   | `date`       |\n| `datetime`               | `datetime`   |\n| `array`                  | `array`      |\n| `object`                 | `object`     |",
    "tag": "airbyte"
  },
  {
    "title": "PersistIq",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/persistiq.md",
    "content": "PersistIq\nSync overview\nThe PersistIq source supports Full Refresh syncs only.\nThis source syncs data for the PersistIq API.\nOutput schema\nThis Source is capable of syncing the following streams:\n\nUsers\nLeads\nCampaigns\n\nFeatures\n| Feature                   | Supported?(Yes/No) |\n| :------------------------ | :------------------- |\n| Full Refresh Sync         | Yes                  |\n| Incremental - Append Sync | No                   |\n| Namespaces                | No                   |\nPerformance considerations\nThe PersistIq connector should not run into PersistIq API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.\nGetting started\nRequirements\n\nPersistIq API Key\n\nSetup guide\nPlease read How to find your API key.",
    "tag": "airbyte"
  },
  {
    "title": "Metabase",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/metabase.md",
    "content": "Metabase\nThis page contains the setup guide and reference information for the Metabase source connector.\nPrerequisites\nTo set up Metabase you need:\n  * `username` and `password` - Credential pairs to authenticate with Metabase instance. This may be used to generate a new `session_token` if necessary. An email from Metabase may be sent to the owner's account everytime this is being used to open a new session.\n  * `session_token` - Credential token to authenticate requests sent to Metabase API. Usually expires after 14 days. \n  * `instance_api_url` - URL to interact with metabase instance API, that uses https.\nSetup guide\nYou can find or create authentication tokens from Metabase by running the following command:`\n`bash\ncurl -X POST \\\n-H \"Content-Type: application/json\" \\\n-d '{\"username\": \"person@metabase.com\", \"password\": \"fakepassword\"}' \\\nhttp://localhost:3000/api/session`\nIf you\u2019re working with a remote server, you\u2019ll need to replace localhost:3000 with your server address. This request will return a JSON object with a key called id and the token as the key\u2019s value, e.g.:\n`{\"id\":\"38f4939c-ad7f-4cbe-ae54-30946daf8593\"}`\nYou can use this id value as your `session_token` when configuring the connector.\nNote that these credentials tokens may expire after 14 days by default, and you might need to update your connector configuration with a new value when that happens (The connector should throw exceptions about Invalid and expired session tokens and return a 401 (Unauthorized) status code in that scenario).\nIf you are hosting your own metabase instance, you can configure this session duration on your metabase server by setting the environment variable MAX_SESSION_AGE (value is in minutes).\nIf the connector is supplied with only username and password, a session_token will be generated everytime an\nauthenticated query is running, which might trigger security alerts on the user's metabase account.\nSupported sync modes\nThe Metabase source connector supports the following sync modes:\n\nFull Refresh - Overwrite\n\nSupported Streams\n\nActivity\nCard\nCollections\nDashboard\nUser\n\nTutorials\nData type mapping\n| Integration Type    | Airbyte Type | Notes |\n|:--------------------|:-------------|:------|\n| `string`            | `string`     |       |\n| `integer`, `number` | `number`     |       |\n| `array`             | `array`      |       |\n| `object`            | `object`     |       |\nFeatures\n| Feature           | Supported?(Yes/No) | Notes |\n|:------------------|:---------------------|:------|\n| Full Refresh Sync | Yes                  |       |\n| Incremental Sync  | No                   |       |\n| SSL connection    | Yes                  |\n| Namespaces        | No                   |       |",
    "tag": "airbyte"
  },
  {
    "title": "Linnworks",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/linnworks.md",
    "content": "Linnworks\nSync overview\nLinnworks source supports both Full Refresh and Incremental syncs. You can choose if this connector will copy only the new or updated data, or all rows in the tables and columns you set up for replication, every time a sync is run.\nThis Source Connector is based on a Airbyte CDK. Airbyte uses Linnworks API to fetch data from Linnworks.\nOutput schema\nThis Source is capable of syncing the following data as streams:\n\nStockLocations\nStockLocationDetails\nStockItems\nProcessedOrders\nProcessedOrderDetails\n\nData type mapping\n| Integration Type | Airbyte Type | Notes                      |\n| :--------------- | :----------- | :------------------------- |\n| `number`         | `number`     | float number               |\n| `integer`        | `integer`    | whole number               |\n| `date`           | `string`     | FORMAT YYYY-MM-DD          |\n| `datetime`       | `string`     | FORMAT YYYY-MM-DDThh:mm:ss |\n| `array`          | `array`      |                            |\n| `boolean`        | `boolean`    | True/False                 |\n| `string`         | `string`     |                            |\nFeatures\n| Feature                                   | Supported?(Yes/No) | Notes |\n| :---------------------------------------- | :------------------- | :---- |\n| Full Refresh Overwrite Sync               | Yes                  |       |\n| Full Refresh Append Sync                  | Yes                  |       |\n| Incremental - Append Sync                 | Yes                  |       |\n| Incremental - Append + Deduplication Sync | Yes                  |       |\n| Namespaces                                | No                   |       |\nPerformance considerations\nRate limit varies across Linnworks API endpoint. See the endpoint documentation to learn more. Rate limited requests will receive a 429 response. The Linnworks connector should not run into Linnworks API limitations under normal usage.\nGetting started\nAuthentication\nLinnworks platform has two portals: seller and developer. First, to create API credentials, log in to the developer portal and create an application of type `System Integration`. Then click on provided Installation URL and proceed with an installation wizard. The wizard will show a token that you will need for authentication. The installed application will be present on your account on seller portal.\nAuthentication credentials can be obtained on developer portal section Applications -> Your application name -> Edit -> General. And the token, if you missed it during the install, can be obtained anytime under the section Applications -> Your application name -> Installs.",
    "tag": "airbyte"
  },
  {
    "title": "Fastbill ",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/fastbill.md",
    "content": "Fastbill\nThis page contains the setup guide and reference information for the Fastbill source connector.\nYou can find more information about the Fastbill REST API here.\nPrerequisites\nYou can find your Project ID and find or create an API key within Fastbill.\nSetup guide\nStep 1: Set up the Fastbill connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the Fastbill connector and select Fastbill from the Source type dropdown.\nEnter your `username` - Fastbill username/email.\nEnter your `api_key` - Fastbill API key with read permissions.\nClick Set up source.\n\nFor Airbyte OSS:\n\nNavigate to the Airbyte Open Source dashboard.\nSet the name for your source. \nEnter your `project_id` - Fastbill Project ID.\nEnter your `api_key` - Fastbill API key with read permissions.\nClick Set up source.\n\nSupported sync modes\nThe Fastbill source connector supports the following sync modes:\n| Feature           | Supported? |\n| :---------------- |:-----------|\n| Full Refresh Sync | Yes        |\n| Incremental Sync  | No         |\n| SSL connection    | No         |\n| Namespaces        | No         |\nSupported Streams\n\nCustomers\nInvoices\nProducts\nRecurring_invoices\nRevenues\n\nData type map\n| Integration Type    | Airbyte Type |\n| :------------------ | :----------- |\n| `string`            | `string`     |\n| `integer`, `number` | `number`     |\n| `array`             | `array`      |\n| `object`            | `object`     |",
    "tag": "airbyte"
  },
  {
    "title": "Workable",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/workable.md",
    "content": "Workable\nThis page guides you through the process of setting up the Workable source connector.\nPrerequisites\nYou can find or create a Workable access token within the Workable Integrations Settings page. See this page for a step-by-step guide.\nSetup guide\nStep 1: Set up the Workable connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the Workable connector and select Workable from the Source type dropdown.\nEnter your `api_token` - Workable Access Token.\nEnter your `account_subdomain` - Sub-domain for your organization on Workable, e.g. https://YOUR_ACCOUNT_SUBDOMAIN.workable.com.\nEnter your `created_after_date` - The earliest created at date from which you want to sync your Workable data.\nClick Set up source.\n\nFor Airbyte OSS:\n\nNavigate to the Airbyte Open Source dashboard.\nSet the name for your source. \nEnter your `api_token` - Workable Access Token.\nEnter your `account_subdomain` - Sub-domain for your organization on Workable, e.g. https://YOUR_ACCOUNT_SUBDOMAIN.workable.com.\nEnter your `created_after_date` - The earliest created at date from which you want to sync your Workable data.\nClick Set up source.\n\nSupported sync modes\nThe Workable source connector supports the following sync modes:\n| Feature           | Supported? |\n| :---------------- | :--------- |\n| Full Refresh Sync | Yes        |\n| Incremental Sync  | No         |\n| SSL connection    | Yes        |\n| Namespaces        | No         |\nSupported Streams\n\nJobs\nCandidates\nStages\nRecruiters\n",
    "tag": "airbyte"
  },
  {
    "title": "Zencart",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/zencart.md",
    "content": "Zencart\nZencart is an open source online store management system built on PHP, MySQL, and HTML.\nSync overview\nZencart runs on a MySQL database. You can use Airbyte to sync your Zencart instance by connecting to the underlying MySQL database and leveraging the MySQL connector.\n:::info\nReach out to your service representative or system admin to find the parameters required to connect to the underlying database\n:::\nOutput schema\nThe output schema is the same as that of the Zencart Database described here.",
    "tag": "airbyte"
  },
  {
    "title": "Confluence",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/confluence.md",
    "content": "Confluence\nOverview\nThe Confluence source supports Full Refresh syncs. \nOutput schema\nThis Source is capable of syncing the following core Streams:\n\nPages\nBlog Posts\nSpace\nGroup\nAudit\n\nData type mapping\nThe Confluence API uses the same JSONSchema types that Airbyte uses internally (`string`, `date-time`, `object`, `array`, `boolean`, `integer`, and `number`), so no type conversions happen as part of this source.\nFeatures\n| Feature | Supported? |\n| :--- | :--- |\n| Full Refresh Sync | Yes |\n| Incremental - Append Sync | No |\n| Incremental - Dedupe Sync | No |\n| SSL connection | No |\n| Namespaces | No |\nPerformance considerations\nThe Confluence connector should not run into Confluence API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.\nGetting started\nRequirements\n\nConfluence Account\nConfluence API Token\nConfluence domain name\nConfluence email address\n\nSetup guide\nFollow this article to create an API token for your Confluence account. ",
    "tag": "airbyte"
  },
  {
    "title": "QuickBooks",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/quickbooks-singer.md",
    "content": "QuickBooks\nOverview\nThe QuickBooks source supports both Full Refresh and Incremental syncs. You can choose if this connector will copy only the new or updated data, or all rows in the tables and columns you set up for replication, every time a sync is run.\nThis source wraps the Singer QuickBooks Tap.\nOutput schema\nThis Source is capable of syncing the following Streams:\n\nAccounts\nBillPayments\nBudgets\nBills\nClasses\nCreditMemos\nCustomers\nDepartments\nDeposits\nEmployees\nEstimates\nInvoices\nItems\nJournalEntries\nPayments\nPaymentMethods\nPurchases\nPurchaseOrders\nRefundReceipts\nSalesReceipts\nTaxAgencies\nTaxCodes\nTaxRates\nTerms\nTimeActivities\nTransfers\nVendorCredits\nVendors\n\nData type mapping\n| Integration Type | Airbyte Type | Notes |\n| :--------------- | :----------- | :---- |\n| `string`         | `string`     |       |\n| `number`         | `number`     |       |\n| `array`          | `array`      |       |\n| `object`         | `object`     |       |\nFeatures\n| Feature           | Supported?(Yes/No) | Notes |\n| :---------------- | :------------------- | :---- |\n| Full Refresh Sync | Yes                  |       |\n| Incremental Sync  | Yes                  |       |\n| SSL connection    | Yes                  |       |\n| Namespaces        | No                   |       |\nGetting started\n\nCreate an Intuit Developer account\nCreate an app\nObtain credentials\n\nRequirements\n\nClient ID\nClient Secret\nRealm ID\nRefresh token\n\nThe easiest way to get these credentials is by using Quickbook's OAuth 2.0 playground\nImportant note: The refresh token expires every 100 days. You will need to manually revisit the Oauth playground to obtain a refresh token every 100 days, or your syncs will expire. We plan on offering full Oauth support soon so you don't need to redo this process manually.",
    "tag": "airbyte"
  },
  {
    "title": "Webflow",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/webflow.md",
    "content": "\ndescription: 'This connector extracts \"collections\" from Webflow'\nWebflow\nWebflow is a CMS system that is used for publishing websites and blogs. This connector returns data that is made available by Webflow APIs.\nWebflow uses Collections to store different kinds of information. A collection can be \"Blog Posts\", or \"Blog Authors\", etc. Collection names are not pre-defined, the number of collections is not known in advance, and the schema for each collection may be different.\nThis connector dynamically figures our which collections are available, creates the schema for each collection based on data extracted from Webflow, and creates an Airbyte Stream for each collection.\nWebflow credentials\nYou should be able to create a Webflow `API key` (aka `API token`) as described in Intro to the Webflow API.\nOnce you have the `API Key`/`API token`, you can confirm a list of available sites and get their `_id` by executing the following:\n`curl https://api.webflow.com/sites \\\n  -H \"Authorization: Bearer <your API Key>\" \\\n  -H \"accept-version: 1.0.0\"`\nWhich should respond with something similar to:\n`[{\"_id\":\"<redacted>\",\"createdOn\":\"2021-03-26T15:46:04.032Z\",\"name\":\"Airbyte\",\"shortName\":\"airbyte-dev\",\"lastPublished\":\"2022-06-09T12:55:52.533Z\",\"previewUrl\":\"https://screenshots.webflow.com/sites/<redacted>\",\"timezone\":\"America/Los_Angeles\",\"database\":\"<redacted>\"}]`\nYou will need to provide the `Site id` and `API key` to the Webflow connector in order for it to pull data from your Webflow site.\nRelated tutorial\nIf you are interested in learning more about the Webflow API and implementation details of this connector, you may wish to consult the tutorial about how to build a connector to extract data from the Webflow API.",
    "tag": "airbyte"
  },
  {
    "title": "DataScope",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/datascope.md",
    "content": "DataScope\nThis page contains the setup guide and reference information for the DataScope source connector.\nPrerequisites\nA DataScope account with access to the API. You can create a free account here. \nSetup guide\nStep 1: Set up DataScope connection\n\nCreate a DataScope account\nCreate an API key and copy it to Airbyte\n\nStep 2: Set up the DataScope connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the DataScope connector and select DataScope from the Source type dropdown.\nEnter your `api_key`.\nEnter the params configuration if needed. Supported params are: sort, alt, prettyPrint (Optional)\nClick Set up source.\n\nFor Airbyte OSS:\n\nNavigate to the Airbyte Open Source dashboard.\nSet the name for your source.\nEnter your `api_key` which will be flagged with Authorization header.\nClick Set up source.\n\nSupported sync modes\nThe DataScope source connector supports the following sync modes:\n| Feature                       | Supported? |\n| :---------------------------- | :--------- |\n| Full Refresh Sync             | Yes        |\n| Incremental Sync              | No         |\n| Replicate Incremental Deletes | No         |\n| SSL connection                | Yes        |\n| Namespaces                    | No         |\nSupported Streams\n\nLocations\nanswers\n\nImplemented but not added streams:\n- Lists\n- Notifications\nAPI method example\nGET https://www.mydatascope.com/api/external/locations",
    "tag": "airbyte"
  },
  {
    "title": "Mailchimp",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/mailchimp.md",
    "content": "Mailchimp\nThis page guides you through setting up the Mailchimp source connector.\nPrerequisite\nYou can use OAuth or an API key to authenticate your Mailchimp account. If you choose to authenticate with OAuth, register your Mailchimp account.\nSet up the Mailchimp source connector\n\nLog into your Airbyte Cloud or Airbyte Open Source account.\nClick Sources and then click + New source. \nOn the Set up the source page, select Mailchimp from the Source type dropdown.\n\nEnter a name for your source.\n\n\nYou can use OAuth or an API key to authenticate your Mailchimp account. We recommend using OAuth for Airbyte Cloud and an API key for Airbyte Open Source.\n\nTo authenticate using OAuth for Airbyte Cloud, ensure you have registered your Mailchimp account and then click Authenticate your Mailchimp account to sign in with Mailchimp and authorize your account. \nTo authenticate using an API key for Airbyte Open Source, select API key from the Authentication dropdown and enter the API key for your Mailchimp account.  \n:::note\nCheck the performance considerations before using an API key.\n:::\n\n\nClick Set up source.\n\nSupported sync modes\nThe Mailchimp source connector supports the following sync modes:\n\nFull Refresh\nIncremental\n\nAirbyte doesn't support Incremental Deletes for the `Campaigns`, `Lists`, and `Email Activity` streams because Mailchimp doesn't provide any information about deleted data in these streams.\nPerformance considerations\nMailchimp does not impose rate limits on how much data is read from its API in a single sync process. However, Mailchimp enforces a maximum of 10 simultaneous connections to its API, which means that Airbyte is unable to run more than 10 concurrent syncs from Mailchimp using API keys generated from the same account.\nSupported streams\nThe Mailchimp source connector supports the following streams:\nLists Stream\n`{\n  \"id\": \"q1w2e3r4t5\",\n  \"web_id\": 000001,\n  \"name\": \"Newsletter Subscribers\",\n  \"contact\": {\n    \"company\": \"\",\n    \"address1\": \"\",\n    \"address2\": \"\",\n    \"city\": \"San Francisco\",\n    \"state\": \"CA\",\n    \"zip\": \"00000-1111\",\n    \"country\": \"US\",\n    \"phone\": \"\"\n  },\n  \"permission_reminder\": \"You are receiving this email because you opted in via our website.\",\n  \"use_archive_bar\": true,\n  \"campaign_defaults\": {\n    \"from_name\": \"Airbyte Community\",\n    \"from_email\": \"hey@email.com\",\n    \"subject\": \"\",\n    \"language\": \"en\"\n  },\n  \"notify_on_subscribe\": \"\",\n  \"notify_on_unsubscribe\": \"\",\n  \"date_created\": \"2020-09-17T04:48:49+00:00\",\n  \"list_rating\": 3,\n  \"email_type_option\": false,\n  \"subscribe_url_short\": \"http://eepurl.com/hfpWAr\",\n  \"subscribe_url_long\": \"https://daxtarity.us2.list-manage.com/subscribe?u=q1q1q1q1q1q1q1q1q1q&id=q1w2e3r4t5\",\n  \"beamer_address\": \"us2-00000000-qqqqqqqqq@inbound.mailchimp.com\",\n  \"visibility\": \"prv\",\n  \"double_optin\": false,\n  \"has_welcome\": false,\n  \"marketing_permissions\": false,\n  \"modules\": [],\n  \"stats\": {\n    \"member_count\": 4204,\n    \"unsubscribe_count\": 194,\n    \"cleaned_count\": 154,\n    \"member_count_since_send\": 91,\n    \"unsubscribe_count_since_send\": 19,\n    \"cleaned_count_since_send\": 23,\n    \"campaign_count\": 27,\n    \"campaign_last_sent\": \"2022-04-01T14:29:31+00:00\",\n    \"merge_field_count\": 5,\n    \"avg_sub_rate\": 219,\n    \"avg_unsub_rate\": 10,\n    \"target_sub_rate\": 18,\n    \"open_rate\": 39.478173607626694,\n    \"click_rate\": 8.504017780817234,\n    \"last_sub_date\": \"2022-04-12T07:39:29+00:00\",\n    \"last_unsub_date\": \"2022-04-11T08:08:07+00:00\"\n  },\n  \"_links\": [\n    {\n      \"rel\": \"self\",\n      \"href\": \"https://us2.api.mailchimp.com/3.0/lists/q1w2e3r4t5\",\n      \"method\": \"GET\",\n      \"targetSchema\": \"https://us2.api.mailchimp.com/schema/3.0/Definitions/Lists/Response.json\"\n    }\n  ]\n}`\nCampaigns Stream\n`{\n    \"id\": \"q1w2e3r4t5\", \n    \"web_id\": 0000000, \n    \"type\": \"regular\", \n    \"create_time\": \"2020-11-03T22:46:43+00:00\", \n    \"archive_url\": \"http://eepurl.com/hhSLxH\", \n    \"long_archive_url\": \"https://mailchi.mp/xxxxxxxx/weekly-bytes-learnings-from-soft-launch-and-our-vision-0000000\", \n    \"status\": \"sent\", \n    \"emails_sent\": 89, \n    \"send_time\": \"2020-11-05T16:15:00+00:00\", \n    \"content_type\": \"template\", \n    \"needs_block_refresh\": false, \n    \"resendable\": true, \n    \"recipients\": {\n        \"list_id\": \"1q2w3e4r\", \n        \"list_is_active\": true, \n        \"list_name\": \"Newsletter Subscribers\", \n        \"segment_text\": \"\",     \n        \"recipient_count\": 89\n    }, \n    \"settings\": {\n        \"subject_line\": \"Some subject\", \n        \"preview_text\": \"Text\", \n        \"title\": \"Newsletter\", \n        \"from_name\": \"Weekly Bytes from Airbyte\", \n        \"reply_to\": \"hey@email.com\", \n        \"use_conversation\": false, \n        \"to_name\": \"\", \n        \"folder_id\": \"\", \n        \"authenticate\": true, \n        \"auto_footer\": false, \n        \"inline_css\": false, \n        \"auto_tweet\": false, \n        \"fb_comments\": true, \n        \"timewarp\": false, \n        \"template_id\": 0000000, \n        \"drag_and_drop\": false\n    }, \n    \"tracking\": {\n        \"opens\": true, \n        \"html_clicks\": true, \n        \"text_clicks\": false, \n        \"goal_tracking\": false, \n        \"ecomm360\": false, \n        \"google_analytics\": \"\", \n        \"clicktale\": \"\"\n    }, \n    \"report_summary\": {\n        \"opens\": 46, \n        \"unique_opens\": 33, \n        \"open_rate\": 0.0128372, \n        \"clicks\": 13, \n        \"subscriber_clicks\": 7, \n        \"click_rate\": 0.0383638, \n        \"ecommerce\": {\n            \"total_orders\": 0, \n            \"total_spent\": 0, \n            \"total_revenue\": 0\n        }\n    }, \n    \"delivery_status\": {\n        \"enabled\": false\n    }, \n    \"_links\": [\n        {\n            \"rel\": \"parent\", \n            \"href\": \"https://us2.api.mailchimp.com/3.0/campaigns\", \n            \"method\": \"GET\", \n            \"targetSchema\": \"https://us2.api.mailchimp.com/schema/3.0/Definitions/Campaigns/CollectionResponse.json\", \n            \"schema\": \"https://us2.api.mailchimp.com/schema/3.0/Paths/Campaigns/Collection.json\"\n        }\n    ]\n}`\nEmail Activity Stream\n`{\n  \"campaign_id\": \"q1w2q1w2q1w2\",\n  \"list_id\": \"123qwe\",\n  \"list_is_active\": true,\n  \"email_id\": \"qwerty123456\",\n  \"email_address\": \"email@email.com\",\n  \"_links\": [\n    {\n      \"rel\": \"parent\",\n      \"href\": \"https://us2.api.mailchimp.com/3.0/reports/q1w2q1w2q1w2/email-activity\",\n      \"method\": \"GET\",\n      \"targetSchema\": \"https://us2.api.mailchimp.com/schema/3.0/Definitions/Reports/EmailActivity/CollectionResponse.json\"\n    }\n  ],\n  \"action\": \"open\",\n  \"timestamp\": \"2020-10-08T22:15:43+00:00\",\n  \"ip\": \"00.000.00.5\"\n}`\nA note on the primary keys\nThe `Lists` and `Campaigns` streams have `id` as the primary key. The `Email Activity` stream doesn't have a primary key because Mailchimp does not provide one. \nData type mapping\n| Integration Type           | Airbyte Type | Notes                                                                               |\n|:---------------------------|:-------------|:------------------------------------------------------------------------------------|\n| `array`                    | `array`      | the type of elements in the array is determined based on the mappings in this table |\n| `date`, `time`, `datetime` | `string`     |                                                                                     |\n| `int`, `float`, `number`   | `number`     |                                                                                     |\n| `object`                   | `object`     | properties within objects are mapped based on the mappings in this table            |\n| `string`                   | `string`     |                                                                                     |\nTutorials\nNow that you have set up the Mailchimp source connector, check out the following Mailchimp tutorial:\n\nBuild a data ingestion pipeline from Mailchimp to Snowflake\n",
    "tag": "airbyte"
  },
  {
    "title": "Launchdarkly API",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/launchdarkly.md",
    "content": "Launchdarkly API\nSync overview\nThis source can sync data from the Launchdarkly API. At present this connector only supports full refresh syncs meaning that each time you use the connector it will sync all available records from scratch. Please use cautiously if you expect your API to have a lot of records.\nThis Source Supports the Following Streams\n\nprojects\nenvironments\nmetrics\nmembers\naudit_log\nflags\n\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | No |  |\nPerformance considerations\nLaunchdarkly APIs are under rate limits for the number of API calls allowed per API keys per second. If you reach a rate limit, API will return a 429 HTTP error code. See here\nGetting started\nRequirements\n\nAccess Token\n",
    "tag": "airbyte"
  },
  {
    "title": "New York Times",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/nytimes.md",
    "content": "New York Times\nOverview\nThe New York Times source supports full refresh syncs\nOutput schema\nSeveral output streams are available from this source:\nArchive.\nMost Popular Emailed Articles.\nMost Popular Shared Articles.\nMost Popular Viewed Articles.\nIf there are more endpoints you'd like Airbyte to support, please create an issue.\nFeatures\n| Feature           | Supported? |\n|:------------------|:-----------|\n| Full Refresh Sync | Yes        |\n| Incremental Sync  | Yes        |\nPerformance considerations\nThe New York Times connector should not run into limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.\nGetting started\nRequirements\n\nNew York Times API Key. \n\nConnect using `API Key`:\n\nCreate a new App here (You need to have an account to create a new App).\nEnable API access for the supported endpoints (see Output schema section for supported streams).\nWrite the key into `secrets/config.json` file.\n",
    "tag": "airbyte"
  },
  {
    "title": "Drupal",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/drupal.md",
    "content": "Drupal\nDrupal is an open source Content Management Platform.\nSync overview\n:::caution\nYou will only be able to connect to a self-hosted instance of Drupal using these instructions.\n:::\nDrupal can run on MySQL, Percona, MariaDb, MSSQL, MongoDB, Postgres, or SQL-Lite. If you're not using SQL-lite, you can use Airbyte to sync your Drupal instance by connecting to the underlying database using the appropriate Airbyte connector:\n\nMySQL/Percona/MariaDB\nMSSQL\nMongo\nPostgres\n\n:::info\nReach out to your service representative or system admin to find the parameters required to connect to the underlying database\n:::\nOutput schema\nThe schema will be loaded according to the rules of the underlying database's connector.",
    "tag": "airbyte"
  },
  {
    "title": "Whisky Hunter",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/whisky-hunter.md",
    "content": "Whisky Hunter\nOverview\nThe Whisky Hunter source can sync data from the Whisky Hunter API\nOutput schema\nThis source is capable of syncing the following streams:\n* `auctions_data`\n    * Provides stats about specific auctions.\n* `auctions_info`\n    * Provides information and metadata about recurring and one-off auctions.\n* `distilleries_info`\n    * Provides information about distilleries.\nFeatures\n| Feature | Supported? |\n| :--- | :--- |\n| Full Refresh Sync | Yes |\n| Incremental - Append Sync | No |\n| Namespaces | No |\nRequirements / Setup Guide\nNo config is required.\nPerformance considerations\nThere is no published rate limit. However, since this data updates infrequently, it is recommended to set the update cadence to 24hr or higher.",
    "tag": "airbyte"
  },
  {
    "title": "Retently",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/retently.md",
    "content": "Retently\nOverview\nThe Retently source supports full refresh sync.\nOutput schema\nSeveral output streams are available from this source:\n\nCustomers\nCompanies\nReports\n\nIf there are more endpoints you'd like Airbyte to support, please create an issue.\nFeatures\n| Feature | Supported? |\n| :--- | :--- |\n| Full Refresh Sync | Yes |\n| Incremental Sync | No |\n| SSL connection | No |\n| Namespaces | No |\nGetting started\nRequirements\n\nRetently Account\nRetently API Token\n\nSetup guide\nRetently supports two types of authentication: by API Token or using Retently oAuth application.\nYou can get the API Token for Retently here.\nOAuth application is here.",
    "tag": "airbyte"
  },
  {
    "title": "SFTP  Bulk",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/sftp-bulk.md",
    "content": "SFTP  Bulk\nThis page contains the setup guide and reference information for the FTP source connector.\nThis connector allows you to:\n- Fetch files from an FTP server matching a folder path and define an optional file pattern to bulk ingest files into a single stream\n- Incrementally load files into your destination from an FTP server based on when files were last added or modified\n- Optionally load only the latest file matching a folder path and optional pattern and overwrite the data in your destination (helpful when a snapshot file gets added on a regular basis containing the latest data)\nPrerequisites\n\nThe Server with FTP connection type support\nThe Server host\nThe Server port\nUsername-Password/Public Key Access Rights\n\nSetup guide\nStep 1: Set up SFTP\n\nUse your username/password credential to connect the server.\nAlternatively generate Public Key Access\n\nThe following simple steps are required to set up public key authentication:\nKey pair is created (typically by the user). This is typically done with ssh-keygen.\nPrivate key stays with the user (and only there), while the public key is sent to the server. Typically with the ssh-copy-id utility.\nServer stores the public key (and \"marks\" it as authorized).\nServer will now allow access to anyone who can prove they have the corresponding private key.\nStep 2: Set up the SFTP connector in Airbyte\n\nIn the left navigation bar, click `Sources`. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the FTP connector and select SFTP Bulk from the Source type dropdown.\nEnter your `User Name`, `Host Address`, `Port`\nEnter authentication details for the FTP server (`Password` and/or `Private Key`)\nChoose a `File type`\nEnter `Folder Path` (Optional) to specify server folder for sync\nEnter `File Pattern` (Optional). e.g. `log-([0-9]{4})([0-9]{2})([0-9]{2})`. Write your own regex\nCheck `Most recent file` (Optional) if you only want to sync the most recent file matching a folder path and optional file pattern\nProvide a `Start Date` for incremental syncs to only sync files modified/added after this date\nClick on `Check Connection` to finish configuring the FTP source.\n\nSupported sync modes\nThe FTP source connector supports the following sync modes:\n| Feature                       | Support  | Notes                                                                                 |\n|:------------------------------|:--------:|:--------------------------------------------------------------------------------------|\n| Full Refresh - Overwrite      |    \u2705    |                                                                                      |\n| Full Refresh - Append Sync    |    \u2705    |                                                                                      |\n| Incremental - Append          |    \u2705    |                                                                                      |\n| Incremental - Deduped History |    \u274c    |                                                                                      |\n| Namespaces                    |    \u274c    |                                                                                      |\nSupported Streams\nThis source provides a single stream per file with a dynamic schema. The current supported type file: `.csv` and `.json`\nMore formats (e.g. Apache Avro) will be supported in the future.",
    "tag": "airbyte"
  },
  {
    "title": "TPL/3PL Central",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/tplcentral.md",
    "content": "TPL/3PL Central\nOverview\nThe 3PL Central source supports both Full Refresh and Incremental syncs. You can choose if this connector will copy only the new or updated data, or all rows in the tables and columns you set up for replication, every time a sync is run.\nThis Source Connector is based on a Airbyte CDK.\nOutput schema\nSeveral output streams are available from this source:\n\nStockSummaries (Full table)\nCustomers (Full table)\nItems (Incremental)\nStockDetails (Incremental)\nInventory (Incremental)\nOrders (Incremental)\n\nIf there are more endpoints you'd like Airbyte to support, please create an issue.\nFeatures\n| Feature                   | Supported? |\n| :------------------------ | :--------- |\n| Full Refresh Sync         | Yes        |\n| Incremental - Append Sync | Yes        |\n| SSL connection            | Yes        |\n| Namespaces                | No         |\nGetting started\nRequirements\n\nClient ID\nClient Secret\nUser login ID and/or name\n3PL GUID\nCustomer ID\nFacility ID\nStart date\n\nSetup guide\nPlease read How to get your APIs credentials.",
    "tag": "airbyte"
  },
  {
    "title": "Redshift",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/redshift.md",
    "content": "Redshift\nOverview\nThe Redshift source supports Full Refresh syncs. That is, every time a sync is run, Airbyte will copy all rows in the tables and columns you set up for replication into the destination in a new table.\nThis Redshift source connector is built on top of the source-jdbc code base and is configured to rely on JDBC 4.2 standard drivers provided by Amazon via Mulesoft here as described in Redshift documentation here.\nSync overview\nResulting schema\nThe Redshift source does not alter the schema present in your warehouse. Depending on the destination connected to this source, however, the schema may be altered. See the destination's documentation for more details.\nFeatures\n| Feature | Supported | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | Coming soon |  |\n| Replicate Incremental Deletes | Coming soon |  |\n| Logical Replication (WAL) | Coming soon |  |\n| SSL Support | Yes |  |\n| SSH Tunnel Connection | Coming soon |  |\n| Namespaces | Yes | Enabled by default |\n| Schema Selection | Yes | Multiple schemas may be used at one time. Keep empty to process all of existing schemas |\nIncremental Sync\nIncremental sync (copying only the data that has changed) for this source is coming soon.\nGetting started\nRequirements\n\nActive Redshift cluster\nAllow connections from Airbyte to your Redshift cluster (if they exist in separate VPCs)\n\nSetup guide\n1. Make sure your cluster is active and accessible from the machine running Airbyte\nThis is dependent on your networking setup. The easiest way to verify if Airbyte is able to connect to your Redshift cluster is via the check connection tool in the UI. You can check AWS Redshift documentation with a tutorial on how to properly configure your cluster's access here\n2. Fill up connection info\nNext is to provide the necessary information on how to connect to your cluster such as the `host` whcih is part of the connection string or Endpoint accessible here without the `port` and `database` name (it typically includes the cluster-id, region and end with `.redshift.amazonaws.com`).\nEncryption\nAll Redshift connections are encrypted using SSL",
    "tag": "airbyte"
  },
  {
    "title": "Firebolt",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/firebolt.md",
    "content": "Firebolt\nOverview\nThe Firebolt source allows you to sync your data from Firebolt. Only Full refresh is supported at the moment.\nThe connector is built on top of a pure Python firebolt-sdk and does not require additonal dependencies.\nResulting schema\nThe Firebolt source does not alter schema present in your database. Depending on the destination connected to this source, however, the result schema may be altered. See the destination's documentation for more details.\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental - Append Sync | No |  |\nGetting started\nRequirements\n\nAn existing AWS account\n\nSetup guide\n\n\nCreate a Firebolt account following the guide\n\n\nFollow the getting started tutorial to setup a database\n\n\nLoad data\n\n\nCreate an Analytics (read-only) engine as described in here\n\n\nYou should now have the following\n\nAn existing Firebolt account\nConnection parameters handy\nUsername\nPassword\nAccount, in case of a multi-account setup (Optional)\nHost (Optional)\nEngine (Optional), preferably Analytics/read-only\n\n\nA running engine (if an engine is stopped or booting up you won't be able to connect to it)\nYour data in either Fact or Dimension tables.\n\nYou can now use the Airbyte Firebolt source.",
    "tag": "airbyte"
  },
  {
    "title": "NASA",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/nasa.md",
    "content": "NASA\nOverview\nThe NASA source supports full refresh syncs\nOutput schema\nAsingle output stream is available (at the moment) from this source:\n*APOD.\nIf there are more endpoints you'd like Airbyte to support, please create an issue.\nFeatures\n| Feature           | Supported? |\n|:------------------|:-----------|\n| Full Refresh Sync | Yes        |\n| Incremental Sync  | Yes        |\n| SSL connection    | No         |\n| Namespaces        | No         |\nPerformance considerations\nThe NASA connector should not run into NASA API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.\nGetting started\nRequirements\n\nNASA API Key. You can use `DEMO_KEY` (see rate limits here).\n\nConnect using `API Key`:\n\nGenerate an API Key as described here.\nUse the generated `API Key` in the Airbyte connection.\n",
    "tag": "airbyte"
  },
  {
    "title": "Salesloft",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/salesloft.md",
    "content": "Salesloft\nOverview\nThe Salesloft source supports both `Full Refresh` and `Incremental` syncs. You can choose if this connector will copy only the new or updated data, or all rows in the tables and columns you set up for replication, every time a sync is run.\nOutput schema\nSeveral output streams are available from this source. A list of these streams can be found below in the Streams section.\nFeatures\n| Feature | Supported? |\n| :--- | :--- |\n| Full Refresh Sync | Yes |\n| Incremental Sync | Yes |\n| SSL connection | Yes |\n| Namespaces | No |\nGetting started\nRequirements\n\nSalesloft Account\nSalesloft OAuth credentials\n\nSetup guide\nGetting an admin level oauth credentials require registering as partner. Check out here and here\nStreams\nList of available streams:\n\nCadenceMemberships\nCadences\nPeople\nUsers\nEmails\nAccount Stages\nAccount Tiers\nAccounts\nActions\nCalls\nEmails Templates\nEmails Template Attachements\nImports\nNotes\nPerson Stages\nPhone Number Assignments\nSteps\nTeam Templates\nTeam Template Attachements\nCRM Activities\nCRM Users\nGroups\nSuccesses\n",
    "tag": "airbyte"
  },
  {
    "title": "Statuspage.io API",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/statuspage.md",
    "content": "Statuspage.io API\nSync overview\nThis source can sync data from the Statuspage.io API. At present this connector only supports full refresh syncs meaning that each time you use the connector it will sync all available records from scratch. Please use cautiously if you expect your API to have a lot of records.\nThis Source Supports the Following Streams\n\npages\nsubscribers\nsubscribers_histogram_by_state\nincident_templates\nincidents\ncomponents\nmetrics\n\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | No |  |\nPerformance considerations\nMailjet APIs are under rate limits for the number of API calls allowed per API keys per second. If you reach a rate limit, API will return a 429 HTTP error code. See here\nGetting started\nRequirements\n\nStatuspage.io API KEY\n",
    "tag": "airbyte"
  },
  {
    "title": "Zendesk Sunshine",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/zendesk-sell.md",
    "content": "Zendesk Sunshine\nSync overview\nThe Zendesk Sell source supports Full Refresh.\nThis source can sync data for the Zendesk Sell API.\nOutput schema\nThis Source is capable of syncing the following core Streams:\n\nCall Outcomes\nCalls\nCollaborations\nContacts\nDeal Sources\nDeal Unqualified Reason\nDeals\nLead Conversions\nLead Sources\nLead Unqualified Reason\nLeads\nLoss Reasons\nNotes\nOrders\nPipelines\nProducts\nStages\nTags\nTasks\nText Messages\nUsers\nVisit Outcomes\nVisits\n\nData type mapping\n| Integration Type | Airbyte Type | Notes |\n| :--- | :--- | :--- |\n| `string` | `string` |  |\n| `number` | `number` |  |\n| `array` | `array` |  |\n| `object` | `object` |  |\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | Yes |  |\nPerformance considerations\nThe connector is restricted by normal Zendesk requests limitation\nThe Zendesk connector should not run into Zendesk API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.\nGetting started\nRequirements\n\nZendesk Sell API Token\n\nSetup guide\nPlease follow this guide\nGenerate an API Token or oauth2.0 Access token as described in here\nWe recommend creating a restricted, read-only key specifically for Airbyte access. This will allow you to control which resources Airbyte should be able to access.",
    "tag": "airbyte"
  },
  {
    "title": "Paypal Transaction",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/paypal-transaction.md",
    "content": "Paypal Transaction\nThis page contains the setup guide and reference information for the Paypal Transaction source connector.\nPrerequisites\nThe Paypal Transaction API is used to get the history of transactions for a PayPal account.\nSetup guide\nStep 1: Set up Paypal Transaction\nIn order to get an `Client ID` and `Secret` please go to this page and follow the instructions. After registration you may find your `Client ID` and `Secret` here.\n:::note\nOur Paypal Transactions Source Connector does not support OAuth at this time due to limitations outside of our control. If OAuth for Paypal Transactions is critical to your business, please reach out to us to discuss how we may be able to partner on this effort.\n:::\nStep 2: Set up the Paypal Transaction connector in Airbyte\n\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the Paypal Transaction connector and select Paypal Transaction from the Source type dropdown.\nEnter your client id\nEnter your secret\nChoose if your account is sandbox\nEnter the date you want your sync to start from\nClick Set up source.\n\n\n\nFor Airbyte Open Source:\n\nNavigate to the Airbyte Open Source dashboard\nSet the name for your source\nEnter your client id\nEnter your secret\nChoose if your account is sandbox\nEnter the date you want your sync to start from\nClick Set up source\n\n\nSupported sync modes\nThe PayPal Transaction source connector supports the following sync modes:\n| Feature                   | Supported? |\n| :------------------------ | :--------- |\n| Full Refresh Sync         |    Yes     |\n| Incremental - Append Sync |    Yes     |\n| Namespaces                |     No     |\nSupported Streams\nThis Source is capable of syncing the following core Streams:\n\nTransactions\nBalances\n\nPerformance considerations\nPaypal transaction API has some limits\n\n`start_date_min` = 3 years, API call lists transaction for the previous three years.\n`start_date_max` = 1.5 days, it takes a maximum of three hours for executed transactions to appear in the list transactions call. It is set to 1.5 days by default based on experience, otherwise API throw an error.\n`stream_slice_period` = 1 day, the maximum supported date range is 31 days.\n`records_per_request` = 10000, the maximum number of records in a single request.\n`page_size` = 500, the maximum page size is 500.\n`requests_per_minute` = 30, maximum limit is 50 requests per minute from IP address to all endpoint\n\nTransactions sync is performed with default `stream_slice_period` = 1 day, it means that there will be 1 request for each day between start_date and now or end_date. if `start_date` is greater then `start_date_max`. Balances sync is similarly performed with default `stream_slice_period` = 1 day, but it will do additional request for the end_date of the sync now.\nData type map\n| Integration Type | Airbyte Type |\n| :--------------- | :----------- |\n|     `string`     |   `string`   |\n|     `number`     |   `number`   |\n|     `array`      |   `array`    |\n|     `object`     |   `object`   |",
    "tag": "airbyte"
  },
  {
    "title": "Harness",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/harness.md",
    "content": "Harness\nOverview\nThe Harness source is maintained by Faros\nAI.\nPlease file any support requests on that repo to minimize response time from the\nmaintainers. The source supports both Full Refresh and Incremental syncs. You\ncan choose if this source will copy only the new or updated data, or all rows in\nthe tables and columns you set up for replication, every time a sync is run.\nOutput schema\nOnly one stream is currently available from this source:\n\nExecutions (Incremental)\n\nIf there are more endpoints you'd like Faros AI to support, please create an\nissue.\nFeatures\n| Feature | Supported? |\n| :--- | :--- |\n| Full Refresh Sync | Yes |\n| Incremental Sync | Yes |\n| SSL connection | Yes |\n| Namespaces | No |\nPerformance considerations\nThe Harness source should not run into Harness API limitations under normal\nusage.  Please create an\nissue if you see any\nrate limit issues that are not automatically retried successfully.\nGetting started\nRequirements\n\nHarness Account Id\nHarness API Key\nHarness API URL, if using a self-hosted Harness instance\n\nPlease follow the their documentation for generating a Harness API\nKey.",
    "tag": "airbyte"
  },
  {
    "title": "OpenWeather",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/openweather.md",
    "content": "OpenWeather\nOverview\nThis source connector syncs data from the OpenWeather One Call API. This API allows to obtain current and weather data from a geolocation expressed in latitude and longitude.\nOutput schema\nThis source currently has a single stream, `openweather_one_call`. An example of the data outputted by this stream is available here.\nFeatures\n| Feature | Supported? |\n| :--- | :--- |\n| Full Refresh Sync - (append only) | Yes |\n| Incremental - Append Sync | Yes |\n| Namespaces | No |\nGetting started\nRequirements\n\nAn OpenWeather API key\nLatitude and longitude of the location for which you want to get weather data\n\nSetup guide\nVisit the OpenWeather to create a user account and obtain an API key. The One Call API is available with the free plan.\nRate limiting\nThe free plan allows 60 calls per minute and 1,000,000 calls per month, you won't get beyond these limits with existing Airbyte's sync frequencies.",
    "tag": "airbyte"
  },
  {
    "title": "Zendesk Talk",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/zendesk-talk.md",
    "content": "Zendesk Talk\nPrerequisites\n\nZendesk API Token or Zendesk OAuth Client\nZendesk Email (For API Token authentication)\nZendesk Subdomain\n\nSetup guide\nStep 1: Set up Zendesk\nGenerate a API access token as described in Zendesk docs\nWe recommend creating a restricted, read-only key specifically for Airbyte access. This will allow you to control which resources Airbyte should be able to access.\nAnother option is to use OAuth2.0 for authentication. See Zendesk docs for details.\n\nStep 2: Set up the Zendesk Talk connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the Zendesk Talk connector and select Zendesk Talk from the Source type dropdown.\nFill in the rest of the fields:\nSubdomain\nAuthentication (API Token / OAuth2.0)\nStart Date\nClick Set up source\n\n\nSupported sync modes\nThe Zendesk Talk source connector supports the following sync modes:\n* Full Refresh\n* Incremental Sync\nSupported Streams\nThis Source is capable of syncing the following core Streams:\n\nAccount Overview\nAddresses\nAgents Activity\nAgents Overview\nCalls (Incremental sync)\nCall Legs (Incremental sync)\nCurrent Queue Activity\nGreeting Categories\nGreetings\nIVRs\nIVR Menus\nIVR Routes\nPhone Numbers\n\nPerformance considerations\nThe connector is restricted by normal Zendesk requests limitation.\nThe Zendesk connector should not run into Zendesk API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.\nData type map\n| Integration Type | Airbyte Type | Notes |\n| :------- | :------- | :--- |\n| `string` | `string` |      |\n| `number` | `number` |      |\n| `array`  | `array`  |      |\n| `object` | `object` |      |",
    "tag": "airbyte"
  },
  {
    "title": "Courier",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/courier.md",
    "content": "Courier\nThis page contains the setup guide and reference information for the Courier source connector.\nPrerequisites\nGenerate an API key per the Courier documentation.\nSetup guide\nStep 1: Set up Courier\n\nCourier Account\nCourier API Key\n\nStep 2: Set up the Courier connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the Courier connector and select Courier from the Source type dropdown.\nEnter your `api_key`.\nClick Set up source.\n\nFor Airbyte OSS:\n\nNavigate to the Airbyte Open Source dashboard.\nSet the name for your source.\nEnter your `api_key`.\nClick Set up source.\n\nSupported sync modes\nThe Courier source connector supports the following sync modes:\n| Feature                       | Supported? |\n| :---------------------------- | :--------- |\n| Full Refresh Sync             | Yes        |\n| Incremental Sync              | No         |\n| Replicate Incremental Deletes | No         |\n| SSL connection                | Yes        |\n| Namespaces                    | No         |\nSupported Streams\n\nMessages\n\nPerformance considerations\nCourier's API reference does not address rate limiting but the connector implements exponential backoff when a 429 response status code is received.",
    "tag": "airbyte"
  },
  {
    "title": "Flexport",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/flexport.md",
    "content": "Flexport\nSync overview\nFlexport source uses Flexport API to extract data from Flexport.\nOutput schema\nThis Source is capable of syncing the following data as streams:\n\nCompanies\nLocations\nProducts\nInvoices\nShipments\n\nData type mapping\n| Integration Type | Airbyte Type | Notes |\n| :--- | :--- | :--- |\n| `number` | `number` | float number |\n| `integer` | `integer` | whole number |\n| `date` | `string` | FORMAT YYYY-MM-DD |\n| `datetime` | `string` | FORMAT YYYY-MM-DDThh:mm:ss |\n| `array` | `array` |  |\n| `boolean` | `boolean` | True/False |\n| `string` | `string` |  |\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Overwrite Sync | Yes |  |\n| Full Refresh Append Sync | Yes |  |\n| Incremental - Append Sync | Yes |  |\n| Incremental - Append + Deduplication Sync | Yes |  |\n| Namespaces | No |  |\nGetting started\nAuthentication\nAuthentication uses a pre-created API token which can be created in the UI.",
    "tag": "airbyte"
  },
  {
    "title": "Iterable",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/iterable.md",
    "content": "Iterable\nThis page contains the setup guide and reference information for the Iterable source connector.\nPrerequisites\nTo set up the Iterable source connector, you'll need the Iterable Server-side API Key with standard permissions.\nSet up the Iterable connector in Airbyte\n\nLog into your Airbyte Cloud account or navigate to the Airbyte Open Source dashboard.\nClick Sources and then click + New source.\nOn the Set up the source page, select Iterable from the Source type dropdown.\nEnter the name for the Iterable connector.\nFor API Key, enter the Iterable API key.\nFor Start Date, enter the date in YYYY-MM-DDTHH:mm:ssZ format. The data added on and after this date will be replicated.\nClick Set up source.\n\nSupported sync modes\nThe Iterable source connector supports the following sync modes:\n\nFull Refresh - Overwrite\nFull Refresh - Append\nIncremental - Append\nIncremental - Deduped History\n\nSupported Streams\n\nCampaigns\nCampaign Metrics\nChannels\nEmail Bounce (Incremental)\nEmail Click (Incremental)\nEmail Complaint (Incremental)\nEmail Open (Incremental)\nEmail Send (Incremental)\nEmail Send Skip (Incremental)\nEmail Subscribe (Incremental)\nEmail Unsubscribe (Incremental)\nEvents\nLists\nList Users\nMessage Types\nMetadata\nTemplates (Incremental)\nUsers (Incremental)\nPushSend (Incremental)\nPushSendSkip (Incremental)\nPushOpen (Incremental)\nPushUninstall (Incremental)\nPushBounce (Incremental)\nWebPushSend (Incremental)\nWebPushClick (Incremental)\nWebPushSendSkip (Incremental)\nInAppSend (Incremental)\nInAppOpen (Incremental)\nInAppClick (Incremental)\nInAppClose (Incremental)\nInAppDelete (Incremental)\nInAppDelivery (Incremental)\nInAppSendSkip (Incremental)\nInboxSession (Incremental)\nInboxMessageImpression (Incremental)\nSmsSend (Incremental)\nSmsBounce (Incremental)\nSmsClick (Incremental)\nSmsReceived (Incremental)\nSmsSendSkip (Incremental)\nSmsUsageInfo (Incremental)\nPurchase (Incremental)\nCustomEvent (Incremental)\nHostedUnsubscribeClick (Incremental)\n",
    "tag": "airbyte"
  },
  {
    "title": "Shortio",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/shortio.md",
    "content": "Shortio\nSync overview\nThe Shopify source supports both Full Refresh and Incremental syncs. You can choose if this connector will copy only the new or updated data, or all rows in the tables and columns you set up for replication, every time a sync is run.\nThis source can sync data for the Shortio API.\nThis Source Connector is based on a Airbyte CDK.\nOutput schema\nThis Source is capable of syncing the following Streams:\n\nClicks\nLinks\n\nData type mapping\n| Integration Type | Airbyte Type | Notes |\n| :--- | :--- | :--- |\n| `string` | `string` |  |\n| `number` | `number` |  |\n| `array` | `array` |  |\n| `object` | `object` |  |\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental - Append Sync | Yes |  |\n| Namespaces | No |  |\nGetting started\n\nSign in at `app.short.io`.\nGo to settings and click on `Integrations & API`.\nIn the API tab, click `Create API Kay`. Select `Private Key`.\nUse the created secret key to configure your source!\n",
    "tag": "airbyte"
  },
  {
    "title": "Cart.com",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/cart.md",
    "content": "Cart.com\nSync overview\nThis source can sync data for the Cart.com API. It supports both Full Refresh and Incremental sync for all streams. You can choose if this connector will copy only the new or updated data, or all rows in the tables and columns you set up for replication, every time a sync is run.\nOutput schema\nThis Source is capable of syncing the following core Streams:\n\nCustomersCart\nOrders\nOrderPayments\nProducts\n\nData type mapping\n| Integration Type | Airbyte Type | Notes |\n| :--------------- | :----------- | :---- |\n| `string`         | `string`     |       |\n| `number`         | `number`     |       |\n| `array`          | `array`      |       |\n| `object`         | `object`     |       |\nFeatures\n| Feature           | Supported?(Yes/No) | Notes |\n| :---------------- | :------------------- | :---- |\n| Full Refresh Sync | Yes                  |       |\n| Incremental Sync  | Yes                  |       |\n| Namespaces        | No                   |       |\nPerformance considerations\nThe Cart.com API has some request limitation. See this .\nGetting started\nRequirements\n\nAmeriCommerce account\nAdmin access\nAccess Token\n\nSetup guide\nPlease follow these steps to obtain Access Token for your account.",
    "tag": "airbyte"
  },
  {
    "title": "Railz",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/railz.md",
    "content": "Railz\nThis source can sync data for the Railz API.\nThis page guides you through the process of setting up the Railz source connector.\nPrerequisites\n\nA Railz account with permission to access data from accounts you want to sync.\nRailz Client ID and Secret key\n\nSetup guide\nStep 1: Set up Railz\nGenerate API key on the dashboard and take it's client_id and secret_key.\nStep 2: Set up the source connector in Airbyte\n\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account;\nIn the left navigation bar, click Sources. In the top-right corner, click + new source;\nOn the source setup page, select Railz from the Source type dropdown and enter a name for this connector;\nEnter `Client ID (client_id)`;\nEnter `Secret key (secret_key)`;\nEnter `Start date` (optional);\nclick `Set up source`.\n\n\n\nFor Airbyte Open Source:\n\nGo to local Airbyte page;\nIn the left navigation bar, click Sources. In the top-right corner, click + new source;\nOn the source setup page, select Railz from the Source type dropdown and enter a name for this connector;\nEnter `Client ID (client_id)`;\nEnter `Secret key (secret_key)`;\nEnter `Start date`;\nclick `Set up source`.\n\n\nSupported sync modes\nThe Railz supports full refresh and incremental sync.\n| Feature           | Supported? |\n| :---------------- | :--------- |\n| Full Refresh Sync | Yes        |\n| Incremental Sync  | Yes        |\nSupported Streams\nSeveral output streams are available from this source:\n\nBusinesses\nConnections\nCustomers\nAccounts\nInventory\nTax Rates\nTracking Categories\nVendors\nBank Accounts\nAccounting Transactions (Incremental sync)\nBank Transfers (Incremental sync)\nBills (Incremental sync)\nBills Credit Notes (Incremental sync)\nBills Payments (Incremental sync)\nDeposits (Incremental sync)\nEstimates (Incremental sync)\nInvoices (Incremental sync)\nInvoices Credit Notes (Incremental sync)\nInvoices Payments (Incremental sync)\nJournal Entries (Incremental sync)\nPurchase Orders (Incremental sync)\nRefunds (Incremental sync)\nCommerce Disputes (Incremental sync)\nCommerce Orders (Incremental sync)\nCommerce Products (Incremental sync)\nCommerce Transactions (Incremental sync)\n\nIf there are more endpoints you'd like Airbyte to support, please create an issue.\nPerformance considerations\nThe Railz connector should gracefully handle Railz API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.",
    "tag": "airbyte"
  },
  {
    "title": "ConvertKit",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/convertkit.md",
    "content": "ConvertKit\nSync overview\nThis source can sync data from the ConvertKit API. At present this connector only supports full refresh syncs meaning that each time you use the connector it will sync all available records from scratch. Please use cautiously if you expect your API to have a lot of records.\nThis Source Supports the Following Streams\n\nsequences\nsubscribers\nbroadcasts\ntags\nforms\n\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | No |  |\nPerformance considerations\nThe connector has a rate limit of no more than 120 requests over a rolling 60 second period, for a given api secret.\nGetting started\nRequirements\n\nConvertKit API Secret\n",
    "tag": "airbyte"
  },
  {
    "title": "RSS",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/rss.md",
    "content": "RSS\nOverview\nThe RSS source allows you to read data from any individual RSS feed.\nOutput schema\nThis source is capable of syncing the following streams:\n* `items`\n    * Provides stats about specific RSS items.\n    * Most fields are simply kept from RSS items as strings if present (`title`, `link`, `description`, `author`, `category`, `comments`, `enclosure`, `guid`).\n    * The date field is handled differently. It's transformed into a UTC datetime in a `published` field for easier use in data warehouses and other destinations.\n    * The RSS feed you're subscribing to must have a valid `pubDate` field for each item for incremental syncs to work properly.\n    * Since `guid` is not a required field, there is no primary key for the feed, only a cursor on the published date.\nFeatures\n| Feature | Supported? |\n| :--- | :--- |\n| Full Refresh Sync | Yes |\n| Incremental - Append Sync | Yes |\n| Namespaces | No |\nRequirements / Setup Guide\nOnly the `url` of an RSS feed is required.\nPerformance considerations\nNone",
    "tag": "airbyte"
  },
  {
    "title": "Google-webfonts",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/google-webfonts.md",
    "content": "Google-webfonts\nThis page contains the setup guide and reference information for the Google-webfonts source connector.\nPrerequisites\nApi key is mandate for this connector to work, It could be generated by a gmail account for free at https://console.cloud.google.com/apis/dashboard.\nJust pass the generated API key and optional parameters for establishing the connection. Example:123\nSetup guide\nStep 1: Set up Google-webfonts connection\n\nGenerate an API key (Example: 12345)\nParams (If specific info is needed)\nAvailable params\nsort: SORT_UNDEFINED, ALPHA, DATE, STYLE, TRENDING, POPULARITY\nalt: json, media or proto\nprettyPrint: boolean\n\nStep 2: Set up the Google-webfonts connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the Google-webfonts connector and select Google-webfonts from the Source type dropdown.\nEnter your `api_key`.\nEnter the params configuration if needed. Supported params are: sort, alt, prettyPrint (Optional)\nClick Set up source.\n\nFor Airbyte OSS:\n\nNavigate to the Airbyte Open Source dashboard.\nSet the name for your source.\nEnter your `api_key`.\nEnter the params configuration if needed. Supported params are: sort, alt, prettyPrint (Optional)\nClick Set up source.\n\nSupported sync modes\nThe Google-webfonts source connector supports the following sync modes:\n| Feature                       | Supported? |\n| :---------------------------- | :--------- |\n| Full Refresh Sync             | Yes        |\n| Incremental Sync              | No         |\n| Replicate Incremental Deletes | No         |\n| SSL connection                | Yes        |\n| Namespaces                    | No         |\nSupported Streams\n\nWebfonts (Single stream API)\n\nAPI method example\nGET https://webfonts.googleapis.com/v1/webfonts?key=<1234567>&sort=SORT_UNDEFINED&prettyPrint=true&alt=json\nPerformance considerations\nGoogle Webfont's API reference has v1 at present and v2 is at development. The connector as default uses v1.",
    "tag": "airbyte"
  },
  {
    "title": "Microsoft Dynamics Customer Engagement",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/microsoft-dynamics-customer-engagement.md",
    "content": "Microsoft Dynamics Customer Engagement\nMS Dynamics Customer Engagement is an on-premise Customer Relationship Management (CRM) software.\nSync overview\nMS Dynamics Customer Engagement runs on MSSQL database. You can use the MSSQL connector to sync your MS Dynamics Customer Engagement instance by connecting to the underlying database.\n:::info\nReach out to your service representative or system admin to find the parameters required to connect to the underlying database\n:::\nOutput schema\nTo understand your MS Dynamics Customer Engagement database schema, see the Entity Reference documentation. Otherwise, the schema will be loaded according to the rules of MSSQL connector.",
    "tag": "airbyte"
  },
  {
    "title": "Tyntec SMS",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/tyntec-sms.md",
    "content": "Tyntec SMS\nOverview\nThis page contains the setup guide and reference information for the Tyntec SMS source connector.\nGetting Started\nPrerequisites\nA Tyntec SMS API Key and SMS message request ID are required for this connector to work. The API key can be generated by setting up a free Tyntec account. Be sure to pass the generated API key and message request ID when establishing the connection.\nStep 1: Set up a Tyntec SMS connection\n\nCreate a new Tyntec account here. \nIn the left navigation bar, click API Settings and navigate to API Keys to access your API key.\n\nStep 2: Set up a Tyntec SMS connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the Tyntec SMS connector and select Tyntec SMS from the Source type dropdown.\nEnter your `Tyntec API Key`.\nEnter your `SMS Message Request ID`.\nClick Set up source.\n\nFor Airbyte OSS:\n\nNavigate to the Airbyte Open Source dashboard.\nSet the name for your source.\nEnter your `Tyntec API Key`.\nEnter your `SMS Message Request ID`.\nClick Set up source.\n\nSupported Sync Modes\nThe Tyntec SMS source connector supports the following sync modes:\n| Feature           | Supported? |\n|:------------------|:-----------|\n| Full Refresh Sync | Yes        |\n| Incremental Sync  | No         |\nSupported Streams\nSeveral output streams are available from this source:\n\nSMS Message Status\nSMS Contacts\nSMS Phone Numbers\nSMS Phone Registrations\n\nIf there are more endpoints you'd like Airbyte to support, please create an issue.\nPerformance Considerations\nThe Tyntec SMS connector should not run into limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.",
    "tag": "airbyte"
  },
  {
    "title": "Rocket.chat API",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/rocket-chat.md",
    "content": "Rocket.chat API\nSync overview\nThis source can sync data from the Rocket.chat API. At present this connector only supports full refresh syncs meaning that each time you use the connector it will sync all available records from scratch. Please use cautiously if you expect your API to have a lot of records.\nThis Source Supports the Following Streams\n\nteams\nrooms\nchannels\nroles\nsubscriptions\nusers\n\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :-- | :-- | :--* |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | No |  |\nPerformance considerations\nRocket.chat APIs are under rate limits for the number of API calls allowed per API keys per second. If you reach a rate limit, API will return a 429 HTTP error code. See here\nGetting started\nRequirements\nYou need to setup a personal access token within the Rocket.chat workspace, see here for step-by-step.\n\ntoken\nuser_id\nendpoint\n",
    "tag": "airbyte"
  },
  {
    "title": "Elasticsearch",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/elasticsearch.md",
    "content": "Elasticsearch\nThis page contains the setup guide and reference information for the Elasticsearch source connector.\nPrerequisites\nRequirements\n\nElasticsearch endpoint URL\nElasticsearch credentials (optional)\n\nSupported sync modes\n| Feature           | Supported?(Yes/No) | Notes |\n| :---------------- | :------------------- | :---- |\n| Full Refresh Sync | Yes                  |       |\n| Incremental Sync  | No                   |       |\nThis source syncs data from an ElasticSearch domain.\nSupported Streams\nThis source automatically discovers all indices in the domain and can sync any of them.\nPerformance Considerations\nElasticSearch calls may be rate limited by the underlying service.\nThis is specific to each deployment.\nData type map\nElasticsearch data types: https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-types.html\nAirbyte data types: https://docs.airbyte.com/understanding-airbyte/supported-data-types/\nIn Elasticsearch, there is no dedicated array data type.\nAny field can contain zero or more values by default, however,\nall values in the array must be of the same data type. Hence, every field can be an array as well.\n| Integration Type          | Airbyte Type                               | Notes |\n| :------------------------ | :----------------------------------------- | :---- |\n| `binary`                  | `[\"string\", \"array\"]`                      |       |\n| `boolean`                 | `[\"boolean\", \"array\"]`                     |       |\n| `keyword`                 | `[\"string\", \"array\", \"number\", \"integer\"]` |       |\n| `constant_keyword`        | `[\"string\", \"array\", \"number\", \"integer\"]` |       |\n| `wildcard`                | `[\"string\", \"array\", \"number\", \"integer\"]` |       |\n| `long`                    | `[\"integer\", \"array\"]`                     |       |\n| `unsigned_long`           | `[\"integer\", \"array\"]`                     |       |\n| `integer`                 | `[\"integer\", \"array\"]`                     |       |\n| `short`                   | `[\"integer\", \"array\"]`                     |       |\n| `byte`                    | `[\"integer\", \"array\"]`                     |       |\n| `double`                  | `[\"number\", \"array\"]`                      |       |\n| `float`                   | `[\"number\", \"array\"]`                      |       |\n| `half_float`              | `[\"number\", \"array\"]`                      |       |\n| `scaled_float`            | `[\"number\", \"array\"]`                      |       |\n| `date`                    | `[\"string\", \"array\"]`                      |       |\n| `date_nanos`              | `[\"number\", \"array\"]`                      |       |\n| `object`                  | `[\"object\", \"array\"]`                      |       |\n| `flattened`               | `[\"object\", \"array\"]`                      |       |\n| `nested`                  | `[\"object\", \"string\"]`                     |       |\n| `join`                    | `[\"object\", \"string\"]`                     |       |\n| `integer_range`           | `[\"object\", \"array\"]`                      |       |\n| `float_range`             | `[\"object\", \"array\"]`                      |       |\n| `long_range`              | `[\"object\", \"array\"]`                      |       |\n| `double_range`            | `[\"object\", \"array\"]`                      |       |\n| `date_range`              | `[\"object\", \"array\"]`                      |       |\n| `ip_range`                | `[\"object\", \"array\"]`                      |       |\n| `ip`                      | `[\"string\", \"array\"]`                      |       |\n| `version`                 | `[\"string\", \"array\"]`                      |       |\n| `murmur3`                 | `[\"string\", \"array\", \"number\", \"integer\"]` |       |\n| `aggregate_metric_double` | `[\"string\", \"array\", \"number\", \"integer\"]` |       |\n| `histogram`               | `[\"string\", \"array\", \"number\", \"integer\"]` |       |\n| `text`                    | `[\"string\", \"array\", \"number\", \"integer\"]` |       |\n| `alias`                   | `[\"string\", \"array\", \"number\", \"integer\"]` |       |\n| `search_as_you_type`      | `[\"string\", \"array\", \"number\", \"integer\"]` |       |\n| `token_count`             | `[\"string\", \"array\", \"number\", \"integer\"]` |       |\n| `dense_vector`            | `[\"string\", \"array\", \"number\", \"integer\"]` |       |\n| `geo_point`               | `[\"string\", \"array\", \"number\", \"integer\"]` |       |\n| `geo_shape`               | `[\"string\", \"array\", \"number\", \"integer\"]` |       |\n| `shape`                   | `[\"string\", \"array\", \"number\", \"integer\"]` |       |\n| `point`                   | `[\"string\", \"array\", \"number\", \"integer\"]` |       |",
    "tag": "airbyte"
  },
  {
    "title": "Postmarkapp",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/postmarkapp.md",
    "content": "Postmarkapp\nOverview\nThe Postmarkapp source can sync data from the Postmarkapp API\nRequirements\nPostmarkapp requires an API key to make request and retrieve data. You can find your API key in the Postmarkapp dashboard.\nStreams\nCurrent supported streams: \nServer-API\n- Bounces: Deliverystats: Lets you access all reports regarding your bounces for a specific server. Bounces are available for 45 days after a bounce.\n- Message-Streams: Lets you manage message streams for a specific server. Please note: A Server may have up to 10 Streams, including the default ones. Default Streams cannot be deleted, and Servers can only have 1 Inbound Stream.\n- Outbound stats: Lets you get all of the statistics of your outbound emails for a specific server. These statistics are stored permantently and do not expire. All stats use EST timezone\nAccount-API\n- Servers: Lets you manage servers for a specific account.\n- Domains: Gets a list of domains containing an overview of the domain and authentication status.\n- Sender signatures: Gets a list of sender signatures containing brief details associated with your account.\nSetup guide\nStep 1: Set up the Postmarkapp connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, select Postmarkapp from the Source type dropdown.\nClick Set up source.\n\nFor Airbyte OSS:\n\nNavigate to the Airbyte Open Source dashboard.\nSet the name for your source (Postmarkapp).\nClick Set up source.\n\nSupported sync modes\nThe Postmarkapp source connector supports the following sync modes:\n| Feature           | Supported? |\n| :---------------- | :--------- |\n| Full Refresh Sync | Yes        |\n| Incremental Sync  | No         |\n| Namespaces        | No         |",
    "tag": "airbyte"
  },
  {
    "title": "Public APIs",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/public-apis.md",
    "content": "Public APIs\nSync overview\nThis source can sync data for the Public APIs REST API. It supports only Full Refresh syncs.\nOutput schema\nThis Source is capable of syncing the following Streams:\n\nServices\nCategories\n\nData type mapping\n| Integration Type | Airbyte Type | Notes |\n| :--- | :--- | :--- |\n| `string` | `string` |  |\n| `integer`, `number` | `number` |  |\n| `boolean` | `boolean` |  |\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | No |  |\n| SSL connection | Yes |\n| Namespaces | No |  |\n| Pagination | No |  |\nGetting started\nRequirements\nThere is no requirements to setup this source.\nSetup guide\nThis source requires no setup.",
    "tag": "airbyte"
  },
  {
    "title": "CoinAPI",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/coin-api.md",
    "content": "CoinAPI\nSync overview\nThis source can sync OHLCV and trades historical data for a single coin listed on \nCoinAPI. It currently only supports Full Refresh\nsyncs.\nOutput schema\nThis source is capable of syncing the following streams:\n\n`ohlcv_historical_data`\n`trades_historical_data`\n\nFeatures\n| Feature           | Supported? (Yes/No) | Notes                                                   |\n|:------------------|:----------------------|:--------------------------------------------------------|\n| Full Refresh Sync | Yes                   |                                                         |\n| Incremental Sync  | No                    |                                                         |\n| API Environments  | Yes                   | Both sandbox and production environments are supported. |\nPerformance considerations\nCoinAPI allows only 100 daily requests on the free plan. Use of this connector\nmay require a paid plan.\nGetting started\nRequirements\n\nObtain an API key from CoinAPI.\nChoose a symbol to pull data for. You can find a list of symbols here. \nChoose a time interval to pull data for. You can find a list of intervals here.\n\nSetup guide\nThe following fields are required fields for the connector to work:\n\n`api_key`: Your CoinAPI API key.\n`environment`: The environment to use. Can be either `sandbox` or `production`.\n`symbol_id`: The symbol to pull data for.\n`period`: The time interval to pull data for.\n`start_date`: The start date to pull `history` data from.\n(optional) `end_date`: The end date to pull `history` data until.\n(optional) `limit`: The maximum number of records to pull per request. Defaults to 100.\n",
    "tag": "airbyte"
  },
  {
    "title": "Google Analytics (Universal Analytics)",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/google-analytics-v4.md",
    "content": "Google Analytics (Universal Analytics)\nThis page contains the setup guide and reference information for the Google Analytics (Universal Analytics) source connector.\nThis connector supports Universal Analytics properties through the Reporting API v4.\nSetup guide\n\nFor Airbyte Cloud:\nTo set up Google Analytics as a source in Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click + New source.\nOn the Set up the source page, select Google Analytics from the Source type dropdown.\nFor Name, enter a name for the Google Analytics connector.\nAuthenticate your Google account via OAuth or Service Account Key Authentication.\n(Recommended) To authenticate your Google account via OAuth, click Sign in with Google and complete the authentication workflow.\nTo authenticate your Google account via Service Account Key Authentication, enter your Google Cloud service account key in JSON format. \n\n\nEnter the Replication Start Date in YYYY-MM-DD format. The data added on and after this date will be replicated. If this field is blank, Airbyte will replicate all data.\nEnter the View ID for the Google Analytics View you want to fetch data from.\nLeave Data request time increment in days (Optional) blank or set to 1. For faster syncs, set this value to more than 1 but that might result in the Google Analytics API returning sampled data, potentially causing inaccuracies in the returned results. The maximum allowed value is 364.\n\n\n\nFor Airbyte Open Source:\nTo set up Google Analytics as a source in Airbyte Open Source:\n\nGo to the Google Analytics Reporting API dashboard in the project for your service user and enable the Reporting API for your account. Then go to the Google Analytics API dashboard in the project for your service user and enable the API for your account.\nGo to the Airbyte UI and click Sources and then click + New source.\nOn the Set up the source page, select Google Analytics from the Source type dropdown.\nEnter a name for the Google Analytics connector.\nAuthenticate your Google account via OAuth or Service Account Key Authentication:\nTo authenticate your Google account via OAuth, enter your Google application's client ID, client secret, and refresh token.\nTo authenticate your Google account via Service Account Key Authentication, enter your Google Cloud service account key in JSON format. Use the service account email address to add a user to the Google analytics view you want to access via the API and grant Read and Analyze permissions.\n\n\nEnter the Replication Start Date in YYYY-MM-DD format. The data added on and after this date will be replicated. If this field is blank, Airbyte will replicate all data.\nEnter the View ID for the Google Analytics View you want to fetch data from.\nOptionally, enter a JSON object as a string in the Custom Reports field. For details, refer to Requesting custom reports\nLeave Data request time increment in days (Optional) blank or set to 1. For faster syncs, set this value to more than 1 but that might result in the Google Analytics API returning sampled data, potentially causing inaccuracies in the returned results. The maximum allowed value is 364.\n\n\nSupported sync modes\nThe Google Analytics source connector supports the following sync modes:\n\nFull Refresh - Overwrite\nFull Refresh - Append\nIncremental Sync - Append\nIncremental Sync - Deduped History\n\nSupported streams\nThe Google Analytics (Universal Analytics) source connector can sync the following tables:\n| Stream name              | Schema                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| :----------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| website_overview         | `{\"ga_date\":\"2021-02-11\",\"ga_users\":1,\"ga_newUsers\":0,\"ga_sessions\":9,\"ga_sessionsPerUser\":9.0,\"ga_avgSessionDuration\":28.77777777777778,\"ga_pageviews\":63,\"ga_pageviewsPerSession\":7.0,\"ga_avgTimeOnPage\":4.685185185185185,\"ga_bounceRate\":0.0,\"ga_exitRate\":14.285714285714285,\"view_id\":\"211669975\"}`                                                                                                                                                         |\n| traffic_sources          | `{\"ga_date\":\"2021-02-11\",\"ga_source\":\"(direct)\",\"ga_medium\":\"(none)\",\"ga_socialNetwork\":\"(not set)\",\"ga_users\":1,\"ga_newUsers\":0,\"ga_sessions\":9,\"ga_sessionsPerUser\":9.0,\"ga_avgSessionDuration\":28.77777777777778,\"ga_pageviews\":63,\"ga_pageviewsPerSession\":7.0,\"ga_avgTimeOnPage\":4.685185185185185,\"ga_bounceRate\":0.0,\"ga_exitRate\":14.285714285714285,\"view_id\":\"211669975\"}`                                                                              |\n| pages                    | `{\"ga_date\":\"2021-02-11\",\"ga_hostname\":\"mydemo.com\",\"ga_pagePath\":\"/home5\",\"ga_pageviews\":63,\"ga_uniquePageviews\":9,\"ga_avgTimeOnPage\":4.685185185185185,\"ga_entrances\":9,\"ga_entranceRate\":14.285714285714285,\"ga_bounceRate\":0.0,\"ga_exits\":9,\"ga_exitRate\":14.285714285714285,\"view_id\":\"211669975\"}`                                                                                                                                                          |\n| locations                | `{\"ga_date\":\"2021-02-11\",\"ga_continent\":\"Americas\",\"ga_subContinent\":\"Northern America\",\"ga_country\":\"United States\",\"ga_region\":\"Iowa\",\"ga_metro\":\"Des Moines-Ames IA\",\"ga_city\":\"Des Moines\",\"ga_users\":1,\"ga_newUsers\":0,\"ga_sessions\":1,\"ga_sessionsPerUser\":1.0,\"ga_avgSessionDuration\":29.0,\"ga_pageviews\":7,\"ga_pageviewsPerSession\":7.0,\"ga_avgTimeOnPage\":4.666666666666667,\"ga_bounceRate\":0.0,\"ga_exitRate\":14.285714285714285,\"view_id\":\"211669975\"}` |\n| monthly_active_users     | `{\"ga_date\":\"2021-02-11\",\"ga_30dayUsers\":1,\"view_id\":\"211669975\"}`                                                                                                                                                                                                                                                                                                                                                                                                |\n| four_weekly_active_users | `{\"ga_date\":\"2021-02-11\",\"ga_28dayUsers\":1,\"view_id\":\"211669975\"}`                                                                                                                                                                                                                                                                                                                                                                                                |\n| two_weekly_active_users  | `{\"ga_date\":\"2021-02-11\",\"ga_14dayUsers\":1,\"view_id\":\"211669975\"}`                                                                                                                                                                                                                                                                                                                                                                                                |\n| weekly_active_users      | `{\"ga_date\":\"2021-02-11\",\"ga_7dayUsers\":1,\"view_id\":\"211669975\"}`                                                                                                                                                                                                                                                                                                                                                                                                 |\n| daily_active_users       | `{\"ga_date\":\"2021-02-11\",\"ga_1dayUsers\":1,\"view_id\":\"211669975\"}`                                                                                                                                                                                                                                                                                                                                                                                                 |\n| devices                  | `{\"ga_date\":\"2021-02-11\",\"ga_deviceCategory\":\"desktop\",\"ga_operatingSystem\":\"Macintosh\",\"ga_browser\":\"Chrome\",\"ga_users\":1,\"ga_newUsers\":0,\"ga_sessions\":9,\"ga_sessionsPerUser\":9.0,\"ga_avgSessionDuration\":28.77777777777778,\"ga_pageviews\":63,\"ga_pageviewsPerSession\":7.0,\"ga_avgTimeOnPage\":4.685185185185185,\"ga_bounceRate\":0.0,\"ga_exitRate\":14.285714285714285,\"view_id\":\"211669975\"}`                                                                    |\n| Any custom reports       | See below for details.                                                                                                                                                                                                                                                                                                                                                |\nReach out to us on Slack or create an issue if you need to send custom Google Analytics report data with Airbyte.\nRate Limits and Performance Considerations (Airbyte Open-Source)\nAnalytics Reporting API v4\n\nNumber of requests per day per project: 50,000\nNumber of requests per view (profile) per day: 10,000 (cannot be increased)\nNumber of requests per 100 seconds per project: 2,000\nNumber of requests per 100 seconds per user per project: 100 (can be increased in Google API Console to 1,000).\n\nThe Google Analytics connector should not run into the \"requests per 100 seconds\" limitation under normal usage. Create an issue if you see any rate limit issues that are not automatically retried successfully and try increasing the `window_in_days` value.\nSampled data in reports\nIf you are not on the Google Analytics 360 tier, the Google Analytics API may return sampled data if the amount of data in your Google Analytics account exceeds Google's pre-determined compute thresholds. This means the data returned in the report is an estimate which may have some inaccuracy. This Google page provides a comprehensive overview of how Google applies sampling to your data.  \nIn order to minimize the chances of sampling being applied to your data, Airbyte makes data requests to Google in one day increments (the smallest allowed date increment). This reduces the amount of data the Google API processes per request, thus minimizing the chances of sampling being applied. The downside of requesting data in one day increments is that it increases the time it takes to export your Google Analytics data. If sampling is not a concern, you can override this behavior by setting the optional `window_in_day` parameter to specify the number of days to look back and avoid sampling.\nWhen sampling occurs, a warning is logged to the sync log.\nData processing latency\nAccording to the Google Analytics API documentation, all report data may continue to be updated 48 hours after it appears in the Google Analytics API. This means if you request the same report twice within 48 hours of that data being sent to Google Analytics, the report data might be different across the two requests. This happens when Google Analytics is still processing all events it received.\nWhen this occurs, the returned data will set the flag `isDataGolden` to false. As mentioned in the Google Analytics API docs, the `isDataGolden` flag indicates if [data] is golden or not. Data is golden when the exact same request [for a report] will not produce any new results if asked at a later point in time.\nTo address this issue, the connector adds a lookback window of 2 days to ensure any previously synced non-golden data is re-synced with its potential updates. For example: If your last sync occurred 5 days ago and a sync is initiated today, the connector will attempt to sync data from 7 days ago up to the latest data available.\nTo determine whether data is finished processing or not, the `isDataGolden` flag is exposed and should be used.\nRequesting Custom Reports\nTo replicate Google Analytics Custom Reports using this connector, input a JSON object as a string in the Custom Reports field when setting up the connector. The JSON is an array of objects where each object has the following schema:\n`text\n{\"name\": string, \"dimensions\": [string], \"metrics\": [string]}`\nHere is an example input \"Custom Reports\" field:\n`text\n[{\"name\": \"new_users_per_day\", \"dimensions\": [\"ga:date\",\"ga:country\",\"ga:region\"], \"metrics\": [\"ga:newUsers\"]}, {\"name\": \"users_per_city\", \"dimensions\": [\"ga:city\"], \"metrics\": [\"ga:users\"]}]`\nTo create a list of dimensions, you can use default Google Analytics dimensions (listed below) or custom dimensions if you have some defined. Each report can contain no more than 7 dimensions, and they must all be unique. The default Google Analytics dimensions are:\n\n`ga:browser`\n`ga:city`\n`ga:continent`\n`ga:country`\n`ga:date`\n`ga:deviceCategory`\n`ga:hostname`\n`ga:medium`\n`ga:metro`\n`ga:operatingSystem`\n`ga:pagePath`\n`ga:region`\n`ga:socialNetwork`\n`ga:source`\n`ga:subContinent`\n\nTo create a list of metrics, use a default Google Analytics metric (values from the list below) or custom metrics if you have defined them.\nA custom report can contain no more than 10 unique metrics. The default available Google Analytics metrics are:\n\n`ga:14dayUsers`\n`ga:1dayUsers`\n`ga:28dayUsers`\n`ga:30dayUsers`\n`ga:7dayUsers`\n`ga:avgSessionDuration`\n`ga:avgTimeOnPage`\n`ga:bounceRate`\n`ga:entranceRate`\n`ga:entrances`\n`ga:exitRate`\n`ga:exits`\n`ga:newUsers`\n`ga:pageviews`\n`ga:pageviewsPerSession`\n`ga:sessions`\n`ga:sessionsPerUser`\n`ga:uniquePageviews`\n`ga:users`\n\nIncremental sync is supported only if you add `ga:date` dimension to your custom report.",
    "tag": "airbyte"
  },
  {
    "title": "Oura",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/oura.md",
    "content": "Oura\nSync overview\nThis source can sync various data from the Oura ring.\nIt currently only supports authentication through personal access tokens, and\nis therefore not suitable for syncing data from multiple Oura rings.\nOutput schema\nThis source is capable of syncing the following streams:\n\n`daily_activity`\n`daily_readiness`\n`daily_sleep`\n`heart_rate`\n`sessions`\n`sleep_periods`\n`tags`\n`workouts`\n\nFeatures\n| Feature           | Supported? (Yes/No) | Notes                             |\n|:------------------|:----------------------|:----------------------------------|\n| Full Refresh Sync | Yes                   |                                   |\n| Incremental Sync  | No                    |                                   |\n| Multiple rings    | No                    | May be implemented in the future. |\nPerformance considerations\nThere are no documented rate limits for the Oura V2 API at the time of writing.\nHowever, users must have an up-to-date version of the Oura app installed to use\nthe API.\nGetting started\nRequirements\n\nPurchase an Oura ring.\nCreate a personal access token via the\n   Oura developer portal.\n\nSetup guide\nThe following fields are required fields for the connector to work:\n\n`api_key`: Your Oura API key.\n(optional) `start_datetime`: The start date and time for the sync.\n(optional) `end_datetime`: The end date and time for the sync.\n",
    "tag": "airbyte"
  },
  {
    "title": "Greenhouse",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/greenhouse.md",
    "content": "Greenhouse\nThis page contains the setup guide and reference information for the Greenhouse source connector.\nPrerequisites\nTo set up the Greenhouse source connector, you'll need the Harvest API key with permissions to the resources Airbyte should be able to access.\nSet up the Greenhouse connector in Airbyte\n\nLog into your Airbyte Cloud account or navigate to the Airbyte Open Source dashboard.\nClick Sources and then click + New source.\nOn the Set up the source page, select Greenhouse from the Source type dropdown.\nEnter the name for the Greenhouse connector.\nEnter your Harvest API Key that you obtained from Greenhouse.\nClick Set up source.\n\nSupported sync modes\nThe Greenhouse source connector supports the following sync modes:\n\nFull Refresh - Overwrite\nFull Refresh - Append\nIncremental - Append\nIncremental - Deduped History\n\nSupported Streams\n\nApplications\nApplications Interviews\nCandidates\nClose Reasons\nCustom Fields\nDegrees\nDepartments\nInterviews\nJob Posts\nJob Stages\nJobs\nJob Openings\nJobs Stages\nOffers\nRejection Reasons\nScorecards\nSources\nUsers\n\nPerformance considerations\nThe Greenhouse connector should not run into Greenhouse API limitations under normal usage. Create an issue if you encounter any rate limit issues that are not automatically retried successfully.",
    "tag": "airbyte"
  },
  {
    "title": "Omnisend",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/omnisend.md",
    "content": "Omnisend\nSync overview\nThis source can sync data from the Omnisend API. At present this connector only supports full refresh syncs meaning that each time you use the connector it will sync all available records from scratch. Please use cautiously if you expect your API to have a lot of records.\nThis Source Supports the Following Streams\n\ncontacts\ncampaigns\ncarts\norders\nproducts\n\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | No |  |\nPerformance considerations\nThe connector has a rate limit of 400 requests per 1 minute.\nGetting started\nRequirements\n\nOmnisend API Key\n",
    "tag": "airbyte"
  },
  {
    "title": "Monday",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/monday.md",
    "content": "Monday\nPrerequisites\n\nMonday API Token / Monday Access Token\n\nYou can find your Oauth application in Monday main page -> Profile picture (bottom left corner) -> Developers -> My Apps -> Select your app.\nYou can get the API token for Monday by going to Profile picture (bottom left corner) -> Admin -> API.\nSetup guide\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the Monday connector and select Monday from the Source type dropdown.\nFill in your API Key or authenticate using OAuth and then click Set up source.\n\nConnect using `OAuth 2.0` option:\n\nSelect `OAuth2.0` in `Authorization Method`.\nClick on `authenticate your Monday account`.\nProceed the authentication using your credentials for your Monday account.\n\nConnect using `API Token` option:\n\nGenerate an API Token as described here.\nUse the generated `api_token` in the Airbyte connection.\n\nSupported sync modes\nThe Monday supports full refresh syncs\n| Feature           | Supported? |\n|:------------------|:-----------|\n| Full Refresh Sync | Yes        |\n| Incremental Sync  | No         |\n| SSL connection    | No         |\n| Namespaces        | No         |\nSupported Streams\nSeveral output streams are available from this source:\n\nItems\nBoards\nTeams\nUpdates\nUsers\n\nIf there are more endpoints you'd like Airbyte to support, please create an issue.\nPerformance considerations\nThe Monday connector should not run into Monday API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.",
    "tag": "airbyte"
  },
  {
    "title": "Pinterest",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/pinterest.md",
    "content": "Pinterest\nThis page contains the setup guide and reference information for the Pinterest source connector.\nPrerequisites\nTo set up the Pinterest source connector with Airbyte Open Source, you'll need your Pinterest App ID and secret key and the refresh token.\nSetup guide\n\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nClick Sources and then click + New source.\nOn the Set up the source page, select Pinterest from the Source type dropdown.\nEnter the name for the Pinterest connector.\nFor Start Date, enter the date in YYYY-MM-DD format. The data added on and after this date will be replicated. If this field is blank, Airbyte will replicate all data. As per Pinterest API restriction, the date cannot be more than 914 days in the past.\nThe OAuth2.0 authorization method is selected by default. Click Authenticate your Pinterest account. Log in and authorize your Pinterest account.\nClick Set up source.\n\n\n\nFor Airbyte Open Source:\n\nNavigate to the Airbyte Open Source dashboard.\nClick Sources and then click + New source.\nOn the Set up the source page, select Pinterest from the Source type dropdown.\nEnter the name for the Pinterest connector.\nFor Start Date, enter the date in YYYY-MM-DD format. The data added on and after this date will be replicated. If this field is blank, Airbyte will replicate all data. As per Pinterest API restriction, the date cannot be more than 914 days in the past.\nThe OAuth2.0 authorization method is selected by default. For Client ID and Client Secret, enter your Pinterest App ID and secret key. For Refresh Token, enter your Pinterest Refresh Token.\nClick Set up source.\n\n\nSupported sync modes\nThe Pinterest source connector supports the following sync modes:\n\nFull Refresh - Overwrite\nFull Refresh - Append\nIncremental - Append\nIncremental - Deduped History\n\nSupported Streams\n\nAccount analytics (Incremental)\nBoards (Full table)\nBoard sections (Full table)\nPins on board section (Full table)\n\n\nPins on board (Full table)\nAd accounts (Full table)\nAd account analytics (Incremental)\nCampaigns (Incremental)\nCampaign analytics (Incremental)\n\n\nAd groups (Incremental)\nAd group analytics (Incremental)\n\n\nAds (Incremental)\nAd analytics (Incremental)\n\n\n\nPerformance considerations\nThe connector is restricted by the Pinterest requests limitation.\nRate Limits\n\nAnalytics streams: 300 calls per day / per user \\\nAd accounts streams (Campaigns, Ad groups, Ads): 1000 calls per min / per user / per app \\\nBoards streams: 10 calls per sec / per user / per app\n",
    "tag": "airbyte"
  },
  {
    "title": "Reply.io",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/reply-io.md",
    "content": "Reply.io\nSync overview\nThe Reply.io source supports both Full Refresh only.\nThis source can sync data for the Reply.io API.\nOutput schema\nThis Source is capable of syncing the following core Streams:\n\nblacklist\ncampaigns\nemail_accounts\npeople\ntemplates\n\nFeatures\n| Feature                   | Supported?(Yes/No) | Notes |\n| :------------------------ | :------------------- | :---- |\n| Full Refresh Sync         | Yes                  |       |\n| Incremental - Append Sync | No                   |       |\n| Namespaces                | No                   |       |\nPerformance considerations\n\nEach Reply user has a limit of 15000 API calls per month.\nThe time limit between API calls makes 10 seconds.\nThe limit for syncing contacts using native integrations is the same as the limit for the number of contacts in your Reply account.\n\nRequirements\n\nReply.io API key. See the Reply.io docs for information on how to obtain an API key.\n",
    "tag": "airbyte"
  },
  {
    "title": "Adjust",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/adjust.md",
    "content": "Adjust\nThis is a setup guide for the Adjust source connector which ingests data from the reports API.\nPrerequisites\nAn API token is required to get hold of reports from the Adjust reporting API. See the Adjust API authentication help article on how to obtain a key.\nAs Adjust allows you to setup custom events etc that are specific to your apps, only a subset of available metrics are made pre-selectable. To list all metrics that are available, query the filters data endpoint. Information about available metrics are available in the Datascape metrics glossary.\nFull Metrics Listing\nTake a look at the filters data endpoint documentation to see available filters. The example below shows how to obtain the events that are defined for your apps (replace the `API_KEY` with the key obtained in the previous step):\n`sh\ncurl --header 'Authorization: Bearer API_KEY' 'https://dash.adjust.com/control-center/reports-service/filters_data?required_filters=event_metrics' | jq`\nSet up the Adjust source connector\n\nClick Sources and then click + New source.\nOn the Set up the source page, select Adjust from the Source type dropdown.\nEnter a name for your new source.\nFor API Token, enter your API key obtained in the previous step.\nFor Ingestion Start Date, enter a date in YYYY-MM-DD format (UTC timezone is assumed). Data starting from this date will be replicated.\nIn the Metrics to Ingest field, select the metrics of interest to query.\nEnter any additional, custom metrics, to query in the Additional Metrics box. Available metrics can be listed as described in the Prerequisites section. These selected metrics are assumed to be decimal values.\nIn the Dimensions field, select the dimensions to group metrics by.\nClick Set up source.\n\nSupported sync modes\nThe source connector supports the following sync modes:\n\nFull Refresh\nIncremental\n",
    "tag": "airbyte"
  },
  {
    "title": "Xero",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/xero.md",
    "content": "Xero\nThis is a setup guide for the Xero source connector which ingests data from the Accounting API.\nPrerequisites\nFirst of all you should create an application in Xero development center. The only supported integration type is to use Xero Custom Connections so you should choose it on creating your Xero App.\nAfter creating an application, on configuration screen, authorize user for your Xero Organisation. Also, issue new Client Secret and remember it - it will be required for setting up Xero connector in your Airbyte instance.\nSupported streams\nAccounts\nBankTransactions\nBankTransfers\nBrandingThemes\nContactGroups\nContacts\nCreditNotes\nCurrencies\nEmployees\nInvoices\nItems\nManualJournals\nOrganisation\nOverpayments\nPayments\nPrepayments\nPurchaseOrders\nRepeatingInvoices\nTaxRates\nTrackingCategories\nUsers\nDates transformation\nAs Xero uses .NET, some date fields in records could be in .NET JSON date format. These dates are transformed into ISO 8601.\nSet up the Xero source connector\n\nClick Sources and then click + New source.\nOn the Set up the source page, select Xerp from the Source type dropdown.\nEnter a name for your new source.\nFor Client ID, enter Client ID of your Xero App.\nFor Client Secret, enter a Client Secret created on \"Configuration\" screen of your Xero App\nFor Tenant ID field, enter your Xero Organisation's Tenant ID\nFor Scopes field enter scopes you used for user's authorization on \"Configuration\" screen of your Xero App\nChoose Custom Connections Authentication as Authentication option\nFor Start date enter UTC date and time in the format `YYYY-MM-DDTHH:mm:ssZ` as the start date and time of ingestion.\nClick Set up source.\n\nSupported sync modes\nThe source connector supports the following sync modes:\n\nFull Refresh\nIncremental\n",
    "tag": "airbyte"
  },
  {
    "title": "Weatherstack",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/weatherstack.md",
    "content": "Weatherstack\nOverview\nThis source connector syncs data from the Weatherstack API. This API allows to obtain current, historical, location lookup, and weather forecast.\nOutput schema\nThis source currently has four streams: `current`, `historical`, `forecast`, and `autocomplete`. The Current Weather API is available on all plans. The Historical Weather and Autocomplete API's are available on the standard plan and higher. The Forecast API is available on the Professional plan and higher. Examples of the data outputted by this stream are available here.\nFeatures\n| Feature | Supported? |\n| :--- | :--- |\n| Full Refresh Sync - (append only) | Yes |\n| Incremental - Append Sync | Yes |\n| Namespaces | No |\nGetting started\nRequirements\n\nAn Weatherstack API key\nA city or zip code location for which you want to get weather data\nA historical date to enable the api stream to gather data for a specific date\n\nSetup guide\nVisit the Wetherstack to create a user account and obtain an API key. The current and forecast streams are available with the free plan.\nRate limiting\nThe free plan allows 250 calls per month, you won't get beyond these limits with existing Airbyte's sync frequencies.",
    "tag": "airbyte"
  },
  {
    "title": "Braintree",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/braintree.md",
    "content": "Braintree\nSync overview\nThis source can sync data for the Braintree API. It supports both Full Refresh and Incremental syncs. You can choose if this connector will copy only the new or updated data, or all rows in the tables and columns you set up for replication, every time a sync is run.\nOutput schema\nThis Source is capable of syncing the following core Streams:\n\nCustomers\nDiscounts\nDisputes\nTransactions\nMerchant Accounts\nPlans\nSubscriptions\n\nData type mapping\n| Integration Type | Airbyte Type | Notes |\n| :--- | :--- | :--- |\n| `string` | `string` |  |\n| `number` | `number` |  |\n| `array` | `array` |  |\n| `object` | `object` |  |\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | Yes |  |\n| Namespaces | No |  |\nPerformance considerations\nThe connector is restricted by normal Braintree requests limitation on search transactions.\nThe Braintree connector should not run into Braintree API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.\nGetting started\nRequirements\n\nBraintree Merchant ID \nBraintree Public Key \nBraintree Private Key \nEnvironment \n\nSetup guide\nGenerate all requirements using the Braintree documentation.\nWe recommend creating a restricted, read-only key specifically for Airbyte access. This will allow you to control which resources Airbyte should be able to access.",
    "tag": "airbyte"
  },
  {
    "title": "Exchange Rates API",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/exchange-rates.md",
    "content": "Exchange Rates API\nOverview\nThe exchange rates integration is a toy integration to demonstrate how Airbyte works with a very simple source.\nIt pulls all its data from https://exchangeratesapi.io\nOutput schema\nIt contains one stream: `exchange_rates`\nEach record in the stream contains many fields:\n\nThe date of the record\nOne field for every supported currency which contain the value of that currency on that date.\n\nData type mapping\nCurrencies are `number` and the date is a `string`.\nFeatures\n| Feature                   | Supported? |\n| :------------------------ | :--------- |\n| Full Refresh Sync         | Yes        |\n| Incremental - Append Sync | Yes        |\n| Namespaces                | No         |\nGetting started\nRequirements\n\nAPI Access Key\n\nSetup guide\nIn order to get an `API Access Key` please go to this page and enter needed info. After registration and login you will see your `API Access Key`, also you may find it here.\nIf you have `free` subscription plan (you may check it here) this means that you will have 2 limitations:\n\n1000 API calls per month.\nYou won't be able to specify the `base` parameter, meaning that you will be dealing only with default base value which is EUR.\n",
    "tag": "airbyte"
  },
  {
    "title": "Outreach",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/outreach.md",
    "content": "Outreach\nOverview\nThe Outreach source supports both `Full Refresh` and `Incremental` syncs. You can choose if this connector will copy only the new or updated data, or all rows in the tables and columns you set up for replication, every time a sync is run.\nOutput schema\nSome output streams are available from this source. A list of these streams can be found below in the Streams section.\nFeatures\n| Feature | Supported? |\n| :--- | :--- |\n| Full Refresh Sync | Yes |\n| Incremental Sync | Yes |\n| SSL connection | Yes |\n| Namespaces | No |\nGetting started\nRequirements\n\nOutreach Account\nOutreach OAuth credentials\n\nSetup guide\nGetting oauth credentials require contacting Outreach to request an account. Check out here.\nStreams\nList of available streams:\n\nProspects\nSequences\nSequenceStates\n",
    "tag": "airbyte"
  },
  {
    "title": "Strava",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/strava.md",
    "content": "Strava\nOverview\nThe Strava source can sync data from the Strava API\nUseful links:\n\nGetting Started with the Strava API \nStrava API v3 API and SDK Reference\nAuthentication\n\nOutput schema\nThis Source is capable of syncing the following Streams:\n\nAthlete Stats\nActivities (Incremental)\n\nFeatures\n| Feature | Supported? |\n| :--- | :--- |\n| Full Refresh Sync | Yes |\n| Incremental - Append Sync | Yes |\n| Namespaces | No |\nRequirements\n\nclient_id - Strava account client ID\nclient_secret - Strava account client secret\nrefresh_token - Strava account refresh token \nathlete_id - Strava athlete ID (only used for Athlete Stats)\nquery_start_timestamp - Starting timestamp for listing activities (only used for Activities)\n\nSetup guide\nFollow these steps to get the required credentials and inputs:\n* `client_id` and `client_secret`\n    * Create a Strava account\n    * Continue to follow the instructions from the doc above to obtain `client_id` and `client_secret`\n* `refresh_token`\n    * Enter this URL into your browser (make sure to add your `client_id` from previous step:\n        * `http://www.strava.com/oauth/authorize?client_id=[REPLACE_WITH_YOUR_CLIENT_ID]&response_type=code&redirect_uri=http://localhost/exchange_token&approval_prompt=force&scope=activity:read_all`\n    * Authorize through the UI\n    * Browser will redirect you to an empty page with a URL similar to `http://localhost/exchange_token?state=&code=b55003496d87a9f0b694ca1680cd5690d27d9d28&scope=activity:read_all`\n    * Copy the authorization code above (in this example it would be `b55003496d87a9f0b694ca1680cd5690d27d9d28`)\n    * Make a cURL request to exchange the authorization code and scope for a refresh token:\n    * `curl -X POST https://www.strava.com/oauth/token \\\n      -F client_id=YOUR_CLIENT_ID \\\n      -F client_secret=YOUR_CLIENT_SECRET \\\n      -F code=AUTHORIZATION_CODE \\\n      -F grant_type=authorization_code`\n    * The resulting json will contain the `refresh_token`\n    * Example Result:\n    * `{\n            \"token_type\": \"Bearer\",\n            \"expires_at\": 1562908002,\n            \"expires_in\": 21600,\n            \"refresh_token\": \"REFRESHTOKEN\",\n            \"access_token\": \"ACCESSTOKEN\",\n            \"athlete\": {\n                \"id\": 123456,\n                \"username\": \"MeowTheCat\",\n                \"resource_state\": 2,\n                \"firstname\": \"Meow\",\n                \"lastname\": \"TheCat\",\n                \"city\": \"\",\n                \"state\": \"\",\n                \"country\": null,\n                ...\n            }\n        }`\n    * Refer to Strava's Getting Started - Oauth or Authentication documents for more information\n* `athlete_id`\n    * Go to your athlete page by clicking your name on the Strava dashboard or click on \"My Profile\" on the drop down after hovering on your top bar icon\n    * The number at the end of the url will be your `athlete_id`. For example `17831421` would be the `athlete_id` for https://www.strava.com/athletes/17831421\nPerformance considerations\nStrava API has limitations to 100 requests every 15 minutes, 1000 daily.\nMore information about Strava rate limits and adjustments to those limits can be found here.",
    "tag": "airbyte"
  },
  {
    "title": "Drift",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/drift.md",
    "content": "Drift\nOverview\nThe Drift source supports Full Refresh syncs. That is, every time a sync is run, Airbyte will copy all rows in the tables and columns you set up for replication into the destination in a new table.\nOutput schema\nSeveral output streams are available from this source:\n\nAccounts\nConversations\nUsers\n\nIf there are more endpoints you'd like Airbyte to support, please create an issue.\nFeatures\n| Feature | Supported? |\n| :--- | :--- |\n| Full Refresh Sync | Yes |\n| Incremental Sync | Coming soon |\n| Replicate Incremental Deletes | Coming soon |\n| SSL connection | Yes |\n| Namespaces | No |\nPerformance considerations\nThe Drift connector should not run into Drift API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.\nGetting started\nRequirements\n\nA Drift API token linked to a Drift App with the following scopes: \n`conversation_read` to access Conversions\n`user_read` to access Users\n`account_read` to access Accounts\n\nSetup guide\nAuthenticate using `Access Token`\n\nFollow Drift's Setting Things Up guide for a more detailed description of how to obtain the API token.\n\nAuthenticate using `OAuth2.0`\n\nSelect `OAuth2.0` from `Authorization Method` dropdown\nClick on `Authenticate your Drift account`\nProceed the authentication in order to obtain the `access_token`\n",
    "tag": "airbyte"
  },
  {
    "title": "Baton",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/baton.md",
    "content": "Baton\nSync overview\nThis source can sync data from the baton API. At present this connector only supports full refresh syncs meaning that each time you use the connector it will sync all available records from scratch. Please use cautiously if you expect your API to have a lot of records.\nThis Source Supports the Following Streams\n\nactivity\ncompanies\nmilestones\nphases\nproject_attachments\nprojects\ntask_attachemnts\ntasks\ntemplates\ntime_entries\nusers\n\nBaton adds new streams fairly regularly please submit an issue or PR if this project doesn't support required streams for your use case.\nData type mapping\n| Integration Type | Airbyte Type | Notes |\n| :--------------- | :----------- | :---- |\n| `string`         | `string`     |       |\n| `integer`        | `integer`    |       |\n| `number`         | `number`     |       |\n| `array`          | `array`      |       |\n| `object`         | `object`     |       |\nFeatures\n| Feature           | Supported?(Yes/No) | Notes |\n| :---------------- | :------------------- | :---- |\n| Full Refresh Sync | Yes                  |       |\n| Incremental Sync  | No                   |       |\n| Namespaces        | No                   |       |\nPerformance considerations\nThe connector is rate limited at 1000 requests per minute per api key. If you find yourself receiving errors contact your customer success manager and request a rate limit increase.\nGetting started\nRequirements\n\nBaton account\nBaton api key\n",
    "tag": "airbyte"
  },
  {
    "title": "Qualaroo",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/qualaroo.md",
    "content": "Qualaroo\nOverview\nThe Qualaroo source supports Full Refresh syncs. You can choose if this connector will copy only the new or updated data, or all rows in the tables and columns you set up for replication, every time a sync is run.\nThis Source Connector is based on a Airbyte CDK.\nOutput schema\nSeveral output streams are available from this source:\n\nSurveys (Full table)\nResponses (Full table)\n\nIf there are more endpoints you'd like Airbyte to support, please create an issue.\nFeatures\n| Feature | Supported? |\n| :--- | :--- |\n| Full Refresh Sync | Yes |\n| Incremental - Append Sync | NO |\n| SSL connection | Yes |\n| Namespaces | No |\nPerformance considerations\nThe connector is not yet restricted by normal requests limitation. As a result, the Qualaroo connector might run into API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.\nGetting started\nRequirements\n\nQualaroo API Key\nQualaroo API Token\n\nSetup guide\n\nPlease read How to get your APIs Token and Key or you can log in to Qualaroo and visit Reporting API.",
    "tag": "airbyte"
  },
  {
    "title": "BigQuery",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/bigquery.md",
    "content": "\ndescription: >-\n  BigQuery is a serverless, highly scalable, and cost-effective data warehouse\n  offered by Google Cloud Provider.\n\nBigQuery\nOverview\nThe BigQuery source supports both Full Refresh and Incremental syncs. You can choose if this connector will copy only the new or updated data, or all rows in the tables and columns you set up for replication, every time a sync is running.\nResulting schema\nThe BigQuery source does not alter the schema present in your database. Depending on the destination connected to this source, however, the schema may be altered. See the destination's documentation for more details.\nData type mapping\nThe BigQuery data types mapping:\n| BigQuery Type | Resulting Type | Notes |\n| :--- | :--- | :--- |\n| `BOOL` | Boolean |  |\n| `INT64` | Number |  |\n| `FLOAT64` | Number |  |\n| `NUMERIC` | Number |  |\n| `BIGNUMERIC` | Number |  |\n| `STRING` | String |  |\n| `BYTES` | String |  |\n| `DATE` | String | In ISO8601 format |\n| `DATETIME` | String | In ISO8601 format |\n| `TIMESTAMP` | String | In ISO8601 format |\n| `TIME` | String |  |\n| `ARRAY` | Array |  |\n| `STRUCT` | Object |  |\n| `GEOGRAPHY` | String |  |\nFeatures\n| Feature | Supported | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | Yes |  |\n| Change Data Capture | No |  |\n| SSL Support | Yes |  |\nGetting started\nRequirements\nTo use the BigQuery source, you'll need:\n\nA Google Cloud Project with BigQuery enabled\nA Google Cloud Service Account with the \"BigQuery User\" and \"BigQuery Data Editor\" roles in your GCP project\nA Service Account Key to authenticate into your Service Account\n\nSee the setup guide for more information about how to create the required resources.\nService account\nIn order for Airbyte to sync data from BigQuery, it needs credentials for a Service Account with the \"BigQuery User\" and \"BigQuery Data Editor\" roles, which grants permissions to run BigQuery jobs, write to BigQuery Datasets, and read table metadata. We highly recommend that this Service Account is exclusive to Airbyte for ease of permissioning and auditing. However, you can use a pre-existing Service Account if you already have one with the correct permissions.\nThe easiest way to create a Service Account is to follow GCP's guide for Creating a Service Account. Once you've created the Service Account, make sure to keep its ID handy as you will need to reference it when granting roles. Service Account IDs typically take the form `<account-name>@<project-name>.iam.gserviceaccount.com`\nThen, add the service account as a Member in your Google Cloud Project with the \"BigQuery User\" role. To do this, follow the instructions for Granting Access in the Google documentation. The email address of the member you are adding is the same as the Service Account ID you just created.\nAt this point you should have a service account with the \"BigQuery User\" project-level permission.\nService account key\nService Account Keys are used to authenticate as Google Service Accounts. For Airbyte to leverage the permissions you granted to the Service Account in the previous step, you'll need to provide its Service Account Keys. See the Google documentation for more information about Keys.\nFollow the Creating and Managing Service Account Keys guide to create a key. Airbyte currently supports JSON Keys only, so make sure you create your key in that format. As soon as you created the key, make sure to download it, as that is the only time Google will allow you to see its contents. Once you've successfully configured BigQuery as a source in Airbyte, delete this key from your computer.\nSetup the BigQuery source in Airbyte\nYou should now have all the requirements needed to configure BigQuery as a source in the UI. You'll need the following information to configure the BigQuery source:\n\nProject ID\nDefault Dataset ID [Optional]: the schema name if only one schema is interested. Dramatically boost source discover operation.\nCredentials JSON: the contents of your Service Account Key JSON file\n\nOnce you've configured BigQuery as a source, delete the Service Account Key from your computer.",
    "tag": "airbyte"
  },
  {
    "title": "End-to-End Testing Source",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/e2e-test.md",
    "content": "End-to-End Testing Source\nOverview\nThis is a mock source for testing the Airbyte pipeline. It can generate arbitrary data streams.\nMode\nContinuous Feed\nThis is the only mode available starting from `2.0.0`.\nThis mode allows users to specify a single-stream or multi-stream catalog with arbitrary schema. The schema should be compliant with Json schema draft-07.\nThe single-stream catalog config exists just for convenient, since in many testing cases, one stream is enough. If only one stream is specified in the multi-stream catalog config, it is equivalent to a single-stream catalog config.\nHere is its configuration:\n| Mock Catalog Type | Parameters          | Type    | Required | Default             | Notes                                                                                                                                                   |\n| ----------------- | ------------------- | ------- | -------- | ------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| Single-stream     | stream name         | string  | yes      |                     | Name of the stream in the catalog.                                                                                                                      |\n|                   | stream schema       | json    | yes      |                     | Json schema of the stream in the catalog. It must be a valid Json schema.                                                                               |\n|                   | stream duplication  | integer | no       | 1                   | Duplicate the stream N times to quickly create a multi-stream catalog.                                                                                  |\n| Multi-stream      | streams and schemas | json    | yes      |                     | A Json object specifying multiple data streams and their schemas. Each key in this object is one stream name. Each value is the schema for that stream. |\n| Both              | max records         | integer | yes      | 100                 | The number of record messages to emit from this connector. Min 1. Max 100 billion.                                                                      |\n|                   | random seed         | integer | no       | current time millis | The seed is used in random Json object generation. Min 0. Max 1 million.                                                                                |\n|                   | message interval    | integer | no       | 0                   | The time interval between messages in millisecond. Min 0 ms. Max 60000 ms (1 minute).                                                                   |\nLegacy Infinite Feed\nThis is a legacy mode used in Airbyte integration tests. It has been removed since `2.0.0`. It has a simple catalog with one `data` stream that has the following schema:\n`json\n{\n  \"type\": \"object\",\n  \"properties\":\n    {\n      \"column1\": { \"type\": \"string\" }\n    }\n}`\nThe value of `column1` will be an increasing number starting from `1`.\nThis mode can generate infinite number of records, which can be dangerous. That's why it is excluded from the Cloud variant of this connector. Usually this mode should not be used.\nThere are two configurable parameters:\n| Parameters       | Type    | Required | Default | Notes                                                                                                              |\n| ---------------- | ------- | -------- | ------- | ------------------------------------------------------------------------------------------------------------------ |\n| max records      | integer | no       | `null`  | Number of message records to emit. When it is left empty, the connector will generate infinite number of messages. |\n| message interval | integer | no       | `null`  | Time interval between messages in millisecond.                                                                     |\nException after N\nThis is a legacy mode used in Airbyte integration tests. It has been removed since `2.0.0`. It throws an `IllegalStateException` after certain number of messages. The number of messages to emit before exception is the only parameter for this mode.\nThis mode is also excluded from the Cloud variant of this connector.",
    "tag": "airbyte"
  },
  {
    "title": "US Census API",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/us-census.md",
    "content": "US Census API\nOverview\nThis connector syncs data from the US Census API\nOutput schema\nThis source always outputs a single stream, `us_census_stream`. The output of the stream depends on the configuration of the connector.\nFeatures\n| Feature           | Supported? |\n| :---------------- | :--------- |\n| Full Refresh Sync | Yes        |\n| Incremental Sync  | No         |\n| SSL connection    | Yes        |\n| Namespaces        | No         |\nGetting started\nRequirements\n\nUS Census API key\nUS Census dataset path & query parameters\n\nSetup guide\nVisit the US Census API page to obtain an API key.\nIn addition, to understand how to configure the dataset path and query parameters, follow the guide and examples in the API documentation. Some particularly helpful pages:\n\nAvailable Datasets\nCore Concepts\nExample Queries\n",
    "tag": "airbyte"
  },
  {
    "title": "CoinGecko Coins",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/coingecko-coins.md",
    "content": "CoinGecko Coins\nSync overview\nThis source can sync market chart and historical data for a single coin listed on the\nCoinGecko API. It currently supports only Full Refresh syncing.\nOutput schema\nThis source is capable of syncing the following streams:\n\n`market_chart`\n`history`\n\nFeatures\n| Feature           | Supported? (Yes/No) | Notes                                                  |\n|:------------------|:----------------------|:-------------------------------------------------------|\n| Full Refresh Sync | Yes                   |                                                        |\n| Incremental Sync  | No                    |                                                        |\n| CoinGecko Pro API | Yes                   | Will default to free API unless an API key is provided |\nPerformance considerations\nThe CoinGecko API has a rate limit of 10-50 requests per minute. The connector should not run into this\nunder normal operation.\nCoinGecko also request that free users provide attribution when using CoinGecko data. Please read more about\nthis here.\nGetting started\nRequirements\n\nChoose a coin to pull data from. The coin must be listed on CoinGecko, and can be listed via the `/coins/list` endpoint.\nChoose a `vs_currency` to pull data in. This can be any currency listed on CoinGecko, and can be listed via the `/simple/supported_vs_currencies` endpoint.\n\nSetup guide\nThe following fields are required fields for the connector to work:\n\n`coin_id`: The ID of the coin to pull data for. This can be found via the `/coins/list` endpoint.\n`vs_currency`: The currency to pull data for. This can be found via the `/simple/supported_vs_currencies` endpoint.\n`days`: The number of days to pull `market_chart` data for.\n`start_date`: The start date to pull `history` data from.\n(optional) `end_date`: The end date to pull `history` data until.\n",
    "tag": "airbyte"
  },
  {
    "title": "Hubplanner",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/hubplanner.md",
    "content": "Hubplanner\nHubplanner is a tool to plan, schedule, report and manage your entire team.\nPrerequisites\n\nCreate the API Key to access your data in Hubplanner.\n\nAirbyte Open Source\n\nAPI Key\n\nAirbyte Cloud\n\nComming Soon.\n\nSetup guide\nFor Airbyte Open Source:\n\nAccess https://your-domain.hubplanner.com/settings#api or access the panel in left side Integrations/Hub Planner API\nClick in Generate Key\n\nSupported sync modes\nThe Okta source connector supports the following sync modes:\n - Full Refresh\nSupported Streams\n\nBilling Rates\nBookings\nClients\nEvents\nHolidays\nProjects\nResources\n",
    "tag": "airbyte"
  },
  {
    "title": "Ip2whois API",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/ip2whois.md",
    "content": "Ip2whois API\nSync overview\nThis source can sync data from the Ip2whois API. At present this connector only supports full refresh syncs meaning that each time you use the connector it will sync all available records from scratch.\nThis Source Supports the Following Streams\n\nwhois\n\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | No |  |\nPerformance considerations\nIp2whois APIs allows you to query up to 500 WHOIS domain name per month.\nGetting started\nRequirements\n\nAPI token\n",
    "tag": "airbyte"
  },
  {
    "title": "Prerequisite",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/google-analytics-v4.inapp.md",
    "content": "Prerequisite\n\nAdministrator access to a Google Analytics 4 (GA4) property\n\nSetup guide\n\nClick Authenticate your account by selecting Oauth (recommended).\nIf you select Service Account Key Authentication, follow the instructions in our full documentation.\nLog in and Authorize the Google Analytics account.\nEnter your Property ID\nEnter the Start Date from which to replicate report data in the format YYYY-MM-DD.\n(Optional) Airbyte generates 8 default reports. To add more reports, you need to add Custom Reports as a JSON array describing the custom reports you want to sync from Google Analytics. See below for more information.\n(Optional) Enter the Data request time increment in days. The bigger this value is, the faster the sync will be, but the more likely that sampling will be applied to your data, potentially causing inaccuracies in the returned results. We recommend setting this to 1 unless you have a hard requirement to make the sync faster at the expense of accuracy. The minimum allowed value for this field is 1, and the maximum is 364.\n\n(Optional) Custom reports\n\nCustom reports in format `[{\"name\": \"<report-name>\", \"dimensions\": [\"<dimension-name>\", ...], \"metrics\": [\"<metric-name>\", ...]}]`\nCustom report format when using segments and / or filters `[{\"name\": \"<report-name>\", \"dimensions\": [\"<dimension-name>\", ...], \"metrics\": [\"<metric-name>\", ...], \"segments\":  [\"<segment-id-or-dynamic-segment-v3-format]\", filter: \"<filter-definition-v3-format>\"}]`\nWhen using segments, make sure you add the `ga:segment` dimension.\nCustom reports: Dimensions and metrics explorer\n",
    "tag": "airbyte"
  },
  {
    "title": "WooCommerce",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/woocommerce.md",
    "content": "WooCommerce\nThis page contains the setup guide and reference information for the WooCommerce source connector.\nPrerequisites\nTo set up the WooCommerce source connector with Airbyte, you must be using:\n\nWooCommerce 3.5+\nWordPress 4.4+\nPretty permalinks in `Settings > Permalinks` so that the custom endpoints are supported.\n  e.g. `/%year%/%monthnum%/%day%/%postname%/`\n\nYou will need to generate new API key with read permissions and use `Customer key` and `Customer Secret`.\nSetup guide\nStep 1: Set up WooCommerce\n\nGenerate new Rest API key\nObtain `Customer key` and `Customer Secret`.\n\nStep 2: Set up the WooCommerce connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click + New source.\nOn the Set up the source page, enter the name for the WooCommerce connector and select WooCommerce from the Source\n   type dropdown.\nFill in `Customer key` and `Customer Secret` with data from Step 1 of this guide.\nFill in `Shop Name`. For `https://EXAMPLE.com`, the shop name is 'EXAMPLE.com'.\nChoose start date you want to start sync from.\n(Optional) Fill in Conversion Window.\n\nFor Airbyte OSS:\n\nNavigate to the Airbyte Open Source dashboard.\nSet the name for your source.\nOn the Set up the source page, enter the name for the WooCommerce connector and select WooCommerce from the Source\n   type dropdown.\nFill in `Customer key` and `Customer Secret` with data from Step 1 of this guide.\nFill in `Shop Name`. For `https://EXAMPLE.com`, the shop name is 'EXAMPLE.com'.\nChoose start date you want to start sync from.\n(Optional) Fill in Conversion Window.\n\nSupported sync modes\nThe WooCommerce source connector supports the\nfollowing sync modes:\n\nFull Refresh - Overwrite\nFull Refresh - Append\nIncremental - Append\nIncremental - Deduped History\n\nSupported Streams\n\nCoupons (Incremental)\nCustomers (Incremental)\norders (Incremental)\nOrder notes\nPayment gateways\nProduct attribute terms\nProduct attributes\nProduct categories\nProduct reviews (Incremental)\nProduct shipping classes\nProduct tags\nProduct variations\nProducts (Incremental)\nRefunds\nShipping methods\nShipping zone locations\nShipping zone methods\nShipping zones\nSystem status tools\nTax classes\nTax rates\n\nConnector-specific features & highlights\nUseful links:\n\nWooCommerce Rest API Docs\n\nData type map\n| Integration Type | Airbyte Type | Notes |\n| :--------------- | :----------- | :---- |\n| `string`         | `string`     |       |\n| `integer`        | `integer`    |       |\n| `number`         | `number`     |       |\n| `array`          | `array`      |       |\n| `object`         | `object`     |       |\n| `boolean`        | `boolean`    |       |",
    "tag": "airbyte"
  },
  {
    "title": "Waiteraid",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/waiteraid.md",
    "content": "Waiteraid\nThis page contains the setup guide and reference information for the Waiteraid source connector.\nPrerequisites\nYou can find or create authentication tokens within Waiteraid.\nSetup guide\nStep 1: Set up the Waiteraid connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the Waiteraid connector and select Waiteraid from the Source type dropdown.\nEnter your `auth_token` - Waiteraid Authentication Token.\nEnter your `restaurant ID` - The Waiteraid ID of the Restaurant you wanto sync. \nClick Set up source.\n\nFor Airbyte OSS:\n\nNavigate to the Airbyte Open Source dashboard.\nSet the name for your source. \nEnter your `auth_token` - Waiteraid Authentication Token.\nEnter your `restaurant ID` - The Waiteraid ID of the Restaurant you wanto sync. \nClick Set up source.\n\nSupported sync modes\nThe Waiteraid source connector supports the following sync modes:\n| Feature           | Supported? |\n| :---------------- | :--------- |\n| Full Refresh Sync | Yes        |\n| Incremental Sync  | No         |\n| SSL connection    | No         |\n| Namespaces        | No         |\nSupported Streams\n\nBookings\n\nData type map\n| Integration Type    | Airbyte Type |\n| :------------------ | :----------- |\n| `string`            | `string`     |\n| `integer`, `number` | `number`     |\n| `array`             | `array`      |\n| `object`            | `object`     |",
    "tag": "airbyte"
  },
  {
    "title": "Appfollow",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/appfollow.md",
    "content": "Appfollow\nThis page guides you through setting up the Appfollow source connector to sync data for the Appfollow API.\nPrerequisite\nTo set up the Appfollow source connector, you'll need your Appfollow `ext_id`, `cid`, `api_secret` and `Country`.\nSet up the Appfollow source connector\n\nLog into your Airbyte Cloud or Airbyte Open Source account.\nClick Sources and then click + New source.\nOn the Set up the source page, select Appfollow from the Source type dropdown.\nEnter a name for your source.\nFor ext_id, cid, api_secret and Country, enter the Appfollow ext_id, cid, api_secret and country.\nClick Set up source.\n\nSupported Streams\nThe Appfollow source connector supports the following streams:\n\nRatings (Full Refresh sync)\n\nIf there are more endpoints you'd like Airbyte to support, please create an issue.\nSupported sync modes\nThe Appfollow source connector supports the following sync modes:\n\nFull Refresh\n\nPerformance considerations\nThe Appfollow connector ideally should gracefully handle Appfollow API limitations under normal usage. Create an issue if you see any rate limit issues that are not automatically retried successfully.",
    "tag": "airbyte"
  },
  {
    "title": "Google Analytics 4 (GA4)",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/google-analytics-data-api.md",
    "content": "Google Analytics 4 (GA4)\nThis page guides you through the process of setting up the Google Analytics source connector.\nThis connector supports GA4 properties through the Analytics Data API v1.\nPrerequisites\n\nJSON credentials for the service account that has access to Google Analytics. For more details check instructions\nOAuth 2.0 credentials for the service account that has access to Google Analytics\nProperty ID\nStart Date\nCustom Reports (Optional)\nData request time increment in days (Optional)\n\nStep 1: Set up Source\nCreate a Service Account\nFirst, you need to select existing or create a new project in the Google Developers Console:\n\nSign in to the Google Account you are using for Google Analytics as an admin.\nGo to the Service Accounts page.\nClick `Create service account`.\nCreate a JSON key file for the service user. The contents of this file will be provided as the `credentials_json` in the UI when authorizing GA after you grant permissions (see below).\n\nAdd service account to the Google Analytics account\nUse the service account email address to add a user to the Google analytics view you want to access via the API. You will need to grant Viewer permissions.\nEnable the APIs\n\nGo to the Google Analytics Reporting API dashboard in the project for your service user. Enable the API for your account. You can set quotas and check usage.\nGo to the Google Analytics API dashboard in the project for your service user. Enable the API for your account.\n\nProperty ID\nTo determine a Google Analytics 4 Property ID\nStep 2: Set up the Google Analytics connector in Airbyte\nFor Airbyte Cloud:\n\nLogin to your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click + new source.\nOn the source setup page, select Google Analytics 4 (GA4) from the Source type dropdown and enter a name for this connector.\nClick `Authenticate your account` by selecting Oauth or Service Account for Authentication.\nLog in and Authorize the Google Analytics account.\nEnter the Property ID whose events are tracked.\nEnter the Start Date from which to replicate report data in the format YYYY-MM-DD.\nEnter the Custom Reports (Optional) a JSON array describing the custom reports you want to sync from Google Analytics.\nEnter the Data request time increment in days (Optional). The bigger this value is, the faster the sync will be, but the more likely that sampling will be applied to your data, potentially causing inaccuracies in the returned results. We recommend setting this to 1 unless you have a hard requirement to make the sync faster at the expense of accuracy. The minimum allowed value for this field is 1, and the maximum is 364.\n\nSupported sync modes\nThe Google Analytics source connector supports the following sync modes:\n\nFull Refresh - Overwrite\nFull Refresh - Append\nIncremental - Append\nIncremental - Deduped History\n\nSupported Streams\nThis connector outputs the following incremental streams:\n\ndaily_active_users\ndevices\nfour_weekly_active_users\nlocations\npages\ntraffic_sources\nwebsite_overview\nweekly_active_users\n\nCustom reports\n\nSupport for multiple custom reports\nCustom reports in format `[{\"name\": \"<report-name>\", \"dimensions\": [\"<dimension-name>\", ...], \"metrics\": [\"<metric-name>\", ...]}]`\nCustom report format when using segments and / or filters `[{\"name\": \"<report-name>\", \"dimensions\": [\"<dimension-name>\", ...], \"metrics\": [\"<metric-name>\", ...], \"segments\":  [\"<segment-id-or-dynamic-segment-v3-format]\", filter: \"<filter-definition-v3-format>\"}]`\nWhen using segments, make sure you add the `ga:segment` dimension.\nCustom reports: Dimensions and metrics explorer\n\nRate Limits & Performance Considerations (Airbyte Open-Source)\nGoogle Analytics Data API\n\nNumber of requests per day per project: 50,000\n\nReports\nThe reports are custom by setting the dimensions and metrics required. To support Incremental sync, the `date` dimension is\nadded by default to all reports. There are 8 default reports. To add more reports, you need to specify the `custom reports` field.",
    "tag": "airbyte"
  },
  {
    "title": "LinkedIn Pages",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/linkedin-pages.md",
    "content": "LinkedIn Pages\nSync overview\nThe LinkedIn Pages source only supports Full Refresh for now. Incremental Sync will be coming soon.\nThis Source Connector is based on a Airbyte CDK. Airbyte uses LinkedIn Marketing Developer Platform - API to fetch data from LinkedIn Pages.\nOutput schema\nThis Source is capable of syncing the following data as streams:\n\nOrganization Lookup\nFollower Statistics\nPage Statistics\nShare Statistics\nShares (Latest 50)\nTotal Follower Count\nUGC Posts\n\nNOTE:\nAll streams only sync all-time statistics at this time. A `start_date` field will be added soon to pull data starting at a single point in time.\nData type mapping\n| Integration Type | Airbyte Type | Notes                      |\n| :--------------- | :----------- | :------------------------- |\n| `number`         | `number`     | float number               |\n| `integer`        | `integer`    | whole number               |\n| `array`          | `array`      |                            |\n| `boolean`        | `boolean`    | True/False                 |\n| `string`         | `string`     |                            |\nFeatures\n| Feature                                   | Supported?(Yes/No) | Notes |\n| :---------------------------------------- | :------------------- | :---- |\n| Full Refresh Overwrite Sync               | Yes                  |       |\n| Full Refresh Append Sync                  | No                   |       |\n| Incremental - Append Sync                 | No                   |       |\n| Incremental - Append + Deduplication Sync | No                   |       |\n| Namespaces                                | No                   |       |\nPerformance considerations\nThere are official Rate Limits for LinkedIn Pages API Usage, more information here. Rate limited requests will receive a 429 response. Rate limits specify the maximum number of API calls that can be made in a 24 hour period. These limits reset at midnight UTC every day. In rare cases, LinkedIn may also return a 429 response as part of infrastructure protection. API service will return to normal automatically. In such cases you will receive the next error message:\n`text\n\"Caught retryable error '<some_error> or null' after <some_number> tries. Waiting <some_number> seconds then retrying...\"`\nThis is expected when the connector hits the 429 - Rate Limit Exceeded HTTP Error. If the maximum of available API requests capacity is reached, you will have the following message:\n`text\n\"Max try rate limit exceded...\"`\nAfter 5 unsuccessful attempts - the connector will stop the sync operation. In such cases check your Rate Limits on this page > Choose your app > Analytics. \nGetting started\nThe API user account should be assigned the following permissions for the API endpoints:\nEndpoints such as: `Organization Lookup API`, `Follower Statistics`, `Page Statistics`, `Share Statistics`, `Shares`, `UGC Posts` require these permissions:\n* `r_organization_social`: Retrieve your organization's posts, comments, reactions, and other engagement data.\n* `rw_organization_admin`: Manage your organization's pages and retrieve reporting data.\nThe API user account should be assigned the `ADMIN` role.\nAuthentication\nThere are 2 authentication methods: Access Token or OAuth2.0.\nOAuth2.0 is recommended since it will continue streaming data for 12 months instead of 2 months with an access token.\nCreate the `Refresh_Token` or `Access_Token`:\nThe source LinkedIn Pages can use either the `client_id`, `client_secret` and `refresh_token` for OAuth2.0 authentication or simply use an `access_token` in the UI connector's settings to make API requests. Access tokens expire after `2 months from creation date (60 days)` and require a user to manually authenticate again. Refresh tokens expire after `12 months from creation date (365 days)`. If you receive a `401 invalid token response`, the error logs will state that your token has expired and to re-authenticate your connection to generate a new token. This is described more here.\n\n\nLog in to LinkedIn as the API user\n\n\nCreate an App here:\n\n`App Name`: airbyte-source\n`Company`: search and find your LinkedIn Company Page\n`Privacy policy URL`: link to company privacy policy\n`Business email`: developer/admin email address\n`App logo`: Airbyte's (or Company's) logo\nReview/agree to legal terms and create app\n\nReview the Auth tab:\n\nSave your `client_id` and `client_secret` (for later steps)\nOauth 2.0 settings: Provide a `redirect_uri` (for later steps): `https://airbyte.io`\n\n\n\nVerify App:\n\nIn the Settings tab of your app dashboard, you'll see a Verify button. Click that button!\n\nGenerate and provide the verify URL to your Company's LinkedIn Admin to verify the app.\n\n\nRequest API Access:\n\nNavigate to the Products tab\nSelect the Marketing Developer Platform and agree to the legal terms\nAfter a few minutes, refresh the page to see a link to `View access form` in place of the Select button\n\nFill out the access form and access should be granted within 72 hours (usually quicker)\n\n\nCreate A Refresh Token (or Access Token):\n\nNavigate to the LinkedIn Developers' OAuth Token Tools and click Create token\nSelect your newly created app and check the boxes for the following scopes:\n`r_organization_social`\n`rw_organization_admin`\n\n\n\nClick Request access token and once generated, save your Refresh token\n\n\nUse the `client_id`, `client_secret` and `refresh_token` from Steps 2 and 5 to autorize the LinkedIn Pages connector within the Airbyte UI.\n\nAs mentioned earlier, you can also simply use the Access token auth method for 60-day access.\n",
    "tag": "airbyte"
  },
  {
    "title": "MailerLite",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/mailerlite.md",
    "content": "MailerLite\nSync overview\nThis source can sync data from the MailerLite API. At present this connector only supports full refresh syncs meaning that each time you use the connector it will sync all available records from scratch. Please use cautiously if you expect your API to have a lot of records.\nThis Source Supports the Following Streams\n\ncampaigns\nsubscribers\nautomations\ntimezones\nsegments\nforms_popup\nforms_embedded\nforms_promotion\n\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | No |  |\nPerformance considerations\nMailerLite API has a global rate limit of 120 requests per minute.\nGetting started\nRequirements\n\nMailerLite API Key\n",
    "tag": "airbyte"
  },
  {
    "title": "Kustomer",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/kustomer-singer.md",
    "content": "Kustomer\nSync overview\nThe Kustomer source supports both Full Refresh and Incremental syncs. You can choose if this connector will copy only the new or updated data, or all rows in the tables and columns you set up for replication, every time a sync is run.\nThis source can sync data for the Kustomer API.\nThis Source Connector is based on a Singer tap.\nOutput schema\nThis Source is capable of syncing the following core Streams:\n\nConversations\nCustomers\nKObjects\nMessages\nNotes\nShortcuts\nTags\nTeams\nUsers\n\nFeatures\n| Feature                   | Supported?(Yes/No) | Notes |\n| :------------------------ | :------------------- | :---- |\n| Full Refresh Sync         | Yes                  |       |\n| Incremental - Append Sync | Yes                  |       |\n| Namespaces                | No                   |       |\nPerformance considerations\nKustomer has some rate limit restrictions.\nRequirements\n\nKustomer API token. See the Kustomer docs for information on how to obtain an API token.\n",
    "tag": "airbyte"
  },
  {
    "title": "BigCommerce",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/bigcommerce.md",
    "content": "BigCommerce\nSync overview\nThe BigCommerce source supports both Full Refresh and Incremental syncs. You can choose if this connector will copy only the new or updated data, or all rows in the tables and columns you set up for replication, every time a sync is run.\nThis source can sync data for the BigCommerce API.\nThis Source Connector is based on a Airbyte CDK.\nOutput schema\nThis Source is capable of syncing the following core Streams:\n\nCustomers\nOrders\nTransactions\nPages\nProducts\nChannels\nStore\nOrderProducts\nBrands\nCategories\n\nData type mapping\n| Integration Type | Airbyte Type | Notes |\n| :--------------- | :----------- | :---- |\n| `string`         | `string`     |       |\n| `number`         | `number`     |       |\n| `array`          | `array`      |       |\n| `object`         | `object`     |       |\nFeatures\n| Feature                   | Supported?(Yes/No) | Notes |\n| :------------------------ | :------------------- | :---- |\n| Full Refresh Sync         | Yes                  |       |\n| Incremental - Append Sync | Yes                  |       |\n| Namespaces                | No                   |       |\nPerformance considerations\nBigCommerce has some rate limit restrictions.\nGetting started\n\nNavigate to your store\u2019s control panel (Advanced Settings > API Accounts > Create API Account)\nCreate an API account.\nSelect the resources you want to allow access to. Airbyte only needs read-level access.\nNote: The UI will show all possible data sources and will show errors when syncing if it doesn't have permissions to access a resource.\nThe generated `Access Token` is what you'll use as the `access_token` for the integration.\nYou're ready to set up BigCommerce in Airbyte!\n",
    "tag": "airbyte"
  },
  {
    "title": "PostHog",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/posthog.md",
    "content": "PostHog\nThis page contains the setup guide and reference information for the PostHog source connector.\nPrerequisites\n\napi_key - obtain Private API Key for your account following these steps\nbase_url - 'https://app.posthog.com' by default, but it can be different if self-hosted posthog instances is used \n\nSetup guide\nStep 1: Set up PostHog\n\nPostHog Account\n\nStep 2: Set up the PostHog connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the PostHog connector and select PostHog from the Source type dropdown.\nEnter your `apikey`.\nEnter your `start_date`. \nChange default `base_url` if self-hosted posthog instances is used\nClick Set up source.\n\nFor Airbyte OSS:\n\nNavigate to the Airbyte Open Source dashboard.\nSet the name for your source. \nEnter your `api_key`.\nEnter your `start_date`. \nChange default `base_url` if self-hosted posthog instances is used\nClick Set up source.\n\nSupported streams and sync modes\n\nProjects\nAnnotations\nCohorts\nEvents (Incremental)\nFeatureFlags\nInsights\nPersons\n\nPerformance considerations\nThe PostHog API doesn't have any known request limitation.\nPlease create an issue if you see any rate limit issues that are not automatically retried successfully.",
    "tag": "airbyte"
  },
  {
    "title": "GNews",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/gnews.md",
    "content": "GNews\nOverview\nThe GNews source supports full refresh syncs\nOutput schema\nTwo output streams are available from this source:\nSearch.\nTop Headlines.\nFeatures\n| Feature           | Supported? |\n|:------------------|:-----------|\n| Full Refresh Sync | Yes        |\n| Incremental Sync  | Yes        |\nPerformance considerations\nRate Limiting is based on the API Key tier subscription, get more info here.\nGetting started\nRequirements\n\nGNews API Key.\n\nConnect using `API Key`:\n\nGenerate an API Key as described here.\nUse the generated `API Key` in the Airbyte connection.\n",
    "tag": "airbyte"
  },
  {
    "title": "Intruder.io API",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/intruder.md",
    "content": "Intruder.io API\nSync overview\nThis source can sync data from the Intruder.io API. At present this connector only supports full refresh syncs meaning that each time you use the connector it will sync all available records from scratch. Please use cautiously if you expect your API to have a lot of records.\nThis Source Supports the Following Streams\n\nIssues\nOccurrences issue\nTargets\nScans\n\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | No |  |\nPerformance considerations\nIntruder.io APIs are under rate limits for the number of API calls allowed per API keys per second. If you reach a rate limit, API will return a 429 HTTP error code. See here\nGetting started\nRequirements\n\nIntruder.io Access token\n",
    "tag": "airbyte"
  },
  {
    "title": "AlloyDB for PostgreSQL",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/alloydb.md",
    "content": "AlloyDB for PostgreSQL\nThis page contains the setup guide and reference information for the AlloyDB for PostgreSQL.\nPrerequisites\n\nFor Airbyte Open Source users, upgrade your Airbyte platform to version `v0.40.0-alpha` or newer\nFor Airbyte Cloud (and optionally for Airbyte Open Source), ensure SSL is enabled in your environment\n\nSetup guide\nWhen to use AlloyDB with CDC\nConfigure AlloyDB with CDC if:\n\nYou need a record of deletions\nYour table has a primary key but doesn't have a reasonable cursor field for incremental syncing (`updated_at`). CDC allows you to sync your table incrementally\n\nIf your goal is to maintain a snapshot of your table in the destination but the limitations prevent you from using CDC, consider using non-CDC incremental sync and occasionally reset the data and re-sync.\nIf your dataset is small and you just want a snapshot of your table in the destination, consider using Full Refresh replication for your table instead of CDC.\nStep 1: (Optional) Create a dedicated read-only user\nWe recommend creating a dedicated read-only user for better permission control and auditing. Alternatively, you can use an existing AlloyDB user in your database.\nTo create a dedicated user, run the following command:\n`CREATE USER <user_name> PASSWORD 'your_password_here';`\nGrant access to the relevant schema:\n`GRANT USAGE ON SCHEMA <schema_name> TO <user_name>`\n:::note\nTo replicate data from multiple AlloyDB schemas, re-run the command to grant access to all the relevant schemas. Note that you'll need to set up multiple Airbyte sources connecting to the same AlloyDB database on multiple schemas.\n:::\nGrant the user read-only access to the relevant tables:\n`GRANT SELECT ON ALL TABLES IN SCHEMA <schema_name> TO <user_name>;`\nAllow user to see tables created in the future:\n`ALTER DEFAULT PRIVILEGES IN SCHEMA <schema_name> GRANT SELECT ON TABLES TO <user_name>;`\nAdditionally, if you plan to configure CDC for the AlloyDB source connector, grant `REPLICATION` permissions to the user:\n`ALTER USER <user_name> REPLICATION;`\nSyncing a subset of columns\u200b\nCurrently, there is no way to sync a subset of columns using the AlloyDB source connector:\n\nWhen setting up a connection, you can only choose which tables to sync, but not columns.\nIf the user can only access a subset of columns, the connection check will pass. However, the data sync will fail with a permission denied exception.\n\nThe workaround for partial table syncing is to create a view on the specific columns, and grant the user read access to that view:\n`CREATE VIEW <view_name> as SELECT <columns> FROM <table>;`\n`GRANT SELECT ON TABLE <view_name> IN SCHEMA <schema_name> to <user_name>;`\nNote: The workaround works only for non-CDC setups since CDC requires data to be in tables and not views.\nThis issue is tracked in #9771.\nStep 2: Set up the AlloyDB connector in Airbyte\n\nLog into your Airbyte Cloud or Airbyte Open Source account.\nClick Sources and then click + New source.\nOn the Set up the source page, select AlloyDB from the Source type dropdown.\nEnter a name for your source.\nFor the Host, Port, and DB Name, enter the hostname, port number, and name for your AlloyDB database.\nList the Schemas you want to sync.\n   :::note\n   The schema names are case sensitive. The 'public' schema is set by default. Multiple schemas may be used at one time. No schemas set explicitly - will sync all of existing.\n   :::\nFor User and Password, enter the username and password you created in Step 1.\nTo customize the JDBC connection beyond common options, specify additional supported JDBC URL parameters as key-value pairs separated by the symbol & in the JDBC URL Parameters (Advanced) field.\n\nExample: key1=value1&key2=value2&key3=value3\nThese parameters will be added at the end of the JDBC URL that the AirByte will use to connect to your AlloyDB database.\nThe connector now supports `connectTimeout` and defaults to 60 seconds. Setting connectTimeout to 0 seconds will set the timeout to the longest time available.\nNote: Do not use the following keys in JDBC URL Params field as they will be overwritten by Airbyte:\n   `currentSchema`, `user`, `password`, `ssl`, and `sslmode`.\n:::warning\n   This is an advanced configuration option. Users are advised to use it with caution.\n   :::\n\nFor Airbyte Open Source, toggle the switch to connect using SSL. Airbyte Cloud uses SSL by default.\nFor Replication Method, select Standard or Logical CDC from the dropdown. Refer to Configuring AlloyDB connector with Change Data Capture (CDC) for more information.\nFor SSH Tunnel Method, select:\nNo Tunnel for a direct connection to the database\nSSH Key Authentication to use an RSA Private as your secret for establishing the SSH tunnel\nPassword Authentication to use a password as your secret for establishing the SSH tunnel\n  Refer to Connect via SSH Tunnel for more information.\n\n\nClick Set up source.\n\nConnect via SSH Tunnel\u200b\nYou can connect to a AlloyDB instance via an SSH tunnel.\nWhen using an SSH tunnel, you are configuring Airbyte to connect to an intermediate server (also called a bastion server) that has direct access to the database. Airbyte connects to the bastion and then asks the bastion to connect directly to the server.\nTo connect to a AlloyDB instance via an SSH tunnel:\n\nWhile setting up the AlloyDB source connector, from the SSH tunnel dropdown, select:\nSSH Key Authentication to use an RSA Private as your secret for establishing the SSH tunnel\nPassword Authentication to use a password as your secret for establishing the SSH Tunnel\n\n\nFor SSH Tunnel Jump Server Host, enter the hostname or IP address for the intermediate (bastion) server that Airbyte will connect to.\nFor SSH Connection Port, enter the port on the bastion server. The default port for SSH connections is 22.\nFor SSH Login Username, enter the username to use when connecting to the bastion server. Note: This is the operating system username and not the AlloyDB username.\nFor authentication:\nIf you selected SSH Key Authentication, set the SSH Private Key to the RSA Private Key that you are using to create the SSH connection.\nIf you selected Password Authentication, enter the password for the operating system user to connect to the bastion server. Note: This is the operating system password and not the AlloyDB password.\n\n\n\nGenerating an RSA Private Key\u200b\nThe connector expects an RSA key in PEM format. To generate this key, run:\n`ssh-keygen -t rsa -m PEM -f myuser_rsa`\nThe command produces the private key in PEM format and the public key remains in the standard format used by the `authorized_keys` file on your bastion server. Add the public key to your bastion host to the user you want to use with Airbyte. The private key is provided via copy-and-paste to the Airbyte connector configuration screen to allow it to log into the bastion server.\nConfiguring AlloyDB connector with Change Data Capture (CDC)\nAirbyte uses logical replication of the Postgres write-ahead log (WAL) to incrementally capture deletes using a replication plugin. To learn more how Airbyte implements CDC, refer to Change Data Capture (CDC)\nCDC Considerations\n\nIncremental sync is only supported for tables with primary keys. For tables without primary keys, use Full Refresh sync.\nData must be in tables and not views.\nThe modifications you want to capture must be made using `DELETE`/`INSERT`/`UPDATE`. For example, changes made using `TRUNCATE`/`ALTER` will not appear in logs and therefore in your destination.\nSchema changes are not supported automatically for CDC sources. Reset and resync data if you make a schema change.\nThe records produced by `DELETE` statements only contain primary keys. All other data fields are unset.\nLog-based replication only works for master instances of AlloyDB.\nUsing logical replication increases disk space used on the database server. The additional data is stored until it is consumed.\nSet frequent syncs for CDC to ensure that the data doesn't fill up your disk space.\nIf you stop syncing a CDC-configured AlloyDB instance with Airbyte, delete the replication slot. Otherwise, it may fill up your disk space.\n\n\n\nSetting up CDC for AlloyDB\nAirbyte requires a replication slot configured only for its use. Only one source should be configured that uses this replication slot. See Setting up CDC for AlloyDB for instructions.\nStep 2: Select a replication plugin\u200b\nWe recommend using a pgoutput plugin (the standard logical decoding plugin in AlloyDB). If the replication table contains multiple JSON blobs and the table size exceeds 1 GB, we recommend using a wal2json instead. Note that wal2json may require additional installation for Bare Metal, VMs (EC2/GCE/etc), Docker, etc. For more information read the wal2json documentation.\nStep 3: Create replication slot\u200b\nTo create a replication slot called `airbyte_slot` using pgoutput, run:\n`SELECT pg_create_logical_replication_slot('airbyte_slot', 'pgoutput');`\nTo create a replication slot called `airbyte_slot` using wal2json, run:\n`SELECT pg_create_logical_replication_slot('airbyte_slot', 'wal2json');`\nStep 4: Create publications and replication identities for tables\u200b\nFor each table you want to replicate with CDC, add the replication identity (the method of distinguishing between rows) first:\nTo use primary keys to distinguish between rows, run:\n`ALTER TABLE tbl1 REPLICA IDENTITY DEFAULT;`\nAfter setting the replication identity, run:\n`CREATE PUBLICATION airbyte_publication FOR TABLE <tbl1, tbl2, tbl3>;``\nThe publication name is customizable. Refer to the Postgres docs if you need to add or remove tables from your publication in the future.\n:::note\nYou must add the replication identity before creating the publication. Otherwise, `ALTER`/`UPDATE`/`DELETE` statements may fail if AlloyDB cannot determine how to uniquely identify rows.\nAlso, the publication should include all the tables and only the tables that need to be synced. Otherwise, data from these tables may not be replicated correctly.\n:::\n:::warning\nThe Airbyte UI currently allows selecting any tables for CDC. If a table is selected that is not part of the publication, it will not be replicated even though it is selected. If a table is part of the publication but does not have a replication identity, that replication identity will be created automatically on the first run if the Airbyte user has the necessary permissions.\n:::\nStep 5: [Optional] Set up initial waiting time\n:::warning\nThis is an advanced feature. Use it if absolutely necessary.\n:::\nThe AlloyDB connector may need some time to start processing the data in the CDC mode in the following scenarios:\n\nWhen the connection is set up for the first time and a snapshot is needed\nWhen the connector has a lot of change logs to process\n\nThe connector waits for the default initial wait time of 5 minutes (300 seconds). Setting the parameter to a longer duration will result in slower syncs, while setting it to a shorter duration may cause the connector to not have enough time to create the initial snapshot or read through the change logs. The valid range is 120 seconds to 1200 seconds.\nIf you know there are database changes to be synced, but the connector cannot read those changes, the root cause may be insufficient waiting time. In that case, you can increase the waiting time (example: set to 600 seconds) to test if it is indeed the root cause. On the other hand, if you know there are no database changes, you can decrease the wait time to speed up the zero record syncs.\nStep 6: Set up the AlloyDB source connector\nIn Step 2 of the connector setup guide, enter the replication slot and publication you just created.\nSupported sync modes\nThe AlloyDB source connector supports the following sync modes:\n\nFull Refresh - Overwrite\nFull Refresh - Append\nIncremental Sync - Append\nIncremental Sync - Deduped History\n\nSupported cursors\n\n`TIMESTAMP`\n`TIMESTAMP_WITH_TIMEZONE`\n`TIME`\n`TIME_WITH_TIMEZONE`\n`DATE`\n`BIT`\n`BOOLEAN`\n`TINYINT/SMALLINT`\n`INTEGER`\n`BIGINT`\n`FLOAT/DOUBLE`\n`REAL`\n`NUMERIC/DECIMAL`\n`CHAR/NCHAR/NVARCHAR/VARCHAR/LONGVARCHAR`\n`BINARY/BLOB`\n\nData type mapping\nThe AlloyDb is a fully managed PostgreSQL-compatible database service.\nAccording to Postgres documentation, Postgres data types are mapped to the following data types when synchronizing data. You can check the test values examples here. If you can't find the data type you are looking for or have any problems feel free to add a new test!\n| Postgres Type                         | Resulting Type | Notes                                                                                                                                                                                           |\n|:--------------------------------------|:---------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| `bigint`                              | number         |                                                                                                                                                                                                 |\n| `bigserial`, `serial8`                | number         |                                                                                                                                                                                                 |\n| `bit`                                 | string         | Fixed-length bit string (e.g. \"0100\").                                                                                                                                                          |\n| `bit varying`, `varbit`               | string         | Variable-length bit string (e.g. \"0100\").                                                                                                                                                       |\n| `boolean`, `bool`                     | boolean        |                                                                                                                                                                                                 |\n| `box`                                 | string         |                                                                                                                                                                                                 |\n| `bytea`                               | string         | Variable length binary string with hex output format prefixed with \"\\x\" (e.g. \"\\x6b707a\").                                                                                                      |\n| `character`, `char`                   | string         |                                                                                                                                                                                                 |\n| `character varying`, `varchar`        | string         |                                                                                                                                                                                                 |\n| `cidr`                                | string         |                                                                                                                                                                                                 |\n| `circle`                              | string         |                                                                                                                                                                                                 |\n| `date`                                | string         | Parsed as ISO8601 date time at midnight. CDC mode doesn't support era indicators. Issue: #14590                                            |\n| `double precision`, `float`, `float8` | number         | `Infinity`, `-Infinity`, and `NaN` are not supported and converted to `null`. Issue: #8902.                                                 |\n| `hstore`                              | string         |                                                                                                                                                                                                 |\n| `inet`                                | string         |                                                                                                                                                                                                 |\n| `integer`, `int`, `int4`              | number         |                                                                                                                                                                                                 |\n| `interval`                            | string         |                                                                                                                                                                                                 |\n| `json`                                | string         |                                                                                                                                                                                                 |\n| `jsonb`                               | string         |                                                                                                                                                                                                 |\n| `line`                                | string         |                                                                                                                                                                                                 |\n| `lseg`                                | string         |                                                                                                                                                                                                 |\n| `macaddr`                             | string         |                                                                                                                                                                                                 |\n| `macaddr8`                            | string         |                                                                                                                                                                                                 |\n| `money`                               | number         |                                                                                                                                                                                                 |\n| `numeric`, `decimal`                  | number         | `Infinity`, `-Infinity`, and `NaN` are not supported and converted to `null`. Issue: #8902.                                                 |\n| `path`                                | string         |                                                                                                                                                                                                 |\n| `pg_lsn`                              | string         |                                                                                                                                                                                                 |\n| `point`                               | string         |                                                                                                                                                                                                 |\n| `polygon`                             | string         |                                                                                                                                                                                                 |\n| `real`, `float4`                      | number         |                                                                                                                                                                                                 |\n| `smallint`, `int2`                    | number         |                                                                                                                                                                                                 |\n| `smallserial`, `serial2`              | number         |                                                                                                                                                                                                 |\n| `serial`, `serial4`                   | number         |                                                                                                                                                                                                 |\n| `text`                                | string         |                                                                                                                                                                                                 |\n| `time`                                | string         | Parsed as a time string without a time-zone in the ISO-8601 calendar system.                                                                                                                    |\n| `timetz`                              | string         | Parsed as a time string with time-zone in the ISO-8601 calendar system.                                                                                                                         |\n| `timestamp`                           | string         | Parsed as a date-time string without a time-zone in the ISO-8601 calendar system.                                                                                                               |\n| `timestamptz`                         | string         | Parsed as a date-time string with time-zone in the ISO-8601 calendar system.                                                                                                                    |\n| `tsquery`                             | string         |                                                                                                                                                                                                 |\n| `tsvector`                            | string         |                                                                                                                                                                                                 |\n| `uuid`                                | string         |                                                                                                                                                                                                 |\n| `xml`                                 | string         |                                                                                                                                                                                                 |\n| `enum`                                | string         |                                                                                                                                                                                                 |\n| `tsrange`                             | string         |                                                                                                                                                                                                 |\n| `array`                               | array          | E.g. \"[\\\"10001\\\",\\\"10002\\\",\\\"10003\\\",\\\"10004\\\"]\".                                                                                                                                               |\n| composite type                        | string         |                                                                                                                                                                                                 |\nLimitations\n\nThe AlloyDB source connector currently does not handle schemas larger than 4MB.\nThe AlloyDB source connector does not alter the schema present in your database. Depending on the destination connected to this source, however, the schema may be altered. See the destination's documentation for more details.\nThe following two schema evolution actions are currently supported:\nAdding/removing tables without resetting the entire connection at the destination\n  Caveat: In the CDC mode, adding a new table to a connection may become a temporary bottleneck. When a new table is added, the next sync job takes a full snapshot of the new table before it proceeds to handle any changes.\nResetting a single table within the connection without resetting the rest of the destination tables in that connection\n\n\nChanging a column data type or removing a column might break connections.\n",
    "tag": "airbyte"
  },
  {
    "title": "Ashby",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/ashby.md",
    "content": "Ashby\nSync overview\nThe Ashby source supports both Full Refresh only.\nThis source can sync data for the Ashby API.\nOutput schema\nThis Source is capable of syncing the following core Streams:\n\napplications\narchive_reasons\ncandidate_tags\ncandidates\ncustom_fields\ndepartments\nfeedback_form_definitions\ninterview_schedules\njob_postings\njobs\nlocations\noffers\nsources\nusers\n\nFeatures\n| Feature                   | Supported?(Yes/No) | Notes |\n| :------------------------ | :------------------- | :---- |\n| Full Refresh Sync         | Yes                  |       |\n| Incremental - Append Sync | No                   |       |\n| Namespaces                | No                   |       |\nPerformance considerations\nThe Ashby connector should not run into Ashby API limitations under normal usage.\nRequirements\n\nAshby API key. See the Ashby docs for information on how to obtain an API key.\n",
    "tag": "airbyte"
  },
  {
    "title": "Coda",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/coda.md",
    "content": "Coda\nThis page contains the setup guide and reference information for the Coda source connector.\nPrerequisites\nYou can find or create authentication tokens within Coda.\nSetup guide\nStep 1: Set up the Coda connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the Coda connector and select Coda from the Source type dropdown.\nEnter your `auth_token` - Coda Authentication Token with the necessary permissions (described below).\nEnter your `doc_id` - Document id for a specific document created on Coda. You can check it under Advanced Settings\n   by exporting data and copying the id in doc_manifest.json from the downloaded zip.\nClick Set up source.\n\nFor Airbyte OSS:\n\nNavigate to the Airbyte Open Source dashboard.\nSet the name for your source.\nEnter your `auth_token` - Coda Authentication Token with the necessary permissions (described below).\nEnter your `doc_id` - Document id for a specific document created on Coda. You can check it under Advanced Settings\n   by exporting data and copying the id in doc_manifest.json from the downloaded zip.\nClick Set up source.\n\nSupported sync modes\nThe Coda source connector supports the following sync modes:\n| Feature           | Supported? |\n| :---------------- | :--------- |\n| Full Refresh Sync | Yes        |\n| Incremental Sync  | No         |\n| SSL connection    | No         |\n| Namespaces        | No         |\nSupported Streams\n\nDocs\nPermissions\nCategories\nPages\nTables\nFormulas\nControls\n\nData type map\n| Integration Type | Airbyte Type |\n| :--------------- | :----------- |\n| `string`         | `string`     |\n| `integer`        | `number`     |\n| `array`          | `array`      |\n| `object`         | `object`     |",
    "tag": "airbyte"
  },
  {
    "title": "K6 Cloud API",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/k6-cloud.md",
    "content": "K6 Cloud API\nSync overview\nThis source can sync data from the K6 Cloud API. At present this connector only supports full refresh syncs meaning that each time you use the connector it will sync all available records from scratch. Please use cautiously if you expect your API to have a lot of records.\nThis Source Supports the Following Streams\n\norganizations\nprojects\ntests\n\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | No |  |\nPerformance considerations\nGetting started\nRequirements\n\nAPI Token\n",
    "tag": "airbyte"
  },
  {
    "title": "Commcare",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/commcare.md",
    "content": "Commcare\nThis page guides you through the process of setting up the Commcare source connector.\nPrerequisites\n\nYour Commcare API Key\nThe Application ID you are interested in\nThe start date to replicate records\n\nSet up the Commcare source connector\n\nLog into your Airbyte Cloud or Airbyte Open Source account.\nClick Sources and then click + New source.\nOn the Set up the source page, select Commcare from the Source type dropdown.\nEnter a name for your source.\nFor API Key, enter your Commcare API Key.\nClick Set up source.\n\nSupported sync modes\nThe Commcare source connector supports the following sync modes:\n\nFull Refresh\nOverwrite\nIncremental\n\nSupported Streams\nThe Commcare source connector supports the following streams:\n\nApplication\nCase\nForm\n",
    "tag": "airbyte"
  },
  {
    "title": "Amazon Seller Partner",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/amazon-seller-partner.md",
    "content": "Amazon Seller Partner\nThis page guides you through the process of setting up the Amazon Seller Partner source connector.\n:::caution\nAuthenticating this Alpha connector is currently blocked. This is a known issue being tracked here: https://github.com/airbytehq/airbyte/issues/14734\n:::\nPrerequisites\n\napp_id\nlwa_app_id\nlwa_client_secret\nrefresh_token\naws_access_key\naws_secret_key\nrole_arn\naws_environment\nregion\nreplication_start_date\n\nStep 1: Set up Amazon Seller Partner\nRegister Amazon Seller Partner application.\nCreate IAM user.\nStep 2: Set up the source connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account. \nIn the left navigation bar, click Sources. In the top-right corner, click + new source. \nOn the source setup page, select Amazon Seller Partner from the Source type dropdown and enter a name for this connector.\nClick `Authenticate your account`.\nLog in and Authorize to the Amazon Seller Partner account.\nPaste all other data to required fields using your IAM user.\nClick `Set up source`.\n\nFor Airbyte Open Source:\n\nUsing developer application from Step 1, generate refresh token. \nGo to local Airbyte page.\nIn the left navigation bar, click Sources. In the top-right corner, click + new source. \nOn the Set up the source page, enter the name for the Amazon Seller Partner connector and select Amazon Seller Partner from the Source type dropdown. \nPaste all data to required fields using your IAM user and developer account.\nClick `Set up source`.\n\nSupported sync modes\nThe Amazon Seller Partner source connector supports the following sync modes:\n - Full Refresh\n - Incremental\nPerformance considerations\nInformation about rate limits you may find here.\nSupported streams\nThis source is capable of syncing the following tables and their data:\n- FBA Inventory Reports\n- FBA Orders Reports\n- FBA Shipments Reports\n- FBA Replacements Reports\n- FBA Storage Fees Report\n- Restock Inventory Reports\n- Flat File Open Listings Reports\n- Flat File Orders Reports\n- Flat File Orders Reports By Last Update (incremental)\n- Amazon-Fulfilled Shipments Report\n- Merchant Listings Reports\n- Vendor Direct Fulfillment Shipping\n- Vendor Inventory Health Reports\n- Orders (incremental)\n- Seller Feedback Report (incremental)\n- Brand Analytics Alternate Purchase Report\n- Brand Analytics Item Comparison Report\n- Brand Analytics Market Basket Report\n- Brand Analytics Repeat Purchase Report\n- Brand Analytics Search Terms Report\n- Browse tree report\n- Financial Event Groups\n- Financial Events\n- FBA Fee Preview Report\n- FBA Daily Inventory History Report\n- FBA Promotions Report\n- FBA Inventory Adjustments Report\n- FBA Received Inventory Report\n- FBA Inventory Event Detail Report\n- FBA Monthly Inventory History Report\n- FBA Manage Inventory\n- Subscribe and Save Forecast Report\n- Subscribe and Save Performance Report\n- Flat File Archived Orders Report\n- Flat File Returns Report by Return Date\n- Canceled Listings Report\n- Active Listings Report\n- Open Listings Report\n- Suppressed Listings Report\n- Inactive Listings Report\n- FBA Stranded Inventory Report\n- XML Orders By Order Date Report\n- Inventory Ledger Report - Detailed View\n- FBA Manage Inventory Health Report\n- Inventory Ledger Report - Summary View\nReport options\nMake sure to configure the required parameters in the report options setting for the reports configured.\nFor `GET_AMAZON_FULFILLED_SHIPMENTS_DATA_GENERAL` and `GET_FLAT_FILE_RETURNS_DATA_BY_RETURN_DATE` streams maximum value for `period_in_days` 30 days and 60 days. \nSo, for any value that exceeds the limit, the `period_in_days` will be automatically reduced to the limit for the stream.\nData type mapping\n| Integration Type         | Airbyte Type |\n| :----------------------- | :----------- |\n| `string`                 | `string`     |\n| `int`, `float`, `number` | `number`     |\n| `date`                   | `date`       |\n| `datetime`               | `datetime`   |\n| `array`                  | `array`      |\n| `object`                 | `object`     |",
    "tag": "airbyte"
  },
  {
    "title": "SurveyMonkey",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/surveymonkey.md",
    "content": "SurveyMonkey\nThis page guides you through the process of setting up the SurveyMonkey source connector.\n:::note\nOAuth for Survey Monkey is officially supported only for the US. We are testing how to enable it in the EU at the moment. If you run into any issues, please reach out to us so we can promptly assist you.\n:::\n\nPrerequisites\nFor Airbyte Open Source:\n\nAccess Token\n\n\nSetup guide\nStep 1: Set up SurveyMonkey\nPlease read this docs. Register your application here Then go to Settings and copy your access token\nStep 2: Set up the source connector in Airbyte\n\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click + new source.\nOn the source setup page, select SurveyMonkey from the Source type dropdown and enter a name for this connector.\nlick `Authenticate your account`.\nLog in and Authorize to the SurveyMonkey account\nChoose required Start date\nclick `Set up source`.\n\n\n\nFor Airbyte Open Source:\n\nGo to local Airbyte page.\nIn the left navigation bar, click Sources. In the top-right corner, click + new source.\nOn the source setup page, select SurveyMonkey from the Source type dropdown and enter a name for this connector.\nAdd Access Token\nChoose required Start date\nClick `Set up source`.\n\n\nSupported streams and sync modes\n\nSurveys (Incremental)\nSurveyPages\nSurveyQuestions\nSurveyResponses (Incremental)\n\nPerformance considerations\nThe SurveyMonkey API applies heavy API quotas for default private apps, which have the following limits:\n\n125 requests per minute\n500 requests per day\n\nTo cover more data from this source we use caching.",
    "tag": "airbyte"
  },
  {
    "title": "Plausible",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/plausible.md",
    "content": "Plausible\nRequirements\n\nPlausible account\nPlausible API key\n\nSupported sync modes\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes | Overwrite |\n| Incremental Sync | No |  |\nSupported Streams\n\nStats - Time Series\n\nNotes\nPlausible is a privacy-first analytics service, and the data available from its API is intentionally 1) less granular and 2) less comprehensive than those available from Google Analytics. As such:\n\nwhen retrieving multi-day data, metrics are aggregated to a daily grain; and\nnon-metric properties (e.g., referrer, entry page, exit page) cannot be directly exported, only grouped on.\n\nThus, this source connector retrieves all possible metrics on a daily grain, for all days with nonzero website activity.\nPerformance Considerations\nThe stated rate limit is 600 requests per hour per API key, with higher capacities potentially available upon request.",
    "tag": "airbyte"
  },
  {
    "title": "HubSpot",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/hubspot.md",
    "content": "HubSpot\nThis page guides you through setting up the HubSpot source connector.\nPrerequisite\nYou can use OAuth, API key, or Private App to authenticate your HubSpot account. If you choose to use OAuth or Private App, you need to configure the appropriate scopes for the following streams:\n| Stream                      | Required Scope                                                                   |\n| :-------------------------- | :------------------------------------------------------------------------------- |\n| `campaigns`                 | `content`                                                                        |\n| `companies`                 | `crm.objects.companies.read`, `crm.schemas.companies.read`                       |\n| `contact_lists`             | `crm.objects.lists.read`                                                         |\n| `contacts`                  | `crm.objects.contacts.read`                                                      |\n| `contacts_list_memberships` | `crm.objects.contacts.read`                                                      |\n| `deal_pipelines`            | either the `crm.objects.contacts.read` scope (to fetch deals pipelines) or the `tickets` scope. |\n| `deals`                     | `crm.objects.deals.read`, `crm.schemas.deals.read`                               |\n| `email_events`              | `content`                                                                        |\n| `engagements`               | `crm.objects.companies.read`, `crm.objects.contacts.read`, `crm.objects.deals.read`, `tickets`, `e-commerce`|\n| `engagements_emails`        | `sales-email-read`                                                               |\n| `forms`                     | `forms`                                                                          |\n| `form_submissions`          | `forms`                                                                          |\n| `line_items`                | `e-commerce`                                                                     |\n| `owners`                    | `crm.objects.owners.read`                                                        |\n| `products`                  | `e-commerce`                                                                     |\n| `property_history`          | `crm.objects.contacts.read`                                                      |\n| `subscription_changes`      | `content`                                                                        |\n| `tickets`                   | `tickets`                                                                        |\n| `workflows`                 | `automation`                                                                     |\nSet up the HubSpot source connector\n\nLog into your Airbyte Cloud or Airbyte Open Source account.\nClick Sources and then click + New source. \nOn the Set up the source page, select HubSpot from the Source type dropdown.\nEnter a name for your source.\nFor Start date, enter the date in YYYY-MM-DDTHH:mm:ssZ format. The data added on and after this date will be replicated. If this field is blank, Airbyte will replicate all data.\nYou can use OAuth or an API key to authenticate your HubSpot account. We recommend using OAuth for Airbyte Cloud and an API key for Airbyte Open Source.\nTo authenticate using OAuth for Airbyte Cloud, ensure you have set the appropriate scopes for HubSpot and then click Authenticate your HubSpot account to sign in with HubSpot and authorize your account. \nTo authenticate using an API key for Airbyte Open Source, select API key from the Authentication dropdown and enter the API key for your HubSpot account.  \n:::note\nCheck the performance considerations before using an API key.\n:::\n\n\nClick Set up source.\n\nSupported sync modes\nThe HubSpot source connector supports the following sync modes:\n\nFull Refresh\nIncremental\n\nSupported Streams\nThe HubSpot source connector supports the following streams:\n\nCampaigns\nCompanies (Incremental)\nContact Lists (Incremental)\nContacts (Incremental)\nContacts List Memberships\nDeal Pipelines\nDeals (including Contact associations) (Incremental)\nEmail Events (Incremental)\nEngagements (Incremental)\nEngagements Calls (Incremental)\nEngagements Emails (Incremental)\nEngagements Meetings (Incremental)\nEngagements Notes (Incremental)\nEngagements Tasks (Incremental)\nForms\nForm Submissions\nLine Items (Incremental)\nMarketing Emails\nOwners\nProducts (Incremental)\nProperty History (Incremental)\nSubscription Changes (Incremental)\nTickets (Incremental)\nTicket Pipelines\nWorkflows\n\nA note on the `engagements` stream\nObjects in the `engagements` stream can have one of the following types: `note`, `email`, `task`, `meeting`, `call`. Depending on the type of engagement, different properties is set for that object in the `engagements_metadata` table in the destination:\n\nA `call` engagement has a corresponding `engagements_metadata` object with non-null values in the `toNumber`, `fromNumber`, `status`, `externalId`, `durationMilliseconds`, `externalAccountId`, `recordingUrl`, `body`, and `disposition` columns.\nAn `email` engagement has a corresponding `engagements_metadata` object with non-null values in the `subject`, `html`, and `text` columns. In addition, there will be records in four related tables, `engagements_metadata_from`, `engagements_metadata_to`, `engagements_metadata_cc`, `engagements_metadata_bcc`.\nA `meeting` engagement has a corresponding `engagements_metadata` object with non-null values in the `body`, `startTime`, `endTime`, and `title` columns.\nA `note` engagement has a corresponding `engagements_metadata` object with non-null values in the `body` column.\nA `task` engagement has a corresponding `engagements_metadata` object with non-null values in the `body`, `status`, and `forObjectType` columns.\n\nNew state strategy on Incremental streams\nDue to some data loss because an entity was updated during the synch, instead of updating the state by reading the latest record the state will be save with the initial synch time. With the proposed `state strategy`, it would capture all possible updated entities in incremental synch.\nPerformance considerations\nThe connector is restricted by normal HubSpot rate limitations.\nSome streams, such as `workflows` need to be enabled before they can be read using a connector authenticated using an `API Key`. If reading a stream that is not enabled, a log message returned to the output and the sync operation only sync the other streams available.\nExample of the output message when trying to read `workflows` stream with missing permissions for the `API Key`:\n`text\n{\n    \"type\": \"LOG\",\n    \"log\": {\n        \"level\": \"WARN\",\n        \"message\": 'Stream `workflows` cannot be proceed. This API Key (EXAMPLE_API_KEY) does not have proper permissions! (requires any of [automation-access])'\n    }\n}`\nHubSpot's API will rate limit the amount of records you can sync daily, so make sure that you are on the appropriate plan if you are planning on syncing more than 250,000 records per day.\nTutorials\nNow that you have set up the Hubspot source connector, check out the following Hubspot tutorial:\nBuild a single customer view with open-source tools",
    "tag": "airbyte"
  },
  {
    "title": "Smaily",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/smaily.md",
    "content": "Smaily\nSync overview\nThis source can sync data from the Smaily API. At present this connector only supports full refresh syncs meaning that each time you use the connector it will sync all available records from scratch. Please use cautiously if you expect your API to have a lot of records.\nThis Source Supports the Following Streams\n\nusers\nsegments\ncampaigns\ntemplates\nautomations\nA/B tests\n\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | No |  |\nPerformance considerations\nThe connector has a rate limit of 5 API requests per second per IP-address.\nGetting started\nRequirements\n\nSmaily API user username\nSmaily API user password\nSmaily API subdomain\n",
    "tag": "airbyte"
  },
  {
    "title": "Salesforce",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/salesforce.md",
    "content": "Salesforce\nSetting up the Salesforce source connector involves creating a read-only Salesforce user and configuring the Salesforce connector through the Airbyte UI.\nThis page guides you through the process of setting up the Salesforce source connector.\nPrerequisites\n\nSalesforce Account with Enterprise access or API quota purchased\nDedicated Salesforce user (optional)\n\n\n\n(For Airbyte Open Source) Salesforce OAuth credentials\n\n\nSetup guide\nStep 1: (Optional, Recommended) Create a read-only Salesforce user\nWhile you can set up the Salesforce connector using any Salesforce user with read permission, we recommend creating a dedicated read-only user for Airbyte. This allows you to granularly control the data Airbyte can read.\nTo create a dedicated read only Salesforce user:\n\nLog into Salesforce with an admin account.\nOn the top right of the screen, click the gear icon and then click Setup.\nIn the left navigation bar, under Administration, click Users > Profiles. The Profiles page is displayed. Click New profile.\nFor Existing Profile, select Read only. For Profile Name, enter Airbyte Read Only User.\nClick Save. The Profiles page is displayed. Click Edit.\nScroll down to the Standard Object Permissions and Custom Object Permissions and enable the Read checkbox for objects that you want to replicate via Airbyte.\nScroll to the top and click Save.\nOn the left side, under Administration, click Users > Users. The All Users page is displayed. Click New User.\nFill out the required fields:\nFor License, select Salesforce.\nFor Profile, select Airbyte Read Only User.\nFor Email, make sure to use an email address that you can access.\n\n\nClick Save.\nCopy the Username and keep it accessible.\nLog into the email you used above and verify your new Salesforce account user. You'll need to set a password as part of this process. Keep this password accessible.\n\nStep 2: Set up Salesforce as a Source in Airbyte\n\nFor Airbyte Cloud:\nTo set up Salesforce as a source in Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click + New source.\nOn the Set up the source page, select Salesforce from the Source type dropdown.\nFor Name, enter a name for the Salesforce connector.\nToggle whether your Salesforce account is a Sandbox account or a production account.\nFor Start Date, enter the date in YYYY-MM-DD format. The data added on and after this date will be replicated. If this field is blank, Airbyte will replicate all data.\n(Optional) In the Salesforce Object filtering criteria section, click Add. From the Search criteria dropdown, select the criteria relevant to you. For Search value, add the search terms relevant to you. If this field is blank, Airbyte will replicate all data.\nClick Authenticate your account to authorize your Salesforce account. Airbyte will authenticate the Salesforce account you are already logged in to. Make sure you are logged into the right account.\nClick Set up source.\n\n\n\nFor Airbyte Open Source:\nTo set up Salesforce as a source in Airbyte Open Source:\n\n\nFollow this walkthrough with the following modifications:\n\nIf your Salesforce URL\u2019s is not in the `X.salesforce.com` format, use your Salesforce domain name. For example, if your Salesforce URL is `awesomecompany.force.com` then use that instead of `awesomecompany.salesforce.com`.\nWhen running a curl command, run it with the `-L` option to follow any redirects.\nIf you created a read-only user, use the user credentials when logging in to generate OAuth tokens.\n\n\n\nNavigate to the Airbute Open Source dashboard and follow the same steps as setting up Salesforce as a source in Airbyte Cloud.\n\n\n\nSupported sync modes\nThe Salesforce source connector supports the following sync modes:\n\nFull Refresh - Overwrite\nFull Refresh - Append\nIncremental Sync - Append\n(Recommended) Incremental Sync - Deduped History\n\nIncremental Deletes Sync\nThe Salesforce connector retrieves deleted records from Salesforce. For the streams which support it, a deleted record will be marked with the field `isDeleted=true` value.\nPerformance considerations\nThe Salesforce connector is restricted by Salesforce\u2019s Daily Rate Limits. The connector syncs data until it hits the daily rate limit, then ends the sync early with success status, and starts the next sync from where it left off. Note that picking up from where it ends will work only for incremental sync, which is why we recommend using the Incremental Sync - Deduped History sync mode.\nSupported Objects\nThe Salesforce connector supports reading both Standard Objects and Custom Objects from Salesforce. Each object is read as a separate stream. See a list of all Salesforce Standard Objects here.\nAirbyte fetches and handles all the possible and available streams dynamically based on:\n\n\nIf the authenticated Salesforce user has the Role and Permissions to read and fetch objects\n\n\nIf the stream has the queryable property set to true. Airbyte can fetch only queryable streams via the API. If you don\u2019t see your object available via Airbyte, check if it is API-accessible to the Salesforce user you authenticated with in Step 2.\n\n\nNote: BULK API cannot be used to receive data from the following streams due to Salesforce API limitations. The Salesforce connector syncs them using the REST API which will occasionally cost more of your API quota:\n\nAcceptedEventRelation\nAttachment\nCaseStatus\nContractStatus\nDeclinedEventRelation\nFieldSecurityClassification\nKnowledgeArticle\nKnowledgeArticleVersion\nKnowledgeArticleVersionHistory\nKnowledgeArticleViewStat\nKnowledgeArticleVoteStat\nOrderStatus\nPartnerRole\nRecentlyViewed\nServiceAppointmentStatus\nShiftStatus\nSolutionStatus\nTaskPriority\nTaskStatus\nUndecidedEventRelation\n\nSalesforce tutorials\nNow that you have set up the Salesforce source connector, check out the following Salesforce tutorials:\n\nReplicate Salesforce data to BigQuery\nReplicate Salesforce and Zendesk data to Keen for unified analytics\n",
    "tag": "airbyte"
  },
  {
    "title": "Lokalise ",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/lokalise.md",
    "content": "Lokalise\nThis page contains the setup guide and reference information for the Lokalise source connector.\nYou can find more information about the Lokalise REST API here.\nPrerequisites\nYou can find your Project ID and find or create an API key within Lokalise.\nSetup guide\nStep 1: Set up the Lokalise connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the Lokalise connector and select Lokalise from the Source type dropdown.\nEnter your `project_id` - Lokalise Project ID.\nEnter your `api_key` - Lokalise API key with read permissions.\nClick Set up source.\n\nFor Airbyte OSS:\n\nNavigate to the Airbyte Open Source dashboard.\nSet the name for your source. \nEnter your `project_id` - Lokalise Project ID.\nEnter your `api_key` - Lokalise API key with read permissions.\nClick Set up source.\n\nSupported sync modes\nThe Lokalise source connector supports the following sync modes:\n| Feature           | Supported? |\n| :---------------- | :--------- |\n| Full Refresh Sync | Yes        |\n| Incremental Sync  | No         |\n| SSL connection    | Yes        |\n| Namespaces        | No         |\nSupported Streams\n\nKeys\nLanguages\nComments\nContributors\nTranslations\n\nData type map\n| Integration Type    | Airbyte Type |\n| :------------------ | :----------- |\n| `string`            | `string`     |\n| `integer`, `number` | `number`     |\n| `array`             | `array`      |\n| `object`            | `object`     |",
    "tag": "airbyte"
  },
  {
    "title": "Amazon SQS",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/amazon-sqs.md",
    "content": "Amazon SQS\nSync overview\nThis source will sync messages from an SQS Queue.\nOutput schema\nThis source will output one stream for the configured SQS Queue.\nThe stream record data will have three fields:\n* id (a UUIDv4 as a STRING)\n* body (message body as a STRING)\n* attributes (attributes of the messages as an OBJECT or NULL)\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | yes |  |\n| Incremental Sync | no |  |\n| Namespaces | no |  |\nPerformance considerations\nGetting started\nRequirements\n\nAWS IAM Access Key\nAWS IAM Secret Key\nAWS SQS Queue\n\nProperties\nRequired properties are 'Queue URL', 'AWS Region' and 'Delete Messages After Read' as noted in bold below.\n\nQueue URL (STRING)\nThe full AWS endpoint URL of the queue e.g. `https://sqs.eu-west-1.amazonaws.com/1234567890/example-queue-url`\nAWS Region (STRING)\nThe region code for the SQS Queue e.g. eu-west-1\nDelete Messages After Read (BOOLEAN)\nWARNING: Setting this option to TRUE can result in data loss, do not enable this option unless you understand the risk. See the Data loss warning section below.\nShould the message be deleted from the SQS Queue after being read? This prevents the message being read more than once\nBy default messages are NOT deleted, thus can be re-read after the `Message Visibility Timeout`\nDefault: False\nMax Batch Size (INTEGER)\nThe max amount of messages to consume in a single poll e.g. 5\nMinimum of 1, maximum of 10\nDefault: 10\nMax Wait Time (INTEGER)\nThe max amount of time (in seconds) to poll for messages before commiting a batch (or timing out) unless we fill a batch (as per `Max Batch Size`)\nMinimum of 1, maximum of 20\nDefault: 20\nMessage Attributes To Return (STRING)\nA comma separated list of Attributes to return for each message\nDefault: All\nMessage Visibility Timeout (INTEGER)\nAfter a message is read, how much time (in seconds) should the message be hidden from other consumers\nAfter this timeout, the message is not deleted and can be re-read\nDefault: 30\nAWS IAM Access Key ID (STRING)\nThe Access Key for the IAM User with permissions on this Queue\nIf `Delete Messages After Read` is `false` then only `sqs:ReceiveMessage`\nIf `Delete Messages After Read` is `true` then `sqs:DeleteMessage` is also needed\nAWS IAM Secret Key (STRING)\nThe Secret Key for the IAM User with permissions on this Queue\n\nData loss warning\nWhen enabling Delete Messages After Read, the Source will delete messages from the SQS Queue after reading them. The message is deleted after the configured Destination takes the message from this Source, but makes no guarentee that the downstream destination has commited/persisted the message. This means that it is possible for the Airbyte Destination to read the message from the Source, the Source deletes the message, then the downstream application fails - resulting in the message being lost permanently.\nExtra care should be taken to understand this risk before enabling this option.\nSetup guide\n\nCreate IAM Keys\nCreate SQS Queue\n\n\nNOTE:\n* If `Delete Messages After Read` is `false` then the IAM User needs only `sqs:ReceiveMessage` in the AWS IAM Policy\n* If `Delete Messages After Read` is `true` then both `sqs:ReceiveMessage` and `sqs:DeleteMessage` are needed in the AWS IAM Policy\n",
    "tag": "airbyte"
  },
  {
    "title": "Dockerhub",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/dockerhub.md",
    "content": "Dockerhub\nSync overview\nThis source can sync data for the DockerHub API. It currently supports only listing public repos and Full Refresh syncing for now. You supply a `docker_username`, and it will sync down all info about repos published under that name.\nOutput schema\nThis Source is capable of syncing the following Streams:\n\nDockerHub\n\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | No |  |\n| Namespaces | No |  |\nPerformance considerations\nThis connector has been tested for the Airbyte organization, which has 266 repos, and works fine. It should not run into limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.\nGetting started\nRequirements\n\nNone\n\nSetup guide\n\nDefine a `docker_username`: the username that the connector will pull all repo data from.\n",
    "tag": "airbyte"
  },
  {
    "title": "Prerequisites",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/facebook-marketing.inapp.md",
    "content": "Prerequisites\n\nA Facebook Ad Account ID\n\nSetup guide\n\nClick Authenticate your account to authorize your Meta for Developers account. Airbyte will authenticate the account you are already logged in to. Make sure you are logged into the right account.\n\nFor Start Date, enter the date in the `YYYY-MM-DDTHH:mm:ssZ` format. The data added on and after this date will be replicated. If this field is blank, Airbyte will replicate all data.\n:::warning\nInsight tables are only able to pull data from 37 months. If you are syncing insight tables and your start date is older than 37 months, your sync will fail.\n:::\n\n\nFor End Date, enter the date in the `YYYY-MM-DDTHH:mm:ssZ` format. The data added on and before this date will be replicated. If this field is blank, Airbyte will replicate the latest data.\n\nFor Account ID, enter your Facebook Ad Account ID Number.\n\n(Optional) Toggle the Include Deleted button to include data from deleted Campaigns, Ads, and AdSets.\n:::info\nThe Facebook Marketing API does not have a concept of deleting records in the same way that a database does. While you can archive or delete an ad campaign, the API maintains a record of the campaign. Toggling the Include Deleted button lets you replicate records for campaigns or ads even if they were archived or deleted from the Facebook platform.\n:::\n\n\n(Optional) Toggle the Fetch Thumbnail Images button to fetch the `thumbnail_url` and store the result in `thumbnail_data_url` for each Ad Creative.\n\n\n(Optional) In the Custom Insights section, click Add.\n    To retrieve specific fields from Facebook Ads Insights combined with other breakdowns, you can choose which fields and breakdowns to sync.\n:::warning\nAdditional streams for Facebook Marketing are dynamically created based on the specified Custom Insights. For an existing Facebook Marketing source, when you are updating or removing Custom Insights, you should also ensure that any connections syncing to these streams are either disabled or have had their source schema refreshed.\n:::\nWe recommend following the Facebook Marketing documentation to understand the breakdown limitations. Some fields can not be requested and many others only work when combined with specific fields. For example, the breakdown `app_id` is only supported with the `total_postbacks` field.\nTo configure Custom Insights:\n\nFor Name, enter a name for the insight. This will be used as the Airbyte stream name\nFor Fields, enter a list of the fields you want to pull from the Facebook Marketing API.\nFor End Date, enter the date in the `YYYY-MM-DDTHH:mm:ssZ` format. The data added on and before this date will be replicated. If this field is blank, Airbyte will replicate the latest data.\nFor Breakdowns, enter a list of the breakdowns you want to configure.\nFor Start Date, enter the date in the `YYYY-MM-DDTHH:mm:ssZ` format. The data added on and after this date will be replicated. If this field is blank, Airbyte will replicate all data.\nFor Time Increment, enter the number of days over which you want to aggregate statistics.\n\n```For example, if you set this value to 7, Airbyte will report statistics as 7-day aggregates starting from the Start Date. Suppose the start and end dates are October 1st and October 30th, then the connector will output 5 records: 01 - 06, 07 - 13, 14 - 20, 21 - 27, and 28 - 30 (3 days only).  \n7. For **Action Breakdown**, enter a list of the action breakdowns you want to configure.\n8. For **Custom Insights Lookback Window**, fill in the appropriate value. See [more](#facebook-marketing-attribution-reporting) on this parameter.\n9. Click **Done**.\n```\n\n\n\nFor Page Size of Requests, fill in the size of the page in case pagintion kicks in. Feel free to ignore it, the default value should work in most cases.\nFor Insights Lookback Window, fill in the appropriate value. See more on this parameter.\n\n\n\n\n",
    "tag": "airbyte"
  },
  {
    "title": "Paystack",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/paystack.md",
    "content": "Paystack\nOverview\nThe Paystack source supports both Full Refresh and Incremental syncs. You can choose if this connector will copy only the new or updated data, or all rows in the tables and columns you set up for replication, every time a sync is run.\nOutput schema\nThis Source is capable of syncing the following core streams:\nCustomers (Incremental)\nDisputes (Incremental)\nInvoices (Incremental)\nRefunds (Incremental)\nSettlements (Incremental)\nSubscriptions (Incremental)\nTransactions (Incremental)\nTransfers (Incremental)\nNote on Incremental Syncs\nThe Paystack API does not allow querying objects which were updated since the last sync. Therefore, this connector uses the `createdAt` field to query for new data in your Paystack account.\nIf your data is updated after creation, you can use the Loockback Window option when configuring the connector to always reload data from the past N days. This will allow you to pick up updates to the data.\nData type mapping\nThe Paystack API is compatible with the JSONSchema types that Airbyte uses internally (`string`, `date-time`, `object`, `array`, `boolean`, `integer`, and `number`), so no type conversions happen as part of this source.\nFeatures\n| Feature | Supported? |\n| :--- | :--- |\n| Full Refresh Sync | Yes |\n| Incremental - Append Sync | Yes |\n| Incremental - Dedupe Sync | Yes |\n| SSL connection | Yes |\nPerformance considerations\nThe Paystack connector should not run into Paystack API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.\nGetting started\nRequirements\n\nPaystack API Secret Key\n\nSetup guide\nVisit the Paystack dashboard settings page with developer level access or more to see the secret key for your account. Secret keys for the live Paystack environment will be prefixed with `sk_live_`.\nUnfortunately Paystack does not yet support restricted permission levels on secret keys. This means that you will have to use the same secret key here that you use for charging customers. Use at your own risk. In the future Paystack might support restricted access levels and in that case Airbyte only requires a read-only access level key.\nIf you would like to test Airbyte using test data on Paystack, `sk_test_` API keys are also supported.",
    "tag": "airbyte"
  },
  {
    "title": "Configcat API",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/configcat.md",
    "content": "Configcat API\nSync overview\nThis source can sync data from the Configcat API. At present this connector only supports full refresh syncs meaning that each time you use the connector it will sync all available records from scratch. Please use cautiously if you expect your API to have a lot of records.\nThis Source Supports the Following Streams\n\norganizations\norganization_members\nproducts\ntags\nenvironments\n\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | No |  |\nPerformance considerations\nConfigcat APIs are under rate limits for the number of API calls allowed per API keys per second. If you reach a rate limit, API will return a 429 HTTP error code. See here\nGetting started\nRequirements\n\nUsername\nPassword\n",
    "tag": "airbyte"
  },
  {
    "title": "Primetric",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/primetric.md",
    "content": "Primetric\nOverview\nThe Primetric source currently supports Full Refresh syncs only. This means that all contents for all chosen streams will be replaced with every sync.\nOutput schema\nThis Source is capable of syncing the following core Streams:\n\nAssignments\nCapacities\nEmployees\nHashtags\nOrganization Clients\nOrganization Company Groups\nOrganization Departments\nOrganization Identity Providers\nOrganization Positions\nOrganization Rag Scopes\nOrganization Roles\nOrganization Seniorities\nOrganization Tags\nOrganization Teams\nOrganization Timeoff Types\nPeople\nProjects\nProjects Vacancies\nRag Ratings\nSkills\nTimeoffs\nWorklogs\n\nIf there are more endpoints you'd like Airbyte to support, please create an issue.\nFeatures\n| Feature                       | Supported? |     |\n| :---------------------------- | :--------- | :-- |\n| Full Refresh Sync             | Yes        |     |\n| Incremental Sync              | No         |     |\n| Replicate Incremental Deletes | No         |     |\n| SSL connection                | Yes        |     |\n| Namespaces                    | No         |     |\nGetting started\nPrimetric facilitates resource planning. With it you can manage your employee's skills and schedule assignment of\nyour employees to the right projects.\nRequirements\n\nPrimetric client secret\nPrimetric client id\n",
    "tag": "airbyte"
  },
  {
    "title": "Display & Video 360",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/dv-360.md",
    "content": "Display & Video 360\nGoogle DoubleClick Bid Manager (DBM) is the API that enables developers to manage Queries and retrieve Reports from Display & Video 360.\nDoubleClick Bid Manager API `v1.1` is the latest available and recommended version.\nLink to the official documentation\nFeatures\n| Feature           | Supported? |\n| :---------------- | :--------- |\n| Full Refresh Sync | Yes        |\n| Incremental Sync  | Yes        |\nSupported Tables\nThis source is capable of syncing the following tables and their data:\n\nAudience Composition\nFloodlight\nReach\nStandard\nUnique Reach Audience\n\nNote: It is recommended to first build the desired report in the UI to avoid any errors, since there are several limilations and requirements pertaining to reporting types, filters, dimensions, and metrics (such as valid combinations of metrics and dimensions).\nAvailable filters and metrics:\nAvailable filters and metrics are provided in this page.\nGetting Started (Airbyte-Cloud)\n\nClick `Authenticate your Display & Video 360 account` to sign in with Google and authorize your account.\nGet the partner ID for your account.\nFill out a start date, and optionally, an end date and filters (check the Queries documentation) .\nYou're done.\n\nGetting Started (Airbyte Open-Source)\nRequirements\nYou can use the setup tool to create credentials and enable the DBM API in the Google API Console.\n\naccess_token\nrefresh_token\ntoken_uri\nclient_id\nclient_secret\nstart_date\nend_date\npartner_id\nfilters\n\nSetup guide\n\n\nGetting Started guide\n\n\nUsing OAuth 2.0 to Access Google APIs\n\n\nRate Limiting & Performance Considerations (Airbyte Open Source)\nThis source is constrained by the limits set by the DBM API. You can read more about those limits in the Display & Video 360 docs.",
    "tag": "airbyte"
  },
  {
    "title": "GitLab",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/gitlab.md",
    "content": "GitLab\nThis page contains the setup guide and reference information for the Gitlab Source connector.\nPrerequisites\n\nGitlab instance or an account at Gitlab\nStart date\nGitLab Groups (Optional)\nGitLab Projects (Optional)\n\n\nFor Airbyte Cloud:\n\nPersonal Access Token (see personal access token)\nOAuth\n\n\n\nFor Airbyte Open Source:\n\nPersonal Access Token (see personal access token)\n\n\nSetup guide\nStep 1: Set up GitLab\nCreate a GitLab Account or set up a local instance of GitLab.\n\nAirbyte Open Source additional setup steps\nLog into GitLab and then generate a personal access token. Your token should have the `read_api` scope, that Grants read access to the API, including all groups and projects, the container registry, and the package registry.\n\n\nStep 2: Set up the GitLab connector in Airbyte\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click + new source.\nOn the source setup page, select GitLab from the Source type dropdown and enter a name for this connector.\nClick `Authenticate your GitLab account` by selecting Oauth or Personal Access Token for Authentication.\nLog in and Authorize to the GitLab account.\nStart date - The date from which you'd like to replicate data for streams.\nAPI URL - The URL to access you self-hosted GitLab instance or `gitlab.com` (default).\nGroups (Optional) - Space-delimited list of GitLab group IDs, e.g. `airbytehq` for single group, `airbytehq another-repo` for multiple groups.\nProjects (Optional) - Space-delimited list of GitLab projects to pull data for, e.g. `airbytehq/airbyte`.\nClick Set up source.\n\nNote: You can specify either Group IDs or Project IDs in the source configuration. If both fields are blank, the connector will retrieve a list of all the groups that are accessible to the configured token and ingest as normal.\n\n\nFor Airbyte Open Source:\n\nAuthenticate with Personal Access Token.\n\n\nSupported sync modes\nThe Gitlab Source connector supports the following  sync modes:\n\nFull Refresh - Overwrite\nFull Refresh - Append\nIncremental - Append\nIncremental - Deduped History\n\nSupported Streams\nThis connector outputs the following streams:\n\nBranches\nCommits (Incremental)\nIssues (Incremental)\nGroup Issue Boards\nPipelines (Incremental)\nJobs\nProjects\nProject Milestones\nProject Merge Requests (Incremental)\nUsers\nGroups\nGroup Milestones\nGroup and Project members\nTags\nReleases\nGroup Labels\nProject Labels\nEpics (only available for GitLab Ultimate and GitLab.com Gold accounts)\nEpic Issues (only available for GitLab Ultimate and GitLab.com Gold accounts)\n\nAdditional information\nGitLab source works with GitLab API v4. It can also work with self-hosted GitLab API v4.\nPerformance considerations\nGitlab has the rate limits, but the Gitlab connector should not run into Gitlab API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.",
    "tag": "airbyte"
  },
  {
    "title": "Microsoft Dataverse",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/microsoft-dataverse.md",
    "content": "Microsoft Dataverse\nSync overview\nThis source can sync data for the Microsoft Dataverse API to work with Microsoft Dataverse.\nThis connector currently uses version v9.2 of the API\nOutput schema\nThis source will automatically discover the schema of the Entities of your Dataverse instance using the API\n`https://<url>/api/data/v9.2/EntityDefinitions?$expand=Attributes`\nData type mapping\n| Integration Type   | Airbyte Type              | Notes                 |\n| :----------------- | :------------------------ | :-------------------- |\n| `String`           | `string`                  |                       |\n| `UniqueIdentifier` | `string`                  |                       |\n| `DateTime`         | `timestamp with timezone` |                       |\n| `Integer`          | `integer`                 |                       |\n| `BigInt`           | `integer`                 |                       |\n| `Money`            | `number`                  |                       |\n| `Boolean`          | `boolean`                 |                       |\n| `Double`           | `number`                  |                       |\n| `Decimal`          | `number`                  |                       |\n| `Status`           | `integer`                 |                       |\n| `State`            | `integer`                 |                       |\n| `Virtual`          | None                      | We skip virtual types |\nOther types are defined as `string`.\nFeatures\n| Feature                       | Supported?(Yes/No) | Notes                                                      |\n| :---------------------------- | :------------------- | :--------------------------------------------------------- |\n| Full Refresh Sync             | Yes                  |                                                            |\n| Incremental Sync              | Yes                  |                                                            |\n| CDC                           | Yes                  | Not all entities support it. Deleted data only have the ID |\n| Replicate Incremental Deletes | Yes                  |                                                            |\n| SSL connection                | Yes                  |                                                            |\n| Namespaces                    | No                   |                                                            |\nGetting started\nRequirements\n\nApplication (client) ID\nDirectory (tenant) ID\nClient secrets\n\nSetup guide\nThe Microsoft Dataverse API uses OAuth2 for authentication. We need a 'client_credentials' type, that we usually get by using an App Registration.\nhttps://learn.microsoft.com/en-us/power-apps/developer/data-platform/authenticate-oauth\nThe procedure to generate the credentials and setup the necessary permissions is well described in this post from Magnetism blog:\nhttps://blog.magnetismsolutions.com/blog/paulnieuwelaar/2021/9/21/setting-up-an-application-user-in-dynamics-365",
    "tag": "airbyte"
  },
  {
    "title": "Microsoft Dynamics GP",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/microsoft-dynamics-gp.md",
    "content": "Microsoft Dynamics GP\nMS Dynamics GP is a mid-market business accounting or enterprise resource planning (ERP) software.\nSync overview\nMS Dynamics GP runs on the MSSQL database. You can use the MSSQL connector to sync your MS Dynamics GP instance by connecting to the underlying database.\n:::info\nReach out to your service representative or system admin to find the parameters required to connect to the underlying database\n:::\nOutput schema\nTo understand your MS Dynamics GP database schema, see the Microsoft docs. Otherwise, the schema will be loaded according to the rules of MSSQL connector.",
    "tag": "airbyte"
  },
  {
    "title": "Fauna",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/fauna.md",
    "content": "Fauna\nThis page guides you through setting up a Fauna source.\nOverview\nThe Fauna source supports the following sync modes:\n\nFull Sync - exports all the data from a Fauna collection.\nIncremental Sync - exports data incrementally from a Fauna collection.\n\nYou need to create a separate source per collection that you want to export.\nPreliminary setup\nEnter the domain of the collection's database that you are exporting. The URL can be found in\nthe docs.\nFull sync\nFollow these steps if you want this connection to perform a full sync.\n\nCreate a role that can read the collection that you are exporting. You can create the role in the Dashboard or the fauna shell with the following query:\n`javascript\nCreateRole({\n  name: \"airbyte-readonly\",\n  privileges: [\n    {\n      resource: Collections(),\n      actions: { read: true }\n    },\n    {\n      resource: Indexes(),\n      actions: { read: true }\n    },\n    {\n      resource: Collection(\"COLLECTION_NAME\"),\n      actions: { read: true }\n    }\n  ],\n})`\n\nReplace `COLLECTION_NAME` with the name of the collection configured for this connector. If you'd like to sync\nmultiple collections, add an entry for each additional collection you'd like to sync. For example, to sync\n`users` and `products`, run this query instead:\n`javascript\nCreateRole({\n  name: \"airbyte-readonly\",\n  privileges: [\n    {\n      resource: Collections(),\n      actions: { read: true }\n    },\n    {\n      resource: Indexes(),\n      actions: { read: true }\n    },\n    {\n      resource: Collection(\"users\"),\n      actions: { read: true }\n    },\n    {\n      resource: Collection(\"products\"),\n      actions: { read: true }\n    }\n  ],\n})`\n\nCreate a key with that role. You can create a key using this query:\n`javascript\nCreateKey({\n  name: \"airbyte-readonly\",\n  role: Role(\"airbyte-readonly\"),\n})`\nCopy the `secret` output by the `CreateKey` command and enter that as the \"Fauna Secret\" on the left.\n   Important: The secret is only ever displayed once. If you lose it, you would have to create a new key.\n\nIncremental sync\nFollow these steps if you want this connection to perform incremental syncs.\n\nCreate the \"Incremental Sync Index\". This allows the connector to perform incremental syncs. You can create the index with the fauna shell or in the Dashboard with the following query:\n`javascript\nCreateIndex({\n  name: \"INDEX_NAME\",\n  source: Collection(\"COLLECTION_NAME\"),\n  terms: [],\n  values: [\n    { \"field\": \"ts\" },\n    { \"field\": \"ref\" }\n  ]\n})`\n\nReplace `COLLECTION_NAME` with the name of the collection configured for this connector.\nReplace `INDEX_NAME` with the name that you configured for the Incremental Sync Index.\nRepeat this step for every collection you'd like to sync.\n\nCreate a role that can read the collection, the index, and the metadata of all indexes. It needs access to index metadata in order to validate the index settings. You can create the role with this query:\n`javascript\nCreateRole({\n  name: \"airbyte-readonly\",\n  privileges: [\n    {\n      resource: Collections(),\n      actions: { read: true }\n    },\n    {\n      resource: Indexes(),\n      actions: { read: true }\n    },\n    {\n      resource: Collection(\"COLLECTION_NAME\"),\n      actions: { read: true }\n    },\n    {\n      resource: Index(\"INDEX_NAME\"),\n      actions: { read: true }\n    }\n  ],\n})`\n\nReplace `COLLECTION_NAME` with the name of the collection configured for this connector.\nReplace `INDEX_NAME` with the name that you configured for the Incremental Sync Index.\nIf you'd like to sync multiple collections, add an entry for every collection and index\nyou'd like to sync. For example, to sync `users` and `products` with Incremental Sync, run\nthe following query:\n`javascript\nCreateRole({\n  name: \"airbyte-readonly\",\n  privileges: [\n    {\n      resource: Collections(),\n      actions: { read: true }\n    },\n    {\n      resource: Indexes(),\n      actions: { read: true }\n    },\n    {\n      resource: Collection(\"users\"),\n      actions: { read: true }\n    },\n    {\n      resource: Index(\"users-ts\"),\n      actions: { read: true }\n    },\n    {\n      resource: Collection(\"products\"),\n      actions: { read: true }\n    },\n    {\n      resource: Index(\"products-ts\"),\n      actions: { read: true }\n    }\n  ],\n})`\n\nCreate a key with that role. You can create a key using this query:\n`javascript\nCreateKey({\n  name: \"airbyte-readonly\",\n  role: Role(\"airbyte-readonly\"),\n})`\nCopy the `secret` output by the `CreateKey` command and enter that as the \"Fauna Secret\" on the left.\n   Important: The secret is only ever displayed once. If you lose it, you would have to create a new key.\n\nExport formats\nThis section captures export formats for all special case data stored in Fauna. This list is exhaustive.\nNote that the `ref` column in the exported database contains only the document ID from each document's\nreference (or \"ref\"). Since only one collection is involved in each connector configuration, it is\ninferred that the document ID refers to a document within the synced collection.\n|                                  Fauna Type                                         |                             Format                                  |                   Note                      |\n| ----------------------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------- |\n| Document Ref  | `{ id: \"id\", \"collection\": \"collection-name\", \"type\": \"document\" }` |                                             |\n| Other Ref     | `{ id: \"id\", \"type\": \"ref-type\" }`                                  | This includes all other refs, listed below. |\n| Byte Array   | base64 url formatting                                               |                                             |\n| Timestamp    | date-time, or an iso-format timestamp                               |                                             |\n| Query, SetRef | a string containing the wire protocol of this value                 | The wire protocol is not documented.        |\nRef types\nEvery ref is serialized as a JSON object with 2 or 3 fields, as listed above. The `type` field must be\none of these strings:\n|                                    Reference Type                                       |    `type` string    |\n| --------------------------------------------------------------------------------------- | ------------------- |\n| Document                                                                                | `\"document\"`        |\n| Collection         | `\"collection\"`      |\n| Database             | `\"database\"`        |\n| Index                  | `\"index\"`           |\n| Function             | `\"function\"`        |\n| Role                     | `\"role\"`            |\n| AccessProvider | `\"access_provider\"` |\n| Key                      | `\"key\"`             |\n| Token                  | `\"token\"`           |\n| Credential        | `\"credential\"`      |\nFor all other refs (for example if you stored the result of `Collections()`), the `type` must be `\"unknown\"`.\nThere is a difference between a specific collection ref (retrieved with `Collection(\"name\")`), and all the reference\nto all collections (retrieved with `Collections()`). This is why the `type` is `\"unknown\"` for `Collections()`,\nbut not for `Collection(\"name\")`\nTo select the document ID from a ref, add `\"id\"` to the \"Path\" of the additional column. For example, if \"Path\"\nis `[\"data\", \"parent\"]`, change the \"Path\" to `[\"data\", \"parent\", \"id\"]`.\nTo select the collection name, add `\"collection\", \"id\"` to the \"Path\" of the additional column. For example, if\n\"Path\" is `[\"data\", \"parent\"]`, change the \"Path\" to `[\"data\", \"parent\", \"collection\", \"id\"]`. Internally, the\nFQL Select is used.",
    "tag": "airbyte"
  },
  {
    "title": "Microsoft Teams",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/microsoft-teams.md",
    "content": "Microsoft Teams\nSync overview\nThis source can sync data for the Microsoft Graph API to work with Microsoft Teams.\nThere are currently 2 versions of Microsoft Graph REST APIs - v1.0 and beta. Beta version contains new or enhanced APIs that are still in preview status. But APIs in preview status are subject to change, and may break existing scenarios without notice. It isn't recommended taking a production dependency on APIs in the beta endpoint. This Source Connector is based on a API v1.0.\nOutput schema\nThis Source is capable of syncing the following core Streams:\n\nusers\ngroups\ngroup_members\ngroup_owners\nchannels\nchannel_members\nchannel_tabs\nconversations\nconversation_threads\nconversation_posts\nteam_drives\nteam_device_usage_report\n\nIf there are more endpoints you'd like Airbyte to support, please create an issue.\nSome APIs aren't supported in v1.0, e.g. channel messages and channel messages replies.\nData type mapping\n| Integration Type | Airbyte Type | Notes |\n| :--- | :--- | :--- |\n| `string` | `string` |  |\n| `number` | `number` |  |\n| `array` | `array` |  |\n| `object` | `object` |  |\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | Coming soon |  |\n| Replicate Incremental Deletes | Coming soon |  |\n| SSL connection | Yes |  |\n| Namespaces | No |  |\nPerformance considerations\nThe connector is restricted by normal Microsoft Graph requests limitation.\nGetting started\nRequirements\n\nApplication (client) ID \nDirectory (tenant) ID\nClient secrets \n\nSetup guide\nThe Microsoft Graph API uses OAuth for authentication. Microsoft Graph exposes granular permissions that control the access that apps have to resources, like users, groups, and mail. When a user signs in to your app they, or, in some cases, an administrator, are given a chance to consent to these permissions. If the user consents, your app is given access to the resources and APIs that it has requested. For apps that don't take a signed-in user, permissions can be pre-consented to by an administrator when the app is installed.\nMicrosoft Graph has two types of permissions:\n\nDelegated permissions are used by apps that have a signed-in user present. For these apps, either the user or an administrator consents to the permissions that the app requests, and the app can act as the signed-in user when making calls to Microsoft Graph. Some delegated permissions can be consented by non-administrative users, but some higher-privileged permissions require administrator consent.\nApplication permissions are used by apps that run without a signed-in user present; for example, apps that run as background services or daemons. Application permissions can only be consented by an administrator.\n\nThis source requires Application permissions. Follow these instructions for creating an app in the Azure portal. This process will produce the `client_id`, `client_secret`, and `tenant_id` needed for the tap configuration file.\n\nLogin to Azure Portal\nClick upper-left menu icon and select Azure Active Directory\nSelect App Registrations\nClick New registration\nRegister an application\nName: \nSupported account types: Accounts in this organizational directory only\nRegister (button)\nRecord the client_id, tenant_id, and which will be used by the tap for authentication and API integration.\nSelect Certificates & secrets\nProvide Description and Expires\nDescription: tap-microsoft-teams client secret\nExpires: 1-year\nAdd\nCopy the client secret value, this will be the client_secret\nSelect API permissions\nClick Add a permission\n\n\nSelect Microsoft Graph\nSelect Application permissions\nSelect the following permissions:\nUsers \nUser.Read.All\nUser.ReadWrite.All \nDirectory.Read.All\nDirectory.ReadWrite.All\nGroups\nGroupMember.Read.All\nGroup.Read.All\nDirectory.Read.All\nGroup.ReadWrite.All \nDirectory.ReadWrite.All\nGroup members\nGroupMember.Read.All\nGroup.Read.All\nDirectory.Read.All\nGroup owners\nGroup.Read.All\nUser.Read.All\nGroup.Read.All\nUser.ReadWrite.All\nGroup.Read.All\nUser.Read.All\nApplication.Read.All\nChannels\nChannelSettings.Read.Group\nChannelSettings.ReadWrite.Group\nChannel.ReadBasic.All\nChannelSettings.Read.All\nChannelSettings.ReadWrite.All\nGroup.Read.All \nGroup.ReadWrite.All\nDirectory.Read.All\nDirectory.ReadWrite.All\nChannel members\nChannelMember.Read.All\nChannelMember.ReadWrite.All\nChannel tabs\nTeamsTab.Read.Group\nTeamsTab.ReadWrite.Group\nTeamsTab.Read.All\nTeamsTab.ReadWriteForTeam.All\nTeamsTab.ReadWrite.All\nGroup.Read.All\nGroup.ReadWrite.All\nDirectory.Read.All\nDirectory.ReadWrite.All\nConversations\nGroup.Read.All\nGroup.ReadWrite.All\nConversation threads\nGroup.Read.All\nGroup.ReadWrite.All\nConversation posts\nGroup.Read.All\nGroup.ReadWrite.All\n\n\nTeam drives\nFiles.Read.All \nFiles.ReadWrite.All\nSites.Read.All\nSites.ReadWrite.All\n\n\nTeam device usage report\nReports.Read.All\n\n\n\n\nClick Add permissions\n\nToken acquiring implemented by instantiate the confidential client application with a client secret and calling AcquireTokenForClient from Microsoft Authentication Library (MSAL) for Python",
    "tag": "airbyte"
  },
  {
    "title": "Google Search Console",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/google-search-console.md",
    "content": "Google Search Console\nThis page contains the setup guide and reference information for the google search console source connector.\nPrerequisites\n\nCredentials to a Google Service Account (or Google Service Account with delegated Domain Wide Authority) or Google User Account\n\n:::note\nSince Google has deprecated certain OAuth workflows, OAuth isn't supported for this connector at this time.\n:::\nSetup guide\nStep 1: Set up google search console\nHow to create the client credentials for Google Search Console, to use with Airbyte?\nYou can either:\n\nUse the existing `Service Account` for your Google Project with granted Admin Permissions\nUse your personal Google User Account with oauth. If you choose this option, your account must have permissions to view the Google Search Console project you choose.\nCreate the new `Service Account` credentials for your Google Project, and grant Admin Permissions to it\nFollow the `Delegating domain-wide authority` process to obtain the necessary permissions to your google account from the administrator of Workspace\n\nCreating a Google service account\nA service account's credentials include a generated email address that is unique and at least one public/private key pair. If domain-wide delegation is enabled, then a client ID is also part of the service account's credentials.\n\nOpen the Service accounts page\nIf prompted, select an existing project, or create a new project.\nClick `+ Create service account`.\nUnder Service account details, type a `name`, `ID`, and `description` for the service account, then click `Create`.\nOptional: Under `Service account permissions`, select the `IAM roles` to grant to the service account, then click `Continue`.\nOptional: Under `Grant users access to this service account`, add the `users` or `groups` that are allowed to use and manage the service account.\nGo to API Console/Credentials, check the `Service Accounts` section, click on the Email address of service account you just created.\nOpen `Details` tab and find `Show domain-wide delegation`, checkmark the `Enable Google Workspace Domain-wide Delegation`.\nOn `Keys` tab click `+ Add key`, then click `Create new key`.\n\nYour new public/private key pair should be now generated and downloaded to your machine as `<project_id>.json` you can find it in the `Downloads` folder or somewhere else if you use another default destination for downloaded files. This file serves as the only copy of the private key. You are responsible for storing it securely. If you lose this key pair, you will need to generate a new one!\nUsing the existing Service Account\n\nGo to API Console/Credentials, check the `Service Accounts` section, click on the Email address of service account you just created.\nClick on `Details` tab and find `Show domain-wide delegation`, checkmark the `Enable Google Workspace Domain-wide Delegation`.\nOn `Keys` tab click `+ Add key`, then click `Create new key`.\n\nYour new public/private key pair should be now generated and downloaded to your machine as `<project_id>.json` you can find it in the `Downloads` folder or somewhere else if you use another default destination for downloaded files. This file serves as the only copy of the private key. You are responsible for storing it securely. If you lose this key pair, you will need to generate a new one!\nNote\nYou can return to the API Console/Credentials at any time to view the email address, public key fingerprints, and other information, or to generate additional public/private key pairs. For more details about service account credentials in the API Console, see Service accounts in the API Console help file.\nCreate a Service Account with delegated domain-wide authority\nFollow the Google Documentation for performing Delegating domain-wide authority to create a Service account with delegated domain-wide authority. This account must be created by an administrator of your Google Workspace. Please make sure to grant the following `OAuth scopes` to the service user:\n\n`https://www.googleapis.com/auth/webmasters.readonly`\n\nAt the end of this process, you should have JSON credentials to this Google Service Account.\nStep 2: Set up the google search console connector in Airbyte\n\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the google search console connector and select google search console from the Source type dropdown.\nClick Authenticate your account to sign in with Google and authorize your account.\nFill in the `site_urls` field.\nFill in the `start date` field.\nFill in the `custom reports` (optionally) in format `{\"name\": \"<report-name>\", \"dimensions\": [\"<dimension-name>\", ...]}`\nYou should be ready to sync data.\n\n\n\nFor Airbyte Open Source:\n\nFill in the `service_account_info` and `email` fields for authentication.\nFill in the `site_urls` field.\nFill in the `start date` field.\nFill in the `custom reports` (optionally) in format `{\"name\": \"<report-name>\", \"dimensions\": [\"<dimension-name>\", ...]}`\nYou should be ready to sync data.\n\n\nSupported sync modes\nThe google search console source connector supports the following sync modes:\n| Feature           | Supported?(Yes/No) | Notes                     |\n| :---------------- | :------------------- | :------------------------ |\n| Full Refresh Sync | Yes                  |                           |\n| Incremental Sync  | Yes                  | except Sites and Sitemaps |\n| SSL connection    | Yes                  |                           |\n| Namespaces        | No                   |                           |\nSupported Streams\n\nSites\nSitemaps\nFull Analytics report (this stream has a long sync time because it is very detailed, use with care)\nAnalytics report by country\nAnalytics report by date\nAnalytics report by device\nAnalytics report by page\nAnalytics report by query\nAnalytics report by custom dimensions\n\nPerformance considerations\nThis connector attempts to back off gracefully when it hits Reports API's rate limits. To find more information about limits, see Usage Limits documentation.\nData type map\n| Integration Type | Airbyte Type | Notes |\n| :--------------- | :----------- | :---- |\n| `string`         | `string`     |       |\n| `number`         | `number`     |       |\n| `array`          | `array`      |       |\n| `object`         | `object`     |       |",
    "tag": "airbyte"
  },
  {
    "title": "Customer.io",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/customer-io.md",
    "content": "Customer.io\nOverview\nThe Customer.io source is maintained by Faros\nAI.\nPlease file any support requests on that repo to minimize response time from the\nmaintainers. The source supports both Full Refresh and Incremental syncs. You\ncan choose if this source will copy only the new or updated data, or all rows\nin the tables and columns you set up for replication, every time a sync is run.\nOutput schema\nSeveral output streams are available from this source:\n\nCampaigns (Incremental)\nCampaign Actions (Incremental)\nNewsletters (Incremental)\n\nIf there are more endpoints you'd like Faros AI to support, please create an\nissue.\nFeatures\n| Feature | Supported? |\n| :--- | :--- |\n| Full Refresh Sync | Yes |\n| Incremental Sync | Yes |\n| SSL connection | Yes |\n| Namespaces | No |\nPerformance considerations\nThe Customer.io API is divided into three different hosts, each serving a\ndifferent component of Customer.io. This source only uses the Beta API host,\nwhich enforces a rate limit of 10 requests per second.  Please create an\nissue if you see any\nrate limit issues.\nGetting started\nRequirements\n\nCustomer.io App API Key\n\nPlease follow the their documentation for generating an App API Key.",
    "tag": "airbyte"
  },
  {
    "title": "Facebook Marketing",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/facebook-marketing.md",
    "content": "Facebook Marketing\nThis page guides you through the process of setting up the Facebook Marketing source connector.\nPrerequisites\n\nA Facebook Ad Account ID\n\n\n\n(For Open Source) A Facebook App with the Marketing API enabled\n\n\nSetup guide\n\nFor Airbyte Cloud:\nTo set up Facebook Marketing as a source in Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click + New source.\nOn the Set up the source page, select Facebook Marketing from the Source type dropdown.\nFor Name, enter a name for the Facebook Marketing connector.\nClick Authenticate your account to authorize your Meta for Developers account. Airbyte will authenticate the account you are already logged in to. Make sure you are logged into the right account.\n\nFor Start Date, enter the date in the `YYYY-MM-DDTHH:mm:ssZ` format. The data added on and after this date will be replicated. If this field is blank, Airbyte will replicate all data.\n:::warning\nInsight tables are only able to pull data from 37 months. If you are syncing insight tables and your start date is older than 37 months, your sync will fail.\n:::\n\n\nFor End Date, enter the date in the `YYYY-MM-DDTHH:mm:ssZ` format. The data added on and before this date will be replicated. If this field is blank, Airbyte will replicate the latest data.\n\nFor Account ID, enter your Facebook Ad Account ID Number.\n\n(Optional) Toggle the Include Deleted button to include data from deleted Campaigns, Ads, and AdSets.\n:::info\nThe Facebook Marketing API does not have a concept of deleting records in the same way that a database does. While you can archive or delete an ad campaign, the API maintains a record of the campaign. Toggling the Include Deleted button lets you replicate records for campaigns or ads even if they were archived or deleted from the Facebook platform.\n:::\n\n\n(Optional) Toggle the Fetch Thumbnail Images button to fetch the `thumbnail_url` and store the result in `thumbnail_data_url` for each Ad Creative.\n\n\n(Optional) In the Custom Insights section, click Add.\n    To retrieve specific fields from Facebook Ads Insights combined with other breakdowns, you can choose which fields and breakdowns to sync.\n:::warning\nAdditional streams for Facebook Marketing are dynamically created based on the specified Custom Insights. For an existing Facebook Marketing source, when you are updating or removing Custom Insights, you should also ensure that any connections syncing to these streams are either disabled or have had their source schema refreshed.\n:::\nWe recommend following the Facebook Marketing documentation to understand the breakdown limitations. Some fields can not be requested and many others only work when combined with specific fields. For example, the breakdown `app_id` is only supported with the `total_postbacks` field.\nTo configure Custom Insights:\n\nFor Name, enter a name for the insight. This will be used as the Airbyte stream name\nFor Fields, enter a list of the fields you want to pull from the Facebook Marketing API.\nFor End Date, enter the date in the `YYYY-MM-DDTHH:mm:ssZ` format. The data added on and before this date will be replicated. If this field is blank, Airbyte will replicate the latest data.\nFor Breakdowns, enter a list of the breakdowns you want to configure.\nFor Start Date, enter the date in the `YYYY-MM-DDTHH:mm:ssZ` format. The data added on and after this date will be replicated. If this field is blank, Airbyte will replicate all data.\nFor Time Increment, enter the number of days over which you want to aggregate statistics.\n\n```For example, if you set this value to 7, Airbyte will report statistics as 7-day aggregates starting from the Start Date. Suppose the start and end dates are October 1st and October 30th, then the connector will output 5 records: 01 - 06, 07 - 13, 14 - 20, 21 - 27, and 28 - 30 (3 days only).  \n7. For **Action Breakdown**, enter a list of the action breakdowns you want to configure.\n8. For **Custom Insights Lookback Window**, fill in the appropriate value. See [more](#facebook-marketing-attribution-reporting) on this parameter.\n9. Click **Done**.\n```\n\n\n\nFor Page Size of Requests, fill in the size of the page in case pagintion kicks in. Feel free to ignore it, the default value should work in most cases.\nFor Insights Lookback Window, fill in the appropriate value. See more on this parameter.\nClick Set up source.\n\n\n\n\n\n\n\nFor Airbyte Open Source:\nTo set up Facebook Marketing as a source in Airbyte Open Source:\n\nNavigate to Meta for Developers and create an app with the app type Business.\nFrom your App\u2019s dashboard, setup the Marketing API.\nGenerate a Marketing API access token: From your App\u2019s Dashboard, click Marketing API --> Tools. Select all the available token permissions (`ads_management`, `ads_read`, `read_insights`, `business_management`) and click Get token. Copy the generated token for later use.\n\nRequest a rate increase limit: Facebook heavily throttles API tokens generated from Facebook Apps with the \"Standard Access\" tier (the default tier for new apps), making it infeasible to use the token for syncs with Airbyte. You'll need to request an upgrade to Advanced Access for your app on the following permissions:\n\nAds Management Standard Access\nads_read\nAds_management\n\nSee the Facebook documentation on Authorization to request Advanced Access to the relevant permissions.\n5. Navigate to the Airbyte Open Source Dashboard. Add the access token when prompted to do so and follow the same instructions as for setting up the Facebook Connector on Airbyte Cloud.\n\n\n\nSupported sync modes\nThe Facebook Marketing source connector supports the following sync modes:\n\nFull Refresh - Overwrite\nFull Refresh - Append\nIncremental Sync - Append (except for the AdCreatives and AdAccount tables)\nIncremental Sync - Deduped History (except for the AdCreatives and AdAccount tables)\n\nSupported tables\nYou can replicate the following tables using the Facebook Marketing connector:\n\nActivities\nAdAccount\nAdCreatives\nAdSets\nAds\nAdInsights\nCampaigns\nCustomConversions\nImages\nVideos\n\nYou can segment the AdInsights table into parts based on the following information. Each part will be synced as a separate table if normalization is enabled:\n\nCountry\nDMA (Designated Market Area)\nGender & Age\nPlatform & Device\nRegion\n\nFor more information, see the Facebook Insights API documentation.\nPay attention, that not all fields (e.g. conversions, conversion_values) will be returned for AdInsights, see docs.\nTo get all fields You should use custom insights with breakdowns.\nFacebook Marketing Attribution Reporting\nPlease be informed that the connector uses the `lookback_window` parameter to perform the repetitive read of the last `<lookback_window>` days in the Incremental sync mode. This means some data will be synced twice (or possibly more often) despite the cursor value being up-to-date. You can change this date window by modifying the `lookback_window` parameter when setting up the source. The smaller the value - the fewer duplicates you will have. The greater the value - the more precise results you will get. More details on what the attribution window is and what purpose it serves can be found in this Facebook Article.\nData type mapping\n| Integration Type | Airbyte Type |\n| :--------------: | :----------: |\n|      string      |    string    |\n|      number      |    number    |\n|      array       |    array     |\n|      object      |    object    |",
    "tag": "airbyte"
  },
  {
    "title": "Zoho CRM",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/zoho-crm.md",
    "content": "Zoho CRM\nSync overview\nThe Zoho CRM source supports both Full Refresh and Incremental syncs. You can choose if this connector will copy only the new or updated data, or all rows in the tables and columns you set up for replication, every time a sync is run.\nAirbyte uses REST API to fetch data from Zoho CRM.\nThis Source Connector is based on an Airbyte CDK.\nOutput schema\nThis Source is capable of syncing:\n\nstandard modules available in Zoho CRM account\ncustom modules manually added by user, available in Zoho CRM account\ncustom fields in both standard and custom modules, available in Zoho CRM account\n\nThe discovering of Zoho CRM module schema is made dynamically based on Metadata API and should generally take no longer than 10 to 30 seconds.\nNotes:\nSome of Zoho CRM Modules may not be available for sync due to limitations of Zoho CRM Edition or permissions scope. For details refer to the Scopes section in the Zoho CRM documentation.\nConnector streams and schemas are built dynamically on top of Metadata that is available from the REST API - please see Modules API, Modules Metadata API, Fields Metadata API.\nThe list of available streams is the list of Modules as long as Module Metadata is available for each of them from the Zoho CRM API, and Fields Metadata is available for each of the fields. If a module you want to sync is not available from this connector, it's because the Zoho CRM API does not make it available. \nData type mapping\n| Integration Type      | Airbyte Type | Notes                     |\n|:----------------------|:-------------|:--------------------------|\n| `boolean`             | `boolean`    |                           |\n| `double`              | `number`     |                           |\n| `currency`            | `number`     |                           |\n| `integer`             | `integer`    |                           |\n| `profileimage`        | `string`     |                           |\n| `picklist`            | `string`     | enum                      |\n| `textarea`            | `string`     |                           |\n| `website`             | `string`     | format: uri               |\n| `date`                | `string`     | format: date              |\n| `datetime`            | `string`     | format: date-time         |\n| `text`                | `string`     |                           |\n| `phone`               | `string`     |                           |\n| `bigint`              | `string`     | airbyte_type: big_integer |\n| `event_reminder`      | `string`     |                           |\n| `email`               | `string`     | format: email             |\n| `autonumber`          | `string`     | airbyte_type: big_integer |\n| `jsonarray`           | `array`      |                           |\n| `jsonobject`          | `object`     |                           |\n| `multiselectpicklist` | `array`      |                           |\n| `lookup`              | `object`     |                           |\n| `ownerlookup`         | `object`     |                           |\n| `RRULE`               | `object`     |                           |\n| `ALARM`               | `object`     |                           |\nAny other data type not listed in the table above will be treated as `string`.\nFeatures\n| Feature                                   | Supported? (Yes/No) |\n|:------------------------------------------|:----------------------|\n| Full Refresh Overwrite Sync               | Yes                   |\n| Full Refresh Append Sync                  | Yes                   |\n| Incremental - Append Sync                 | Yes                   |\n| Incremental - Append + Deduplication Sync | Yes                   |\n| Namespaces                                | No                    |\nList of Supported Environments for Zoho CRM\nProduction\n| Environment | Base URL                |\n|:------------|:------------------------|\n| US          | https://zohoapis.com    |\n| AU          | https://zohoapis.com.au |\n| EU          | https://zohoapis.eu     |\n| IN          | https://zohoapis.in     |\n| CN          | https://zohoapis.com.cn |\n| JP          | https://zohoapis.jp     |\nSandbox\n| Environment | Endpoint                        |\n|:------------|:--------------------------------|\n| US          | https://sandbox.zohoapis.com    |\n| AU          | https://sandbox.zohoapis.com.au |\n| EU          | https://sandbox.zohoapis.eu     |\n| IN          | https://sandbox.zohoapis.in     |\n| CN          | https://sandbox.zohoapis.com.cn |\n| JP          | https://sandbox.zohoapis.jp     |\nDeveloper\n| Environment | Endpoint                           |\n|:------------|:-----------------------------------|\n| US          | https://developer.zohoapis.com     |\n| AU          | https://developer.zohoapis.com.au  |\n| EU          | https://developer.zohoapis.eu      |\n| IN          | https://developer.zohoapis.in      |\n| CN          | https://developer.zohoapis.com.cn  |\n| JP          | https://developer.zohoapis.jp      |\nFor more information about available environments, please visit this page\nPerformance considerations\nAlso, Zoho CRM API calls are associated with credits, each Zoho CRM edition has a limit in a 24-hour rolling window, so please, consider it when configuring your connections.\nMore details about Zoho CRM API credit system can be found here.\nNote about using the Zoho Developer Environment\nThe Zoho Developer environment API is inconsistent with production environment API. It contains about half of the modules supported in the production environment. Keep this in mind when pulling data from the Developer environment.\nSetup Guide (Airbyte Open Source)\nTo set up a connection with a Zoho CRM source, you will need to choose start sync date, Zoho CRM edition, region and environment. The latest are described above. Except for those, you will need OAuth2.0 credentials - Client ID, Client Secret and Refresh Token.\nGet Client ID, Client Secret, and Grant Token\n\nLog into https://api-console.zoho.com/\nChoose client\nEnter a scope the future refresh and access tokens will cover. For instance, it can be `ZohoCRM.modules.ALL, ZohoCRM.settings.ALL, ZohoCRM.settings.modules.ALL`. Make sure the scope covers all needed modules.\nEnter grant token's lifetime and description, click \"Create\".\nCopy Grant token, close the popup and copy Client ID and Client Secret on the \"Client Secret\" tab.\n\nCreate Refresh Token\nFor generating the refresh token, please refer to this page.\nMake sure to complete the auth flow quickly, as the initial token granted by Zoho CRM is only live for a few minutes before it can no longer be used to generate a refresh token. ",
    "tag": "airbyte"
  },
  {
    "title": "Mailjet - Mail API",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/mailjet-mail.md",
    "content": "Mailjet - Mail API\nSync overview\nThis source can sync data from the Mailjet Mail API. At present this connector only supports full refresh syncs meaning that each time you use the connector it will sync all available records from scratch. Please use cautiously if you expect your API to have a lot of records.\nThis Source Supports the Following Streams\n\ncontact list\ncontacts\nmessages\ncampaigns\nstats\n\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | No |  |\nPerformance considerations\nMailjet APIs are under rate limits for the number of API calls allowed per API keys per second. If you reach a rate limit, API will return a 429 HTTP error code. See here\nGetting started\nRequirements\n\nMailjet Mail API_KEY\nMailjet Mail SECRET_KEY\n",
    "tag": "airbyte"
  },
  {
    "title": "N8n",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/n8n.md",
    "content": "N8n\nSync overview\nThis source can sync data from N8n. At present this connector only supports full refresh syncs meaning that each time you use the connector it will sync all available records from scratch.\nThis Source Supports the Following Streams\n\nexecitions\n\nFeatures\n| Feature           | Supported?(Yes/No) | Notes |\n| :---------------- | :------------------- | :---- |\n| Full Refresh Sync | Yes                  |       |\n| Incremental Sync  | No                   |       |\nGetting started\nYou need a n8n instance or use cloud version\nCreate an API key\n\nLog in to n8n.\nGo to Settings > API.\nSelect Create an API key.\n",
    "tag": "airbyte"
  },
  {
    "title": "Amplitude",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/amplitude.md",
    "content": "Amplitude\nThis page guides you through setting up the Amplitude source connector to sync data for the Amplitude API.\nPrerequisite\nTo set up the Amplitude source connector, you'll need your Amplitude API Key and Secret Key.\nSet up the Amplitude source connector\n\nLog into your Airbyte Cloud or Airbyte Open Source account.\nClick Sources and then click + New source. \nOn the Set up the source page, select Amplitude from the Source type dropdown.\nEnter a name for your source.\nFor API Key and Secret Key, enter the Amplitude API key and secret key.\nFor Replication Start Date, enter the date in YYYY-MM-DDTHH:mm:ssZ format. The data added on and after this date will be replicated. If this field is blank, Airbyte will replicate all data.\nClick Set up source.\n\nSupported Streams\nThe Amplitude source connector supports the following streams:\n\nActive Users Counts (Incremental sync)\nAnnotations\nAverage Session Length (Incremental sync)\nCohorts\nEvents (Incremental sync)\n\nIf there are more endpoints you'd like Airbyte to support, please create an issue.\nSupported sync modes\nThe Amplitude source connector supports the following sync modes:\n\nFull Refresh\nIncremental\n\nPerformance considerations\nThe Amplitude connector ideally should gracefully handle Amplitude API limitations under normal usage. Create an issue if you see any rate limit issues that are not automatically retried successfully.",
    "tag": "airbyte"
  },
  {
    "title": "Gridly",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/gridly.md",
    "content": "Gridly\nThis page contains the setup guide and reference information for the Gridly source connector.\nPrerequisites\nA Gridly account.\nSetup guide\nGet api Key\n\nTo quickly get your API key, access your Gridly Dashboard, then select a Grid View and you can find the key in API quick start right panel.\n   \nOwner and Administrators can go to Settings/API keys to create company-level API keys with scoped privileges and accesses.\n   \n\nGet grid id\nThe grid id is available in the url.\nGridly support version control, by default the `grid id` is the same to the `branch id` when `Master` branch is selected. For fetching records on other branches, use `branch id` instead.\n\nSupported sync modes\n| Feature           | Supported? |\n| :---------------- | :--------- |\n| Full Refresh Sync | Yes        |\n| Incremental Sync  | No         |\nSupported Streams\n\nRecords\n",
    "tag": "airbyte"
  },
  {
    "title": "Chargebee",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/chargebee.md",
    "content": "Chargebee\nThis page contains the setup guide and reference information for the Chargebee source connector.\nPrerequisites\nTo set up the Chargebee source connector, you'll need the Chargebee API key and the Product Catalog version.\nSet up the Chargebee connector in Airbyte\n\nLog into your Airbyte Cloud account or navigate to the Airbyte Open Source dashboard.\nClick Sources and then click + New source.\nOn the Set up the source page, select Chargebee from the Source type dropdown.\nEnter the name for the Chargebee connector.\nFor Site, enter the site prefix for your Chargebee instance.\nFor Start Date, enter the date in YYYY-MM-DDTHH:mm:ssZ format. The data added on and after this date will be replicated.\nFor API Key, enter the Chargebee API key.\nFor Product Catalog, enter the Chargebee Product Catalog version.\nClick Set up source.\n\nSupported sync modes\nThe Chargebee source connector supports the following sync modes:\n\nFull Refresh - Overwrite\nFull Refresh - Append\nIncremental - Append\n\nSupported Streams\n\nSubscriptions\nCustomers\nInvoices\nOrders\nPlans\nAddons\nItems\nItem Prices\nAttached Items\n\nSome streams are available only for specific on Product Catalog versions:\n\nAvailable in `Product Catalog 1.0` and `Product Catalog 2.0`:\nCustomers\nEvents\nInvoices\nCredit Notes\nOrders\nCoupons\nSubscriptions\nTransactions\nAvailable only in `Product Catalog 1.0`:\nPlans\nAddons\nAvailable only in `Product Catalog 2.0`:\nItems\nItem Prices\nAttached Items\n\nNote that except the `Attached Items` stream, all the streams listed above are incremental streams, which means they:\n\nRead only new records\nOutput only new records\n\nThe `Attached Items` stream is also incremental but it reads all records and outputs only new records, which is why syncing the `Attached Items` stream, even in incremental mode, is expensive in terms of your Chargebee API quota. \nGenerally speaking, it incurs a number of API calls equal to the total number of attached items in your chargebee instance divided by 100, regardless of how many `AttachedItems` were actually changed or synced in a particular sync job.\nPerformance considerations\nThe Chargebee connector should not run into Chargebee API limitations under normal usage. Create an issue if you encounter any rate limit issues that are not automatically retried successfully.",
    "tag": "airbyte"
  },
  {
    "title": "Airtable",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/airtable.md",
    "content": "Airtable\nThis page contains the setup guide and reference information for the Airtable source connector.\nThis source syncs data from the Airtable API.\nPrerequisites\n\nAn active Airtable account\nAPI Key (Personal Access Token)\n\nSetup guide\nStep 1: Set up Airtable\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the Airtable connector and select Airtable from the Source type dropdown.\nEnter your `API Key` obtained by following these steps\nLog in and Authorize to the Airtable account and click `Set up source`.\n\nFor Airbyte OSS:\n\nNavigate to the Airbyte Open Source dashboard\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the Airtable connector and select Airtable from the Source type dropdown.\nClick Authenticate your Airtable account.\nProceed with `login` and `grant the permissions` to the target `bases` you need the access to, we recommend to use `All Workspaces and bases`.\nLog in and Authorize to the Airtable account and click `Set up source`.\n\nSupported sync modes\nThe airtable source connector supports the following sync modes:\n| Feature           | Supported?(Yes/No) | Notes |\n| :---------------- | :------------------- | :---- |\n| Full Refresh Sync | Yes                  |       |\n| Incremental Sync  | No                   |       |\nSupported Tables and Plans\nThis source allows you to pull all available tables and bases using `Metadata API` for a given authenticated user. In case you you rename or add a column to any existing table, you will need to recreate the source to update the Airbyte catalog. \nCurrently, this source connector works with `standard` subscription plan only.\nStandard Scopes required for the successfull authentication:\n* data.records:read\n* data.recordComments:read\n* schema.bases:read\nThe `Enterprise` level accounts are not supported yet.\nData type map\n| Integration Type       | Airbyte Type                                            | Nullable |\n| :--------------------- | :------------------------------------------------------ | -------- |\n| `multipleAttachments`  | `string`                                                | Yes      |\n| `autoNumber`           | `string`                                                | Yes      |\n| `barcode`              | `string`                                                | Yes      |\n| `button`               | `string`                                                | Yes      |\n| `checkbox`             | `boolean`                                               | Yes      |\n| `singleCollaborator`   | `string`                                                | Yes      |\n| `count`                | `number`                                                | Yes      |\n| `createdBy`            | `string`                                                | Yes      |\n| `createdTime`          | `datetime`, `format: date-time`                         | Yes      |\n| `currency`             | `number`                                                | Yes      |\n| `email`                | `string`                                                | Yes      |\n| `date`                 | `string`, `format: date`                                | Yes      |\n| `duration`             | `number`                                                | Yes      |\n| `lastModifiedBy`       | `string`                                                | Yes      |\n| `lastModifiedTime`     | `datetime`, `format: date-time`                         | Yes      |\n| `multipleRecordLinks`  | `array with strings`                                    | Yes      |\n| `multilineText`        | `string`                                                | Yes      |\n| `multipleCollaborators`| `array with strings`                                    | Yes      |\n| `multipleSelects`      | `array with strings`                                    | Yes      |\n| `number`               | `number`                                                | Yes      |\n| `percent`              | `number`                                                | Yes      |\n| `phoneNumber`          | `string`                                                | Yes      |\n| `rating`               | `number`                                                | Yes      |\n| `richText`             | `string`                                                | Yes      |\n| `singleLineText`       | `string`                                                | Yes      |\n| `externalSyncSource`   | `string`                                                | Yes      |\n| `url`                  | `string`                                                | Yes      |\n| `formula`              | `array with any`                                        | Yes      |\n| `lookup`               | `array with any`                                        | Yes      |\n| `multipleLookupValues` | `array with any`                                        | Yes      |\n| `rollup`               | `array with any`                                        | Yes      |\n\nAll the fields are `nullable` by default, meaning that the field could be empty.\nThe `array with any` - represents the classic array with one of the other Airtable data types inside, such as:\nstring\nnumber/integer\nnested lists/objects\netc\n\n\n\nPerformance Considerations (Airbyte Open-Source)\nSee information about rate limits here.",
    "tag": "airbyte"
  },
  {
    "title": "Gong",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/gong.md",
    "content": "Gong\nSync overview\nThe Gong source supports both Full Refresh only.\nThis source can sync data for the Gong API.\nOutput schema\nThis Source is capable of syncing the following core Streams:\n\nanswered scorecards\ncalls\nscorecards\nusers\n\nFeatures\n| Feature                   | Supported?(Yes/No) | Notes |\n| :------------------------ | :------------------- | :---- |\n| Full Refresh Sync         | Yes                  |       |\n| Incremental - Append Sync | No                   |       |\n| Namespaces                | No                   |       |\nPerformance considerations\nThe Gong connector should not run into Gong API limitations under normal usage.\nBy default Gong limits your company's access to the service to 3 API calls per second, and 10,000 API calls per day.\nRequirements\n\nGong API keys. See the Gong docs for information on how to obtain the API keys.\n",
    "tag": "airbyte"
  },
  {
    "title": "Google PageSpeed Insights",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/google-pagespeed-insights.md",
    "content": "Google PageSpeed Insights\nThis page guides you through the process of setting up the Google PageSpeed Insights source connector.\nSync overview\nPrerequisites\n\nYour Google PageSpeed API Key\n\nSet up the Google PageSpeed Insights source connector\n\nLog into your Airbyte Cloud or Airbyte Open Source account.\nClick Sources and then click + New source.\nOn the Set up the source page, select Google PageSpeed Insights from the Source type dropdown.\nEnter a name for your source.\nFor API Key, enter your Google PageSpeed API Key.\nFor URLs to analyse, enter one or many URLs you want to create PageSpeed Insights for. Example: https://www.google.com.\nFor Analyses Strategies, enter either \"desktop\", \"mobile\" or both to define which Analyses strategy to use.\nFor Lighthouse Categories, select one or many of the provided options. Categories are also called \"audits\" in some of the Google Lighthouse documentation.\nClick Set up source.\n\n\nIMPORTANT: As of 2022-12-13, the PageSpeed Insights API - as well as this Airbyte Connector - allow to specify a URL with prefix \"origin:\" - like `origin:https://www.google.com`. This results in condensed, aggregated reports about the specified origin - see this FAQ. However: This option is not specified in any official documentation anymore, therefore it might be deprecated anytime soon!\n\nSupported sync modes\nThe Google PageSpeed Insights source connector supports the following sync modes:\n\nFull Refresh\n\nSupported Streams\nThe Google PageSpeed Insights source connector supports the following stream:\n\npagespeed: Full pagespeed report of the selected URLs, lighthouse categories and analyses strategies.\n\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | No |  |\nPerformance considerations\nWhen using the connector without an API key, Google utilizes an undocumented, but strict rate limit - which also depends on how many global requests are currently sent to the PageSpeed API. The connector will retry, using an exponential backoff interval.\nIf the connector is used with an API key, Google allows for 25.000 queries per day and 240 queries per minute. Therefore, under normal usage, the connector should not trigger any rate limits.\nCreate an issue if you see any rate limit issues that are not automatically retried successfully.",
    "tag": "airbyte"
  },
  {
    "title": "Sugar CRM",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/sugar-crm.md",
    "content": "Sugar CRM\nSugar CRM is an open source eCommerce platform built on Wordpress.\nSync overview\n:::caution\nYou will only be able to connect to a self-hosted instance of Sugar CRM using these instructions.\n:::\nSugar CRM can run on the MySQL, MSSQL, Oracle, or Db2 databases. You can use Airbyte to sync your Sugar CRM instance by connecting to the underlying database using the appropriate Airbyte connector:\n\nDB2\nMySQL\nMSSQL\nOracle\n\n:::info\nTo use Oracle or DB2, you'll require an Enterprise or Ultimate Sugar subscription.\n:::\n:::info\nReach out to your service representative or system admin to find the parameters required to connect to the underlying database\n:::\nOutput schema\nTo understand your Sugar CRM database schema, see the VarDefs documentation. Otherwise, the schema will be loaded according to the rules of the underlying database's connector.",
    "tag": "airbyte"
  },
  {
    "title": "TikTok Marketing",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/tiktok-marketing.md",
    "content": "TikTok Marketing\nThis page guides you through the process of setting up the TikTok Marketing source connector.\nPrerequisites\n\nFor Airbyte Cloud:\n\nA Tiktok Ads Business account with permission to access data from accounts you want to sync\n\n\n\nFor Airbyte Open Source:\nFor the Production environment:\n* Access token\n* Secret\n* App ID\nTo access the Sandbox environment:\n* Access token\n* Advertiser ID\n\nSetup guide\nStep 1: Set up TikTok\n\nCreate a TikTok For Business account: Link\n(Open source only) Create developer application: Link\n(Open source only) For a sandbox environment: create a Sandbox Ad Account Link\n\nStep 2: Set up the source connector in Airbyte\n\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nIn the left navigation bar, click Sources. In the top-right corner, click + new source.\nOn the source setup page, select Tiktok Marketing from the Source type dropdown and enter a name for this connector.\nSelect `OAuth2.0` Authorization method, then click `Authenticate your account`.\nLog in and Authorize to the Tiktok account\nChoose required Start date\nclick `Set up source`.\n\n\n\nFor Airbyte Open Source:\n\nGo to local Airbyte page.\nIn the left navigation bar, click Sources. In the top-right corner, click + new source.\nOn the Set up the source page, enter the name for the connector and select Tiktok Marketing from the Source type dropdown.\nSelect `Production Access Token` or `Sandbox Access Token` Authorization method, then copy and paste info from step 1.\nChoose required Start date\nClick `Set up source`.\n\n\nSupported streams and sync modes\n| Stream                                  | Environment  | Key         | Incremental |\n| :-------------------------------------- | ------------ | ----------- | :---------- |\n| Advertisers                             | Prod,Sandbox | id          | No          |\n| AdGroups                                | Prod,Sandbox | adgroup_id  | Yes         |\n| Ads                                     | Prod,Sandbox | ad_id       | Yes         |\n| Campaigns                               | Prod,Sandbox | campaign_id | Yes         |\n| AdsReportsHourly                        | Prod,Sandbox | None        | Yes         |\n| AdsReportsDaily                         | Prod,Sandbox | None        | Yes         |\n| AdsReportsLifetime                      | Prod,Sandbox | None        | No          |\n| AdvertisersReportsHourly                | Prod         | None        | Yes         |\n| AdvertisersReportsDaily                 | Prod         | None        | Yes         |\n| AdvertisersReportsLifetime              | Prod         | None        | No          |\n| AdGroupsReportsHourly                   | Prod,Sandbox | None        | Yes         |\n| AdGroupsReportsDaily                    | Prod,Sandbox | None        | Yes         |\n| AdGroupsReportsLifetime                 | Prod,Sandbox | None        | No          |\n| CampaignsReportsHourly                  | Prod,Sandbox | None        | Yes         |\n| CampaignsReportsDaily                   | Prod,Sandbox | None        | Yes         |\n| CampaignsReportsLifetime                | Prod,Sandbox | None        | No          |\n| AdvertisersAudienceReportsHourly        | Prod         | None        | Yes         |\n| AdvertisersAudienceReportsDaily         | Prod         | None        | Yes         |\n| AdvertisersAudienceReportsLifetime      | Prod         | None        | No          |\n| AdGroupAudienceReportsHourly            | Prod,Sandbox | None        | Yes         |\n| AdGroupAudienceReportsDaily             | Prod,Sandbox | None        | Yes         |\n| AdsAudienceReportsHourly                | Prod,Sandbox | None        | Yes         |\n| AdsAudienceReportsDaily                 | Prod,Sandbox | None        | Yes         |\n| CampaignsAudienceReportsByCountryHourly | Prod,Sandbox | None        | Yes         |\n| CampaignsAudienceReportsByCountryDaily  | Prod,Sandbox | None        | Yes         |\nReport Aggregation\nReports synced by this connector can use either hourly, daily, or lifetime granularities for aggregating performance data. For example, if you select the daily-aggregation flavor of a report, the report will contain a row for each day for the duration of the report. Each row will indicate the number of impressions recorded on that day.\nOutput Schemas\nAdvertisers Stream\n`{\n  \"contacter\": \"Ai***te\",\n  \"phonenumber\": \"+13*****5753\",\n  \"license_no\": \"\",\n  \"promotion_center_city\": null,\n  \"balance\": 10,\n  \"license_url\": null,\n  \"timezone\": \"Etc/GMT+8\",\n  \"reason\": \"\",\n  \"telephone\": \"+14*****6785\",\n  \"id\": 7002238017842757633,\n  \"language\": \"en\",\n  \"country\": \"US\",\n  \"role\": \"ROLE_ADVERTISER\",\n  \"license_province\": null,\n  \"display_timezone\": \"America/Los_Angeles\",\n  \"email\": \"i***************@**********\",\n  \"license_city\": null,\n  \"industry\": \"291905\",\n  \"create_time\": 1630335591,\n  \"promotion_center_province\": null,\n  \"address\": \"350 29th avenue, San Francisco\",\n  \"currency\": \"USD\",\n  \"promotion_area\": \"0\",\n  \"status\": \"STATUS_ENABLE\",\n  \"description\": \"https://\",\n  \"brand\": null,\n  \"name\": \"Airbyte0830\",\n  \"company\": \"Airbyte\"\n}`\nAdGroups Stream\n`{\n  \"placement_type\": \"PLACEMENT_TYPE_AUTOMATIC\",\n  \"budget\": 20,\n  \"budget_mode\": \"BUDGET_MODE_DAY\",\n  \"display_mode\": null,\n  \"schedule_infos\": null,\n  \"billing_event\": \"CPC\",\n  \"conversion_window\": null,\n  \"adgroup_name\": \"Ad Group20211020010107\",\n  \"interest_keywords\": [],\n  \"is_comment_disable\": 0,\n  \"rf_buy_type\": null,\n  \"frequency\": null,\n  \"bid_type\": \"BID_TYPE_NO_BID\",\n  \"placement\": null,\n  \"bid\": 0,\n  \"include_custom_actions\": [],\n  \"operation_system\": [],\n  \"pixel_id\": null,\n  \"dayparting\": \"111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\",\n  \"app_type\": null,\n  \"conversion_id\": 0,\n  \"rf_predict_cpr\": null,\n  \"deep_bid_type\": null,\n  \"scheduled_budget\": 0.0,\n  \"adgroup_id\": 1714125049901106,\n  \"frequency_schedule\": null,\n  \"exclude_custom_actions\": [],\n  \"advertiser_id\": 7002238017842757633,\n  \"deep_cpabid\": 0,\n  \"is_new_structure\": true,\n  \"buy_impression\": null,\n  \"external_type\": \"WEBSITE\",\n  \"excluded_audience\": [],\n  \"deep_external_action\": null,\n  \"interest_category_v2\": [],\n  \"rf_predict_frequency\": null,\n  \"audience\": [],\n  \"pacing\": \"PACING_MODE_SMOOTH\",\n  \"brand_safety_partner\": null,\n  \"daily_retention_ratio\": null,\n  \"optimize_goal\": \"CLICK\",\n  \"enable_search_result\": false,\n  \"conversion_bid\": 0,\n  \"schedule_end_time\": \"2021-10-31 09:01:07\",\n  \"opt_status\": \"ENABLE\",\n  \"status\": \"ADGROUP_STATUS_CAMPAIGN_DISABLE\",\n  \"app_id\": null,\n  \"external_action\": null,\n  \"schedule_type\": \"SCHEDULE_START_END\",\n  \"brand_safety\": \"NO_BRAND_SAFETY\",\n  \"campaign_id\": 1714125042508817,\n  \"campaign_name\": \"Website Traffic20211020010104\",\n  \"split_test_adgroup_ids\": [],\n  \"action_v2\": [],\n  \"is_hfss\": false,\n  \"keywords\": null,\n  \"create_time\": \"2021-10-20 08:04:05\",\n  \"feed_type\": null,\n  \"languages\": [\"en\"],\n  \"enable_inventory_filter\": false,\n  \"device_price\": [],\n  \"location\": [6252001],\n  \"schedule_start_time\": \"2021-10-20 09:01:07\",\n  \"skip_learning_phase\": 0,\n  \"gender\": \"GENDER_UNLIMITED\",\n  \"creative_material_mode\": \"CUSTOM\",\n  \"app_download_url\": null,\n  \"device_models\": [],\n  \"automated_targeting\": \"OFF\",\n  \"connection_type\": [],\n  \"ios14_quota_type\": \"UNOCCUPIED\",\n  \"modify_time\": \"2022-03-24 12:06:54\",\n  \"category\": 0,\n  \"statistic_type\": null,\n  \"video_download\": \"ALLOW_DOWNLOAD\",\n  \"age\": [\"AGE_25_34\", \"AGE_35_44\", \"AGE_45_54\"],\n  \"buy_reach\": null,\n  \"is_share_disable\": false\n}`\nAds Stream\n`{\n  \"vast_moat\": false,\n  \"is_new_structure\": true,\n  \"campaign_name\": \"CampaignVadimTraffic\",\n  \"landing_page_urls\": null,\n  \"card_id\": null,\n  \"adgroup_id\": 1728545385226289,\n  \"campaign_id\": 1728545382536225,\n  \"status\": \"AD_STATUS_CAMPAIGN_DISABLE\",\n  \"brand_safety_postbid_partner\": \"UNSET\",\n  \"advertiser_id\": 7002238017842757633,\n  \"is_aco\": false,\n  \"ad_text\": \"Open-source\\ndata integration for modern data teams\",\n  \"identity_id\": \"7080121820963422209\",\n  \"display_name\": \"airbyte\",\n  \"open_url\": \"\",\n  \"external_action\": null,\n  \"playable_url\": \"\",\n  \"create_time\": \"2022-03-28 12:09:09\",\n  \"product_ids\": [],\n  \"adgroup_name\": \"AdGroupVadim\",\n  \"fallback_type\": \"UNSET\",\n  \"creative_type\": null,\n  \"ad_name\": \"AdVadim-Optimized Version 3_202203281449_2022-03-28 05:03:44\",\n  \"video_id\": \"v10033g50000c90q1d3c77ub6e96fvo0\",\n  \"ad_format\": \"SINGLE_VIDEO\",\n  \"profile_image\": \"https://p21-ad-sg.ibyteimg.com/large/ad-site-i18n-sg/202203285d0de5c114d0690a462bb6a4\",\n  \"open_url_type\": \"NORMAL\",\n  \"click_tracking_url\": null,\n  \"page_id\": null,\n  \"ad_texts\": null,\n  \"landing_page_url\": \"https://airbyte.com\",\n  \"identity_type\": \"CUSTOMIZED_USER\",\n  \"avatar_icon_web_uri\": \"ad-site-i18n-sg/202203285d0de5c114d0690a462bb6a4\",\n  \"app_name\": \"\",\n  \"modify_time\": \"2022-03-28 21:34:26\",\n  \"opt_status\": \"ENABLE\",\n  \"call_to_action_id\": \"7080120957230238722\",\n  \"image_ids\": [\"v0201/7f371ff6f0764f8b8ef4f37d7b980d50\"],\n  \"ad_id\": 1728545390695442,\n  \"impression_tracking_url\": null,\n  \"is_creative_authorized\": false\n}`\nCampaigns Stream\n`{\n  \"create_time\": \"2021-10-19 18:18:08\",\n  \"campaign_id\": 1714073078669329,\n  \"roas_bid\": 0.0,\n  \"advertiser_id\": 7002238017842757633,\n  \"modify_time\": \"2022-03-28 12:01:56\",\n  \"campaign_type\": \"REGULAR_CAMPAIGN\",\n  \"status\": \"CAMPAIGN_STATUS_DISABLE\",\n  \"objective_type\": \"TRAFFIC\",\n  \"split_test_variable\": null,\n  \"opt_status\": \"DISABLE\",\n  \"budget\": 50,\n  \"is_new_structure\": true,\n  \"deep_bid_type\": null,\n  \"campaign_name\": \"Website Traffic20211019110444\",\n  \"budget_mode\": \"BUDGET_MODE_DAY\",\n  \"objective\": \"LANDING_PAGE\"\n}`\nAdsReportsDaily Stream - BasicReports\n`{\n  \"dimensions\": {\n    \"ad_id\": 1728545390695442,\n    \"stat_time_day\": \"2022-03-29 00:00:00\"\n  },\n  \"metrics\": {\n    \"real_time_result_rate\": 0.93,\n    \"campaign_id\": 1728545382536225,\n    \"placement\": \"Automatic Placement\",\n    \"frequency\": 1.17,\n    \"cpc\": 0.35,\n    \"ctr\": 0.93,\n    \"cost_per_result\": 0.3509,\n    \"impressions\": 6137,\n    \"cost_per_conversion\": 0,\n    \"real_time_result\": 57,\n    \"adgroup_id\": 1728545385226289,\n    \"result_rate\": 0.93,\n    \"cost_per_1000_reached\": 3.801,\n    \"ad_text\": \"Open-source\\ndata integration for modern data teams\",\n    \"spend\": 20,\n    \"conversion_rate\": 0,\n    \"real_time_cost_per_conversion\": 0,\n    \"promotion_type\": \"Website\",\n    \"tt_app_id\": 0,\n    \"real_time_cost_per_result\": 0.3509,\n    \"conversion\": 0,\n    \"secondary_goal_result\": null,\n    \"campaign_name\": \"CampaignVadimTraffic\",\n    \"cpm\": 3.26,\n    \"result\": 57,\n    \"ad_name\": \"AdVadim-Optimized Version 3_202203281449_2022-03-28 05:03:44\",\n    \"secondary_goal_result_rate\": null,\n    \"clicks\": 57,\n    \"reach\": 5262,\n    \"cost_per_secondary_goal_result\": null,\n    \"real_time_conversion\": 0,\n    \"real_time_conversion_rate\": 0,\n    \"mobile_app_id\": \"0\",\n    \"tt_app_name\": \"0\",\n    \"adgroup_name\": \"AdGroupVadim\",\n    \"dpa_target_audience_type\": null\n  }\n}`\nAdvertisersReportsDaily Stream - BasicReports\n```\n{\n  \"metrics\": {\n    \"cpm\": 5.43,\n    \"impressions\": 3682,\n    \"frequency\": 1.17,\n    \"reach\": 3156,\n    \"cash_spend\": 20,\n    \"ctr\": 1.14,\n    \"spend\": 20,\n    \"cpc\": 0.48,\n    \"cost_per_1000_reached\": 6.337,\n    \"clicks\": 42,\n    \"voucher_spend\": 0\n  },\n  \"dimensions\": {\n    \"stat_time_day\": \"2022-03-30 00:00:00\",\n    \"advertiser_id\": 7002238017842757633\n  }\n}\n```\nAdGroupsReportsDaily Stream - BasicReports\n`{\n  \"metrics\": {\n    \"real_time_conversion\": 0,\n    \"real_time_cost_per_conversion\": 0,\n    \"cost_per_1000_reached\": 3.801,\n    \"mobile_app_id\": \"0\",\n    \"reach\": 5262,\n    \"cpm\": 3.26,\n    \"conversion\": 0,\n    \"promotion_type\": \"Website\",\n    \"clicks\": 57,\n    \"real_time_result_rate\": 0.93,\n    \"real_time_conversion_rate\": 0,\n    \"cost_per_conversion\": 0,\n    \"dpa_target_audience_type\": null,\n    \"result\": 57,\n    \"cpc\": 0.35,\n    \"impressions\": 6137,\n    \"cost_per_result\": 0.3509,\n    \"tt_app_id\": 0,\n    \"cost_per_secondary_goal_result\": null,\n    \"frequency\": 1.17,\n    \"spend\": 20,\n    \"secondary_goal_result_rate\": null,\n    \"real_time_cost_per_result\": 0.3509,\n    \"real_time_result\": 57,\n    \"placement\": \"Automatic Placement\",\n    \"result_rate\": 0.93,\n    \"tt_app_name\": \"0\",\n    \"campaign_name\": \"CampaignVadimTraffic\",\n    \"secondary_goal_result\": null,\n    \"campaign_id\": 1728545382536225,\n    \"conversion_rate\": 0,\n    \"ctr\": 0.93,\n    \"adgroup_name\": \"AdGroupVadim\"\n  },\n  \"dimensions\": {\n    \"adgroup_id\": 1728545385226289,\n    \"stat_time_day\": \"2022-03-29 00:00:00\"\n  }\n}`\nCampaignsReportsDaily Stream - BasicReports\n```\n{\n  \"metrics\": {\n    \"cpc\": 0.43,\n    \"spend\": 20,\n    \"clicks\": 46,\n    \"cost_per_1000_reached\": 4.002,\n    \"impressions\": 5870,\n    \"ctr\": 0.78,\n    \"frequency\": 1.17,\n    \"cpm\": 3.41,\n    \"campaign_name\": \"CampaignVadimTraffic\",\n    \"reach\": 4997\n  },\n  \"dimensions\": {\n    \"campaign_id\": 1728545382536225,\n    \"stat_time_day\": \"2022-03-28 00:00:00\"\n  }\n}\n```\nAdsAudienceReportsDaily Stream - AudienceReports\n`{\n  {\n    \"result\": 17,\n    \"clicks\": 17,\n    \"real_time_conversion_rate\": 0,\n    \"adgroup_id\": 1728545385226289,\n    \"cpm\": 3.01,\n    \"cost_per_result\": 0.4165,\n    \"real_time_cost_per_result\": 0.4165,\n    \"mobile_app_id\": 0,\n    \"spend\": 7.08,\n    \"cpc\": 0.42,\n    \"placement\": \"Automatic Placement\",\n    \"real_time_conversion\": 0,\n    \"dpa_target_audience_type\": null,\n    \"real_time_result_rate\": 0.72,\n    \"adgroup_name\": \"AdGroupVadim\",\n    \"tt_app_id\": 0,\n    \"ctr\": 0.72,\n    \"ad_text\": \"Open-source\\ndata integration for modern data teams\",\n    \"result_rate\": 0.72,\n    \"ad_name\": \"AdVadim-Optimized Version 3_202203281449_2022-03-28 05:03:44\",\n    \"conversion_rate\": 0,\n    \"real_time_result\": 17,\n    \"tt_app_name\": \"0\",\n    \"cost_per_conversion\": 0,\n    \"real_time_cost_per_conversion\": 0,\n    \"conversion\": 0,\n    \"impressions\": 2350,\n    \"promotion_type\": \"Website\",\n    \"campaign_id\": 1728545382536225,\n    \"campaign_name\": \"CampaignVadimTraffic\"\n  },\n  \"dimensions\": {\n    \"gender\": \"MALE\",\n    \"age\": \"AGE_25_34\",\n    \"ad_id\": 1728545390695442,\n    \"stat_time_day\": \"2022-03-28 00:00:00\"\n  }\n}`\nAdvertisersAudienceReportsDaily Stream - AudienceReports\n`{\n  \"dimensions\": {\n    \"stat_time_day\": \"2022-03-28 00:00:00\",\n    \"gender\": \"FEMALE\",\n    \"advertiser_id\": 7002238017842757633,\n    \"age\": \"AGE_35_44\"\n  },\n  \"metrics\": {\n    \"spend\": 3.09,\n    \"ctr\": 0.93,\n    \"cpc\": 0.44,\n    \"clicks\": 7,\n    \"cpm\": 4.11,\n    \"impressions\": 752\n  }\n}`\nAdGroupAudienceReportsDaily Stream - AudienceReports\n`{\n  \"dimensions\": {\n    \"gender\": \"MALE\",\n    \"age\": \"AGE_25_34\",\n    \"stat_time_day\": \"2022-03-29 00:00:00\",\n    \"adgroup_id\": 1728545385226289\n  },\n  \"metrics\": {\n    \"cost_per_conversion\": 0,\n    \"campaign_id\": 1728545382536225,\n    \"campaign_name\": \"CampaignVadimTraffic\",\n    \"clicks\": 20,\n    \"dpa_target_audience_type\": null,\n    \"mobile_app_id\": \"0\",\n    \"promotion_type\": \"Website\",\n    \"conversion_rate\": 0,\n    \"cpm\": 3.9,\n    \"cost_per_result\": 0.3525,\n    \"cpc\": 0.35,\n    \"real_time_cost_per_conversion\": 0,\n    \"ctr\": 1.11,\n    \"spend\": 7.05,\n    \"result\": 20,\n    \"real_time_result\": 20,\n    \"impressions\": 1806,\n    \"conversion\": 0,\n    \"real_time_result_rate\": 1.11,\n    \"real_time_conversion_rate\": 0,\n    \"real_time_conversion\": 0,\n    \"adgroup_name\": \"AdGroupVadim\",\n    \"tt_app_name\": \"0\",\n    \"placement\": \"Automatic Placement\",\n    \"real_time_cost_per_result\": 0.3525,\n    \"result_rate\": 1.11,\n    \"tt_app_id\": 0\n  }\n}`\nCampaignsAudienceReportsByCountryDaily Stream - AudienceReports\n```\n{\n  \"metrics\": {\n    \"impressions\": 5870,\n    \"campaign_name\": \"CampaignVadimTraffic\",\n    \"cpm\": 3.41,\n    \"clicks\": 46,\n    \"spend\": 20,\n    \"ctr\": 0.78,\n    \"cpc\": 0.43\n  },\n  \"dimensions\": {\n    \"stat_time_day\": \"2022-03-28 00:00:00\",\n    \"campaign_id\": 1728545382536225,\n    \"country_code\": \"US\"\n  }\n}\n```\nPerformance considerations\nThe connector is restricted by requests limitation. This connector should not run into TikTok Marketing API limitations under normal usage. Please create an issue if you see any rate limit issues that are not automatically retried successfully.",
    "tag": "airbyte"
  },
  {
    "title": "Mixpanel",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/mixpanel.md",
    "content": "Mixpanel\nThis page contains the setup guide and reference information for the Mixpanel source connector.\nPrerequisites\nTo set up the Harvest source connector, you'll need a Mixpanel Service Account and it's Project ID, the Project Timezone, and the Project region (`US` or `EU`).\nSet up the Mixpanel connector in Airbyte\n\nLog into your Airbyte Cloud or navigate to the Airbyte Open Source dashboard.\nClick Sources and then click + New source.\nOn the Set up the source page, select Mixpanel from the Source type dropdown.\nEnter the name for the Mixpanel connector.\nFor Authentication, select Service Account from the dropdown and enter the Mixpanel Service Account secret.\nFor Project ID, enter the Mixpanel Project ID.\nFor Attribution Window, enter the number of days for the length of the attribution window.\nFor Project Timezone, enter the timezone for your Mixpanel project.\nFor Start Date, enter the date in YYYY-MM-DD format. The data added on and after this date will be replicated. If left blank, the connector will replicate data from up to one year ago by default.\nFor End Date, enter the date in YYYY-MM-DD format. \nFor Region, enter the region for your Mixpanel project.\nFor Date slicing window, enter the number of days to slice through data. If you encounter RAM usage issues due to a huge amount of data in each window, try using a lower value for this parameter.\nClick Set up source.\n\nSupported sync modes\nThe Mixpanel source connector supports the following sync modes:\n\nFull Refresh - Overwrite\nFull Refresh - Append\nIncremental - Append\nIncremental - Deduped History\n\nNote: Incremental sync returns duplicated (old records) for the state date due to API filter limitation, which is granular to the whole day only.\nSupported Streams\n\nExport (Incremental)\nEngage (Incremental)\nFunnels (Incremental)\nRevenue (Incremental)\nAnnotations (Full table)\nCohorts (Incremental)\nCohort Members (Incremental)\n\nPerformance considerations\nSyncing huge date windows may take longer due to Mixpanel's low API rate-limits (60 reqs per hour).",
    "tag": "airbyte"
  },
  {
    "title": "Timely",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/timely.md",
    "content": "Timely\nThis page contains the setup guide and reference information for the Timely source connector.\nPrerequisites\n\nPlease follow these steps to obtain `Bearer_token` for your account.\nLogin into your `https://app.timelyapp.com` portal, fetch the `account-id` present in the URL (example: URL `https://app.timelyapp.com/12345/calendar` and account-id `12345`).\nGet a start-date to your events. Dateformat `YYYY-MM-DD`.\n\nSetup guide\nStep 1: Set up the Timely connector in Airbyte\nFor Airbyte OSS:\n\nNavigate to the Airbyte Open Source dashboard.\nIn the left navigation bar, click Sources. In the top-right corner, click +new source.\nOn the Set up the source page, enter the name for the Timely connector and select Timely from the Source type dropdown.\nEnter your `Bearer_token`, `account-id`, and `start-date`.\nSelect `Authenticate your account`.\nClick Set up source.\n\nSupported sync modes\nThe Timely source connector supports the following sync modes:\n| Feature           | Supported? |\n| :---------------- | :--------- |\n| Full Refresh Sync | Yes        |\n| Incremental Sync  | No         |",
    "tag": "airbyte"
  },
  {
    "title": "Younium",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/younium.md",
    "content": "Younium\nThis page contains the setup guide and reference information for the Younium source connector.\nPrerequisites\nThis Younium source uses the Younium API.\nSetup guide\nStep 1: Set up Younium\nFor Airbyte OSS:\n\nNavigate to the Airbyte Open Source dashboard\nEnter a name for your source\nEnter your Younium `username`\nEnter your Younium `password`\nEnter your Younium `legal_entity`. You can find the legal entity name in your account setting if you log in to the Younium web platform\nClick Set up source\n\nSupported sync modes\nThe Younium source connector supports the following sync modes:\n| Feature                       | Supported? |\n| :---------------------------- | :--------- |\n| Full Refresh Sync             | Yes        |\n| Incremental - Append Sync     | No         |\n| Replicate Incremental Deletes | No         |\n| SSL connection                | Yes        |\n| Namespaces                    | No         |\nSupported Streams\n\nSubscriptions\nProducts\nInvoices\n",
    "tag": "airbyte"
  },
  {
    "title": "Spree Commerce",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/spree-commerce.md",
    "content": "Spree Commerce\nSpree Commerce is an open source eCommerce platform for global brands.\nSync overview\nSpree Commerce can run on the MySQL or Postgres databases. You can use Airbyte to sync your Spree Commerce instance by connecting to the underlying database using the appropriate Airbyte connector:\n\nMySQL\nPostgres\n\n:::info\nReach out to your service representative or system admin to find the parameters required to connect to the underlying database\n:::\nOutput schema\nThe Spree Commerce schema is described in the Spree Internals section of the Spree docs. Otherwise, the schema will follow the rules of the MySQL or Postgres connectors.",
    "tag": "airbyte"
  },
  {
    "title": "Sendinblue API",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/sendinblue.md",
    "content": "Sendinblue API\nSync overview\nThis source can sync data from the Sendinblue API. At present this connector only supports full refresh syncs meaning that each time you use the connector it will sync all available records from scratch. Please use cautiously if you expect your API to have a lot of records.\nThis Source Supports the Following Streams\n\ncontacts\ncampaigns\ntemplates\n\nFeatures\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync | Yes |  |\n| Incremental Sync | No |  |\nPerformance considerations\nSendinblue APIs are under rate limits for the number of API calls allowed per API keys per second. If you reach a rate limit, API will return a 429 HTTP error code. See here\nGetting started\nRequirements\n\nSendinblue API KEY\n",
    "tag": "airbyte"
  },
  {
    "title": "Insightly",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/insightly.md",
    "content": "Insightly\nThis page guides you through the process of setting up the Insightly source connector.\nSet up the Insightly connector\n\nLog into your Airbyte Cloud or Airbyte Open Source account.\nClick Sources and then click + New source.\nOn the Set up the source page, select Insightly from the Source type dropdown.\nEnter a name for your source.\nFor API token, enter the API token for your Insightly account. You can find your API token in your Insightly Account > Click on your avatar > User Settings > API.\nFor Start date, enter the date in YYYY-MM-DDTHH:mm:ssZ format. The data added on and after this date will be replicated.\nClick Set up source.\n\nSupported sync modes\nThe Insightly source connector supports the following sync modes:\n\nFull Refresh\nIncremental\n\nSupported Streams\nThe Insightly source connector supports the following streams, some of them may need elevated permissions:\n\nActivity Sets (Full table)\nContacts (Incremental)\nCountries (Full table)\nCurrencies (Full table)\nEmails (Full table)\nEvents (Incremental)\nKnowledge Article Categories (Incremental)\nKnowledge Article Folders (Incremental)\nKnowledge Articles (Incremental)\nLeads (Incremental)\nLead Sources (Full table)\nLead Statuses (Full table)\nMilestones (Incremental)\nNotes (Incremental)\nOpportunities (Incremental)\nOpportunity Categories (Full table)\nOpportunity Products (Incremental)\nOpportunity State Reasons (Full table)\nOrganisations (Incremental)\nPipelines (Full table)\nPipeline Stages (Full table)\nPrice Book Entries (Incremental)\nPrice Books (Incremental)\nProducts (Incremental)\nProject Categories (Full table)\nProjects (Incremental)\nProspects (Incremental)\nQuote Products (Incremental)\nQuotes (Incremental)\nRelationships (Full table)\nTags (Full table)\nTask Categories (Full table)\nTasks (Incremental)\nTeam Members (Full table)\nTeams (Full table)\nTickets (Incremental)\nUsers (Incremental)\n\nPerformance considerations\nThe connector is restricted by Insightly requests limitation.",
    "tag": "airbyte"
  },
  {
    "title": "Zendesk Chat",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/integrations/sources/zendesk-chat.md",
    "content": "Zendesk Chat\nThis page contains the setup guide and reference information for the Zendesk Chat source connector.\nPrerequisites\n\nA Zendesk Account with permission to access data from accounts you want to sync.\n\n\n\n(Airbyte Open Source) An Access Token (https://developer.zendesk.com/rest_api/docs/chat/auth). We recommend creating a restricted, read-only key specifically for Airbyte access to allow you to control which resources Airbyte should be able to access.\n\n\nSetup guide\n\nFor Airbyte Cloud:\n\nLog into your Airbyte Cloud account.\nClick Sources and then click + New source.\nOn the Set up the source page, select Zendesk Chat from the Source type dropdown.\nEnter the name for the Zendesk Chat connector.\nIf you access Zendesk Chat from a Zendesk subdomain, enter the Subdomain.\nFor Start Date, enter the date in `YYYY-MM-DDTHH:mm:ssZ` format. The data added on and after this date will be replicated.\nClick Authenticate your Zendesk Chat account. Log in and authorize your Zendesk Chat account.\nClick Set up source.\n\n\n\nFor Airbyte Open Source:\n\nNavigate to the Airbyte Open Source dashboard.\nClick Sources and then click + New source.\nOn the Set up the source page, select Zendesk Chat from the Source type dropdown.\nEnter the name for the Zendesk Chat connector.\nIf you access Zendesk Chat from a Zendesk subdomain, enter the Subdomain.\nFor Start Date, enter the date in `YYYY-MM-DDTHH:mm:ssZ` format. The data added on and after this date will be replicated.\nFor Authorization Method, select Access Token from the dropdown and enter your Zendesk access token.\nClick Set up source.\n\n\nSupported sync modes\nThe Zendesk Chat source connector supports the following sync modes:\n\nFull Refresh - Overwrite\nFull Refresh - Append\nIncremental - Append\nIncremental - Deduped History\n\nSupported Streams\n\nAccounts\nAgents (Incremental)\nAgent Timelines (Incremental)\nChats\nShortcuts\nTriggers\nBans (Incremental)\nDepartments\nGoals\nSkills\nRoles\nRouting Settings\n\nPerformance considerations\nThe connector is restricted by Zendesk's requests limitation.\nData type map\n| Integration Type | Airbyte Type |\n| :--------------- | :----------- |\n| `string`         | `string`     |\n| `number`         | `number`     |\n| `array`          | `array`      |\n| `object`         | `object`     |",
    "tag": "airbyte"
  },
  {
    "title": "Product Release Stages",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/project-overview/product-release-stages.md",
    "content": "Product Release Stages\nThe following release stages describe the lifecycle of an Airbyte product, feature, or connector.\n| Expectations | Alpha | Beta | General Availability (GA)|\n|:-------------|:------|:-----|:-------------------------|\n| Customer Availability | Alpha features and products may have limited availability (by invitation only)  Alpha connectors are available to all users | Beta features and products may have limited availability (by invitation only)  Beta connectors are available to all users | Available to all users |\n|Support | Cloud: No Support SLAs  Open-source: Community Slack Support | Cloud: Official Beta Support SLA  Open-source: Community Slack Support | Cloud: Official GA Support SLA  Open-source: Community Slack Support |\n| Production Readiness | No | Yes (with caveats) | Yes |\nAlpha\nAn alpha release signifies a product, feature, or connector under development and helps Airbyte gather early feedback and issues reported by early adopters. We strongly discourage using alpha releases for production use cases and do not offer Cloud Support SLAs around these products, features, or connectors.\nWhat you should know about an alpha release\n\nAn alpha release might not be feature-complete (features planned for the release are under development) and may include backward-incompatible/breaking API changes. \nAccess for alpha features and products may not be enabled for all Airbyte users by default. Depending on the feature, you may enable the feature either from the Airbyte UI or by contacting Airbyte Support. Alpha connectors are available to all users. \nAlpha releases may be announced via email, in the Airbyte UI, and/or through certain pages of the Airbyte docs.\n\nBeta\nA beta release is considered stable and reliable with no backwards incompatible changes but has not been validated by a broader group of users. We expect to find and fix a few issues and bugs in the release before it\u2019s ready for GA.\nWhat you should know about a beta release\n\nA beta release is generally feature-complete (features planned for the release have been mostly implemented) and does not include backward-incompatible/breaking API changes. \nAccess may be enabled for all Airbyte users by default. Depending on the feature, you may enable the feature either from the Airbyte UI or by contacting Airbyte Support. Beta connectors are available to all users. \nBeta releases may be announced via email, in the Airbyte UI, and/or through certain pages of the Airbyte docs.\n\nGeneral availability (GA)\nA generally available release has been deemed ready for use in a production environment and is officially supported by Airbyte. Its documentation is considered sufficient to support widespread adoption.\nWhat you should know about a GA release\n\nA GA release is feature-complete (features planned for the release have been fully implemented) and does not include backward-incompatible/breaking API changes. \nAccess is enabled for all Airbyte users by default. Depending on the feature, you may enable the feature either from the Airbyte UI or by contacting Airbyte Support. \nGA releases may be announced via email, in the Airbyte UI, and/or through certain pages of the Airbyte docs. \n\nDeprecated",
    "tag": "airbyte"
  },
  {
    "title": "Slack Code of Conduct",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/project-overview/slack-code-of-conduct.md",
    "content": "\ndescription: Be nice to one another.\nSlack Code of Conduct\nAirbyte's Slack community is growing incredibly fast. We're home to over 1500 data professionals and are growing at an awesome pace. We are proud of our community, and have provided these guidelines to support new members in maintaining the wholesome spirit we have developed here. We appreciate your continued commitment to making this a community we are all excited to be a part of.\nRule 1: Be respectful.\nOur desire is for everyone to have a positive, fulfilling experience in Airbyte Slack, and we sincerely appreciate your help in making this happen.\nAll of the guidelines we provide below are important, but there\u2019s a reason respect is the first rule. We take it seriously, and while the occasional breach of etiquette around Slack is forgivable, we cannot condone disrespectful behavior. \nRule 2: Use the most relevant channels.\nWe deliberately use topic-specific Slack channels so members of the community can opt-in on various types of conversations. Our members take care to post their messages in the most relevant channel, and you\u2019ll often see reminders about the best place to post a message (respectfully written, of course!). If you're looking for help directly from the Community Assistance Team or other Airbyte employees, please stick to posting in the airbyte-help channel, so we know you're asking us specifically!\nRule 3: Don\u2019t double-post.\nPlease be considerate of our community members\u2019 time. We know your question is important, but please keep in mind that Airbyte Slack is not a customer service platform but a community of volunteers who will help you as they are able around their own work schedule. You have access to all the history, so it\u2019s easy to check if your question has already been asked. \nRule 4: Check question for clarity and thoughtfulness.\nAirbyte Slack is a community of volunteers. Our members enjoy helping others; they are knowledgeable, gracious, and willing to give their time and expertise for free. Putting some effort into a well-researched and thoughtful post shows consideration for their time and will gain more responses.\nRule 5: Keep it public.\nThis is a public forum; please do not contact individual members of this community without their express permission, regardless of whether you are trying to recruit someone, sell a product, or solicit help. \nRule 6: No soliciting!\nThe purpose of the Airbyte Slack community is to provide a forum for data practitioners to discuss their work and share their ideas and learnings. It is not intended as a place to generate leads for vendors or recruiters, and may not be used as such.\nIf you\u2019re a vendor, you may advertise your product in #shameless-plugs. Advertising your product anywhere else is strictly against the rules. \nRule 7: Don't spam tags, or use @here or @channel.\nUsing the @here and @channel keywords in a post will not help, as they are disabled in Slack for everyone excluding admins. Nonetheless, if you use them we will remind you with a link to this rule, to help you better understand the way Airbyte Slack operates. \nDo not tag specific individuals for help on your questions. If someone chooses to respond to your question, they will do so. You will find that our community of volunteers is generally very responsive and amazingly helpful! \nRule 8: Use threads for discussion.\nThe simplest way to keep conversations on track in Slack is to use threads. The Airbyte Slack community relies heavily on threads, and if you break from this convention, rest assured one of our community members will respectfully inform you quickly! \nIf you see a message or receive a direct message that violates any of these rules, please contact an Airbyte team member and we will take the appropriate moderation action immediately. We have zero tolerance for intentional rule-breaking and hate speech.",
    "tag": "airbyte"
  },
  {
    "title": "Examples",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/project-overview/licenses/examples.md",
    "content": "Examples\nWe chose ELv2 because it is very permissive with what you can do with the software. \nWe are still being asked whether one's project are concerned by the ELv2 license. So we decided to list some projects to make this very explicit. \nDon't hesitate to ask us about this or to do a pull request to add your project here. If we merge it, it means you're good to go.\nLet's start with the list of projects that falls under ELv2 and for which you can't leverage Airbyte's technology that is under Elv2, because there aren't actually many projects.\nExamples of projects that can't leverage the technology under ELv2 without a contract\n\nHosting Airbyte yourself and selling it as an ELT/ETL tool. That means selling a competitive alternative to Airbyte Cloud or the future Airbyte Enterprise.\nSelling a product that directly exposes Airbyte\u2019s UI or API.\n\nExamples of projects for which you can leverage Airbyte fully\n\nCreating an analytics or attribution platform for which you want to use Airbyte to bring data in on behalf of your customers.\nCreating any kind of platform on which you offer Airbyte's connectors to your customers to bring their data in, unless you're selling some ELT / ETL solution.\nBuilding your internal data stack and configuring pipelines through Airbyte's UI or API.\n",
    "tag": "airbyte"
  },
  {
    "title": "ELv2",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/project-overview/licenses/elv2-license.md",
    "content": "ELv2\nElastic License 2.0 (ELv2)\nAcceptance By using the software, you agree to all of the terms and conditions below.\nCopyright License The licensor grants you a non-exclusive, royalty-free, worldwide, non-sublicensable, non-transferable license to use, copy, distribute, make available, and prepare derivative works of the software, in each case subject to the limitations and conditions below\nLimitations You may not provide the software to third parties as a hosted or managed service, where the service provides users with access to any substantial set of the features or functionality of the software.\nYou may not move, change, disable, or circumvent the license key functionality in the software, and you may not remove or obscure any functionality in the software that is protected by the license key.\nYou may not alter, remove, or obscure any licensing, copyright, or other notices of the licensor in the software. Any use of the licensor\u2019s trademarks is subject to applicable law.\nPatents The licensor grants you a license, under any patent claims the licensor can license, or becomes able to license, to make, have made, use, sell, offer for sale, import and have imported the software, in each case subject to the limitations and conditions in this license. This license does not cover any patent claims that you cause to be infringed by modifications or additions to the software. If you or your company make any written claim that the software infringes or contributes to infringement of any patent, your patent license for the software granted under these terms ends immediately. If your company makes such a claim, your patent license ends immediately for work on behalf of your company.\nNotices You must ensure that anyone who gets a copy of any part of the software from you also gets a copy of these terms.\nIf you modify the software, you must include in any modified copies of the software prominent notices stating that you have modified the software.\nNo Other Rights These terms do not imply any licenses other than those expressly granted in these terms.\nTermination If you use the software in violation of these terms, such use is not licensed, and your licenses will automatically terminate. If the licensor provides you with a notice of your violation, and you cease all violation of this license no later than 30 days after you receive that notice, your licenses will be reinstated retroactively. However, if you violate these terms after such reinstatement, any additional violation of these terms will cause your licenses to terminate automatically and permanently.\nNo Liability As far as the law allows, the software comes as is, without any warranty or condition, and the licensor will not be liable to you for any damages arising out of these terms or the use or nature of the software, under any kind of legal claim.\nDefinitions The licensor is the entity offering these terms, and the software is the software the licensor makes available under these terms, including any portion of it.\nyou refers to the individual or entity agreeing to these terms.\nyour company is any legal entity, sole proprietorship, or other kind of organization that you work for, plus all organizations that have control over, are under the control of, or are under common control with that organization. control means ownership of substantially all the assets of an entity, or the power to direct its management and policies by vote, contract, or otherwise. Control can be direct or indirect.\nyour licenses are all the licenses granted to you for the software under these terms.\nuse means anything you do with the software requiring one of your licenses.\ntrademark means trademarks, service marks, and similar rights.",
    "tag": "airbyte"
  },
  {
    "title": "Licenses",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/project-overview/licenses",
    "content": "Licenses\nAirbyte monorepo uses multiple licenses.\nThe license for a particular work is defined with following prioritized rules:\n\nLicense directly present in the file\nLICENSE file in the same directory as the work\nFirst LICENSE found when exploring parent directories up to the project top level directory\nDefaults to Elastic License 2.0\n\nIf you have any question regarding licenses, just visit our FAQ or contact us.\nIf you want to see a list of examples supported by ELv2, and not, to have a better understanding whether you should be concerned or not, check the examples. \nTL;DR: Unless you want to host Airbyte yourself and sell it as an ELT/ETL tool, or to sell a product that directly exposes Airbyte\u2019s UI or API, you should be good to go!",
    "tag": "airbyte"
  },
  {
    "title": "License FAQ",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/project-overview/licenses/license-faq.md",
    "content": "License FAQ\nAirbyte Licensing Overview\n\nAirbyte Connectors are open sourced and available under the MIT License.\nAirbyte Protocol is open sourced and available under the MIT License.\nAirbyte CDK (Connector Development Kit) is open sourced and available under the MIT License.\nAirbyte Core is licensed under the Elastic License 2.0 (ELv2).\nAirbyte Cloud & Airbyte Enterprise are both closed source and require a commercial license from Airbyte.\n\n\nAbout Elastic License 2.0 (ELv2)\nELv2 is a simple, non-copyleft license, allowing for the right to \u201cuse, copy, distribute, make available, and prepare derivative works of the software\u201d. Anyone can use Airbyte, free of charge. You can run the software at scale on your infrastructure. There are only three high-level limitations. You cannot:\n1. Provide the products to others as a managed service (read more);\n2. Circumvent the license key functionality or remove/obscure features protected by license keys; or\n3. Remove or obscure any licensing, copyright, or other notices.\nIn case you want to work with Airbyte without these limitations, we offer alternative licenses. These licenses include maintenance, support, and customary commercial terms. If you need a different license, please get in touch with us at: contact@airbyte.io.\nView License\nFAQ\nWhat limitations does ELv2 impose on my use of Airbyte?\nIf you are an Airbyte Cloud customer, nothing changes for you.\nFor open-source users, everyone can continue to use Airbyte as they are doing today: no limitations on volume, number of users, number of connections\u2026\nThere are only a few high-level limitations. You cannot:\n1. Provide the products to others as a managed service. For example, you cannot sell a cloud service that provides users with direct access to Airbyte. You can sell access to applications built and run using Airbyte (read more).\n2. Circumvent the license key functionality or remove/obscure features protected by license keys. For example, our code may contain watermarks or keys to unlock proprietary functionality. Those elements of our code will be marked in our source code. You can\u2019t remove or change them.\nWhy did Airbyte adopt ELv2?\nWe are releasing Airbyte Cloud, a managed version of Airbyte that will offer alternatives to how our users operate Airbyte, including additional features and new execution models. We want to find a great way to execute our mission to commoditize data integration with open source and our ambition to create a sustainable business.\nELv2 gives us the best of both worlds. \nOn one hand, our users can continue to use Airbyte freely, and on the other hand, we can safely create a sustainable business and continue to invest in our community, project and product. We don\u2019t have to worry about other large companies taking the product to monetize it for themselves, thus hurting our community.\nWill Airbyte connectors continue to be open source?\nOur own connectors remain open-source, and our contributors can also develop their own connectors and continue to choose whichever license they prefer. This is our way to accomplish Airbyte\u2019s vision of commoditizing data integration: access to data shouldn\u2019t be behind a paywall. Also, we want Airbyte\u2019s licensing to work well with applications that are integrated using connectors.\nWe are continuously investing in Airbyte's data protocol and all the tooling around it. The Connector Development Kit (CDK), which helps our community and our team build and maintain connectors at scale, is a cornerstone of our commoditization strategy and also remains open-source.\nHow do I continue to contribute to Airbyte under ELv2?\nAirbyte\u2019s projects are available here. Anyone can contribute to any of these projects (including those licensed with ELv2). We are introducing a Contributor License Agreement that you will have to sign with your first contribution.\nWhen will ELv2 be effective?\nELv2 will apply from the following Airbyte core version as of September 27, 2021: version 0.30.0.\nWhat is the \u201cmanaged service\u201d use case that is not allowed under ELv2?\nWe chose ELv2 because it is very permissive with what you can do with the software. \nYou can basically build ANY product on top of Airbyte as long as you don\u2019t:\n* Host Airbyte yourself and sell it as an ELT/ETL tool, or a replacement for the Airbyte solution.\n* Sell a product that directly exposes Airbyte\u2019s UI or API.\nHere is a non-exhaustive list of what you can do (without providing your customers direct access to Airbyte functionality):\n* I am creating an analytics platform and I want to use Airbyte to bring data in on behalf of my customers.\n* I am building my internal data stack and I want my team to be able to interact with Airbyte to configure the pipelines through the UI or the API.\n* ...\nMy company has a policy against using code that restricts commercial use \u2013 can I still use Airbyte under ELv2?\nYou can use software under ELv2 for your commercial business, you simply cannot offer it as a managed service. \nAs a Data Agency, I currently use Airbyte to fulfill my customer needs. How does ELv2 affect me?\nYou can continue to use Airbyte, as long as you don\u2019t offer it as a managed service.\nI started to use Airbyte to ingest my customer\u2019s data. What should I do?\nYou can continue to use Airbyte, as long as you don\u2019t offer it as a managed service.\nCan I customize ELv2 software?\nYes, you can customize ELv2 software. ELv2 is similar in this sense to permissive open-source licenses. You can modify the software, integrate the variant into your application, and operate the modified application, as long as you don\u2019t go against any of the limitations.\nWhy didn\u2019t you use a closed-source license for Airbyte Core?\nWe want to provide developers with free access to our Airbyte Core source code \u2014 including rights to modify it. Since this wouldn\u2019t be possible with a closed-source license, we decided to use the more permissive ELv2.\nIs there any revenue sharing for those who create Airbyte connectors?\nWe will be introducing a new participative model in the next few months. There are still a lot of details to figure out, but the general idea is that maintainers of connectors would have the option to obtain a share of revenue when the connectors are being used in the paid version of Airbyte. In exchange, maintainers would be responsible for SLAs, new features, and bug fixes for the said connector.",
    "tag": "airbyte"
  },
  {
    "title": "MIT",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/project-overview/licenses/mit-license.md",
    "content": "MIT\nMIT License\nCopyright (c) 2020 Airbyte, Inc.\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.",
    "tag": "airbyte"
  },
  {
    "title": "Getting Started with Airbyte Cloud",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/cloud/getting-started-with-airbyte-cloud.md",
    "content": "Getting Started with Airbyte Cloud\nThis page guides you through setting up your Airbyte Cloud account, setting up a source, destination, and connection, verifying the sync, and allowlisting an IP address.\nSet up your Airbyte Cloud account\nTo use Airbyte Cloud:\n\nIf you haven't already, sign up for Airbyte Cloud using your email address, Google login, or GitHub login.\n\nAirbyte Cloud offers a 14-day free trial. For more information, see Pricing.\n:::note\n   If you are invited to a workspace, you cannot use your Google login to create a new Airbyte account.\n   :::\n\nIf you signed up using your email address, Airbyte will send you an email with a verification link. On clicking the link, you'll be taken to your new workspace.\n\n:::info\n   A workspace lets you collaborate with team members and share resources across your team under a shared billing account.\n   :::\nSet up a source\n:::info\nA source is an API, file, database, or data warehouse that you want to ingest data from.\n:::\nTo set up a source:\n\nOn the Airbyte Cloud dashboard, click Sources and then click + New source.\nOn the Set up the source page, select the source you want to set up from the Source type dropdown.\n\nThe fields relevant to your source are displayed. The Setup Guide provides information to help you fill out the fields for your selected source.\n\nClick Set up source.\n\nSet up a destination\n:::info\nA destination is a data warehouse, data lake, database, or an analytics tool where you want to load your extracted data.\n:::\nTo set up a destination:\n\nOn the Airbyte Cloud dashboard, click Destinations and then click + New destination.\nOn the Set up the destination page, select the destination you want to set up from the Destination type dropdown.\n\nThe fields relevant to your destination are displayed. The Setup Guide provides information to help you fill out the fields for your selected destination.\n\nClick Set up destination.\n\nSet up a connection\n:::info\nA connection is an automated data pipeline that replicates data from a source to a destination.\n:::\nSetting up a connection involves configuring the following parameters:\n| Parameter                              | Description                                                                                                                                                                                                                                                           |\n|----------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Replication frequency                  | How often should the data sync?                                                                                                                                                                                                                                       |\n| Data residency                         | Where should the data be processed? |\n| Destination Namespace and stream names | Where should the replicated data be written?                                                                                                                                                                                                                          |\n| Catalog selection                      | Which streams and fields should be replicated from the source to the destination?                                                                                                                                                                                     |\n| Sync mode                              | How should the streams be replicated (read and written)?                                                                                                                                                                                                              |\n| Optional transformations               | How should Airbyte protocol messages (raw JSON blob) data be converted into other data representations?                                                                                                                                                               |\nFor more information, see Connections and Sync Modes and Namespaces\nIf you need to use cron scheduling:\n1. In the Replication Frequency dropdown, click Cron. \n2. Enter a cron expression and choose a time zone to create a sync schedule.\n:::note\n\nOnly one sync per connection can run at a time. \nIf cron schedules a sync to run before the last one finishes, the scheduled sync will start after the last sync completes.\nCloud does not allow schedules that sync more than once per hour. \n\n:::\nTo set up a connection:\n\nOn the Airbyte Cloud dashboard, click Connections and then click + New connection.\n\nOn the New connection page, select a source:\n\n\nTo use an existing source, select your desired source from the Source dropdown. Click Use existing source.\n\n\nTo set up a new source, select the source you want to set up from the Source type dropdown. The fields relevant to your source are displayed. The Setup Guide provides information to help you fill out the fields for your selected source. Click Set up source.\n\n\nSelect a destination:\n\n\nTo use an existing destination, select your desired destination from the Destination dropdown. Click Use existing destination.\n\nTo set up a new destination, select the destination you want to set up from the Destination type dropdown. The fields relevant to your destination are displayed. The Setup Guide provides information to help you fill out the fields for your selected destination. Click Set up destination.\n\nThe Set up the connection page is displayed.\n\nFrom the Replication frequency dropdown, select how often you want the data to sync from the source to the destination.\n\nNote: The default replication frequency is Every 24 hours.\n\nFrom the Destination Namespace dropdown, select the format in which you want to store the data in the destination:\n\nNote: The default configuration is Mirror source structure.\n\n\nConfiguration\n\nDescription\n\n\n\nMirror source structure\n   \nSome sources (for example, databases) provide namespace information for a stream. If a source provides the namespace information, the destination will reproduce the same namespace when this configuration is set. For sources or streams where the source namespace is not known, the behavior will default to the \"Destination default\" option\n   \n\n\nDestination default\n   \nAll streams will be replicated and stored in the default namespace defined on the Destination Settings page. For more information, see \u200b\u200bDestination Connector Settings\n\n\n\nCustom format\n   \nAll streams will be replicated and stored in a custom format. See Custom format for more details\n   \n\n\n:::tip\nTo better understand the destination namespace configurations, see Destination Namespace example\n:::\n\n(Optional) In the Destination Stream Prefix (Optional) field, add a prefix to stream names (for example, adding a prefix `airbyte_` renames `projects` to `airbyte_projects`).\n(Optional) Click Refresh schema if you had previously triggered a sync with a subset of tables in the stream and now want to see all the tables in the stream.\nActivate the streams you want to sync:\n(Optional) If your source has multiple tables, type the name of the stream you want to enable in the Search stream name search box.\n(Optional) To configure the sync settings for multiple streams, select the checkbox next to the desired streams, configure the settings in the purple box, and click Apply.\n\nConfigure the sync settings:\n\n\nToggle the Sync button to enable sync for the stream.\n\nSource:\nNamespace: The database schema of your source tables (auto-populated for your source)\nStream name: The table name in the source (auto-populated for your source)\n\n\n\nSync mode: Select how you want the data to be replicated from the source to the destination:\nFor the source:\n\nSelect Full Refresh to copy the entire dataset each time you sync\nSelect Incremental to replicate only the new or modified data\n\nFor the destination:\n\nSelect Overwrite to erase the old data and replace it completely\nSelect Append to capture changes to your table\n    Note: This creates duplicate records\n\nSelect Deduped + history to mirror your source while keeping records unique\nNote: Some sync modes may not yet be available for your source or destination\n\n\n\n\nCursor field: Used in Incremental sync mode to determine which records to sync. Airbyte pre-selects the cursor field for you (example: updated date). If you have multiple cursor fields, select the one you want.\n\nPrimary key: Used in Deduped + history sync mode to determine the unique identifier.\n\nDestination:\n\nNamespace: The database schema of your destination tables.\nStream name: The final table name in destination.\n\n\n\nClick Set up connection.\n\nAirbyte tests the connection. If the sync is successful, the Connection page is displayed.\n\nVerify the connection\nVerify the sync by checking the logs:\n\nOn the Airbyte Cloud dashboard, click Connections. The list of connections is displayed. Click on the connection you just set up.\nThe Sync History is displayed. Click on the first log in the sync history.\nCheck the data at your destination. If you added a Destination Stream Prefix while setting up the connection, make sure to search for the stream name with the prefix.\n\nAllowlist IP addresses\nDepending on your data residency location, you may need to allowlist the following IP addresses to enable access to Airbyte:\nUnited States and Airbyte Default\nGCP region: us-west3\n\n34.106.109.131\n34.106.196.165\n34.106.60.246\n34.106.229.69\n34.106.127.139\n34.106.218.58\n34.106.115.240\n34.106.225.141\n\nEuropean Union\n:::note \nSome workflows still run in the US, even when the data residency is in the EU. If you use the EU as a data residency, you must allowlist the following IP addresses from both GCP us-west3 and AWS eu-west-3.\n:::\nGCP region: us-west3\n\n34.106.109.131\n34.106.196.165\n34.106.60.246\n34.106.229.69\n34.106.127.139\n34.106.218.58\n34.106.115.240\n34.106.225.141\n\nAWS region: eu-west-3\n\n13.37.4.46\n13.37.142.60\n",
    "tag": "airbyte"
  },
  {
    "title": "Using the dbt Cloud integration ",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/cloud/dbt-cloud-integration.md",
    "content": "Using the dbt Cloud integration\nStep 1: Generate a service token\nGenerate a service token for your dbt Cloud transformation.  \n:::note\n\nTo use the dbt Cloud integration, you must use a paid version of dbt Cloud.\nThe service token must have Member, Job Admin, or Account Admin permissions.\n\n:::\nStep 2: Set up the dbt Cloud integration in Airbyte Cloud\nTo set up the dbt Cloud integration in Airbyte Cloud:\n\n\nOn the Airbyte Cloud dashboard, click Settings.\n\n\nClick dbt Cloud integration.\n\n\nPaste the service token from Step 1 and click Save changes.\n\n\nClick Connections and select the connection you want to add a dbt transformation to.\n\n\nGo to the Transformation tab and click + Add transformation.\n\n\nSelect the transformation from the dropdown and click Save changes. The transformation will run during the subsequent syncs until you remove it. \n\n\n:::note\nYou can have multiple transformations per connection.\n:::",
    "tag": "airbyte"
  },
  {
    "title": "Managing Airbyte Cloud",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/cloud/managing-airbyte-cloud.md",
    "content": "Managing Airbyte Cloud\nThis page will help you manage your Airbyte Cloud workspaces and understand Airbyte Cloud limitations.\nManage your Airbyte Cloud workspace\nAn Airbyte workspace allows you to collaborate with other users and manage connections under a shared billing account.\n:::info\nAirbyte credits are assigned per workspace and cannot be transferred between workspaces.\n:::\nAdd users to your workspace\nTo add a user to your workspace:\n\n\nOn the Airbyte Cloud dashboard, click Settings.\n\n\nClick Access Management.\n\n\nClick + New user.\n\n\nOn the Add new users dialog, enter the email address of the user you want to invite to your workspace. \n\n\nClick Send invitation.\n:::info\nThe user will have access to only the workspace you invited them to. They will be added as a workspace admin by default.\n:::\n\n\nRemove users from your workspace\u200b\nTo remove a user from your workspace:\n\n\nOn the Airbyte Cloud dashboard, click Settings.\n\n\nClick Access Management.\n\n\nClick Remove next to the user\u2019s email.\n\n\nThe Remove user dialog displays. Click Remove.\n\n\nRename a workspace\nTo rename a workspace:\n\n\nOn the Airbyte Cloud dashboard, click Settings.\n\n\nClick General Settings.\n\n\nIn the Workspace name field, enter the new name for your workspace. \n\n\nClick Save changes.\n\n\nDelete a workspace\nTo delete a workspace:\n\n\nOn the Airbyte Cloud dashboard, click Settings.\n\n\nClick General Settings.\n\n\nIn the Delete your workspace section, click Delete.\n\n\nSingle workspace vs. multiple workspaces\nYou can use one or multiple workspaces with Airbyte Cloud. \nAccess\n| Number of workspaces | Benefits                                                                      | Considerations                                                                                                                              |\n|----------------------|-------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------|\n| Single               | All users in a workspace have access to the same data.                        | If you add a user to a workspace, you cannot limit their access to specific data within that workspace.                                     |\n| Multiple             | You can create multiple workspaces to allow certain users to access the data. | Since you have to manage user access for each workspace individually, it can get complicated if you have many users in multiple workspaces. | \nBilling\n| Number of workspaces | Benefits                                                                      | Considerations                                                                                                                              |\n|----------------------|-------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------|\n| Single               | You can use the same payment method for all purchases.                        | Credits pay for the use of resources in a workspace when you run a sync. Resource usage cannot be divided and paid for separately (for example, you cannot bill different departments in your organization for the usage of some credits in one workspace).                                     |\n| Multiple             | Workspaces are independent of each other, so you can use a different payment method card for each workspace (for example, different credit cards per department in your organization). | You can use the same payment method for different workspaces, but each workspace is billed separately. Managing billing for each workspace can become complicated if you have many workspaces. |\nSwitch between multiple workspaces\nTo switch between workspaces:\n\n\nOn the Airbyte Cloud dashboard, click the current workspace name under the Airbyte logo in the navigation bar.\n\n\nClick View all workspaces.\n\n\nClick the name of the workspace you want to switch to.\n\n\nChoose your default data residency\nDefault data residency allows you to choose where your data is processed.\n:::note \nConfiguring default data residency only applies to new connections and does not affect existing connections.   \n:::\nFor individual connections, you can choose a data residency that is different from the default through connection settings or when you create a new connection.\n:::note \nWhile the data is processed in a data plane in the chosen residency, the cursor and primary key data is stored in the US control plane. If you have data that cannot be stored in the US, do not use it as a cursor or primary key.\n:::\nTo choose your default data residency:\n\n\nOn the Airbyte Cloud dashboard, click Settings.\n\n\nClick Data Residency.\n\n\nClick the dropdown and choose the location for your default data residency.\n\n\nClick Save changes. \n\n\n:::info \nDepending on your network configuration, you may need to add IP addresses to your allowlist.   \n:::\nManage Airbyte Cloud notifications\nTo set up Slack notifications:\n\n\nOn the Airbyte Cloud dashboard, click Settings.\n\n\nClick Notifications.\n\n\nCreate an Incoming Webhook for Slack.\n\n\nNavigate back to the Airbyte Cloud dashboard > Settings > Notifications and enter the Webhook URL.\n\n\nToggle the When sync fails and When sync succeeds buttons as required.\n\n\nClick Save changes.\n\n\nUnderstand Airbyte Cloud limits\nUnderstanding the following limitations will help you better manage Airbyte Cloud:\n\nMax number of workspaces per user: 100\nMax number of sources in a workspace: 100\nMax number of destinations in a workspace: 100\nMax number of connections in a workspace: 100\nMax number of streams that can be returned by a source in a discover call: 1K\nMax number of streams that can be configured to sync in a single connection: 1K\nSize of a single record: 100MB\nShortest sync schedule: Every 60 min\nSchedule accuracy: +/- 30 min\n\nView the sync summary\nThe sync summary displays information about the data moved during a sync.\nTo view the sync summary:\n1. On the Airbyte Cloud dashboard, click Connections.   \n\n\nClick a connection in the list to view its sync history.\nSync History displays the sync status or reset status (Succeeded, Partial Success, Failed, Cancelled, or Running) and the sync summary.  \n:::note \nAirbyte will try to sync your data three times. After a third failure, it will stop attempting to sync.\n:::\n\n\nTo view the full sync log, click the sync summary dropdown.\n\n\nSync summary\n| Data                            | Description                                                                                                                                             |\n|--------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|\n| x GB (also measured in KB, MB) | Amount of data moved during the sync. If basic normalization is on, the amount of data would not change since normalization occurs in the destination.  |\n| x emitted records              | Number of records read from the source during the sync.                                                                                                 |\n| x committed records            | Number of records the destination confirmed it received.                                                                                                |\n| xh xm xs                   | Total time (hours, minutes, seconds) for the sync and basic normalization, if enabled, to complete.                                                     | \n:::note\nIn a successful sync, the number of emitted records and committed records should be the same.\n::: \nEdit stream configuration\n\n\nOn the Airbyte Cloud dashboard, click Connections and then click the connection you want to change.   \n\n\nClick the Replication tab.\n\n\nThe Transfer and Streams settings include the following parameters:\n| Parameter                            | Description                                                                         |\n|--------------------------------------|-------------------------------------------------------------------------------------|\n| Replication frequency                | How often the data syncs                                                            |\n| Non-breaking schema updates detected | How Airbyte handles syncs when it detects non-breaking schema changes in the source |\n| Destination Namespace                | Where the replicated data is written                                                |\n| Destination Stream Prefix            | Helps you identify streams from different connectors                                |\n:::note \nThese parameters apply to all streams in the connection.\n:::\nIf you need to use cron scheduling:\n1. In the Replication Frequency dropdown, click Cron. \n2. Enter a cron expression and choose a time zone to create a sync schedule.\n:::note\n\nOnly one sync per connection can run at a time. \nIf cron schedules a sync to run before the last one finishes, the scheduled sync will start after the last sync completes.\nAirbyte Cloud does not allow schedules that sync more than once per hour. \n\n:::\nIn the Activate the streams you want to sync section, you can make changes to any stream you choose.\nTo search for a stream:\n\n\nClick the Search stream name search box. \n\n\nType the name of the stream you want to find.\n\n\nStreams matching your search are displayed in the list.\n\n\nTo change individual stream configuration:\n\n\n\nIn the Sync column of the stream, toggle the sync on or off. \n\n\nClick the dropdown arrow in the Sync mode column and select the sync mode you want to apply.\n\n\n:::note \nDepending on the sync mode you select, you may need to choose a cursor or primary key.\n:::\n\nIf there is a dropdown arrow in the Cursor or Primary key fields, click the dropdown arrow and choose the cursor or primary key. \n\nTo change multiple stream configurations:\n\n\n\nClick the first checkbox in the table header to select all streams in the connection.\n\n\nDeselect the checkboxes of streams you do not want to apply these changes to.\n\n\nIn the highlighted header of the table, toggle the sync on or off. \n\n\nClick the dropdown arrow in the Sync mode column and select the sync mode you want to apply to these streams.\n\n\nIf there is a dropdown arrow in the Cursor or Primary key fields of the highlighted table header, click the dropdown arrow and choose the cursor or primary key.\n\n\nClick Apply to apply these changes to the streams you selected, or click Cancel to discard the changes.\n\n\nTo save the changes:\n1. Click Save changes, or click Cancel to discard the changes.\n\nThe Stream configuration changed dialog displays. This gives you the option to reset streams when you save the changes.\n\n:::caution\nAirbyte recommends that you reset streams. A reset will delete data in the destination of the affected streams and then re-sync that data. Skipping a reset is discouraged and might lead to unexpected behavior.\n:::\n\nClick Save connection, or click Cancel to close the dialog. \n\nTo refresh the source schema:\n1. Click Refresh source schema to fetch the schema of your data source.\n\nIf the schema has changed, the Refreshed source schema dialog displays them.\n\nManage schema changes\nOnce every 24 hours, Airbyte checks for changes in your source schema and allows you to review the changes and fix breaking changes.\n:::note \nSchema changes are flagged in your connection but are not propagated to your destination.\n:::\nReview non-breaking schema changes\nTo review non-breaking schema changes:\n1. On the Airbyte Cloud dashboard, click Connections and select the connection with non-breaking changes (indicated by a yellow exclamation mark icon).\n\n\nClick Review changes.\n\n\nThe Refreshed source schema dialog displays the changes. \n\n\nReview the changes and click OK to close the dialog.\n\n\nScroll to the bottom of the page and click Save changes.\n\n\n:::note \nBy default, Airbyte ignores non-breaking changes and continues syncing. You can configure how Airbyte handles syncs when it detects non-breaking changes by editing the stream configuration.\n:::\nFix breaking schema changes\n:::note \nBreaking changes can only occur in the Cursor or Primary key fields.\n:::\nTo review and fix breaking schema changes:\n1. On the Airbyte Cloud dashboard, click Connections and select the connection with breaking changes (indicated by a red exclamation mark icon).\n\n\nClick Review changes.\n\n\nThe Refreshed source schema dialog displays the changes.\n\n\nReview the changes and click OK to close the dialog.\n\n\nIn the streams table, the stream with a breaking change is highlighted.\n\n\nFix the breaking change by selecting a new Cursor or Primary key.\n\n\nScroll to the bottom of the page and click Save changes.\n\n\n:::note \nIf a connection\u2019s source schema has breaking changes, it will stop syncing. You must review and fix the changes before editing the connection or resuming syncs.\n:::\nEnable schema update notifications\nTo get notified when your source schema changes: \n1. Make sure you have webhook notifications set up.\n\n\nOn the Airbyte Cloud dashboard, click Connections and select the connection you want to receive notifications for.\n\n\nClick the Settings tab on the Connection page.\n\n\nToggle Schema update notifications.\n\n\nDisplay the connection state\nConnection state provides additional information about incremental syncs. It includes the most recent values for the global or stream-level cursors, which can aid in debugging or determining which data will be included in the next syncs. \nTo display the connection state:\n1. On the Airbyte Cloud dashboard, click Connections and then click the connection you want to display.\n\n\nClick the Settings tab on the Connection page.\n\n\nClick the Advanced dropdown arrow. \nConnection State displays.\n\n\nChoose the data residency for a connection\nYou can choose the data residency for your connection in the connection settings. You can also choose data residency when creating a new connection, or you can set the default data residency for your workspace.\nTo choose the data residency for your connection: \n\n\nOn the Airbyte Cloud dashboard, click Connections and then click the connection that you want to change. \n\n\nClick the Settings tab. \n\n\nClick the Data residency dropdown and choose the location for your default data residency.\n\n\nClick Save changes\n\n\n:::note \nChanges to data residency will not affect any sync in progress. \n:::\nManage credits\nEnroll in the Free Connector Program\nThe Free Connector Program allows you to sync connections with alpha or beta connectors at no cost.\n:::note \nYou must be enrolled in the program to use alpha and beta connectors for free. If either the source or destination is in alpha or beta, the whole connection is free to sync. When both the source and destination of a connection become generally available (GA), the connection will no longer be free. We will email you two weeks before both connectors in a connection move to GA.\n:::\nBefore enrolling in the program, set up at least one alpha or beta connector and verify your email if you haven't already.\nTo enroll in the program:\n1. On the Airbyte Cloud dashboard, click Credits in the navigation bar.\n\n\nClick Enroll now in the Free Connector Program banner.\n\n\nClick Enroll now.\n\n\nInput your credit card information and click Save card.\n\n\n:::note \nCredit card information is required, even if you previously bought credits on Airbyte Cloud. This ensures uninterrupted syncs when both connectors move to GA.\n:::\nSince alpha and beta connectors are still in development, support is not provided. For additional resources, check out our Connector Catalog, Troubleshooting & FAQ, and our Community Slack.\nBuy credits\nThis section guides you through purchasing credits on Airbyte Cloud. An Airbyte credit is a unit of measure used to pay for Airbyte resources when you run a sync. \nTo buy credits:\n\n\nOn the Airbyte Cloud dashboard, click Credits in the navigation bar.\n\n\nIf you are unsure of how many credits you need, click Talk to Sales to find the right amount for your team.\n\n\nClick Buy credits.\n\n\nThe Stripe payment page displays. If you want to change the amount of credits, click Qty 200. The Update quantity dialog displays, and you can either type the amount or use minus (-) or plus (+) to change the quantity. Click Update. \n:::note \nPurchase limits:\n* Minimum: 100 credits\n* Maximum: 999 credits\n:::\nTo buy more credits or a subscription plan, reach out to Sales.\n\n\nFill out the payment information. \nAfter you enter your billing address, sales tax is calculated and added to the total.\n\n\nClick Pay.\nYour payment is processed. The Credits page displays the updated quantity of credits, total credit usage, and the credit usage per connection. \nA receipt for your purchase is sent to your email. Email us for an invoice.\n:::note \nCredits expire after one year if they are not used.\n\n",
    "tag": "airbyte"
  },
  {
    "title": "Core Concepts",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/cloud/core-concepts.md",
    "content": "Core Concepts\nAirbyte enables you to build data pipelines and replicate data from a source to a destination. You can configure how frequently the data is synced, what data is replicated, what format the data is written to in the destination, and if the data is stored in raw tables format or basic normalized (or JSON) format. \nThis page describes the concepts you need to know to use Airbyte.\nSource\nA source is an API, file, database, or data warehouse that you want to ingest data from. \nDestination\nA destination is a data warehouse, data lake, database, or an analytics tool where you want to load your ingested data.\nConnector\nAn Airbyte component which pulls data from a source or pushes data to a destination.\nConnection\nA connection is an automated data pipeline that replicates data from a source to a destination. \nSetting up a connection involves configuring the following parameters:\n\n\nParameter\n\nDescription\n\n\n\nSync schedule\n   \nWhen should a data sync be triggered?\n   \n\n\nDestination Namespace and stream names\n   \nWhere should the replicated data be written? \n   \n\n\nCatalog selection\n   \nWhat data should be replicated from the source to the destination?\n   \n\n\nSync mode\n   \nHow should the streams be replicated (read and written)?\n   \n\n\nOptional transformations\n   \nHow should Airbyte protocol messages (raw JSON blob) data be converted into other data representations?\n   \n\n\nStream\nA stream is a group of related records. \nExamples of streams:\n\nA table in a relational database \nA resource or API endpoint for a REST API \nThe records from a directory containing many files in a filesystem\n\nField\nA field is an attribute of a record in a stream. \nExamples of fields: \n\nA column in the table in a relational database \nA field in an API response\n\nNamespace\nNamespace is a group of streams in a source or destination. Common use cases for namespaces are enforcing permissions, segregating test and production data, and general data organization.\nA schema in a relational database system is an example of a namespace. \nIn a source, the namespace is the location from where the data is replicated to the destination.\nIn a destination, the namespace is the location where the replicated data is stored in the destination. Airbyte supports the following configuration options for destination namespaces:\n\n\nConfiguration\n\nDescription\n\n\n\nMirror source structure\n   \nSome sources (for example, databases) provide namespace information for a stream. If a source provides the namespace information, the destination will reproduce the same namespace when this configuration is set. For sources or streams where the source namespace is not known, the behavior will default to the \"Destination default\" option.\n   \n\n\nDestination default\n   \nAll streams will be replicated and stored in the default namespace defined on the destination settings page. For settings for popular destinations, see \u200b\u200bDestination Connector Settings\n\n\n\nCustom format\n   \nAll streams will be replicated and stored in a user-defined custom format. See Custom format for more details.\n   \n\n\nConnection sync modes\nA sync mode governs how Airbyte reads from a source and writes to a destination. Airbyte provides different sync modes to account for various use cases.\n\nFull Refresh | Overwrite: Sync all records from the source and replace data in destination by overwriting it.\nFull Refresh | Append: Sync all records from the source and add them to the destination without deleting any data.\nIncremental Sync | Append: Sync new records from the source and add them to the destination without deleting any data.\nIncremental Sync | Deduped History: Sync new records from the source and add them to the destination. Also provides a de-duplicated view mirroring the state of the stream in the source.\n\nNormalization\nNormalization is the process of structuring data from the source into a format appropriate for consumption in the destination. For example, when writing data from a nested, dynamically typed source like a JSON API to a relational destination like Postgres, normalization is the process which un-nests JSON from the source into a relational table format which uses the appropriate column types in the destination.\nNote that normalization is only relevant for the following relational database & warehouse destinations: \n\nBigQuery\nSnowflake\nRedshift\nPostgres\nOracle\nMySQL\nMSSQL\n\nOther destinations do not support normalization as described in this section, though they may normalize data in a format that makes sense for them. For example, the S3 destination connector offers the option of writing JSON files in S3, but also offers the option of writing statically typed files such as Parquet or Avro. \nAfter a sync is complete, Airbyte normalizes the data. When setting up a connection, you can choose one of the following normalization options:\n\nRaw data (no normalization): Airbyte places the JSON blob version of your data in a table called `_airbyte_raw_<stream name>`\nBasic Normalization: Airbyte converts the raw JSON blob version of your data to the format of your destination. Note: Not all destinations support normalization.\ndbt Cloud integration: Airbyte's dbt Cloud integration allows you to use dbt Cloud for transforming and cleaning your data during the normalization process.\n\n:::note\nNormalizing data may cause an increase in your destination's compute cost. This cost will vary depending on the amount of data that is normalized and is not related to Airbyte credit usage.\n:::\nWorkspace\nA workspace is a grouping of sources, destinations, connections, and other configurations. It lets you collaborate with team members and share resources across your team under a shared billing account. \nWhen you sign up for Airbyte Cloud, we automatically create your first workspace where you are the only user with access. You can set up your sources and destinations to start syncing data and invite other users to join your workspace.\nGlossary of Terms",
    "tag": "airbyte"
  },
  {
    "title": "Docusaurus Settings",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/docusaurus/docusaurus_settings.md",
    "content": "Docusaurus Settings\nDocusaurus is the tool that generates the website that hosts docs in cloud and OSS.\nHere are some quick relevant guides using docusaurus at Airbyte.\nI want to change the sidebar\n\nOSS: sidebar is generated in a JSON blob here\nOSS: Here is a guide to the JSON blob structure\nCloud: sidebar is autogenerated\nfolders will become dropdown items\nif there is a `README.md` in the folder\nit will be the default view\nthe first `# heading` will be the folder title\n\n\nif there is not a `README.md` in the folder name of folder will be the drop down name\n\nBut how do I change the links on top?\n\ngo to one of these config files\nOSS file location\nCloud file location\nCopy an existing JSON object like this but change the values a bit so people don't think you copied my homework\n`js\n{\n    href: 'https://theuselessweb.com',\n    position: 'left',\n    label: 'A collection of useless websites',\n},`\n\ntest locally following this guide\n\nUpdating docusaurus\nFor security and an occasional cool features you may want to update docusaurus.  From time to time docusaurus will suggest you do just that.\nIt is a reasonable decision to copy the update command docusaurus suggests.  It should look something like this:\n`bash\nyarn upgrade @docusaurus/core@latest @docusaurus/plugin-google-gtag@latest @docusaurus/preset-classic@latest`\nKeep in mind this won't update dependencies.  The upside is that the dependencies probably won't break.  ",
    "tag": "airbyte"
  },
  {
    "title": "Locally testing your changes",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/docusaurus/locally_testing_docusaurus.md",
    "content": "Locally testing your changes\n\nYou can test any change you make to see how it will look in production\nThe processes are almost identical from local testing to production so\nyou can have a high degree of confidence in the results\n```bash\nnavigate to docusaurus\ncd docusaurus\ninstall the packages to run docusaurus\nyarn install\ncompile the current state of airbyte-cloud/docs\ninto the website and serve it at http://localhost:3000\nyarn build && yarn serve\ncontrol-c sends the SIGTERM command to the running process\nthis is a common way to exit running shell applications\nto exit the running server use control-c\n```\n\nIf you encounter a build error there may be multiple causes but usually this is due to a broken link:\nfix your broken links and the build should work\nlook at the changes you made locally, if they look great commit and add a funny picture to the PR for karma (technically optional)\n\nimportant note",
    "tag": "airbyte"
  },
  {
    "title": "Contributions to docs",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/docusaurus/contributing_to_docs.md",
    "content": "Contributions to docs\n\nDid you just learn something?!\n\nmaybe write a doc for it!\n\nWhere to put stuff\n\nTry and utilize existing folder structure\nIf it fits in an existing category: great!\nIf it doesn't add it to `/docs/` top level\nIf it makes sense to make a new category with other top level make a new directory\nimages and other assets live in `/docs/assets/`\nNew folders need at least 2 markdown files for docusarus to recognise the folder in the sidebar\n\nTesting your changes locally\nmore in depth local change guide here\n- You can see what the docs website will look like locally try:\n```bash\ncd docusaurus\nyarn build && yarn serve\npress control-c to exit the server\n```\n- if you change anything at all ",
    "tag": "airbyte"
  },
  {
    "title": "So you want to make a redirect in docs?",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/docusaurus/making_a_redirect.md",
    "content": "So you want to make a redirect in docs?\n\nPlugin Client Redirects\nA silly name, but a useful plugin that adds redirect functionality to docusuaurs\nOfficial documentation here\nYou will need to edit this docusaurus file\nYou will see a commented section the reads something like this \n`js\n//                        {\n//                         from: '/some-lame-path',\n//                         to: '/a-much-cooler-uri',\n//                        },`\nCopy this section, replace the values, and test it locally by going to the \npath you created a redirect for and checked to see that the address changes to your new one.",
    "tag": "airbyte"
  },
  {
    "title": "Deploying and Reverting Docs",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/docusaurus/deploying_and_reverting_docs.md",
    "content": "Deploying and Reverting Docs\n\nWe use Github Pages for hosting this docs website, and Docusaurus as the docs framework. Docusaurus has a strange deployment pattern. Luckily that pattern is abstracted away from you.\nThe source code for the docs lives in the airbyte monorepo's docs/ directory. To publish the updated docs on this website after you've committed a change to the `docs/` markdown files, it is required to locally run a manual publish flow.\nDocs will deploy from whatever branch you are in. You will probably want to deploy from master, but that is at your discretion.\nThis is the deployment tool. You will need a github ssh key, the tool will properly tell you if you don't have one though\nAt it's simplest just open the airbyte repo and run `./tools/bin/deploy_docusaurus`\nA typical deployment will look like this\n```bash\ncd airbyte\nor cd airbyte-cloud\ngit checkout master\ngit pull\n./tools/bin/deploy_docusaurus\n```\nIf docs has a problem this procedure will work the same on older branches. The push to production is a force push so collisions are unlikely\nIf you want to revert/rollback it will look something like this\n`bash\ncd airbyte\ngit checkout $SOME_OLDER_BRANCH\n./tools/bin/deploy_docusaurus`",
    "tag": "airbyte"
  },
  {
    "title": "Code Style",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/contributing-to-airbyte/code-style.md",
    "content": "Code Style\nConfigure Java Style for IntelliJ\nFirst, download the style configuration.\n`text\ncurl https://raw.githubusercontent.com/google/styleguide/gh-pages/intellij-java-google-style.xml -o ~/Downloads/intellij-java-google-style.xml`\nInstall it in IntelliJ:\n\nGo to `Preferences > Editor > Code Style`\nPress the little cog:\n`Import Scheme > IntelliJ IDEA code style XML`\nSelect the file we just downloaded\nSelect `GoogleStyle` in the dropdown\nChange default `Hard wrap at` in `Wrapping and Braces` tab to 150\nUse explicit imports \nUnder `Preferences > Code Style > Java > Imports`\nchange `Class count to use import with '*'` to `9999` \nchange `Names count to use static import with '*'` to `9999`\n\n\nUnder `Preferences > Code Style > Kotlin > Imports`\nchange `Top Level Symbols` to `Use single name import`\nchange `Java Statics and Enum Members` to `Use single name import`\n\n\nAdd the `final` keyword wherever possible. You can either set this as the default for your IDE or you can set it just for the Airbyte project(s) that you are using\nTurn on the inspection. Go into `Preferences > Editor > Inspections`\nSearch `\"Field may be 'final'\"` > check the box\nSearch `\"local variable or parameter can be 'final'\"` > check the box\nApply the changes\n\n\nTurn on the auto add final. Go into IntelliJ Preferences\nPlugins - install Save Actions if not already installed\nGo to Save Actions in the preferences left navigation column (NOT Tools > Actions on Save -- that is a different tool)\n`Activate save actions on save` > check the box\n`Active save actions on shortcut` > check the box\n`Activate save actions on batch` > check the box\n`Add final modifier to field` > check the box\n`Add final modifier to local variable or parameter` > check the box\nApply the changes\n\n\n\n\nYou're done!\n\nSource code comments\nIt's hard to pin down exactly what to do around source code comments, but there are two (very subjective) and rough guidelines:\nIf something is not obvious, write it down. Examples include:\n\nnon-trivial class definitions should have docstrings\nmagic variables should have comments explaining why those values are used (e.g: if using a page size of 10 in a connector, describe why if possible. If there is no reason, that's also fine, just mention in a comment).\nComplicated subroutines/logic which cannot be refactored should have comments explaining what they are doing and why\n",
    "tag": "airbyte"
  },
  {
    "title": "Gradle Cheatsheet",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/contributing-to-airbyte/gradle-cheatsheet.md",
    "content": "Gradle Cheatsheet\nOverview\nWe have 3 ways of slicing our builds:\n\nBuild Everything: Including every single connectors.\nBuild Platform: Build only modules related to the core platform.\nBuild Connectors Base: Build only modules related to code infrastructure for connectors.\n\nBuild Everything is really not particularly functional as building every single connector at once is really prone to transient errors. As there are more connectors the chance that there is a transient issue while downloading any single dependency starts to get really high.\nIn our CI we run Build Platform and Build Connectors Base. Then separately, on a regular cadence, we build each connector and run its integration tests.\nWe split Build Platform and Build Connectors Base from each other for a few reasons:\n\nThe tech stacks are very different. The Platform is almost entirely Java. Because of differing needs around separating environments, the Platform build can be optimized separately from the Connectors one.\nWe want to the iteration cycles of people working on connectors or the platform faster and independent. e.g. Before this change someone working on a Platform feature needs to run formatting on the entire codebase (including connectors). This led to a lot of cosmetic build failures that obfuscated actually problems. Ideally a failure on the connectors side should not block progress on the platform side.\nThe lifecycles are different. One can safely release the Platform even if parts of Connectors Base is failing (and vice versa).\n\nFuture Work: The next step here is to figure out how to more formally split connectors and platform. Right now we exploit behavior in settings.gradle to separate them. This is not a best practice. Ultimately, we want these two builds to be totally separate. We do not know what that will look like yet.\nCheatsheet\nHere is a cheatsheet for common gradle commands.\nList Gradle Tasks\nTo view all available tasks:\n`text\n./gradlew tasks`\nTo view all tasks available for a given namespace:\n`text\n./gradlew <namespace>:tasks`\nfor example:\n`text\n./gradlew :airbyte-integrations:connectors:source-bigquery:tasks`\nBasic Build Syntax\nHere is the syntax for running gradle commands on the different parts of the code base that we called out above.\nBuild Everything\n`text\n./gradlew <gradle command>`\nBuild Platform\n`text\nSUB_BUILD=PLATFORM ./gradlew <gradle command>`\nBuild Connectors Base\n`text\nSUB_BUILD=CONNECTORS_BASE ./gradlew <gradle command>`\nBuild\nIn order to \"build\" the project. This task includes producing all artifacts and running unit tests (anything called in the `:test` task). It does not include integration tests (anything called in the `:integrationTest` task).\nFor example all the following are valid.\n`shell\n./gradlew build # builds the entire Airbyte project including every single connector supported\nSUB_BUILD=PLATFORM ./gradlew build -x test # builds Airbyte Platform without running tests\nSUB_BUILD=CONNECTORS_BASE ./gradlew build # builds all Airbyte connectors and runs unit tests`\nDebugging\nTo debug a Gradle task, add `--scan` to the `./gradlew` command. After the task has completed, you should see a message like:\n`text\nPublishing build scan...\nhttps://gradle.com/s/6y7ritpvzkwp4`\nClicking the link opens a browser page which contains lots of information pertinent to debugging why a build failed, or understanding what sub-tasks were run during a task.\nFormatting\nThe build system has a custom task called `format`. It is not called as part of `build`. If the command is called on a subset of the project, it will (mostly) target just the included modules. The exception is that `spotless` (a gradle formatter) will always format any file types that it is configured to manage regardless of which sub build is run. `spotless` is relatively fast, so this should not be too much of an annoyance. It can lead to formatting changes in unexpected parts of the code base.\nFor example all the following are valid.\n`shell\n./gradlew format\nSUB_BUILD=PLATFORM ./gradlew format\nSUB_BUILD=CONNECTORS_BASE ./gradlew format`\nPlatform-Specific Commands\nBuild Artifacts\nThis command just builds the docker images that are used as artifacts in the platform. It bypasses running tests.\n`shell\nSUB_BUILD=PLATFORM ./gradlew build`\nRunning Tests\nThe Platform has 3 different levels of tests: Unit Tests, Acceptance Tests, Frontend Acceptance Tests.\n| Test        | Used | Description                                                                                   |\n|:------------|:----:|:----------------------------------------------------------------------------------------------|\n| Unit        |  X   | Aims to test each component (e.g. a method function)                                          |\n| Integration |      | Checks the data flow from one module to other modules                                         |\n| System      |      | Tests overall interaction of components, includes load, performance, reliability and security |\n| Acceptance  |  X   | Assess whether the Product is working for the user's viewpoint                                |\nUnit Tests\nUnit Tests can be run using the `:test` task on any submodule. These test class-level behavior. They should avoid using external resources (e.g. calling staging services or pulling resources from the internet). We do allow these tests to spin up local resources (usually in docker containers). For example, we use test containers frequently to spin up test postgres databases.\nAcceptance Tests\nWe split Acceptance Tests into 2 different test suites:\n\nPlatform Acceptance Tests: These tests are a coarse test to sanity check that each major feature in the platform. They are run with the following command: `SUB_BUILD=PLATFORM ./gradlew :airbyte-tests:acceptanceTests`. These tests expect to find a local version of Airbyte running. For testing the docker version start Airbyte locally. For an example, see the acceptance_test script that is used by the CI. For Kubernetes, see the acceptance_test_helm script that is used by the CI.\nMigration Acceptance Tests: These tests make sure the end-to-end process of migrating from one version of Airbyte to the next works. These tests are run with the following command: `SUB_BUILD=PLATFORM ./gradlew :airbyte-tests:automaticMigrationAcceptanceTest --scan`. These tests do not expect there to be a separate deployment of Airbyte running.\n\nThese tests currently all live in airbyte-tests\nFrontend Acceptance Tests\nThese are acceptance tests for the frontend. They are run with\n`shell\nSUB_BUILD=PLATFORM ./gradlew --no-daemon :airbyte-webapp-e2e-tests:e2etest` \nLike the Platform Acceptance Tests, they expect Airbyte to be running locally. See the script that is used by the CI.\nThese tests currently all live in airbyte-webapp-e2e-tests\nFuture Work\nOur story around \"integration testing\" or \"E2E testing\" is a little ambiguous. Our Platform Acceptance Test Suite is getting somewhat unwieldy. It was meant to just be some coarse sanity checks, but over time we have found more need to test interactions between systems more granular. Whether we start supporting a separate class of tests (e.g. integration tests) or figure out how allow for more granular tests in the existing Acceptance Test framework is TBD.\nConnectors-Specific Commands (Connector Development)\nCommands used in CI\nAll connectors, regardless of implementation language, implement the following interface to allow uniformity in the build system when run from CI:\nBuild connector, run unit tests, and build Docker image:\n`shell\n./gradlew :airbyte-integrations:connectors:<connector_name>:build` \nRun integration tests:\n`shell\n./gradlew :airbyte-integrations:connectors:<connector_name>:integrationTest`\nPython\nThe ideal end state for a Python connector developer is that they shouldn't have to know Gradle exists.\nWe're almost there, but today there is only one Gradle command that's needed when developing in Python, used for formatting code.\nFormatting python module:\n`shell\n./gradlew :airbyte-integrations:connectors:<connector_name>:airbytePythonFormat`",
    "tag": "airbyte"
  },
  {
    "title": "Developing on Docker",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/contributing-to-airbyte/developing-on-docker.md",
    "content": "Developing on Docker\nIncrementality\nThe docker build is fully incremental for the platform build, which means that it will only build an image if it is needed. We need to keep it that \nway.\nThe top level `build.gradle` file defines several convenient tasks for building a docker image.\n1) The `copyGeneratedTar` task copies a generated TAR file from a default location into the default location used by the docker plugin.\n2) The `buildDockerImage` task is a convenience class for configuring the above linked docker plugin that centralizes configuration logic commonly found in our dockerfiles.\n3) Makes the `buildDockerImage` task depend on the Gradle `assemble` task.\nThese tasks are created in a subproject if the subproject has a `gradle.properties` file with the `dockerImageName` property. This property sets the built docker image's name.\nAdding a new docker build\nOnce you have a `Dockerfile`, generating the docker image is done in the following way:\n1. Create a `gradle.properties` file in the subproject with the `dockerImageName` property set to the docker image name.\nFor example:\n`groovy\n// In the gradle.properties file.\ndockerImageName=cron`\n\nIf this is a subproject producing a TAR, take advantage of the pre-provided task by configuring the build docker task to\n   depend on the copy TAR task in the subproject's build.gradle.\n\nFor example:\n`groovy\ntasks.named(\"buildDockerImage\") {\n    dependsOn copyGeneratedTar\n}`\n\nIf this is a subproject with a more custom copy strategy, define your own task to copy the necessary files and configure\n   the build docker task to depend on this custom copy task in the subproject's build.gradle.\n```groovy\ntask copyScripts(type: Copy) {\n    dependsOn copyDocker\n    from('scripts')\n    into 'build/docker/bin/scripts'\n}\n\ntasks.named(\"buildDockerImage\") {\n    dependsOn copyScripts\n}\n```\nBuilding the docker images\nThe gradle task `generate-docker` allows to build all the docker images.\nHandling the OSS version\nThe docker images that are running using a jar need to the latest published OSS version on master. Here is how it is handle:\nExisting modules\nThe version should already be present. If a new version is published while a PR is open, it should generate a conflict, that will prevent you from \nmerging the review. There are scenarios where it is going to generate and error (The Dockerfile is moved for example), the way to avoid any issue \nis to:\n- Check the `.env` file to make sure that the latest version align with the version in the PR\n- Merge the `master` branch in the PR and make sure that the build is working right before merging.\nIf the version don't align, it will break the remote `master` build.\nThe version will be automatically replace with new version when releasing the OSS version using the `.bumpversion.cfg`.\nNew module\nThis is trickier than handling the version of an exiting module.\nFirst your docker file generating an image need to be added to the `.bumpversion.cfg`. For each and every version you want to build with, the \ndocker image will need to be manually tag and push until the PR is merge. The reason is that the build has a check to know if all the potential \ndocker images are present in the docker repository. It is done the following way:\n`shell\ndocker tag 7d94ea2ad657 airbyte/temporal:0.30.35-alpha\ndocker push airbyte/temporal:0.30.35-alpha`",
    "tag": "airbyte"
  },
  {
    "title": "Developing on Kubernetes",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/contributing-to-airbyte/developing-on-kubernetes.md",
    "content": "Developing on Kubernetes\nMake sure to read our docs for developing locally first.\nArchitecture\n\nIteration Cycle (Locally)\nIf you're developing locally using Minikube/Docker Desktop/Kind, you can iterate with the following series of commands:\n`bash\n./gradlew build # build dev images\nkubectl delete -k kube/overlays/dev # optional (allows you to recreate resources from scratch)\nkubectl apply -k kube/overlays/dev # applies manifests\nkubectl port-forward svc/airbyte-webapp-svc 8000:80 # port forward the api/ui`\nIteration Cycle (on GKE)\nThe process is similar to developing on a local cluster, except you will need to build the local version and push it to your own container registry with names such as `your-registry/scheduler`. Then you will need to configure an overlay to override the name of images and apply your overlay with `kubectl apply -k <path to your overlay>`.\nWe are working to improve this process.\nCompletely resetting a local cluster\nIn most cases, running `kubectl delete -k kube/overlays/dev` is sufficient to remove the core Airbyte-related components. However, if you are in a dev environment on a local cluster only running Airbyte and want to start completely from scratch (removing all PVCs, pods, completed pods, etc.), you can use the following command to destroy everything on the cluster:\n```bash\nBE CAREFUL, THIS COMMAND DELETES ALL RESOURCES, EVEN NON-AIRBYTE ONES!\nkubectl delete \"$(kubectl api-resources --namespaced=true --verbs=delete -o name | tr \"\\n\" \",\" | sed -e 's/,$//')\" --all\n```",
    "tag": "airbyte"
  },
  {
    "title": "Code of Conduct",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/contributing-to-airbyte/code-of-conduct.md",
    "content": "\ndescription: Our Community Code of Conduct\nCode of Conduct\nOur Pledge\nIn the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to make participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.\nOur Standards\nExamples of behavior that contributes to creating a positive environment include:\n\nUsing welcoming and inclusive language\nBeing respectful of differing viewpoints and experiences\nGracefully accepting constructive criticism\nFocusing on what is best for the community\nShowing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\nThe use of sexualized language or imagery and unwelcome sexual attention or advances\nTrolling, insulting/derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others\u2019 private information, such as a physical or electronic address, without explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\nOur Responsibilities\nProject maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.\nProject maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.\nScope\nThis Code of Conduct applies within all project spaces, and it also applies when an individual is representing the project or its community in public spaces. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.\nEnforcement\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at conduct@airbyte.io. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.\nProject maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project\u2019s leadership.\nAttribution\nThis Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html",
    "tag": "airbyte"
  },
  {
    "title": "Developing Locally",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/contributing-to-airbyte/developing-locally.md",
    "content": "Developing Locally\nThe following technologies are required to build Airbyte locally.\n\nJava 17\n`Node 16`\n`Python 3.9`\n`Docker`\n`Jq`\n\n:::info\nManually switching between different language versions can get hairy. We recommend using a version manager such as pyenv or jenv.\n:::\nTo start contributing:\n\nFork the airbyte repository to develop connectors or the  airbyte-platform repository to develop the Airbyte platform. \nClone the fork on your workstation:\n\nIf developing connectors, you can work on connectors locally but additionally start the platform independently locally using :\n`bash\n   git clone git@github.com:{YOUR_USERNAME}/airbyte.git\n   cd airbyte\n   ./run-ab-platform.sh`\nIf developing platform:\n`bash\n   git clone git@github.com:{YOUR_USERNAME}/airbyte-platform.git\n   cd airbyte-platform\n   docker compose up`\nBuild with `gradle`\nTo compile and build the platform, run the following command in your local `airbyte-platform` repository:\n`bash\nSUB_BUILD=PLATFORM ./gradlew build`\nThis will build all the code and run all the unit tests.\n`SUB_BUILD=PLATFORM ./gradlew build` creates all the necessary artifacts (Webapp, Jars and Docker images) so that you can run Airbyte locally. Since this builds everything, it can take some time.\n:::info\nOptionally, you may pass a `VERSION` environment variable to the gradle build command. If present, gradle will use this value as a tag for all created artifacts (both Jars and Docker images).\nIf unset, gradle will default to using the current VERSION in `.env` for Jars, and `dev` as the Docker image tag.\n:::\n:::info\nGradle will use all CPU cores by default. If Gradle uses too much/too little CPU, tuning the number of CPU cores it uses to better suit a dev's need can help.\nAdjust this by either, 1. Setting an env var: `export GRADLE_OPTS=\"-Dorg.gradle.workers.max=3\"`. 2. Setting a cli option: `SUB_BUILD=PLATFORM ./gradlew build --max-workers 3` 3. Setting the `org.gradle.workers.max` property in the `gradle.properties` file.\nA good rule of thumb is to set this to (# of cores - 1).\n:::\n:::info\nOn Mac, if you run into an error while compiling openssl (this happens when running pip install), you may need to explicitly add these flags to your bash profile so that the C compiler can find the appropriate libraries.\n`text\nexport LDFLAGS=\"-L/usr/local/opt/openssl/lib\"\nexport CPPFLAGS=\"-I/usr/local/opt/openssl/include\"`\n:::\nRun in `dev` mode with `docker-compose`\nThese instructions explain how to run a version of Airbyte Platform that you are developing on (e.g. has not been released yet).\nIn your local `airbyte-platform` repository, run the following commands:\n`bash\nSUB_BUILD=PLATFORM ./gradlew build\nVERSION=dev docker compose up`\nThe build will take a few minutes. Once it completes, Airbyte compiled at current git revision will be running in `dev` mode in your environment.\nIf you are running just connectors, you don't need the first step:\n`bash\nVERSION=dev docker compose up`\nIn `dev` mode, all data will be persisted in `/tmp/dev_root`.\nAdd a connector under development to Airbyte\nThese instructions explain how to run a version of an Airbyte connector that you are developing on (e.g. has not been released yet).\nIn your local `airbyte` repository, run the following command:\n`bash\n./run-ab-platform`\n\nThen, build the connector image:\n`docker build ./airbyte-integrations/connectors/<connector-name> -t airbyte/<connector-name>:dev`\n\n:::info\nThe above connector image is tagged with `dev`. You can change this to use another tag if you'd like.\n:::\n\nIn your browser, visit http://localhost:8000/\nLog in with the default user `airbyte` and default password `password`\nGo to `Settings` (gear icon in lower left corner) \nGo to `Sources` or `Destinations` (depending on which connector you are testing)\nUpdate the version number to use your docker image tag (default is `dev`)\nClick `Change` to save the changes\n\nNow when you run a sync with that connector, it will use your local docker image\nRun platform acceptance tests\nIn your local `airbyte-platform` repository, run the following commands to run acceptance (end-to-end) tests for the platform:\nSUB_BUILD=PLATFORM ./gradlew clean build\nSUB_BUILD=PLATFORM ./gradlew :airbyte-tests:acceptanceTests\n```\nTest containers start Airbyte locally, run the tests, and shutdown Airbyte after running the tests. If you want to run acceptance tests against local Airbyte that is not managed by the test containers, you need to set `USE_EXTERNAL_DEPLOYMENT` environment variable to true:\n`bash\nUSE_EXTERNAL_DEPLOYMENT=true SUB_BUILD=PLATFORM ./gradlew :airbyte-tests:acceptanceTests`\nRun formatting automation/tests\nAirbyte runs a code formatter as part of the build to enforce code styles. You should run the formatter yourself before submitting a PR (otherwise the build will fail).\nThe command to run formatting varies slightly depending on which part of the codebase you are working in.\nPlatform\nIf you are working in the platform run `SUB_BUILD=PLATFORM ./gradlew format` from the root of the `airbyte-platform` repository.\nConnector\nTo format an individual connector in python, run the following command in your local `airbyte` repository:\n`./gradlew :airbyte-integrations:connectors:<connector_name>:airbytePythonFormat`\nFor instance:\n`./gradlew :airbyte-integrations:connectors:source-s3:airbytePythonFormat`\nTo format connectors in java, run `./gradlew format`\nConnector Infrastructure\nFinally, if you are working in any module in `:airbyte-integrations:bases` or `:airbyte-cdk:python`, run the following command in your local `airbyte` repository:\n`bash\nSUB_BUILD=CONNECTORS_BASE ./gradlew format`\nNote: If you are contributing a Python file without imports or function definitions, place the following comment at the top of your file:\n`python\n\"\"\"\n[FILENAME] includes [INSERT DESCRIPTION OF CONTENTS HERE]\n\"\"\"`\nDevelop on `airbyte-webapp`\n\nSpin up Airbyte locally in your local `airbyte-platform` repository so the UI can make requests against the local API.\n\n`bash\nBASIC_AUTH_USERNAME=\"\" BASIC_AUTH_PASSWORD=\"\" docker compose up`\nNote: basic auth must be disabled by setting `BASIC_AUTH_USERNAME` and `BASIC_AUTH_PASSWORD` to empty values, otherwise requests from the development server will fail against the local API.\n\nInstall nvm (Node Version Manager) if not installed\nUse `nvm` to install the required node version:\n\n`bash\ncd airbyte-webapp\nnvm install`\n\nInstall the `pnpm` package manager in the required version:\n\n```bash\n must be the exact version from airbyte-webapp/package.json > engines.pnpm\nnpm install -g pnpm@\n```\n\nStart up the react app.\n\n`bash\npnpm install\npnpm start`\n\nHappy Hacking!\n\nUsing a custom version of the CDK declarative manifest schema for the connector builder UI\nWhen working on the connector builder UI and doing changes to the CDK and the webapp at the same time, you can start the dev server with `CDK_MANIFEST_PATH` or `CDK_VERSION` environment variables set to have the correct Typescript types built. If `CDK_VERSION` is set, it's loading the specified version of the CDK from pypi instead of the default one, if `CDK_MANIFEST_PATH` is set, it's copying the schema file locally.\nFor example:\n`CDK_MANIFEST_PATH=../../airbyte/airbyte-cdk/python/airbyte_cdk/sources/declarative/declarative_component_schema.yaml pnpm start`\nConnector Specification Caching\nThe Configuration API caches connector specifications. This is done to avoid needing to run Docker everytime one is needed in the UI. Without this caching, the UI crawls. If you update the specification of a connector and need to clear this cache so the API / UI picks up the change, you have two options:\n\nGo to the Admin page in the UI and update the version of the connector. Updating to any version, including the one you're already on, will trigger clearing the cache.\nFrom your local `airbyte-platform` repository, restart the server by running the following commands:\n\n`bash\nVERSION=dev docker compose down -v\nVERSION=dev docker compose up`\nResetting the Airbyte developer environment\nSometimes you'll want to reset the data in your local environment. One common case for this is if you are updating an connector's entry in the database (`airbyte-config/init/src/main/resources/config`), often the easiest thing to do, is wipe the local database and start it from scratch. To reset your data back to a clean install of Airbyte, follow these steps:\n\n\nMake sure you are in your local `airbyte-platform` repository\n\n\nDelete the datastore volumes in docker\n\n\n`bash\n    VERSION=dev docker compose down -v`\n\nRemove the data on disk\n\n`bash\n    rm -rf /tmp/dev_root\n    rm -rf /tmp/airbyte_local`\n\nRebuild the project\n\n`bash\n   SUB_BUILD=PLATFORM ./gradlew clean build\n   VERSION=dev docker compose up -V`\nWhile not as common as the above steps, you may also get into a position where want to erase all of the data on your local docker server. This is useful if you've been modifying image tags while developing.\n`bash\ndocker system prune -a\ndocker volume prune`\nIf you are working on python connectors, you may also need to reset the `virtualenv` and re-install the connector's dependencies.\n```bash\nAssuming you have a virtualenv loaded into your shell\ndeactivate\nFrom the connector's directory\nremove the venv directory entirely\nrm -rf .venv\nmake and activate a new venv\npython3 -m venv .venv\nsource .venv/bin/activate\npip install -e \".[dev]\"\n```\nTroubleshooting\n`gradlew Could not target platform: 'Java SE 14' using tool chain: 'JDK 8 (1.8)'.`\nSomehow gradle didn't pick up the right java version for some reason. Find the install version and set the `JAVA_HOME` environment to point to the JDK folder.\nFor example:\n`text\nenv JAVA_HOME=/usr/lib/jvm/java-14-openjdk ./gradlew  :airbyte-integrations:connectors:your-connector-dir:build`\nInspecting the messages passed between connectors",
    "tag": "airbyte"
  },
  {
    "title": "Updating Documentation",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/contributing-to-airbyte/updating-documentation.md",
    "content": "Updating Documentation\nWe welcome contributions to the Airbyte documentation! \nOur docs are written in Markdown following the Google developer documentation style guide and the files are stored in our Github repository. The docs are published at docs.airbyte.com using Docusaurus and GitHub Pages. \nFinding good first issues\nThe Docs team maintains a list of #good-first-issues for new contributors. \n\nIf you're new to technical writing, start with the smaller issues (fixing typos, broken links, spelling and grammar, and so on). You can edit the files directly on GitHub.\nIf you're an experienced technical writer or a developer interested in technical writing, comment on an issue that interests you to discuss it with the Docs team. Once we decide on the approach and the tasks involved, edit the files and open a Pull Request for the Docs team to review.\n\nContributing to Airbyte docs\nBefore contributing to Airbyte docs, read the Airbyte Community Code of Conduct\n:::tip\nIf you're new to GitHub and Markdown, complete the First Contributions tutorial and learn Markdown basics before contributing to Airbyte documentation. \n:::\nYou can contribute to Airbyte docs in two ways:\nEditing directly on GitHub\nTo make minor changes (example: fixing typos) or edit a single file, you can edit the file directly on GitHub:\n\nClick Edit this page at the bottom of any published document on docs.airbyte.com. You'll be taken to the GitHub editor. \nEdit the file directly on GitHub and open a Pull Request.\n\nEditing on your local machine\nTo make complex changes or edit multiple files, edit the files on your local machine:\n\nFork the Airbyte repository.\nClone the fork on your local machine:\n\n`bash\n   git clone git@github.com:{YOUR_USERNAME}/airbyte.git\n   cd airbyte`\nOr\n`bash\n   git clone https://github.com/{YOUR_USERNAME}/airbyte.git\n   cd airbyte`\nWhile cloning on Windows, you might encounter errors about long filenames. Refer to the instructions here to correct it.\n\nTest changes locally:\n\nTo install the docs locally, run the following commands in your terminal:\n`bash\n   cd docusaurus\n   yarn install`\nTo see changes as you make them, run:\n`bash\n   yarn start`\nThen navigate to http://localhost:3000/. Whenever you make and save changes, you will see them reflected in the server. You can stop the running server in OSX/Linux by pressing `Ctrl-C` in the terminal.  \nYou can also build the docs locally and see the resulting changes. This is useful if you introduce changes that need to be run at build-time (e.g. adding a docs plug-in). To do so, run:\n`bash\n   yarn build\n   yarn serve`\nThen navigate to http://localhost:3000/ to see your changes. You can stop the running server in OSX/Linux by pressing `Ctrl-C` in the terminal.  \n\n\nFollow the GitHub workflow to edit the files and create a pull request.\n:::note\nBefore we accept any contributions, you'll need to sign the Contributor License Agreement (CLA). By signing a CLA, we can ensure that the community is free and confident in its ability to use your contributions. You will be prompted to sign the CLA while opening a pull request.\n:::\n\n\nAssign `airbytehq/docs` as a Reviewer for your pull request. \n\n\nAdditional guidelines\n\nIf you're updating a connector doc, follow the Connector documentation template\nIf you're adding a new file, update the sidebars.js file\nIf you're adding a README to a code module, make sure the README has the following components:\nA brief description of the module\nDevelopment pre-requisites (like which language or binaries are required for development)\nHow to install dependencies\nHow to build and run the code locally & via Docker\nAny other information needed for local iteration\n\n\n\nAdvanced tasks\nAdding a redirect\nTo add a redirect, open the docusaurus.config.js file and locate the following commented section:\n`js\n//                        {\n//                         from: '/some-lame-path',\n//                         to: '/a-much-cooler-uri',\n//                        },`\nCopy this section, replace the values, and test the changes locally by going to the path you created a redirect for and verify that the address changes to the new one.\n:::note \nYour path needs a leading slash `/` to work\n:::\nDeploying and reverting the documentation site\n:::note\nOnly the Airbyte team and maintainers have permissions to deploy the documentation site.\n:::\nAutomated documentation site deployment\nWhen `docs/` folder gets changed in `master` branch of the repository, Deploy docs.airbyte.com Github workflow steps in, builds and deploys the documentation site.  This process is automatic, takes five to ten minutes, and needs no human intervention.\nManual documentation site deployment\n:::note\nManual deployment is reserved for emergency cases.  Please, bear in mind that automatic deployment is triggered by changes to `docs/` folder, so it needs to be disabled to avoid interference with manual deployment.\n:::\nYou'll need a GitHub SSH key to deploy the documentation site using the deployment tool. \nTo deploy the documentation site, run:\n```bash\ncd airbyte\nor cd airbyte-cloud\ngit checkout master\ngit pull\n./tools/bin/deploy_docusaurus\n```\nTo revert/rollback doc changes, run:\n```\ncd airbyte\ngit checkout \n./tools/bin/deploy_docusaurus",
    "tag": "airbyte"
  },
  {
    "title": "Contributing to Airbyte",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/contributing-to-airbyte",
    "content": "\ndescription: 'We love contributions to Airbyte, big or small.'\nContributing to Airbyte\nThank you for your interest in contributing! We love community contributions. Contribution guidelines are listed below. If you're unsure about how to start contributing or have any questions even after reading them, feel free to ask us on Slack in the #dev or #general channel.\nHowever, for those who want a bit more guidance on the best way to contribute to Airbyte, read on. This document will cover what we're looking for. By addressing the points below, the chances that we can quickly merge or address your contributions will increase.\nCode of conduct\nPlease follow our Code of conduct in the context of any contributions made to Airbyte.\nAirbyte specification\nBefore you can start contributing, you need to understand Airbyte's data protocol specification.\nFirst-time contributors, welcome!\nWe appreciate first time contributors and we are happy to assist you in getting started. In case of questions, just reach out to us via email or Slack!\nHere is a list of easy good first issues to do.\nAreas for contributing\nWe gladly welcome all improvements existing on the codebase. \n1. Open an issue, or find a similar one.\nBefore jumping into the code please first:\n1. Verify if an existing connector or platform GitHub issue matches your contribution project.\n2. If you don't find an existing issue, create a new connector or platform issue to explain what you want to achieve.\n3. Assign the issue to yourself and add a comment to tell that you want to work on this.\nThis will enable our team to make sure your contribution does not overlap with existing works and will comply with the design orientation we are currently heading the product toward.\nIf you do not receive an update on the issue from our team, please ping us on Slack!\n2. Let's code\n\nTo contribute to a connector, fork the Connector repository. To contribute to the Airbyte platform, fork our Platform repository.  \nOpen a branch for your work.\nCode, and please write tests.\nEnsure all tests pass. For connectors, this includes acceptance tests as well. \n\n3. Open a pull request\n\nRebase master with your branch before submitting a pull request.\nOpen the pull request.\nWait for a review from a community maintainer or our team.\n\n4. Review process\nWhen we review, we look at:\n* \u200cDoes the PR solve the issue?\n* Is the proposed solution reasonable?\n* Is it tested? (unit tests or integration tests)\n* Is it introducing security risks?\n\u200cOnce your PR passes, we will merge it \ud83c\udf89.\nNew connectors\nIt's easy to add your own connector to Airbyte! Since Airbyte connectors are encapsulated within Docker containers, you can use any language you like. Here are some links on how to add sources and destinations. We haven't built the documentation for all languages yet, so don't hesitate to reach out to us if you'd like help developing connectors in other languages.\nFor sources, simply head over to our Python CDK.\n:::info\nThe CDK currently does not support creating destinations, but it will very soon.\n::::\n\nSee Building new connectors to get started.\nSince we frequently build connectors in Python, on top of Singer or in Java, we've created generator libraries to get you started quickly: Build Python Source Connectors and Build Java Destination Connectors\nIntegration tests (tests that run a connector's image against an external resource) can be run one of three ways, as detailed here\n\nPlease note that, at no point in time, we will ask you to maintain your connector. The goal is that the Airbyte team and the community helps maintain the connector.\nDocumentation\nOur goal is to keep our docs comprehensive and updated. If you would like to help us in doing so, we are grateful for any kind of contribution:\n\nReport missing content\nFix errors in existing docs\nHelp us in adding to the docs\n\nThe contributing guide for docs can be found here.\nCommunity content\nWe welcome contributions as new tutorials / showcases / articles, or to any of the existing guides on our tutorials page:\n\nFix errors in existing tutorials\nAdd new tutorials (please reach out to us if you have ideas to avoid duplicate work)\nRequest tutorials\n\nWe have a repo dedicated to community content. Everything is documented there.\nFeel free to submit a pull request in this repo, if you have something to add even if it's not related to anything mentioned above.\nOther ways you can contribute\nUpvoting issues, feature and connector requests\nYou are welcome to add your own reactions to the existing issues. We will take them in consideration in our prioritization efforts, especially for connectors.\n\u2764\ufe0f means that this task is CRITICAL to you.\n\ud83d\udc4d means it is important to you.\nRequesting new features\nTo request new features, please create an issue on this project.\nIf you would like to suggest a new feature, we ask that you please use our issue template. It contains a few essential questions that help us understand the problem you are looking to solve and how you think your recommendation will address it. We also tag incoming issues from this template with the \"community_new\" label. This lets our teams quickly see what has been raised and better address the community recommendations.\nTo see what has already been proposed by the community, you can look here.\nWatch out for duplicates! If you are creating a new platform issue, please check open, or recently closed. \nRequesting new connectors\nThis is very similar to requesting new features. The template will change a bit and all connector requests will be tagged with the \u201ccommunity\u201d and \u201carea/connectors\u201d labels.\nTo see what has already been proposed by the community, you can look here. Again, watch out for duplicates!\nReporting bugs\n\u200cBug reports help us make Airbyte better for everyone. We provide a preconfigured template for bugs to make it very clear what information we need.\n\u200cPlease search within our already reported bugs before raising a new one to make sure you're not raising a duplicate.\nReporting security issues\nPlease do not create a public GitHub issue. If you've found a security issue, please email us directly at security@airbyte.io instead of raising an issue.\nAirbyte CI workflows",
    "tag": "airbyte"
  },
  {
    "title": "Monorepo Python Development",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/contributing-to-airbyte/python-gradle-setup.md",
    "content": "Monorepo Python Development\nThis guide contains instructions on how to setup Python with Gradle within the Airbyte Monorepo. If you are a contributor working on one or two connectors, this page is most likely not relevant to you. Instead, you should use your standard Python development flow.\nPython Connector Development\nBefore working with connectors written in Python, we recommend running the following command from the airbyte root directory\n`bash\npython3 tools/bin/update_intellij_venv.py -modules <connector directory name> --install-venv`\ne.g\n`bash\npython tools/bin/update_intellij_venv.py -modules source-stripe --install-venv`\nIf using Pycharm or IntelliJ, you'll also want to add the interpreter to the IDE's list of known interpreters. You can do this by adding the `--update-intellij` flag. More details can be found here\n`bash\npython tools/bin/update_intellij_venv.py -modules <connector directory name> --install-venv --update-intellij`\nIf working with many connectors, you can use the `--all-modules` flag to install the virtual environments for all connectors\n`bash\npython tools/bin/update_intellij_venv.py --all-modules --install-venv`\nThis will create a `virtualenv` and install dependencies for the connector you want to work on as well as any internal Airbyte python packages it depends on.\nWhen iterating on a single connector, you will often iterate by running\n`text\n./gradlew :airbyte-integrations:connectors:your-connector-dir:build`\nThis command will:\n\nInstall a virtual environment at `airbyte-integrations/connectors/<your-connector-dir>/.venv`\nInstall local development dependencies specified in `airbyte-integrations/connectors/your-connector-dir/requirements.txt`\nRuns the following pip modules:\nBlack to lint the code\nisort to sort imports\nFlake8 to check formatting\nMyPy to check type usage\n\n\n\nFormatting/linting\nTo format and lint your code before commit you can use the Gradle command above, but for convenience we support pre-commit tool. To use it you need to install it first:\n`bash\npip install pre-commit`\nthen, to install `pre-commit` as a git hook, run\n`text\npre-commit install`\nThat's it, `pre-commit` will format/lint the code every time you commit something. You find more information about pre-commit here.\nIDE\nAt Airbyte, we use IntelliJ IDEA for development. Although it is possible to develop connectors with any IDE, we typically recommend IntelliJ IDEA or PyCharm, since we actively work towards compatibility.\nAutocompletion\nInstall the Pydantic plugin. This will help autocompletion with some of our internal types.\nPyCharm (ItelliJ IDEA)\nThe following setup steps are written for PyCharm but should have similar equivalents for IntelliJ IDEA:\n1.`python tools/bin/update_intellij_venv.py -modules <your-connector-dir> --update-intellij`\n\nRestart PyCharm\nGo to `File -> New -> Project...`\nSelect `Pure Python`.\nSelect a project name like `airbyte` and a directory outside of the `airbyte` code root.\nGo to `Preferences -> Project -> Python Interpreter`\nFind a gear \u2699\ufe0f button next to `Python interpreter` dropdown list, click and select `Add`\nSelect `Virtual Environment -> Existing`\nSet the interpreter path to the one that was created by Python command, i.e. `airbyte-integrations/connectors/<your-connector-dir>/.venv/bin/python`.\nWait for PyCharm to finish indexing and loading skeletons from selected virtual environment.\n\nYou should now have access to code completion and proper syntax highlighting for python projects.\nIf you need to work on another connector you can quickly change the current virtual environment in the bottom toolbar.\nExcluding files from venv\nBy default, the find function in IntelliJ is not scoped and will include all files in the monorepo, including all the libraries installed as part of a connector's virtual environment. This huge volume of files makes indexing and search very slow. You can ignore files from the connectors' virtual environment with the following steps:\n\nOpen the project structure using `cmd-;`\nNavigate to the \"Project Settings / Modules\" section in the right-side of the menu\nSelect the top level `airbyte` module so the change is applied to all submodules\nAdd the following filter to the `Exclude files` option: `connectors/**/.venv`\nPress OK to confirm your options.\n\n\nManual Workaround\nWe have seen the above solution not being applied by IntelliJ. The exact reason is not clear to us but as a workaround, you can:\n1. Open `.gitignore` in your IntelliJ\n2. There will be a banner saying `Some of the ignored directories are not excluded from indexing and search`. Click on `View Directories`",
    "tag": "airbyte"
  },
  {
    "title": "Maintainer Code of Conduct",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/contributing-to-airbyte/maintainer-code-of-conduct.md",
    "content": "\ndescription: Be nice to one another.\nMaintainer Code of Conduct\nOur maintainer program can only succeed if a certain number of rules are respected. We appreciate your continued commitment to making this program a success, and are proud of counting you in our maintainer community. \nRule 1: Be respectful.\nYou will be reviewing the PRs of Airbyte\u2019s contributors. They are a central part of our community, as you are, so we want everyone to have a fulfilling experience. All of the guidelines we provide below are important, but there\u2019s a reason respect is the first rule. We take it seriously, we cannot condone disrespectful behavior.\nRule 2: Be empowered to claim a PR.\nOnce you have commit access, you will be able to claim a PR, so another maintainer won\u2019t review it. This will remove some pressure on getting it reviewed before any other maintainer. The quality of the review is important, as it has a huge impact on the contributors\u2019 experiences.\nRule 3: Review claimed PRs within 24 hours.\nAt the same time, we can\u2019t let a PR stay open for too long. The goal of claiming a PR is to indicate the other maintainers that you\u2019re on it. Claiming but not reviewing it is not fair to other maintainers. This explains why we give a reasonable amount of time, i.e. 24 hours to review the PR. Claiming a PR means you intend to review it today. \nRule 4: Claim one PR at a time.\nAgain, to be fair with other maintainers, this rule prevents any maintainer from monopolizing all the bounties.\nRule 5: Claim another PR, if an existing PR is blocked.\nOnce you have reviewed a PR, you should be able to claim and review another one. This rule is about enabling you to do that, even though the first PR you reviewed wasn\u2019t approved. The Code of Conduct is about reviewing a PR one at a time. That means you could be the active reviewed of 3 PRs, only if at least 2 of them are awaiting code changes by the contributor.\nRule 6: Comment with clarity and thoughtfulness.\nOur contributor\u2019s and maintainer\u2019s experience is paramount to us. Putting some effort into a well-researched and thoughtful comment shows consideration for the contributors\u2019 time and will get a more efficient PR review process.\nRule 7: Low-quality code should not be merged.\nAirbyte is infrastructure, and therefore needs to be very reliable. The code we accept should only be high quality and not patch code. Please have the same level of expectations as if it was code within your own infrastructure. \nIf you see a message or receive a direct message that violates any of these rules, please contact an Airbyte team member and we will take the appropriate moderation action immediately. We have zero tolerance for intentional rule-breaking and hate speech.",
    "tag": "airbyte"
  },
  {
    "title": "Updating Gradle Dependencies",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/contributing-to-airbyte/gradle-dependency-update.md",
    "content": "Updating Gradle Dependencies\nWe use Gradle Catalogs\nto keep dependencies synced up across different Java projects. This is particularly useful for Airbyte Cloud, and can be\nused by any project seeking to build off Airbyte.\nCatalogs allow dependencies to be represented as dependency coordinates. A user can reference preset dependencies/versions\nwhen declaring dependencies in a build script.\n\nVersion Catalog Example:\n`gradle\ndependencies {\n   implementation(libs.groovy.core)\n}`\nIn this context, libs is a catalog and groovy represents a dependency available in this catalog. Instead of declaring a\nspecific version, we reference the version in the Catalog.\n\nThis helps reduce the chances of dependency drift and dependency hell.\nThus, please use the Catalog when:\n- declaring new common dependencies.\n- specifying new common dependencies.\nA common dependency is a foundational Java package e.g. Apache commons, Log4j etc that is often the basis on which libraries\nare built upon.\nThis is a relatively new addition, so devs should keep this in mind and use the top-level Catalog on a best-effort basis.\nSetup Details\nThis section is for engineers wanting to understand Gradle Catalog details and how Airbyte has set this up.\nThe version catalog TOML file format\nGradle offers a conventional file to declare a catalog.\nIt\u2019s a conventional location to declare dependencies that are both consumed and published.\nThe TOML file consists of 4 major sections:\n- the [versions] section is used to declare versions which can be referenced by dependencies\n- the [libraries] section is used to declare the aliases to coordinates\n- the [bundles] section is used to declare dependency bundles\n- the [plugins] section is used to declare plugins\n\nTOML file Example:\n```gradle\n[versions]\ngroovy = \"3.0.5\"\n[libraries]\ngroovy-core = { module = \"org.codehaus.groovy:groovy\", version.ref = \"groovy\" }\n[bundles]\ngroovy = [\"groovy-core\", \"groovy-json\", \"groovy-nio\"]\n[plugins]\njmh = { id = \"me.champeau.jmh\", version = \"0.6.5\" }\n```\nNOTE: for more information please follow this link.\n\nAs described above this project contains TOML file `deps.toml` which is fully fulfilled with respect to official documentation.\nIn case when new versions should be used please update `deps.toml` accordingly.\n\ndeps.toml\n\n[versions]\nfasterxml_version = \"2.13.0\"\nglassfish_version = \"2.31\"\ncommons_io = \"2.7\"\nlog4j = \"2.17.1\"\nslf4j = \"1.7.30\"\nlombok = \"1.18.22\"\njunit-jupiter = \"5.8.2\"\n\n[libraries]\nfasterxml = { module = \"com.fasterxml.jackson:jackson-bom\", version.ref = \"fasterxml_version\" }\nglassfish = { module = \"org.glassfish.jersey:jackson-bom\", version.ref = \"glassfish_version\" }\njackson-databind = { module = \"com.fasterxml.jackson.core:jackson-databind\", version.ref = \"fasterxml_version\" }\njackson-annotations = { module = \"com.fasterxml.jackson.core:jackson-annotations\", version.ref = \"fasterxml_version\" }\njackson-dataformat = { module = \"com.fasterxml.jackson.dataformat:jackson-dataformat-yaml\", version.ref = \"fasterxml_version\" }\njackson-datatype = { module = \"com.fasterxml.jackson.datatype:jackson-datatype-jsr310\", version.ref = \"fasterxml_version\" }\nguava = { module = \"com.google.guava:guava\", version = \"30.1.1-jre\" }\ncommons-io = { module = \"commons-io:commons-io\", version.ref = \"commons_io\" }\napache-commons = { module = \"org.apache.commons:commons-compress\", version = \"1.20\" }\napache-commons-lang = { module = \"org.apache.commons:commons-lang3\", version = \"3.11\" }\nslf4j-api = { module = \"org.slf4j:slf4j-api\", version = \"1.7.30\" }\nlog4j-api = { module = \"org.apache.logging.log4j:log4j-api\", version.ref = \"log4j\" }\nlog4j-core = { module = \"org.apache.logging.log4j:log4j-core\", version.ref = \"log4j\" }\nlog4j-impl = { module = \"org.apache.logging.log4j:log4j-slf4j-impl\", version.ref = \"log4j\" }\nlog4j-web = { module = \"org.apache.logging.log4j:log4j-web\", version.ref = \"log4j\" }\njul-to-slf4j = { module = \"org.slf4j:jul-to-slf4j\", version.ref = \"slf4j\" }\njcl-over-slf4j = { module = \"org.slf4j:jcl-over-slf4j\", version.ref = \"slf4j\" }\nlog4j-over-slf4j = { module = \"org.slf4j:log4j-over-slf4j\", version.ref = \"slf4j\" }\nappender-log4j2 = { module = \"com.therealvan:appender-log4j2\", version = \"3.6.0\" }\naws-java-sdk-s3 = { module = \"com.amazonaws:aws-java-sdk-s3\", version = \"1.12.6\" }\ngoogle-cloud-storage = { module = \"com.google.cloud:google-cloud-storage\", version = \"2.2.2\" }\ns3 = { module = \"software.amazon.awssdk:s3\", version = \"2.16.84\" }\nlombok = { module = \"org.projectlombok:lombok\", version.ref = \"lombok\" }\njunit-jupiter-engine = { module = \"org.junit.jupiter:junit-jupiter-engine\", version.ref = \"junit-jupiter\" }\njunit-jupiter-api = { module = \"org.junit.jupiter:junit-jupiter-api\", version.ref = \"junit-jupiter\" }\njunit-jupiter-params = { module = \"org.junit.jupiter:junit-jupiter-params\", version.ref = \"junit-jupiter\" }\nmockito-junit-jupiter = { module = \"org.mockito:mockito-junit-jupiter\", version = \"4.0.0\" }\nassertj-core = { module = \"org.assertj:assertj-core\", version = \"3.21.0\" }\njunit-pioneer = { module = \"org.junit-pioneer:junit-pioneer\", version = \"1.6.2\" }\nfindsecbugs-plugin = { module = \"com.h3xstream.findsecbugs:findsecbugs-plugin\", version = \"1.11.0\" }\n\n[bundles]\njackson = [\"jackson-databind\", \"jackson-annotations\", \"jackson-dataformat\", \"jackson-datatype\"]\napache = [\"apache-commons\", \"apache-commons-lang\"]\nlog4j = [\"log4j-api\", \"log4j-core\", \"log4j-impl\", \"log4j-web\"]\nslf4j = [\"jul-to-slf4j\", \"jcl-over-slf4j\", \"log4j-over-slf4j\"]\njunit = [\"junit-jupiter-api\", \"junit-jupiter-params\", \"mockito-junit-jupiter\"]\n\n\nDeclaring a version catalog\nVersion catalogs can be declared in the settings.gradle file.\nThere should be specified section `dependencyResolutionManagement` which uses `deps.toml` file as a declared catalog.\n\nExample:\n`gradle\ndependencyResolutionManagement {\n    repositories {\n        maven {\n            url 'https://airbyte.mycloudrepo.io/public/repositories/airbyte-public-jars/'\n       }\n    }\n    versionCatalogs {\n        libs {\n            from(files(\"deps.toml\"))\n        }\n    }\n}`\n\nSharing Catalogs\nTo share this catalog for further usage by other Projects, we do the following 2 steps:\n- Define `version-catalog` plugin in `build.gradle` file (ignore if this record exists)\n  `gradle\n  plugins {\n      id '...'\n      id 'version-catalog'`\n- Prepare Catalog for Publishing\n  `gradle\n  catalog {\n      versionCatalog {\n          from(files(\"deps.toml\")) < --- declere either dependencies or specify existing TOML file\n      }\n  }`\nConfigure the Plugin Publishing Plugin\nTo Publishing, first define the `maven-publish` plugin in `build.gradle` file (ignore if this already exists):\n`gradle\nplugins {\n    id '...'\n    id 'maven-publish'\n}`\nAfter that, describe the publishing section. Please use this official documentation for more details.\n\nExample:\n```gradle\npublishing {\n    publications {\n        maven(MavenPublication) {\n            groupId = 'io.airbyte'\n            artifactId = 'oss-catalog'\n\n\n```            from components.versionCatalog\n    }\n}\n\nrepositories {\n    maven {\n        url 'https://airbyte.mycloudrepo.io/repositories/airbyte-public-jars'\n        credentials {\n            name 'cloudrepo'\n            username System.getenv('CLOUDREPO_USER')\n            password System.getenv('CLOUDREPO_PASSWORD')\n        }\n    }\n\n    mavenLocal()\n}\n```\n\n\n}\n",
    "tag": "airbyte"
  },
  {
    "title": "Issues & Pull Requests",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/contributing-to-airbyte/issues-and-pull-requests.md",
    "content": "Issues & Pull Requests\nTitles\nDescribe outputs, not implementation: An issue or PR title should describe the desired end result, not the implementation. The exception is child issues/subissues of an epic. Be specific about the domain. Airbyte operates a monorepo, so being specific about what is being changed in the PR or issue title is important.\nSome examples: subpar issue title: `Remove airbyteCdk.dependsOn(\"unrelatedPackage\")`. This describes a solution not a problem.\ngood issue title: `Building the Airbyte Python CDK should not build unrelated packages`. Describes desired end state and the intent is understandable without reading the full issue.\nsubpar PR title: `Update tests`. Which tests? What was the update?\ngood PR title: `Source MySQL: update acceptance tests to connect to SSL-enabled database`. Specific about the domain and change that was made.\nPR title conventions When creating a PR, follow the naming conventions depending on the change being made:\n\nNotable updates to Airbyte Core: \"\ud83c\udf89\"\ne.g: `\ud83c\udf89 enable configuring un-nesting in normalization`\nNew connectors: \u201c\ud83c\udf89 New source or destination: \u201d e.g: `\ud83c\udf89 New Source: Okta`\nNew connector features: \u201c\ud83c\udf89 :  E.g:\n`\ud83c\udf89 Destination Redshift: write JSONs as SUPER type instead of VARCHAR`\n`\ud83c\udf89 Source MySQL: enable logical replication`\nBugfixes should start with the  \ud83d\udc1b emoji\n`\ud83d\udc1b Source Facebook Marketing: fix incorrect parsing of lookback window`\nDocumentation improvements should start with any of the book/paper emojis: \ud83d\udcda \ud83d\udcdd etc\u2026\nAny refactors, cleanups, etc.. that are not visible improvements to the user should not have emojis\n\nThe emojis help us identify which commits should be included in the product release notes.\nDescriptions\nContext: Provide enough information (or a link to enough information) in the description so team members with no context can understand what the issue or PR is trying to accomplish. This usually means you should include two things:\n\nSome background information motivating the problem\nA description of the problem itself\nGood places to start reading and file changes that can be skipped\n\nSome examples:\ninsufficient context: `Create an OpenAPI to JSON schema generator`. Unclear what the value or problem being solved here is.\ngood context:\n```text\nWhen creating or updating connectors, we spend a lot of time manually transcribing JSON Schema files based on OpenAPI docs. This is ncessary because OpenAPI and JSON schema are very similar but not perfectly compatible. This process is automatable. Therefore we should create a program which converts from OpenAPI to JSONSchema format.",
    "tag": "airbyte"
  },
  {
    "title": "SonarQube workflow",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/contributing-to-airbyte/sonar-qube-workflow.md",
    "content": "SonarQube workflow\nGoals\n\u00a0The Airbyte monorepo receives contributions from a lot of developers, and there is no way around human errors while merging PRs.\nLikely every language has different tools for testing and validation of source files. And while it's best practice to lint and validate code before pushing to git branches, it doesn't always happen.\nBut it is optional, and as rule as we detect possible problems after launch test/publish commands only. Therefore, using of automated CI code validation can  provided the following benefits:\n* Problem/vulnerability reports available when the PR was created. And developers would fix bugs and remove smells before code reviews.\n* Reviewers would be sure all standard checks were made and code changes satisfy the requirements.\n* Set of tools and their options can be changed anytime globally.\n* Progress of code changes are saved in SonarQube and this information helps to analyse quality of the product  integrally and also its separate parts.\nUML diagram\n\nUsed tools\nPython\n\nflake8\nmypy\nisort\nblack\ncoverage\n\nAll Python tools use the common pyproject.toml file.\nCommon tools\n\nSonarQube Scanner\n\nAccess to SonarQube\nThe Airbyte project uses a custom SonarQube instance. Access to it is explained here.\nSonarQube settings",
    "tag": "airbyte"
  },
  {
    "title": "Connector Doc Template",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/contributing-to-airbyte/templates/integration-documentation-template.md",
    "content": "\ndescription: >-\n  This is the template that should be used when adding documentation for a new\n  connector.\n\nConnector Doc Template\nSync overview\nOutput schema\nIs the output schema fixed (e.g: for an API like Stripe)? If so, point to the connector's schema (e.g: link to Stripe\u2019s documentation) or describe the schema here directly (e.g: include a diagram or paragraphs describing the schema).\nDescribe how the connector's schema is mapped to Airbyte concepts. An example description might be: \u201cMagicDB tables become Airbyte Streams and MagicDB columns become Airbyte Fields. In addition, an extracted_at column is appended to each row being read.\u201d\nData type mapping\nThis section should contain a table mapping each of the connector's data types to Airbyte types. At the moment, Airbyte uses the same types used by JSONSchema. `string`, `date-time`, `object`, `array`, `boolean`, `integer`, and `number` are the most commonly used data types.\n| Integration Type | Airbyte Type | Notes |\n| :--- | :--- | :--- |\nFeatures\nThis section should contain a table with the following format:\n| Feature | Supported?(Yes/No) | Notes |\n| :--- | :--- | :--- |\n| Full Refresh Sync |  |  |\n| Incremental Sync |  |  |\n| Replicate Incremental Deletes |  |  |\n| For databases, WAL/Logical replication |  |  |\n| SSL connection |  |  |\n| SSH Tunnel Support |  |  |\n| (Any other source-specific features) |  |  |\nPerformance considerations\nCould this connector hurt the user's database/API/etc... or put too much strain on it in certain circumstances? For example, if there are a lot of tables or rows in a table? What is the breaking point (e.g: 100mm> records)? What can the user do to prevent this? (e.g: use a read-only replica, or schedule frequent syncs, etc..)\nGetting started\nRequirements\n\nWhat versions of this connector does this implementation support? (e.g: `postgres v3.14 and above`) \nWhat configurations, if any, are required on the connector? (e.g: `buffer_size > 1024`)\nNetwork accessibility requirements\nCredentials/authentication requirements? (e.g: A  DB user with read permissions on certain tables) \n\nSetup guide\nFor each of the above high-level requirements as appropriate, add or point to a follow-along guide. See existing source or destination guides for an example.\nFor each major cloud provider we support, also add a follow-along guide for setting up Airbyte to connect to that destination. See the Postgres destination guide for an example of what this should look like.",
    "tag": "airbyte"
  },
  {
    "title": "API Documentation Folder",
    "source": "https://github.com/airbytehq/airbyte/tree/master/docs/reference/api",
    "content": "API Documentation Folder\n\n`generated-api-html`: Plain HTML file automatically generated from the Airbyte OAS spec as part of the build.\n`api-documentation.md`: Markdown for API documentation Gitbook page.\n`rapidoc-api-docs.html`: HTML for actual API Spec Documentation and linked to in the above Gitbook page. This is a S3 static website hosted out of\n  the airbyte-public-api-docs bucket with a Cloudfront Distribution\n  for SSL. This file points to the Airbyte OAS spec on Master and will automatically mirror spec changes.\n",
    "tag": "airbyte"
  }
]