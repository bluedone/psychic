[
  {
    "title": "MindsDB newsletter \ud83d\udcf0",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/community.mdx",
    "content": "\ntitle: Join our Community \ud83d\udc3b\nsidebarTitle: Join our Community\n\nIf you have questions or you want to chat with the MindsDB core team or other\ncommunity members, you can join our\nSlack workspace.\nMindsDB newsletter \ud83d\udcf0\nTo get updates on MindsDB's latest announcements, releases and events,\nsign up for our newsletter.\nBecome a MindsDB Beta tester \ud83d\udd0e\nIf you want to become a part of our product and get first access to all of our\nlatest updates, join our\nBeta testers community.\nTalk to our engineers \u260e\nIf you want to use MindsDB or you have more questions, you can schedule a call\nby clicking on `Talk to our Engineers` button.\nGet in touch for collaboration \ud83d\udcde",
    "tag": "mindsdb"
  },
  {
    "title": "Natural Language Processing (NLP)",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/ml-types.mdx",
    "content": "\ntitle: Introduction to Machine Learning\nsidebarTitle: Introduction to ML\n\nMindsDB offers four ways you can do machine learning:\n\nNatural Language Processing (NLP)\nTime Series\nClassification\nRegression\n\nNatural Language Processing (NLP)\nNatural Language Processing (NLP) is a branch of artificial intelligence that enables computers to understand, interpret, and manipulate human language.\nLearn more about NLP here and check out the examples!\nTime Series\nTime series models fall under the regression or classification category. But what\u2019s distinct about them is that we order data by date, time, or any value defining sequential order of events. Usually, predictions made by time series models are referred to as forecasts.\nLearn more about Timie Series here and check out the examples!\nClassification\nClassification is a type of predictive modeling that analyses input data, including relationships between dependent and independent variables and the target variable that is to be predicted. In the case of classification models, the target variable belongs to a set of discrete values.\nLearn more about Classification here and check out the examples!\nRegression\nRegression is a type of predictive modeling that analyses input data, including relationships between dependent and independent variables and the target variable that is to be predicted. In the case of regression models, the target variable belongs to a set of continuous values.",
    "tag": "mindsdb"
  },
  {
    "title": "Contribution issues \ud83d\udd27",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/contribute.mdx",
    "content": "\ntitle: Contribute to MindsDB \ud83d\udc3b\nsidebarTitle: How to Contribute\n\nThank you for your interest in contributing to MindsDB. MindsDB is free, open\nsource software, and all types of contributions are welcome, whether they\u2019re\ndocumentation changes, bug reports, bug fixes or new source code changes.\nContribution issues \ud83d\udd27\nMost of the issues that are open for contributions will be tagged with\n`good-first-issue` or `help-wanted`. A great place to start looking will be our\nGitHub projects for:\n\nCommunity writers dashboard.\nCommunity code contributors\n  dashboard.\n\nAlso, we are always open to suggestions so feel free to open new issues with\nyour ideas and we can give you guidance!\nAfter you find the issue that you want to contribute to, follow the\n`fork-and-pull` workflow:\n\nFork the MindsDB repository\nClone the repository locally\nMake changes and commit them\nPush your local branch to your fork\nSubmit a Pull Request so that we can review your changes\nWrite a commit message\nMake sure that the CI tests are GREEN\n\n\n  Be sure to merge the latest from \"upstream\" before making a Pull Request!\n\nPull Request reviews are done on a regular basis. Please make sure you respond\nto our feedback/questions and sign our CLA.\nDocumentation \ud83d\udcd6\nWe are always trying to improve our documentation. All Pull Requests that\nimprove our grammar or docs structure or fix typos are welcomed.\n\nCheck the\n  documentation tagged issues\n  and help us.\n\nWrite for us \ud83d\udcdd\nDo you find MindsDB useful and want to share your story? Make a PR to this repo\nwith your writing in a markdown file, or just post it on Medium, Dev or your own",
    "tag": "mindsdb"
  },
  {
    "title": "What are AI Tables?",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/what-is-mindsdb.mdx",
    "content": "\ntitle: What is MindsDB?\nsidebarTitle: What is MindsDB?\n\nData that lives in your database is a valuable asset. MindsDB enables you to use\nyour data and make forecasts. It speeds up the ML development process by\nbringing machine learning into the database.\nWith MindsDB, you can build, train, optimize, and deploy your ML models without\nthe need for other platforms. And to get the forecasts, simply query your data\nand ML models. Read along to see some examples.\n\nWhat are AI Tables?\nMindsDB brings machine learning into databases by employing the concept of AI\nTables.\nAI Tables are machine learning models stored as virtual tables inside a\ndatabase. They facilitate making predictions based on your data. You can perform\nthe time series, regression, and classification predictions within your database\nand get the output almost instantly by querying an AI Table with simple SQL\nstatements.\nDeep Dive into the AI Tables\nCurrent Challenges\nLet\u2019s consider the following `income_table` table that stores the `income` and\n`debt` values.\n`sql\nSELECT income, debt\nFROM income_table;`\nOn execution, we get:\n`sql\n+------+-----+\n|income|debt |\n+------+-----+\n|60000 |20000|\n|80000 |25100|\n|100000|30040|\n|120000|36010|\n+------+-----+`\nA simple visualization of the data present in the `income_table` table is as\nfollows:\n\nQuerying the income table to get the `debt` value for a particular `income`\nvalue results in the following:\n`sql\nSELECT income, debt\nFROM income_table\nWHERE income = 80000;`\nOn execution, we get:\n`sql\n+------+-----+\n|income|debt |\n+------+-----+\n|80000 |25100|\n+------+-----+`\nAnd here is what we get:\n\nBut what happens when querying the table for an `income` value that is not\npresent there?\n`sql\nSELECT income, debt\nFROM income_table\nWHERE income = 90000;`\nOn execution, we get:\n`sql\nEmpty set (0.00 sec)`\nWhen the `WHERE` clause condition is not fulfilled for any of the rows, no\nvalue is returned.\n\nWhen a table doesn\u2019t have an exact match, the query returns an empty set or null\nvalue. This is where the AI Tables come into play!\nSolution Offered by MindsDB\nLet\u2019s create a `debt_model` model that allows us to approximate the `debt` value\nfor any `income` value. We train the `debt_model` model using the data from the\n`income_table` table.\n`sql\nCREATE MODEL mindsdb.debt_model\nFROM income_table\nPREDICT debt;`\nOn execution, we get:\n`sql\nQuery OK, 0 rows affected (x.xxx sec)`\nMindsDB provides the CREATE MODEL\nstatement. On execution of this statement, the predictive model works in the\nbackground, automatically creating a vector representation of the data that can\nbe visualized as follows:\n\nLet\u2019s now look for the `debt` value of some random `income` value. To get the\napproximated `debt` value, we query the `mindsdb.debt_model` model instead\nof the `income_table` table.\n`sql\nSELECT income, debt\nFROM mindsdb.debt_model\nWHERE income = 90000;`\nOn execution, we get:\n`sql\n+------+-----+\n|income|debt |\n+------+-----+\n|90000 |27820|\n+------+-----+`\nAnd here is how it looks:\n\nWhy Choose MindsDB?\nShift to Data Analysis Paradigm\nThere is an ongoing transformational shift within the modern business world from\nthe \u201cwhat happened and why\u201d based on historical data analysis to the \u201cwhat will\nhappen and how can we make it happen\u201d based on machine learning predictive\nmodeling.\n\nThe success of your predictions depends both on the data you have available and\nthe models trained with the data. Data Scientists and Data Engineers require\nefficient and easy-to-use tools to prepare the data for feature engineering,\nthen training the models, and finally, deploying, monitoring, and managing these\nimplementations for optimal prediction confidence.\nThe Machine Learning Lifecycle\nThe ML lifecycle is a process that consists of the data preparation phase,\nmodeling phase, and deployment phase. The diagram below presents all the steps\nincluded in each of the stages.\n\nCurrent solutions for implementing machine learning encounter various\nchallenges, such as time-consuming preparation, cleaning, and labeling of\nsubstantial amounts of data, and difficulties in finding qualified ML/AI data\nscientists.\nThe processes that must be followed by the ML/AI data scientists to implement\nmachine learning include the following:\n\nfeature engineering,\nbuilding, training, and optimizing models,\nassembling, verifying, and deploying models to production,\ncontinuously monitoring and improving the models,\ncontinuously training the models, as they require multiple training iterations\n  with existing data,\nextracting, transforming, and loading (ETL) data from one system to another,\n  which is complicated and may lead to multiple copies of information.\n\nA recent study has shown it takes 64% of companies a month up to over a year to\ndeploy a machine learning model into production. Leveraging existing databases\nand automating all the aforementioned processes is called AutoML. AutoML has\nbeen gaining traction within enterprises for enabling non-experts to use machine\nlearning models for practical applications.\nWhy MindsDB?\nWell, as with most names, we needed one. We like science fiction and\nThe Culture series,\nwhere the AI super-smart entities are called Minds. So that's for the first\npart of our name.\nAs for the second part - the DB, it is quite self-explanatory. Although we\nwill support all kinds of data in the future, but currently, our objective is to\nadd intelligence to existing data stores and databases. Hence, the term DB\ncomes along.\nSo there we have it, MindsDB.\nAnd why the bear? We wanted to honor the open-source tradition of animals\nrelated to projects. We went for a bear because MindsDB was born at UC Berkeley,\nwhere the first codes were written. Then, we went a step further and decided for\na polar bear.\nHow to Help Democratize Machine Learning?\nHere is what you can do:\n\n\nGo ahead and try out MindsDB by following our tutorials, and in case of\n      problems, you can always\n      report an issue here.\n\n\nAre you familiar with Python? You can then help us out in resolving open\n      issues. At first, have a look at\n      issues labeled with the good first issue tag,\n      as these should be easy to start.\n\n\nYou can also help us with documentation and tutorials. Here is how you can\n      contribute by writing\n      documentation and\n      tutorials. Don't\n      forget to follow the style guide.\n\n\nShare with your friends and spread the word about MindsDB.\n\n\nJoin our team! We are a fast-growing company, so we always have\n      a few open positions.\n\n\n\nFrom Our Community\nCheck out the articles and video guides created by our community:\n\n\nArticle on What is MindsDB?\n  by Gloria Okeke E.J\n\n\nArticle on What is MindsDB? How to get started with it \n  by Hritik Dangi\n\n\nVideo guide on Video: What is MindsDB? by\n  Alissa Troiano\n\n\nVideo guide on What is MindsDB | How to Get Started | A Cloud/AI Enabled Database\n  by Arman Chand\n\n\nVideo guide on What is MindsDB and How to Get Started by\n  Hritik Dangi\n\n\nVideo guide on What is MindsDB? uploaded on\n  ExploringTech by\n  Rutam Prita Mishra\n\n\nVideo guide on What is MindsDB - AI Database Prediction\n  by Bhavesh Mishra\n\n\nVideo guide on What is MindsDB ? by\n  Syed Zubeen\n\n",
    "tag": "mindsdb"
  },
  {
    "title": "What is an ML Model?",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/model-types.mdx",
    "content": "\ntitle: ML Models in MindsDB\nIn this section, we go through the ML model types available in MindsDB. These are regression models, classification models, and time series models.\n\nDisclaimer\nIn this section, we describe the default behavior using the Lightwood ML engine. Other ML handlers may behave differently. For example, some may not perform validation automatically when creating a model, as numerous behaviors are handler-specific.\n\nWhat is an ML Model?\nA machine learning (ML) model is a program trained using the available data in order to learn how to recognize patterns and behaviors to predict future data. There are various types of ML models that use different learning paradigms, but MindsDB models are all supervised because they learn from pairs of input data and expected output.\nYou input data into an ML model. It processes this input data, searching for patterns and correlations. After that, the ML model returns output data defined based on the input data.\nFeatures\nFeatures are variables that an ML model uses as input data to search for patterns and predict the target variable. In tabular datasets, features usually correspond to single columns.\nTarget\nThe target is a variable of interest that an ML model predicts based on the information fetched out of the features.\nTraining Dataset\nThe training dataset is used during the training phase of an ML model. It contains both feature variables and a target variable. As its name indicates, it is used to train an ML model.\nThe ML model takes the entire training dataset as input. It learns the patterns and relationships between feature variables and target values.\nOnce the training process is complete, one can move on to the validation phase.\nValidation Dataset\nThe validation dataset is used during the validation phase of an ML model. It contains both feature variables and a target variable, like the training dataset. But as its name indicates, it is used to validate the predictions made by an ML model. It has no overlap with the training dataset, as it is a held-out set to simulate a real scenario where the model generates predictions for novel input data.\nThe ML model takes only the feature variables from the validation dataset as input. Based on what the model learns during the training process, it makes predictions for the values of a target variable.\nNow comes the validation step. To assess the accuracy of the ML model, one compares the target variable values from the validation dataset with the target variable values predicted by the ML model. The closer these values are to each other, the better accuracy of the ML model.\nInput Dataset\nAfter completing the training and validation phases, one can provide the input dataset consisting of only the feature variables to predict the target variable values.\nHow is an ML Model Created?\nIn MindsDB, we use the CREATE MODEL statement to create, train, and validate a model.\nTraining Phase\nLet's look at our training dataset. It contains both features and a target.\n`sql\nSELECT *\nFROM files.salary_dataset\nLIMIT 5;`\nOn execution, we get:\n`sql\n+---------+--------------+-----------+---------+--------+---------------+-------------------+------+\n|companyId|jobType       |degree     |major    |industry|yearsExperience|milesFromMetropolis|salary|\n+---------+--------------+-----------+---------+--------+---------------+-------------------+------+\n|COMP37   |CFO           |MASTERS    |MATH     |HEALTH  |10             |83                 |130   |\n|COMP19   |CEO           |HIGH_SCHOOL|NONE     |WEB     |3              |73                 |101   |\n|COMP52   |VICE_PRESIDENT|DOCTORAL   |PHYSICS  |HEALTH  |10             |38                 |137   |\n|COMP38   |MANAGER       |DOCTORAL   |CHEMISTRY|AUTO    |8              |17                 |142   |\n|COMP7    |VICE_PRESIDENT|BACHELORS  |PHYSICS  |FINANCE |8              |16                 |163   |\n+---------+--------------+-----------+---------+--------+---------------+-------------------+------+`\nHere, the features are `companyId`, `jobType`, `degree`, `major`, `industry`, `yearsExperience`, and `milesFromMetropolis`.\nAnd the target variable is `salary`.\nLet's create and train an ML model using this training dataset.\n`sql\nCREATE MODEL salary_predictor\nFROM files\n    (SELECT * FROM salary_dataset)\nPREDICT salary;`\nOn execution, we get:\n`sql\nQuery successfully completed`\nProgress\nHere is how to check whether the training process is completed:\n`sql\nSELECT *\nFROM mindsdb.models\nWHERE name = 'salary_predictor';`\nOnce the status is `complete`, the training phase is completed.\nValidation Phase\nBy default, the `CREATE MODEL` statement performs validation of the model.\nAdditionally, we can validate the model manually by querying it and providing the feature values in the `WHERE` clause like this:\n`sql\nSELECT salary, salary_explain\nFROM mindsdb.salary_predictor\nWHERE companyId = 'COMP37'\nAND jobType = 'MANAGER'\nAND degree = 'DOCTORAL'\nAND major = 'MATH'\nAND industry = 'FINANCE'\nAND yearsExperience = 5\nAND milesFromMetropolis = 50;`\nOn execution, we get:\n`sql\n+------+------------------------------------------------------------------------------------------------------------------------------------------+\n|salary|salary_explain                                                                                                                            |\n+------+------------------------------------------------------------------------------------------------------------------------------------------+\n|128   |{\"predicted_value\": 128, \"confidence\": 0.67, \"anomaly\": null, \"truth\": null, \"confidence_lower_bound\": 109, \"confidence_upper_bound\": 147}|\n+------+------------------------------------------------------------------------------------------------------------------------------------------+`\nBy comparing the real salary values for the defined individuals and the predicted salary values, one can figure out the accuracy of the ML model.\n\nPlease note that MindsDB calculates the model's accuracy by default while running the `CREATE MODEL` statement. However, it is not guaranteed that all ML engines do this.\nBy default, the `CREATE MODEL` statement does the following:\n\nit creates a model,\nit divides the input data into training and validation datasets,\nit trains a model using the training dataset,\nit validates a model using the validation dataset,\nit compares the true and predicted values of a target to define the model's accuracy.\n\n\nLet's look at the basic types of ML models.\nML Model Types\nRegression Models\nRegression is a type of predictive modeling that analyses input data, including relationships between dependent and independent variables and the target variable that is to be predicted.\nIn the case of regression models, the target variable belongs to a set of continuous values. For example, having data on real estates, such as the number of rooms, location, and rental price, one can predict the rental price using regression. The rental price is predicted based on the input data, and its value is any value from a range between minimum and maximum rental price values from the training data.\nExample\nFirst, let's look at our input data.\n`sql\nSELECT *\nFROM example_db.demo_data.home_rentals\nLIMIT 5;`\nOn execution, we get:\n`sql\n+---------------+-------------------+----+--------+--------------+--------------+------------+\n|number_of_rooms|number_of_bathrooms|sqft|location|days_on_market|neighborhood  |rental_price|\n+---------------+-------------------+----+--------+--------------+--------------+------------+\n|2              |1                  |917 |great   |13            |berkeley_hills|3901        |\n|0              |1                  |194 |great   |10            |berkeley_hills|2042        |\n|1              |1                  |543 |poor    |18            |westbrae      |1871        |\n|2              |1                  |503 |good    |10            |downtown      |3026        |\n|3              |2                  |1066|good    |13            |thowsand_oaks |4774        |\n+---------------+-------------------+----+--------+--------------+--------------+------------+`\nHere, the features are `number_of_rooms`, `number_of_bathrooms`, `sqft`, `location`, `days_on_market`, and `neighborhood`.\nAnd the target variable is `rental_price`.\nLet's create and train an ML model.\n`sql\nCREATE MODEL mindsdb.home_rentals_model\nFROM example_db\n  (SELECT * FROM demo_data.home_rentals)\nPREDICT rental_price;`\nOn execution, we get:\n`sql\nQuery successfully completed`\nOnce the training process is completed, we can query for predictions.\n`sql\nSELECT rental_price, rental_price_explain\nFROM mindsdb.home_rentals_model\nWHERE sqft = 823\nAND location='good'\nAND neighborhood='downtown'\nAND days_on_market=10;`\nOn execution, we get:\n`sql\n+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------+\n| rental_price | rental_price_explain                                                                                                                          |\n+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------+\n| 4394         | {\"predicted_value\": 4394, \"confidence\": 0.99, \"anomaly\": null, \"truth\": null, \"confidence_lower_bound\": 4313, \"confidence_upper_bound\": 4475} |\n+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------+`\nFor details, check out this tutorial.\nClassification Models\nClassification is a type of predictive modeling that analyses input data, including relationships between dependent and independent variables and the target variable that is to be predicted.\nIn the case of classification models, the target variable belongs to a set of discrete values. For example, having data on each customer of a telecom company, one can predict the churn possibility using classification. The churn is predicted based on the input data, and its value is either `Yes` or `No`. This is a special case called binary classification.\nExample\nFirst, let's look at our input data.\n`sql\nSELECT *\nFROM example_db.demo_data.customer_churn\nLIMIT 5;`\nOn execution, we get:\n`sql\n+----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+-------------------------+--------------+------------+------+\n|customerid|gender|seniorcitizen|partner|dependents|tenure|phoneservice|multiplelines   |internetservice|onlinesecurity|onlinebackup|deviceprotection|techsupport|streamingtv|streamingmovies|contract      |paperlessbilling|paymentmethod            |monthlycharges|totalcharges|churn |\n+----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+-------------------------+--------------+------------+------+\n|7590-VHVEG|Female|0            |Yes    |No        |1     |No          |No phone service|DSL            |No            |Yes         |No              |No         |No         |No             |Month-to-month|Yes             |Electronic check         |$29.85        |$29.85      |No    |\n|5575-GNVDE|Male  |0            |No     |No        |34    |Yes         |No              |DSL            |Yes           |No          |Yes             |No         |No         |No             |One year      |No              |Mailed check             |$56.95        |$1,889.50   |No    |\n|3668-QPYBK|Male  |0            |No     |No        |2     |Yes         |No              |DSL            |Yes           |Yes         |No              |No         |No         |No             |Month-to-month|Yes             |Mailed check             |$53.85        |$108.15     |Yes   |\n|7795-CFOCW|Male  |0            |No     |No        |45    |No          |No phone service|DSL            |Yes           |No          |Yes             |Yes        |No         |No             |One year      |No              |Bank transfer (automatic)|$42.30        |$1,840.75   |No    |\n|9237-HQITU|Female|0            |No     |No        |2     |Yes         |No              |Fiber optic    |No            |No          |No              |No         |No         |No             |Month-to-month|Yes             |Electronic check         |$70.70        |$151.65     |Yes   |\n+----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+-------------------------+--------------+------------+------+`\nHere, the features are `customerid`, `gender`, `seniorcitizen`, `partner`, `dependents`, `tenure`, `phoneservice`, `multiplelines`, `internetservice`, `onlinesecurity`, `onlinebackup`, `deviceprotection`, `techsupport`, `streamingtv`, `streamingmovies`, `contract`, `paperlessbilling`, `paymentmethod`, `monthlycharges`, and `totalcharges`.\nAnd the target variable is `churn`.\nLet's create and train an ML model.\n`sql\nCREATE MODEL churn_predictor\nFROM example_db\n    (SELECT * FROM demo_data.customer_churn)\nPREDICT churn;`\nOn execution, we get:\n`sql\nQuery successfully completed`\nOnce the training process is completed, we can query for predictions.\n`sql\nSELECT churn, churn_confidence, churn_explain\nFROM mindsdb.customer_churn_predictor\nWHERE seniorcitizen=0\nAND partner='Yes'\nAND dependents='No'\nAND tenure=1\nAND phoneservice='No'\nAND multiplelines='No phone service'\nAND internetservice='DSL';`\nOn execution, we get:\n`sql\n+-------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Churn | Churn_confidence    | Churn_explain                                                                                                                                                    |\n+-------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Yes   | 0.7752808988764045  | {\"predicted_value\": \"Yes\", \"confidence\": 0.7752808988764045, \"anomaly\": null, \"truth\": null, \"probability_class_No\": 0.4756, \"probability_class_Yes\": 0.5244}    |\n+-------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+`\nFor details, check out this tutorial.\nTime Series Models\nTime series models fall under the regression or classification category. But what's distinct about them is that we order data by date, time, or any value defining sequential order of events. Usually, predictions made by time series models are referred to as forecasts.\nA time series model predicts a target that comes from a continuous set (regression) or a discrete set (classification).\nThere is a mandatory `ORDER BY` clause followed by a sequential column, such as a date. It orders all the rows accordingly.\nIf you want to group your predictions, there is an optional `GROUP BY` clause. By following this clause with a column name, or multiple column names, one can make predictions for partitions of data defined by these columns.\nIn the case of time series models, one should define how many data rows are used to train the model. The `WINDOW` clause followed by an integer does just that.\nThere is an optional `HORIZON` clause where you can define how many rows, or how far into the future, you want to predict. By default, it is one.\nExample\nFirst, let's look at our input data.\n`sql\nSELECT *\nFROM example_db.demo_data.house_sales\nLIMIT 5;`\nOn execution, we get:\n`sql\n+----------+------+-----+--------+\n|saledate  |ma    |type |bedrooms|\n+----------+------+-----+--------+\n|2007-09-30|441854|house|2       |\n|2007-12-31|441854|house|2       |\n|2008-03-31|441854|house|2       |\n|2008-06-30|441854|house|2       |\n|2008-09-30|451583|house|2       |\n+----------+------+-----+--------+`\nHere, the features are `saledate`, `type`, and `bedrooms`.\nAnd the target variable is `ma`.\nLet's create and train an ML model.\n`sql\nCREATE MODEL mindsdb.house_sales_predictor\nFROM files\n  (SELECT * FROM house_sales)\nPREDICT MA\nORDER BY saledate\nGROUP BY bedrooms, type\n-- the target column to be predicted stores one row per quarter\nWINDOW 8      -- using data from the last two years to make forecasts (last 8 rows)\nHORIZON 4;    -- making forecasts for the next year (next 4 rows)`\nOn execution, we get:\n`sql\nQuery successfully completed`\nOnce the training process is completed, we can query for predictions.\n`sql\nSELECT m.saledate AS date, m.MA AS forecast, MA_explain\nFROM mindsdb.house_sales_predictor AS m\nJOIN files.house_sales AS t\nWHERE t.saledate > LATEST\nAND t.type = 'house'\nAND t.bedrooms = 2\nLIMIT 4;`\nOn execution, we get:\n`sql\n+-------------+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| date        | forecast          | MA_explain                                                                                                                                                                                    |\n+-------------+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| 2019-12-31  | 441413.5849598734 | {\"predicted_value\": 441413.5849598734, \"confidence\": 0.99, \"anomaly\": true, \"truth\": null, \"confidence_lower_bound\": 440046.28237074096, \"confidence_upper_bound\": 442780.88754900586}        |\n| 2020-04-01  | 443292.5194586229 | {\"predicted_value\": 443292.5194586229, \"confidence\": 0.9991, \"anomaly\": null, \"truth\": null, \"confidence_lower_bound\": 427609.3325864327, \"confidence_upper_bound\": 458975.7063308131}        |\n| 2020-07-02  | 443292.5194585953 | {\"predicted_value\": 443292.5194585953, \"confidence\": 0.9991, \"anomaly\": null, \"truth\": null, \"confidence_lower_bound\": 424501.59192981094, \"confidence_upper_bound\": 462083.4469873797}       |\n| 2020-10-02  | 443292.5194585953 | {\"predicted_value\": 443292.5194585953, \"confidence\": 0.9991, \"anomaly\": null, \"truth\": null, \"confidence_lower_bound\": 424501.59192981094, \"confidence_upper_bound\": 462083.4469873797}       |\n+-------------+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+`\nFor details, check out this tutorial.\nHow it Works in the Background\nMindsDB uses the Lightwood ML engine by default. This section takes a closer look at how this package automatically chooses what type of model to use.\nModels in Lightwood follow an encoder-mixer-decoder pattern, where refined, or encoded, representations of all features are mixed to produce target predictions. Here are the mixers used by Lightwood.\nPlease note that there is an ensembling step after training all mixers in case multiple mixers are used. Read on to learn more.\nTo give you some details on how MindsDB creates a model using different mixers, here is the full code.\nAnd here comes the breakdown:\n- This piece of code adds mixers to the `submodels` array depending on the model type and the data type of the target variable.\n- And here, we choose the best of submodels to be used to create, train, and validate our ML model.\nLet's dive into the details of how MindsDB picks the mixers.\n\n\nHere is the piece of code being analyzed.\nIf we deal with a simple encoder/decoder pair performing the task, we use the Unit mixer that can be thought of as a bypass mixer.\nA good example is the Spam Classifier model of Hugging Face because it uses a single column as input.\nOtherwise, we choose from a range of other mixers depending on the following conditions:\n\n\nIf it is not a time series case, we use the Neural mixer.\nA good example is the Customer Churn model.\n\n\nIf it is a time series case, we use the NeuralTs mixer.\nA good example is the House Sales model.\n\n\n\n\nHere is the piece of code being analyzed.\nIf it is not a time series case, or it is a time series case with the HORIZON value being one, and the target variable type is not an array of values, we use the LightGBM, XGBoostMixer, Regression, and RandomForest mixers.\nA good example is the Home Rentals model.\nOtherwise, if it is a time series case and the HORIZON value is greater than one, we use the LightGBMArray mixer.\nA good example is the House Sales model.\n\n\nHere is the piece of code being analyzed.\nIf it is an autoregressive case and the target type is an integer, float, or quantity, we use the SkTime, ETSMixer, and ARIMAMixer mixers.\nThe autoregressive case means that we use the target values (as encoded features) from as many previous rows as defined in the `WINDOW` clause.\nA good example is the Home Rentals model.\n\n\n\n\nMindsDB may use one or multiple mixers while preparing a model. Depending on the model type and the data type of the target variable, one mixer is chosen or a set of mixers are ensembled to create, train, and validate an ML model.\n\nThe three cases above describe how MindsDB chooses the mixer candidates and stores them in the `submodels` array.\nBy default, after training all relevant mixers in the `submodels` array, MindsDB uses the BestOf ensemble to single out the best mixer as the final model.\nBut you can always use a different ensemble that may aggregate multiple mixers per model, such as the MeanEnsemble, ModeEnsemble, StackedEnsemble, TsStackedEnsemble, or WeightedMeanEnsemble ensemble type.\nHere, you'll find implementations of all ensemble types.\nWhat's Next?\nHave fun while trying out different ML models yourself!\n\nBookmark MindsDB repository on GitHub.\nSign up for a free MindsDB account.\nEngage with the MindsDB community on\n  Slack or\n  GitHub to ask questions and\n  share your ideas and thoughts.\n\nIf this material was helpful, please give us a GitHub star",
    "tag": "mindsdb"
  },
  {
    "title": "Community Video Guides",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/tutorials.mdx",
    "content": "\ntitle: Community Tutorials List\nsidebarTitle: Community Tutorials List\n\n\nDeploying MindsDB on Scaleway Cloud\n  by Rutam Prita Mishra\nPredict Tesla Stock Prices using MindsDB\n  by Rutam Prita Mishra\nCreating Views with MindsDB\n  by Rutam Prita Mishra\nPredicting Logistics Quality using MindsDB\n  by Arman Chand\nPredict Wine Quality using MindsDB\n  by Rutam Prita Mishra\nPredicting Employee Attrition & Performance using MindsDB Cloud\n  by Arman Chand\nDeploying MindsDB on Oracle Cloud\n  by Rutam Prita Mishra\nPredict Water Quality using MindsDB\n  by Rutam Prita Mishra\nPredicting Life Expectancy using MindsDB Cloud\n  by Arman Chand\nPredict Purchasing Power per Country using MindsDB\n  by Rutam Prita Mishra\nPredicting Data Science Job Salaries using MindsDB Cloud\n  by Rutam Prita Mishra\nPredicting Used Car Prices using MindsDB\n  by Rutam Prita Mishra\nDeploying MindsDB on a Digital Ocean Droplet\n  by Rutam Prita Mishra\nDeploying MindsDB on a Vultr Cloud Instance\n  by Rutam Prita Mishra\nDeploying MindsDB on Civo Compute\n  by Rutam Prita Mishra\nDeploying MindsDB on Google Cloud Platform\n  by Rutam Prita Mishra\nHow To Maintain ML-Analytics Cycle With Headless BI\n  by Jan Kadlec\nHow To Visualize MindsDB Predictions with Tableau\n  by Ephraimx\nPredicting Supermarket Future Sales Using Machine Learning with MindsDB\n  by Ephraimx\nPredicting Home Rental Prices with MindsDB in Python\n  by Temidayo\nMindsDB: Your Introduction to Creating Machine Learning Predictive Models\n  by Chandre Van Der Westhuizen\nSelf-Service Machine Learning with Intelligent Databases\n  by MindsDB Team\nHow To Forecast Purchase Orders for Shopify Stores Using Open-Source\n  by John Lafleur\nMindsDB: Your introduction to creating machine learning predictive models.\n  by Chandre Van Der Westhuizen\nPredicting Loan Default using Machine Learning with MindsDB\n  by Eliel Godsent\nMultivariate time series forecasts inside databases with MindsDB and PyTorch\n  by Patricio Cerda Mardini\nHow to bring your own machine learning model to databases\n  by Patricio Cerda Mardini\nIntroduction to AI Tables\n  by Javi S\u00e1nchez\nModel agnostic confidence estimation with conformal predictors for AutoML\n  by Patricio Cerda Mardini\nPredict insurance cost using MindsDB\n  by Kinie K Kusuma\nForecast bitcoin price using MindsDB\n  by Kinie K Kusuma\nPredicting car prices using MindsDB\n  by Marcel Coetzee\nPredict Gold Prices Using MindsDB\n  by Alissa Troiano\nMindsDB: Predicting Supply Chain Demand with Machine Learning using SQL\n  by Chandre Van Der Westhuizen\nPredicting Poker Hand Strength with MindsDB\n  by Teslim Odumuyiwa\nWhy Your Database Needs a Machine Learning Brain\n  by James Wilson\nMachine Learning for a Shopify store \u2014 a step-by-step guide\n  by James Wilson\nPredicting Home Rental Prices with MindsDB in Java\n  by Temidayo\nPredicting Mobile Phone Price using MindsDB Cloud\n  by Arman Chand\nPredicting Machine Failure Rates using MindsDB\n  by Rutam Prita Mishra\nPredicting Turbine Yield Energy using MindsDB\n  by DhaneshRagu\nPredicting Store Sales Data using MindsDB\n  by Kevin Heng\nUsing MindsDB to Predict Future Retail Sales\n  by Alissa Troiano\nPredicting the Rating of Cars using Mindsdb\n  by Atharva Shirdhankar\nPredicting & Visualizing Hourly Electricity Demand in the US with MindsDB and Tableau\n  by Teslim Odumuyiwa\nBank Customer Churn Prediction using MindsDB\n  by Ayush Sharma\nLet's connect Mindsdb with Mariadb shell Locally and Predict the Mobile Price\n  by Atharva Shirdhankar\nGold price prediction using MindsDB\n  by Pritish Sinha\nWhat is MindsDB?\n  by Gloria Okeke E.J\nForecasting Honey Production with MindsDB\n  by Adithya Narayan\nPredicting the Genre of Books using MongoDB with MindsDB\n  by Sarvesh S. Kulkarni\nPredicting gold prices with MindsDB and MongoDB\n  by yagueto\nPredicting the Genre of Books using MongoDB with MindsDB\n  by Sarvesh S. Kulkarni\nPredict Diamond prices with SQL Alchemy\n  by Teslim Odumuyiwa\nPredict Municipal Debt Risk Analysis\n  by Teslim Odumuyiwa\nWhat is MindsDB? How to get started with it \n  by Hritik Dangi\nHow to Design a custom mixer model in Lightwood\n  by Teslim Odumuyiwa\nPredicting & Visualizing Petroleum Production with MindsDB and Tableau\n  by Teslim Odumuyiwa\nPredicting & Visualizing Gas Prices with MindsDB and Tableau\n  by Teslim Odumuyiwa\nPredicting Environment Impact of Food Production caused by C02 Emission\n  by Teslim Odumuyiwa\nUsing MindsDB to Predict Amazon Rainforest Degradation\n  by Alissa Troiano\nHow to Design a custom mixer model in Lightwood\n  by Teslim Odumuyiwa\nUsing MindsDB to Predict Microsoft Stock Prices \n  by Vivek Shah\nTutorial to Predict the Weather Using MindsDB and MongoDB\n  by Nupoor Shetye\nTutorial to Predict Amsterdam Housing Prices Using MindsDB and MongoDB\n  by Nupoor Shetye\nTutorial to Predict the Energy Usage using MindsDB and MongoDB\n  by Salim Dohri\nTutorial to Predict the Type of Glass using MongoDB\n  by Salim Dohri\n\nCommunity Video Guides\n\nHeart Disease Predictor using #MindsDB by\n  @akhilcoder\nVideo: What is MindsDB? by\n  Alissa Troiano\nConnecting MindsDB to Tableau\n  by Alissa Troiano\nConnect Postgres Database to MindsDB by\n  @akhilcoder\nExploring Learning Hub on MindsDB by predicting house rental prices and forecasting house sales\n  by @akhilcoder\nEasily connect to MindsDB Cloud from MongoShell\n  by @akhilcoder\nSinging Up with MindsDB Cloud uploaded on\n  ExploringTech by\n  Rutam Prita Mishra\nHow to connect to MindsDB with SQL Alchemy(Python)\n  by Nishant Sapkota\nWhat is MindsDB | How to Get Started | A Cloud/AI Enabled Database\n  by Arman Chand\nHow to deploy MindsDB on your local machine (Docker, PIP)\n  by Munyoudoum Pav\nWhat are AI Tables and How to use MindsDB ? by\n  @akhilcoder\nWhat is MindsDB and How to Get Started by\n  Hritik Dangi\nSetup MindsDB Free Cloud Account in 2 minutes\n  by @akhilcoder\nWhat is MindsDB? uploaded on\n  ExploringTech by\n  Rutam Prita Mishra\nTutorial: Create a Free MindsDB Cloud Account\n  by Alissa Troiano\nWhat is MindsDB - AI Database Prediction\n  by Bhavesh Mishra\nHow to create an account on MindsDB cloud by\n  HellFire\nSetting Up Docker for MindsDB\n  by Alissa Troiano\nHow to create a free account on MindsDB Cloud\n  by Anamika\nHow to create a free account on MindsDB Cloud\n  by MichaelLantz\nSimple starting guide video tutorial for MindsDB\n  by Posterizedsoul\nHow to connect mongo compass to MindsDB\n  by HellFire\nMindsDB - Predict data with machine learning\n  by HellFire\nVisualizing prediction result in Tableau by\n  Teslim Odumuyiwa\nMindsDB Data Insights by\n  @akhilcoder\nCREATE Predictor with USING statement using Cement Manufacturing Dataset Civil Engineering\n  by Teslim Odumuyiwa\nWhat is MindsDB ? by\n  Syed Zubeen\nHoney Production Prediction Time Series by\n  Adithya Narayan\nForecasting Quarterly House Sales with MindsDB\n  by Hritik Dangi\nForecasting Quarterly House Sales with MindsDB\n  by Adithya Narayan\nHow to access MindsDB by\n  Syed Zubeen\nPredict house prices with MindsDB\n  by HellFire\nDeploying an instance of MindsDB on Google Cloud Platform\n  by Syed Zubeen\nConnecting MindsDB to Tableau\n  by Alissa Troiano\nIntegrating MindsDB into MongoDB\n  by Syed Zubeen\nDeploy MindsDB on the Google Cloud Platform\n  by Alissa Troiano\nRegression Tutorial - Home Rentals by\n  Teslim Odumuyiwa\nSetting up data sources with mindsDB\n  by Syed Zubeen\nIntegrating your MindsDB instance into MongoDB\n  by Syed Zubeen\nPredicting car prices with MindsDB\n  by Syed Zubeen\nForcasting quarterly house sales with MindsDB\n  by HellFire\nHow Mindsdb is impacting the world of Machine Learning\n",
    "tag": "mindsdb"
  },
  {
    "title": "getting-started.mdx",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/getting-started.mdx",
    "content": "\ntitle: Getting Started\nMindsDB can be integrated with the most popular databases, as well as with the DBT and MLflow workflows.\nTo try out MindsDB right away without bringing in your own data or models, follow our Quickstart guide.\n1. Create a MindsDB Cloud Account or Install MindsDB Locally\n\n\n    Create your free MindsDB Cloud account.\n  \n\n    To get started with a Docker installation, follow the MindsDB installation\n    instructions using Docker.\n  \n\n    You can also install MindsDB using pip on\n    Windows,\n    Mac, and Linux\n\n\n2. Connect to MindsDB from a SQL Client\n\n  If you do not have a preferred SQL client yet, we recommend using the MindsDB\n  SQL Editor or DBeaver Community\n  Edition. Follow this guide to\n  set up your MindsDB SQL Editor. And here, you'll find how\n  to connect to MindsDB from DBeaver.\n\n\n\n        By default, on MindsDB Cloud the SQL Editor is already connected. Skip to step 3\n    \n\n        a. Create a new MySQL connection.\n\n\n```        ![DBeaver Create Connection](/assets/dbeaver-create-connection.png)\n\n    b. Configure it using the parameters below, as well as your username and password.\n\n        ```\n        Host: `cloud.mindsdb.com`\n        Port: `3306`\n        Database: `mindsdb`\n        ```\n\n        ![DBeaver Configure Connection](/assets/dbeaver-configure-cloud-connection.png)\n</Tab>\n<Tab title=\"Local to Dbeaver\">\n    a. Create a new MySQL connection.\n\n        ![DBeaver Create Connection](/assets/dbeaver-create-connection.png)\n\n    b. Configure it using the following parameters:\n\n        ```\n        Host: `localhost`\n        Port: `47335`\n        Database: `mindsdb`\n        Username: `mindsdb`\n        Password: <leave it empty>\n        ```\n\n        ![DBeaver Configure Connection](/assets/dbeaver-configure-docker-connection.png)\n</Tab>\n```\n\n\n\n3. Connect your Data to MindsDB Using CREATE DATABASE\n`sql\nCREATE DATABASE example_data\nWITH ENGINE = \"postgres\",\nPARAMETERS = {\n  \"user\": \"demo_user\",\n  \"password\": \"demo_password\",\n  \"host\": \"3.220.66.106\",\n  \"port\": \"5432\",\n  \"database\": \"demo\"\n};`\n4. Preview the Available Data Using SELECT\n`sql\nSELECT *\nFROM example_data.demo_data.home_rentals\nLIMIT 10;`\n5. Create a Model Using CREATE MODEL\nIf you already have a model in MLFlow, you can connect to your model.\n\n\n`sql\n        CREATE MODEL mindsdb.home_rentals_predictor\n        FROM example_data\n            (SELECT * FROM demo_data.home_rentals)\n        PREDICT rental_price;`\n\n\n`sql\n        CREATE MODEL mindsdb.home_rentals_predictor\n            FROM example_data (select * from demo_data.home_rentals)\n            PREDICT rental_price\n            USING url.predict='http://host.docker.internal:1234/invocations',\n            format='mlflow',\n            dtype_dict={\"number_of_rooms\": \"categorical\", \"number_of_bathrooms\": \"categorical\", \"sqft\": \"integer\", \"days_on_market\": \"integer\",\n                        \"initial_price\": \"integer\", \"location\": \"categorical\", \"neighborhood\":\"categorical\" };`\n\n\n6. Make Predictions Using SELECT\n`sql\nSELECT rental_price\nFROM mindsdb.home_rentals_predictor\nWHERE number_of_bathrooms = 2\nAND sqft = 1000;`\nOn execution, we get:\n`sql\n+--------------+\n| rental_price |\n+--------------+\n| 1130         |\n+--------------+`\n7. Integrate your Predictions into the DBT Workflow\nTo do so, you need to make the following changes:\n\n`yml profiles.yml\nmindsdb:\n    type: mysql\n    host: mysql.mindsdb.com\n    user: mindsdb.user@example.com\n    password: mindsdbpassword\n    port: 3306\n    dbname: mindsdb\n    schema: example_data\n    threads: 1\n    keepalives_idle: 0 # default 0, indicating the system default\n    connect_timeout: 10 # default 10 seconds`\n```yml schema.yml\nversion: 2\nmodels:\n    - name: predicted_rentals\n    description: \"Integrating MindsDB predictions and historical   data\"\n```\n`sql predicted_rentals.sql\nwith predictions as (\n    SELECT hrp.rental_price as predicted_price, hr.rental_price as actual_price\n    FROM mindsdb.home_rentals_predictor hrp\n    JOIN exampleData.demo_data.home_rentals hr\n    WHERE hr.number_of_bathrooms=2 AND hr.sqft=1000;\n)\nselect * from predictions;`\n`yml dbt_project.yml\nmodels:\n  home_rentals:\n    +materialized: view`",
    "tag": "mindsdb"
  },
  {
    "title": "Syntax for SQL commands",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/docs-rules.mdx",
    "content": "\ntitle: Style Guide for MindsDB Documentation\nsidebarTitle: Docs Style Guide\n\nSyntax for SQL commands\nFollow the rules below when writing an SQL command.\n\nAdd a semi-colon `;` at the end of each SQL command.\nUse all-caps when writing the keywords, such as `SELECT`, `FROM`, `JOIN`,\n  `WHERE`, `GROUP BY`, `ORDER BY`, `PREDICT`, `AS`, `CREATE TABLE`,\n  `INSERT INTO`, etc.\nWhen writing a query, start a new line for the following keywords: `SELECT`,\n  `FROM`, `JOIN`, `WHERE`, `GROUP BY`, `ORDER BY`, `PREDICT`, `USING`, `AND`,\n  `OR`. It is to avoid the horizontal scrollbar.\n\nExample\n`sql\nSELECT *\nFROM table_name_1 a\nJOIN table_name_2 b\nWHERE column_name_1=value_name_1\nAND column_name_2=value_name_2\nGROUP BY a.column_name_2\nORDER BY b.column_name_1;`\nSyntax for SQL commands along with their output\nFollow the syntax below when documenting an SQL command and its output.\n\n\n``````sql\nQUERY GOES HERE\n```\n\nOn execution, we get:\n\n```sql\n+-------------+-------------+\n| column_name | column_name |\n+-------------+-------------+\n| value       | value       |\n+-------------+-------------+\n```\n\nWhere:\n\n| Name                                | Description                         |\n| ----------------------------------- | ----------------------------------- |\n| `VARIABLE NAME GOES HERE`           | VARIABLE DESCRIPTION GOES HERE      |\n```\n\n\n\n  If the output is not a table, remove the output table from above and place\n  your output message there.\n\nExample 1\n\n\n``````sql\nSELECT *\nFROM table_name_1 a\nJOIN table_name_2 b\nWHERE column_name=value_name;\n```\n\nOn execution, we get:\n\n```sql\n+-------------+-------------+\n| column_name | column_name |\n+-------------+-------------+\n| value       | value       |\n+-------------+-------------+\n```\n\nWhere:\n\n| Name                                | Description                 |\n| ----------------------------------- | --------------------------- |\n| `column_name`                       | column description          |\n```\n\n\nOutput of Example 1\n`sql\nSELECT *\nFROM table_name_1 a\nJOIN table_name_2 b\nWHERE column_name=value_name;`\nOn execution, we get:\n`sql\n+-------------+-------------+\n| column_name | column_name |\n+-------------+-------------+\n| value       | value       |\n+-------------+-------------+`\nWhere:\n| Name          | Description        |\n| ------------- | ------------------ |\n| `column_name` | column description |\nExample 2\n\n\n``````sql\nCREATE MODEL mindsdb.predictor_name\nFROM integration_name\n    (SELECT column_name_1, column_name_2, target_column FROM table_name)\nPREDICT target_column;\n```\n\nOn execution, we get:\n\n```sql\nOUTPUT GOES HERE\n```\n```\n\n\nOutput of Example 2\n`sql\nCREATE MODEL mindsdb.predictor_name\nFROM integration_name\n    (SELECT column_name_1, column_name_2, target_column FROM table_name)\nPREDICT target_column;`\nOn execution, we get:\n```sql\nOUTPUT GOES HERE",
    "tag": "mindsdb"
  },
  {
    "title": "1. Create a MindsDB Cloud Account or Install MindsDB Locally",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/quickstart.mdx",
    "content": "\ntitle: Quickstart\nsidebarTitle: Try It Out!\n\nFollow the steps below to start making data forecasts with MindsDB using standard SQL.\nCheck out our Getting Started guide to set up and work with\nMindsDB using your own data and models.\n1. Create a MindsDB Cloud Account or Install MindsDB Locally\nCreate your free MindsDB Cloud account to\nstart practicing right away using the MindsDB Cloud Editor.\nIf you prefer a local MindsDB installation, follow the Deployment guides of\nMindsDB documentation. You can install MindsDB using\nDocker or follow the standard installation using\npip.\n2. Connect to MindsDB from a SQL Client\nYou can use the MindsDB Cloud Editor or open your preferred SQL client, such\nas DBeaver or MySQL CLI, and connect to MindsDB.\n\n\n    Log in to your MindsDB Cloud account. The\n    Editor is the first thing you'll see!\n  \n\n    To connect to MindsDB from a third-party SQL client, use the connection details\n    below.\n\n\n``````\nUser: your_mindsdb_cloud_username\nPassword: your_mindsdb_cloud_password\nHost: `cloud.mindsdb.com`\nPort: `3306`\n```\n\n<Tip>\n  If you do not have a preferred SQL client yet, we recommend using the [MindsDB\n  SQL Editor](https://cloud.mindsdb.com/editor) or [DBeaver Community\n  Edition](https://dbeaver.io/download/). Follow [this guide](/setup/cloud/) to\n  set up your MindsDB SQL Editor. And [here](/connect/dbeaver/), you'll find how\n  to connect to MindsDB from DBeaver.\n</Tip>\n```\n\n\n\n\n3. Connect a Database Using CREATE DATABASE\nWe have a sample database that you can use right away. To connect a database to your MindsDB Cloud account, use the CREATE DATABASE statement, as below.\n`sql\nCREATE DATABASE example_data\nWITH ENGINE = \"postgres\",\nPARAMETERS = {\n  \"user\": \"demo_user\",\n  \"password\": \"demo_password\",\n  \"host\": \"3.220.66.106\",\n  \"port\": \"5432\",\n  \"database\": \"demo\"\n};`\nOn execution, we get:\n`sql\nQuery OK, 0 rows affected (3.22 sec)`\n4. Preview the Available Data Using SELECT\nYou can now preview the available data with a standard `SELECT` statement.\n`sql\nSELECT *\nFROM example_data.demo_data.home_rentals\nLIMIT 10;`\nOn execution, we get:\n`sql\n+-----------------+---------------------+------+----------+----------------+---------------+--------------+--------------+\n| number_of_rooms | number_of_bathrooms | sqft | location | days_on_market | initial_price | neighborhood | rental_price |\n+-----------------+---------------------+------+----------+----------------+---------------+--------------+--------------+\n| 0.0             | 1.0                 | 484  | great    | 10             | 2271          | south_side   | 2271         |\n| 1.0             | 1.0                 | 674  | good     | 1              | 2167          | downtown     | 2167         |\n| 1.0             | 1.0                 | 554  | poor     | 19             | 1883          | westbrae     | 1883         |\n| 0.0             | 1.0                 | 529  | great    | 3              | 2431          | south_side   | 2431         |\n| 3.0             | 2.0                 | 1219 | great    | 3              | 5510          | south_side   | 5510         |\n| 1.0             | 1.0                 | 398  | great    | 11             | 2272          | south_side   | 2272         |\n| 3.0             | 2.0                 | 1190 | poor     | 58             | 4463          | westbrae     | 4124         |\n| 1.0             | 1.0                 | 730  | good     | 0              | 2224          | downtown     | 2224         |\n| 0.0             | 1.0                 | 298  | great    | 9              | 2104          | south_side   | 2104         |\n| 2.0             | 1.0                 | 878  | great    | 8              | 3861          | south_side   | 3861         |\n+-----------------+---------------------+------+----------+----------------+---------------+--------------+--------------+`\nYou could also browse the databases of MindsDB using the command below.\n`sql\nSHOW databases;`\nOn execution, we get:\n`sql\n+---------------------+\n| Database            |\n+---------------------+\n| information_schema  |\n| mindsdb             |\n| files               |\n| example_data        |\n+---------------------+`\nTo learn more about MindsDB tables structure, check out\nthis guide.\n5. Create a Model Using CREATE MODEL\nNow you are ready to create your first model. Use the\nCREATE MODEL statement, as below.\n`sql\nCREATE MODEL mindsdb.home_rentals_model\nFROM example_data\n  (SELECT * FROM demo_data.home_rentals)\nPREDICT rental_price;`\nOn execution, we get:\n`sql\nQuery OK, 0 rows affected (9.79 sec)`\n6. Check the Status of a Model\nIt may take a couple of minutes until the model is trained. You can monitor\nthe status of your model by executing the following command:\n`sql\nSELECT status\nFROM mindsdb.models\nWHERE name = 'home_rentals_model';`\nOn execution, we get:\n`sql\n+------------+\n| status     |\n+------------+\n| generating |\n+------------+`\nAfter a short time, we get:\n`sql\n+----------+\n| status   |\n+----------+\n| training |\n+----------+`\nAnd finally, we get:\n`sql\n+----------+\n| status   |\n+----------+\n| complete |\n+----------+`\nAlternatively, you can use the `SHOW MODELS` command as below.\n`sql\nSHOW MODELS\n[FROM project_name]\n[LIKE 'model_name']\n[WHERE column_name = value];`\nHere is an example:\n`sql\nSHOW MODELS\nFROM mindsdb\nLIKE 'home_rentals_model'\nWHERE status = 'complete';`\n\n  The status of the model must be `complete` before you can start making predictions.\n\n7. Make Predictions Using SELECT\nThe SELECT statement allows you to make predictions based\non features, where features are the input variables, or input columns, that are\nused to make forecasts.\nLet's predict what would be the rental price of a 1000 square feet house with\ntwo bathrooms.\n`sql\nSELECT rental_price\nFROM mindsdb.home_rentals_model\nWHERE number_of_bathrooms = 2\nAND sqft = 1000;`\nOn execution, we get:\n`sql\n+--------------+\n| rental_price |\n+--------------+\n| 1130         |\n+--------------+`\nHere is how to make batch predictions:\n`sql\nSELECT m.rental_price, m.rental_price_explain\nFROM mindsdb.home_rentals_model AS m\nJOIN example_data.demo_data.home_rentals AS d;`\n\nCongratulations! If you got this far, you have successfully trained a predictive model using SQL\nand got the future data!",
    "tag": "mindsdb"
  },
  {
    "title": "What are Handlers?",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/mindsdb-handlers.mdx",
    "content": "\ntitle: Handlers in MindsDB\nsidebarTitle: Handlers in MindsDB\n\nWhat are Handlers?\nAt the heart of the MindsDB philosophy lies the belief that predictive insights are best leveraged when produced as close as possible to the data layer. Usually, this data layer is a SQL-compatible database, but it could also be a non-SQL database, data stream, or any other tool that interacts with data stored somewhere else.\nThe above description fits an enormous set of tools used across the software industry. The complexity increases further by bringing Machine Learning into the equation, as the set of popular ML tools is similarly huge. We aim to support most technology stacks, requiring a simple integration procedure so that anyone can easily contribute the necessary glue to enable any predictive system for usage within data layers.\nThis motivates the concept of handlers, which is an abstraction for the two types of entities mentioned above: data layers and ML frameworks. Handlers are meant to enforce a common and sufficient set of behaviors that all MindsDB-compatible entities should support. By creating a handler, the target system is effectively integrated into the wider MindsDB ecosystem.\nTypes of Handlers\nDatabase Handlers\nDatabase handlers act as a bridge to any database. You use database handlers to create databases using the CREATE DATABASE command. So you can reach data from any database that has its handler implemented within MindsDB.\n\nGo ahead and implement a handler for the database of your choice! Here you'll find instructions on how to implement a database handler.\n\nMachine Learning (ML) Handlers\nML handlers act as a bridge to any ML framework. You use ML handlers to create ML engines using the CREATE ML_ENGINE command. So you can expose ML models from any supported ML engine as an AI table.\n\nGo ahead and implement a handler for the ML library or framework of your choice! Here you'll find instructions on how to implement an ML handler.\n\nHandlers in the MindsDB Repository\nThe source code for integrations is located in the main MindsDB repository under the /integrations directory.\n`integrations                      # Contains handlers' source codes\n\u251c\u2500 handlers/                      # Each handler has its own handler directory\n\u2502  \u251c\u2500 mysql_handler/              # MySQL integration code\n\u2502  \u251c\u2500 lightwood_handler/          # Lightwood integration code\n\u2502  \u251c\u2500  .../                       # Other handlers\n\u251c\u2500 handlers_client/               # Handler clients directory\n\u2502  \u251c\u2500 db_client/              \n\u2502  \u251c\u2500 ml_client/              \n\u251c\u2500 libs/                          # Handler libraries directory\n\u2502  \u251c\u2500 base.py                     # Each handler class inherits from one of the base classes\n\u2514\u2500 utilities                      # Handler utility directory\n\u2502  \u251c\u2500 install.py                  # Script that installs all handler dependencies`\nStructure of a Handler\nIn technical terms, a handler is a self-contained Python package having everything required for MindsDB to interact with it. It includes aspects like dependencies, unit tests, and continuous integration logic. It is up to the author to determine the nature of the package, for example, closed or open source, version control, and more. Although, we encourage opening pull requests to expand the default set of supported tools.\nThe entry point for a database handler is a class definition that should inherit directly from the mindsdb.integrations.libs.base.DatabaseHandler class and thus indirectly from the mindsdb.integrations.libs.base.BaseHandler class, which defines all the methods that must be overwritten in order to achieve a functional implementation.\nAnd the entry point for an ML handler is a class definition that should inherit from the mindsdb.integrations.libs.base.BaseMLEngine class, which defines all the methods that must be overwritten in order to achieve a functional implementation.\n\nAll other details of the handler's structure are not enforced, so the author can decide on its design.\n\nThings to Remember\nHere are a few points to keep in mind while implementing a handler.\nHandlers Inherit from Base Classes\nInherit from the DatabaseHandler class when adding a new database handler. For more info, visit our doc page here.\nInherit from the BaseMLEngine class when adding a new ML handler. For more info, visit our doc page here.\nParsing SQL\nWhenever you want to parse a string that contains SQL, we strongly recommend using the `mindsdb_sql` package. It provides a parser that fully supports the MindsDB SQL dialect and partially the standard SQL dialect. There is also a render feature to map other dialects into the already supported ones.\nFormatting Output\nIn the case of data handlers, when it comes to building the response of the public methods, the output should be wrapped by the mindsdb.integrations.libs.response.HandlerResponse or mindsdb.integrations.libs.response.HandlerStatusResponse class. These classes are used by the MindsDB executioner to orchestrate and coordinate multiple handler instances in parallel.",
    "tag": "mindsdb"
  },
  {
    "title": "Airtable",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/data-integrations/all-data-integrations.mdx",
    "content": "\ntitle: Supported Integrations\nsidebarTitle: Supported Integrations\n\nThe list of databases supported by MindsDB keeps growing. Here are the currently\nsupported integrations:\n\n\n\nYou can find particular\ndatabases' handler files here\nto see their connection arguments. For example, to see the latest updates to the\nOracle handler, check\nOracle's readme.md file here.\nLet's look at sample codes showing how to connect to each of the supported\nintegrations.\nAirtable\n\n\n`sql\n    CREATE DATABASE airtable_datasource          --- display name for the database\n    WITH ENGINE = 'airtable',                    --- name of the MindsDB handler\n    PARAMETERS = {\n      \"base_id\": \" \",                            --- the Airtable base ID\n      \"table_name\": \" \",                         --- the Airtable table name\n      \"api_key\": \" \"                             --- the API key for the Airtable API\n    };`\n\n\n`sql\n    CREATE DATABASE airtable_datasource\n    WITH ENGINE = 'airtable',\n    PARAMETERS = {\n      \"base_id\": \"appve10klsda2\",\n      \"table_name\": \"my_table\",\n      \"api_key\": \"KdJX2Q5km%5b$T$sQYm^gvN\"\n    };`\n\n\nAmazon Redshift\n\n\n`sql\n    CREATE DATABASE amazonredshift_datasource         --- display name for the database\n    WITH ENGINE = 'amazonredshift',                   --- name of the MindsDB handler\n    PARAMETERS = {\n      \"host\": \" \",                                    --- host name or IP address of the Redshift cluster\n      \"port\": ,                                       --- port used when connecting to the Redshift cluster\n      \"database\": \" \",                                --- database name used when connecting to the Redshift cluster\n      \"user\": \" \",                                    --- user to authenticate with the Redshift cluster\n      \"password\": \" \"                                 --- password used to authenticate with the Redshift cluster\n    };`\n\n\n`sql\n    CREATE DATABASE amazonredshift_datasource\n    WITH ENGINE = 'amazonredshift',\n    PARAMETERS = {\n      \"host\": \"127.0.0.1\",\n      \"port\": 5439,\n      \"database\": \"test\",\n      \"user\": \"amazonredshift\",\n      \"password\": \"password\"\n    };`\n\n\nAWS DynamoDB\n\n\n`sql\n    CREATE DATABASE dynamodb_datasource       --- display name for the database\n    WITH ENGINE = 'dynamodb',                 --- name of the MindsDB handler\n    PARAMETERS = {\n      \"aws_access_key_id\": \" \",               --- the AWS access key\n      \"aws_secret_access_key\": \" \",           --- the AWS secret access key\n      \"region_name\": \" \"                      --- the AWS region\n    };`\n\n\n`sql\n    CREATE DATABASE dynamodb_datasource\n    WITH ENGINE = 'dynamodb',\n    PARAMETERS = {\n      \"aws_access_key_id\": \"PCAQ2LJDOSWLNSQKOCPW\",\n      \"aws_secret_access_key\": \"U/VjewPlNopsDmmwItl34r2neyC6WhZpUiip57i\",\n      \"region_name\": \"us-east-1\"\n    };`\n\n\nCassandra\n\n\n`sql\n    CREATE DATABASE cassandra_datasource        --- display name for the database\n    WITH ENGINE = 'cassandra',                  --- name of the MindsDB handler\n    PARAMETERS = {\n      \"host\": \" \",                              --- host name or IP address\n      \"port\": ,                                 --- port used to make TCP/IP connection\n      \"user\": \" \",                              --- database user\n      \"password\": \" \",                          --- database password\n      \"keyspace\": \" \",                          --- database name\n      \"protocol_version\": ,                     --- optional, protocol version (defaults to 4 if left blank)\n      \"secure_connect_bundle\": {                --- optional, secure connect bundle file\n        \"path\": \" \"                                 --- either \"path\" or \"url\"\n      }\n    };`\n\n\n`sql\n    CREATE DATABASE cassandra_datasource\n    WITH ENGINE = 'cassandra',\n    PARAMETERS = {\n      \"host\": \"127.0.0.1\",\n      \"port\": 9043,\n      \"user\": \"user\",\n      \"password\": \"password\",\n      \"keyspace\": \"test_data\",\n      \"protocol_version\": 4\n    };`\n\n\nCkan\n\n\n`sql\n    CREATE DATABASE ckan_datasource          --- display name for the database\n    WITH ENGINE = 'ckan',                    --- name of the MindsDB handler\n    PARAMETERS = {\n      \"url\": \" \",                            --- host name, IP address, or a URL\n      \"apikey\": \" \"                          --- the API key used for authentication\n    };`\n\n\n`sql\n    CREATE DATABASE ckan_datasource\n    WITH ENGINE = 'ckan',\n    PARAMETERS = {\n      \"url\": \"http://demo.ckan.org/api/3/action/\",\n      \"apikey\": \"YOUR_API_KEY\"\n    };`\n\n\nClickHouse\n\n\n`sql\n    CREATE DATABASE clickhouse_datasource       --- display name for the database\n    WITH ENGINE = 'clickhouse',                 --- name of the MindsDB handler\n    PARAMETERS = {\n      \"host\": \" \",                              --- host name or IP address\n      \"port\": ,                                 --- port used to make TCP/IP connection\n      \"database\": \" \",                          --- database name\n      \"user\": \" \",                              --- database user\n      \"password\": \" \",                          --- database password\n      \"protocol\": \" \"                           --- optional, http or https (defaults to `native`)\n    };`\n\n\n`sql\n    CREATE DATABASE clickhouse_datasource\n    WITH ENGINE = 'clickhouse',\n    PARAMETERS = {\n      \"host\": \"127.0.0.1\",\n      \"port\": 9000,\n      \"database\": \"test_data\",\n      \"user\": \"root\",\n      \"password\": \"password\"\n    };`\n\n\nCockroach Labs\n\n\n`sql\n    CREATE DATABASE cockroach_datasource        --- display name for the database\n    WITH ENGINE = 'cockroachdb',                --- name of the MindsDB handler\n    PARAMETERS = {\n      \"host\": \" \",                              --- host name or IP address\n      \"port\": ,                                 --- port used to make TCP/IP connection\n      \"database\": \" \",                          --- database name\n      \"user\": \" \",                              --- database user\n      \"password\": \" \",                          --- database password\n      \"publish\": \" \"                            --- optional, publish\n    };`\n\n\n`sql\n    CREATE DATABASE cockroach_datasource\n    WITH ENGINE = 'cockroachdb',\n    PARAMETERS = {\n      \"host\": \"127.0.0.1\",\n      \"port\": 26257,\n      \"database\": \"cockroachdb\",\n      \"user\": \"username\",\n      \"password\": \"password\"\n    };`\n\n\nCouchbase\n\n\n`sql\n    CREATE DATABASE couchbase_datasource        --- display name for the database\n    WITH ENGINE = 'couchbase',                  --- name of the MindsDB handler\n    PARAMETERS = {\n      \"host\": \" \",                              --- host name or IP address of the Couchbase server\n      \"user\": \" \",                              --- user to authenticate with the Couchbase server\n      \"password\": \" \",                          --- password used to authenticate with the Couchbase server\n      \"bucket\": \" \",                            --- bucket name\n      \"scope\": \" \"                              --- scope used to query (defaults to `_default` if left blank)\n    };                                              --- a scope in Couchbase is equivalent to a schema in MySQL`\n\n\n`sql\n    CREATE DATABASE couchbase_datasource\n    WITH ENGINE = 'couchbase',\n    PARAMETERS = {\n      \"host\": \"127.0.0.1\",\n      \"user\": \"couchbase\",\n      \"password\": \"password\",\n      \"bucket\": \"test-bucket\"\n    };`\n\n\nCrateDB\n\n\n`sql\n    CREATE DATABASE cratedb_datasource        --- display name for the database\n    WITH ENGINE = 'crate',                    --- name of the MindsDB handler\n    PARAMETERS = {\n      \"host\": \" \",                            --- host name or IP address\n      \"port\": ,                               --- port used to make TCP/IP connection\n      \"user\": \" \",                            --- database user\n      \"password\": \" \",                        --- database password\n      \"schema_name\": \" \"                      --- database schema name (defaults to `doc` if left blank)\n    };`\n\n\n`sql\n    CREATE DATABASE cratedb_datasource\n    WITH ENGINE = 'crate',\n    PARAMETERS = {\n      \"host\": \"127.0.0.1\",\n      \"port\": 4200,\n      \"user\": \"crate\",\n      \"password\": \"password\",\n      \"schema_name\": \"doc\"\n    };`\n\n\nD0lt\n\n\n`sql\n    CREATE DATABASE d0lt_datasource             --- display name for the database\n    WITH ENGINE = 'd0lt',                       --- name of the MindsDB handler\n    PARAMETERS = {\n      \"host\": \" \",                              --- host name or IP address\n      \"port\": ,                                 --- port used to make TCP/IP connection\n      \"database\": \" \",                          --- database name\n      \"user\": \" \",                              --- database user\n      \"password\": \" \",                          --- database password\n      \"ssl\": ,                                  --- optional, the `ssl` parameter value indicates whether SSL is enabled (`True`) or disabled (`False`)\n      \"ssl_ca\": {                               --- optional, SSL Certificate Authority\n        \"path\": \" \"                                 --- either \"path\" or \"url\"\n      },\n      \"ssl_cert\": {                             --- optional, SSL certificates\n        \"url\": \" \"                                  --- either \"path\" or \"url\"\n      },\n      \"ssl_key\": {                              --- optional, SSL keys\n        \"path\": \" \"                                 --- either \"path\" or \"url\"\n      }\n    };`\n\n\n`sql\n    CREATE DATABASE d0lt_datasource\n    WITH ENGINE = 'd0lt',\n    PARAMETERS = {\n      \"host\": \"127.0.0.1\",\n      \"port\": 3306,\n      \"database\": \"information_schema\",\n      \"user\": \"root\",\n      \"password\": \"password\"\n    };`\n\n\nDatabend\n\n\n`sql\n    CREATE DATABASE databend_datasource     --- display name for the database\n    WITH ENGINE = 'databend',               --- name of the MindsDB handler\n    PARAMETERS = {\n      \"protocol\": \" \",                      --- protocol used to query Databend (defaults to `native` if left blank); supported protocols: native, http, https\n      \"user\": \" \",                          --- username used to authenticate with the Databend warehouse\n      \"port\": ,                             --- TCP/IP port of the Databend warehouse\n      \"password\": \" \",                      --- password used to authenticate with the Databend warehouse\n      \"host\": \" \",                          --- host name or IP address of the Databend warehouse (use '127.0.0.1' instead of 'localhost' when connecting to a local server)\n      \"database\": \" \"                       --- database name used when connecting to the Databend warehouse\n    };`\n\n\n`sql\n    CREATE DATABASE databend_datasource\n    WITH ENGINE = 'databend',\n    PARAMETERS = {\n      \"protocol\": \"native\",\n      \"user\": \"databend_user\",\n      \"port\": 443,\n      \"password\": \"password\",\n      \"host\": \"127.0.0.1\",\n      \"database\": \"databend_db\"\n    };`\n\n\nDatabricks\n\n\n`sql\n    CREATE DATABASE databricks_datasource         --- display name for the database\n    WITH ENGINE = 'databricks',                   --- name of the MindsDB handler\n    PARAMETERS = {\n      \"server_hostname\": \" \",                     --- server hostname of the cluster or SQL warehouse\n      \"http_path\": \" \",                           --- http path to the cluster or SQL warehouse\n      \"access_token\": \" \",                        --- personal Databricks access token\n      \"schema\": \" \",                              --- schema name (defaults to `default` if left blank)\n      \"session_configuration\": \" \",               --- optional, dictionary of Spark session configuration parameters\n      \"http_headers\": \" \",                        --- optional, additional (key, value) pairs to set in HTTP headers on every RPC request the client makes\n      \"catalog\": \" \"                              --- catalog (defaults to `hive_metastore` if left blank)\n    };`\n\n\n`sql\n    CREATE DATABASE databricks_datasource\n    WITH ENGINE = 'databricks',\n    PARAMETERS = {\n      \"server_hostname\": \"adb-1234567890123456.7.azuredatabricks.net\",\n      \"http_path\": \"sql/protocolv1/o/1234567890123456/1234-567890-test123\",\n      \"access_token\": \"dapi1234567890ab1cde2f3ab456c7d89efa\",\n      \"schema\": \"example_db\"\n    };`\n\n\nDatastax\n\n\n`sql\n    CREATE DATABASE datastax_datasource           --- display name for the database\n    WITH ENGINE = 'astra',                        --- name of the MindsDB handler\n    PARAMETERS = {\n      \"user\": \" \",                                --- user to be authenticated\n      \"password\": \" \",                            --- password for authentication\n      \"secure_connection_bundle\": {               --- secure connection bundle zip file\n        \"path\": \" \"                                   --- either \"path\" or \"url\"\n      },\n      \"host\": \" \",                                --- optional, host name or IP address\n      \"port\": ,                                   --- optional, port used to make TCP/IP connection\n      \"protocol_version\": ,                       --- optional, protocol version\n      \"keyspace\": \" \"                             --- optional, keyspace\n    };`\n\n\n`sql\n    CREATE DATABASE datastax_datasource\n    WITH ENGINE = 'astra',\n    PARAMETERS = {\n      \"host\": \"127.0.0.1\",\n      \"port\": 7077,\n      \"user\": \"datastax\",\n      \"password\": \"password\",\n      \"secure_connection_bundle\": {\n        \"path\": \"/home/Downloads/file.zip\"\n      }\n    };`\n\n\nDB2\n\n\n`sql\n    CREATE DATABASE db2_datasource        --- display name for the database\n    WITH ENGINE = 'DB2',                  --- name of the MindsDB handler\n    PARAMETERS = {\n      \"host\": \" \",                        --- host name or IP address\n      \"port\": ,                           --- port used to make TCP/IP connection\n      \"database\": \" \",                    --- database name\n      \"user\": \" \",                        --- database user\n      \"password\": \" \",                    --- database password\n      \"schema_name\": \" \"                  --- database schema name\n    };`\n\n\n`sql\n    CREATE DATABASE db2_datasource\n    WITH ENGINE = 'DB2',\n    PARAMETERS = {\n      \"host\": \"127.0.0.1\",\n      \"port\": 25000,\n      \"database\": \"BOOKS\",\n      \"user\": \"db2admin\",\n      \"password\": \"password\",\n      \"schema_name\": \"db2admin\"\n    };`\n\n\nDruid\n\n\n`sql\n    CREATE DATABASE druid_datasource        --- display name for the database\n    WITH ENGINE = 'druid',                  --- name of the MindsDB handler\n    PARAMETERS = {\n      \"host\": \" \",                          --- host name or IP address of Apache Druid\n      \"port\": ,                             --- port where Apache Druid runs\n      \"user\": \" \",                          --- optional, user to authenticate with Apache Druid\n      \"password\": \" \",                      --- optional, password used to authenticate with Apache Druid\n      \"path\": \" \",                          --- query path\n      \"scheme\": \" \"                         --- the URI scheme (defaults to `http` if left blank)\n    };`\n\n\n`sql\n    CREATE DATABASE druid_datasource\n    WITH ENGINE = 'druid',\n    PARAMETERS = {\n      \"host\": \"127.0.0.1\",\n      \"port\": 8888,\n      \"path\": \"/druid/v2/sql/\",\n      \"scheme\": \"http\"\n    };`\n\n\nDuckDB\n\n\n`sql\n    CREATE DATABASE duckdb_datasource       --- display name for the database\n    WITH ENGINE = 'duckdb',                 --- name of the MindsDB handler\n    PARAMETERS = {\n      \"database\": \" \",                      --- database file name\n      \"read_only\":                          --- flag used to set the connection to read-only mode\n    };`\n\n\n`sql\n    CREATE DATABASE duckdb_datasource\n    WITH ENGINE = 'duckdb',\n    PARAMETERS = {\n      \"database\": \"db.duckdb\",\n      \"read_only\": False\n    };`\n\n\nElastic Search\n\n\n`sql\n    CREATE DATABASE elastic_datasource      --- display name for the database\n    WITH ENGINE = 'elasticsearch',          --- name of the MindsDB handler\n    PARAMETERS = {\n      \"hosts\": \" \",                         --- one or more host names or IP addresses of the Elasticsearch server\n      \"username\": \" \",                      --- optional, username to authenticate with the Elasticsearch server\n      \"password\": \" \",                      --- optional, password used to authenticate with the Elasticsearch server\n      \"cloud_id\": \" \"                       --- optional, unique ID of your hosted Elasticsearch cluster (must be provided when \"hosts\" is left blank)\n    };`\n\n\n`sql\n    CREATE DATABASE elastic_datasource\n    WITH ENGINE = 'elasticsearch',\n    PARAMETERS = {\n      \"hosts\": \"localhost:9200\"\n    };`\n\n\nFirebird\n\n\n`sql\n    CREATE DATABASE firebird_datasource         --- display name for the database\n    WITH ENGINE = 'firebird',                   --- name of the MindsDB handler\n    PARAMETERS = {\n      \"host\": \" \",                              --- host name or IP address of the Firebird server\n      \"database\": \" \",                          --- database name\n      \"user\": \" \",                              --- user to authenticate with the Firebird server\n      \"password\": \" \"                           --- password used to authenticate with the Firebird server\n    };`\n\n\n`sql\n    CREATE DATABASE firebird_datasource\n    WITH ENGINE = 'firebird',\n    PARAMETERS = {\n      \"host\": \"127.0.0.1\",\n      \"database\": \"test\",\n      \"user\": \"firebird\",\n      \"password\": \"password\"\n    };`\n\n\nGoogle BigQuery\n\n\n`sql\n    CREATE DATABASE bigquery_datasource       --- display name for the database\n    WITH ENGINE = 'bigquery',                 --- name of the MindsDB handler\n    PARAMETERS = {\n      \"project_id\": \" \",                      --- globally unique project identifier\n      \"service_account_keys\": {               --- service account keys file\n        \"path\": \" \"                               --- either \"path\" or \"url\"\n      }\n    };`\n\n\n`sql\n    CREATE DATABASE bigquery_datasource\n    WITH ENGINE = 'bigquery',\n    PARAMETERS = {\n      \"project_id\": \"badger-345908\",\n      \"service_account_keys\": {\n        \"path\": \"/home/Downloads/badger-345908.json\"\n      }\n    };`\n\n\n`sql\n    CREATE DATABASE bigquery_datasource\n    WITH ENGINE = 'bigquery',\n    PARAMETERS = {\n      \"project_id\": \"badger-345908\",\n      \"service_account_keys\": {\n        \"url\": \"https://url/badger-345908.json\"\n      }\n    };`\n\n\nGoogle Sheets\n\n\n`sql\n    CREATE DATABASE sheets_datasource       --- display name for the database\n    WITH ENGINE = 'sheets',                 --- name of the MindsDB handler\n    PARAMETERS = {\n      \"spreadsheet_id\": \" \",                --- unique ID of the Google Sheet\n      \"sheet_name\": \" \"                     --- name of the Google Sheet\n    };`\n\n\n`sql\n    CREATE DATABASE sheets_datasource\n    WITH ENGINE = 'sheets',\n    PARAMETERS = {\n      \"spreadsheet_id\": \"abc1234567\",       --- located in the URL: https://docs.google.com/spreadsheets/d/abc1234567/edit#gid=0\n      \"sheet_name\": \"Invoice\"\n    };`\n\n\nHive\n\n\n`sql\n    CREATE DATABASE hive_datasource   --- display name for the database\n    WITH ENGINE = 'hive',             --- name of the MindsDB handler\n    PARAMETERS = {\n      \"user\": \" \",                    --- database user\n      \"password\": \" \",                --- database password\n      \"host\": \" \",                    --- host name or IP address\n      \"port\": ,                       --- port used to make TCP/IP connection\n      \"database\": \" \",                --- database name\n      \"auth\": \" \"                     --- defaults to CUSTOM if not provided; check for options here: https://pypi.org/project/PyHive/\n    };`\n\n\n`sql\n    CREATE DATABASE hive_datasource\n    WITH ENGINE = 'hive',\n    PARAMETERS = {\n      \"user\": \"hive\",\n      \"password\": \"password\",\n      \"host\": \"127.0.0.1\",\n      \"port\": 10000,\n      \"database\": \"hive_db\",\n      \"auth\": \"CUSTOM\"\n    };`\n\n\nImpala\n\n\n`sql\n    CREATE DATABASE impala_datasource      --- display name for the database\n    WITH ENGINE = 'impala',                --- name of the MindsDB handler\n    PARAMETERS = {\n      \"user\": \" \",                         --- database user\n      \"password\": \" \",                     --- database password\n      \"host\": \" \",                         --- host name or IP address\n      \"port\": ,                            --- port used to make TCP/IP connection\n      \"database\": \" \"                      --- database name\n    };`\n\n\n`sql\n    CREATE DATABASE impala_datasource\n    WITH ENGINE = 'impala',\n    PARAMETERS = {\n      \"user\": \"impala_user\",\n      \"password\": \"password\",\n      \"host\": \"127.0.0.1\",\n      \"port\": 21050,\n      \"database\": \"impala_db\"\n    };`\n\n\nInformix\n\n\n`sql\n    CREATE DATABASE informix_datasource       --- display name for the database\n    WITH ENGINE = 'informix',                 --- name of the MindsDB handler\n    PARAMETERS = {\n      \"server\": \" \",                          --- server name\n      \"host\": \" \",                            --- host name or IP address\n      \"port\": ,                               --- port used to make TCP/IP connection\n      \"database\": \" \",                        --- database name\n      \"user\": \" \",                            --- database user\n      \"password\": \" \",                        --- database password\n      \"schema_name\": \" \",                     --- database schema name\n      \"logging_enabled\":                      --- indicates whether logging is enabled (defaults to `True` if left blank)\n    };`\n\n\n`sql\n    CREATE DATABASE informix_datasource\n    WITH ENGINE = 'informix',\n    PARAMETERS = {\n      \"server\": \"server\",\n      \"host\": \"127.0.0.1\",\n      \"port\": 9091,\n      \"database\": \"stores_demo\",\n      \"user\": \"informix\",\n      \"password\": \"password\",\n      \"schema_name\": \"demo_schema\",\n      \"logging_enabled\": False\n    };`\n\n\nMariaDB\n\n\n`sql\n    CREATE DATABASE maria_datasource            --- display name for the database\n    WITH ENGINE = 'mariadb',                    --- name of the MindsDB handler\n    PARAMETERS = {\n      \"host\": \" \",                              --- host IP address or URL\n      \"port\": ,                                 --- port used to make TCP/IP connection\n      \"database\": \" \",                          --- database name\n      \"user\": \" \",                              --- database user\n      \"password\": \" \",                          --- database password\n      \"ssl\": ,                                  --- optional, the `ssl` parameter value indicates whether SSL is enabled (`True`) or disabled (`False`)\n      \"ssl_ca\": {                               --- optional, SSL Certificate Authority\n        \"path\": \" \"                                 --- either \"path\" or \"url\"\n      },\n      \"ssl_cert\": {                             --- optional, SSL certificates\n        \"url\": \" \"                                  --- either \"path\" or \"url\"\n      },\n      \"ssl_key\": {                              --- optional, SSL keys\n        \"path\": \" \"                                 --- either \"path\" or \"url\"\n      }\n    };`\n\n\n`sql\n    CREATE DATABASE maria_datasource\n    WITH ENGINE = 'mariadb',\n    PARAMETERS = {\n      \"host\": \"127.0.0.1\",\n      \"port\": 3306,\n      \"database\": \"mariadb\",\n      \"user\": \"root\",\n      \"password\": \"password\"\n    };`\n\n\nMariaDB SkySQL\n\n\n`sql\n    CREATE DATABASE skysql            --- display name for the database\n    WITH ENGINE = 'mariadb',          --- name of the MindsDB handler\n    PARAMETERS = {\n      \"user\": \" \",                    --- database user\n      \"password\": \" \",                --- database password\n      \"host\": \" \",                    --- host IP address or URL\n      \"port\": ,                       --- port used to make TCP/IP connection\n      \"ssl\": ,                        --- optional, the `ssl` parameter value indicates whether SSL is enabled (`True`) or disabled (`False`)\n      \"ssl-ca\": {                     --- optional, SSL Certificate Authority\n        \"path\": \" \"                       --- either \"path\" or \"url\"\n      },\n      \"database\": \" \"                 --- database name\n    };`\n\n\n`sql\n    CREATE DATABASE skysql_datasource\n    WITH ENGINE = 'mariadb',\n    PARAMETERS = {\n      \"host\": \"mindsdbtest.mdb0002956.db1.skysql.net\",\n      \"port\": 5001,\n      \"database\": \"mindsdb_data\",\n      \"user\": \"DB00007539\",\n      \"password\": \"password\",\n      \"ssl-ca\": {\n        \"url\": \"https://mindsdb-web-builds.s3.amazonaws.com/aws_skysql_chain.pem\"\n      }\n    };`\n\n\nFor more information on how to connect MariaDB SkySQL and MindsDB, visit our doc page here.\nMatrix One\n\n\n`sql\n    CREATE DATABASE matrixone_datasource        --- display name for the database\n    WITH ENGINE = 'matrixone',                  --- name of the MindsDB handler\n    PARAMETERS = {\n      \"host\": \" \",                              --- host IP address or URL\n      \"port\": ,                                 --- port used to make TCP/IP connection\n      \"database\": \" \",                          --- database name\n      \"user\": \" \",                              --- database user\n      \"password\": \" \",                          --- database password\n      \"ssl\": ,                                  --- optional, the `ssl` parameter value indicates whether SSL is enabled (`True`) or disabled (`False`)\n      \"ssl_ca\": {                               --- optional, SSL Certificate Authority\n        \"path\": \" \"                                 --- either \"path\" or \"url\"\n      },\n      \"ssl_cert\": {                             --- optional, SSL certificates\n        \"url\": \" \"                                  --- either \"path\" or \"url\"\n      },\n      \"ssl_key\": {                              --- optional, SSL keys\n        \"path\": \" \"                                 --- either \"path\" or \"url\"\n      }\n    };`\n\n\n`sql\n    CREATE DATABASE matrixone_datasource\n    WITH ENGINE = 'matrixone',\n    PARAMETERS = {\n      \"host\": \"127.0.0.1\",\n      \"port\": 6001,\n      \"database\": \"mo_catalog\",\n      \"user\": \"matrixone\",\n      \"password\": \"password\"\n    };`\n\n\nMicrosoft Access\n\n\n`sql\n    CREATE DATABASE access_datasource       --- display name for the database\n    WITH ENGINE = 'access',                 --- name of the MindsDB handler\n    PARAMETERS = {\n      \"db_file\": \" \"                        --- path to the database file to be used\n    };`\n\n\n`sql\n    CREATE DATABASE access_datasource\n    WITH ENGINE = 'access',\n    PARAMETERS = {\n      \"db_file\": \"example_db.accdb\"\n    };`\n\n\nMicrosoft SQL Server\n\n\n`sql\n    CREATE DATABASE mssql_datasource        --- display name for the database\n    WITH ENGINE = 'mssql',                  --- name of the MindsDB handler\n    PARAMETERS = {\n      \"host\": \" \",                          --- host name or IP address\n      \"port\": ,                             --- port used to make TCP/IP connection\n      \"database\": \" \",                      --- database name\n      \"user\": \" \",                          --- database user\n      \"password\": \" \"                       --- database password\n    };`\n\n\n`sql\n    CREATE DATABASE mssql_datasource\n    WITH ENGINE = 'mssql',\n    PARAMETERS = {\n      \"host\": \"127.0.0.1\",\n      \"port\": 1433,\n      \"database\": \"master\",\n      \"user\": \"sa\",\n      \"password\": \"password\"\n    };`\n\n\nMonetDB\n\n\n`sql\n    CREATE DATABASE monetdb_datasource          --- display name for the database\n    WITH ENGINE = 'monetdb',                    --- name of the MindsDB handler\n    PARAMETERS = {\n      \"host\": \" \",                              --- host name or IP address\n      \"port\": ,                                 --- port used to make TCP/IP connection\n      \"database\": \" \",                          --- database name\n      \"user\": \" \",                              --- database user\n      \"password\": \" \",                          --- database password\n      \"schema_name\": \" \"                        --- database schema name (defaults to the current schema if left blank)\n    };`\n\n\n`sql\n    CREATE DATABASE monetdb_datasource\n    WITH ENGINE = 'monetdb',\n    PARAMETERS = {\n      \"host\": \"127.0.0.1\",\n      \"port\": 50000,\n      \"database\": \"demo\",\n      \"user\": \"monetdb\",\n      \"password\": \"password\",\n      \"schema_name\": \"sys\"\n    };`\n\n\nMongoDB\n\nFor this connection, we strongly suggest using the Mongo API instead of the SQL API\nMindsDB has a dedicated Mongo API that allows you to use the full power of the MindsDB platform.\nUsing the Mongo API will feel more natural for MongoDB users and allow you to use all the features of MindsDB.\nYou can find the instructions on how to connect MindsDB to MongoDB Compass here\nor MongoDB Shell here and proceed with the Mongo API documentation for further details.\n\n\n\n`sql\n    CREATE DATABASE mongo_datasource          --- display name for the database\n    WITH ENGINE = 'mongo',                    --- name of the MindsDB handler\n    PARAMETERS = {\n      \"host\": \" \",                            --- host name or IP address\n      \"port\": ,                               --- port used to make TCP/IP connection\n      \"user\": \" \",                            --- database user\n      \"password\": \" \"                         --- database password\n      \"database\": \" \"                         --- database name\n    };`\n\n\n`sql\n    CREATE DATABASE mongo_datasource\n    WITH ENGINE = 'mongo',\n    PARAMETERS = {\n      \"host\": \"127.0.0.1\",\n      \"port\": 27017,\n      \"user\": \"mongo\",\n      \"password\": \"password\",\n      \"database\": \"database\"\n    };`\n\n\nMySQL\n\n\n`sql\n    CREATE DATABASE mysql_datasource            --- display name for the database\n    WITH ENGINE = 'mysql',                      --- name of the MindsDB handler\n    PARAMETERS = {\n      \"host\": \" \",                              --- host name or IP address\n      \"port\": ,                                 --- port used to make TCP/IP connection\n      \"database\": \" \",                          --- database name\n      \"user\": \" \",                              --- database user\n      \"password\": \" \",                          --- database password\n      \"ssl\": ,                                  --- optional, the `ssl` parameter value indicates whether SSL is enabled (`True`) or disabled (`False`)\n      \"ssl_ca\": {                               --- optional, SSL Certificate Authority\n        \"path\": \" \"                                 --- either \"path\" or \"url\"\n      },\n      \"ssl_cert\": {                             --- optional, SSL certificates\n        \"url\": \" \"                                  --- either \"path\" or \"url\"\n      },\n      \"ssl_key\": {                              --- optional, SSL keys\n        \"path\": \" \"                                 --- either \"path\" or \"url\"\n      }\n    };`\n\n\n`sql\n    CREATE DATABASE mysql_datasource\n    WITH ENGINE = 'mysql',\n    PARAMETERS = {\n      \"host\": \"127.0.0.1\",\n      \"port\": 3306,\n      \"database\": \"mysql\",\n      \"user\": \"root\",\n      \"password\": \"password\"\n    };`\n\n\nOceanBase\n\n\n`sql\n    CREATE DATABASE oceanbase_datasource      --- display name for the database\n    WITH ENGINE = 'oceanbase',                --- name of the MindsDB handler\n    PARAMETERS = {\n      \"host\": \" \",                            --- host name or IP address\n      \"user\": \" \",                            --- database user\n      \"password\": \" \",                        --- database password\n      \"port\": ,                               --- port used to make TCP/IP connection\n      \"database\": \" \"                         --- database name\n    };`\n\n\n`sql\n    CREATE DATABASE oceanbase_datasource\n    WITH ENGINE = 'oceanbase',\n    PARAMETERS = {\n      \"host\": \"127.0.0.1\",\n      \"user\": \"oceanbase_user\",\n      \"password\": \"password\",\n      \"port\": 2881,\n      \"database\": \"oceanbase_db\"\n    };`\n\n\nOpenGauss\n\n\n`sql\n    CREATE DATABASE opengauss_datasource            --- display name for the database\n    WITH ENGINE = 'opengauss',                      --- name of the MindsDB handler\n    PARAMETERS = {\n      \"host\": \" \",                                  --- host name or IP address\n      \"port\": ,                                     --- port used to make TCP/IP connection\n      \"database\": \" \",                              --- database name\n      \"user\": \" \",                                  --- database user\n      \"password\": \" \",                              --- database password\n    };`\n\n\n`sql\n    CREATE DATABASE opengauss_datasource\n    WITH ENGINE = 'opengauss',\n    PARAMETERS = {\n      \"host\": \"127.0.0.1\",\n      \"port\": 5432,\n      \"database\": \"opengauss\",\n      \"user\": \"mindsdb\",\n      \"password\": \"password\"\n    };`\n\n\nOracle\n\n\n`sql\n    CREATE DATABASE oracle_datasource         --- display name for the database\n    WITH ENGINE = 'oracle',                   --- name of the MindsDB handler\n    PARAMETERS = {\n      \"host\": \" \",                            --- host name or IP address\n      \"port\": ,                               --- port used to make TCP/IP connection\n      \"sid\": \" \",                             --- unique identifier of the database instance\n      \"user\": \" \",                            --- database user\n      \"password\": \" \"                         --- database password\n    };`\n\n\n`sql\n    CREATE DATABASE oracle_datasource\n    WITH ENGINE = 'oracle',\n    PARAMETERS = {\n      \"host\": \"127.0.0.1\",\n      \"port\": 1521,\n      \"sid\": \"ORCL\",\n      \"user\": \"sys\",\n      \"password\": \"password\"\n    };`\n\n\nOrioleDB\n\n\n`sql\n    CREATE DATABASE orioledb_datasource      --- display name for the database\n    WITH ENGINE = 'orioledb',                --- name of the MindsDB handler\n    PARAMETERS = {\n      \"user\": \" \",                           --- database user\n      \"password\": \" \",                       --- database password\n      \"host\": \" \",                           --- host name or IP address\n      \"port\": ,                              --- port used to make TCP/IP connection\n      \"server\": \" \",                         --- sets the current server\n      \"database\": \" \"                        --- sets the current database\n    };`\n\n\n`sql\n    CREATE DATABASE orioledb_datasource\n    WITH ENGINE = 'orioledb',\n    PARAMETERS = {\n      \"user\": \"orioledb_user\",\n      \"password\": \"password\",\n      \"host\": \"127.0.0.1\",\n      \"port\": 55505,\n      \"server\": \"server_name\",\n      \"database\": \"oriole_db\"\n    };`\n\n\nPinot\n\n\n`sql\n    CREATE DATABASE pinot_datasource        --- display name for the database\n    WITH ENGINE = 'pinot',                  --- name of the MindsDB handler\n    PARAMETERS = {\n      \"host\": \" \",                          --- host name or IP address of the Apache Pinot cluster\n      \"broker_port\": ,                      --- port where the broker of the Apache Pinot cluster runs\n      \"controller_port\": ,                  --- port where the controller of the Apache Pinot cluster runs\n      \"path\": \" \",                          --- query path\n      \"scheme\": \" \",                        --- scheme (defaults to `http` if left blank)\n      \"username\": \" \",                      --- optional, user\n      \"password\": \" \",                      --- optional, password\n      \"verify_ssl\": \" \"                     --- optional, verify SSL\n    };`\n\n\n`sql\n    CREATE DATABASE pinot_datasource\n    WITH ENGINE = 'pinot',\n    PARAMETERS = {\n      \"host\": \"127.0.0.1\",\n      \"broker_port\": 8000,\n      \"controller_port\": 9000,\n      \"path\": \"/query/sql\",\n      \"scheme\": \"http\"\n    };`\n\n\nPlanetScale\n\n\n`sql\n    CREATE DATABASE planetscale_datasource     --- display name for the database\n    WITH ENGINE = 'planet_scale',              --- name of the MindsDB handler\n    PARAMETERS = {\n      \"host\": \" \",                             --- host name or IP address\n      \"port\": ,                                --- port used to make TCP/IP connection\n      \"user\": \" \",                             --- database user\n      \"password\": \" \",                         --- database password\n      \"database\": \" \"                          --- database name\n    };`\n\n\n`sql\n    CREATE DATABASE planetscale_datasource\n    WITH ENGINE = 'planet_scale',\n    PARAMETERS = {\n      \"host\": \"127.0.0.1\",\n      \"port\": 3306,\n      \"user\": \"planetscale_user\",\n      \"password\": \"password\",\n      \"database\": \"planetscale_db\"\n    };`\n\n\nPostgreSQL\n\n\n`sql\n    CREATE DATABASE psql_datasource         --- display name for the database\n    WITH ENGINE = 'postgres',               --- name of the MindsDB handler\n    PARAMETERS = {\n      \"host\": \" \",                          --- host name or IP address\n      \"port\": ,                             --- port used to make TCP/IP connection\n      \"database\": \" \",                      --- database name\n      \"user\": \" \",                          --- database user\n      \"password\": \" \"                       --- database password\n    };`\n\n\n`sql\n    CREATE DATABASE psql_datasource\n    WITH ENGINE = 'postgres',\n    PARAMETERS = {\n      \"host\": \"127.0.0.1\",\n      \"port\": 5432,\n      \"database\": \"postgres\",\n      \"user\": \"postgres\",\n      \"password\": \"password\"\n    };`\n\n\nQuestDB\n\n\n`sql\n    CREATE DATABASE questdb_datasource      --- display name for the database\n    WITH ENGINE = 'questdb',                --- name of the MindsDB handler\n    PARAMETERS = {\n      \"host\": \" \",                          --- host name or IP address\n      \"port\": ,                             --- port used to make TCP/IP connection\n      \"database\": \" \",                      --- database name\n      \"user\": \" \",                          --- database user\n      \"password\": \" \",                      --- database password\n      \"public\":                             --- value of `True` or `False` (defaults to `True` if left blank)\n    };`\n\n\n`sql\n    CREATE DATABASE questdb_datasource\n    WITH ENGINE = 'questdb',\n    PARAMETERS = {\n      \"host\": \"127.0.0.1\",\n      \"port\": 8812,\n      \"database\": \"qdb\",\n      \"user\": \"admin\",\n      \"password\": \"password\"\n    };`\n\n\nS3\n\n\n`sql\n    CREATE DATABASE amazons3_datasource     --- display name for the database\n    WITH ENGINE = 's3',                     --- name of the MindsDB handler\n    PARAMETERS = {\n      \"aws_access_key_id\": \" \",             --- the AWS access key\n      \"aws_secret_access_key\": \" \",         --- the AWS secret access key\n      \"region_name\": \" \",                   --- the AWS region\n      \"bucket\": \" \",                        --- name of the S3 bucket\n      \"key\": \" \",                           --- key of the object to be queried\n      \"input_serialization\": \" \"            --- format of the data to be queried\n    };`\n\n\n`sql\n    CREATE DATABASE amazons3_datasource\n    WITH ENGINE = 's3',\n    PARAMETERS = {\n        \"aws_access_key_id\": \"PCAQ2LJDOSWLNSQKOCPW\",\n        \"aws_secret_access_key\": \"U/VjewPlNopsDmmwItl34r2neyC6WhZpUiip57i\",\n        \"region_name\": \"us-east-1\",\n        \"bucket\": \"mindsdb-bucket\",\n        \"key\": \"iris.csv\",\n        \"input_serialization\": \"{'CSV': {'FileHeaderInfo': 'NONE'}}\"\n    };`\n\n\nSAP HANA\n\n\n`sql\n    CREATE DATABASE sap_hana_datasource           --- display name for the database\n    WITH ENGINE = 'hana',                         --- name of the MindsDB handler\n    PARAMETERS = {\n      \"host\": \" \",                                --- host name or IP address\n      \"port\": ,                                   --- port used to make TCP/IP connection\n      \"user\": \" \",                                --- user\n      \"password\": \" \",                            --- password\n      \"schema\": \" \",                              --- database schema name (defaults to the current schema if left blank)\n      \"encrypt\":                                  --- indicates whether connection is encrypted (required for cloud usage)\n    };`\n\n\n`sql\n    CREATE DATABASE sap_hana_datasource\n    WITH ENGINE = 'hana',\n    PARAMETERS = {\n      \"host\": \"<uuid>.hana.trial-us10.hanacloud.ondemand.com\",\n      \"port\": \"443\",\n      \"user\": \"DBADMIN\",\n      \"password\": \"password\",\n      \"schema\": \"MINDSDB\",\n      \"encrypt\": True\n    };`\n\n\nScylla\n\n\n`sql\n    CREATE DATABASE scylladb_datasource           --- display name for the database\n    WITH ENGINE = 'scylladb',                     --- name of the MindsDB handler\n    PARAMETERS = {\n      \"host\": \" \",                                --- host name or IP address\n      \"port\": ,                                   --- port used to make TCP/IP connection\n      \"user\": \" \",                                --- user\n      \"password\": \" \",                            --- password\n      \"protocol_version\": ,                       --- optional, protocol version (defaults to 4 if left blank)\n      \"keyspace\": \" \",                            --- keyspace name (it is the top level container for tables)\n      \"secure_connect_bundle\": {                  --- secure connect bundle file\n        \"path\": \" \"                                   --- either \"path\" or \"url\"\n      }\n    };`\n\n\n`sql\n    CREATE DATABASE scylladb_datasource\n    WITH ENGINE = 'scylladb',\n    PARAMETERS = {\n      \"host\": \"127.0.0.1\",\n      \"port\": 7199,\n      \"user\": \"user@mindsdb.com\",\n      \"password\": \"password\",\n      \"protocol_version\": 4,\n      \"keyspace\": \"keyspace_name\",\n      \"secure_connect_bundle\": {\n        \"path\": \"/home/zoran/Downloads/secure-connect-mindsdb.zip\"\n      }\n    };`\n\n\nSingleStore\n\n\n`sql\n    CREATE DATABASE singlestore_datasource          --- display name for the database\n    WITH ENGINE = 'singlestore',                    --- name of the MindsDB handler\n    PARAMETERS = {\n      \"host\": \" \",                                  --- host name or IP address\n      \"port\": ,                                     --- port used to make TCP/IP connection\n      \"database\": \" \",                              --- database name\n      \"user\": \" \",                                  --- database user\n      \"password\": \" \",                              --- database password\n      \"ssl\": ,                                      --- optional, the `ssl` parameter value indicates whether SSL is enabled (`True`) or disabled (`False`)\n      \"ssl_ca\": {                                   --- optional, SSL Certificate Authority\n        \"path\": \" \"                                     --- either \"path\" or \"url\"\n      },\n      \"ssl_cert\": {                                 --- optional, SSL certificates\n        \"url\": \" \"                                      --- either \"path\" or \"url\"\n      },\n      \"ssl_key\": {                                  --- optional, SSL keys\n        \"path\": \" \"                                     --- either \"path\" or \"url\"\n      }\n    };`\n\n\n`sql\n    CREATE DATABASE singlestore_datasource\n    WITH ENGINE = 'singlestore',\n    PARAMETERS = {\n      \"host\": \"127.0.0.1\",\n      \"port\": 3306,\n      \"database\": \"singlestore\",\n      \"user\": \"root\",\n      \"password\": \"password\"\n    };`\n\n\nSnowflake\n\n\n`sql\n    CREATE DATABASE snowflake_datasource              --- display name for the database\n    WITH ENGINE = 'snowflake',                        --- name of the MindsDB handler\n    PARAMETERS = {\n      \"host\": \" \",                                    --- host name or IP address\n      \"port\": ,                                       --- port used to make TCP/IP connection\n      \"database\": \" \",                                --- database name\n      \"user\": \" \",                                    --- database user\n      \"password\": \" \",                                --- database password\n      \"account\": \" \",                                 --- the Snowflake account\n      \"schema\": \" \",                                  --- schema name (defaults to `public` if left blank)\n      \"protocol\": \" \",                                --- protocol (defaults to `https` if left blank)\n      \"warehouse\": \" \"                                --- the warehouse account\n    };`\n\n\n`sql\n    CREATE DATABASE snowflake_datasource\n    WITH ENGINE = 'snowflake',\n    PARAMETERS = {\n      \"host\": \"account_name.snowflakecomputing.com\",\n      \"port\": 443,\n      \"database\": \"snowflake\",\n      \"user\": \"user\",\n      \"password\": \"password\",\n      \"account\": \"account_name\",\n      \"schema\": \"public\",\n      \"protocol\": \"https\",\n      \"warehouse\": \"warehouse\"\n    };`\n\n\nSolr\n\n\n`sql\n    CREATE DATABASE solr_datasource   --- display name for the database\n    WITH ENGINE = 'solr',             --- name of the MindsDB handler\n    PARAMETERS = {\n      \"username\": \" \",                --- optional, username used to authenticate with the Solr server\n      \"password\": \" \",                --- optional, password used to authenticate with the Solr server\n      \"host\": \" \",                    --- host name or IP address of the Solr serve\n      \"port\": ,                       --- port number of the Solr server\n      \"server_path\": \" \",             --- defaults to `solr` if left blank\n      \"collection\": \" \",              --- Solr Collection name\n      \"use_ssl\": \" \"                  --- defaults to `false` if left blank; refer to https://pypi.org/project/sqlalchemy-solr/\n    };`\n\n\n`sql\n    CREATE DATABASE solr_datasource\n    WITH ENGINE = 'solr',\n    PARAMETERS = {\n      \"username\": \"solr_user\",\n      \"password\": \"password\",\n      \"host\": \"127.0.0.1\",\n      \"port\": 8981,\n      \"server_path\": \"solr\",\n      \"collection\": \"collection_name\",\n      \"use_ssl\": \"false\"\n    };`\n\n\nSQL Anywhere\n\n\n`sql\n    CREATE DATABASE sqlany_datasource   --- display name for the database\n    WITH ENGINE = 'sqlany',             --- name of the MindsDB handler\n    PARAMETERS = {\n      \"user\": \" \",                      --- username\n      \"password\": \" \",                  --- password\n      \"host\": \" \",                      --- host name or IP address of the SAP SQL Anywhere instance\n      \"port\": ,                         --- port number of the SAP SQL Anywhere instance\n      \"server\": \" \",                    --- sets the current server\n      \"database\": \" \"                   --- sets the current database\n    };`\n\n\n`sql\n    CREATE DATABASE sqlany_datasource\n    WITH ENGINE = 'sqlany',\n    PARAMETERS = {\n      \"user\": \"sqlany_user\",\n      \"password\": \"password\",\n      \"host\": \"127.0.0.1\",\n      \"port\": 55505,\n      \"server\": \"server_name\",\n      \"database\": \"sqlany_db\"\n    };`\n\n\nSQLite\n\n\n`sql\n    CREATE DATABASE sqlite_datasource         --- display name for the database\n    WITH ENGINE = 'sqlite',                   --- name of the MindsDB handler\n    PARAMETERS = {\n      \"db_file\": \" \"                          --- path to the database file to be used\n    };`\n\n\n`sql\n    CREATE DATABASE sqlite_datasource\n    WITH ENGINE = 'sqlite',\n    PARAMETERS = {\n      \"db_file\": \"example.db\"\n    };`\n\n\nStarRocks\n\n\n`sql\n    CREATE DATABASE starrocks_datasource      --- display name for the database\n    WITH ENGINE = 'starrocks',                --- name of the MindsDB handler\n    PARAMETERS = {\n      \"host\": \" \",                            --- host name or IP address\n      \"user\": \" \",                            --- database user\n      \"password\": \" \",                        --- database password\n      \"port\": ,                               --- port used to make TCP/IP connection\n      \"database\": \" \"                         --- database name\n    };`\n\n\n`sql\n    CREATE DATABASE starrocks_datasource\n    WITH ENGINE = 'starrocks',\n    PARAMETERS = {\n      \"host\": \"127.0.0.1\",\n      \"user\": \"starrocks_user\",\n      \"password\": \"password\",\n      \"port\": 8030,\n      \"database\": \"starrocks_db\"\n    };`\n\n\nSupabase\n\n\n`sql\n    CREATE DATABASE supabase_datasource             --- display name for the database\n    WITH ENGINE = 'supabase',                       --- name of the MindsDB handler\n    PARAMETERS = {\n      \"host\": \" \",                                  --- host name or IP address\n      \"port\": ,                                     --- port used to make TCP/IP connection\n      \"database\": \" \",                              --- database name\n      \"user\": \" \",                                  --- database user\n      \"password\": \" \",                              --- database password\n    };`\n\n\n`sql\n    CREATE DATABASE supabase_datasource\n    WITH ENGINE = 'supabase',\n    PARAMETERS = {\n      \"host\": \"127.0.0.1\",\n      \"port\": 54321,\n      \"database\": \"test\",\n      \"user\": \"supabase\",\n      \"password\": \"password\"\n    };`\n\n\nTDEngine\n\n\n`sql\n    CREATE DATABASE tdengine_datasource   --- display name for the database\n    WITH ENGINE = 'tdengine',             --- name of the MindsDB handler\n    PARAMETERS = {\n      \"user\": \" \",                        --- server username\n      \"password\": \" \",                    --- server password\n      \"url\": \" \",                         --- URL to the TDEngine server (for local server, it is localhost:6041 by default)\n      \"token\": \" \",                       --- unique token provided when using TDEngine Cloud\n      \"database\": \" \"                     --- database name\n    };`\n\n\n`sql\n    CREATE DATABASE tdengine_datasource\n    WITH ENGINE = 'tdengine',\n    PARAMETERS = {\n      \"user\": \"tdengine_user\",\n      \"password\": \"password\",\n      \"url\": \"localhost:6041\",\n      \"token\": \"token\",\n      \"database\": \"tdengine_db\"\n    };`\n\n\nTeradata\n\n\n`sql\n    CREATE DATABASE teradata_datasource     --- display name for the database\n    WITH ENGINE = 'teradata',               --- name of the MindsDB handler\n    PARAMETERS = {\n      \"host\": \" \",                          --- host name or IP address\n      \"user\": \" \",                          --- database user\n      \"password\": \" \",                      --- database password\n      \"database\": \" \",                      --- database name\n      \"port\":                               --- port used to make TCP/IP connection\n    };`\n\n\n`sql\n    CREATE DATABASE teradata_datasource\n    WITH ENGINE = 'teradata',\n    PARAMETERS = {\n      \"host\": \"127.0.0.1\",\n      \"user\": \"teradata\",\n      \"password\": \"password\",\n      \"database\": \"teradata_db\",\n      \"port\": 1025\n    };`\n\n\nTiDB\n\n\n`sql\n    CREATE DATABASE tidb_datasource                 --- display name for the database\n    WITH ENGINE = 'tidb',                           --- name of the MindsDB handler\n    PARAMETERS = {\n      \"host\": \" \",                                  --- host name or IP address\n      \"port\": ,                                     --- port used to make TCP/IP connection\n      \"database\": \" \",                              --- database name\n      \"user\": \" \",                                  --- database user\n      \"password\": \" \",                              --- database password\n    };`\n\n\n`sql\n    CREATE DATABASE tidb_datasource\n    WITH ENGINE = 'tidb',\n    PARAMETERS = {\n      \"host\": \"127.0.0.1\",\n      \"port\": 4000,\n      \"database\": \"tidb\",\n      \"user\": \"root\",\n      \"password\": \"password\"\n    };`\n\n\nTimescaledb\n\n\n`sql\n    CREATE DATABASE timescaledb_datasource    --- display name for the database\n    WITH ENGINE = 'timescaledb',              --- name of the MindsDB handler\n    PARAMETERS = {\n      \"user\": \" \",                            --- database user\n      \"password\": \" \",                        --- database password\n      \"host\": \" \",                            --- host name or IP address\n      \"port\": ,                               --- port used to make TCP/IP connection\n      \"database\": \" \"                         --- database name\n    };`\n\n\n`sql\n    CREATE DATABASE timescaledb_datasource\n    WITH ENGINE = 'timescaledb',\n    PARAMETERS = {\n      \"user\": \"timescaledb\",\n      \"password\": \"password\",\n      \"host\": \"127.0.0.1\",\n      \"port\": 36806,\n      \"database\": \"timescaledb_db\"\n    };`\n\n\nTrino\n\n\n`sql\n    CREATE DATABASE trino_datasource          --- display name for the database\n    WITH ENGINE = 'trino',                    --- name of the MindsDB handler\n    PARAMETERS = {\n      \"host\": \" \",                            --- host name or IP address\n      \"port\": ,                               --- port used to make TCP/IP connection\n      \"auth\": \" \",                            --- optional, authentication method, currently only `basic` is supported\n      \"http_scheme\": \" \",                     --- optional, `http`(default) or `https`\n      \"user\": \" \",                            --- database user\n      \"password\": \" \",                        --- database password\n      \"catalog\": \" \",                         --- optional, catalog\n      \"schema\": \" \"                           --- optional, schema\n      \"with\":                                 --- optional, default WITH-clause (properties) for ALL tables\n                                                  --- this parameter is experimental and might be changed or removed in future release\n    };`\n\n\n`sql\n    CREATE DATABASE trino_datasource\n    WITH ENGINE = 'trino',\n    PARAMETERS = {\n      \"host\": \"127.0.0.1\",\n      \"port\": 8080,\n      \"user\": \"trino\",\n      \"password\": \"password\",\n      \"catalog\": \"default\",\n      \"schema\": \"test\"\n    };`\n\n\n`sql\n    CREATE DATABASE trino_datasource\n    WITH ENGINE = 'trino',\n    PARAMETERS = {\n      \"host\": \"127.0.0.1\",\n      \"port\": 443,\n      \"auth\": \"basic\",\n      \"http_scheme\": \"https\",\n      \"user\": \"trino\",\n      \"password\": \"password\",\n      \"catalog\": \"default\",\n      \"schema\": \"test\",\n      \"with\": \"with (transactional = true)\"\n    };`\n\n\nVertica\n\n\n`sql\n    CREATE DATABASE vertica_datasource        --- display name for the database\n    WITH ENGINE = 'vertica',                  --- name of the MindsDB handler\n    PARAMETERS = {\n      \"host\": \" \",                            --- host name or IP address\n      \"port\": ,                               --- port used to make TCP/IP connection\n      \"database\": \" \",                        --- database name\n      \"user\": \" \",                            --- database user\n      \"password\": \" \",                        --- database password\n      \"schema_name\": \" \"                      --- database schema name\n    };`\n\n\n`sql\n    CREATE DATABASE vertica_datasource\n    WITH ENGINE = 'vertica',\n    PARAMETERS = {\n      \"host\": \"127.0.0.1\",\n      \"port\": 5433,\n      \"database\": \"VMart\",\n      \"user\": \"vertica\",\n      \"password\": \"password\",\n      \"schema_name\": \"public\"\n    };`\n\n\nVitess\n\n\n`sql\n    CREATE DATABASE vitess_datasource       --- display name for the database\n    WITH ENGINE = 'vitess',                 --- name of the MindsDB handler\n    PARAMETERS = {\n      \"host\": \" \",                          --- host name or IP address\n      \"user\": \" \",                          --- database user\n      \"password\": \" \",                      --- database password\n      \"port\": ,                             --- port used to make TCP/IP connection\n      \"database\": \" \"                       --- database name\n    };`\n\n\n`sql\n    CREATE DATABASE vitess_datasource\n    WITH ENGINE = 'vitess',\n    PARAMETERS = {\n      \"host\": \"127.0.0.1\",\n      \"user\": \"vitess_user\",\n      \"password\": \"password\",\n      \"port\": 33577,\n      \"database\": \"vitess_db\"\n    };`\n\n\nYugabyte\n\n\n`sql\n    CREATE DATABASE yugabyte_datasource       --- display name for the database\n    WITH ENGINE = 'yugabyte',                 --- name of the MindsDB handler\n    PARAMETERS = {\n      \"user\": \" \",                            --- database user\n      \"password\": \" \",                        --- database password\n      \"host\": \" \",                            --- host name or IP address\n      \"port\": ,                               --- port used to make TCP/IP connection\n      \"database\": \" \"                         --- database name\n    };`\n\n\n`sql\n    CREATE DATABASE yugabyte_datasource\n    WITH ENGINE = 'yugabyte',\n    PARAMETERS = {\n      \"user\": \"yugabyte\",\n      \"password\": \"password\",\n      \"host\": \"127.0.0.1\",\n      \"port\": 5433,\n      \"database\": \"yugabyte_db\"\n    };`\n",
    "tag": "mindsdb"
  },
  {
    "title": "CrateDB",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/data-integrations/cratedb.mdx",
    "content": "\ntitle: CrateDB\nsidebarTitle: CrateDB\n\nThis is the implementation of the CrateDB handler for MindsDB.\nCrateDB\nCrateDB is a distributed SQL database management system that integrates a fully searchable document-oriented data store. It is open-source, written in Java, based on a shared-nothing architecture, and designed for high scalability. CrateDB includes components from Lucene, Elasticsearch and Netty.\nImplementation\nThis handler was implemented using `crate`, a Python library that allows you to use Python code to run SQL commands on CrateDB.\nThe required arguments to establish a connection are,\n* `user`: username associated with database\n* `password`: password to authenticate your access\n* `host`: host to server IP address or hostname\n* `port`: port through which connection is to be made\n* `schema_name`: schema name to get tables from\n\n\n```_Note: Default value of schema_name is 'doc'_\n```\n\n\nUsage\nIn order to make use of this handler and connect to CrateDB in MindsDB, the following syntax can be used,\n`sql\nCREATE DATABASE crate_datasource\nWITH\nengine='crate',\nparameters={\n    \"user\":\"crate\",\n    \"password\":\"\",\n    \"host\":\"127.0.0.1\",\n    \"port\":4200,\n    \"schema_name\":\"doc\"\n};`\nNow, you can use this established connection to query your database as follows,\n```sql\nSELECT * FROM crate_datasource.demo;",
    "tag": "mindsdb"
  },
  {
    "title": "Redshift Handler",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/data-integrations/amazon-redshift.mdx",
    "content": "\ntitle: Amazon Redshift\nsidebarTitle: Amazon Redshift\n\nRedshift Handler\nThis is the implementation of the Redshift handler for MindsDB.\nRedshift\nAmazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud. You can start with just a few hundred gigabytes of data and scale to a petabyte or more. This enables you to use your data to acquire new insights for your business and customers.\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/welcome.html\nImplementation\nThis handler was implemented using the `redshift_connector` library that is provided by Amazon Web Services.\nThe required arguments to establish a connection are,\n* `host`: the host name or IP address of the Redshift cluster\n* `port`: the port to use when connecting with the Redshift cluster\n* `database`: the database name to use when connecting with the Redshift cluster\n* `user`: the user to authenticate the user with the Redshift cluster\n* `password`: the password to authenticate the user with the Redshift cluster\nUsage\nBefore attempting to connect to a Redshift cluster using MindsDB, ensure that it accepts incoming connections. The following can be used as a guideline to accomplish this,\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/cannot-connect-redshift-cluster/\nIn order to make use of this handler and connect to a Redshift cluster in MindsDB, the following syntax can be used,\n`sql\nCREATE DATABASE redshift_datasource\nWITH\nengine='redshift',\nparameters={\n    \"host\": \"examplecluster.abc123xyz789.us-west-1.redshift.amazonaws.com\",\n    \"port\": 5439,\n    \"database\": \"example_db\",\n    \"user\": \"awsuser\",\n    \"password\": \"my_password\"\n};`\nNow, you can use this established connection to query your database as follows,\n```sql\nSELECT * FROM redshift_datasource.example_tbl",
    "tag": "mindsdb"
  },
  {
    "title": "MindsDB Storage ",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/setup/environment-vars.mdx",
    "content": "\ntitle: Environment Variables\nsidebarTitle: Environment Variables\n\nMost of the MindsDB functionality can be modified by extending the default configuration, but some of the configuration options \ncan be added as environment variables on the server where MindsDB is deployed.\nMindsDB Storage\nBy default, MindsDB stores the configuration files by determining appropriate platform-specific dirs, e.g. a \"user data dir\": \n\nOn Linux `~/.local/share/mindsdb/var`\nOn MacOS `~/Library/Application Support/mindsdb/var`\nOn Windows `C:\\Documents and Settings\\<User>\\Application Data\\Local Settings\\<AppAuthor>\\mindsdb\\var`\n\nIn the `MINDSDB_STORAGE_DIR` location, MindsDB stores users' data, models and uploaded data files, the static assets for the frontend application and the \n`sqlite.db` file. \nYou can change the default storage location using `MINDSDB_STORAGE_DIR` variable.\nExample\n`MINDSDB_STORAGE_DIR='~/home/mindsdb/var'`\nMindsDB Configuration Storage\nMindsDB uses `sqlite` database by default to store the required configuration as models, projects, files metadata etc. \nThe full list of the above schemas can be found here. You can change the \ndefault storage option and use different database by adding the new connection string using `MINDSDB_DB_CON` variable.\nExample\n`MINDSDB_DB_CON='postgresql://user:secret@localhost'`\nMindsDB server\nBy default for the HTTP API, MindsDB uses Waitress which is a pure-Python WSGI server. There is an option to change that \nand use Flask or Gunircorn\n\nIf you want to use Gunicorn as a default server, make sure you run `pip install gunicorn`\n\nExample\n```\nMINDSDB_DEFAULT_SERVER=flask",
    "tag": "mindsdb"
  },
  {
    "title": "Create a Free Account at [MindsDB Cloud](https://cloud.mindsdb.com/register)",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/setup/cloud.mdx",
    "content": "\ntitle: MindsDB Cloud\nsidebarTitle: MindsDB Cloud\n\nMindsDB Cloud is a service hosted by MindsDB. It contains all of the latest\n  updates and provides a handy SQL editor so you can run your queries right\n  away.\nYou can sign up for\na free account at MindsDB Cloud here.\nFollow the steps below that guide you through the sign-up process.\nCreate a Free Account at MindsDB Cloud\n\n\nFollow this link and fill out the sign-up\nform, as below.\n\n\nMindsDB Cloud Terms and Conditions You can view it\n  here.\n\nLog In to the MindsDB Cloud\nFollow this link and input your credentials,\nas below.\n\nEmail Validation\nAfter you sign up for a free MindsDB Cloud account, we'll send you a\nconfirmation email. Click on the Verify Email button to validate your account.\n\nUse MindsDB Cloud\nNow, you are ready to use MindsDB Cloud.\n\n\nWhat's next? We recommend you follow one of our tutorials or learn more about the\nMindsDB Database.\n\n\nFrom Our Community\nCheck out the video guides created by our community:\n\n\nVideo guide on Singing Up with MindsDB Cloud uploaded on\n  ExploringTech by\n  Rutam Prita Mishra\n\n\nVideo guide on Setup MindsDB Free Cloud Account in 2 minutes\n  by @akhilcoder\n\n\nVideo guide on Tutorial: Create a Free MindsDB Cloud Account\n  by Alissa Troiano\n\n\nVideo guide on How to create an account on MindsDB cloud by\n  HellFire\n\n\nVideo guide on How to create a free account on MindsDB Cloud\n  by Anamika\n\n\nVideo guide on How to create a free account on MindsDB Cloud\n  by MichaelLantz\n\n\nVideo guide on How to access MindsDB by\n  Syed Zubeen\n\n",
    "tag": "mindsdb"
  },
  {
    "title": "Starting MindsDB with Default Configuration",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/setup/custom-config.mdx",
    "content": "\ntitle: Extend the Default MindsDB Configuration\nsidebarTitle: Extend the Default MindsDB Configuration\n\n\n  To follow this guide, please make sure you have a local installation of\n  MindsDB. Here, you\n  can find out how to install MindsDB locally.\n\nStarting MindsDB with Default Configuration\nIt is very straightforward to start MindsDB locally with the default config\nfile - just run the commands below.\nFirst, activate the virtual environment with this command:\n`bash\nsource mindsdb/bin/activate`\nAnd then, start MindsDB using this command:\n`bash\npython -m mindsdb`\nNow you can access your MindsDB locally at `127.0.0.1:47334`.\nStarting MindsDB with Extended Configuration\nFirst, you should prepare a `config.json` file based on the following template; remember to substitute the values for your custom configuration.\n`bash\n{\n    \"permanent_storage\": {\n        \"location\": \"local\"\n    },\n    \"paths\": {},\n    \"log\": {\n        \"level\": {\n            \"console\": \"INFO\",\n            \"file\": \"DEBUG\",\n            \"db\": \"WARNING\"\n        }\n    },\n    \"debug\": false,\n    \"integrations\": {},\n    \"gui\": {\n        \"autoupdate\": True\n    }\n    \"auth\":{\n        \"username\": \"mindsdb\",\n        \"password\": \"123\"\n    }\n    \"api\": {\n        \"http\": {\n            \"host\": \"127.0.0.1\",\n            \"port\": \"47334\"\n        },\n        \"mysql\": {\n            \"host\": \"127.0.0.1\",\n            \"port\": \"47335\",\n            \"database\": \"mindsdb\",\n            \"ssl\": true\n        },\n        \"mongodb\": {\n            \"host\": \"127.0.0.1\",\n            \"port\": \"47336\",\n            \"database\": \"mindsdb\"\n        }\n    },\n    \"cache\": {\n        \"type\": \"local\"\n    }\n}`\nNow that your `config.json` file is ready, run the command below to start\nMindsDB locally with your custom configuration.\n`bash\npython -m mindsdb --config=/path-to-the-extended-config-file/config-file.json`\nYou can access your MindsDB locally at `127.0.0.1:47334`, or any other IP\naddress and port combination if you altered them.\n\nWhat's next? We recommend you follow one of our tutorials or learn more about the\nMindsDB Database.",
    "tag": "mindsdb"
  },
  {
    "title": "Install Docker",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/setup/self-hosted/docker.mdx",
    "content": "\ntitle: Setup for Docker\nsidebarTitle: Docker\n\n\n\nInstall Docker\nIf you haven't done that already, install Docker on your machine following\nthe instructions. To make sure Docker is\nsuccessfully installed on your machine, run a test container as follows:\n`bash\ndocker run hello-world`\nYou should see the `Hello from Docker!` message. Otherwise, check the\nDocker's Get Started documentation.\n\nDocker for Mac users - RAM allocation issues\nBy default, Docker for Mac allocates 2 GB of RAM, which is insufficient for\ndeploying MindsDB with Docker. We recommend increasing the default RAM limit to\n4 GB. Please refer to the\nDocker Desktop for Mac users manual\nfor more information on how to increase the allocated memory.\n\nStart MindsDB\nRun the command below to start MindsDB in Docker.\n`bash\ndocker run -p 47334:47334 -p 47335:47335 mindsdb/mindsdb`\nLet's analyze this command part by part:\n\n`docker run` is a native Docker command used to start a container\n`-p 47334:47334` publishes port 47334 to access MindsDB GUI and HTTP\n  API\n`-p 47335:47335` publishes port 47335 to access MindsDB MySQL API\n`mindsdb/mindsdb` is the container we want to start\n\nDefault Configuration\nThe default configuration for MindsDB's Docker image is stored as a JSON code,\nas below.\n`json\n{\n  \"config_version\": \"1.4\",\n  \"storage_dir\": \"/root/mdb_storage\",\n  \"log\": {\n    \"level\": {\n      \"console\": \"ERROR\",\n      \"file\": \"WARNING\",\n      \"db\": \"WARNING\"\n    }\n  },\n  \"debug\": false,\n  \"integrations\": {},\n  \"api\": {\n    \"http\": {\n      \"host\": \"0.0.0.0\",\n      \"port\": \"47334\"\n    },\n    \"mysql\": {\n      \"host\": \"0.0.0.0\",\n      \"password\": \"\",\n      \"port\": \"47335\",\n      \"user\": \"mindsdb\",\n      \"database\": \"mindsdb\",\n      \"ssl\": true\n    },\n    \"mongodb\": {\n      \"host\": \"0.0.0.0\",\n      \"port\": \"47336\",\n      \"database\": \"mindsdb\"\n    }\n  }\n}`\nCustom Configuration\nTo override the default configuration, you can provide values via the\n`MDB_CONFIG_CONTENT` environment variable, as below.\n`bash\ndocker run -e MDB_CONFIG_CONTENT='{\"api\":{\"http\": {\"host\": \"0.0.0.0\",\"port\": \"8080\"}}}' mindsdb/mindsdb`\n\nNote that changing only one API value will not override the default values for other API keys. For e.g if you want to change the values for `mysql api` include the \nvalues for `http api` too since it will not be started. This should be resolved soon, \ncheck the progress here.\n\nKnown Issues\n#1\nIf you experience any issues related to MKL or your training process does not\ncomplete, please add the `MKL_SERVICE_FORCE_INTEL` environment variable,\nas below.\n`bash\ndocker run -e MKL_SERVICE_FORCE_INTEL=1 -p 47334:47334 -p 47335:47335 mindsdb/mindsdb`\n\nWhat's next? We recommend you follow one of our tutorials or learn more about the\nMindsDB Database.",
    "tag": "mindsdb"
  },
  {
    "title": "Before You Start",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/setup/self-hosted/pip/linux.mdx",
    "content": "\ntitle: Setup for Linux via pip\nsidebarTitle: pip on Linux\n\n\n\nBefore You Start\nThere are some points that you should consider before jumping into the\ninstallation. Please have a look at them below.\nPip and Python Versions\nDue to some of our dependencies having issues with the latest versions of Python\n3.9.x, we suggest using Python 3.7.x or 3.8.x versions for now. We are\nworking on Python 3.9.x to be supported soon.\nTo successfully install MindsDB, use Python 64-bit version. Also, make sure\nthat Python >= 3.7 and pip >= 19.3. You can check the pip and python\nversions by running the `pip --version` and\n`python --version` commands.\nPlease note that depending on your environment and installed pip and python\npackages, you might have to use pip3 instead of pip or python3.x\ninstead of py. For example, `pip3 install mindsdb` instead of\n`pip install mindsdb`.\nHow to Avoid Dependency Issues\nInstall MindsDB in a virtual environment using pip to avoid dependency\nissues.\nOr you could try to install MindsDB with\nAnaconda and run the\ninstallation from the Anaconda prompt.\nHow to Avoid Common Errors\nMindsDB requires around 3 GB of free disk space to install all of its\ndependencies. Make sure to allocate min. 3 GB of disk space to avoid the\n`IOError: [Errno 28] No space left on device while installing MindsDB` error.\nBefore anything, activate your virtual environment where your MindsDB is\ninstalled. It is to avoid the `No module named mindsdb` error.\nUsing the Python venv Module\n\nCreate a new virtual environment called `mindsdb`:\n\n`bash\n   python -m venv mindsdb`\nNow, activate it:\n`bash\n   source mindsdb/bin/activate`\n\nOnce inside the virtual environment, run the command below to mitigate the\n   dependency issues:\n\n`bash\n   pip install --upgrade pip setuptools wheel`\n\nInstall MindsDB:\n\n`bash\n   pip install mindsdb`\n\nVerify MindsDB installation:\n\n`bash\n   pip freeze`\nYou should see a list of installed packages including but not limited to the\n   following:\n`bash\n   ...\n   alembic==1.7.7\n   aniso8601==9.0.1\n   appdirs==1.4.4\n   lightgbm==3.3.0\n   lightwood==22.4.1.0\n   MindsDB==22.4.5.0\n   mindsdb-datasources==1.8.2\n   mindsdb-sql==0.3.3\n   mindsdb-streams==0.0.5\n   ...`\nUsing Anaconda\nHere, you need either Anaconda\nor Conda installed on\nyour machine.\n\nOpen Anaconda prompt and create a new virtual environment:\n\n`bash\n   conda create -n mindsdb`\nNow, activate it:\n`bash\n   conda activate mindsdb`\n\nOnce inside the virtual environment, run the command below to mitigate the\n   dependency issues:\n\n`bash\n   pip install --upgrade pip setuptools wheel`\n\nInstall MindsDB:\n\n`bash\n   pip install mindsdb`\n\nVerify MindsDB installation:\n\n`bach\n   conda list`\nYou should see a list of installed packages including but not limited to the\n   following:\n`bash\n   ...\n   alembic==1.7.7\n   aniso8601==9.0.1\n   appdirs==1.4.4\n   lightgbm==3.3.0\n   lightwood==22.4.1.0\n   MindsDB==22.4.5.0\n   mindsdb-datasources==1.8.2\n   mindsdb-sql==0.3.3\n   mindsdb-streams==0.0.5\n   ...`\nFurther Issues?\nYou can try to replicate your issue using the\nDocker setup.\nAlso, please create an issue with detailed description in the\nMindsDB GitHub repository so we can\nhelp you. Usually, we review issues and respond within a few hours.\n\nWhat's next? We recommend you follow one of our tutorials or learn more\n  about the MindsDB Database.",
    "tag": "mindsdb"
  },
  {
    "title": "Before You Start",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/setup/self-hosted/pip/source.mdx",
    "content": "\ntitle: Setup for Source Code via pip\nsidebarTitle: pip from Source\n\nThis section describes how to deploy MindsDB from the source code. It is the preferred way to use MindsDB if you want to contribute to our code or debug MindsDB.\nBefore You Start\nThere are some points that you should consider before jumping into the\ninstallation. Please have a look at them below.\nPip and Python Versions\nDue to some of our dependencies having issues with the latest versions of Python\n3.9.x, we suggest using Python 3.7.x or 3.8.x versions for now. We are\nworking on Python 3.9.x to be supported soon.\nTo successfully install MindsDB, use Python 64-bit version. Also, make sure\nthat Python >= 3.7 and pip >= 19.3. You can check the pip and python\nversions by running the `pip --version` and\n`python --version` commands.\nPlease note that depending on your environment and installed pip and python\npackages, you might have to use pip3 instead of pip or python3.x\ninstead of py. For example, `pip3 install mindsdb` instead of\n`pip install mindsdb`.\nHow to Avoid Dependency Issues\nInstall MindsDB in a virtual environment using pip to avoid dependency\nissues.\nHow to Avoid Common Errors\nMindsDB requires around 3 GB of free disk space to install all of its\ndependencies. Make sure to allocate min. 3 GB of disk space to avoid the\n`IOError: [Errno 28] No space left on device while installing MindsDB` error.\nBefore anything, activate your virtual environment where your MindsDB is\ninstalled. It is to avoid the `No module named mindsdb` error.\nThe `ImportError: No module named {dependency name}` error may occur if you skip\nstep 3 of the Installation section. Make sure you install all\nof the prerequisites.\nIf you encounter the `This site can\u2019t be reached. 127.0.0.1 refused to connect.`\nerror, please check the MindsDB server console to see if the server is still in\nthe `starting` phase. But if the server has started and you still get this\nerror, please report it on our\nGitHub repository.\nInstallation\n\nClone the MindsDB repository:\n\n`bash\n   git clone git@github.com:mindsdb/mindsdb.git`\n\nCreate a new virtual environment called `mindsdb-venv`:\n\n`bash\n   python -m venv mindsdb-venv`\nNow, activate it:\n`bash\n   source mindsdb-venv/bin/activate`\n\nInstall MindsDB prerequisites:\n\n`bash\n   cd mindsdb && pip install -r requirements.txt`\n\nInstall MindsDB:\n\n`bash\n   python setup.py develop`\n\nVerify MindsDB installation by starting the MindsDB server:\n\n`bash\n   python -m mindsdb`\n\nNow, you can access the following:\n\n\n`bash MindsDB Studio\nhttp://127.0.0.1:47334/`\n`bash MindsDB using MySQL\nmysql -h 127.0.0.1 --port 3306 -u mindsdb -p`\n\nFurther Issues?\nYou can try to use\nDocker setup in case you are expiriencing issues using pip.\nAlso, please create an issue with detailed description in the\nMindsDB GitHub repository so we can\nhelp you. Usually, we review issues and respond within a few hours.\n\nWhat's next? We recommend you follow one of our tutorials or learn more about the\nMindsDB Database.",
    "tag": "mindsdb"
  },
  {
    "title": "Before You Start",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/setup/self-hosted/pip/windows.mdx",
    "content": "\ntitle: Setup for Windows via pip\nsidebarTitle: pip on Windows\n\nBefore You Start\nThere are some points that you should consider before jumping into the\ninstallation. Please have a look at them below.\nPip and Python Versions\nDue to some of our dependencies having issues with the latest versions of Python\n3.9.x, we suggest using Python 3.7.x or 3.8.x versions for now. We are\nworking on Python 3.9.x to be supported soon.\nTo successfully install MindsDB, use Python 64-bit version. Also, make sure\nthat Python >= 3.7 and pip >= 19.3. You can check the pip and python\nversions by running the `pip --version` and\n`python --version` commands.\nPlease note that depending on your environment and installed pip and python\npackages, you might have to use pip3 instead of pip or python3.x\ninstead of py. For example, `pip3 install mindsdb` instead of\n`pip install mindsdb`.\nHow to Avoid Dependency Issues\nInstall MindsDB in a virtual environment using pip to avoid dependency\nissues.\nOr you could try to install MindsDB with\nAnaconda and run the\ninstallation from the Anaconda prompt.\nInstalling torch or torchvision\nIf the installation fails when installing torch or torchvision, try to\ninstall them manually by following the instructions on their\nofficial website.\nUsing the Python venv Module\n\nCreate a new virtual environment called `mindsdb`:\n\n`bash\n   py -m venv mindsdb`\nNow, activate it:\n`bash\n   .\\mindsdb\\Scripts\\activate.bat`\n\nOnce inside the virtual environment, run the command below to mitigate the\n   dependency issues:\n\n`bash\n   pip install --upgrade pip setuptools wheel`\n\nInstall MindsDB:\n\n`bash\n   pip install mindsdb`\n\nVerify MindsDB installation:\n\n`bash\n   pip freeze`\nYou should see a list of installed packages including but not limited to the\n   following:\n`bash\n   ...\n   alembic==1.7.7\n   aniso8601==9.0.1\n   appdirs==1.4.4\n   lightgbm==3.3.0\n   lightwood==22.4.1.0\n   MindsDB==22.4.5.0\n   mindsdb-datasources==1.8.2\n   mindsdb-sql==0.3.3\n   mindsdb-streams==0.0.5\n   ...`\nUsing Anaconda\nHere, you need either Anaconda\nor Conda installed on\nyour machine.\n\nOpen Anaconda prompt and create a new virtual environment:\n\n`bash\n   conda create -n mindsdb`\nNow, activate it:\n`bash\n   conda activate mindsdb`\n\nOnce inside the virtual environment, run the command below to mitigate the\n   dependency issues:\n\n`bash\n   pip install --upgrade pip setuptools wheel`\n\nInstall MindsDB:\n\n`bash\n   pip install mindsdb`\n\nVerify MindsDB installation:\n\n`bash\n   conda list`\nYou should see a list of installed packages including but not limited to the\n   following:\n`bash\n   ...\n   alembic==1.7.7\n   aniso8601==9.0.1\n   appdirs==1.4.4\n   lightgbm==3.3.0\n   lightwood==22.4.1.0\n   MindsDB==22.4.5.0\n   mindsdb-datasources==1.8.2\n   mindsdb-sql==0.3.3\n   mindsdb-streams==0.0.5\n   ...`\nFurther Issues?\nYou can try to replicate your issue using the\nDocker setup.\nAlso, please create an issue with detailed description in the\nMindsDB GitHub repository so we can\nhelp you. Usually, we review issues and respond within a few hours.\n\nWhat's next? We recommend you follow one of our tutorials or learn more\n  about the MindsDB Database.",
    "tag": "mindsdb"
  },
  {
    "title": "Before You Start",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/setup/self-hosted/pip/macos.mdx",
    "content": "\ntitle: Setup for MacOS via pip\nsidebarTitle: pip on MacOS\n\nBefore You Start\nThere are some points that you should consider before jumping into the\ninstallation. Please have a look at them below.\nPip and Python Versions\nDue to some of our dependencies having issues with the latest versions of Python\n3.9.x, we suggest using Python 3.7.x or 3.8.x versions for now. We are\nworking on Python 3.9.x to be supported soon.\nTo successfully install MindsDB, use Python 64-bit version. Also, make sure\nthat Python >= 3.7 and pip >= 19.3. You can check the pip and python\nversions by running the `pip --version` and\n`python --version` commands.\nPlease note that depending on your environment and installed pip and python\npackages, you might have to use pip3 instead of pip or python3.x\ninstead of py. For example, `pip3 install mindsdb` instead of\n`pip install mindsdb`.\nHow to Avoid Dependency Issues\nInstall MindsDB in a virtual environment using pip to avoid dependency\nissues.\nOr you could try to install MindsDB with\nAnaconda and run the\ninstallation from the Anaconda prompt.\nHow to Avoid Common Errors\nMindsDB requires around 3 GB of free disk space to install all of its\ndependencies. Make sure to allocate min. 3 GB of disk space to avoid the\n`IOError: [Errno 28] No space left on device while installing MindsDB` error.\nBefore anything, activate your virtual environment where your MindsDB is\ninstalled. It is to avoid the `No module named mindsdb` error.\nThe\n`numpy.distutils.system_info.NotFoundError: No lapack/blas resources found. Note: Accelerate is no longer supported.`\nerror might be caused by some dependencies not working with Python 3.9 version.\nFor now, please use Python 3.7.x or Python 3.8.x, as mentioned before.\nSome users can get `OSError: dlopen Library not loaded 'libomp.dylib'`. Please make sure you have installed `libomp` and run the export commands.\n`brew install libomp\nexport LDFLAGS=\"-L/usr/local/opt/libomp/lib\"\nexport CPPFLAGS=\"-I/usr/local/opt/libomp/include\"`\nUsing the Python venv Module\n\nCreate a new virtual environment called `mindsdb`:\n\n`bash\n   python -m venv mindsdb`\nNow, activate it:\n`bash\n   source mindsdb/bin/activate`\n\nOnce inside the virtual environment, run the command below to mitigate the\n   dependency issues:\n\n`bash\n   pip install --upgrade pip setuptools wheel`\n\nInstall MindsDB:\n\n`bash\n   pip install mindsdb`\n\nVerify MindsDB installation:\n\n`bash\n   pip freeze`\nYou should see a list of installed packages including but not limited to the\n   following:\n`bash\n   ...\n   alembic==1.7.7\n   aniso8601==9.0.1\n   appdirs==1.4.4\n   lightgbm==3.3.0\n   lightwood==22.4.1.0\n   MindsDB==22.4.5.0\n   mindsdb-datasources==1.8.2\n   mindsdb-sql==0.3.3\n   mindsdb-streams==0.0.5\n   ...`\nUsing Anaconda\nHere, you need either Anaconda\nor Conda installed on\nyour machine.\n\nOpen Anaconda prompt and create a new virtual environment:\n\n`bash\n   conda create -n mindsdb`\nNow, activate it:\n`bash\n   conda activate mindsdb`\n\nOnce inside the virtual environment, run the command below to mitigate the\n   dependency issues:\n\n`bash\n   pip install --upgrade pip setuptools wheel`\n\nInstall MindsDB:\n\n`bash\n   pip install mindsdb`\n\nVerify MindsDB installation:\n\n`bach\n   conda list`\nYou should see a list of installed packages including but not limited to the\n   following:\n`bash\n   ...\n   alembic==1.7.7\n   aniso8601==9.0.1\n   appdirs==1.4.4\n   lightgbm==3.3.0\n   lightwood==22.4.1.0\n   MindsDB==22.4.5.0\n   mindsdb-datasources==1.8.2\n   mindsdb-sql==0.3.3\n   mindsdb-streams==0.0.5\n   ...`\nFurther Issues?\nYou can try to replicate your issue using the\nDocker setup.\nAlso, please create an issue with detailed description in the\nMindsDB GitHub repository so we can\nhelp you. Usually, we review issues and respond within a few hours.\n\nWhat's next? We recommend you follow one of our tutorials or learn more about the\nMindsDB Database.",
    "tag": "mindsdb"
  },
  {
    "title": "How to Bring the Hugging Face Model to MindsDB",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/custom-model/huggingface.mdx",
    "content": "\ntitle: MindsDB and HuggingFace\nsidebarTitle: HuggingFace\n\nHugging Face facilitates building, training, and deploying ML models. Now you can create Hugging Face models within MindsDB.\n\nUsing Local Installation of MindsDB\n\n\n```Please note that if you use local installation of MindsDB, instead of MindsDB Cloud, you should install `transformers==4.21.0` to be able to use the Hugging Face models.\n```\n\n\n\nHow to Bring the Hugging Face Model to MindsDB\nWe use the CREATE MODEL statement to bring the Hugging Face models to MindsDB.\nLet's go through some sample models.\n\nPlease note that the examples presented here use SQL. To see how to create Hugging Face models in Mongo database using MQL, check out this example on sentiment classification.\n\nModel 1: Spam Classifier\nHere is an example of a binary classification. The model determines whether a text string is a spam or not.\n`sql\nCREATE MODEL mindsdb.spam_classifier\nPREDICT PRED\nUSING\n  engine = 'huggingface',\n  task = 'text-classification',\n  model_name = 'mrm8488/bert-tiny-finetuned-sms-spam-detection',\n  input_column = 'text_spammy',\n  labels = ['ham', 'spam'];`\nOn execution, we get:\n`sql\nQuery successfully completed`\nBefore querying for predictions, we should verify the status of the `spam_classifier` model.\n`sql\nSELECT *\nFROM mindsdb.models\nWHERE name = 'spam_classifier';`\nOn execution, we get:\n`sql\n+---------------+-------+--------+--------+-------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|NAME           |PROJECT|STATUS  |ACCURACY|PREDICT|UPDATE_STATUS|MINDSDB_VERSION|ERROR |SELECT_DATA_QUERY|TRAINING_OPTIONS                                                                                                                                                                                               |\n+---------------+-------+--------+--------+-------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|spam_classifier|mindsdb|complete|[NULL]  |PRED   |up_to_date   |22.10.2.1      |[NULL]|[NULL]           |{'target': 'PRED', 'using': {'engine': 'huggingface', 'task': 'text-classification', 'model_name': 'mrm8488/bert-tiny-finetuned-sms-spam-detection', 'input_column': 'text_spammy', 'labels': ['ham', 'spam']}}|\n+---------------+-------+--------+--------+-------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+`\nOnce the status is `complete`, we can query for predictions.\n`sql\nSELECT h.*, t.text_spammy AS input_text\nFROM example_db.demo_data.hf_test AS t\nJOIN mindsdb.spam_classifier AS h;`\nOn execution, we get:\n`sql\n+----+---------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|PRED|PRED_explain                                             |input_text                                                                                                                                                       |\n+----+---------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|spam|{'spam': 0.9051626920700073, 'ham': 0.09483727067708969} |Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's      |\n|ham |{'ham': 0.9380123615264893, 'spam': 0.061987683176994324}|Nah I don't think he goes to usf, he lives around here though                                                                                                    |\n|spam|{'spam': 0.9064534902572632, 'ham': 0.09354648739099503} |WINNER!! As a valued network customer you have been selected to receivea \u00a3900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.    |\n+----+---------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+`\nModel 2: Sentiment Classifier\nHere is an example of a multi-value classification. The model determines a sentiment of a text string, where possible values are `negative`, `neutral`, and `positive`.\n`sql\nCREATE MODEL mindsdb.sentiment_classifier\nPREDICT sentiment\nUSING\n  engine = 'huggingface',\n  task = 'text-classification',\n  model_name = 'cardiffnlp/twitter-roberta-base-sentiment',\n  input_column = 'text_short',\n  labels = ['negative', 'neutral', 'positive'];`\nOn execution, we get:\n`sql\nQuery successfully completed`\nBefore querying for predictions, we should verify the status of the `sentiment_classifier` model.\n`sql\nSELECT *\nFROM mindsdb.models\nWHERE name = 'sentiment_classifier';`\nOn execution, we get:\n`sql\n+--------------------+-------+--------+--------+---------+-------------+---------------+------+-----------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|NAME                |PROJECT|STATUS  |ACCURACY|PREDICT  |UPDATE_STATUS|MINDSDB_VERSION|ERROR |SELECT_DATA_QUERY|TRAINING_OPTIONS                                                                                                                                                                                                                  |\n+--------------------+-------+--------+--------+---------+-------------+---------------+------+-----------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|sentiment_classifier|mindsdb|complete|[NULL]  |sentiment|up_to_date   |22.10.2.1      |[NULL]|[NULL]           |{'target': 'sentiment', 'using': {'engine': 'huggingface', 'task': 'text-classification', 'model_name': 'cardiffnlp/twitter-roberta-base-sentiment', 'input_column': 'text_short', 'labels': ['negative', 'neutral', 'positive']}}|\n+--------------------+-------+--------+--------+---------+-------------+---------------+------+-----------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+`\nOnce the status is `complete`, we can query for predictions.\n`sql\nSELECT h.*, t.text_short AS input_text\nFROM example_db.demo_data.hf_test AS t\nJOIN mindsdb.sentiment_classifier AS h;`\nOn execution, we get:\n`sql\n+---------+----------------------------------------------------------------------------------------------------+-------------------+\n|sentiment|sentiment_explain                                                                                   |input_text         |\n+---------+----------------------------------------------------------------------------------------------------+-------------------+\n|negative |{'negative': 0.9679920077323914, 'neutral': 0.02736542373895645, 'positive': 0.0046426113694906235} |I hate tacos       |\n|positive |{'positive': 0.7607280015945435, 'neutral': 0.2332666665315628, 'negative': 0.006005281116813421}   |I want to dance    |\n|positive |{'positive': 0.9835041761398315, 'neutral': 0.014900505542755127, 'negative': 0.0015953202964738011}|Baking is the best |\n+---------+----------------------------------------------------------------------------------------------------+-------------------+`\nModel 3: Zero-Shot Classifier\nHere is an example of a zero-shot classification. The model determines to which of the defined categories a text string belongs.\n`sql\nCREATE MODEL mindsdb.zero_shot_tcd\nPREDICT topic\nUSING\n  engine = 'huggingface',\n  task = 'zero-shot-classification',\n  model_name = 'facebook/bart-large-mnli',\n  input_column = 'text_short',\n  candidate_labels = ['travel', 'cooking', 'dancing'];`\nOn execution, we get:\n`sql\nQuery successfully completed`\nBefore querying for predictions, we should verify the status of the `zero_shot_tcd` model.\n`sql\nSELECT *\nFROM mindsdb.models\nWHERE name = 'zero_shot_tcd';`\nOn execution, we get:\n`sql\n+-------------+-------+--------+--------+--------+-------------+---------------+------+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|NAME         |PROJECT|STATUS  |ACCURACY|PREDICT |UPDATE_STATUS|MINDSDB_VERSION|ERROR |SELECT_DATA_QUERY|TRAINING_OPTIONS                                                                                                                                                                                                         |\n+-------------+-------+--------+--------+--------+-------------+---------------+------+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|zero_shot_tcd|mindsdb|complete|[NULL]  |topic   |up_to_date   |22.10.2.1      |[NULL]|[NULL]           |{'target': 'topic', 'using': {'engine': 'huggingface', 'task': 'zero-shot-classification', 'model_name': 'facebook/bart-large-mnli', 'input_column': 'text_short', 'candidate_labels': ['travel', 'cooking', 'dancing']}}|\n+-------------+-------+--------+--------+--------+-------------+---------------+------+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+`\nOnce the status is `complete`, we can query for predictions.\n`sql\nSELECT h.*, t.text_short AS input_text\nFROM example_db.demo_data.hf_test AS t\nJOIN mindsdb.zero_shot_tcd AS h;`\nOn execution, we get:\n`sql\n+-------+--------------------------------------------------------------------------------------------------+-------------------+\n|topic  |topic_explain                                                                                     |input_text         |\n+-------+--------------------------------------------------------------------------------------------------+-------------------+\n|cooking|{'cooking': 0.7530364990234375, 'travel': 0.1607145369052887, 'dancing': 0.08624900877475739}     |I hate tacos       |\n|dancing|{'dancing': 0.9746809601783752, 'travel': 0.015539299696683884, 'cooking': 0.009779711253941059}  |I want to dance    |\n|cooking|{'cooking': 0.9936348795890808, 'travel': 0.0034196735359728336, 'dancing': 0.0029454431496560574}|Baking is the best |\n+-------+--------------------------------------------------------------------------------------------------+-------------------+`\nModel 4: Translation\nHere is an example of a translation. The model gets an input string in English and translates it into French.\n`sql\nCREATE MODEL mindsdb.translator_en_fr\nPREDICT translated\nUSING\n  engine = 'huggingface',\n  task = 'translation',\n  model_name = 't5-base',\n  input_column = 'text_short',\n  lang_input = 'en',\n  lang_output = 'fr';`\nOn execution, we get:\n`sql\nQuery successfully completed`\nBefore querying for predictions, we should verify the status of the `translator_en_fr` model.\n`sql\nSELECT *\nFROM mindsdb.models\nWHERE name = 'translator_en_fr';`\nOn execution, we get:\n`sql\n+----------------+-------+--------+--------+----------+-------------+---------------+------+-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|NAME            |PROJECT|STATUS  |ACCURACY|PREDICT   |UPDATE_STATUS|MINDSDB_VERSION|ERROR |SELECT_DATA_QUERY|TRAINING_OPTIONS                                                                                                                                                                   |\n+----------------+-------+--------+--------+----------+-------------+---------------+------+-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|translator_en_fr|mindsdb|complete|[NULL]  |translated|up_to_date   |22.10.2.1      |[NULL]|[NULL]           |{'target': 'translated', 'using': {'engine': 'huggingface', 'task': 'translation', 'model_name': 't5-base', 'input_column': 'text_short', 'lang_input': 'en', 'lang_output': 'fr'}}|\n+----------------+-------+--------+--------+----------+-------------+---------------+------+-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+`\nOnce the status is `complete`, we can query for predictions.\n`sql\nSELECT h.*, t.text_short AS input_text\nFROM example_db.demo_data.hf_test AS t\nJOIN mindsdb.translator_en_fr AS h;`\nOn execution, we get:\n`sql\n+-------------------------------+-------------------+\n|translated                     |input_text         |\n+-------------------------------+-------------------+\n|Je d\u00e9teste les tacos           |I hate tacos       |\n|Je veux danser                 |I want to dance    |\n|La boulangerie est la meilleure|Baking is the best |\n+-------------------------------+-------------------+`\nModel 5: Summarisation\nHere is an example of a summarisation.\n`sql\nCREATE MODEL mindsdb.summarizer_10_20\nPREDICT text_summary\nUSING\n  engine = 'huggingface',\n  task = 'summarization',\n  model_name = 'sshleifer/distilbart-cnn-12-6',\n  input_column = 'text_long',\n  min_output_length = 10,\n  max_output_length = 20;`\nOn execution, we get:\n`sql\nQuery successfully completed`\nBefore querying for predictions, we should verify the status of the `summarizer_10_20` model.\n`sql\nSELECT *\nFROM mindsdb.models\nWHERE name = 'summarizer_10_20';`\nOn execution, we get:\n`sql\n+----------------+-------+--------+--------+------------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|NAME            |PROJECT|STATUS  |ACCURACY|PREDICT     |UPDATE_STATUS|MINDSDB_VERSION|ERROR |SELECT_DATA_QUERY|TRAINING_OPTIONS                                                                                                                                                                                                     |\n+----------------+-------+--------+--------+------------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|summarizer_10_20|mindsdb|complete|[NULL]  |text_summary|up_to_date   |22.10.2.1      |[NULL]|[NULL]           |{'target': 'text_summary', 'using': {'engine': 'huggingface', 'task': 'summarization', 'model_name': 'sshleifer/distilbart-cnn-12-6', 'input_column': 'text_long', 'min_output_length': 10, 'max_output_length': 20}}|\n+----------------+-------+--------+--------+------------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+`\nOnce the status is `complete`, we can query for predictions.\n`sql\nSELECT h.*, t.text_long AS input_text\nFROM example_db.demo_data.hf_test AS t\nJOIN mindsdb.summarizer_10_20 AS h;`\nOn execution, we get:\n`sql\n+--------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|text_summary                                                                                                  |input_text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n+--------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|A taco is a traditional Mexican food consisting of a small hand-sized corn- or                                |A taco is a traditional Mexican food consisting of a small hand-sized corn- or wheat-based tortilla topped with a filling. The tortilla is then folded around the filling and eaten by hand. A taco can be made with a variety of fillings, including beef, pork, chicken, seafood, beans, vegetables, and cheese, allowing for great versatility and variety.                                                                                                                                                                                                                                                                                                                                              |\n|Dance is a performing art form consisting of sequences of movement, either improvised or purposefully selected|Dance is a performing art form consisting of sequences of movement, either improvised or purposefully selected. This movement has aesthetic and often symbolic value.[nb 1] Dance can be categorized and described by its choreography, by its repertoire of movements, or by its historical period or place of origin.                                                                                                                                                                                                                                                                                                                                                                                     |\n|Baking is a method of preparing food that uses dry heat, typically in an oven                                 |Baking is a method of preparing food that uses dry heat, typically in an oven, but can also be done in hot ashes, or on hot stones. The most common baked item is bread but many other types of foods can be baked. Heat is gradually transferred from the surface of cakes, cookies, and pieces of bread to their center. As heat travels through, it transforms batters and doughs into baked goods and more with a firm dry crust and a softer center. Baking can be combined with grilling to produce a hybrid barbecue variant by using both methods simultaneously, or one after the other. Baking is related to barbecuing because the concept of the masonry oven is similar to that of a smoke pit.|\n+--------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+`\nHuggingFace + MindsDB Models Library\nText Classification\nSpam\nLet's create a model.\n`sql\nCREATE MODEL mindsdb.hf_spam\nPREDICT PRED\nUSING\n engine = 'huggingface',\n task = 'text-classification',\n model_name = 'mariagrandury/roberta-base-finetuned-sms-spam-detection',\n input_column = 'text',\n labels = ['spam', 'ham'];`\nAnd check its status.\n`sql\nSELECT *\nFROM mindsdb.models \nWHERE name = 'hf_spam';`\nOnce the status is `complete`, we can query for predictions.\n`sql\nSELECT *\nFROM mindsdb.hf_spam\nWHERE text = 'I like you. I love you.';`\nOn evecution, we get:\n`sql\n+----+--------------------------------------------------------+-----------------------+\n|PRED|PRED_explain                                            |text                   |\n+----+--------------------------------------------------------+-----------------------+\n|spam|{\"ham\":0.00020051795581821352,\"spam\":0.9997995495796204}|I like you. I love you.|\n+----+--------------------------------------------------------+-----------------------+`\nSentiment\nLet's create a model.\n`sql\nCREATE MODEL mindsdb.hf_sentiment\nPREDICT PRED\nUSING\n engine = 'huggingface',\n task = 'text-classification',\n model_name = 'cardiffnlp/twitter-roberta-base-sentiment',\n input_column = 'text',\n labels = ['neg', 'neu', 'pos'];`\nAnd check its status.\n`sql\nSELECT *\nFROM mindsdb.models \nWHERE name = 'hf_sentiment';`\nOnce the status is `complete`, we can query for predictions.\n`sql\nSELECT *\nFROM mindsdb.hf_sentiment\nWHERE text = 'I like you. I love you.';`\nOn evecution, we get:\n`sql\n+----+--------------------------------------------------------------------------------+-----------------------+\n|PRED|PRED_explain                                                                    |text                   |\n+----+--------------------------------------------------------------------------------+-----------------------+\n|pos |{\"neg\":0.003046575468033552,\"neu\":0.021965451538562775,\"pos\":0.9749879240989685}|I like you. I love you.|\n+----+--------------------------------------------------------------------------------+-----------------------+`\nSentiment (Finance)\nLet's create a model.\n`sql\nCREATE MODEL mindsdb.hf_sentiment_finance\nPREDICT PRED\nUSING\n engine = 'huggingface',\n task = 'text-classification',\n model_name = 'ProsusAI/finbert',\n input_column = 'text';`\nAnd check its status.\n`sql\nSELECT *\nFROM mindsdb.models \nWHERE name = 'hf_sentiment_finance';`\nOnce the status is `complete`, we can query for predictions.\n`sql\nSELECT *\nFROM mindsdb.hf_sentiment_finance\nWHERE text = 'Stocks rallied and the British pound gained.';`\nOn evecution, we get:\n`sql\n+--------+-------------------------------------------------------------------------------------------+--------------------------------------------+\n|PRED    |PRED_explain                                                                               |text                                        |\n+--------+-------------------------------------------------------------------------------------------+--------------------------------------------+\n|positive|{\"negative\":0.0344734713435173,\"neutral\":0.06716493517160416,\"positive\":0.8983616232872009}|Stocks rallied and the British pound gained.|\n+--------+-------------------------------------------------------------------------------------------+--------------------------------------------+`\nEmotions (6)\nLet's create a model.\n`sql\nCREATE MODEL mindsdb.hf_emotions_6\nPREDICT PRED\nUSING\n engine = 'huggingface',\n task = 'text-classification',\n model_name = 'j-hartmann/emotion-english-distilroberta-base',\n input_column = 'text';`\nAnd check its status.\n`sql\nSELECT *\nFROM mindsdb.models \nWHERE name = 'hf_emotions_6';`\nOnce the status is `complete`, we can query for predictions.\n`sql\nSELECT *\nFROM mindsdb.hf_emotions_6\nWHERE text = 'Oh Happy Day';`\nOn evecution, we get:\n`sql\n+----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+\n|PRED|PRED_explain                                                                                                                                                                                                   |text        |\n+----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+\n|joy |{\"anger\":0.0028446922078728676,\"disgust\":0.0009613594156689942,\"fear\":0.0007112706662155688,\"joy\":0.7692911624908447,\"neutral\":0.037753619253635406,\"sadness\":0.015293814241886139,\"surprise\":0.17314413189888}|Oh Happy Day|\n+----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+`\nToxicity\nLet's create a model.\n`sql\nCREATE MODEL mindsdb.hf_toxicity\nPREDICT PRED\nUSING\n engine = 'huggingface',\n task = 'text-classification',\n model_name = 'SkolkovoInstitute/roberta_toxicity_classifier',\n input_column = 'text';`\nAnd check its status.\n`sql\nSELECT *\nFROM mindsdb.models \nWHERE name = 'hf_toxicity';`\nOnce the status is `complete`, we can query for predictions.\n`sql\nSELECT *\nFROM mindsdb.hf_toxicity\nWHERE text = 'I like you. I love you.';`\nOn evecution, we get:\n`sql\n+-------+-------------------------------------------------------------+-----------------------+\n|PRED   |PRED_explain                                                 |text                   |\n+-------+-------------------------------------------------------------+-----------------------+\n|neutral|{\"neutral\":0.9999547004699707,\"toxic\":0.00004535282641882077}|I like you. I love you.|\n+-------+-------------------------------------------------------------+-----------------------+`\nESG (6)\nLet's create a model.\n`sql\nCREATE MODEL mindsdb.hf_esg_6\nPREDICT PRED\nUSING\n engine = 'huggingface',\n task = 'text-classification',\n model_name = 'yiyanghkust/finbert-esg',\n input_column = 'text';`\nAnd check its status.\n`sql\nSELECT *\nFROM mindsdb.models \nWHERE name = 'hf_esg_6';`\nOnce the status is `complete`, we can query for predictions.\n`sql\nSELECT * FROM  mindsdb.hf_esg_6\nWHERE text = 'Rhonda has been volunteering for several years for a variety of charitable community programs.';`\nOn evecution, we get:\n`sql\n+------+---------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+\n|PRED  |PRED_explain                                                                                                                     |text                                                                                          |\n+------+---------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+\n|Social|{\"Environmental\":0.0034267122391611338,\"Governance\":0.004729956854134798,\"None\":0.001239194767549634,\"Social\":0.9906041026115417}|Rhonda has been volunteering for several years for a variety of charitable community programs.|\n+------+---------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+`\nESG (26)\nLet's create a model.\n`sql\nCREATE MODEL mindsdb.hf_esg_26\nPREDICT PRED\nUSING\n engine = 'huggingface',\n task = 'text-classification',\n model_name = 'yiyanghkust/finbert-esg',\n input_column = 'text';`\nAnd check its status.\n`sql\nSELECT *\nFROM mindsdb.models \nWHERE name = 'hf_esg_26';`\nOnce the status is `complete`, we can query for predictions.\n`sql\nSELECT *\nFROM mindsdb.hf_esg_26\nWHERE text = 'We believe it is essential to establish validated conflict-free sources of 3TG within the Democratic Republic of the Congo (the \u201cDRC\u201d) and adjoining countries (together, with the DRC, the \u201cCovered Countries\u201d), so that these minerals can be procured in a way that contributes to economic growth and development in the region. To aid in this effort, we have established a conflict minerals policy and an internal team to implement the policy.';`\nOn evecution, we get:\n`sql\n+------+-----------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|PRED  |PRED_explain                                                                                                                 |text                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n+------+-----------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|Social|{\"Environmental\":0.2031959593296051,\"Governance\":0.08251894265413284,\"None\":0.050893042236566544,\"Social\":0.6633920073509216}|We believe it is essential to establish validated conflict-free sources of 3TG within the Democratic Republic of the Congo (the \u201cDRC\u201d) and adjoining countries (together, with the DRC, the \u201cCovered Countries\u201d), so that these minerals can be procured in a way that contributes to economic growth and development in the region. To aid in this effort, we have established a conflict minerals policy and an internal team to implement the policy.|\n+------+-----------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+`\nHate Speech\nLet's create a model.\n`sql\nCREATE MODEL mindsdb.hf_hate\nPREDICT PRED\nUSING\n engine = 'huggingface',\n task = 'text-classification',\n model_name = 'Hate-speech-CNERG/bert-base-uncased-hatexplain',\n input_column = 'text';`\nAnd check its status.\n`sql\nSELECT *\nFROM mindsdb.models \nWHERE name = 'hf_hate';`\nOnce the status is `complete`, we can query for predictions.\n`sql\nSELECT *\nFROM mindsdb.hf_hate\nWHERE text = 'I like you. I love you.';`\nOn evecution, we get:\n`sql\n+------+-----------------------------------------------------------------------------------------------+-----------------------+\n|PRED  |PRED_explain                                                                                   |text                   |\n+------+-----------------------------------------------------------------------------------------------+-----------------------+\n|normal|{\"hate speech\":0.03551718592643738,\"normal\":0.7747423648834229,\"offensive\":0.18974047899246216}|I like you. I love you.|\n+------+-----------------------------------------------------------------------------------------------+-----------------------+`\nCrypto Buy Signals\nLet's create a model.\n`sql\nCREATE MODEL mindsdb.hf_crypto\nPREDICT PRED\nUSING\n engine = 'huggingface',\n task = 'text-classification',\n model_name = 'ElKulako/cryptobert',\n input_column = 'text';`\nAnd check its status.\n`sql\nSELECT *\nFROM mindsdb.models \nWHERE name = 'hf_crypto';`\nOnce the status is `complete`, we can query for predictions.\n`sql\nSELECT *\nFROM mindsdb.hf_crypto\nWHERE text = 'BTC is killing it right now';`\nOn evecution, we get:\n`sql\n+-------+------------------------------------------------------------------------------------------+---------------------------+\n|PRED   |PRED_explain                                                                              |text                       |\n+-------+------------------------------------------------------------------------------------------+---------------------------+\n|Bullish|{\"Bearish\":0.0002816587220877409,\"Bullish\":0.559426486492157,\"Neutral\":0.4402918517589569}|BTC is killing it right now|\n+-------+------------------------------------------------------------------------------------------+---------------------------+`\nUS Political Party\nLet's create a model.\n`sql\nCREATE MODEL mindsdb.hf_us_party\nPREDICT PRED\nUSING\n engine = 'huggingface',\n task = 'text-classification',\n model_name = 'm-newhauser/distilbert-political-tweets',\n input_column = 'text';`\nAnd check its status.\n`sql\nSELECT *\nFROM mindsdb.models \nWHERE name = 'hf_us_party';`\nOnce the status is `complete`, we can query for predictions.\n`sql\nSELECT *\nFROM mindsdb.hf_us_party\nWHERE text = 'This pandemic has shown us clearly the vulgarity of our healthcare system. Highest costs in the world, yet not enough nurses or doctors. Many millions uninsured, while insurance company profits soar. The struggle continues. Healthcare is a human right. Medicare for all.';`\nOn evecution, we get:\n`sql\n+--------+-------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|PRED    |PRED_explain                                                       |text                                                                                                                                                                                                                                                                          |\n+--------+-------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|Democrat|{\"Democrat\":0.9999973773956299,\"Republican\":0.00000261212517216336}|This pandemic has shown us clearly the vulgarity of our healthcare system. Highest costs in the world, yet not enough nurses or doctors. Many millions uninsured, while insurance company profits soar. The struggle continues. Healthcare is a human right. Medicare for all.|\n+--------+-------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+`\nQuestion Detection\nLet's create a model.\n`sql\nCREATE MODEL mindsdb.hf_question\nPREDICT PRED\nUSING\n engine = 'huggingface',\n task = 'text-classification',\n model_name = 'shahrukhx01/bert-mini-finetune-question-detection',\n input_column = 'text',\n labels = ['question', 'query'];`\nAnd check its status.\n`sql\nSELECT *\nFROM mindsdb.models \nWHERE name = 'hf_question';`\nOnce the status is `complete`, we can query for predictions.\n`sql\nSELECT *\nFROM mindsdb.hf_question\nWHERE text = 'Where can I buy electronics in London';`\nOn evecution, we get:\n`sql\n+-----+--------------------------------------------------------------+-------------------------------------+\n|PRED |PRED_explain                                                  |text                                 |\n+-----+--------------------------------------------------------------+-------------------------------------+\n|query|{\"query\":0.9997773766517639,\"question\":0.00022261829872149974}|Where can I buy electronics in London|\n+-----+--------------------------------------------------------------+-------------------------------------+`\nIndustry\nLet's create a model.\n`sql\nCREATE MODEL mindsdb.hf_industry\nPREDICT PRED\nUSING\n engine = 'huggingface',\n task = 'text-classification',\n model_name = 'sampathkethineedi/industry-classification',\n input_column = 'text';`\nAnd check its status.\n`sql\nSELECT *\nFROM mindsdb.models \nWHERE name = 'hf_industry';`\nOnce the status is `complete`, we can query for predictions.\n`sql\nSELECT *\nFROM mindsdb.hf_industry\nWHERE text = 'Low latency is one of our best cloud features';`\nOn evecution, we get:\n`sql\n+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------+\n|PRED            |PRED_explain                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |text                                         |\n+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------+\n|Systems Software|{\"Advertising\":0.000006795735771447653,\"Aerospace & Defense\":0.00001537964453746099,\"Apparel Retail\":5.350161131900677e-7,\"Apparel, Accessories & Luxury Goods\":0.000002604161181807285,\"Application Software\":0.009111878462135792,\"Asset Management & Custody Banks\":0.00003155150625389069,\"Auto Parts & Equipment\":0.000015504940165556036,\"Biotechnology\":6.533917940032552e-8,\"Building Products\":7.348538133555849e-8,\"Casinos & Gaming\":0.000013775999832432717,\"Commodity Chemicals\":0.0000010432338513055583,\"Communications Equipment\":0.000019887389498762786,\"Construction & Engineering\":0.000001826199536480999,\"Construction Machinery & Heavy Trucks\":0.000009827364920056425,\"Consumer Finance\":0.0000018292046206624946,\"Data Processing & Outsourced Services\":0.0000010666744856280275,\"Diversified Metals & Mining\":0.000006960767223063158,\"Diversified Support Services\":0.000016824227714096196,\"Electric Utilities\":0.000003896044290740974,\"Electrical Components & Equipment\":0.000001626394464437908,\"Electronic Equipment & Instruments\":0.00003863943129545078,\"Environmental & Facilities Services\":0.000736175337806344,\"Gold\":0.00002220332135038916,\"Health Care Equipment\":4.6927588925882446e-8,\"Health Care Facilities\":7.432880124724761e-7,\"Health Care Services\":6.929263918209472e-7,\"Health Care Supplies\":2.1007431882935634e-7,\"Health Care Technology\":0.000003907185146090342,\"Homebuilding\":3.903339234057057e-7,\"Hotels, Resorts & Cruise Lines\":6.0527639789143e-7,\"Human Resource & Employment Services\":5.48697983049351e-7,\"IT Consulting & Other Services\":0.0000723653138265945,\"Industrial Machinery\":7.230253231682582e-7,\"Integrated Telecommunication Services\":2.8266379104024963e-7,\"Interactive Media & Services\":0.00003454017496551387,\"Internet & Direct Marketing Retail\":0.000003871373337460682,\"Internet Services & Infrastructure\":0.0007196652004495263,\"Investment Banking & Brokerage\":0.0000040634336073708255,\"Leisure Products\":0.000002158361439796863,\"Life Sciences Tools & Services\":0.000002861268058040878,\"Movies & Entertainment\":0.000007286199888767442,\"Oil & Gas Equipment & Services\":0.000004376991455501411,\"Oil & Gas Exploration & Production\":0.000005569149834627751,\"Oil & Gas Refining & Marketing\":0.000012647416951949708,\"Oil & Gas Storage & Transportation\":0.000005852583853993565,\"Packaged Foods & Meats\":0.0000011130315442642313,\"Personal Products\":0.00000970239307207521,\"Pharmaceuticals\":0.0000037546726616710657,\"Property & Casualty Insurance\":0.000006116194072092185,\"Real Estate Operating Companies\":0.00001882187461887952,\"Regional Banks\":0.0000011669454806906288,\"Research & Consulting Services\":0.000024276219846797176,\"Restaurants\":8.598511840318679e-7,\"Semiconductors\":0.0000021006283077440457,\"Specialty Chemicals\":0.000004160017397225602,\"Specialty Stores\":2.644004553076229e-7,\"Steel\":0.0000013566890402216814,\"Systems Software\":0.9889177083969116,\"Technology Distributors\":0.00001339179198112106,\"Technology Hardware, Storage & Peripherals\":0.00004790363891515881,\"Thrifts & Mortgage Finance\":3.924862141957419e-7,\"Trading Companies & Distributors\":0.0000035233156268077437}|Low latency is one of our best cloud features|\n+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------+`\nZero-Shot Classification\nBart\nLet's create a model.\n`sql\nCREATE MODEL mindsdb.hf_zs_bart\nPREDICT PRED\nUSING\n engine = 'huggingface',\n task = 'zero-shot-classification',\n model_name = 'facebook/bart-large-mnli',\n input_column = 'text',\n candidate_labels = ['Books', 'Household', 'Clothing & Accessories', 'Electronics'];`\nAnd check its status.\n`sql\nSELECT *\nFROM mindsdb.models \nWHERE name = 'hf_zs_bart';`\nOnce the status is `complete`, we can query for predictions.\n`sql\nSELECT *\nFROM mindsdb.hf_zs_bart\nWHERE text = 'Paper Plane Design Framed Wall Hanging Motivational Office Decor Art Prints';`\nOn evecution, we get:\n`sql\n+---------+------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------+\n|PRED     |PRED_explain                                                                                                                              |text                                                                       |\n+---------+------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------+\n|Household|{\"Books\":0.1876104772090912,\"Clothing & Accessories\":0.08688066899776459,\"Electronics\":0.14785148203372955,\"Household\":0.5776574015617371}|Paper Plane Design Framed Wall Hanging Motivational Office Decor Art Prints|\n+---------+------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------+`\nTranslation\nEnglish to French (T5)\nLet's create a model.\n`sql\nCREATE MODEL mindsdb.hf_t5_en_fr\nPREDICT PRED\nUSING\n engine = 'huggingface',\n task = 'translation',\n model_name = 't5-base',\n input_column = 'text',\n lang_input = 'en',\n lang_output = 'fr';`\nAnd check its status.\n`sql\nSELECT *\nFROM mindsdb.models \nWHERE name = 'hf_t5_en_fr';`\nOnce the status is `complete`, we can query for predictions.\n`sql\nSELECT *\nFROM mindsdb.hf_t5_en_fr\nWHERE text = 'The monkey is on the branch';`\nOn evecution, we get:\n`sql\n+---------------------------+---------------------------+\n|PRED                       |text                       |\n+---------------------------+---------------------------+\n|Le singe est sur la branche|The monkey is on the branch|\n+---------------------------+---------------------------+`\nSummarization\nBart\nLet's create a model.\n`sql\nCREATE MODEL mindsdb.hf_bart_sum_20\nPREDICT PRED\nUSING\n engine = 'huggingface',\n task = 'summarization',\n model_name = 'sshleifer/distilbart-cnn-12-6',\n input_column = 'text',\n min_output_length = 5,\n max_output_length = 20;`\nAnd check its status.\n`sql\nSELECT *\nFROM mindsdb.models \nWHERE name = 'hf_bart_sum_20';`\nOnce the status is `complete`, we can query for predictions.\n`sql\nSELECT *\nFROM mindsdb.hf_bart_sum_20\nWHERE text = 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.';`\nOn evecution, we get:\n`sql\n+-------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|PRED                                                   |text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n+-------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|The tower is 324 metres (1,063 ft) tall, about the same|The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.|\n+-------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+`\nGoogle Pegasus\nLet's create a model.\n`sql\nCREATE MODEL mindsdb.hf_peg_sum_20\nPREDICT PRED\nUSING\n engine = 'huggingface',\n task = 'summarization',\n model_name = 'google/pegasus-xsum',\n input_column = 'text',\n min_output_length = 5,\n max_output_length = 20;`\nAnd check its status.\n`sql\nSELECT *\nFROM mindsdb.models \nWHERE name = 'hf_peg_sum_20';`\nOnce the status is `complete`, we can query for predictions.\n`sql\nSELECT *\nFROM mindsdb.hf_peg_sum_20\nWHERE text = 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.';`\nOn evecution, we get:\n```sql\n+------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|PRED                                            |text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n+------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|The Eiffel Tower is a landmark in Paris, France.|The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.|\n+------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+",
    "tag": "mindsdb"
  },
  {
    "title": "Simple Example of Logistic Regression",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/custom-model/ray-serve.mdx",
    "content": "\ntitle: MindsDB and Ray Serve\nsidebarTitle: Ray Serve\n\nRay Serve is a simple high-throughput model serving library that can wrap around your ML model.\nSimple Example of Logistic Regression\nIn this example, we train an external scikit-learn model to use for making\npredictions.\nCreating the Ray Serve Model\nLet's look at an actual model wrapped by a class that complies with the\nrequirements.\n```python\nimport ray\nfrom fastapi import Request, FastAPI\nfrom ray import serve\nimport time\nimport pandas as pd\nimport json\nfrom sklearn.linear_model import LogisticRegression\napp = FastAPI()\nray.init()\nserve.start(detached=True)\nasync def parse_req(request: Request):\n    data = await request.json()\n    target = data.get('target', None)\n    di = json.loads(data['df'])\n    df = pd.DataFrame(di)\n    return df, target\n@serve.deployment(route_prefix=\"/my_model\")\n@serve.ingress(app)\nclass MyModel:\n    @app.post(\"/train\")\n    async def train(self, request: Request):\n        df, target = await parse_req(request)\n        feature_cols = list(set(list(df.columns)) - set([target]))\n        self.feature_cols = feature_cols\n        X = df.loc[:, self.feature_cols]\n        Y = list(df[target])\n        self.model = LogisticRegression()\n        self.model.fit(X, Y)\n        return {'status': 'ok'}\n\n\n```@app.post(\"/predict\")\nasync def predict(self, request: Request):\n    df, _ = await parse_req(request)\n    X = df.loc[:, self.feature_cols]\n    predictions = self.model.predict(X)\n    pred_dict = {'prediction': [float(x) for x in predictions]}\n    return pred_dict\n```\n\n\nMyModel.deploy()\nwhile True:\n    time.sleep(1)\n```\nIt is important to have the `/train` and `/predict` endpoints.\nThe `/train` endpoint accepts two parameters to be sent via POST:\n\n`df` is a serialized dictionary that can be converted into a pandas dataframe.\n`target` is the name of the target column to be predicted.\n\nIt returns a JSON object containing the `status` key and the `ok` value.\nThe `/predict` endpoint requires one parameter to be sent via POST:\n\n`df` is a serialized dictionary that can be converted into a pandas dataframe.\n\nIt returns a dictionary containing the `prediction` key. It stores the\npredictions. Additional keys can be returned for confidence and confidence\nintervals.\nBringing the Ray Serve Model to MindsDB\nOnce you start the RayServe-wrapped model, you can create and train it in\nMindsDB.\n`sql\nCREATE MODEL mindsdb.byom_ray_serve\nFROM mydb (\n    SELECT number_of_rooms, initial_price, rental_price\n    FROM test_data.home_rentals\n)\nPREDICT number_of_rooms\nUSING\n    url.train = 'http://127.0.0.1:8000/my_model/train',\n    url.predict = 'http://127.0.0.1:8000/my_model/predict',\n    dtype_dict={\"number_of_rooms\": \"categorical\", \"initial_price\": \"integer\", \"rental_price\": \"integer\"},\n    format='ray_server';`\nNow, you can fetch predictions using the standard MindsDB syntax. Follow the\nguide on the SELECT statement to learn more.\nYou can directly pass input data in the `WHERE` clause to get a single\nprediction.\n`sql\nSELECT *\nFROM byom_ray_serve\nWHERE initial_price=3000\nAND rental_price=3000;`\nOr you can `JOIN` the model wth a data table to get bulk predictions.\n`sql\nSELECT tb.number_of_rooms, t.rental_price\nFROM mydb.test_data.home_rentals AS t\nJOIN mindsdb.byom_ray_serve AS tb\nWHERE t.rental_price > 5300;`\n\nLimit for POST Requests\nPlease note that if your model is behind a reverse proxy like nginx, you might\nhave to increase the maximum limit for POST requests in order to receive the\ntraining data. MindsDB can send as much as you'd like - it has been\nstress-tested with over a billion rows.\n\nExample of Keras NLP Model\nHere, we consider a\nnatural language processing (NLP) task\nwhere we want to train a neural network using Keras to\ndetect if a tweet is related to a natural disaster, such as fires, earthquakes,\netc. Please download\nthis dataset to follow the\nexample.\nCreating the Ray Serve Model\nWe create a Ray Serve service that wraps around the\nKaggle NLP Model\nthat can be trained and used for making predictions.\n```python\nimport re\nimport time\nimport json\nimport string\nimport requests\nfrom collections import Counter, defaultdict\nimport ray\nfrom ray import serve\nimport gensim\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom nltk.util import ngrams\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom fastapi import Request, FastAPI\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\nfrom tensorflow.keras.initializers import Constant\nfrom tensorflow.keras.optimizers import Adam\napp = FastAPI()\nstop = set(stopwords.words('english'))\nasync def parse_req(request: Request):\n    data = await request.json()\n    target = data.get('target', None)\n    di = json.loads(data['df'])\n    df = pd.DataFrame(di)\n    return df, target\n@serve.deployment(route_prefix=\"/nlp_kaggle_model\")\n@serve.ingress(app)\nclass Model:\n    MAX_LEN = 100\n    GLOVE_DIM = 50\n    EPOCHS = 10\n\n\n```def __init__(self):\n    self.model = None\n\n@app.post(\"/train\")\nasync def train(self, request: Request):\n    df, target = await parse_req(request)\n\n    target_arr = df.pop(target).values\n    df = self.preprocess_df(df)\n    train_corpus = self.create_corpus(df)\n\n    self.embedding_dict = {}\n    with open('./glove.6B.50d.txt', 'r') as f:\n        for line in f:\n            values = line.split()\n            word = values[0]\n            vectors = np.asarray(values[1:], 'float32')\n            self.embedding_dict[word] = vectors\n    f.close()\n\n    self.tokenizer_obj = Tokenizer()\n    self.tokenizer_obj.fit_on_texts(train_corpus)\n\n    sequences = self.tokenizer_obj.texts_to_sequences(train_corpus)\n    tweet_pad = pad_sequences(sequences, maxlen=self.__class__.MAX_LEN, truncating='post', padding='post')\n    df = tweet_pad[:df.shape[0]]\n\n    word_index = self.tokenizer_obj.word_index\n    num_words = len(word_index) + 1\n    embedding_matrix = np.zeros((num_words, self.__class__.GLOVE_DIM))\n\n    for word, i in tqdm(word_index.items()):\n        if i > num_words:\n            continue\n\n        emb_vec = self.embedding_dict.get(word)\n        if emb_vec is not None:\n            embedding_matrix[i] = emb_vec\n\n    self.model = Sequential()\n    embedding = Embedding(num_words,\n                          self.__class__.GLOVE_DIM,\n                          embeddings_initializer=Constant(embedding_matrix),\n                          input_length=self.__class__.MAX_LEN,\n                          trainable=False)\n    self.model.add(embedding)\n    self.model.add(SpatialDropout1D(0.2))\n    self.model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n    self.model.add(Dense(1, activation='sigmoid'))\n\n    optimizer = Adam(learning_rate=1e-5)\n    self.model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n\n    X_train, X_test, y_train, y_test = train_test_split(df, target_arr, test_size=0.15)\n    self.model.fit(X_train, y_train, batch_size=4, epochs=self.__class__.EPOCHS, validation_data=(X_test, y_test), verbose=2)\n\n    return {'status': 'ok'}\n\n@app.post(\"/predict\")\nasync def predict(self, request: Request):\n    df, _ = await parse_req(request)\n\n    df = self.preprocess_df(df)\n    test_corpus = self.create_corpus(df)\n\n    sequences = self.tokenizer_obj.texts_to_sequences(test_corpus)\n    tweet_pad = pad_sequences(sequences, maxlen=self.__class__.MAX_LEN, truncating='post', padding='post')\n    df = tweet_pad[:df.shape[0]]\n\n    y_pre = self.model.predict(df)\n    y_pre = np.round(y_pre).astype(int).flatten().tolist()\n    sub = pd.DataFrame({'target': y_pre})\n\n    pred_dict = {'prediction': [float(x) for x in sub['target'].values]}\n    return pred_dict\n\ndef preprocess_df(self, df):\n    df = df[['text']]\n    df['text'] = df['text'].apply(lambda x: self.remove_URL(x))\n    df['text'] = df['text'].apply(lambda x: self.remove_html(x))\n    df['text'] = df['text'].apply(lambda x: self.remove_emoji(x))\n    df['text'] = df['text'].apply(lambda x: self.remove_punct(x))\n    return df\n\ndef remove_URL(self, text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'', text)\n\ndef remove_html(self, text):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'', text)\n\ndef remove_punct(self, text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\n\ndef remove_emoji(self, text):\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef create_corpus(self, df):\n    corpus = []\n    for tweet in tqdm(df['text']):\n        words = [word.lower() for word in word_tokenize(tweet) if ((word.isalpha() == 1) & (word not in stop))]\n        corpus.append(words)\n    return corpus\n```\n\n\nif name == 'main':\n\n\n```ray.init()\nserve.start(detached=True)\n\nModel.deploy()\n\nwhile True:\n    time.sleep(1)\n```\n\n\n```\nNow, we need access to the training data. For that, we create a table called\n`nlp_kaggle_train` to load the\ndataset that the original model\nuses. The `nlp_kaggle_train` table contains the following columns:\n`sql\nid INT,\nkeyword VARCHAR(255),\nlocation VARCHAR(255),\ntext VARCHAR(5000),\ntarget INT`\nPlease note that the specifics of the schema/table and how to ingest the CSV\ndata vary depending on your database.\nBringing the Ray Serve Model to MindsDB\nNow, we can create and train this custom model in MindsDB.\n`sql\nCREATE MODEL mindsdb.byom_ray_serve_nlp\nFROM maria (\n    SELECT text, target\n    FROM test.nlp_kaggle_train\n) PREDICT target\nUSING\n    url.train = 'http://127.0.0.1:8000/nlp_kaggle_model/train',\n    url.predict = 'http://127.0.0.1:8000/nlp_kaggle_model/predict',\n    dtype_dict={\"text\": \"rich_text\", \"target\": \"integer\"},\n    format='ray_server';`\nThe training process takes some time, considering that this model is a neural\nnetwork rather than a simple logistic regression.\nYou can check the model status using this query:\n`sql\nSELECT *\nFROM mindsdb.models\nWHERE name='byom_ray_serve_nlp';`\nOnce the status of the predictor has a value of `trained`, you can fetch\npredictions using the standard MindsDB syntax. Follow the guide on the\nSELECT statement to learn more.\n`sql\nSELECT *\nFROM mindsdb.byom_ray_serve_nlp\nWHERE text='The tsunami is coming, seek high ground';`\nThe expected output of the query above is `1`.\n`sql\nSELECT *\nFROM mindsdb.byom_ray_serve_nlp\nWHERE text='This is lovely dear friend';`\nThe expected output of the query above is `0`.\n\nWrong Results?\nIf your results do not match this example, try training the model for a longer\namount of epochs.",
    "tag": "mindsdb"
  },
  {
    "title": "How to Bring the OpenAI Model to MindsDB",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/custom-model/openai.mdx",
    "content": "\ntitle: MindsDB and OpenAI\nsidebarTitle: OpenAI\n\nMindsDB lets you create models that utilize features provided by OpenAI GPT-3.\nHow to Bring the OpenAI Model to MindsDB\nWe use the CREATE MODEL statement to bring the OpenAI models to MindsDB.\nLet's go through the available operation modes.\n\nPlease note that the examples presented here use SQL. To see how to create OpenAI models in Mongo database using MQL, check out this example on sentiment classification.\n\nOperation Mode 1: Answering Questions without Context\nHere is how to create a model that answers questions without context:\n`sql\nCREATE MODEL project_a.openai_test_a\nPREDICT answer\nUSING\n    engine = 'openai',\n    question_column = 'question',\n    model_name = 'model_name'\n    api_key = 'YOUR_OPENAI_API_KEY';`\nWhere:\n| Expression                   | Description                                                                                                                                                            |\n|------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| `project_a.openai_test_a`    | The model name is `openai_test_a` and it resides inside the `project_a` project. Learn more about MindsDB projects here.                              |\n| `answer`                     | It is the value to be predicted.                                                                                                                                       |\n| `engine`                     | The `openai` engine is used.                                                                                                                                           |\n| `question_column`            | It is the column that stores input data.                                                                                                                               |\n| `model_name`                 | Optional. By default, the `text-davinci-002` model is used. If you prefer to use a cheaper model or a model that was fine-tuned outside of MindsDB, use this parameter.|\n| `api_key`                    | Optional. If it is left blank, or the `env` value is passed, then MindsDB fetches the `OPENAI_API_KEY` environment variable value.                                     |\nLet's look at an example.\n`sql\nCREATE MODEL project_a.openai_test_a\nPREDICT answer\nUSING\n    engine = 'openai',\n    question_column = 'question';`\nOn execution, we get:\n`sql\nQuery successfully completed`\nNow we can query for answers.\n`sql\nSELECT question, answer\nFROM project_a.openai_test_a\nWHERE question = 'Where is Stockholm located?';`\nOn execution, we get:\n`sql\n+---------------------------+-------------------------------+\n|question                   |answer                         |\n+---------------------------+-------------------------------+\n|Where is Stockholm located?|Stockholm is located in Sweden.|\n+---------------------------+-------------------------------+`\nOperation Mode 2: Answering Questions with Context\nHere is how to create a model that answers questions with context:\n`sql\nCREATE MODEL project_a.openai_test_b\nPREDICT answer\nUSING\n    engine = 'openai',\n    question_column = 'question',\n    context_column = 'context',\n    api_key = 'YOUR_OPENAI_API_KEY';`\nWhere:\n| Expression                   | Description                                                                                                                          |\n|------------------------------|--------------------------------------------------------------------------------------------------------------------------------------|\n| `project_a.openai_test_b`    | The model name is `openai_test_b` and it resides inside the `project_a` project.                                                     |\n| `answer`                     | It is the value to be predicted.                                                                                                     |\n| `engine`                     | The `openai` engine is used.                                                                                                         |\n| `question_column`            | It is the column that stores input data being a question.                                                                            |\n| `context_column`             | It is the column that stores input data being a context.                                                                             |\n| `api_key`                    | Optional. If it is left blank, or the `env` value is passed, then MindsDB fetches the `OPENAI_API_KEY` environment variable value.   |\nLet's look at an example.\n`sql\nCREATE MODEL project_a.openai_test_b\nPREDICT answer\nUSING\n    engine = 'openai',\n    question_column = 'question',\n    context_column = 'context';`\nOn execution, we get:\n`sql\nQuery successfully completed`\nNow we can query for answers.\n`sql\nSELECT context, question, answer\nFROM project_a.openai_test_b\nWHERE context = 'Answer with a joke'\nAND question = 'How to cook soup?';`\nOn execution, we get:\n`sql\n+-------------------+------------------+---------------------------------------------------------+\n|context            |question          |answer                                                   |\n+-------------------+------------------+---------------------------------------------------------+\n|Answer with a joke |How to cook soup? |How do you cook soup? You put it in a pot and heat it up!|\n+-------------------+------------------+---------------------------------------------------------+`\nOperation Mode 3: Prompt Completion\nHere is how to create a model that offers the most flexible mode of operation. It answers any query provided in the `prompt_template` parameter.\n\nGood prompts are the key to getting great completions out of large language models like the ones that OpenAI offers. For best performance, we recommend you read their prompting guide before trying your hand at prompt templating.\n\n`sql\nCREATE MODEL project_a.openai_test_c\nPREDICT answer\nUSING\n    engine = 'openai',\n    prompt_template = 'input your query here',\n    max_tokens = 100,\n    temperature = 0.3,\n    api_key = 'YOUR_OPENAI_API_KEY';`\nWhere:\n| Expression                   | Description                                                                                                                          |\n|------------------------------|--------------------------------------------------------------------------------------------------------------------------------------|\n| `project_a.openai_test_c`    | The model name is `openai_test_c` and it resides inside the `project_a` project.                                                     |\n| `answer`                     | It is the value to be predicted.                                                                                                     |\n| `engine`                     | The `openai` engine is used.                                                                                                         |\n| `prompt_template`            | It is the column that stores a query to be answered. Please note that this parameter can be overriden at prediction time.            |\n| `max_tokens`                 | It defines the maximum token cost of the prediction. Please note that this parameter can be overriden at prediction time.            |\n| `temperature`                | It defines how risky the answers are. The value of `0` marks a well-defined answer, and the value of `0.9` marks a more creative answer. Please note that this parameter can be overriden at prediction time.|\n| `api_key`                    | Optional. If it is left blank, or the `env` value is passed, then MindsDB fetches the `OPENAI_API_KEY` environment variable value.   |\nLet's create a model:\n`sql\nCREATE MODEL project_a.openai_test_c\nPREDICT answer\nUSING\n    engine = 'openai',\n    prompt_template = 'Context: {{context}}. Question: {{question}}. Answer:',\n    max_tokens = 100,\n    temperature = 0.3;`\nLet's look at an example that uses parameters provided at model creation time.\n`sql\nSELECT context, question, answer\nFROM project_a.openai_test_c\nWHERE context = 'Answer accurately'\nAND question = 'How many planets exist in the solar system?';`\nOn execution, we get:\n`sql\n+-------------------+-------------------------------------------+----------------------------------------------+\n|context            |question                                   |answer                                        |\n+-------------------+-------------------------------------------+----------------------------------------------+\n|Answer accurately  |How many planets exist in the solar system?| There are eight planets in the solar system. |\n+-------------------+-------------------------------------------+----------------------------------------------+`\nNow let's look at an example that overrides parameters at prediction time.\n`sql\nSELECT instruction, answer\nFROM project_a.openai_test_c\nWHERE instruction = 'Speculate extensively'\nUSING\n    prompt_template = '{{instruction}}. What does Tom Hanks like?',\n    max_tokens = 100,\n    temperature = 0.5;`\nOn execution, we get:\n`sql\n+----------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|instruction           |answer                                                                                                                                                                                                                         |\n+----------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|Speculate extensively |Some people speculate that Tom Hanks likes to play golf, while others believe that he enjoys acting and directing. It is also speculated that he likes to spend time with his family and friends, and that he enjoys traveling.|\n+----------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+`\nExample\nLet's go through a sentiment classification example to understand better how to bring OpenAI models to MindsDB as AI tables.\n`sql\nCREATE MODEL mindsdb.sentiment_classifier                           \nPREDICT sentiment\nUSING\n  engine = 'openai',              \n  prompt_template = 'predict the sentiment of the text:{{review}} exactly as either positive or negative or neutral';`\nOn execution, we get:\n`sql\nQuery successfully completed`\nWhere:\n| Expressions      | Values                                                                                               |\n| ---------------- | ---------------------------------------------------------------------------------------------------- |\n| `project_name`   | `mindsdb`                                                                                            |\n| `predictor_name` | `sentiment_classifier`                                                                               |\n| `target_column`  | `sentiment`                                                                                          |\n| `engine`         | `openai`                                                                                             |\n| `prompt_template`| `predict the sentiment of the text:{{review}} exactly as either positive or negative or neutral`     |\n\nIn the `prompt_template` parameter, we use a placeholder for a text value that comes from the `review` column, that is, `text:{{review}}`.\n\nBefore querying for predictions, we should verify the status of the `sentiment_classifier` model.\n`sql\nSELECT *\nFROM mindsdb.models\nWHERE name = 'sentiment_classifier';`\nOn execution, we get:\n`sql\n+--------------------+------+-------+-------+--------+--------+---------+-------------+---------------+------+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------+-------+\n|NAME                |ENGINE|PROJECT|VERSION|STATUS  |ACCURACY|PREDICT  |UPDATE_STATUS|MINDSDB_VERSION|ERROR |SELECT_DATA_QUERY|TRAINING_OPTIONS                                                                                                                                       |TAG    |\n+--------------------+------+-------+-------+--------+--------+---------+-------------+---------------+------+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------+-------+\n|sentiment_classifier|openai|mindsdb|1      |complete|[NULL]  |sentiment|up_to_date   |22.12.4.3      |[NULL]|[NULL]           |{'target': 'sentiment', 'using': {'prompt_template': 'predict the sentiment of the text:{{review}} exactly as either positive or negative or neutral'}}|[NULL] |\n+--------------------+------+-------+-------+--------+--------+---------+-------------+---------------+------+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------+-------+`\nOnce the status is `complete`, we can query for predictions.\n`sql\nSELECT output.sentiment, input.review\nFROM example_db.demo_data.amazon_reviews AS input\nJOIN mindsdb.sentiment_classifier AS output\nLIMIT 3;`\n\n    Don't forget to create the `example_db` database before using one of its tables, like in the query above.\n\n\n``````sql\nCREATE DATABASE example_db\nWITH ENGINE = \"postgres\",\nPARAMETERS = {\n    \"user\": \"demo_user\",\n    \"password\": \"demo_password\",\n    \"host\": \"3.220.66.106\",\n    \"port\": \"5432\",\n    \"database\": \"demo\"\n    };\n```\n```\n\n\n\nOn execution, we get:\n```sql\n+----------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| sentiment                              | review                                                                                                                                                                                                                                                                                                                                                                            |\n+----------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| positive                               | Late gift for my grandson. He is very happy with it. Easy for him (9yo ).                                                                                                                                                                                                                                                                                                         |\n| The sentiment of the text is positive. | I'm not super thrilled with the proprietary OS on this unit, but it does work okay and does what I need it to do. Appearance is very nice, price is very good and I can't complain too much - just wish it were easier (or at least more obvious) to port new apps onto it. For now, it helps me see things that are too small on my phone while I'm traveling. I'm a happy buyer.|\n| positive                               | I purchased this Kindle Fire HD 8 was purchased for use by 5 and 8 yer old grandchildren. They basically use it to play Amazon games that you download.                                                                                                                                                                                                                           |\n+----------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+",
    "tag": "mindsdb"
  },
  {
    "title": "these are accessible inside the Model() wrapper",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/custom-model/mlflow.mdx",
    "content": "\ntitle: MindsDB and MLflow\nsidebarTitle: MLflow\n\nMLflow allows you to create, train, and serve machine learning models, apart\nfrom other features, such as organizing experiments, tracking metrics, and more.\nHere, we present two usage examples of the MLflow.\nSimple Example of Logistic Regression\nCurrently, there is no way to train an MLflow-wrapped model using the API. The\ntraining of the model takes place outside of MindsDB. The data must be pulled\nmanually, for example, with a script. It is a good idea to use an MLflow run or\nexperiment.\nCreating the MLflow Model\nWe start by writing a script that creates and trains the model. After that, the\nscript is saved using one of the saving methods offered by MLflow.\nHere, we use the model from\nthis simple tutorial\nand\nthe mlflow.sklearn.log_model method.\nPlease note that the model must be a scikit-learn model.\nOnce your model is trained, ensure that it is served and listens for input at a\nURL of your choice. Please note that your model may run on a different machine\nthan the one where MindsDB runs. Here, we assume the URL to be\n`http://localhost:5000/invocations`, as in the tutorial.\nLet's run the `train.py` script that provides us with the `<run-id>` value for\nthe model.\n`bash\n$ python examples/sklearn_logistic_regression/train.py\nScore: 0.666\nModel saved in run <run-id>`\nNow, let's execute the following command from the directory where the model\nresides, providing the `<run-id>` value.\n`bash\n$ mlflow models serve --model-uri runs:/<run-id>/model`\nWe're ready to move to MindsDB.\nBringing the MLflow Model to MindsDB\nWe execute the command below to create a predictor in MindsDB based on the\ncreated model.\n`sql\nCREATE MODEL mindsdb.byom_mlflow\nPREDICT `1`  -- `1` is the target column name\nUSING\n    url.predict='http://localhost:5000/invocations',\n    format='mlflow',\n    data_dtype={\"0\": \"integer\", \"1\": \"integer\"};`\nNow, you can fetch predictions using the standard MindsDB syntax. Follow the\nguide on the SELECT statement to learn more.\n`sql\nSELECT `1`\nFROM byom_mlflow\nWHERE `0`=2;`\nAdvanced Example of Keras NLP Model\nBefore we start, download\nthe natural language processing (NLP) dataset from Kaggle\nto reproduce the steps of this example.\nHere, we look at the best practices when your model needs custom data\npreprocessing, which is quite common.\nWe use the `mlflow.pyfunc` module to complete the following:\n\nSave the model using `mlflow.pyfunc.save_model`.\nSubclass `mlflow.pyfunc.PythonModel` to wrap the model in a way compatible\n  with MLflow that enables our custom inference logic to be called.\n\nCreating the MLflow Model\nIn the script that trains the model, like\nthis one or\nthat one, there should be a call to the\n`mlflow.pyfunc.save_model` function at the end. It is to save every produced\nartifact.\n`python\nmlflow.pyfunc.save_model(\n    path=\"nlp_kaggle\",\n    python_model=Model(),\n    conda_env=conda_env,\n    artifacts=artifacts\n)`\nHere, `artifacts` is a dictionary storing all the expected output after running\nthe training phase. In this case, we want both a model and a tokenizer to\npreprocess the input text. And `conda_env` specifies the environment in which\nyour model is executed when it is served in a self-contained conda environment.\nIt should include all the required packages and dependencies, like below:\n```python\nthese are accessible inside the Model() wrapper\nartifacts = {\n    'model': model_path,\n    'tokenizer_path': tokenizer_path,\n}\nspecs for an environment that is created when serving the model\nconda_env = {\n    'name': 'nlp_keras_env',\n    'channels': ['defaults'],\n    'dependencies': [\n        'python=3.8',\n        'pip',\n        {\n            'pip': [\n                'mlflow',\n                'tensorflow',\n                'cloudpickle',\n                'nltk',\n                'pandas',\n                'numpy',\n                'scikit-learn',\n                'tqdm',\n            ],\n        },\n    ],\n}\n```\nFinally, to store the model, you need to provide the wrapper class that loads\nall the produced artifacts into an accessible \"context\" and implements all the\nrequired inference logic.\n```python\nclass Model(mlflow.pyfunc.PythonModel):\n    def load_context(self, context):\n        # we use paths in the context to load everything\n        self.model_path = context.artifacts['model']\n        self.model = load_model(self.model_path)\n        with open(context.artifacts['tokenizer_path'], 'rb') as f:\n            self.tokenizer = pickle.load(f)\n\n\n```def predict(self, context, model_input):\n    # preprocess input, tokenize, pad, and call the model\n    df = preprocess_df(model_input)\n    corpus = create_corpus(df)\n    sequences = self.tokenizer.texts_to_sequences(corpus)\n    tweet_pad = pad_sequences(sequences,\n                              maxlen=MAX_LEN,\n                              truncating='post',\n                              padding='post')\n    df = tweet_pad[:df.shape[0]]\n\n    y_pre = self.model.predict(df)\n    y_pre = np.round(y_pre).astype(int).flatten().tolist()\n\n    return list(y_pre)\n```\n\n\n```\nHere, we load multiple artifacts and use them to guarantee the input data is in\nthe same format that was used for training. Ideally, you would abstract this\neven further into a single `preprocess` method that is called both at the\ntraining time and at the inference time.\nFinally, we start serving by going to the directory where you called the script\nabove and executing the `mlflow models serve --model-uri ./nlp_kaggle` command.\nBringing the MLflow Model to MindsDB\nWe execute the command below to create a predictor in MindsDB based on the\ncreated model.\n`sql\nCREATE MODEL mindsdb.byom_mlflow_nlp\nPREDICT `target`\nUSING\n    url.predict='http://localhost:5000/invocations',\n    format='mlflow',\n    dtype_dict={\"text\": \"rich text\", \"target\": \"binary\"};`\nNow, you can fetch predictions using the standard MindsDB syntax. Follow the\nguide on the SELECT statement to learn more.\nYou can directly pass input data in the `WHERE` clause to get a single\nprediction.\n`sql\nSELECT target\nFROM mindsdb.byom_mlflow_nlp\nWHERE text='The tsunami is coming, seek high ground';`\nOr you can `JOIN` the model with a data table to get bulk predictions.\nHere, ensure that the data table exists and the database it belongs to is\nconnected to your MindsDB instance.\n`sql\nSELECT ta.text,\n       tb.target AS predicted\nFROM db_byom.test.nlp_kaggle_test AS ta\nJOIN mindsdb.byom_mlflow_nlp AS tb;`\nFull Script\nFor your reference, here is the full script that trains and saves the model.\n```python\nimport re\nimport pickle\nimport string\nimport mlflow.pyfunc\nimport nltk\nimport tqdm\nimport sklearn\nimport tensorflow\nimport cloudpickle\nimport numpy as np\nimport pandas as pd\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.initializers import Constant\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.models import load_model\nstop = set(stopwords.words('english'))\nMAX_LEN = 100\nGLOVE_DIM = 50\nEPOCHS = 10\ndef preprocess_df(df):\n    df = df[['text']]\n    funcs = [remove_URL, remove_html, remove_emoji, remove_punct]\n    for fn in funcs:\n        df['text'] = df['text'].apply(lambda x: fn(x))\n    return df\ndef remove_URL(text):\n    url = re.compile(r'https?://\\S+|www.\\S+')\n    return url.sub(r'', text)\ndef remove_html(text):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'', text)\ndef remove_punct(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\ndef create_corpus(df):\n    corpus = []\n    for tweet in tqdm.tqdm(df['text']):\n        words = [word.lower() for word in word_tokenize(tweet) if ((word.isalpha() == 1) & (word not in stop))]\n        corpus.append(words)\n    return corpus\nclass Model(mlflow.pyfunc.PythonModel):\n\n\n```def load_context(self, context):\n\n    self.model_path = context.artifacts['model']\n    with open(context.artifacts['tokenizer_path'], 'rb') as f:\n        self.tokenizer = pickle.load(f)\n    self.model = load_model(self.model_path)\n\ndef predict(self, context, model_input):\n\n    df = preprocess_df(model_input)\n    corpus = create_corpus(df)\n    sequences = self.tokenizer.texts_to_sequences(corpus)\n    tweet_pad = pad_sequences(sequences, maxlen=MAX_LEN, truncating='post', padding='post')\n    df = tweet_pad[:df.shape[0]]\n\n    y_pre = self.model.predict(df)\n    y_pre = np.round(y_pre).astype(int).flatten().tolist()\n\n    return list(y_pre)\n```\n\n\nif name == 'main':\n    train_model = True\n\n\n```model_path = './'\ntokenizer_path = './tokenizer.pkl'\nrun_name = 'test_run'\nmlflow_pyfunc_model_path = \"nlp_kaggle\"\nmlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n\nif train_model:\n\n    # preprocess data\n    df = pd.read_csv('./train.csv')\n    target = df[['target']]\n    target_arr = target.values\n    df = preprocess_df(df)\n    train_corpus = create_corpus(df)\n\n    # load embeddings\n    embedding_dict = {}\n    with open('./glove.6B.50d.txt', 'r') as f:\n        for line in f:\n            values = line.split()\n            word = values[0]\n            vectors = np.asarray(values[1:], 'float32')\n            embedding_dict[word] = vectors\n    f.close()\n\n    # generate and save tokenizer\n    tokenizer_obj = Tokenizer()\n    tokenizer_obj.fit_on_texts(train_corpus)\n\n    with open(tokenizer_path, 'wb') as f:\n        pickle.dump(tokenizer_obj, f)\n\n    # tokenize and pad\n    sequences = tokenizer_obj.texts_to_sequences(train_corpus)\n    tweet_pad = pad_sequences(sequences, maxlen=MAX_LEN, truncating='post', padding='post')\n    df = tweet_pad[:df.shape[0]]\n\n    word_index = tokenizer_obj.word_index\n    num_words = len(word_index) + 1\n    embedding_matrix = np.zeros((num_words, GLOVE_DIM))\n\n    # fill embedding matrix\n    for word, i in tqdm.tqdm(word_index.items()):\n        if i > num_words:\n            continue\n\n        emb_vec = embedding_dict.get(word)\n        if emb_vec is not None:\n            embedding_matrix[i] = emb_vec\n\n    X_train, X_test, y_train, y_test = train_test_split(df, target_arr, test_size=0.15)\n\n    # generate model\n    model = Sequential()\n    embedding = Embedding(num_words,\n                          GLOVE_DIM,\n                          embeddings_initializer=Constant(embedding_matrix),\n                          input_length=MAX_LEN,\n                          trainable=False)\n    model.add(embedding)\n    model.add(SpatialDropout1D(0.2))\n    model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n    model.add(Dense(1, activation='sigmoid'))\n\n    optimizer = Adam(learning_rate=1e-5)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n\n    # train and save\n    model.fit(X_train, y_train, batch_size=4, epochs=EPOCHS, validation_data=(X_test, y_test), verbose=2)\n    model.save(model_path)\n\n# save in mlflow format\nartifacts = {\n    'model': model_path,\n    'tokenizer_path': tokenizer_path,\n}\n\nconda_env = {\n    'channels': ['defaults'],\n    'dependencies': [\n        'python=3.8',\n        'pip',\n        {\n            'pip': [\n                'mlflow',\n                'tensorflow=={}'.format(tensorflow.__version__),\n                'cloudpickle=={}'.format(cloudpickle.__version__),\n                'nltk=={}'.format(nltk.__version__),\n                'pandas=={}'.format(pd.__version__),\n                'numpy=={}'.format(np.__version__),\n                'scikit-learn=={}'.format(sklearn.__version__),\n                'tqdm=={}'.format(tqdm.__version__)\n            ],\n        },\n    ],\n    'name': 'nlp_keras_env'\n}\n\n# Save and register the MLflow Model\nwith mlflow.start_run(run_name=run_name) as run:\n    mlflow.pyfunc.save_model(\n        path=mlflow_pyfunc_model_path,\n        python_model=Model(),\n        conda_env=conda_env,\n        artifacts=artifacts)\n\n    result = mlflow.register_model(\n        f\"runs:/{run.info.run_id}/{mlflow_pyfunc_model_path}\",\n        f\"{mlflow_pyfunc_model_path}\"\n    )\n```\n\n",
    "tag": "mindsdb"
  },
  {
    "title": "What are Database Handlers?",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/contribute/data-handlers.mdx",
    "content": "\ntitle: Building a Database Handler\nsidebarTitle: Building a Database Handler\n\nIn this section, you'll find how to add new integrations/databases to MindsDB.\n\nPrerequisite\n\n\n```You should have the latest staging version of the MindsDB repository installed locally. Follow [this guide](/contribute/install/) to learn how to install MindsDB for development.\n```\n\n\n\nWhat are Database Handlers?\nDatabase handlers act as a bridge to any database. You use database handlers to create databases using the CREATE DATABASE command. So you can reach data from any database that has its handler implemented within MindsDB.\n\nML Handlers\n\n\n```To learn more about handlers and how to implement a machine learning (ML) handler, visit our [doc page here](/contribute/ml-handlers/).\n```\n\n\n\nCreating a Database Handler\nYou can create your own database handler within MindsDB by inheriting from the DatabaseHandler class.\nBy providing the implementation for some or all of the methods contained in the `DatabaseHandler` class, you can connect with the database of your choice.\nCore Methods\nApart from the `__init__()` method, there are seven core methods that must be implemented. We recommend checking actual examples in the codebase to get an idea of what goes into each of these methods, as they can change a bit depending on the nature of the system being integrated.\nLet's review the purpose of each method.\n| Method                 | Purpose                                                                                                                                                                                                |\n|------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| `connect()`            | It performs the necessary steps to connect to the underlying system.                                                                                                                                   |\n| `disconnect()`         | It gracefully closes connections established in the `connect()` method.                                                                                                                                |\n| `check_connection()`   | It evaluates if the connection is alive and healthy. This method is called frequently.                                                                                                                 |\n| `native_query()`       | It parses any native statement string and acts upon it (for example, raw SQL commands).                                                                                                              |\n| `query()`              | It takes a parsed SQL command in the form of an abstract syntax tree and executes it.                                                                                                                  |\n| `get_tables()`         | It lists and returns all the available tables. Each handler decides what a table means for the underlying system when interacting with it from the data layer. Typically, these are actual tables.   |\n| `get_columns()`        | It returns columns of a table registered in the handler with the respective data type.                                                                                                                 |\nAuthors can opt for adding private methods, new files and folders, or any combination of these to structure all the necessary work that will enable the core methods to work as intended.\n\nOther Common Methods\n\n\n```Under the `mindsdb.integrations.libs.utils` library, contributors can find various methods that may be useful while implementing new handlers.\n\nAlso, there are wrapper classes for the `DatabaseHandler` instances called [HandlerResponse](https://github.com/mindsdb/mindsdb/blob/staging/mindsdb/integrations/libs/response.py#L7) and [HandlerStatusResponse](https://github.com/mindsdb/mindsdb/blob/staging/mindsdb/integrations/libs/response.py#L32). You should use them to ensure proper output formatting.\n```\n\n\n\nImplementation\nEach database handler should inherit from the DatabaseHandler class.\nHere is a step-by-step guide:\n\n\nSetting the `name` class property:\nMindsDB uses it internally as the name of the handler.\nFor example, the `CREATE DATABASE` statement uses the handler's name.\n`sql\nCREATE DATABASE integration_name\nWITH ENGINE = 'postgres',         --- here, the handler's name is `postgres`\nPARAMETERS = {\n'host': '127.0.0.1',\n'user': 'root',\n'password': 'password'\n};`\n\n\nImplementing the `__init__()` method:\nThis method initializes the handler. The `connection_data` argument contains the `PARAMETERS` from the `CREATE DATABASE` statement, such as `user`, `password`, etc.\n`py\ndef __init__(self, name: str, connection_data: Optional[dict]):\n    \"\"\" constructor\n    Args:\n        name (str): the handler name\n    \"\"\"`\n\n\nImplementing the `connect()` method:\nThe `connect()` method sets up the connection.\n`py\ndef connect(self) -> HandlerStatusResponse:\n    \"\"\" Set up any connections required by the handler\n    Should return output of check_connection() method after attempting\n    connection. Should switch self.is_connected.\n    Returns:\n        HandlerStatusResponse\n    \"\"\"`\n\n\nImplementing the `disconnect()` method:\nThe `disconnect()` method closes the existing connection.\n`py\ndef disconnect(self):\n    \"\"\" Close any existing connections\n    Should switch self.is_connected.\n    \"\"\"`\n\n\nImplementing the `check_connection()` method:\nThe `check_connection()` method performs the health check for the connection.\n`py\ndef check_connection(self) -> HandlerStatusResponse:\n    \"\"\" Check connection to the handler\n    Returns:\n        HandlerStatusResponse\n    \"\"\"`\n\n\nImplementing the `native_query()` method:\nThe `native_query()` method runs commands of the native database language.\n`py\ndef native_query(self, query: Any) -> HandlerResponse:\n    \"\"\"Receive raw query and act upon it somehow.\n    Args:\n        query (Any): query in native format (str for sql databases,\n            dict for mongo, etc)\n    Returns:\n        HandlerResponse\n    \"\"\"`\n\n\nImplementing the `query()` method:\nThe query method runs parsed SQL commands.\n`py\ndef query(self, query: ASTNode) -> HandlerResponse:\n    \"\"\"Receive query as AST (abstract syntax tree) and act upon it somehow.\n    Args:\n        query (ASTNode): sql query represented as AST. May be any kind\n            of query: SELECT, INSERT, DELETE, etc\n    Returns:\n        HandlerResponse\n    \"\"\"`\n\n\nImplementing the `get_tables()` method:\nThe `get_tables()` method lists all the available tables.\n`py\ndef get_tables(self) -> HandlerResponse:\n    \"\"\" Return list of entities\n    Return list of entities that will be accesible as tables.\n    Returns:\n        HandlerResponse: shoud have same columns as information_schema.tables\n            (https://dev.mysql.com/doc/refman/8.0/en/information-schema-tables-table.html)\n            Column 'TABLE_NAME' is mandatory, other is optional.\n    \"\"\"`\n\n\nImplementing the `get_columns()` method:\nThe `get_columns()` method lists all columns of a specified table.\n`py\ndef get_columns(self, table_name: str) -> HandlerResponse:\n    \"\"\" Returns a list of entity columns\n    Args:\n        table_name (str): name of one of tables returned by self.get_tables()\n    Returns:\n        HandlerResponse: shoud have same columns as information_schema.columns\n            (https://dev.mysql.com/doc/refman/8.0/en/information-schema-columns-table.html)\n            Column 'COLUMN_NAME' is mandatory, other is optional. Hightly\n            recomended to define also 'DATA_TYPE': it should be one of\n            python data types (by default it str).\n    \"\"\"`\n\n\nThe `connection_args` Dictionary\nThe `connection_arg` dictionary contains all required arguments to establish the connection.\nHere is an example of the `connection_arg` dictionary from the MySQL handler.\n`py\nconnection_args = OrderedDict(\n    user = {\n        'type': ARG_TYPE.STR,\n        'description': 'The user name used to authenticate with the MySQL server.'\n    },\n    password = {\n        'type': ARG_TYPE.STR,\n        'description': 'The password to authenticate the user with the MySQL server.'\n    },\n    database = {\n        'type': ARG_TYPE.STR,\n        'description': 'The database name to use when connecting with the MySQL server.'\n    },\n    host = {\n        'type': ARG_TYPE.STR,\n        'description': 'The host name or IP address of the MySQL server. NOTE: use \\'127.0.0.1\\' instead of \\'localhost\\' to connect to local server.'\n    },\n    port = {\n        'type': ARG_TYPE.INT,\n        'description': 'The TCP/IP port of the MySQL server. Must be an integer.'\n    },\n    ssl = {\n        'type': ARG_TYPE.BOOL,\n        'description': 'Set it to False to disable ssl.'\n    },\n    ssl_ca = {\n        'type': ARG_TYPE.PATH,\n        'description': 'Path or URL of the Certificate Authority (CA) certificate file'\n    },\n    ssl_cert = {\n        'type': ARG_TYPE.PATH,\n        'description': 'Path name or URL of the server public key certificate file'\n    },\n    ssl_key = {\n        'type': ARG_TYPE.PATH,\n        'description': 'The path name or URL of the server private key file'\n    }\n)`\nThe `connection_args_example` Dictionary\nThe `connection_args_example` dictionary contains an example of all required arguments to establish the connection.\nHere is an example of the `connection_args_example` dictionary from the MySQL handler.\n`py\nconnection_args_example = OrderedDict(\n    host = '127.0.0.1',\n    port = 3306,\n    user = 'root',\n    password = 'password',\n    database = 'database'\n)`\nExporting All Required Variables\nThis is what should be exported in the `__init__.py` file:\n\nThe `Handler` class.\nThe `version` of the handler.\nThe `name` of the handler.\nThe `type` of the handler, either `DATA` handler or `ML` handler.\nThe `icon_path` to the file with the database icon.\nThe `title` of the handler or a short description.\nThe `description` of the handler.\nThe `connection_args` dictionary with the connection arguments.\nThe `connection_args_example` dictionary with an example of the connection arguments.\nThe `import_error` message that is used if the import of the `Handler` class fails.\n\nLet's look at an example of the `__init__.py` file.\n```py\n...\ntitle = 'Trino'\nversion = 0.1\ndescription = 'Integration for connection to TrinoDB'\nname = 'trino'\ntype = HANDLER_TYPE.DATA\nicon_path = 'icon.png'\nall = [\n    'Handler', 'version', 'name', 'type', 'title',\n    'description', 'connection_args_example', 'icon_path'\n]\n```\nCheck out our Database Handlers!\nTo see some integration handlers that are currently in use, we encourage you to check out the following handlers inside the MindsDB repository:\n\nMySQL\nPostgres\n",
    "tag": "mindsdb"
  },
  {
    "title": "How to Create an Issue",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/contribute/issues.mdx",
    "content": "\ntitle: Report an Issue \ud83d\udce2\nsidebarTitle: Report an Issue\n\nIf you want to report a bug, request a feature, suggest docs improvements, or propose a new integration, you can do that on the MindsDB GitHub issues page.\nBut before reporting a new issue, please make sure that it is not already there.\nHow to Create an Issue\nHere is how to get started when you want to create an issue in the MindsDB repository.\nGo to our GitHub issues page\nand click on the New issue button.\n\n\n\nNow you see the available issue types.\n\n\n\nLet's go through all of them one by one.\nReport a Bug \ud83d\udc1e\nHere, we choose `Report a bug` and click on the Get started button.\n\n\n\nNow, it's time to fill up the form.\n\nIt is vital to add a meaningful issue title.\n\n\n\n\n\nHere, you describe the current behavior. Please note that this field is mandatory. Below you can attach any videos or screenshots of the current behavior.\n\n\n\n\n\nIf you know what the expected behavior should be, you can add it here.\n\n\n\n\n\nIt is helpful for us if you add the steps you followed that led you to the\n   error.\n\n\n\n\n\nAny links, references, logs, screenshots, etc., are welcome!\n\n\n\n\n\n  Remeber, all contributions to our repository should follow the contributing\n  guidelines\n  and code of\n  conduct.\n\nThank you for reporting a bug! It helps us improve MindsDB for you and future\nusers.\nRequest a Feature \ud83d\ude80\nHere, we choose `Request a feature` and click on the Get started button.\n\n\n\nNow, it's time to fill up the form.\n\nIt is vital to add a meaningful issue title.\n\n\n\n\n\nHere, you describe the feature request along with the motivation for the proposed feature. Please note that this field is mandatory. Below you can attach any videos or screenshots.\n\n\n\n\n\nIf you know how the solution should look, you can add it here.\n\n\n\n\n\nAny links, references, logs, screenshots, etc., are welcome!\n\n\n\n\n\n  Remember, all contributions to our repository should follow the contributing\n  guidelines\n  and code of\n  conduct.\n\nThank you for submitting a feature request! It helps us improve MindsDB for you\nand future users.\nImprove our Docs \u270d\ufe0f\nHere, we choose `Improve our docs` and click on the Get started button.\n\n\n\nNow, it's time to fill up the form.\n\nIt is vital to add a meaningful issue title.\n\n\n\n\n\nHere, you describe what should be added or improved. Please note that this field is mandatory. Below you can attach any videos or screenshots.\n\n\n\n\n\nAny links, references, logs, screenshots, etc., are welcome!\n\n\n\n\n\n  Remember, all contributions to our repository should follow the contributing\n  guidelines\n  and code of\n  conduct.\n\nThank you for suggesting docs enhancements! It helps us improve MindsDB for you\nand future users.\nPropose a New Integration \ud83e\uddd1\u200d\ud83d\udd27\nHere is how to get started when you want to propose a new integration. Please note it can be a new database integrations or a new machine learning framework.\nWe choose `Propose a new integration` and click on the Get started button.\n\n\n\nNow, it's time to fill up the form.\n\nIt is vital to add a meaningful issue title.\n\n\n\n\n\nPlease make sure that this integration is not already implemented.\n\n\n\n\n\nHere, you descripbe the use case(s) solved by this integration.\n\n\n\n\n\nPlease provide a motivation for your integration proposal.\n\n\n\n\n\nGet creative and describe the implementation! You can use code, text, diagrams, etc.\n\n\n\n\n\nAny links, references, logs, screenshots, etc., are welcome!\n\n\n\n\n\n  Remember, all contributions to our repository should follow the contributing\n  guidelines\n  and code of\n  conduct.\n\nThank you for proposing a new integration! It helps us improve MindsDB for you\nand future users.\nReport a Security Vulnerability\nHere is how to get started when you want to report a security vulnerability. Please note that such issues are visible only to repository maintainers. Also, you will be credited if the advisory is published.\nWe choose `Report a security vulnerability` and click on the Get started button.\n\n\n\nNow, it's time to fill up the form.\n\nIt is vital to add a meaningful issue title.\n\n\n\n\n\nPlease follow the instructions to provide the best description.\n\n\n\n\n\nHere, you can choose one or more affected products along with the versions.\n\n\n\n\n\nPlease assign the severity of the issue. The Calculator feature can help you assess the severity.\n\n\n\n\n\nHere, you can assign one or more common weakness enumerators (CWE).\n\n\n\n\n\n  Remember, all contributions to our repository should follow the contributing\n  guidelines\n  and code of\n  conduct.\n\nThank you for reporting a security vulnerability! It helps us improve MindsDB for you\nand future users.\nIssue Review\n\nIssues are reviewed regularly, usually daily.\nDepending on the issue type, it will be labeled as `Bug` or `enhancement`.\n",
    "tag": "mindsdb"
  },
  {
    "title": "Installing MindsDB",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/contribute/install.mdx",
    "content": "\ntitle: MindsDB Installation for Development\nsidebarTitle: MindsDB Installation for Development\n\nIf you want to contribute to the development of MindsDB, you need to install\nit first.\nThere are a few installation options that you can choose as follows:\n\nfrom source,\nusing PyPi for Windows,\n  Linux, or\n  MacOS,\nusing Docker image.\n\nOur preferred MindsDB installation method for development is the installation\nfrom source.\nInstalling MindsDB\nHere, we recall the steps of MindsDB installation from source. You can either\nfollow the steps below or visit the provided link.\nBefore proceeding, make sure you have `Git` and `Python 3.7.x` or `Python 3.8.x`\ninstalled.\n\n\nFork the MindsDB repository from\n   GitHub.\n\n\nClone the fork locally:\n\n\n`bash\ngit clone git@github.com:YOUR_USERNAME/mindsdb.git`\n\nCreate a new virtual environment:\n\n`bash\npython -m venv mindsdb-venv`\n\nActivate the virtual environment:\n\n`bash\nsource mindsdb-venv/bin/activate`\n\nInstall dependencies:\n\n`bash\ncd mindsdb & pip install -r requirements.txt`\n\nInstall MindsDB:\n\n`bash\npython setup.py develop`\n\nStart MindsDB:\n\n`bash\npython -m mindsdb`\nIf everything works as expected, you should see the following message in the\nconsole:\n```\n...\n2022-06-28 16:21:46,942 - INFO - - GUI available at http://127.0.0.1:47334/\n2022-06-28 16:21:47,010 - INFO - Starting MindsDB Mysql proxy server on\ntcp://127.0.0.1:47335 2022-06-28 16:21:47,015 - INFO - Waiting for incoming\nconnections... mysql API: started on 47335 http API: started on 47334\n```\nWhat's Next?\nNow that you are all set, we recommend you check out our Tutorials and\nCommunity Tutorials sections, where you'll find various examples of\nregression, classification, and time series predictions with MindsDB.\nTo learn more about MindsDB itself, follow the guide on\nMindsDB database structure. Also, don't miss out on the\nremaining pages from the SQL API section, as they explain a common SQL\nsyntax with examples.",
    "tag": "mindsdb"
  },
  {
    "title": "Labels of Interest to Contributors",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/contribute/issue-labels.mdx",
    "content": "\ntitle: Issue Labels\nsidebarTitle: Issue Labels\n\nIn this section, we go through all issue labels available in the MindsDB GitHub repository.\nLabels of Interest to Contributors\nIf you are or plan to be a contributor, look at the issues marked with these labels.\nThe `first-timers-only` Label\nIf you haven't contributed to MindsDB before, you can search for the issues marked as `first-timers-only` to get started.\nPlease note that before we can accept your contribution to MindsDB, we ask you to sign our Contributor License Agreement.\nYou can find all the `first-timers-only` issues here.\nThe `good first issue` Label\nIssues marked as `good first issue` are good for newcomers.\nYou can find all the `good first issue` issues here.\nThe `hactoberfest` Label\nBy taking up an issue marked as `hactoberfest`, you can participate in the Hacktoberfest competition, which takes place every year in October.\nYou can find all the `hactoberfest` issues here.\nThe `help wanted` Label\nIssues marked as `help wanted` are for anybody who wants to contribute to MindsDB.\nYou can find all the `help wanted` issues here.\nThe `integration` Label\nThe `integration` label marks issues that require a contributor to implement a database or ML integration with MindsDB. If you take up such an issue, you can participate in the Integration Contest of MindsDB.\nYou can create an issue with your idea for integration by following the instructions here.\nYou can find all the `integration` issues here.\nOther Labels\nHere are other labels used in the MindsDB repository.\nThe `bug` Label\nThe `bug` label marks issues that describe what's currently not working.\nYou can report a bug by following the instructions here.\nThe `discussion` Label\nIf an issue is marked as `discussion`, it requires further discussion before it can be resolved.\nThe `documentation` Label\nThe `documentation` label marks issues related to our docs.\nYou can improve our docs by creating issues following the instructions here.\nThe `enhancement` Label\nAs MindsDB grows day by day, there are still things that require improvements. All issues suggesting enhancements to MindsDB are marked as `enhancement`.\nYou can request a feature by following the instructions here.\nThe `follow-up` Label\nIf an issue is marked as `follow-up`, it requires users' feedback before it can be resolved.\nThe `question` Label\nIf an issue is marked as `question`, it requires further information before it can be resolved.\nThe `user request` Label",
    "tag": "mindsdb"
  },
  {
    "title": "What are Machine Learning Handlers?",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/contribute/ml-handlers.mdx",
    "content": "\ntitle: Building a Machine Learning Handler\nsidebarTitle: Building an ML Handler\n\nIn this section, you'll find how to create new machine learning (ML) handlers within MindsDB.\n\nPrerequisite\n\n\n```You should have the latest staging version of the MindsDB repository installed locally. Follow [this guide](/contribute/install/) to learn how to install MindsDB for development.\n```\n\n\n\nWhat are Machine Learning Handlers?\nML handlers act as a bridge to any ML framework. You use ML handlers to create ML engines using the CREATE ML_ENGINE command. So you can expose ML models from any supported ML engine as an AI table.\n\nDatabase Handlers\n\n\n```To learn more about handlers and how to implement a database handler, visit our [doc page here](/contribute/data-handlers/).\n```\n\n\n\nCreating a Machine Learning Handler\nYou can create your own ML handler within MindsDB by inheriting from the BaseMLEngine class.\nBy providing the implementation for some or all of the methods contained in the `BaseMLEngine` class, you can connect with the machine learning library or framework of your choice.\nCore Methods\nApart from the `__init__()` method, there are five methods, of which two must be implemented. We recommend checking actual examples in the codebase to get an idea of what goes into each of these methods, as they can change a bit depending on the nature of the system being integrated.\nLet's review the purpose of each method.\n| Method            | Purpose                                                                              |\n|-------------------|--------------------------------------------------------------------------------------|\n| `create()`        | It creates a model inside the engine registry.                                       |\n| `predict()`       | It calls a model and returns prediction data.                                        |\n| `update()`        | Optional. It updates an existing model without resetting its internal structure.     |\n| `describe()`      | Optional. It provides global model insights.                                         |\n| `create_engine()` | Optional. It connects with external sources, such as REST API.                       |\nAuthors can opt for adding private methods, new files and folders, or any combination of these to structure all the necessary work that will enable the core methods to work as intended.\n\nOther Common Methods\n\n\n```Under the `mindsdb.integrations.libs.utils` library, contributors can find various methods that may be useful while implementing new handlers.\n\nAlso, there is a wrapper class for the `BaseMLEngine` instances called [BaseMLEngineExec](https://github.com/mindsdb/mindsdb/blob/staging/mindsdb/integrations/libs/ml_exec_base.py#L157). It is automatically deployed to take care of modifying the data responses into something that can be used alongside data handlers.\n```\n\n\n\nImplementation\nHere are the methods that must be implemented while inheriting from the BaseMLEngine class:\n\nThe create() method saves a model inside the engine registry for later usage.\n\n`py\ndef create(self, target: str, df: Optional[pd.DataFrame] = None, args: Optional[Dict] = None) -> None:\n        \"\"\"\n        Saves a model inside the engine registry for later usage.\n        Normally, an input dataframe is required to train the model.\n        However, some integrations may merely require registering the model instead of training, in which case `df` can be omitted.\n        Any other arguments required to register the model can be passed in an `args` dictionary.\n        \"\"\"`\n\nThe predict() method calls a model with an input dataframe and optionally, arguments to modify model's behaviour. This method returns a dataframe with the predicted values.\n\n`py\ndef predict(self, df: pd.DataFrame, args: Optional[Dict] = None) -> pd.DataFrame:\n        \"\"\"\n        Calls a model with some input dataframe `df`, and optionally some arguments `args` that may modify the model behavior.\n        The expected output is a dataframe with the predicted values in the target-named column.\n        Additional columns can be present, and will be considered row-wise explanations if their names finish with `_explain`.\n        \"\"\"`\nAnd here are the optional methods that you can implement alongside the mandatory ones if your ML framework allows it:\n\nThe update() method is used to update, fine-tune, or adjust an existing model without resetting its internal state.\n\n`py\ndef update(self, df: Optional[pd.DataFrame] = None, args: Optional[Dict] = None) -> None:\n        \"\"\"\n        Optional.\n        Used to update/fine-tune/adjust a pre-existing model without resetting its internal state (e.g. weights).\n        Availability will depend on underlying integration support, as not all ML models can be partially updated.\n        \"\"\"`\n\nThe describe() method provides global model insights, such as framework-level parameters used in training.\n\n`py\ndef describe(self, key: Optional[str] = None) -> pd.DataFrame:\n        \"\"\"\n        Optional.\n        When called, this method provides global model insights, e.g. framework-level parameters used in training.\n        \"\"\"`\n\nThe create_engine() method is used to connect with the external sources, such as REST API.\n\n`py\ndef create_engine(self, connection_args: dict):\n        \"\"\"\n        Optional.\n        Used to connect with external sources (e.g. a REST API) that the engine will require to use any other methods.\n        \"\"\"`\nCheck out our Machine Learning Handlers!\nTo see some ML handlers that are currently in use, we encourage you to check out the following ML handlers inside the MindsDB repository:\n\nLightwood\nHuggingFace\nLudwig\n",
    "tag": "mindsdb"
  },
  {
    "title": "Introduction",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/contribute/tutorials.mdx",
    "content": "\ntitle: How to Write a Tutorial\nsidebarTitle: Writing Tutorials\n\nThis section presents how to write a tutorial with MindsDB.\n\nContent of the Tutorial\nYou are free to create your content. However, you should include in your tutorial all the chapters listed here.\nThe tutorial should be a Markdown file, for example, `mindsdb-tutorial.mdx`. Here is the basic Markdown syntax.\n\nIntroduction\nHere you introduce the readers to the tutorial. You can describe what dataset\nyou use and what you predict.\nData Setup\nThis chapter contains an introduction to the dataset you use.\nConnecting the Data\nLet others follow your tutorial by providing information on where to get the\ndata from and how to connect it to MindsDB.\nUnderstanding the Data\nYou can briefly introduce the dataset you use.\nTraining a Predictor\nHere you use the CREATE MODEL command to create\na predictor.\nStatus of a Predictor\nThe next step is to check the status of a predictor. If its value is `complete`,\nyou can proceed to the next chapter.\nMaking Predictions\nUse the SELECT statement to query for prediction results.\nIt is good to present the output to the readers.\nMaking a Single Prediction\nIn the case of regression and classification predictors, you can make a single\nprediction. It is good to present the output to the readers.\nMaking Batch Predictions\nYou can make batch predictions using the JOIN clause for all\npredictor types, such as regression, classification, and time series. It is good\nto present the output to the readers.\nWhat's Next?\nSubmit a PR with your tutorial. We'll review it, and soon you'll see your",
    "tag": "mindsdb"
  },
  {
    "title": "Step 1. Create a File",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/contribute/data-integration-page.mdx",
    "content": "\ntitle: How to Create a Data Integration Doc Page\nsidebarTitle: Create a Data Integration Doc Page\n\nCurrently, all the supported data integrations are listed here.\nWe plan to provide extensive instructions on how to connect and use each of the data integrations. Thus, each data integration requires its dedicated doc page.\nPlease follow the steps below to create a dedicated doc page for your chosen data integration.\nStep 1. Create a File\nCreate a Markdown file under the `mindsdb/docs/data-integrations` directory and name it with the database name.\n\nIf you implement a doc page for the Amazon Redshift database, name the file as `amazon-redshift.mdx`.\n\nStep 2. Implement the Content\nPlease include the following at the beginning of the file:\n```sql\ntitle: \nsidebarTitle: \n\n\n```\n\nHere is a sample for the Oracle data integration:\n```sql\ntitle: Oracle\nsidebarTitle: Oracle\n\n\n```\n\nHere are the resources you should use to implement the content of the doc page:\n\n\nUse the connection template and example from here.\n\nIf you implement a doc page for the Google BigQuery database, use this chapter.\n\n\n\nCheck out the dedicated handler folder here. Pay attention to the `README.md` file, as it contains valuable information that should be included in the doc page.\n\nIf you implement a doc page for the Google BigQuery database, go to its handler folder here and see the README.md file here.\n\n\n\nYou are welcome to provide usage examples of the data integrations in MindsDB!\nStep 3. Create a PR\nOnce you are done, create a PR. We'll review it soon!",
    "tag": "mindsdb"
  },
  {
    "title": "Running the Docs Locally",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/contribute/docs.mdx",
    "content": "\ntitle: MindsDB Documentation\nsidebarTitle: Writing Documentation\n\nThis section gets you started on how to contribute to the MindsDB documentation.\nMindsDB's documentation is run using Mintlify. If you want to contribute to our docs, please follow the steps below to set up the environment locally.\nRunning the Docs Locally\n\nPrerequisite\n    You should have installed Git (version 2.30.1 or higher) and Node.js (version 18.10.0 or higher).\n\nStep 1. Clone the MindsDB Git repository:\n`console\ngit clone https://github.com/mindsdb/mindsdb.git`\nStep 2. Install Mintlify on your OS:\n`console\nnpm i mintlify -g`\nStep 3. Go to the `docs` folder inside the cloned MindsDB Git repository and start Mintlify there:\n`console\nmintlify dev`\nThe documentation website is now available at `http://localhost:3000`.\n\nGetting an Error?\n  If you use the Windows operating system, you may get an error saying `no such file or directory: C:/Users/Username/.mintlify/mint/client`. Here are the steps to troubleshoot it:\n        - Go to the `C:/Users/Username/.mintlify/` directory.\n        - Remove the `mint` folder.\n        - Open the Git Bash in this location and run `git clone https://github.com/mintlify/mint.git`.\n        - Repeat step 3.\n\nMindsDB Repository Structure\nHere is the structure of the MindsDB docs repository:\n`docs                          # All documentation source files\n|__assets/                    # Images and icons used throughout the docs\n\u2502  \u251c\u2500 ...\n\u2502__folders_with_mdx_files/    # All remaining folders that store the .mdx files\n|__mdx_files                  # Some of the .mdx files are stored in the docs directory\n|__mintlify.json              # This JSON file stores navigation and page setup`\nWhat's Next?\nFollow our docs rules and have fun.",
    "tag": "mindsdb"
  },
  {
    "title": "Loan Default Prediction",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/contribute/loanTutorial/loanTutorial.mdx",
    "content": "Loan Default Prediction\nIntroduction\nA loan is money borroed to somone (debtor) with the intent to pay back at an agreed date. Ideally, things should go as planned but when the debtor fails to pay the person they borrowed the loan from (creditor), the debtor is said to have defaulted on the loan. This is what loan default is\nIt is the event of a debtor not paying their loan...\nIt is then important for creditors/loan companies to know/predict if a certain debtor will default or not. This is a problem that machine learning solves, this is a classification machine learning problem.\nAs with every machine learning problem, data is the major ingredient to solving it. The dataset for this problem is from the Zindi Loan Default Prediction Challenge for DSN\nThis tutorial is to showcase how MindsDB performs in solveing this problem. We will be exploring how we can use a machine learning model to classify negative and positive cases for predicitng loan default. MindsDB allows you to train your model from a .CSV format directly, and you will:\n\nLoad your dataset \nCreate a predictor, and,\nMake predictions\n\nLoad your dataset\nData wrangling\nif you go through the dataset, you will find that it is split into three different files, you will first wrangle the data to suit your requirements, \nHere, i use python to wrangle the data, you can however use your desired tool (excel perhaps...) or copy my snippet below...\n```\ncode snipet to join data\niomport dependencies and read in data files\nimport pandas as pd \nimport functools as ft\ntrainPerf = pd.read_csv('trainperf.csv')\ntrainDemographics = pd.read_csv('traindemographics.csv')\njoin data together, clean and write to .csv\ndata = trainPerf.merge(trainDemographics, left_on='customerid', right_on='customerid')\ndata.drop_duplicates(inplace=True)\ndata.dropna(axis=1, inplace=True)\ndata.drop(['customerid', 'systemloanid'], axis=1, inplace=True) # remove id columns\ndata.to_csv('loanData.csv')\n```\nNow that the dataset has been cleaned and stored into a .csv file, you can upload it to MindsDB cloud...\nUsing the MindsDB GUI\nTo use the MindsDB GUI, you will need to access the MindsDB cloud tool and sign uo for an account if you haven't yet, and you will be automatically directed to the editor.\nWhile on the editor, you can add your data via several means, but as earlier mentioned, you will upload your dataset as a .csv...\nOn the \"Select your data source\" screen, click on \"Files\" next to \"Databaes\" and you should see the screen below:\n\nClick on the \"Import File\" icon and select your .csv file. \nGive your table a name and hit the \"Save and Con\"tinue\" button. NOTE: Use snake or camel casing when uploading files\nIf you do all things well, you should see a result like this:\n\nNow that you have loaded your dataset, you are now to...\nCreate a Predictor\nCreating a predictor in MindsDB is as easy as four lines of code. Copy and paste the code below into your SQL editor to make a predictor\n`CREATE MODEL mindsdb.loanPredictor\nFROM files (\n    SELECT * FROM files.loanData\n) PREDICT good_bad_flag;`\nIf all is successfull, you should see this screen below:\n\nTo confirm this and view details of your model like the accuracy, use the following code:\n`SELECT * FROM mindsdb.models WHERE name='loanPredictor';`\nand you can see details of your model...\n\nPS: You can see the accuracy is low, this is probably due to low data...\nNow that you have created your prdictor model, now is time to...\nMake a prediction\nYou can use the folowing query to make a prediction... pay attention to use new/unseen/untrained data... \nPS: Pay attention to highlight your prediction query and run it\n`SELECT good_bad_flag\nFROM mindsdb.loanPredictor\nWHERE loannumber=1 AND approveddate='2022-07-04 10:12:66.000000' AND\ncreationdate='2022-07-04 08:10:00.000000' AND loanamount=100000 AND\ntotaldue=110000 AND termdays=30 AND birthdate='1986-11-04 00:00:00.000000' AND\nbank_account_type='Savings' AND longitude_gps=3.3269784 AND latitude_gps=5.799699 AND\nbank_name_clients='UBA';`\nand viola! you just made a prediction!\n\nConclusion\nAnd that's it you shining rockstar! you are now an ML Wizard making predictions straight from the data! This is a taste of the awesome power of MindsDB.\nSome important things to note:\n\nMore data improves the accuracy of the model\nYour table names should not contain spaces but should rather be in snake or camel casing\nHighlight your prediction query and run it to see result\nMinsDB is fast and very helpfull \n",
    "tag": "mindsdb"
  },
  {
    "title": "Introduction",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/nlp/text-summarization-inside-mysql-with-openai.mdx",
    "content": "\ntitle: Text Summarization on MySQL with MindsDB and OpenAI\nsidebarTitle: MySQL Text Summarization\n\nIntroduction\nIn this blog post, we present how to create OpenAI models within MindsDB. In this example, we ask a model to provide a summary of a text. The input data is taken from our sample MySQL database.\nPrerequisites\nTo follow along, you can sign up for an account at cloud.mindsdb.com. Alternatively, head to MindsDB documentation and follow the instructions to manually set up a local instance of MindsDB via Docker or pip.\nTutorial\nIn this tutorial, we create a predictive model to summarize an article.\nWe use a table from our MySQL public demo database, so let's start by connecting MindsDB to it:\n`sql\nCREATE DATABASE mysql_demo_db\nWITH ENGINE = 'mysql',\nPARAMETERS = {\n    \"user\": \"user\",\n    \"password\": \"MindsDBUser123!\",\n    \"host\": \"db-demo-data.cwoyhfn6bzs0.us-east-1.rds.amazonaws.com\",\n    \"port\": \"3306\",\n    \"database\": \"public\"\n};`\nNow that we've connected our database to MindsDB, let's query the data to be used in the example:\n`sql\nSELECT *\nFROM mysql_demo_db.articles\nLIMIT 3;`\nHere is the output:\n`sql\n+----------------------------------------------------------------+--------------------------------------------------------------+\n| article                                                        | highlights                                                   |\n+----------------------------------------------------------------+--------------------------------------------------------------+\n| Video footage has emerged of a law enforcement officer\u2026        | The 53-second video features\u2026                                |\n| A new restaurant is offering a five-course drink-paired menu\u2026  | The Curious Canine Kitchen is\u2026                               |\n| Mother-of-two Anna Tilley survived after spending four days\u2026   | Experts have warned hospitals not using standard treatment\u2026  |\n+----------------------------------------------------------------+--------------------------------------------------------------+`\nLet's create a model table to summarize all articles from the input dataset:\n`sql\nCREATE MODEL text_summarization_model\nPREDICT highlights\nUSING\n    engine = 'openai',              \n    prompt_template = 'provide an informative summary of the text text:{{article}} using full sentences';`\nIn practice, the `CREATE MODEL` statement triggers MindsDB to generate an AI table called `text_summarization_model` that uses the OpenAI integration to predict a column named `highlights`. The model lives inside the default `mindsdb` project. In MindsDB, projects are a natural way to keep artifacts, such as models or views, separate according to what predictive task they solve. You can learn more about MindsDB projects here.\nThe `USING` clause specifies the parameters that this handler requires.\n\nThe `engine` parameter defines that we use the `openai` engine.\nThe `prompt_template` parameter conveys the structure of a message that is to be completed with additional text generated by the model.\n\n\nIf you're using a local deployment, in order to use the OpenAI integration, you need to install the `openai` Python package. You can do this by running the following command:\n`bash\npip install openai`\n\n\nWe use the OpenAI engine to create a model in MindsDB. Please note that the `api_key` parameter is optional on cloud.mindsdb.com but mandatory for local usage/on-premise. You can obtain an OpenAI API key by signing up for OpenAI's API services on their website. Once you have signed up, you can find your API key in the API Key section of the OpenAI dashboard. You can then pass this API key to the MindsDB platform when creating models.\nTo create a `text_summarization_model` model in MindsDB, you can run the following code:\n`sql\nCREATE MODEL text_summarization_model\nPREDICT highlights\nUSING\n    engine = 'openai',              \n    prompt_template = 'provide an informative summary of the text text:{{article}} using full sentences',\n    api_key = 'YOUR_OPENAI_API_KEY'; -- MANDATORY FOR LOCAL MODE`\nAlternatively, you can create a MindsDB ML engine that includes the API key, so you don't have to enter it each time:\n`sql\nCREATE ML_ENGINE openai\nFROM openai\nUSING\n  api_key = 'YOUR_OPENAI_API_KEY';`\n\nOnce the `CREATE MODEL` statement has started execution, we can check the status of the creation process with the following query:\n`sql\nSELECT * FROM models\nWHERE name = 'text_summarization_model';`\nIt may take a while to register as complete depending on the internet connection. Once the creation is complete, the behavior is the same as with any other AI table \u2013 you can query it either by specifying synthetic data in the actual query:\n`sql\nSELECT article, highlights\nFROM text_summarization_model\nWHERE article = \"Apple's Watch hits stores this Friday when customers and employees\n                alike will be able to pre-order the timepiece. And boss Tim Cook is\n                rewarding his staff by offering them a 50 per cent discount on the device.\";`\nHere is the output data:\n`sql\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+\n| article                                                                                                                                                                                                            | highlights                                                                         |\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+\n| Apple's Watch hits stores this Friday when customers and employees alike will be able to pre-order the timepiece. And boss Tim Cook is rewarding his staff by offering them a 50 per cent discount on the device.  | Apple's Watch hits stores this Friday, and employees will be able to pre-order the |\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+`\nOr by joining with another table for batch predictions:\n`sql\nSELECT input.article, output.highlights\nFROM mysql_demo_db.articles AS input\nJOIN text_summarization_model AS output\nLIMIT 3;`\nHere is the output data:\n`sql\n+----------------------------------------------------------------+------------------------------------------------------------------------------------------------+\n| article                                                        | highlights                                                                                     |\n+----------------------------------------------------------------+------------------------------------------------------------------------------------------------+\n| Video footage has emerged of a law enforcement officer\u2026        | A video has emerged of a law enforcement officer grabbing a cell phone from a woman who was    |\n| A new restaurant is offering a five-course drink-paired menu\u2026  | A new restaurant in London is offering a five-course drink-paired menu for dogs                |\n| Mother-of-two Anna Tilley survived after spending four days\u2026   | Sepsis is a potentially life-threatening condition that occurs when the body's response to an  |\n+----------------------------------------------------------------+------------------------------------------------------------------------------------------------+`\nThe `articles` table is used to make batch predictions. Upon joining the `text_summarization_model` model with the `articles` table, the model uses all values from the `article` column.\nLeverage the NLP Capabilities with MindsDB\nBy integrating databases and OpenAI using MindsDB, developers can easily extract insights from text data with just a few SQL commands. These powerful natural language processing (NLP) models are capable of answering questions with or without context and completing general prompts.\nFurthermore, these models are powered by large pre-trained language models from OpenAI, so there is no need for manual development work. Ultimately, this provides developers with an easy way to incorporate powerful NLP capabilities into their applications while saving time and resources compared to traditional ML development pipelines and methods. All in all, MindsDB makes it possible for developers to harness the power of OpenAI efficiently!\nMindsDB is now the fastest-growing open-source applied machine-learning platform in the world. Its community continues to contribute to more than 70 data-source and ML-framework integrations. Stay tuned for the upcoming features - including more control over the interface parameters and fine-tuning models directly from MindsDB!\nExperiment with OpenAI models within MindsDB and unlock the ML capability over your data in minutes. Remember to sign-up for a free demo account and follow the tutorials, perhaps this time using your data.\nFinally, if MindsDB's vision to democratize ML sounds exciting, head to our community Slack, where you can get help and find people to chat about using other available data sources, ML frameworks, or writing a handler to bring your own!\nFollow our introduction to MindsDB's OpenAI integration here. Also, we've got a variety of tutorials that use MySQL and MongoDB:\n\nSentiment Analysis in MySQL\nQuestion Answering in MySQL\nSentiment Analysis in MongoDB\nQuestion Answering in MongoDB\nText Summarization in MongoDB\n\nWhat's Next?\nHave fun while trying it out yourself!\n\nBookmark MindsDB repository on GitHub.\nSign up for a free MindsDB account.\nEngage with the MindsDB community on Slack or GitHub to ask questions and share your ideas and thoughts.\n",
    "tag": "mindsdb"
  },
  {
    "title": "Introduction",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/nlp/sentiment-analysis-inside-mysql-with-openai.mdx",
    "content": "\ntitle: Sentiment Analysis on MySQL with MindsDB and OpenAI\nsidebarTitle: MySQL Sentiment Analysis\n\nIntroduction\nIn this blog post, we present how to create OpenAI models within MindsDB. This example is a sentiment analysis where we infer emotions behind a text. The input data is taken from our sample MySQL database.\nPrerequisites\nTo follow along, you can sign up for an account at cloud.mindsdb.com. Alternatively, head to MindsDB documentation and follow the instructions to manually set up a local instance of MindsDB via Docker or pip.\nTutorial\nIn this tutorial, we create a predictive model to infer emotions behind a text, a task also known as sentiment analysis.\nWe use a table from our MySQL public demo database, so let's start by connecting MindsDB to it:\n`sql\nCREATE DATABASE mysql_demo_db\nWITH ENGINE = 'mysql',\nPARAMETERS = {\n    \"user\": \"user\",\n    \"password\": \"MindsDBUser123!\",\n    \"host\": \"db-demo-data.cwoyhfn6bzs0.us-east-1.rds.amazonaws.com\",\n    \"port\": \"3306\",\n    \"database\": \"public\"\n};`\nNow that we've connected our database to MindsDB, let's query the data to be used in the example:\n`sql\nSELECT *\nFROM mysql_demo_db.amazon_reviews\nLIMIT 3;`\nHere is the output:\n`sql\n+-----------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+\n| product_name                                                                            | review                                                                                               |\n+-----------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+\n| All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi, 16 GB - Includes Special Offers, Magenta | Late gift for my grandson. He is very happy with it. Easy for him (9yo ).                            |\n| All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi, 16 GB - Includes Special Offers, Magenta | I'm not super thrilled with the proprietary OS on this unit, but it does work okay and does what I n |\n| All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi, 16 GB - Includes Special Offers, Magenta | I purchased this Kindle Fire HD 8 was purchased for use by 5 and 8 yer old grandchildren. They basic |\n+-----------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+`\nLet's create a model table to identify sentiment for all reviews:\n`sql\nCREATE MODEL sentiment_classifier_model\nPREDICT sentiment\nUSING\n  engine = 'openai',\n  prompt_template = 'describe the sentiment of the reviews\n                     strictly as \"positive\", \"neutral\", or \"negative\".\n                     \"I love the product\":positive\n                     \"It is a scam\":negative\n                     \"{{review}}.\":';`\nIn practice, the `CREATE MODEL` statement triggers MindsDB to generate an AI table called `sentiment_classifier_model` that uses the OpenAI integration to predict a column named `sentiment`. The model lives inside the default `mindsdb` project. In MindsDB, projects are a natural way to keep artifacts, such as models or views, separate according to what predictive task they solve. You can learn more about MindsDB projects here.\nThe `USING` clause specifies the parameters that this handler requires.\n\nThe `engine` parameter defines that we use the `openai` engine.\nThe `prompt_template` parameter conveys the structure of a message that is to be completed with additional text generated by the model.\n\n\nIf you're using a local deployment, in order to use the OpenAI integration, you need to install the `openai` Python package. You can do this by running the following command:\n`bash\npip install openai`\n\n\nWe use the OpenAI engine to create a model in MindsDB. Please note that the `api_key` parameter is optional on cloud.mindsdb.com but mandatory for local usage/on-premise. You can obtain an OpenAI API key by signing up for OpenAI's API services on their website. Once you have signed up, you can find your API key in the API Key section of the OpenAI dashboard. You can then pass this API key to the MindsDB platform when creating models.\nTo create a `sentiment_classifier_model` model in MindsDB, you can run the following code:\n`sql\nCREATE MODEL sentiment_classifier_model\nPREDICT sentiment\nUSING\n  engine = 'openai',\n  prompt_template = 'describe the sentiment of the reviews\n                     strictly as \"positive\", \"neutral\", or \"negative\".\n                     \"I love the product\":positive\n                     \"It is a scam\":negative\n                     \"{{review}}.\":',\n  api_key = 'YOUR_OPENAI_API_KEY'; -- MANDATORY FOR LOCAL MODE`\nAlternatively, you can create a MindsDB ML engine that includes the API key, so you don't have to enter it each time:\n`sql\nCREATE ML_ENGINE openai\nFROM openai\nUSING\n  api_key = 'YOUR_OPENAI_API_KEY';`\n\nOnce the `CREATE MODEL` statement has started execution, we can check the status of the creation process with the following query:\n`sql\nSELECT * FROM models\nWHERE name = 'sentiment_classifier_model';`\nIt may take a while to register as complete depending on the internet connection. Once the creation is complete, the behavior is the same as with any other AI table \u2013 you can query it either by specifying synthetic data in the actual query:\n`sql\nSELECT review, sentiment\nFROM sentiment_classifier_model\nWHERE review = 'It is ok.';`\nHere is the output data:\n`sql\n+-----------+-----------+\n| review    | sentiment |\n+-----------+-----------+\n| It is ok. | neutral   |\n+-----------+-----------+`\nOr by joining with another table for batch predictions:\n`sql\nSELECT input.review, output.sentiment\nFROM mysql_demo_db.amazon_reviews AS input\nJOIN sentiment_classifier_model AS output\nLIMIT 3;`\nHere is the output data:\n`sql\n+------------------------------------------------------------------------------------------------------+-----------+\n| review                                                                                               | sentiment |\n+------------------------------------------------------------------------------------------------------+-----------+\n| Late gift for my grandson. He is very happy with it. Easy for him (9yo ).                            | positive  |\n| I'm not super thrilled with the proprietary OS on this unit, but it does work okay and does what I n | positive  |\n| I purchased this Kindle Fire HD 8 was purchased for use by 5 and 8 yer old grandchildren. They basic | positive  |\n+------------------------------------------------------------------------------------------------------+-----------+`\nThe `amazon_reviews` table is used to make batch predictions. Upon joining the `sentiment_classifier_model` model with the `amazon_reviews` table, the model uses all values from the `review` column.\nLeverage the NLP Capabilities with MindsDB\nBy integrating databases and OpenAI using MindsDB, developers can easily extract insights from text data with just a few SQL commands. These powerful natural language processing (NLP) models are capable of answering questions with or without context and completing general prompts.\nFurthermore, these models are powered by large pre-trained language models from OpenAI, so there is no need for manual development work. Ultimately, this provides developers with an easy way to incorporate powerful NLP capabilities into their applications while saving time and resources compared to traditional ML development pipelines and methods. All in all, MindsDB makes it possible for developers to harness the power of OpenAI efficiently!\nMindsDB is now the fastest-growing open-source applied machine-learning platform in the world. Its community continues to contribute to more than 70 data-source and ML-framework integrations. Stay tuned for the upcoming features - including more control over the interface parameters and fine-tuning models directly from MindsDB!\nExperiment with OpenAI models within MindsDB and unlock the ML capability over your data in minutes. Remember to sign-up for a free demo account and follow the tutorials, perhaps this time using your data.\nFinally, if MindsDB's vision to democratize ML sounds exciting, head to our community Slack, where you can get help and find people to chat about using other available data sources, ML frameworks, or writing a handler to bring your own!\nFollow our introduction to MindsDB's OpenAI integration here. Also, we've got a variety of tutorials that use MySQL and MongoDB:\n\nQuestion Answering in MySQL\nText Summarization in MySQL\nSentiment Analysis in MongoDB\nQuestion Answering in MongoDB\nText Summarization in MongoDB\n\nWhat's Next?\nHave fun while trying it out yourself!\n\nBookmark MindsDB repository on GitHub.\nSign up for a free MindsDB account.\nEngage with the MindsDB community on Slack or GitHub to ask questions and share your ideas and thoughts.\n",
    "tag": "mindsdb"
  },
  {
    "title": "MindsDB NLP Supported Tasks",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/nlp/nlp-mindsdb-hf.mdx",
    "content": "\ntitle: NLP with MindsDB and Hugging Face\nsidebarTitle: NLP with MindsDB and Hugging Face\n\nMindsDB NLP Supported Tasks\nThere are four main NLP tasks currently supported by MindsDB:\n\nText Classification\nZero-Shot Classification\nTranslation\nSummarization\n\n\nCurrently, MindsDB's NLP engine is powered by Hugging Face and OpenAI. But we plan to expand to other NLP options in the future, so stay tuned!\n\n\nThe MindsDB's Hugging Face engine is extensible. We are actively working on adding more tasks and models.\nIf you have a specific task or model in mind, please let us know in the MindsDB Community.\n\nMindsDB NLP Tested Models\n\n\n        Completes the task of assigning a label to a text. For example, you can use it to classify a movie review as positive or negative.\n        ##### Supported Models\n        - Spam detection\n        - Sentiment analysis\n        - Sentiment analysis Spanish\n        - Sentiment analysis finance\n        - Emotions classifier\n        - Emotions classifier Ekmanm's 6 emotions\n        - Toxicity classifier\n        - Environmental, Social, and Governance (ESG) 4 classifier\n        - Environmental, Social, and Governance (ESG) 26 classifier\n        - Hate speech classifier\n        - Crypto Signals classifier\n        - US political party classifier\n        - Question Detection\n        - Industry classifier\n\n\n```</Accordion>\n<Accordion title=\"Zero-Shot Classification\" defaultOpen=\"true\">\n    Completes the task of assigning a label to a text without training on the labels.\n    For example, you can use it to classify a movie review as positive or negative without training on positive or negative labels.\n    ##### Supported Models\n    -[BART](https://huggingface.co/facebook/bart-large-mnli)\n</Accordion>\n\n<Accordion title=\"Translation\" defaultOpen=\"true\">\n    Completes the task of translating a text from one language to another. For example, you can use it to translate a text from English to French.\n    ##### Supported Models\n    - [English to French T5](https://huggingface.co/t5-base)\n    - [French to English T5](https://huggingface.co/t5-base)\n    - [Spanish to English](https://huggingface.co/Helsinki-NLP/opus-mt-es-en)\n</Accordion>\n\n<Accordion title=\"Summarization\" defaultOpen=\"true\">\n    Completes the task of summarizing a text. For example, you can use it to summarize an abstract into a title.\n    ##### Supported Models\n    - [BART](https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n    - [Google Pegasus](https://huggingface.co/google/pegasus-xsum)\n</Accordion>\n```\n\n\n\nThe Hugging Face models are used to perform these tasks.\nKeep in mind that usually there is more than one model for each task,\nso you can choose the one that suits you best.\nHow to Bring the Hugging Face Model to MindsDB\nWe use the CREATE MODEL statement to bring the Hugging Face models to MindsDB.\nGenerally, it looks like this:\n`sql\nCREATE MODEL project_name.predictor_name        -- AI TABLE TO STORE THE MODEL\nPREDICT target_column                           -- NAME OF THE COLUMN TO STORE PREDICTED VALUES\nUSING\n  engine = 'huggingface',                       -- USING THE HUGGING FACE ENGINE\n  task = 'task',                                -- TASK OF CLASSIFYING TEXT (OPTIONAL)\n  model_name = 'model_name_from_hugging_face',  -- MODEL NAME UNDER THE HUGGING FACE MODEL HUB\n  input_column = 'input_column',                -- COLUMN NAME OF THE INPUT DATA\n  labels = ['label', 'label_1'];                -- ARRAY OF LABELS`\nWhere:\n| Expressions      | Description                                                                                         |\n| ---------------- | --------------------------------------------------------------------------------------------------- |\n| `project_name`   | Name of the project where the model is created. By default, the `mindsdb` project is used.          |\n| `predictor_name` | Name of the model to be created.                                                                    |\n| `target_column`  | Column to store the predicted values.                                                               |\n| `engine`         | Optional. You can provide an ML engine, based on which the model is created.                        |\n| `task`           | Optional. It is relative to the Hugging Face task tag.                                              |\n| `model_name`     | Model name from the Hugging Face model hub.                                                         |\n| `input_column`   | Name of the column that has the input data, especially important for batch predictions using JOIN.  |\n| `labels`         | Depending on the model. Usually used for Zero-Shot Classification models.                           |\n\nFor more examples and explanations, visit our doc page on Hugging Face.\n\nExample using SQL\nLet's go through a Spam Classification example to understand better how to link Hugging Face models and bring them to MindsDB as AI tables.\n\nUsing Local Installation of MindsDB\n\n\n```Please note that if you use a local installation of MindsDB, instead of MindsDB Cloud, you should install `transformers==4.21.0` to be able to use the Hugging Face models.\n```\n\n\n\n`sql\nCREATE MODEL mindsdb.spam_classifier                           \nPREDICT PRED                           \nUSING\n  engine = 'huggingface',              \n  task = 'text-classification',        \n  model_name = 'mrm8488/bert-tiny-finetuned-sms-spam-detection', \n  input_column = 'text_spammy',        \n  labels = ['ham', 'spam'];`\nWhere:\n| Expressions      | Values                                                                                                                  |\n| ---------------- | ----------------------------------------------------------------------------------------------------------------------- |\n| `project_name`   | mindsdb                                                                                                                 |\n| `predictor_name` | spam_classifier                                                                                                         |\n| `target_column`  | PRED                                                                                                                    |\n| `engine`         | huggingface                                                                                                             |\n| `task`           | text-classification                                                                                                     |\n| `model_name`     | mrm8488/bert-tiny-finetuned-sms-spam-detection |\n| `input_column`   | text_spammy                                                                                                             |\n| `labels`         | ['ham', 'spam']                                                                                                         |\nOn execution, we get:\n`sql\nQuery successfully completed`\nBefore querying for predictions, we should verify the status of the `spam_classifier` model.\n`sql\nSELECT *\nFROM mindsdb.models\nWHERE name = 'spam_classifier';`\nOn execution, we get:\n`sql\n+---------------+-------+--------+--------+-------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|NAME           |PROJECT|STATUS  |ACCURACY|PREDICT|UPDATE_STATUS|MINDSDB_VERSION|ERROR |SELECT_DATA_QUERY|TRAINING_OPTIONS                                                                                                                                                                                               |\n+---------------+-------+--------+--------+-------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|spam_classifier|mindsdb|complete|[NULL]  |PRED   |up_to_date   |22.10.2.1      |[NULL]|[NULL]           |{'target': 'PRED', 'using': {'engine': 'huggingface', 'task': 'text-classification', 'model_name': 'mrm8488/bert-tiny-finetuned-sms-spam-detection', 'input_column': 'text_spammy', 'labels': ['ham', 'spam']}}|\n+---------------+-------+--------+--------+-------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+`\nOnce the status is `complete`, we can query for predictions.\n`sql\nSELECT h.PRED, h.PRED_explain, t.text_spammy AS input_text\nFROM example_db.demo_data.hf_test AS t\nJOIN mindsdb.spam_classifier AS h;`\nOn execution, we get:\n`sql\n+----+---------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|PRED|PRED_explain                                             |input_text                                                                                                                                                       |\n+----+---------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|spam|{'spam': 0.9051626920700073, 'ham': 0.09483727067708969} |Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's      |\n|ham |{'ham': 0.9380123615264893, 'spam': 0.061987683176994324}|Nah I don't think he goes to usf, he lives around here though                                                                                                    |\n|spam|{'spam': 0.9064534902572632, 'ham': 0.09354648739099503} |WINNER!! As a valued network customer you have been selected to receivea \u00a3900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.    |\n+----+---------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+`\nFor the full library of supported examples please go here.\nExample using MQL\nLet's go through a Sentiment Classification example, but this time we'll use a Mongo database.\n\nUsing Local Installation of MindsDB\n\n\n```Please note that if you use a local installation of MindsDB, instead of MindsDB Cloud, you should install `transformers==4.21.0` to be able to use the Hugging Face models.\n```\n\n\n\nWe have a sample Mongo database that you can connect to your MindsDB Cloud account by running this command in Mongo Shell:\n`bash\nuse mindsdb`\nFollowed by:\n`bash\ndb.databases.insertOne({\n    'name': 'mongo_test_db', \n    'engine': 'mongodb',\n    'connection_args': {\n        \"host\": \"mongodb+srv://admin:201287aA@cluster0.myfdu.mongodb.net/admin?authSource=admin&replicaSet=atlas-5koz1i-shard-0&readPreference=primary&appname=MongoDB%20Compass&ssl=true\",\n        \"database\": \"test_data\"\n        }   \n})`\nWe use this sample database throughout the example.\nThe next step is to create a connection between Mongo and MindsDB. Follow the instructions to connect MindsDB via Mongo Compass or Mongo Shell.\nNow, we are ready to create a Hugging Face model.\n`bash\ndb.models.insertOne({\n    name: 'sentiment_classifier',\n    predict: 'sentiment',\n    training_options: {\n            engine: 'huggingface',\n            task: 'text-classification',\n            model_name: 'cardiffnlp/twitter-roberta-base-sentiment',\n            input_column: 'comment',\n            labels: ['negative','neutral','positive']\n           }\n})`\nOn execution, we get:\n`bash\n{ acknowledged: true,\n  insertedId: ObjectId(\"63c00c704d444a0b83808420\") }`\nWe can check its status using this command:\n`bash\ndb.getCollection('models').find({'name': 'sentiment_classifier'})`\nOn execution, we get:\n`bash\n{ NAME: 'sentiment_classifier_hf',\n  PROJECT: 'mindsdb',\n  VERSION: 1,\n  STATUS: 'complete',\n  ACCURACY: null,\n  PREDICT: 'sentiment',\n  UPDATE_STATUS: 'up_to_date',\n  MINDSDB_VERSION: '22.11.4.3',\n  ERROR: null,\n  SELECT_DATA_QUERY: null,\n  TRAINING_OPTIONS: '{\\'target\\': \\'sentiment\\', \\'using\\': {\\'task\\': \\'text-classification\\', \\'model_name\\': \\'cardiffnlp/twitter-roberta-base-sentiment\\', \\'input_column\\': \\'comment\\', \\'labels\\': [\\'negative\\', \\'neutral\\', \\'positive\\']}}',\n  TAG: null }`\nOnce the status is `complete`, we can query for predictions.\nHere is how to query for a single prediction:\n`bash\ndb.sentiment_classifier.find({comment: 'It is really easy to do NLP with MindsDB'})`\nOn execution, we get:\n`bash\n{ sentiment: 'positive',\n  sentiment_explain: \n   { positive: 0.9350261688232422,\n     neutral: 0.06265384703874588,\n     negative: 0.0023200225550681353 },\n  comment: 'It is really easy to do NLP with MindsDB' }`\nYou can also query for batch predictions. Here we use the `mongo_test_db` database connected earlier in this example.\n`bash\ndb.sentiment_classifier.find(\n    {'collection': 'mongo_test_db.user_comments'},\n    {'sentiment_classifier.sentiment': 'sentiment',\n     'user_comments.comment': 'comment'\n    }\n)`\nOn execution, we get:\n`bash\n{ sentiment: 'positive', comment: 'I love pizza' }\n{ sentiment: 'negative', comment: 'I hate dancing' }\n{ sentiment: 'neutral', comment: 'Baking is not a big deal' }`\nFor the full library of supported examples please go here.\nWhat's Next?\nHave fun while trying it out yourself!\n\nBookmark MindsDB repository on GitHub.\nSign up for a free MindsDB account.\nEngage with the MindsDB community on\n  Slack or\n  GitHub to ask questions and\n  share your ideas and thoughts.\n\nIf this tutorial was helpful, please give us a GitHub star",
    "tag": "mindsdb"
  },
  {
    "title": "Hugging Face Examples",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/nlp/nlp-extended-examples.mdx",
    "content": "\ntitle: NLP Example Library\nsidebarTitle: NLP Example Library\n\n\nCurrently, MindsDB's NLP engine is powered by Hugging Face and OpenAI. But we plan to expand to other NLP options in the future, so stay tuned!\n\n\nThe MindsDB's Hugging Face engine is extensible. We are actively working on adding more tasks and models.\nIf you have a specific task or model in mind, please let us know in the MindsDB Community.\n\nHugging Face Examples\nHere are the tasks supported by MindsDB and Hugging Face:\n\nText Classification\nZero-Shot Classification\nTranslation\nSummarization\n\nLet's go through the examples.\nText Classification\n\n\n        Let's create a model.\n\n\n```    ```sql\n    CREATE MODEL mindsdb.hf_spam\n    PREDICT PRED\n    USING\n    engine = 'huggingface',\n    task = 'text-classification',\n    model_name = 'mariagrandury/roberta-base-finetuned-sms-spam-detection',\n    input_column = 'text',\n    labels = ['spam', 'ham'];\n    ```\n\n    And check its status.\n\n    ```sql\n    SELECT *\n    FROM mindsdb.models \n    WHERE name = 'hf_spam';\n    ```\n\n    Once the status is `complete`, we can query for predictions.\n\n    ```sql\n    SELECT *\n    FROM mindsdb.hf_spam\n    WHERE text = 'I like you. I love you.';\n    ```\n\n    On evecution, we get:\n\n    ```sql\n    +----+--------------------------------------------------------+-----------------------+\n    |PRED|PRED_explain                                            |text                   |\n    +----+--------------------------------------------------------+-----------------------+\n    |spam|{\"ham\":0.00020051795581821352,\"spam\":0.9997995495796204}|I like you. I love you.|\n    +----+--------------------------------------------------------+-----------------------+\n    ```\n</Accordion>\n\n<Accordion title=\"Sentiment\">\n    Let's create a model.\n\n    ```sql\n    CREATE MODEL mindsdb.hf_sentiment\n    PREDICT PRED\n    USING\n    engine = 'huggingface',\n    task = 'text-classification',\n    model_name = 'cardiffnlp/twitter-roberta-base-sentiment',\n    input_column = 'text',\n    labels = ['neg', 'neu', 'pos'];\n    ```\n\n    And check its status.\n\n    ```sql\n    SELECT *\n    FROM mindsdb.models \n    WHERE name = 'hf_sentiment';\n    ```\n\n    Once the status is `complete`, we can query for predictions.\n\n    ```sql\n    SELECT *\n    FROM mindsdb.hf_sentiment\n    WHERE text = 'I like you. I love you.';\n    ```\n\n    On evecution, we get:\n\n    ```sql\n    +----+--------------------------------------------------------------------------------+-----------------------+\n    |PRED|PRED_explain                                                                    |text                   |\n    +----+--------------------------------------------------------------------------------+-----------------------+\n    |pos |{\"neg\":0.003046575468033552,\"neu\":0.021965451538562775,\"pos\":0.9749879240989685}|I like you. I love you.|\n    +----+--------------------------------------------------------------------------------+-----------------------+\n    ```\n</Accordion>\n\n<Accordion title=\"Sentiment (Finance)\">\n    Let's create a model.\n\n    ```sql\n    CREATE MODEL mindsdb.hf_sentiment_finance\n    PREDICT PRED\n    USING\n    engine = 'huggingface',\n    task = 'text-classification',\n    model_name = 'ProsusAI/finbert',\n    input_column = 'text';\n    ```\n\n    And check its status.\n\n    ```sql\n    SELECT *\n    FROM mindsdb.models \n    WHERE name = 'hf_sentiment_finance';\n    ```\n\n    Once the status is `complete`, we can query for predictions.\n\n    ```sql\n    SELECT *\n    FROM mindsdb.hf_sentiment_finance\n    WHERE text = 'Stocks rallied and the British pound gained.';\n    ```\n\n    On evecution, we get:\n\n    ```sql\n    +--------+-------------------------------------------------------------------------------------------+--------------------------------------------+\n    |PRED    |PRED_explain                                                                               |text                                        |\n    +--------+-------------------------------------------------------------------------------------------+--------------------------------------------+\n    |positive|{\"negative\":0.0344734713435173,\"neutral\":0.06716493517160416,\"positive\":0.8983616232872009}|Stocks rallied and the British pound gained.|\n    +--------+-------------------------------------------------------------------------------------------+--------------------------------------------+\n    ```\n</Accordion>\n\n<Accordion title=\"Emotions (6)\">\n    Let's create a model.\n\n    ```sql\n    CREATE MODEL mindsdb.hf_emotions_6\n    PREDICT PRED\n    USING\n    engine = 'huggingface',\n    task = 'text-classification',\n    model_name = 'j-hartmann/emotion-english-distilroberta-base',\n    input_column = 'text';\n    ```\n\n    And check its status.\n\n    ```sql\n    SELECT *\n    FROM mindsdb.models \n    WHERE name = 'hf_emotions_6';\n    ```\n\n    Once the status is `complete`, we can query for predictions.\n\n    ```sql\n    SELECT *\n    FROM mindsdb.hf_emotions_6\n    WHERE text = 'Oh Happy Day';\n    ```\n\n    On evecution, we get:\n\n    ```sql\n    +----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+\n    |PRED|PRED_explain                                                                                                                                                                                                   |text        |\n    +----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+\n    |joy |{\"anger\":0.0028446922078728676,\"disgust\":0.0009613594156689942,\"fear\":0.0007112706662155688,\"joy\":0.7692911624908447,\"neutral\":0.037753619253635406,\"sadness\":0.015293814241886139,\"surprise\":0.17314413189888}|Oh Happy Day|\n    +----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+\n    ```\n</Accordion>\n\n<Accordion title=\"Toxicity\">\n    Let's create a model.\n\n    ```sql\n    CREATE MODEL mindsdb.hf_toxicity\n    PREDICT PRED\n    USING\n    engine = 'huggingface',\n    task = 'text-classification',\n    model_name = 'SkolkovoInstitute/roberta_toxicity_classifier',\n    input_column = 'text';\n    ```\n\n    And check its status.\n\n    ```sql\n    SELECT *\n    FROM mindsdb.models \n    WHERE name = 'hf_toxicity';\n    ```\n\n    Once the status is `complete`, we can query for predictions.\n\n    ```sql\n    SELECT *\n    FROM mindsdb.hf_toxicity\n    WHERE text = 'I like you. I love you.';\n    ```\n\n    On evecution, we get:\n\n    ```sql\n    +-------+-------------------------------------------------------------+-----------------------+\n    |PRED   |PRED_explain                                                 |text                   |\n    +-------+-------------------------------------------------------------+-----------------------+\n    |neutral|{\"neutral\":0.9999547004699707,\"toxic\":0.00004535282641882077}|I like you. I love you.|\n    +-------+-------------------------------------------------------------+-----------------------+\n    ```\n</Accordion>\n\n<Accordion title=\"ESG (6)\">\n    Let's create a model.\n\n    ```sql\n    CREATE MODEL mindsdb.hf_esg_6\n    PREDICT PRED\n    USING\n    engine = 'huggingface',\n    task = 'text-classification',\n    model_name = 'yiyanghkust/finbert-esg',\n    input_column = 'text';\n    ```\n\n    And check its status.\n\n    ```sql\n    SELECT *\n    FROM mindsdb.models \n    WHERE name = 'hf_esg_6';\n    ```\n\n    Once the status is `complete`, we can query for predictions.\n\n    ```sql\n    SELECT * FROM  mindsdb.hf_esg_6\n    WHERE text = 'Rhonda has been volunteering for several years for a variety of charitable community programs.';\n    ```\n\n    On evecution, we get:\n\n    ```sql\n    +------+---------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+\n    |PRED  |PRED_explain                                                                                                                     |text                                                                                          |\n    +------+---------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+\n    |Social|{\"Environmental\":0.0034267122391611338,\"Governance\":0.004729956854134798,\"None\":0.001239194767549634,\"Social\":0.9906041026115417}|Rhonda has been volunteering for several years for a variety of charitable community programs.|\n    +------+---------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+\n    ```\n</Accordion>\n\n<Accordion title=\"ESG (26)\">\n    Let's create a model.\n\n    ```sql\n    CREATE MODEL mindsdb.hf_esg_26\n    PREDICT PRED\n    USING\n    engine = 'huggingface',\n    task = 'text-classification',\n    model_name = 'yiyanghkust/finbert-esg',\n    input_column = 'text';\n    ```\n\n    And check its status.\n\n    ```sql\n    SELECT *\n    FROM mindsdb.models \n    WHERE name = 'hf_esg_26';\n    ```\n\n    Once the status is `complete`, we can query for predictions.\n\n    ```sql\n    SELECT *\n    FROM mindsdb.hf_esg_26\n    WHERE text = 'We believe it is essential to establish validated conflict-free sources of 3TG within the Democratic Republic of the Congo (the \u201cDRC\u201d) and adjoining countries (together, with the DRC, the \u201cCovered Countries\u201d), so that these minerals can be procured in a way that contributes to economic growth and development in the region. To aid in this effort, we have established a conflict minerals policy and an internal team to implement the policy.';\n    ```\n\n    On evecution, we get:\n\n    ```sql\n    +------+-----------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n    |PRED  |PRED_explain                                                                                                                 |text                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n    +------+-----------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n    |Social|{\"Environmental\":0.2031959593296051,\"Governance\":0.08251894265413284,\"None\":0.050893042236566544,\"Social\":0.6633920073509216}|We believe it is essential to establish validated conflict-free sources of 3TG within the Democratic Republic of the Congo (the \u201cDRC\u201d) and adjoining countries (together, with the DRC, the \u201cCovered Countries\u201d), so that these minerals can be procured in a way that contributes to economic growth and development in the region. To aid in this effort, we have established a conflict minerals policy and an internal team to implement the policy.|\n    +------+-----------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n    ```\n</Accordion>\n\n<Accordion title=\"Hate Speech\">\n    Let's create a model.\n\n    ```sql\n    CREATE MODEL mindsdb.hf_hate\n    PREDICT PRED\n    USING\n    engine = 'huggingface',\n    task = 'text-classification',\n    model_name = 'Hate-speech-CNERG/bert-base-uncased-hatexplain',\n    input_column = 'text';\n    ```\n\n    And check its status.\n\n    ```sql\n    SELECT *\n    FROM mindsdb.models \n    WHERE name = 'hf_hate';\n    ```\n\n    Once the status is `complete`, we can query for predictions.\n\n    ```sql\n    SELECT *\n    FROM mindsdb.hf_hate\n    WHERE text = 'I like you. I love you.';\n    ```\n\n    On evecution, we get:\n\n    ```sql\n    +------+-----------------------------------------------------------------------------------------------+-----------------------+\n    |PRED  |PRED_explain                                                                                   |text                   |\n    +------+-----------------------------------------------------------------------------------------------+-----------------------+\n    |normal|{\"hate speech\":0.03551718592643738,\"normal\":0.7747423648834229,\"offensive\":0.18974047899246216}|I like you. I love you.|\n    +------+-----------------------------------------------------------------------------------------------+-----------------------+\n    ```\n</Accordion>\n\n<Accordion title=\"Crypto Buy Signals\">\n    Let's create a model.\n\n    ```sql\n    CREATE MODEL mindsdb.hf_crypto\n    PREDICT PRED\n    USING\n    engine = 'huggingface',\n    task = 'text-classification',\n    model_name = 'ElKulako/cryptobert',\n    input_column = 'text';\n    ```\n\n    And check its status.\n\n    ```sql\n    SELECT *\n    FROM mindsdb.models \n    WHERE name = 'hf_crypto';\n    ```\n\n    Once the status is `complete`, we can query for predictions.\n\n    ```sql\n    SELECT *\n    FROM mindsdb.hf_crypto\n    WHERE text = 'BTC is killing it right now';\n    ```\n\n    On evecution, we get:\n\n    ```sql\n    +-------+------------------------------------------------------------------------------------------+---------------------------+\n    |PRED   |PRED_explain                                                                              |text                       |\n    +-------+------------------------------------------------------------------------------------------+---------------------------+\n    |Bullish|{\"Bearish\":0.0002816587220877409,\"Bullish\":0.559426486492157,\"Neutral\":0.4402918517589569}|BTC is killing it right now|\n    +-------+------------------------------------------------------------------------------------------+---------------------------+\n    ```\n</Accordion>\n\n<Accordion title=\"US Political Party\">\n    Let's create a model.\n\n    ```sql\n    CREATE MODEL mindsdb.hf_us_party\n    PREDICT PRED\n    USING\n    engine = 'huggingface',\n    task = 'text-classification',\n    model_name = 'm-newhauser/distilbert-political-tweets',\n    input_column = 'text';\n    ```\n\n    And check its status.\n\n    ```sql\n    SELECT *\n    FROM mindsdb.models \n    WHERE name = 'hf_us_party';\n    ```\n\n    Once the status is `complete`, we can query for predictions.\n\n    ```sql\n    SELECT *\n    FROM mindsdb.hf_us_party\n    WHERE text = 'This pandemic has shown us clearly the vulgarity of our healthcare system. Highest costs in the world, yet not enough nurses or doctors. Many millions uninsured, while insurance company profits soar. The struggle continues. Healthcare is a human right. Medicare for all.';\n    ```\n\n    On evecution, we get:\n\n    ```sql\n    +--------+-------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n    |PRED    |PRED_explain                                                       |text                                                                                                                                                                                                                                                                          |\n    +--------+-------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n    |Democrat|{\"Democrat\":0.9999973773956299,\"Republican\":0.00000261212517216336}|This pandemic has shown us clearly the vulgarity of our healthcare system. Highest costs in the world, yet not enough nurses or doctors. Many millions uninsured, while insurance company profits soar. The struggle continues. Healthcare is a human right. Medicare for all.|\n    +--------+-------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n    ```\n</Accordion>\n\n<Accordion title=\"Question Detection\">\n    Let's create a model.\n\n    ```sql\n    CREATE MODEL mindsdb.hf_question\n    PREDICT PRED\n    USING\n    engine = 'huggingface',\n    task = 'text-classification',\n    model_name = 'shahrukhx01/bert-mini-finetune-question-detection',\n    input_column = 'text',\n    labels = ['question', 'query'];\n    ```\n\n    And check its status.\n\n    ```sql\n    SELECT *\n    FROM mindsdb.models \n    WHERE name = 'hf_question';\n    ```\n\n    Once the status is `complete`, we can query for predictions.\n\n    ```sql\n    SELECT *\n    FROM mindsdb.hf_question\n    WHERE text = 'Where can I buy electronics in London';\n    ```\n\n    On evecution, we get:\n\n    ```sql\n    +-----+--------------------------------------------------------------+-------------------------------------+\n    |PRED |PRED_explain                                                  |text                                 |\n    +-----+--------------------------------------------------------------+-------------------------------------+\n    |query|{\"query\":0.9997773766517639,\"question\":0.00022261829872149974}|Where can I buy electronics in London|\n    +-----+--------------------------------------------------------------+-------------------------------------+\n    ```\n</Accordion>\n\n<Accordion title=\"Industry\">\n    Let's create a model.\n\n    ```sql\n    CREATE MODEL mindsdb.hf_industry\n    PREDICT PRED\n    USING\n    engine = 'huggingface',\n    task = 'text-classification',\n    model_name = 'sampathkethineedi/industry-classification',\n    input_column = 'text';\n    ```\n\n    And check its status.\n\n    ```sql\n    SELECT *\n    FROM mindsdb.models \n    WHERE name = 'hf_industry';\n    ```\n\n    Once the status is `complete`, we can query for predictions.\n\n    ```sql\n    SELECT *\n    FROM mindsdb.hf_industry\n    WHERE text = 'Low latency is one of our best cloud features';\n    ```\n\n    On evecution, we get:\n\n    ```sql\n    +----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------+\n    |PRED            |PRED_explain                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |text                                         |\n    +----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------+\n    |Systems Software|{\"Advertising\":0.000006795735771447653,\"Aerospace & Defense\":0.00001537964453746099,\"Apparel Retail\":5.350161131900677e-7,\"Apparel, Accessories & Luxury Goods\":0.000002604161181807285,\"Application Software\":0.009111878462135792,\"Asset Management & Custody Banks\":0.00003155150625389069,\"Auto Parts & Equipment\":0.000015504940165556036,\"Biotechnology\":6.533917940032552e-8,\"Building Products\":7.348538133555849e-8,\"Casinos & Gaming\":0.000013775999832432717,\"Commodity Chemicals\":0.0000010432338513055583,\"Communications Equipment\":0.000019887389498762786,\"Construction & Engineering\":0.000001826199536480999,\"Construction Machinery & Heavy Trucks\":0.000009827364920056425,\"Consumer Finance\":0.0000018292046206624946,\"Data Processing & Outsourced Services\":0.0000010666744856280275,\"Diversified Metals & Mining\":0.000006960767223063158,\"Diversified Support Services\":0.000016824227714096196,\"Electric Utilities\":0.000003896044290740974,\"Electrical Components & Equipment\":0.000001626394464437908,\"Electronic Equipment & Instruments\":0.00003863943129545078,\"Environmental & Facilities Services\":0.000736175337806344,\"Gold\":0.00002220332135038916,\"Health Care Equipment\":4.6927588925882446e-8,\"Health Care Facilities\":7.432880124724761e-7,\"Health Care Services\":6.929263918209472e-7,\"Health Care Supplies\":2.1007431882935634e-7,\"Health Care Technology\":0.000003907185146090342,\"Homebuilding\":3.903339234057057e-7,\"Hotels, Resorts & Cruise Lines\":6.0527639789143e-7,\"Human Resource & Employment Services\":5.48697983049351e-7,\"IT Consulting & Other Services\":0.0000723653138265945,\"Industrial Machinery\":7.230253231682582e-7,\"Integrated Telecommunication Services\":2.8266379104024963e-7,\"Interactive Media & Services\":0.00003454017496551387,\"Internet & Direct Marketing Retail\":0.000003871373337460682,\"Internet Services & Infrastructure\":0.0007196652004495263,\"Investment Banking & Brokerage\":0.0000040634336073708255,\"Leisure Products\":0.000002158361439796863,\"Life Sciences Tools & Services\":0.000002861268058040878,\"Movies & Entertainment\":0.000007286199888767442,\"Oil & Gas Equipment & Services\":0.000004376991455501411,\"Oil & Gas Exploration & Production\":0.000005569149834627751,\"Oil & Gas Refining & Marketing\":0.000012647416951949708,\"Oil & Gas Storage & Transportation\":0.000005852583853993565,\"Packaged Foods & Meats\":0.0000011130315442642313,\"Personal Products\":0.00000970239307207521,\"Pharmaceuticals\":0.0000037546726616710657,\"Property & Casualty Insurance\":0.000006116194072092185,\"Real Estate Operating Companies\":0.00001882187461887952,\"Regional Banks\":0.0000011669454806906288,\"Research & Consulting Services\":0.000024276219846797176,\"Restaurants\":8.598511840318679e-7,\"Semiconductors\":0.0000021006283077440457,\"Specialty Chemicals\":0.000004160017397225602,\"Specialty Stores\":2.644004553076229e-7,\"Steel\":0.0000013566890402216814,\"Systems Software\":0.9889177083969116,\"Technology Distributors\":0.00001339179198112106,\"Technology Hardware, Storage & Peripherals\":0.00004790363891515881,\"Thrifts & Mortgage Finance\":3.924862141957419e-7,\"Trading Companies & Distributors\":0.0000035233156268077437}|Low latency is one of our best cloud features|\n    +----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------+\n    ```\n</Accordion>\n```\n\n\n\nZero-Shot Classification\n\n\n        Let's create a model.\n\n\n```    ```sql\n    CREATE MODEL mindsdb.hf_zs_bart\n    PREDICT PRED\n    USING\n    engine = 'huggingface',\n    task = 'zero-shot-classification',\n    model_name = 'facebook/bart-large-mnli',\n    input_column = 'text',\n    candidate_labels = ['Books', 'Household', 'Clothing & Accessories', 'Electronics'];\n    ```\n\n    And check its status.\n\n    ```sql\n    SELECT *\n    FROM mindsdb.models \n    WHERE name = 'hf_zs_bart';\n    ```\n\n    Once the status is `complete`, we can query for predictions.\n\n    ```sql\n    SELECT *\n    FROM mindsdb.hf_zs_bart\n    WHERE text = 'Paper Plane Design Framed Wall Hanging Motivational Office Decor Art Prints';\n    ```\n\n    On evecution, we get:\n\n    ```sql\n    +---------+------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------+\n    |PRED     |PRED_explain                                                                                                                              |text                                                                       |\n    +---------+------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------+\n    |Household|{\"Books\":0.1876104772090912,\"Clothing & Accessories\":0.08688066899776459,\"Electronics\":0.14785148203372955,\"Household\":0.5776574015617371}|Paper Plane Design Framed Wall Hanging Motivational Office Decor Art Prints|\n    +---------+------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------+\n    ```\n</Accordion>\n```\n\n\n\nTranslation\n\n\n        Let's create a model.\n\n\n```    ```sql\n    CREATE MODEL mindsdb.hf_t5_en_fr\n    PREDICT PRED\n    USING\n    engine = 'huggingface',\n    task = 'translation',\n    model_name = 't5-base',\n    input_column = 'text',\n    lang_input = 'en',\n    lang_output = 'fr';\n    ```\n\n    And check its status.\n\n    ```sql\n    SELECT *\n    FROM mindsdb.models \n    WHERE name = 'hf_t5_en_fr';\n    ```\n\n    Once the status is `complete`, we can query for predictions.\n\n    ```sql\n    SELECT *\n    FROM mindsdb.hf_t5_en_fr\n    WHERE text = 'The monkey is on the branch';\n    ```\n\n    On evecution, we get:\n\n    ```sql\n    +---------------------------+---------------------------+\n    |PRED                       |text                       |\n    +---------------------------+---------------------------+\n    |Le singe est sur la branche|The monkey is on the branch|\n    +---------------------------+---------------------------+\n    ```\n</Accordion>\n```\n\n\n\nSummarization\n\n\n        Let's create a model.\n\n\n```    ```sql\n    CREATE MODEL mindsdb.hf_bart_sum_20\n    PREDICT PRED\n    USING\n    engine = 'huggingface',\n    task = 'summarization',\n    model_name = 'sshleifer/distilbart-cnn-12-6',\n    input_column = 'text',\n    min_output_length = 5,\n    max_output_length = 20;\n    ```\n\n    And check its status.\n\n    ```sql\n    SELECT *\n    FROM mindsdb.models \n    WHERE name = 'hf_bart_sum_20';\n    ```\n\n    Once the status is `complete`, we can query for predictions.\n\n    ```sql\n    SELECT *\n    FROM mindsdb.hf_bart_sum_20\n    WHERE text = 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.';\n    ```\n\n    On evecution, we get:\n\n    ```sql\n    +-------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n    |PRED                                                   |text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n    +-------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n    |The tower is 324 metres (1,063 ft) tall, about the same|The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.|\n    +-------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n    ```\n</Accordion>\n\n<Accordion title=\"Google Pegasus\">\n    Let's create a model.\n\n    ```sql\n    CREATE MODEL mindsdb.hf_peg_sum_20\n    PREDICT PRED\n    USING\n    engine = 'huggingface',\n    task = 'summarization',\n    model_name = 'google/pegasus-xsum',\n    input_column = 'text',\n    min_output_length = 5,\n    max_output_length = 20;\n    ```\n\n    And check its status.\n\n    ```sql\n    SELECT *\n    FROM mindsdb.models \n    WHERE name = 'hf_peg_sum_20';\n    ```\n\n    Once the status is `complete`, we can query for predictions.\n\n    ```sql\n    SELECT *\n    FROM mindsdb.hf_peg_sum_20\n    WHERE text = 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.';\n    ```\n\n    On evecution, we get:\n\n    ```sql\n    +------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n    |PRED                                            |text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n    +------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n    |The Eiffel Tower is a landmark in Paris, France.|The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.|\n    +------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n    ```\n</Accordion>\n```\n\n\n\nOpenAI Examples\nHere are the tasks supported by MindsDB and Hugging Face:\n\nAnswering Questions without Context\nAnswering Questions with Context\nPrompt Completion\n\nLet's go through the examples.\nAnswering Questions without Context\n\n\n        Let's create a model.\n\n\n```    ```sql\n    CREATE MODEL project_a.openai_test_a\n    PREDICT answer\n    USING\n        engine = 'openai',\n        question_column = 'question';\n    ```\n\n    On execution, we get:\n\n    ```sql\n    Query successfully completed\n    ```\n\n    Now we can query for answers.\n\n    ```sql\n    SELECT question, answer\n    FROM project_a.openai_test_a\n    WHERE question = 'Where is Stockholm located?';\n    ```\n\n    On execution, we get:\n\n    ```sql\n    +---------------------------+-------------------------------+\n    |question                   |answer                         |\n    +---------------------------+-------------------------------+\n    |Where is Stockholm located?|Stockholm is located in Sweden.|\n    +---------------------------+-------------------------------+\n    ```\n</Accordion>\n```\n\n\n\nAnswering Questions with Context\n\n\n        Let's create a model.\n\n\n```    ```sql\n    CREATE MODEL project_a.openai_test_b\n    PREDICT answer\n    USING\n        engine = 'openai',\n        question_column = 'question',\n        context_column = 'context';\n    ```\n\n    On execution, we get:\n\n    ```sql\n    Query successfully completed\n    ```\n\n    Now we can query for answers.\n\n    ```sql\n    SELECT context, question, answer\n    FROM project_a.openai_test_b\n    WHERE context = 'Answer with a joke'\n    AND question = 'How to cook soup?';\n    ```\n\n    On execution, we get:\n\n    ```sql\n    +-------------------+------------------+---------------------------------------------------------+\n    |context            |question          |answer                                                   |\n    +-------------------+------------------+---------------------------------------------------------+\n    |Answer with a joke |How to cook soup? |How do you cook soup? You put it in a pot and heat it up!|\n    +-------------------+------------------+---------------------------------------------------------+\n    ```\n</Accordion>\n```\n\n\n\nPrompt Completion\n\n\n        Let's create a model.\n\n\n```    ```sql\n    CREATE MODEL project_a.openai_test_c\n    PREDICT answer\n    USING\n        engine = 'openai',\n        prompt_template = 'Context: {{context}}. Question: {{question}}. Answer:',\n        max_tokens = 100,\n        temperature = 0.3;\n    ```\n\n    Let's look at an example that uses parameters provided at model creation time.\n\n    ```sql\n    SELECT context, question, answer\n    FROM project_a.openai_test_c\n    WHERE context = 'Answer accurately'\n    AND question = 'How many planets exist in the solar system?';\n    ```\n\n    On execution, we get:\n\n    ```sql\n    +-------------------+-------------------------------------------+----------------------------------------------+\n    |context            |question                                   |answer                                        |\n    +-------------------+-------------------------------------------+----------------------------------------------+\n    |Answer accurately  |How many planets exist in the solar system?| There are eight planets in the solar system. |\n    +-------------------+-------------------------------------------+----------------------------------------------+\n    ```\n</Accordion>\n\n<Accordion title=\"Prompt Completion with Parameters Provided at Prediction Time\">\n    Let's create a model.\n\n    ```sql\n    CREATE MODEL project_a.openai_test_c\n    PREDICT answer\n    USING\n        engine = 'openai',\n        prompt_template = 'Context: {{context}}. Question: {{question}}. Answer:',\n        max_tokens = 100,\n        temperature = 0.3;\n    ```\n\n    Let's look at an example that overrides parameters at prediction time.\n\n    ```sql\n    SELECT instruction, answer\n    FROM project_a.openai_test_c\n    WHERE instruction = 'Speculate extensively'\n    USING\n        prompt_template = '{{instruction}}. What does Tom Hanks like?',\n        max_tokens = 100,\n        temperature = 0.5;\n    ```\n\n    On execution, we get:\n\n    ```sql\n    +----------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n    |instruction           |answer                                                                                                                                                                                                                         |\n    +----------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n    |Speculate extensively |Some people speculate that Tom Hanks likes to play golf, while others believe that he enjoys acting and directing. It is also speculated that he likes to spend time with his family and friends, and that he enjoys traveling.|\n    +----------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n    ```\n</Accordion>\n\n<Accordion title=\"Sentiment Classification\">\n    Let's create a model.\n\n    ```sql\n    CREATE MODEL mindsdb.sentiment_classifier                           \n    PREDICT sentiment\n    USING\n      engine = 'openai',              \n      prompt_template = 'predict the sentiment of the text:{{review}} exactly as either positive or negative or neutral';\n    ```\n\n    Now we can query for predictions.\n\n    ```sql\n    SELECT output.sentiment, input.review\n    FROM example_db.demo_data.amazon_reviews AS input\n    JOIN mindsdb.sentiment_classifier AS output\n    LIMIT 3;\n    ```\n\n    On execution, we get:\n\n    ```sql\n    +----------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n    | sentiment                              | review                                                                                                                                                                                                                                                                                                                                                                            |\n    +----------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n    | positive                               | Late gift for my grandson. He is very happy with it. Easy for him (9yo ).                                                                                                                                                                                                                                                                                                         |\n    | The sentiment of the text is positive. | I'm not super thrilled with the proprietary OS on this unit, but it does work okay and does what I need it to do. Appearance is very nice, price is very good and I can't complain too much - just wish it were easier (or at least more obvious) to port new apps onto it. For now, it helps me see things that are too small on my phone while I'm traveling. I'm a happy buyer.|\n    | positive                               | I purchased this Kindle Fire HD 8 was purchased for use by 5 and 8 yer old grandchildren. They basically use it to play Amazon games that you download.                                                                                                                                                                                                                           |\n    +----------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n    ```\n</Accordion>\n```\n\n\n\nWhat's Next?\nHave fun while trying it out yourself!\n\nBookmark MindsDB repository on GitHub.\nSign up for a free MindsDB account.\nEngage with the MindsDB community on\n  Slack or\n  GitHub to ask questions and\n  share your ideas and thoughts.\n\nIf this tutorial was helpful, please give us a GitHub star",
    "tag": "mindsdb"
  },
  {
    "title": "Introduction",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/nlp/sentiment-analysis-inside-mongodb-with-openai.mdx",
    "content": "\ntitle: Sentiment Analysis on MongoDB with MindsDB and OpenAI\nsidebarTitle: MongoDB Sentiment Analysis\n\nIntroduction\nIn this blog post, we present how to create OpenAI models within MindsDB. This example is a sentiment analysis where we infer emotions behind a text. The input data is taken from our sample MongoDB database.\nPrerequisites\nTo follow along, you can sign up for an account at cloud.mindsdb.com. Alternatively, head to MindsDB documentation and follow the instructions to manually set up a local instance of MindsDB via Docker or pip.\nHow to Connect MindsDB to a Database\nWe use a collection from our MongoDB public demo database, so let\u2019s start by connecting MindsDB to it.\nYou can use Mongo Compass or Mongo Shell to connect our sample database like this:\n`bash\ntest> use mindsdb\nmindsdb> db.databases.insertOne({\n            'name': 'mongo_demo_db',\n            'engine': 'mongodb',\n            'connection_args': {\n                \"host\": \"mongodb+srv://user:MindsDBUser123!@demo-data-mdb.trzfwvb.mongodb.net/\",\n                \"database\": \"public\"\n            }\n        })`\nTutorial\nIn this tutorial, we create a predictive model to infer emotions behind a text, a task also known as sentiment analysis.\nNow that we've connected our database to MindsDB, let\u2019s query the data to be used in the example:\n`bash\nmindsdb> use mongo_demo_db\nmongo_demo_db> db.amazon_reviews.find({}).limit(3)`\nHere is the output:\n`bash\n{\n  _id: '63d013b5bbca62e9c7774b1d',\n  product_name: 'All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi, 16 GB - Includes Special Offers, Magenta',\n  review: 'Late gift for my grandson. He is very happy with it. Easy for him (9yo ).'\n}\n{\n  _id: '63d013b5bbca62e9c7774b1e',\n  product_name: 'All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi, 16 GB - Includes Special Offers, Magenta',\n  review: \"I'm not super thrilled with the proprietary OS on this unit, but it does work okay and does what I need it to do. Appearance is very nice, price is very good and I can't complain too much - just wish it were easier (or at least more obvious) to port new apps onto it. For now, it helps me see things that are too small on my phone while I'm traveling. I'm a happy buyer.\"\n}\n{\n  _id: '63d013b5bbca62e9c7774b1f',\n  product_name: 'All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi, 16 GB - Includes Special Offers, Magenta',\n  review: 'I purchased this Kindle Fire HD 8 was purchased for use by 5 and 8 yer old grandchildren. They basically use it to play Amazon games that you download.'\n}`\nLet's create a model collection to identify sentiment for all reviews:\n`bash\nmongo_demo_db> use mindsdb\nmindsdb> db.models.insertOne({\n            name: 'sentiment_classifier',\n            predict: 'sentiment',\n            training_options: {\n                        engine: 'openai',\n                        prompt_template: 'describe the sentiment of the reviews strictly as \"positive\", \"neutral\", or \"negative\". \"I love the product\":positive \"It is a scam\":negative \"{{review}}.\":'\n                }\n        })`\nIn practice, the `insertOne` method triggers MindsDB to generate an AI collection called `sentiment_classifier` that uses the OpenAI integration to predict a field named `sentiment`. The model is created inside the default `mindsdb` project. In MindsDB, projects are a natural way to keep artifacts, such as models or views, separate according to what predictive task they solve. You can learn more about MindsDB projects here.\nThe `training_options` key specifies the parameters that this handler requires.\n\nThe `engine` parameter defines that we use the `openai` engine.\nThe `prompt_template` parameter conveys the structure of a message that is to be completed with additional text generated by the model.\n\n\nIf you're using a local deployment, in order to use the OpenAI integration, you need to install the `openai` Python package. You can do this by running the following command:\n`bash\npip install openai`\n\n\nWe use the OpenAI engine to create a model in MindsDB. Please note that the `api_key` parameter is optional on cloud.mindsdb.com but mandatory for local usage/on-premise. You can obtain an OpenAI API key by signing up for OpenAI's API services on their website. Once you have signed up, you can find your API key in the API Key section of the OpenAI dashboard. You can then pass this API key to the MindsDB platform when creating models.\nTo create a `sentiment_classifier` model in MindsDB, you can run the following code:\n`bash\nmongo_demo_db> use mindsdb\nmindsdb> db.models.insertOne({\n            name: 'sentiment_classifier',\n            predict: 'sentiment',\n            training_options: {\n                        engine: 'openai',\n                        prompt_template: 'describe the sentiment of the reviews strictly as \"positive\", \"neutral\", or \"negative\". \"I love the product\":positive \"It is a scam\":negative \"{{review}}.\":',\n                        api_key: 'YOUR_OPENAI_API_KEY'  -- MANDATORY FOR LOCAL MODE\n                }\n        })`\nAlternatively, you can create a MindsDB ML engine that includes the API key, so you don't have to enter it each time. Please note that this command should be executed from MindsDB.\n`sql\nCREATE ML_ENGINE openai\nFROM openai\nUSING\n  api_key = 'YOUR_OPENAI_API_KEY';`\n\nOnce the `insertOne` method has started execution, we can check the status of the creation process with the following query:\n`bash\nmindsdb> db.getCollection('models').find({\n            'name': 'sentiment_classifier'\n        })`\nIt may take a while to register as complete depending on the internet connection. Once the creation is complete, the behavior is the same as with any other AI collection \u2013 you can query it either by specifying synthetic data in the actual query:\n`bash\nmindsdb> db.sentiment_classifier.find({\n            review: 'It is ok.'\n        })`\nHere is the output data:\n`bash\n{\n  sentiment: 'neutral',\n  review: 'It is ok.'\n}`\nOr by joining with a collection for batch predictions:\n`bash\nmindsdb> db.sentiment_classifier.find(\n            {\n                'collection': 'mongo_demo_db.amazon_reviews'\n            },\n            {\n                'sentiment_classifier.sentiment': 'sentiment',\n                'amazon_reviews.review': 'review'\n            }\n        ).limit(3)`\nHere is the output data:\n`bash\n{\n  sentiment: 'positive',\n  review: 'Late gift for my grandson. He is very happy with it. Easy for him (9yo ).'\n}\n{\n  sentiment: 'positive',\n  review: \"I'm not super thrilled with the proprietary OS on this unit, but it does work okay and does what I need it to do. Appearance is very nice, price is very good and I can't complain too much - just wish it were easier (or at least more obvious) to port new apps onto it. For now, it helps me see things that are too small on my phone while I'm traveling. I'm a happy buyer.\"\n}\n{\n  sentiment: 'positive',\n  review: 'I purchased this Kindle Fire HD 8 was purchased for use by 5 and 8 yer old grandchildren. They basically use it to play Amazon games that you download.'\n}`\nThe `amazon_reviews` collection is used to make batch predictions. Upon joining the `sentiment_classifier` model with the `amazon_reviews` collection, the model uses all values from the `review` field.\nLeverage the NLP Capabilities with MindsDB\nBy integrating databases and OpenAI using MindsDB, developers can easily extract insights from text data with just a few SQL commands. These powerful natural language processing (NLP) models are capable of answering questions with or without context and completing general prompts.\nFurthermore, these models are powered by large pre-trained language models from OpenAI, so there is no need for manual development work. Ultimately, this provides developers with an easy way to incorporate powerful NLP capabilities into their applications while saving time and resources compared to traditional ML development pipelines and methods. All in all, MindsDB makes it possible for developers to harness the power of OpenAI efficiently!\nMindsDB is now the fastest-growing open-source applied machine-learning platform in the world. Its community continues to contribute to more than 70 data-source and ML-framework integrations. Stay tuned for the upcoming features - including more control over the interface parameters and fine-tuning models directly from MindsDB!\nExperiment with OpenAI models within MindsDB and unlock the ML capability over your data in minutes. Remember to sign-up for a free demo account and follow the tutorials, perhaps this time using your data.\nFinally, if MindsDB's vision to democratize ML sounds exciting, head to our community Slack, where you can get help and find people to chat about using other available data sources, ML frameworks, or writing a handler to bring your own!\nFollow our introduction to MindsDB's OpenAI integration here. Also, we've got a variety of tutorials that use MySQL and MongoDB:\n\nSentiment Analysis in MySQL\nQuestion Answering in MySQL\nText Summarization in MySQL\nQuestion Answering in MongoDB\nText Summarization in MongoDB\n\nWhat's Next?\nHave fun while trying it out yourself!\n\nBookmark MindsDB repository on GitHub.\nSign up for a free MindsDB account.\nEngage with the MindsDB community on Slack or GitHub to ask questions and share your ideas and thoughts.\n",
    "tag": "mindsdb"
  },
  {
    "title": "MindsDB NLP Supported Tasks",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/nlp/nlp-mindsdb-openai.mdx",
    "content": "\ntitle: NLP with MindsDB and OpenAI\nMindsDB NLP Supported Tasks\nMindsDB lets you create models that utilize features provided by OpenAI GPT-3. Currently, there are three operation modes:\n\nAnswering Questions without Context\nAnswering Questions with Context\nPrompt Completion\n\n\nCurrently, MindsDB's NLP engine is powered by Hugging Face and OpenAI. But we plan to expand to other NLP options in the future, so stay tuned!\n\nHow to Bring the OpenAI Model to MindsDB\nWe use the CREATE MODEL statement to bring the OpenAI models to MindsDB.\nGenerally, it looks like this:\n`sql\nCREATE MODEL project_name.predictor_name        -- AI TABLE TO STORE THE MODEL\nPREDICT target_column                           -- NAME OF THE COLUMN TO STORE PREDICTED VALUES\nUSING\n  engine = 'openai',                            -- USING THE OPENAI ENGINE\n  prompt_template = 'promt/task                 \n                    {{input_column}}',          -- PROMPT TEMPLATE WITH PLACEHOLDERS FOR INPUT COLUMNS\n  model_name = 'model_name',                    -- OPTIONAL, DEFAULT IS THE text-davinci-002 MODEL\n  api_key = 'YOUR_OPENAI_API_KEY';              -- OPTIONAL, IF NOT PASSED MINDSDB FETCHES THE\n                                                -- `OPENAI_API_KEY` ENVIRONMENT VARIABLE VALUE`\nExample\n\nFor more examples and explanations, visit our doc page on OpenAI.\n\nExample using SQL\nLet's go through a sentiment classification example to understand better how to bring OpenAI models to MindsDB as AI tables.\n`sql\nCREATE MODEL mindsdb.sentiment_classifier                           \nPREDICT sentiment\nUSING\n  engine = 'openai',              \n  prompt_template = 'predict the sentiment of the text:{{review}} exactly as either positive or negative or neutral';`\nOn execution, we get:\n`sql\nQuery successfully completed`\nWhere:\n| Expressions      | Values                                                                                               |\n| ---------------- | ---------------------------------------------------------------------------------------------------- |\n| `project_name`   | `mindsdb`                                                                                            |\n| `predictor_name` | `sentiment_classifier`                                                                               |\n| `target_column`  | `sentiment`                                                                                          |\n| `engine`         | `openai`                                                                                             |\n| `prompt_template`| `predict the sentiment of the text:{{review}} exactly as either positive or negative or neutral`     |\n\nIn the `prompt_template` parameter, we use a placeholder for a text value that comes from the `review` column, that is, `text:{{review}}`.\n\nBefore querying for predictions, we should verify the status of the `sentiment_classifier` model.\n`sql\nSELECT *\nFROM mindsdb.models\nWHERE name = 'sentiment_classifier';`\nOn execution, we get:\n`sql\n+--------------------+------+-------+-------+--------+--------+---------+-------------+---------------+------+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------+-------+\n|NAME                |ENGINE|PROJECT|VERSION|STATUS  |ACCURACY|PREDICT  |UPDATE_STATUS|MINDSDB_VERSION|ERROR |SELECT_DATA_QUERY|TRAINING_OPTIONS                                                                                                                                       |TAG    |\n+--------------------+------+-------+-------+--------+--------+---------+-------------+---------------+------+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------+-------+\n|sentiment_classifier|openai|mindsdb|1      |complete|[NULL]  |sentiment|up_to_date   |22.12.4.3      |[NULL]|[NULL]           |{'target': 'sentiment', 'using': {'prompt_template': 'predict the sentiment of the text:{{review}} exactly as either positive or negative or neutral'}}|[NULL] |\n+--------------------+------+-------+-------+--------+--------+---------+-------------+---------------+------+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------+-------+`\nOnce the status is `complete`, we can query for predictions.\n`sql\nSELECT output.sentiment, input.review\nFROM example_db.demo_data.amazon_reviews AS input\nJOIN mindsdb.sentiment_classifier AS output\nLIMIT 3;`\n\n    Don't forget to create the `example_db` database before using one of its tables, like in the query above.\n\n\n``````sql\nCREATE DATABASE example_db\nWITH ENGINE = \"postgres\",\nPARAMETERS = {\n    \"user\": \"demo_user\",\n    \"password\": \"demo_password\",\n    \"host\": \"3.220.66.106\",\n    \"port\": \"5432\",\n    \"database\": \"demo\"\n    };\n```\n```\n\n\n\nOn execution, we get:\n`sql\n+----------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| sentiment                              | review                                                                                                                                                                                                                                                                                                                                                                            |\n+----------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| positive                               | Late gift for my grandson. He is very happy with it. Easy for him (9yo ).                                                                                                                                                                                                                                                                                                         |\n| The sentiment of the text is positive. | I'm not super thrilled with the proprietary OS on this unit, but it does work okay and does what I need it to do. Appearance is very nice, price is very good and I can't complain too much - just wish it were easier (or at least more obvious) to port new apps onto it. For now, it helps me see things that are too small on my phone while I'm traveling. I'm a happy buyer.|\n| positive                               | I purchased this Kindle Fire HD 8 was purchased for use by 5 and 8 yer old grandchildren. They basically use it to play Amazon games that you download.                                                                                                                                                                                                                           |\n+----------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+`\nFor the full library of supported examples please go here.\nExample using MQL\nNow let's go through a sentiment classification using Mongo database syntax.\nWe have a sample Mongo database that you can connect to your MindsDB Cloud account by running this command in Mongo Shell:\n`bash\nuse mindsdb`\nFollowed by:\n`bash\ndb.databases.insertOne({\n    'name': 'mongo_test_db', \n    'engine': 'mongodb',\n    'connection_args': {\n        \"host\": \"mongodb+srv://admin:201287aA@cluster0.myfdu.mongodb.net/admin?authSource=admin&replicaSet=atlas-5koz1i-shard-0&readPreference=primary&appname=MongoDB%20Compass&ssl=true\",\n        \"database\": \"test_data\"\n        }   \n})`\nWe use this sample database throughout the example.\nThe next step is to create a connection between Mongo and MindsDB. Follow the instructions to connect MindsDB via Mongo Compass or Mongo Shell.\nNow, we are ready to create an OpenAI model.\n`bash\ndb.models.insertOne({\n    name: 'sentiment_classifier_openai_mql',\n    predict: 'sentiment',\n    training_options: {\n            engine: 'openai',\n            prompt_template: 'predict the sentiment of the text:{{review}} exactly as either positive or negative or neutral'\n           }\n})`\nOn execution, we get:\n`bash\n{ acknowledged: true,\n  insertedId: ObjectId(\"63c19c3fe1d9855caa931df6\") }`\nBefore querying for predictions, we should verify the status of the `sentiment_classifier` model.\n`bash\ndb.getCollection('models').find({'name': 'sentiment_classifier_openai_mql'})`\nOn execution, we get:\n`bash\n{ NAME: 'sentiment_classifier_openai_mql',\n  ENGINE: 'openai',\n  PROJECT: 'mindsdb',\n  VERSION: 1,\n  STATUS: 'complete',\n  ACCURACY: null,\n  PREDICT: 'sentiment',\n  UPDATE_STATUS: 'up_to_date',\n  MINDSDB_VERSION: '22.12.4.3',\n  ERROR: null,\n  SELECT_DATA_QUERY: null,\n  TRAINING_OPTIONS: '{\\'target\\': \\'sentiment\\', \\'using\\': {\\'prompt_template\\': \\'predict the sentiment of the text:{{review}} exactly as either positive or negative or neutral\\'}}',\n  TAG: null,\n  _id: ObjectId(\"000000000000002836398080\") }`\nOnce the status is `complete`, we can query for a single prediction.\n`bash\ndb.sentiment_classifier_openai_mql.find({review: 'It is ok.'})`\nOn execution, we get:\n`bash\n{\n  sentiment: 'The sentiment of the text is neutral.',\n  review: 'It is ok.'\n}`\nYou can also query for batch predictions. Here we use the `mongo_test_db` database connected earlier in this example.\n`bash\ndb.sentiment_classifier_openai_mql.find(\n    {'collection': 'mongo_test_db.amazon_reviews'},\n    {'sentiment_classifier_openai_mql.sentiment': 'sentiment',\n     'amazon_reviews.review': 'review'\n    }\n)`\nOn execution, we get:\n`bash\n{\n  sentiment: 'positive',\n  review: 'Late gift for my grandson. He is very happy with it. Easy for him (9yo ).'\n}\n{\n  sentiment: 'The sentiment of the text is positive.',\n  review: \"I'm not super thrilled with the proprietary OS on this unit, but it does work okay and does what I need it to do. Appearance is very nice, price is very good and I can't complain too much - just wish it were easier (or at least more obvious) to port new apps onto it. For now, it helps me see things that are too small on my phone while I'm traveling. I'm a happy buyer.\"\n}\n{\n  sentiment: 'positive',\n  review: 'I purchased this Kindle Fire HD 8 was purchased for use by 5 and 8 yer old grandchildren. They basically use it to play Amazon games that you download.'\n}\n...`\nFor the full library of supported examples please go here.\nParameter descriptions\nMindsDB lets you customize models using parameters provided by OpenAI. Currently, there are eleven parameters to optionaly modify:\n\nmodel_name: An optional string that identifies the model to use, it defaults to text-davinci-002 model, for a list of models available and their description visit: Model overview\nmax_tokens: The maximum number of tokens to generate in the completion. The token count of your prompt plus max_tokens cannot exceed the model's context length.\ntemperature: What sampling temperature to use. Higher values means the model will take more risks.\ntop_p: An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.\nn: How many completions to generate for each prompt.\nstop: Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.\npresence_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\nfrequency_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\nbest_of: Generates best_of completions server-side and returns the \"best\" (the one with the highest log probability per token). Results cannot be streamed. When used with n, best_of controls the number of candidate completions and n specifies how many to return \u2013 best_of must be greater than n.\nlogit_bias: Modify the likelihood of specified tokens appearing in the completion. Accepts a json object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\nuser: A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. \n\nWhat's Next?\nHave fun while trying it out yourself!\n\nBookmark MindsDB repository on GitHub.\nSign up for a free MindsDB account.\nEngage with the MindsDB community on\n  Slack or\n  GitHub to ask questions and\n  share your ideas and thoughts.\n\nIf this tutorial was helpful, please give us a GitHub star",
    "tag": "mindsdb"
  },
  {
    "title": "what-is-nlp.mdx",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/nlp/what-is-nlp.mdx",
    "content": "\ntitle: What is Natural Language Processing?\nsidebarTitle: What is NLP?\n\nNatural Language Processing (NLP) is a branch of artificial intelligence that\nenables computers to understand, interpret, and manipulate human language.\nSome of the most popular applications of NLP include:\n\nchatbots,\nsentiment analysis,\ntext summarization,\nmachine translation,\nquestion answering,\ntext classification,\nnamed entity recognition,\ntopic modeling,\nand more.\n\nIn businesses and applications, NLP is a powerful tool used to utilize the potential of data. It can help to:\n\nextract insights from unstructured data,\nautomate processes,\nimprove customer service,\nand more.\n\nMindsDB provides a powerful NLP engine that can use pre-trained NLP models to extract insights from text-based data\nwithout the need for a vast amount of experience or in-depth knowledge of linguistics and data science.\nMindsDB's NLP engine for pre-trained models is powered by Hugging Face, a leading open-source NLP library that provides a wide range\nof pre-trained NLP models that can be used to extract insights from your text-based data and OpenAI's GPT-3 models, OpenAI's GPT-3 models\nare a powerful set of pre-trained NLP models, It has the record of being the largest neural network ever built with 175 billion parameters.\n\nCurrently, MindsDB's NLP engine is powered by Hugging Face and OpenAI. But we plan to expand to other NLP options in the future, so stay tuned!",
    "tag": "mindsdb"
  },
  {
    "title": "Introduction",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/nlp/question-answering-inside-mysql-with-openai.mdx",
    "content": "\ntitle: Question Answering on MySQL with MindsDB and OpenAI\nsidebarTitle: MySQL Question Answering\n\nIntroduction\nIn this blog post, we present how to create OpenAI models within MindsDB. In this example, we ask a question to a model and get an answer. The input data is taken from our sample MySQL database.\nPrerequisites\nTo follow along, you can sign up for an account at cloud.mindsdb.com. Alternatively, head to MindsDB documentation and follow the instructions to manually set up a local instance of MindsDB via Docker or pip.\nTutorial\nIn this tutorial, we create a predictive model to answer questions in a specified domain.\nWe use a table from our MySQL public demo database, so let's start by connecting MindsDB to it:\n`sql\nCREATE DATABASE mysql_demo_db\nWITH ENGINE = 'mysql',\nPARAMETERS = {\n    \"user\": \"user\",\n    \"password\": \"MindsDBUser123!\",\n    \"host\": \"db-demo-data.cwoyhfn6bzs0.us-east-1.rds.amazonaws.com\",\n    \"port\": \"3306\",\n    \"database\": \"public\"\n};`\nNow that we've connected our database to MindsDB, let's query the data to be used in the example:\n`sql\nSELECT *\nFROM mysql_demo_db.questions\nLIMIT 3;`\nHere is the output:\n`sql\n+------------------+--------------------------------------------------------+-------------+\n| article_title    | question                                               | true_answer |\n+------------------+--------------------------------------------------------+-------------+\n| Alessandro_Volta | Was Volta an Italian physicist?                        | yes         |\n| Alessandro_Volta | Is Volta buried in the city of Pittsburgh?             | no          |\n| Alessandro_Volta | Did Volta have a passion for the study of electricity? | yes         |\n+------------------+--------------------------------------------------------+-------------+`\nLet's create a model table to answer all questions from the input dataset:\n`sql\nCREATE MODEL question_answering_model\nPREDICT answer\nUSING\n    engine = 'openai',\n    prompt_template = 'answer the question of text:{{question}} about text:{{article_title}}';`\nIn practice, the `CREATE MODEL` statement triggers MindsDB to generate an AI table called `question_answering_model` that uses the OpenAI integration to predict a column named `answer`. The model lives inside the default `mindsdb` project. In MindsDB, projects are a natural way to keep artifacts, such as models or views, separate according to what predictive task they solve. You can learn more about MindsDB projects here.\nThe `USING` clause specifies the parameters that this handler requires.\n\nThe `engine` parameter defines that we use the `openai` engine.\nThe `prompt_template` parameter conveys the structure of a message that is to be completed with additional text generated by the model.\n\n\nIf you're using a local deployment, in order to use the OpenAI integration, you need to install the `openai` Python package. You can do this by running the following command:\n`bash\npip install openai`\n\n\nWe use the OpenAI engine to create a model in MindsDB. Please note that the `api_key` parameter is optional on cloud.mindsdb.com but mandatory for local usage/on-premise. You can obtain an OpenAI API key by signing up for OpenAI's API services on their website. Once you have signed up, you can find your API key in the API Key section of the OpenAI dashboard. You can then pass this API key to the MindsDB platform when creating models.\nTo create a `question_answering_model` model in MindsDB, you can run the following code:\n`sql\nCREATE MODEL question_answering_model\nPREDICT answer\nUSING\n    engine = 'openai',\n    prompt_template = 'answer the question of text:{{question}} about text:{{article_title}}',\n    api_key = 'YOUR_OPENAI_API_KEY'; -- MANDATORY FOR LOCAL MODE`\nAlternatively, you can create a MindsDB ML engine that includes the API key, so you don't have to enter it each time:\n`sql\nCREATE ML_ENGINE openai\nFROM openai\nUSING\n  api_key = 'YOUR_OPENAI_API_KEY';`\n\nOnce the `CREATE MODEL` statement has started execution, we can check the status of the creation process with the following query:\n`sql\nSELECT * FROM models\nWHERE name = 'question_answering_model';`\nIt may take a while to register as complete depending on the internet connection. Once the creation is complete, the behavior is the same as with any other AI table \u2013 you can query it either by specifying synthetic data in the actual query:\n`sql\nSELECT article_title, question, answer\nFROM question_answering_model\nWHERE question = 'Was Abraham Lincoln the sixteenth President of the United States?'\nAND article_title = 'Abraham_Lincoln';`\nHere is the output data:\n`sql\n+------------------+-------------------------------------------------------------------+------------------------------------------------------------------------+\n| article_title    | question                                                          | answer                                                                 |\n+------------------+-------------------------------------------------------------------+------------------------------------------------------------------------+\n| Abraham_Lincoln  | Was Abraham Lincoln the sixteenth President of the United States? | Yes, Abraham Lincoln was the sixteenth President of the United States. |\n+------------------+-------------------------------------------------------------------+------------------------------------------------------------------------+`\nOr by joining with another table for batch predictions:\n`sql\nSELECT input.article_title, input.question, output.answer\nFROM mysql_demo_db.questions AS input\nJOIN question_answering_model AS output\nLIMIT 3;`\nHere is the output data:\n`sql\n+------------------+--------------------------------------------------------+--------------------------------------------------------+\n| article_title    | question                                               | answer                                                 |\n+------------------+--------------------------------------------------------+--------------------------------------------------------+\n| Alessandro_Volta | Was Volta an Italian physicist?                        | Yes, Volta was an Italian physicist.                   |\n| Alessandro_Volta | Is Volta buried in the city of Pittsburgh?             | No, Volta is not buried in the city of Pittsburgh.     |\n| Alessandro_Volta | Did Volta have a passion for the study of electricity? | Yes, Volta had a passion for the study of electricity. |\n+------------------+--------------------------------------------------------+--------------------------------------------------------+`\nThe `questions` table is used to make batch predictions. Upon joining the `question_answering_model` model with the `questions` table, the model uses all values from the `article_title` and `question` columns.\nLeverage the NLP Capabilities with MindsDB\nBy integrating databases and OpenAI using MindsDB, developers can easily extract insights from text data with just a few SQL commands. These powerful natural language processing (NLP) models are capable of answering questions with or without context and completing general prompts.\nFurthermore, these models are powered by large pre-trained language models from OpenAI, so there is no need for manual development work. Ultimately, this provides developers with an easy way to incorporate powerful NLP capabilities into their applications while saving time and resources compared to traditional ML development pipelines and methods. All in all, MindsDB makes it possible for developers to harness the power of OpenAI efficiently!\nMindsDB is now the fastest-growing open-source applied machine-learning platform in the world. Its community continues to contribute to more than 70 data-source and ML-framework integrations. Stay tuned for the upcoming features - including more control over the interface parameters and fine-tuning models directly from MindsDB!\nExperiment with OpenAI models within MindsDB and unlock the ML capability over your data in minutes. Remember to sign-up for a free demo account and follow the tutorials, perhaps this time using your data.\nFinally, if MindsDB's vision to democratize ML sounds exciting, head to our community Slack, where you can get help and find people to chat about using other available data sources, ML frameworks, or writing a handler to bring your own!\nFollow our introduction to MindsDB's OpenAI integration here. Also, we've got a variety of tutorials that use MySQL and MongoDB:\n\nSentiment Analysis in MySQL\nText Summarization in MySQL\nSentiment Analysis in MongoDB\nQuestion Answering in MongoDB\nText Summarization in MongoDB\n\nWhat's Next?\nHave fun while trying it out yourself!\n\nBookmark MindsDB repository on GitHub.\nSign up for a free MindsDB account.\nEngage with the MindsDB community on Slack or GitHub to ask questions and share your ideas and thoughts.\n",
    "tag": "mindsdb"
  },
  {
    "title": "Introduction",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/nlp/question-answering-inside-mongodb-with-openai.mdx",
    "content": "\ntitle: Question Answering on MongoDB with MindsDB and OpenAI\nsidebarTitle: MongoDB Question Answering\n\nIntroduction\nIn this blog post, we present how to create OpenAI models within MindsDB. In this example, we ask a question to a model and get an answer. The input data is taken from our sample MongoDB database.\nPrerequisites\nTo follow along, you can sign up for an account at cloud.mindsdb.com. Alternatively, head to MindsDB documentation and follow the instructions to manually set up a local instance of MindsDB via Docker or pip.\nHow to Connect MindsDB to a Database\nWe use a collection from our MongoDB public demo database, so let\u2019s start by connecting MindsDB to it.\nYou can use Mongo Compass or Mongo Shell to connect our sample database like this:\n`bash\ntest> use mindsdb\nmindsdb> db.databases.insertOne({\n            'name': 'mongo_demo_db',\n            'engine': 'mongodb',\n            'connection_args': {\n                \"host\": \"mongodb+srv://user:MindsDBUser123!@demo-data-mdb.trzfwvb.mongodb.net/\",\n                \"database\": \"public\"\n            }\n        })`\nTutorial\nIn this tutorial, we create a predictive model to answer questions in a specified domain.\nNow that we've connected our database to MindsDB, let\u2019s query the data to be used in the example:\n`bash\nmindsdb> use mongo_demo_db\nmongo_demo_db> db.questions.find({}).limit(3)`\nHere is the output:\n`bash\n{\n  _id: '63d01350bbca62e9c77732c0',\n  article_title: 'Alessandro_Volta',\n  question: 'Was Volta an Italian physicist?',\n  true_answer: 'yes'\n}\n{\n  _id: '63d01350bbca62e9c77732c1',\n  article_title: 'Alessandro_Volta',\n  question: 'Is Volta buried in the city of Pittsburgh?',\n  true_answer: 'no'\n}\n{\n  _id: '63d01350bbca62e9c77732c2',\n  article_title: 'Alessandro_Volta',\n  question: 'Did Volta have a passion for the study of electricity?',\n  true_answer: 'yes'\n}`\nLet's create a model collection to answer all questions from the input dataset:\n`bash\nmongo_demo_db> use mindsdb\nmindsdb> db.models.insertOne({\n            name: 'question_answering',\n            predict: 'answer',\n            training_options: {\n                        engine: 'openai',\n                        prompt_template: 'answer the question of text:{{question}} about text:{{article_title}}'\n                }\n        })`\nIn practice, the `insertOne` method triggers MindsDB to generate an AI collection called `question_answering` that uses the OpenAI integration to predict a field named `answer`. The model is created inside the default `mindsdb` project. In MindsDB, projects are a natural way to keep artifacts, such as models or views, separate according to what predictive task they solve. You can learn more about MindsDB projects here.\nThe `training_options` key specifies the parameters that this handler requires.\n\nThe `engine` parameter defines that we use the `openai` engine.\nThe `prompt_template` parameter conveys the structure of a message that is to be completed with additional text generated by the model.\n\n\nIf you're using a local deployment, in order to use the OpenAI integration, you need to install the `openai` Python package. You can do this by running the following command:\n`bash\npip install openai`\n\n\nWe use the OpenAI engine to create a model in MindsDB. Please note that the `api_key` parameter is optional on cloud.mindsdb.com but mandatory for local usage/on-premise. You can obtain an OpenAI API key by signing up for OpenAI's API services on their website. Once you have signed up, you can find your API key in the API Key section of the OpenAI dashboard. You can then pass this API key to the MindsDB platform when creating models.\nTo create a `question_answering` model in MindsDB, you can run the following code:\n`bash\nmindsdb> db.models.insertOne({\n            name: 'question_answering',\n            predict: 'answer',\n            training_options: {\n                        engine: 'openai',\n                        prompt_template: 'answer the question of text:{{question}} about text:{{article_title}}',\n                              api_key: 'YOUR_OPENAI_API_KEY'  -- MANDATORY FOR LOCAL MODE\n                }\n        })`\nAlternatively, you can create a MindsDB ML engine that includes the API key, so you don't have to enter it each time. Please note that this command should be executed from MindsDB.\n`sql\nCREATE ML_ENGINE openai\nFROM openai\nUSING\n  api_key = 'YOUR_OPENAI_API_KEY';`\n\nOnce the `insertOne` method has started execution, we can check the status of the creation process with the following query:\n`bash\nmindsdb> db.getCollection('models').find({\n            'name': 'question_answering'\n        })`\nIt may take a while to register as complete depending on the internet connection. Once the creation is complete, the behavior is the same as with any other AI collection \u2013 you can query it either by specifying synthetic data in the actual query:\n`bash\nmindsdb> db.question_answering.find({\n            question: 'Was Abraham Lincoln the sixteenth President of the United States?',\n            article_title: 'Abraham_Lincoln'\n        })`\nHere is the output data:\n`bash\n{\n  answer: 'Yes, Abraham Lincoln was the sixteenth President of the United States.',\n  question: 'Was Abraham Lincoln the sixteenth President of the United States?',\n  article_title: 'Abraham_Lincoln'\n}`\nOr by joining with a collection for batch predictions:\n`bash\nmindsdb> db.question_answering.find(\n            {\n                'collection': 'mongo_demo_db.questions'\n            },\n            {\n                'question_answering.answer': 'answer',\n                'questions.question': 'question',\n                'questions.article_title': 'article_title'\n            }\n        ).limit(3)`\nHere is the output data:\n`bash\n{\n  answer: 'Yes, Volta was an Italian physicist.',\n  question: 'Was Volta an Italian physicist?',\n  article_title: 'Alessandro_Volta'\n}\n{\n  answer: 'No, Volta is not buried in the city of Pittsburgh.',\n  question: 'Is Volta buried in the city of Pittsburgh?',\n  article_title: 'Alessandro_Volta'\n}\n{\n  answer: 'Yes, Volta had a passion for the study of electricity. He was fascinated by the',\n  question: 'Did Volta have a passion for the study of electricity?',\n  article_title: 'Alessandro_Volta'\n}`\nThe `questions` collection is used to make batch predictions. Upon joining the `question_answering` model with the `questions` collection, the model uses all values from the `article_title` and `question` fields.\nLeverage the NLP Capabilities with MindsDB\nBy integrating databases and OpenAI using MindsDB, developers can easily extract insights from text data with just a few SQL commands. These powerful natural language processing (NLP) models are capable of answering questions with or without context and completing general prompts.\nFurthermore, these models are powered by large pre-trained language models from OpenAI, so there is no need for manual development work. Ultimately, this provides developers with an easy way to incorporate powerful NLP capabilities into their applications while saving time and resources compared to traditional ML development pipelines and methods. All in all, MindsDB makes it possible for developers to harness the power of OpenAI efficiently!\nMindsDB is now the fastest-growing open-source applied machine-learning platform in the world. Its community continues to contribute to more than 70 data-source and ML-framework integrations. Stay tuned for the upcoming features - including more control over the interface parameters and fine-tuning models directly from MindsDB!\nExperiment with OpenAI models within MindsDB and unlock the ML capability over your data in minutes. Remember to sign-up for a free demo account and follow the tutorials, perhaps this time using your data.\nFinally, if MindsDB's vision to democratize ML sounds exciting, head to our community Slack, where you can get help and find people to chat about using other available data sources, ML frameworks, or writing a handler to bring your own!\nFollow our introduction to MindsDB's OpenAI integration here. Also, we've got a variety of tutorials that use MySQL and MongoDB:\n\nSentiment Analysis in MySQL\nQuestion Answering in MySQL\nText Summarization in MySQL\nSentiment Analysis in MongoDB\nText Summarization in MongoDB\n\nWhat's Next?\nHave fun while trying it out yourself!\n\nBookmark MindsDB repository on GitHub.\nSign up for a free MindsDB account.\nEngage with the MindsDB community on Slack or GitHub to ask questions and share your ideas and thoughts.\n",
    "tag": "mindsdb"
  },
  {
    "title": "Introduction",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/nlp/text-summarization-inside-mongodb-with-openai.mdx",
    "content": "\ntitle: Text Summarization on MongoDB with MindsDB and OpenAI\nsidebarTitle: MongoDB Text Summarization\n\nIntroduction\nIn this blog post, we present how to create OpenAI models within MindsDB. In this example, we ask a model to provide a summary of a text. The input data is taken from our sample MongoDB database.\nPrerequisites\nTo follow along, you can sign up for an account at cloud.mindsdb.com. Alternatively, head to MindsDB documentation and follow the instructions to manually set up a local instance of MindsDB via Docker or pip.\nHow to Connect MindsDB to a Database\nWe use a collection from our MongoDB public demo database, so let\u2019s start by connecting MindsDB to it.\nYou can use Mongo Compass or Mongo Shell to connect our sample database like this:\n`bash\ntest> use mindsdb\nmindsdb> db.databases.insertOne({\n            'name': 'mongo_demo_db',\n            'engine': 'mongodb',\n            'connection_args': {\n                \"host\": \"mongodb+srv://user:MindsDBUser123!@demo-data-mdb.trzfwvb.mongodb.net/\",\n                \"database\": \"public\"\n            }\n        })`\nTutorial\nIn this tutorial, we create a predictive model to summarize an article.\nNow that we've connected our database to MindsDB, let\u2019s query the data to be used in the example:\n`bash\nmindsdb> use mongo_demo_db\nmongo_demo_db> db.articles.find({}).limit(3)`\nHere is the output:\n`bash\n{\n  _id: '63d01398bbca62e9c7774ab8',\n  article: \"Video footage has emerged of a law enforcement officer\u2026\",\n  highlights: 'The 53-second video features\u2026\"\n}\n{\n  _id: '63d01398bbca62e9c7774ab9',\n  article: \"A new restaurant is offering a five-course\u2026\",\n  highlights: \"The Curious Canine Kitchen is\u2026\"\n}\n{\n  _id: '63d01398bbca62e9c7774aba',\n  article: 'Mother-of-two Anna Tilley survived after spending four days\u2026',\n  highlights: 'Experts have warned hospitals not using standard treatment\u2026'\n}`\nLet's create a model collection to summarize all articles from the input dataset:\n`bash\nmongo_demo_db> use mindsdb\nmindsdb> db.models.insertOne({\n            name: 'text_summarization',\n            predict: 'highlights',\n            training_options: {\n                        engine: 'openai',\n                        prompt_template: 'provide an informative summary of the text text:{{article}} using full sentences'\n                }\n        })`\nIn practice, the `insertOne` method triggers MindsDB to generate an AI collection called `text_summarization` that uses the OpenAI integration to predict a field named `highlights`. The model is created inside the default `mindsdb` project. In MindsDB, projects are a natural way to keep artifacts, such as models or views, separate according to what predictive task they solve. You can learn more about MindsDB projects here.\nThe `training_options` key specifies the parameters that this handler requires.\n\nThe `engine` parameter defines that we use the `openai` engine.\nThe `prompt_template` parameter conveys the structure of a message that is to be completed with additional text generated by the model.\n\n\nWe use the OpenAI engine to create a model in MindsDB. Please note that the `api_key` parameter is optional on cloud.mindsdb.com but mandatory for local usage/on-premise. You can obtain an OpenAI API key by signing up for OpenAI's API services on their website. Once you have signed up, you can find your API key in the API Key section of the OpenAI dashboard. You can then pass this API key to the MindsDB platform when creating models.\nTo create a `text_summarization` model in MindsDB, you can run the following code:\n`bash\nmindsdb> db.models.insertOne({\n            name: 'text_summarization',\n            predict: 'highlights',\n            training_options: {\n                        engine: 'openai',\n                        prompt_template: 'provide an informative summary of the text text:{{article}} using full sentences',\n                              api_key: 'YOUR_OPENAI_API_KEY'  -- MANDATORY FOR LOCAL MODE\n                }\n        })`\nAlternatively, you can create a MindsDB ML engine that includes the API key, so you don't have to enter it each time. Please note that this command should be executed from MindsDB.\n`sql\nCREATE ML_ENGINE openai\nFROM openai\nUSING\n  api_key = 'YOUR_OPENAI_API_KEY';`\n\nIf you're using a local deployment, in order to use the OpenAI integration, you need to install the `openai` Python package. You can do this by running the following command:\n`bash\npip install openai`\n\n\nOnce the `insertOne` method has started execution, we can check the status of the creation process with the following query:\n`bash\nmindsdb> db.getCollection('models').find({\n            'name': 'text_summarization'\n        })`\nIt may take a while to register as complete depending on the internet connection. Once the creation is complete, the behavior is the same as with any other AI collection \u2013 you can query it either by specifying synthetic data in the actual query:\n`bash\nmindsdb> db.text_summarization.find({\n            article: \"Apple's Watch hits stores this Friday when customers and employees alike will be able to pre-order the timepiece. And boss Tim Cook is rewarding his staff by offering them a 50 per cent discount on the device.\"\n        })`\nHere is the output data:\n`bash\n{\n  highlights: \"Apple's Watch hits stores this Friday, and employees will be able to pre-order the\",\n  article: \"Apple's Watch hits stores this Friday when customers and employees alike will be able to pre-order the timepiece. And boss Tim Cook is rewarding his staff by offering them a 50 per cent discount on the device.\"\n}`\nOr by joining with a collection for batch predictions:\n`bash\nmindsdb> db.text_summarization.find(\n            {\n                'collection': 'mongo_demo_db.articles'\n            },\n            {\n                'text_summarization.highlights': 'highlights',\n                'articles.article': 'article'\n            }\n        ).limit(3)`\nHere is the output data:\n`bash\n{\n  highlights: 'A video has emerged of a law enforcement officer grabbing a cell phone from a woman who was',\n  article: \"Video footage has emerged of a law enforcement officer...\"\n}\n{\n  highlights: 'A new restaurant in London is offering a five-course drink-paired menu for dogs',\n  article: \"A new restaurant is offering a five-course...\"\n}\n{\n  highlights: \"Sepsis is a potentially life-threatening condition that occurs when the body's response to an\",\n  article: 'Mother-of-two Anna Tilley survived after spending four days...'\n}`\nThe `articles` collection is used to make batch predictions. Upon joining the `text_summarization` model with the `articles` collection, the model uses all values from the `article` field.\nLeverage the NLP Capabilities with MindsDB\nBy integrating databases and OpenAI using MindsDB, developers can easily extract insights from text data with just a few SQL commands. These powerful natural language processing (NLP) models are capable of answering questions with or without context and completing general prompts.\nFurthermore, these models are powered by large pre-trained language models from OpenAI, so there is no need for manual development work. Ultimately, this provides developers with an easy way to incorporate powerful NLP capabilities into their applications while saving time and resources compared to traditional ML development pipelines and methods. All in all, MindsDB makes it possible for developers to harness the power of OpenAI efficiently!\nMindsDB is now the fastest-growing open-source applied machine-learning platform in the world. Its community continues to contribute to more than 70 data-source and ML-framework integrations. Stay tuned for the upcoming features - including more control over the interface parameters and fine-tuning models directly from MindsDB!\nExperiment with OpenAI models within MindsDB and unlock the ML capability over your data in minutes. Remember to sign-up for a free demo account and follow the tutorials, perhaps this time using your data.\nFinally, if MindsDB's vision to democratize ML sounds exciting, head to our community Slack, where you can get help and find people to chat about using other available data sources, ML frameworks, or writing a handler to bring your own!\nFollow our introduction to MindsDB's OpenAI integration here. Also, we've got a variety of tutorials that use MySQL and MongoDB:\n\nSentiment Analysis in MySQL\nQuestion Answering in MySQL\nText Summarization in MySQL\nSentiment Analysis in MongoDB\nQuestion Answering in MongoDB\n\nWhat's Next?\nHave fun while trying it out yourself!\n\nBookmark MindsDB repository on GitHub.\nSign up for a free MindsDB account.\nEngage with the MindsDB community on Slack or GitHub to ask questions and share your ideas and thoughts.\n",
    "tag": "mindsdb"
  },
  {
    "title": "Train a model from the MongoDB API",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/mongo/mongo.mdx",
    "content": "Train a model from the MongoDB API\n\nNote: This is work in progress, please join our slack channel if you have any questions.\nTrain new model\nTo train a new model, you will need to `insert()` a new document inside the mindsdb.models collection.\nThe object sent to the `insert()` for training the new model should contain:\n\nname (string) -- The name of the model.\npredict (string) -- The feature you want to predict. To predict multiple features, include a list of features.\nconnection(string) -- The connection string for connecting to MongoDB. If you have used GUI to connect to MongoDB, that connection will be used.\nselect_data_query (object) -- The object that contains info about getting the data to train the model.\ndatabase(string) - The name of the database\ncollection(string) - The name of the collection\nfind(dict) - The dict that selects the documents from the collection, must be valid JSON format. Same as db.collection.find({...})\ntraining_options (dict) -- Optional value that contains additional training parameters. To train timeseries model you need to provide `training_options`.\n\n`javascript\ndb.predictors.insert({\n    'name': str,\n    'predict': str | list of fields,\n    'connection': str,  # optional\n    'select_data_query':{\n    'database': str,\n    'collection': str,\n    'find': dict\n    },\n    'training_options': dict  # optional\n})`\nFor the timeseries model:\n```\ndb.predictors.insert({\n    'name': str,\n    'predict': str | list of fields,\n    'connection': str,  # optional\n    'select_data_query':{\n    'database': str,\n    'collection': str,\n    'find': dict\n    },\n    'training_options': {\n        \"timeseries_settings\": {\n                \"order_by\": list of fields,\n                \"group_by\": list of fields,    #optional\n                \"horizon\": int,         #optional\n                \"use_previous_target\": Boolean,\n                \"window\": int\n            }\n    }\n})\n```\nTrain new model example\nThe following example shows you how to train a new model from a mongo client. The collection used for training the model is the Telcom Customer Churn dataset.\n`sql\ndb.predictors.insert({\n    'name': 'churn',\n    'predict': 'Churn',\n    'select_data_query':{\n        'database': 'test_data',\n        'collection': 'customer_churn',\n        'find': {}\n    }\n})`\n\nThis `INSERT` query will train a new model called `churn` that predicts the customer `Churn` value.\nModel training status\nTo check that the training finished successfully, you can `find()` the model status inside mindsdb.models collection e.g.:\n`javascript\ndb.predictors.find();`\n\n!!! Success \"That's it :tada: :trophy: :computer:\"\nYou have successfully trained a new model from a mongo shell. The next step is to get predictions by querying the model.\nDelete model\nTo delete the model run `remove` function on predictors collection and send the name of the model to delete as:\n`javascript\ndb.predictors.remove({ name: \"model_name\" });`\nQuery the model from MongoDB API\nTo get the predictions from the model, you will need to call `find()` method on the model collection and provide values for which you want to get prediction as an object:\n`javascript\ndb.model_name.find({ key: \"value\", key: \"value\" });`\n!!! Info \"Note\"\nThe object provided to `find()` method must be valid JSON format.\nQuery example\nThe following example shows you how to query the model from a mongo client. The collection used for training the model is the Telcom Customer Churn dataset. MindsDB will predict the customer `Churn` value based on the object values sent to `find()` method.\n`sql\ndb.churn.find({'PhoneService': 'Yes','InternetService': 'DSL', 'OnlineService': 'No','MonthlyCharges': 53.85, 'TotalCharges': 108.15, 'tenure': 2, 'PaperlessBilling': 'Yes'})`\nYou should get a response from MindsDB similar to:\n| predicted_value | confidence | info             |\n| --------------- | ---------- | ---------------- |\n| Yes             | 0.8        | Check JSON below |\n```json\n{\n  \"Churn\": \"Yes\",\n  \"Churn_confidence\": 0.8,\n  \"Churn_explain\": {\n    \"class_distribution\": {\n      \"No\": 0.44513007027299717,\n      \"Yes\": 0.5548699297270028\n    },\n    \"predicted_value\": \"Yes\",\n    \"confidence\": 0.8,\n    \"prediction_quality\": \"very confident\"\n  }\n}",
    "tag": "mindsdb"
  },
  {
    "title": "The `db.databases.insertOne()` Method",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/mongo/database.mdx",
    "content": "\ntitle: Connecting Databases\nsidebarTitle: Databases\n\nIntegrations, or external databases, provide data to be used for making\nforecasts. Here, we use the databases.insertOne() method to connect the\nintegrations to Mongo.\nThe `db.databases.insertOne()` Method\nDescription\nMindsDB enables adding databases to your Mongo instance using the\n`db.databases.insertOne()` method.\nOur MindsDB Mongo API supports creating a connection by passing the database\ncredentials.\nSyntax\nHere is the syntax:\n`sql\ndb.databases.insertOne({\n    name: \"mongo_int\",\n    engine: \"mongodb\",\n    connection_args: {\n            \"port\": 27017,\n            \"host\": \"mongodb+srv://admin:@localhost\",\n            \"database\": \"test_data\"\n    }\n});`\nOn execution, we get:\n`json\n{\n    \"acknowledged\" : true,\n    \"insertedId\" : ObjectId(\"62dff63c6cc2fa93e1d7f12c\")\n}`\nWhere:\n| Name              | Description                                                                              |\n| ----------------- | ---------------------------------------------------------------------------------------- |\n| `name`            | Identifier for the data source to be created.                                            |\n| `engine`          | Database engine to be selected.                                                          |\n| `connection_args` | `{\"key\":\"value\"}` object storing the connection parameters such as port, host, database. |\nExample\nCreating a New Connection\nHere is an example of how to connect to the local MongoDB.\n`sql\ndb.databases.insertOne({\n    name: \"mongo_local\",\n    engine: \"mongodb\",\n    connection_args: {\n            \"port\": 27017,\n            \"host\": \"mongodb+srv://admin:@localhost\",\n            \"database\": \"test_data\"\n  }\n});`\nOn execution, we get:\n`json\n{\n    \"acknowledged\" : true,\n    \"insertedId\" : ObjectId(\"62dff63c6cc2fa93e1d7f12c\")\n}`\nListing Linked Databases\nYou can list all the linked databases using the following command:\n`sql\nSHOW dbs;`\nOn execution, we get:\n```sql\n+--------------------+\n| Database           |\n+--------------------+\n| admin              |\n| files              |\n| information_schema |\n| mindsdb            |\n| mongo_int          |\n| views              |\n+--------------------+",
    "tag": "mindsdb"
  },
  {
    "title": "Example",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/mongo/insert.mdx",
    "content": "\ntitle: Creating Predictors in Mongo\nsidebarTitle: Predictors\n\nPredictors are the machine learning models that enable us to forecast future data based on the available data. By using the `db.predictors.insert()` method, we create and train predictors in Mongo.\nThe `db.predictors.insert()` Method\nDescription\nThe `db.predictors.insert()` method creates and trains a new model.\nSyntax\nHere is the syntax:\n`sql\ndb.predictors.insert({\n     name: \"predictor_name\",\n     predict: \"target_column\",\n     connection: \"integration_name\",\n     select_data_query: \"db.collection_name.find({})\"\n});`\nOn execution, we get:\n`json\nWriteResult({\n    \"nInserted\" : 1\n})`\nWhere:\n| Expressions         | Description                                                                                                                            |\n| ------------------- | -------------------------------------------------------------------------------------------------------------------------------------- |\n| `name`              | The name of the model to be created.                                                                                                   |\n| `predict`           | The name of the target column to be predicted.                                                                                         |\n| `connection`        | The name of the integration created via the db.databases.insertOne() method or file upload. |\n| `select_data_query` | Object that stores the data collection name to be used for training and validation and additional arguments for filtering the data.    |\n\nChecking Predictor Status\nAfter running the `db.predictors.insert()`\nmethod, execute the `db.predictors.find()` method from the `mindsdb.models`\ncollection to check the status of the model.\n`sql\ndb.predictors.find({name: \"model_name\"});`\n\nExample\nCreating a Predictor\nThis example shows how you can create and train the `home_rentals_model` machine\nlearning model to predict the rental prices for real estate properties inside\nthe dataset.\n`sql\ndb.predictors.insert({\n     name: \"home_rentals_model\",\n     predict: \"rental_price\",\n     connection: \"mongo_integration\",\n     select_data_query: \"db.home_rentals.find({})\"\n});`\nOn execution, we get:\n`json\nWriteResult({\n    \"nInserted\" : 1\n})`\nChecking Predictor Status\nTo check the predictor status, query the\n`mindsdb.models` table\nusing the `db.predictors.find()` command.\n`sql\ndb.predictors.find({name: \"home_rentals_model\"});`\nOn execution, we get:\n```json\n{\n  \"name\": \"home_rentals_model\",\n  \"status\": \"complete\",\n  \"accuracy\": 0.91,\n  \"predict\": \"rental_price\",\n  \"update_status\": \"up_to_date\",\n  \"mindsdb_version\": \"22.8.3.1\",\n  \"error\": null,\n  \"select_data_query\": \"\",\n  \"training_options\": \"\"\n}",
    "tag": "mindsdb"
  },
  {
    "title": "General Structure",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/mongo/collection-structure.mdx",
    "content": "\ntitle: Collection Structure\nsidebarTitle: Collection Structure\n\nGeneral Structure\nOn start-up, the MindsDB database consists of 2 collections: `databases` and\n`predictors`.\nYou can verify it by running the following MQL commands:\n`sql\nUSE mindsdb;\nSHOW collections;`\nOn execution, we get:\n`sql\n+---------------------------+\n| Collections_in_mindsdb    |\n+---------------------------+\n| databases                 |\n| predictors                |\n+---------------------------+`\nThe `predictors` Collection\nAll the trained machine learning models are visible as new documents inside the\n`predictors` collection.\nThe `predictors` collection stores information about each model in the JSON\nformat, as shown below.\n`json\n{\n  \"name\": \"model_name\",\n  \"status\": \"status\",\n  \"accuracy\": 0.999,\n  \"predict\": \"value_to_be_predicted\",\n  \"update_status\": \"update_status\",\n  \"mindsdb_version\": \"22.8.2.1\",\n  \"error\": \"error_info\",\n  \"select_data_query\": \"\",\n  \"training_options\": \"\"\n}`\nWhere:\n| Name                  | Description                                                                     |\n| --------------------- | ------------------------------------------------------------------------------- |\n| `\"name\"`              | The name of the model.                                                          |\n| `\"status\"`            | Training status (`generating`, or `training`, or `complete`, or `error`).       |\n| `\"accuracy\"`          | The model accuracy (`0.999` is a sample accuracy value).                        |\n| `\"predict\"`           | The name of the target column to be predicted.                                  |\n| `\"update_status\"`     | Training update status (`up_to_date`, or `updating`, or `available`).           |\n| `\"mindsdb_version\"`   | The MindsDB version used while training (`22.8.2.1` is a sample version value). |\n| `\"error\"`             | Error message stores a value in case of an error, otherwise, it is null.        |\n| `\"select_data_query\"` | It is required for SQL API, otherwise, it is null.                              |\n| `\"training_options\"`  | Additional training parameters.                                                 |\nThe `databases` Collection\nAll the Mongo database connections are stored inside the `databases` collection,\nas shown below.\n`json\n{\n  \"name\": \"mongo_int\",\n  \"database_type\": \"mongodb\",\n  \"host\": \"\",\n  \"port\": 27017,\n  \"user\": null\n}`\nWhere:\n| Name              | Description                          |\n| ----------------- | ------------------------------------ |\n| `\"name\"`          | The name of the integration.         |\n| `\"database_type\"` | The database type (here, `mongodb`). |\n| `\"host\"`          | The Mongo host.                      |\n| `\"port\"`          | The Mongo port.                      |",
    "tag": "mindsdb"
  },
  {
    "title": "The `db.predictors.deleteOne()` Method",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/mongo/delete.mdx",
    "content": "\ntitle: Deleting a Predictor\nsidebarTitle: Delete()\n\nThe `db.predictors.deleteOne()` Method\nDescription\nThe `db.predictors.deleteOne()` method deletes an ML model specified in its argument.\nSyntax\nHere is the syntax:\n`sql\ndb.predictors.deleteOne({name: \"predictor_name\"});`\nOn execution, we get:\n`json\n{\n  \"acknowledged\": true,\n  \"deletedCount\": 1\n}`\nWhere:\n| Name   | Description                      |\n| ------ | -------------------------------- |\n| `name` | Name of the model to be deleted. |\nExample\nListing All the Predictors\nBefore deleting a predictor, let's list all the available predictors using the\n`db.predictors.find()` method.\n`sql\ndb.predictors.find({});`\nOn execution, we get:\n`json\n{\n    \"name\": \"home_rentals_model\",\n    \"status\": \"complete\",\n    \"accuracy\": \"1.0\",\n    \"predict\": \"rental_price\",\n    \"update_status\": \"up_to_date\",\n    \"mindsdb_version\": \"22.8.3.1\",\n    \"error\": null,\n    \"select_data_query\": \"\",\n    \"training_options\": \"\"\n},\n{\n    \"name\": \"other_model\",\n    \"status\": \"complete\",\n    \"accuracy\": \"1.0\",\n    \"predict\": \"value_to_be_predicted\",\n    \"update_status\": \"up_to_date\",\n    \"mindsdb_version\": \"22.8.3.1\",\n    \"error\": null,\n    \"select_data_query\": \"\",\n    \"training_options\": \"\"\n}`\nDropping a Predictor\nThe `db.predictors.deleteOne()` method drops the model collection called\n`home_rentals_model`.\n`sql\ndb.predictors.deleteOne({name: \"home_rentals_model\"});`\nOn execution, we get:\n`json\n{\n  \"acknowledged\": true,\n  \"deletedCount\": 1\n}`\nValidating the Deletion\nYou can validate that the model was removed by listing all the predictors.\n`sql\ndb.predictors.find({});`\nOn execution, we get:\n```json\n{\n  \"name\": \"other_model\",\n  \"status\": \"complete\",\n  \"accuracy\": \"1.0\",\n  \"predict\": \"value_to_be_predicted\",\n  \"update_status\": \"up_to_date\",\n  \"mindsdb_version\": \"22.8.3.1\",\n  \"error\": null,\n  \"select_data_query\": \"\",\n  \"training_options\": \"\"\n}",
    "tag": "mindsdb"
  },
  {
    "title": "The `find()` Method",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/mongo/find.mdx",
    "content": "\ntitle: Making Predictions using ML Models\nsidebarTitle: Find()\n\nThe `find()` Method\nDescription\nThe `find()` method is used to get predictions from the model table. The data is\nnot persistent - it is returned on the fly as a result-document.\nSyntax\nHere is the syntax:\n`sql\ndb.predictor_name.find({column: \"value\", column: \"value\"});`\nOn execution, we get:\n`json\n{\n    \"column_name1\" : \"value\",\n    \"column_name2\": \"value\",\n    ...columns\n    \"select_data_query\": null,\n    \"when_data\": null,\n    \"target_name_original\": \"value\",\n    \"target_name_confidence\": \"value\",\n    \"target_name_explain\": \"{\\\"predicted_value\\\": value, \\\"confidence\\\": value, \\\"anomaly\\\": null, \\\"truth\\\": null, \\\"confidence_lower_bound\\\": value \\\"confidence_upper_bound\\\": value}\",\n    \"target_name_anomaly\": \"value\",\n    \"target_name_min\": \"value\",\n    \"target_name_max\": \"value\"\n}`\nWhere:\n| Expressions                | Description                                                                                                                                                        |\n| -------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| `\"target_name_original\"`   | The real value of the target variable from the collection.                                                                                                         |\n| `\"target_name_confidence\"` | Model confidence.                                                                                                                                                  |\n| `\"target_name_explain\"`    | JSON object that contains additional information, such as `predicted_value`, `confidence`, `anomaly`, `truth`, `confidence_lower_bound`, `confidence_upper_bound`. |\n| `\"target_name_anomaly\"`    | Model anomaly.                                                                                                                                                     |\n| `\"target_name_min\"`        | Lower bound value.                                                                                                                                                 |\n| `\"target_name_max\"`        | Upper bound value.                                                                                                                                                 |\nExample\nMaking a Single Prediction\nThe following MQL statement fetches the predicted value of the `rental_price`\ncolumn from the `home_rentals_model` model. The predicted value is the rental\nprice of a property with attributes listed as a parameter to the `find()`\nmethod.\n`sql\ndb.home_rentals_model.find({sqft: \"823\", location: \"good\", neighborhood: \"downtown\", days_on_market: \"10\"});`\nOn execution, we get:\n`json\n{\n  \"sqft\": 823,\n  \"location\": \"good\",\n  \"neighborhood\": \"downtown\",\n  \"days_on_market\": 10,\n  \"number_of_rooms\": null,\n  \"number_of_bathrooms\": null,\n  \"initial_price\": null,\n  \"rental_price\": 1431.323795180614,\n  \"select_data_query\": null,\n  \"when_data\": null,\n  \"rental_price_original\": null,\n  \"rental_price_confidence\": 0.99,\n  \"rental_price_explain\": \"{\\\"predicted_value\\\": 1431.323795180614, \\\"confidence\\\": 0.99, \\\"anomaly\\\": null, \\\"truth\\\": null, \\\"confidence_lower_bound\\\": 1379.4387560440227, \\\"confidence_upper_bound\\\": 1483.2088343172054}\",\n  \"rental_price_anomaly\": null,\n  \"rental_price_min\": 1379.4387560440227,\n  \"rental_price_max\": 1483.2088343172054\n}`\nMaking Bulk Predictions\n\nBulk Predictions WIP\nThe bulk predictions is a work in progress.",
    "tag": "mindsdb"
  },
  {
    "title": "The `stats()` Method",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/mongo/stats.mdx",
    "content": "\ntitle: Getting the Statistics\nsidebarTitle: Stats()\n\nThe `stats()` Method\nThe `stats()` method is used to display the attributes of an existing model. It accepts the `{scale: \"attribute\"}` object as an argument.\nHere is how to call the `stats()` method:\n`sql\ndb.predictor_name.stats({scale: \"attribute\"});`\nWhere:\n| Name                   | Description                                                                                                                                   |\n| ---------------------- | --------------------------------------------------------------------------------------------------------------------------------------------- |\n| `predictor_name`       | The name of the predictor whose statistics you want to see.                                                                                   |\n| `{scale: \"attribute\"}` | The argument of the `stats()` method defines the type of statistics (`{scale: \"features\"}`, or `{scale: \"model\"}`, or `{scale: \"ensemble\"}`). |\nThe `stats()` Method with the `{scale: \"features\"}` Parameter\nDescription\nThe `db.predictor_name.stats({scale: \"features\"})` method is used to display the\nway the model encoded the data before training.\nSyntax\nHere is the syntax:\n`sql\ndb.predictor_name.stats({scale: \"features\"});`\nOn execution, we get:\n`json\n{\n  \"data\": [\n    {\n      \"column\": \"number_of_rooms\",\n      \"type\": \"categorical\",\n      \"encoder\": \"OneHotEncoder\",\n      \"role\": \"feature\"\n    }\n  ]\n}`\nWhere:\n| Name        | Description                                 |\n| ----------- | ------------------------------------------- |\n| `\"column\"`  | The name of the column.                     |\n| `\"type\"`    | Type of the inferred data.                  |\n| `\"encoder\"` | Encoder used.                               |\n| `\"role\"`    | Role of the column (`feature` or `target`). |\nExample\nLet's describe the `home_rentals_model` model.\n`sql\ndb.home_rentals_model.stats({scale: \"features\"});`\nOn execution, we get:\n`json\n{\n  \"data\": [\n    {\n      \"column\": \"number_of_rooms\",\n      \"type\": \"categorical\",\n      \"encoder\": \"OneHotEncoder\",\n      \"role\": \"feature\"\n    },\n    {\n      \"column\": \"number_of_bathrooms\",\n      \"type\": \"binary\",\n      \"encoder\": \"BinaryEncoder\",\n      \"role\": \"feature\"\n    },\n    {\n      \"column\": \"sqft\",\n      \"type\": \"float\",\n      \"encoder\": \"NumericEncoder\",\n      \"role\": \"feature\"\n    },\n    {\n      \"column\": \"location\",\n      \"type\": \"categorical\",\n      \"encoder\": \"OneHotEncoder\",\n      \"role\": \"feature\"\n    },\n    {\n      \"column\": \"days_on_market\",\n      \"type\": \"integer\",\n      \"encoder\": \"NumericEncoder\",\n      \"role\": \"feature\"\n    },\n    {\n      \"column\": \"initial_price\",\n      \"type\": \"integer\",\n      \"encoder\": \"NumericEncoder\",\n      \"role\": \"feature\"\n    },\n    {\n      \"column\": \"neighborhood\",\n      \"type\": \"categorical\",\n      \"encoder\": \"OneHotEncoder\",\n      \"role\": \"feature\"\n    },\n    {\n      \"column\": \"rental_price\",\n      \"type\": \"float\",\n      \"encoder\": \"NumericEncoder\",\n      \"role\": \"target\"\n    }\n  ],\n  \"ns\": \"mindsdb.home_rentals_model\"\n}`\nThe `stats()` Method with the `{scale: \"model\"}` Parameter\nDescription\nThe `db.predictor_name.stats({scale: \"model\"})` method is used to display the\nperformance of the candidate models.\nSyntax\nHere is the syntax:\n`sql\ndb.predictor_name.stats({scale: \"model\"});`\nOn execution, we get:\n`json\n{\n  \"data\": [\n    {\n       \"name\" : \"candidate_model\",\n       \"performance\" : <0.0|1.0>,\n       \"training_time\" : <seconds>,\n       \"selected\" : <0|1>\n    }\n  ]\n}`\nWhere:\n| Name              | Description                                              |\n| ----------------- | -------------------------------------------------------- |\n| `\"name\"`          | Name of the candidate model.                             |\n| `\"performance\"`   | Accuracy from 0 to 1 depending on the type of the model. |\n| `\"training_time\"` | Time elapsed for the training of the model.              |\n| `\"selected\"`      | `1` for the best performing model and `0` for the rest.  |\nExample\nLet's see the output for the `home_rentals_model` model.\n`sql\ndb.home_rentals_model.stats({scale: \"model\"});`\nOn execution, we get:\n`json\n{\n  \"data\": [\n    {\n      \"name\": \"Neural\",\n      \"performance\": 0.999,\n      \"training_time\": 48.37,\n      \"selected\": 0\n    },\n    {\n      \"name\": \"LightGBM\",\n      \"performance\": 1,\n      \"training_time\": 33,\n      \"selected\": 1\n    },\n    {\n      \"name\": \"Regression\",\n      \"performance\": 0.999,\n      \"training_time\": 0.05,\n      \"selected\": 0\n    }\n  ],\n  \"ns\": \"mindsdb.home_rentals_model\"\n}`\nThe `stats()` Method with the `{scale: \"ensemble\"}` Parameter\nDescription\nThe `db.predictor_name.stats({scale: \"ensemble\"})` method is used to display the\nparameters used to select the best candidate model.\nSyntax\nHere is the syntax:\n`sql\ndb.predictor_name.stats({scale: \"ensemble\"});`\nOn execution, we get:\n`sql\n+-----------------+\n| ensemble        |\n+-----------------+\n| {JSON}          |\n+-----------------+`\nWhere:\n| Name       | Description                                                                                |\n| ---------- | ------------------------------------------------------------------------------------------ |\n| `ensemble` | Object of the JSON type describing the parameters used to select the best candidate model. |\nExample\n\nExample WIP This example is a work in progress.\n\n\nNeed More Info? If you need more information on how to describe your model or understand the\nresults, feel free to ask us on the\ncommunity Slack workspace.",
    "tag": "mindsdb"
  },
  {
    "title": "Setup",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/connect/metabase.mdx",
    "content": "\ntitle: MindsDB and Metabase\nsidebarTitle: Metabase\n\nMetabase is open-source software that facilitates data analysis. It lets you visualize your data easily and intuitively. Now that MindsDB supports the MySQL binary protocol, you can connect it to Metabase and see the forecasts by creating and training the models.\nFor more information, visit Metabase.\nSetup\nMindsDB\nLet's set up the MindsDB following one of the guides from the Deployment\nsection. Please note that the MindsDB Cloud is not yet supported.\n\nCurrently, only local and on-premise installations are stable\n\nYou can choose one of the following approaches to set up the MindsDB locally:\n\nDocker\nWindows via pip\nLinux via pip\nMacOS via pip\nSourcecode via pip\n\nHere, we use the Docker setup for MindsDB.\nMetabase\nNow, let's set up the Metabase by following one of the approaches presented on\nthe Metabase Open Source Edition page.\nHere, we use the\n.jar approach\nfor Metabase.\nHow to Connect\nFollow the steps below to connect your MindsDB to Metabase.\n\nOpen your Metabase and navigate to the Admin settings by clicking the cog\n   in the bottom left corner.\nOnce there, click on Databases in the top navigation bar.\nClick on Add database in the top right corner.\nFill in the form using the following data:\n\n`text\n   Database type: `MySQL`\n   Display name: `MindsDB`\n   Host: `localhost`\n   Port: `47335`\n   Database name: `mindsdb`\n   Username: `mindsdb`\n   Password: *leave it empty*`\n\n\n\n\nClick on Save.\n\nNow you're connected!\n\n\n\nExample\nNow that the connection between MindsDB and Metabase is established, let's do\nsome examples.\nMost of the SQL statements that you usually run in your\nMindsDB SQL Editor can be run in Metabase as well.\nLet's start with something easy.\nOn your Metabase's home page, click on New > SQL query in the top right corner\nand then, select your MindsDB database.\nLet's execute the following command in the editor.\n`sql\nSHOW TABLES;`\nOn execution, we get:\n\n\n\nPlease note that creating a\ndatabase connection using\nthe `CREATE DATABASE` statement fails because of the curly braces (`{}`) being\nused by JDBC as the escape sequences.\n`sql\nCREATE DATABASE example_db\n    WITH ENGINE = \"postgres\",\n    PARAMETERS = {\n        \"user\": \"demo_user\",\n        \"password\": \"demo_password\",\n        \"host\": \"3.220.66.106\",\n        \"port\": \"5432\",\n        \"database\": \"demo\"\n};`\nOn execution, we get:\n\n\n\nYou can overcome this issue using the\nMindsDB SQL Editor to create a database.\nNow, getting back to the Metabase, let's run some queries on the database\ncreated with the help of the MindsDB SQL Editor.\n`sql\nSELECT *\nFROM example_db.demo_data.home_rentals\nLIMIT 10;`\nOn execution, we get:\n\n\n\nWhat's Next?\nNow that you are all set, we recommend you check out our Tutorials and\nCommunity Tutorials sections, where you'll find various examples of\nregression, classification, and time series predictions with MindsDB.\nTo learn more about MindsDB itself, follow the guide on\nMindsDB database structure. Also, don't miss out on the\nremaining pages from the SQL API section, as they explain a common SQL\nsyntax with examples.",
    "tag": "mindsdb"
  },
  {
    "title": "Overview",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/connect/mongo-compass.mdx",
    "content": "\ntitle: MindsDB and MongoDB Compass\nsidebarTitle: MongoDB Compass\n\n\n\nMongoDB Compass is a graphical user interface (GUI) for MongoDB. It provides detailed schema visualizations, real-time performance metrics, sophisticated querying abilities, and more. You can download MongoDB Compass here.\nOverview\nHere is an overview of the connection between MindsDB and MongoDB Compass:\n\n\n\nLet's go through the steps presented above:\n\n\nWe connect MongoDB Compass to MindsDB. It is discussed in the following content.\n\n\nWe connect MindsDB to a database. You can use the CREATE DATABASE statement and run it from MindsDB, passing all required database connection details.\n\n\nHaving completed steps 1 and 2, you can access the connected database from MongoDB Compass via MindsDB.\n\n\nHow to Connect\nHere is how to connect MongoDB Compass to MindsDB using either MindsDB Cloud or local installation.\n\n\n\n\n```First, create a new connection in MongoDB Compass by clicking the `New Connection` button in the left navigation panel.\n\nThe host value is `127.0.0.1` and the port value is `47336`. Input it in the `Host` field, as below.\n\n<p align=\"center\">\n  <img src=\"/assets/connect_mongo_compass_3.png\" />\n</p>\n```\n\n\n\n\n\n\n```First, create a new connection in MongoDB Compass by clicking the `New Connection` button in the left navigation panel.\n\nThe host value is `cloud.mindsdb.com` and the port value is `27017`. Input it in the `Host` field, as below.\n\n<p align=\"center\">\n  <img src=\"/assets/connect_mongo_compass_1.png\" />\n</p>\n\nNext, go to the `Authentication` tab and input the username and password to your MindsDB Cloud account, as below.\n\n<p align=\"center\">\n  <img src=\"/assets/connect_mongo_compass_2.png\" />\n</p>\n```\n\n\n\n\nWhat's Next?\nNow that you are all set, we recommend you check out our Tutorials section, where you'll find various examples of regression, classification, and time series predictions with MindsDB.\nTo learn more about MindsDB itself, follow the guide on MindsDB collection structure. Also, don't miss out on the remaining pages from the Mongo API section, as these explain common MQL syntax with examples.\nHave fun!\n\nFrom Our Community\nCheck out the video guides created by our community:\n\n\nVideo guide on How to connect mongo compass to MindsDB\n  by HellFire\n\n\nVideo guide on Integrating your MindsDB instance into MongoDB\n  by Syed Zubeen\n\n",
    "tag": "mindsdb"
  },
  {
    "title": "What's Next?",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/connect/deepnote.mdx",
    "content": "\ntitle: MindsDB and Deepnote\nsidebarTitle: Deepnote\n\n\nWe have worked with the team at Deepnote, and built native integration to Deepnote notebooks.\nPlease check:\n\nDeepnote Demo Guide\nDeepnote Integration Docs\n\n\nWhat's Next?\nNow that you are all set, we recommend you check out our Tutorials and\nCommunity Tutorials sections, where you'll find various examples of\nregression, classification, and time series predictions with MindsDB.\nTo learn more about MindsDB itself, follow the guide on\nMindsDB database structure. Also, don't miss out on the\nremaining pages from the SQL API section, as they explain a common SQL\nsyntax with examples.",
    "tag": "mindsdb"
  },
  {
    "title": "How to Connect",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/connect/tableau.mdx",
    "content": "\ntitle: MindsDB and Tableau\nsidebarTitle: Tableau\n\n\n\nTableau lets you visualize your data easily and intuitively. Now that MindsDB\nsupports the MySQL binary protocol, you can connect it to Tableau and see the\nforecasts.\nHow to Connect\nFollow the steps below to connect your MindsDB to Tableau.\nFirst, create a new workbook in Tableau and open the Connectors tab in the\nConnect to Data window.\n\n\n\nNext, choose MySQL and provide the details of your MindsDB connection, such as\nthe IP, port, and database name. Optionally, you can provide a username and\npassword. Then, click Sign In.\n\n\n\nNow you're connected!\nOverview of MindsDB in Tableau\nThe content of your MindsDB is visible in the right-side pane.\n\n\n\nAll the predictors are listed under the Table section. You can also switch\nbetween the integrations, such as mindsdb or files, in the Database\nsection using the drop-down.\n\n\n\nNow, let's run some examples!\nExamples\nExample 1\nPreviewing one of the tables from the mysql integration:\n\n\n\nExample 2\nThere is one technical limitation. Namely, we cannot join tables from different\ndatabases/integrations in Tableau. To overcome this challenge, you can use\neither views or custom SQL queries.\n\nPreviewing a view that joins a data table with a predictor table:\n\n\n\n\n\nUsing a custom SQL query by clicking the New Custom SQL button in the\n  right-side pane:\n\n\n\n\nWhat's Next?\nNow that you are all set, we recommend you check out our Tutorials and\nCommunity Tutorials sections, where you'll find various examples of\nregression, classification, and time series predictions with MindsDB.\nTo learn more about MindsDB itself, follow the guide on\nMindsDB database structure. Also, don't miss out on the\nremaining pages from the SQL API section, as they explain a common SQL\nsyntax with examples.\n\nFrom Our Community\nCheck out the articles and video guides created by our community:\n\n\nArticle on Predicting & Visualizing Hourly Electricity Demand in the US with MindsDB and Tableau\n  by Teslim Odumuyiwa\n\n\nArticle on Predicting & Visualizing Petroleum Production with MindsDB and Tableau\n  by Teslim Odumuyiwa\n\n\nArticle on Predicting & Visualizing Gas Prices with MindsDB and Tableau\n  by Teslim Odumuyiwa\n\n\nArticle on How To Visualize MindsDB Predictions with Tableau\n  by Ephraimx\n\n\nVideo guide on Connecting MindsDB to Tableau\n  by Alissa Troiano\n\n\nVideo guide on Visualizing prediction result in Tableau by\n  Teslim Odumuyiwa\n\n\n\nHave fun!\n\nFrom Our Community\nCheck out the articles and video guides created by our community:\n\n\nArticle on Predicting & Visualizing Hourly Electricity Demand in the US with MindsDB and Tableau\n  by Teslim Odumuyiwa\n\n\nArticle on Predicting & Visualizing Petroleum Production with MindsDB and Tableau\n  by Teslim Odumuyiwa\n\n\nArticle on Predicting & Visualizing Gas Prices with MindsDB and Tableau\n  by Teslim Odumuyiwa\n\n\nArticle on How To Visualize MindsDB Predictions with Tableau\n  by Ephraimx\n\n\nVideo guide on Connecting MindsDB to Tableau\n  by Alissa Troiano\n\n\nVideo guide on Visualizing prediction result in Tableau by\n  Teslim Odumuyiwa\n\n",
    "tag": "mindsdb"
  },
  {
    "title": "How to Connect",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/connect/mysql-client.mdx",
    "content": "\ntitle: MindsDB and MySQL CLI\nsidebarTitle: MySQL CLI\n\nMindsDB provides a powerful MySQL API that allows users to connect to it\nusing the MySQL Command Line Client.\nPlease note that connecting to MindsDB's MySQL API is the same as connecting to\na MySQL database. Find more information on MySQL CLI\nhere.\nHow to Connect\nTo connect to MindsDB, run the below command in your MySQL CLI and provide the\nconnection details, such as host, port, username, and password.\n`bash\nmysql -h [hostname] --port [TCP/IP port number] -u [user] -p [password]`\nHere are the commands that allow you to connect to either a local MindsDB\ninstallation or a MindsDB Cloud instance.\n\n\n\n``````bash Self-Hosted Local Deployment\nmysql -h 127.0.0.1 --port 47335 -u mindsdb\n```\n\n```bash MindsDB Cloud\nmysql -h cloud.mindsdb.com --port 3306 -u [mindsdb_cloud_username] -p [mindsdb_cloud_password]\n```\n```\n\n\n\nOn execution, we get:\n```bash\nWelcome to the MariaDB monitor.  Commands end with ; or \\g.\nServer version: 5.7.1-MindsDB-1.0 (MindsDB)\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\nMySQL [(none)]>\n```\nExample\nIn this example, we connect to the MindsDB Cloud instance, as below.\n`bash\nmysql -h cloud.mindsdb.com --port 3306 -u zoran@mindsdb.com -p`\nOn execution, we get:\n```bash\nEnter password:\nWelcome to the MariaDB monitor.  Commands end with ; or \\g.\nServer version: 5.7.1-MindsDB-1.0 (MindsDB)\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\nMySQL [(none)]>\n```\nWhat's Next?\nNow that you are all set, we recommend you check out our Tutorials and\nCommunity Tutorials sections, where you'll find various examples of\nregression, classification, and time series predictions with MindsDB.\nTo learn more about MindsDB itself, follow the guide on\nMindsDB database structure. Also, don't miss out on the\nremaining pages from the SQL API section, as they explain a common SQL\nsyntax with examples.",
    "tag": "mindsdb"
  },
  {
    "title": "Usage",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/connect/dbt.mdx",
    "content": "\ntitle: MindsDB and DBT\nsidebarTitle: Using MindsDB with DBT\n\nDBT is a development framework that facilitates data transformation\nprocesses. You can read more about DBT here.\nUsage\nInstalling Adapter for DBT\nMindsDB provides an adapter to integrate your predictions into the DBT workflow.\nTo find out more about the dbt-mindsdb\nadapter, follow the instructions\nhere.\nYou can install the dbt-mindsdb\nadapter by executing this command: `pip install dbt-mindsdb`.\nProfile Setup\n\n\nCreate a DBT project:\n`bash\ndbt init [project_name]`\n\n\nConfigure your `profiles.yml` file:\n\n  What MindsDB Supports\" Currently, MindsDB supports only user/password\n  authentication. Please see below `~/.dbt/profiles.yml` file for details.\n\n\n`yml Self-Hosted Local Deployment\n    mindsdb:\n      outputs:\n      dev:\n        type: mindsdb\n        database: \"mindsdb\"\n        host: \"127.0.0.1\"\n        port: 47335\n        schema: \"mindsdb\"\n        username: \"mindsdb\"\n        password: \"\"\n      target: dev`\n`yml MindsDB Cloud\n  mindsdb:\n    outputs:\n    dev:\n      type: mindsdb\n      database: \"mindsdb\"\n      host: \"cloud.mindsdb.com\"\n      port: 3306\n      schema: \"[dbt schema]\"\n      username: \"[mindsdb cloud username]\" # your Mindsdb Cloud email address is your username\n      password: \"[mindsdb cloud password]\"\n    target: dev`\n\n\n\nLet's list the required data fields.\n| Key        | Required | Description                                      | Example                                  |\n| ---------- | :------: | ------------------------------------------------ | ---------------------------------------- |\n| `type`     |    \u2714\ufe0f    | The adapter name                                 | `mindsdb`                                |\n| `database` |    \u2714\ufe0f    | The database name                                | `mindsdb`                                |\n| `host`     |    \u2714\ufe0f    | The MindsDB (hostname) to connect to             | `cloud.mindsdb.com`                      |\n| `port`     |    \u2714\ufe0f    | The port to be used                              | `47335` or `3306`                        |\n| `schema`   |    \u2714\ufe0f    | The schema (database) where models are built     | The MindsDB data source                  |\n| `username` |    \u2714\ufe0f    | The username to be used to connect to the server | `mindsdb` or MindsDB Cloud user          |\n| `password` |    \u2714\ufe0f    | The password to be used for authentication       | local password or MindsDB Cloud password |\nCreating a Predictor\nCreate a `table_name.sql` file where `table_name` is the name of the predictor.\n`sql\n{{\n    config(\n        materialized='predictor',\n        integration='photorep',\n        predict='name',\n        predict_alias='predicted_name',\n        using={\n            'encoders.location.module': 'CategoricalAutoEncoder',\n            'encoders.rental_price.module': 'NumericEncoder'\n        }\n    )\n}}\n    SELECT *\n    FROM stores;`\nLet's list the required and optional data fields.\n| Parameter       | Required | Description                                                                               | Example          |\n| --------------- | :------: | ----------------------------------------------------------------------------------------- | ---------------- |\n| `materialized`  |    \u2714\ufe0f    | Use always the value `predictor`                                                          | `predictor`      |\n| `integration`   |    \u2714\ufe0f    | Create an integration in MindsDB that is used to get the data from and save the result to | `photorep`       |\n| `predict`       |    \u2714\ufe0f    | Field to be predicted                                                                     | `name`           |\n| `predict_alias` |          | Alias for predicted field                                                                 | `predicted_name` |\n| `using`         |          | Configuration options for trained model                                                   | ...              |\nCreating Predictions Table\nCreate a `table_name.sql` file where `table_name` is used as the name of the\npredictor. Or you can create a `schema_name.table_name.sql` file where\n`schema_name` is the name of the integration and `table_name` is the name of the\npredictor.\n`sql\n{{\n    config(\n        materialized='table',\n        predictor_name='store_predictor',\n        integration='photorep'\n    )\n}}\n    SELECT a, bc\n    FROM ddd\n    WHERE name > latest;`\nLet's list the required data fields.\n| Parameter        | Required | Description                                                                               | Example           |\n| ---------------- | :------: | ----------------------------------------------------------------------------------------- | ----------------- |\n| `materialized`   |    \u2714\ufe0f    | Use always the value `table`                                                              | `table`           |\n| `predictor_name` |    \u2714\ufe0f    | Name of the predictor model from `CREATE MODEL`                                           | `store_predictor` |\n| `integration`    |    \u2714\ufe0f    | Create an integration in MindsDB that is used to get the data from and save the result to | `photorep`        |\n\n  Note that each time DBT is run, the results table is overwritten.\n\nTesting\n\nInstall dev requirements:\n\n`bash\n   pip install -r dev_requirements.txt`\n\nRun tests:\n\n`bash\n   python -m pytest tests/`\nWhat's Next?\nNow that you are all set, we recommend you check out our Tutorials and\nCommunity Tutorials sections, where you'll find various examples of\nregression, classification, and time series predictions with MindsDB.\nTo learn more about MindsDB itself, follow the guide on\nMindsDB database structure. Also, don't miss out on the\nremaining pages from the SQL API section, as they explain a common SQL\nsyntax with examples.",
    "tag": "mindsdb"
  },
  {
    "title": "Data Setup",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/connect/dbeaver.mdx",
    "content": "\ntitle: MindsDB and DBeaver\nsidebarTitle: DBeaver\n\nDBeaver is a database tool that allows you to connect to and work with\nvarious database engines. You can download it here.\nData Setup\nFirst, create a new database connection in DBeaver by clicking the icon, as\nshown below.\n\n\n\nNext, choose the MySQL database engine and click the Next button.\n\nIf you have multiple `MySQL` options, choose the `Driver for MySQL8 and later`.\n\n\n\n\nNow it's time to fill in the connection details.\n\n\n\nThere are two options, as below.\n\n\n    You can connect to your MindsDB Cloud account. To do that, please use the connection details below:\n\n\n``````\nHostname: `cloud.mindsdb.com`\nPort: `3306`\nUsername: <your MindsDB Cloud username>\nPassword: <your MindsDB Cloud password>\nDatabase: <leave it empty>\n```\n```\n\n\n\n\n    You can connect to your local MindsDB. To do that, please use the connection details below:\n\n\n``````\nHostname: `127.0.0.1`\nPort: `47334`\nUsername: `mindsdb`\nPassword: <leave it empty>\nDatabase: <leave it empty>\n```\n```\n\n\n\n\nNow we are ready to test the connection.\nTesting the Connection\nClick on the `Test Connection...` button to check if all the provided data\nallows you to connect to MindsDB.\nOn success, you should see the message, as below.\n\n\n\nLet's Run Some Queries\nTo finally make sure that our MinsdDB database connection works, let's run some\nqueries.\n`sql\nSHOW FULL DATABASES;`\nOn execution, we get:\n`sql\n+----------------------+---------+--------+\n| Database             | TYPE    | ENGINE |\n+----------------------+---------+--------+\n| information_schema   | system  | [NULL] |\n| mindsdb              | project | [NULL] |\n| files                | data    | files  |\n+----------------------+---------+--------+`\nHere is how it looks in DBeaver:\n\n\n\n\nWhitelist MindsDB Cloud IP address\nIf you need to whitelist the MindsDB Cloud IP address to gain access to your\ndatabase, reach out to the MindsDB team, and we'll share the MindsDB Cloud\nstatic IP address with you.\n\nWhat's Next?\nNow that you are all set, we recommend you to check out our Tutorials section where you'll find various examples of\nregression, classification, and time series predictions with MindsDB or Community Tutorials list.\nTo learn more about MindsDB itself, follow the guide on\nMindsDB database structure. Also, don't miss out on the\nremaining pages from the SQL API section, as they explain a common SQL\nsyntax with examples.",
    "tag": "mindsdb"
  },
  {
    "title": "Overview",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/connect/mongo-shell.mdx",
    "content": "\ntitle: MindsDB and MongoDB Shell\nsidebarTitle: MongoDB Shell\n\nMongoDB Shell is the quickest way to connect and work with MongoDB.\nMindsDB provides a powerful MongoDB API, allowing users to connect MindsDB to the MongoDB Shell. Please note that connection to MongoDB API provided by MindsDB is the same as connection to a MongoDB database. You can download MongoDB Shell here.\nOverview\nHere is an overview of the connection between MindsDB and MongoDB Shell:\n\n\n\nLet's go through the steps presented above:\n\n\nWe connect MongoDB Shell to MindsDB. It is discussed in the following content.\n\n\nWe connect MindsDB to a database. You can use the CREATE DATABASE statement and run it from MindsDB, passing all required database connection details.\n\n\nHaving completed steps 1 and 2, you can access the connected database from MongoDB Shell via MindsDB.\n\n\nHow to Connect\nHere is how to connect MongoDB Shell to MindsDB using either MindsDB Cloud or local installation.\nUpon opening the MongoDB Shell, you see the following message:\n\n\n\nLet's look at the connection strings for both MindsDB Cloud and local installation.\n\n\n\n\n```Provide your local MindsDB connection string to connect to a local MindsDB installation. You can copy the connection string from the MongoDB Compass if you have already created a connection there.\n\nHere is a connection string to connect to a local MindsDB installation:\n\n```bash\nmongodb://127.0.0.1:47336/\n```\n```\n\n\n\n\n\n\n```Here is a connection string to connect to the MindsDB Cloud account. You can copy the connection string from the MongoDB Compass if you have already created a connection there.\n\n```bash\nmongodb://mindsdb_cloud_username:mindsdb_cloud_password@cloud.mindsdb.com:27017/?authMechanism=DEFAULT\n```\n\nPlease replace the `mindsdb_cloud_username` placeholder with your MindsDB Cloud account email. Also, replace the `mindsdb_cloud_password` placeholder with your MindsDB Cloud password. The host value is `cloud.mindsdb.com` and the port value is `27017`. We use the default authentication mechanism.\n\nLet's look at an example:\n\n<p align=\"center\">\n  <img src=\"/assets/connect_mongo_shell_2.png\" />\n</p>\n```\n\n\n\n\nWhat's Next?\nNow that you are all set, we recommend you check out our Tutorials section, where you'll find various examples of regression, classification, and time series predictions with MindsDB.\nTo learn more about MindsDB itself, follow the guide on MindsDB collection structure. Also, don't miss out on the remaining pages from the Mongo API section, as these explain common MQL syntax with examples.\nHave fun!\n\nFrom Our Community\nCheck out the video guide created by our community:\n\nVideo guide on Easily connect to MindsDB Cloud from MongoShell\n  by @akhilcoder\n",
    "tag": "mindsdb"
  },
  {
    "title": "How to Connect",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/connect/sql-alchemy.mdx",
    "content": "\ntitle: MindsDB and SQL Alchemy\nsidebarTitle: SQL Alchemy\n\nSQL Alchemy is a Python SQL toolkit, that provides\nobject-relational mapping features for the Python programming language.\nSQL Alchemy facilitates working with databases and Python. You can download it\nhere or run a `pip install sqlalchemy`.\nHow to Connect\nPlease follow the instructions below to connect your MindsDB to SQL Alchemy.\n\n\n        You can use the Python code below to connect your Cloud MindsDB database to SQL Alchemy.\n\n\n```    Make sure you have the *pymysql* module installed before executing the Python code. To install it, run the `pip install pymysql` command.\n\n    ```python\n    from sqlalchemy import create_engine\n\n    user = 'MindsDB Cloud username' # your Mindsdb Cloud email address is your username\n    password = 'MindsDB Cloud password' # replace this value\n    host = 'cloud.mindsdb.com'\n    port = 3306\n    database = ''\n\n    def get_connection():\n        return create_engine(\n            url=\"mysql+pymysql://{0}:{1}@{2}:{3}/{4}\".format(user, password, host, port, database)\n        )\n\n    if __name__ == '__main__':\n        try:\n            engine = get_connection()\n            engine.connect()\n            print(f\"Connection to the {host} for user {user} created successfully.\")\n        except Exception as ex:\n            print(\"Connection could not be made due to the following error: \\n\", ex)\n    ```\n\n    Please note that we use the following connection details:\n\n    - Username is your MindsDB Cloud email address\n    - Password is your MindsDB Cloud password\n    - Host is `cloud.mindsdb.com`\n    - Port is `3306`\n    - Database name is left empty\n\n    To create a database connection, execute the code above. On success, the following output is expected:\n\n    ```bash\n    Connection to the cloud.mindsdb.com for user MindsDB-Cloud-Username created successfully.\n    ```\n\n    </Tab>\n    <Tab title=\"Connecting Local MindsDB to SQL Alchemy\">\n    You can use the Python code below to connect your Cloud MindsDB database to SQL Alchemy.\n\n    Make sure you have the *pymysql* module installed before executing the Python code. To install it, run the `pip install pymysql` command.\n\n    ```python\n    from sqlalchemy import create_engine\n\n    user = 'mindsdb'\n    password = ''\n    host = '127.0.0.1'\n    port = 47335\n    database = ''\n\n    def get_connection():\n        return create_engine(\n            url=\"mysql+pymysql://{0}:{1}@{2}:{3}/{4}\".format(user, password, host, port, database)\n        )\n\n    if __name__ == '__main__':\n        try:\n            engine = get_connection()\n            engine.connect()\n            print(f\"Connection to the {host} for user {user} created successfully.\")\n        except Exception as ex:\n            print(\"Connection could not be made due to the following error: \\n\", ex)\n    ```\n\n    Please note that we use the following connection details:\n\n    - Username is `mindsdb`\n    - Password is left empty\n    - Host is `127.0.0.1`\n    - Port is `47335`\n    - Database name is left empty\n\n    To create a database connection, execute the code above. On success, the following output is expected:\n\n    ```bash\n    Connection to the 127.0.0.1 for user mindsdb created successfully.\n    ```\n    </Tab>\n```\n\n\n\n\n\n  The Sqlachemy `create_engine` is lazy. This implies any human error when\n  entering the connection details would be undetectable until an action becomes\n  necessary, such as when calling the `execute` method to execute SQL commands.\n\nWhat's Next?\nNow that you are all set, we recommend you check out our Tutorials and\nCommunity Tutorials sections, where you'll find various examples of\nregression, classification, and time series predictions with MindsDB.\nTo learn more about MindsDB itself, follow the guide on\nMindsDB database structure. Also, don't miss out on the\nremaining pages from the SQL API section, as they explain a common SQL\nsyntax with examples.\nHave fun!\n\nFrom Our Community\nCheck out the articles and video guides created by our community:\n\n\nArticle on Predict Diamond prices with SQL Alchemy\n  by Teslim Odumuyiwa\n\n\nArticle on Predicting Home Rental Prices with MindsDB in Python\n  by Temidayo\n\n\nVideo guide on How to connect to MindsDB with SQL Alchemy(Python)\n  by Nishant Sapkota\n\n",
    "tag": "mindsdb"
  },
  {
    "title": "How to Connect",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/connect/jupysql.mdx",
    "content": "\ntitle: MindsDB and Jupyter Notebooks\nsidebarTitle: Jupyter Notebooks\n\nJupysql - full SQL client on Jupyter. It allows you to run SQL and plot large\ndatasets in Jupyter via a %sql and %%sql magics. It also allows users plotting\nthe data directly from the DB ( via %sqlplot magics).\nJupysql facilitates working with databases and Jupyter. You can download it\nhere or run a `pip install jupysql`.\nHow to Connect\nPre-requisite:\n\nMake sure you have jupysql installed: To install it, run `pip install jupysql`\nMake sure you have pymysql installed: To install it, run `pip install pymysql`\n\n\n  You can easily verify the installation of jupysql by running this code:\n`python\n  %load_ext sql`\nThis command loads the package and allows you to run cell magics on top of Jupyter.\nAnd for pymysql, validate by running this commad:\n`python\n  import pymysql`\n\nPlease follow the instructions below to connect into your MindsDB via Jupysql and Jupyter.\n\n\n        You can use the Python code below to connect your Jupyter notebook (or lab) to Cloud MindsDB database (via Jupysql).\n\n\n```    Load the extension:\n\n    ```python\n    %load_ext sql\n    ```\n\n    Connect to your DB:\n    *Make sure* to update the user and password in the connection string.\n    user - your Mindsdb Cloud email address is your username\n    ```python\n    %sql mysql+pymysql://<mindsdb_user>:<mindsdb_pass>@cloud.mindsdb.com:3306/mindsdb\n    ```\n\n    Testing connection by listing the existing tables (pure SQL):\n\n    ```python\n    %sql show tables\n    ```\n\n    Please note that we use the following connection details:\n\n    - Username is your MindsDB Cloud email address\n    - Password is your MindsDB Cloud password\n    - Host is `cloud.mindsdb.com`\n    - Port is `3306`\n    - Database name is `mindsdb`\n\n    Create a database connection and execute the code above. On success,\n    only the last command which lists the tables will output.\n    The expected output is:\n\n    ```bash\n    *  mysql+pymysql://mindsdb:***@127.0.0.1:47335/mindsdb\n    2 rows affected.\n    Tables_in_mindsdb\n    models\n    models_versions\n    ```\n\n    </Tab>\n    <Tab title=\"Jupysql Connection to Local MindsDB\">\n    You can use the Python code below to connect your Jupyter notebook (or lab) to Local MindsDB database (via Jupysql).\n    Load the extension:\n\n    ```python\n    %load_ext sql\n    ```\n\n    Connect to your DB:\n\n    ```python\n    %sql mysql+pymysql://mindsdb:@127.0.0.1:47335/mindsdb\n    ```\n\n    Testing connection by listing the existing tables (pure SQL):\n\n    ```python\n    %sql show tables\n    ```\n\n    Please note that we use the following connection details:\n\n    - Username is `mindsdb`\n    - Password is left empty\n    - Host is `127.0.0.1`\n    - Port is `47335`\n    - Database name is `mindsdb`\n\n    *Docker* - connecting to docker might have a different port.\n\n    Create a database connection and execute the code above. On success,\n    only the last command which lists the tables will output.\n    The expected output is:\n\n    ```bash\n    *  mysql+pymysql://mindsdb:***@127.0.0.1:47335/mindsdb\n    2 rows affected.\n    Tables_in_mindsdb\n    models\n    models_versions\n    ```\n    </Tab>\n```\n\n\n\n\nWhat's Next?\nNow that you are all set, we recommend you check out our Tutorials and\nCommunity Tutorials sections, where you'll find various examples of\nregression, classification, and time series predictions with MindsDB.\nTo learn more about MindsDB itself, follow the guide on\nMindsDB database structure. Also, don't miss out on the\nremaining pages from the SQL API section, as they explain a common SQL\nsyntax with examples.",
    "tag": "mindsdb"
  },
  {
    "title": "Setting up Kafka in Docker",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/connect/kafka.mdx",
    "content": "\ntitle: MindsDB and Kafka\nsidebarTitle: Using MindsDB with Kafka\n\nMindsDB provides the Kafka connector plugin to connect to the Kafka cluster.\nPlease visit the\nKafka Connect MindsDB page\non the official Confluent site. It contains all the instructions on how to\ninstall the connector from the Confluent hub.\nSetting up Kafka in Docker\nLet's review the instructions here as well.\nYou may use the official connector docker image:\n`bash\ndocker pull mindsdb/mindsdb-kafka-connector`\nThe source code of the Kafka connector is in the\nkafka_connector GitHub repository.\nPlease read the\ninstructions\nbefore building the connector from scratch.\nIt is possible to integrate and use the Kafka connector as part of your own\nKafka cluster, or you may use our test docker environment.\nTo try out our\ntest docker environment,\nfollow the steps below.\nBefore you bring the docker container up, please note that there are two types\nof connector configuration:\n\nFor\n  MindsDB Cloud\nFor a separate\n  MindsDB installation\n\nNo matter which option you choose, these files require real values to be set in\nplace of a username, password, Kafka connection details, etc. The SASL mechanism\ndetails are optional, as local Kafka installation may not have this mechanism\nconfigured - or you can use\nthis data\nfor the SASL username and password.\nNow that your config files store real data, you can execute the command below\nfrom the root of the\nkafka_connector GitHub repository\nto build the connector and launch it in the test environment locally.\n`bash\ndocker-compose up -d`\nLet's go over some examples.\nExamples\nPrerequisites\n\nLaunch MindsDB instance where HTTP API interface runs on\n  `docker network interface inet ip`. Usually, the IP address is `172.17.0.1`,\n  and the port is `47334`.\n\nYou can modify the HTTP API interface details in the\n  MindsDB config file.\nNow, to launch your MindsDB, run the following command:\n`bash\n  python -m mindsdb --config=/path-to-the-config-file/your-config-file.json`\nPlease note that the config file must be in JSON format. It must include\n  this part\n  of the config file from the MindsDB repository in a proper JSON format.\n\n\nTrain a new model. You may use this tutorial as an\n  example.\n\n\nRun the test environment as instructed in the\n  Setting up Kafka in Docker section above.\n\n\nCreate the Connector Instance\nTo create a connector, you need to send a POST request to a specific\n`CONNECTORS_URL` with connector configuration in JSON format, as below.\n```python\nimport requests\nMINDSDB_URL = \"http://172.17.0.1:47334\"\nCONNECTOR_NAME = \"MindsDBConnector\"\nINTEGRATION_NAME = 'test_kafka'\nKAFKA_PORT = 9092\nKAFKA_HOST = \"127.0.0.1\"\nCONNECTOR_NAME = \"MindsDBConnector\"\nCONNECTORS_URL = \"http://127.0.0.1:9021/api/connect/connect-default/connectors\"\nSTREAM_IN = \"topic_in\"\nSTREAM_OUT = \"topic_out\"\nPREDICTOR_NAME = \"YOUR_MODEL_NAME\" # set actual model name here\nparams = {\"name\": CONNECTOR_NAME,\n          \"config\": {\"connector.class\": \"com.mindsdb.kafka.connect.MindsDBConnector\",\n                     \"topics\": STREAM_IN,\n                     \"mindsdb.url\": MINDSDB_URL,\n                     \"kafka.api.host\": KAFKA_HOST,\n                     \"kafka.api.port\": KAFKA_PORT,\n                     \"kafka.api.name\": INTEGRATION_NAME,\n                     \"predictor.name\": PREDICTOR_NAME,\n                     \"output.forecast.topic\": STREAM_OUT,\n                     \"security.protocol\": \"SASL_PLAINTEXT\",\n                     \"sasl.mechanism\": \"PLAIN\",\n                     \"sasl.plain.username\": \"admin\",\n                     \"sasl.plain.password\": \"admin-secret\",\n                     }\n          }\nheaders = {\"Content-Type\": \"application/json\"}\nres = requests.post(CONNECTORS_URL, json=params, headers=headers)\n```\nThis code creates a MindsDB Kafka connector that uses the `PREDICTOR_NAME`\nmodel, receives source data from the `STREAM_IN` Kafka topic, and sends\nprediction results to the `STREAM_OUT` Kafka topic.\nSend Source Data and Receive Prediction Results\nThere are many\nKafka client implementations -\nchoose the most suitable one depending on your goals.\nThe code below generates and sends the source records to `topic_in` by default.\nYou can use any other Kafka topic by providing its name as a CMD parameter.\n`python\nimport sys\nimport json\nimport kafka\nconnection_info = {\"bootstrap_servers\": \"127.0.0.1:9092\",\n                   \"security_protocol\": \"SASL_PLAINTEXT\",\n                   \"sasl_mechanism\": \"PLAIN\",\n                   \"sasl_plain_username\": \"admin\",\n                   \"sasl_plain_password\": \"admin-secret\"}\nproducer = kafka.KafkaProducer(**connection_info)\nif __name__ == '__main__':\n    print(json.dumps(connection_info))\n    if len(sys.argv) == 1:\n        topic = \"topic_in\"\n    else:\n        topic = sys.argv[1]\n    for x in range(1, 4):\n        data = {\"Age\": x+20, \"Weight\": x * x * 0.8 + 200, \"Height\": x * x * 0.5 + 65}\n        to_send = json.dumps(data)\n        producer.send(topic, to_send.encode('utf-8'))\n    producer.close()`\nAnd the following code shows how to read prediction results from `topic_out` by\ndefault. Again, you can use any other Kafka topic by providing its name as a CMD\nparameter.\n```python\nimport sys\nimport json\nimport kafka\nconnection_info = {\"bootstrap_servers\": \"127.0.0.1:9092\",\n                   \"security_protocol\": \"SASL_PLAINTEXT\",\n                   \"sasl_mechanism\": \"PLAIN\",\n                   \"sasl_plain_username\": \"admin\",\n                   \"sasl_plain_password\": \"admin-secret\",\n                   \"auto_offset_reset\": 'earliest',\n                   \"consumer_timeout_ms\": 1000}\nconsumer = kafka.KafkaConsumer(**connection_info)\nif name == 'main':\n    print(json.dumps(connection_info))\n    if len(sys.argv) == 1:\n        topic = \"topic_out\"\n    else:\n        topic = sys.argv[1]\n    consumer.subscribe(topics=[topic])\n    for msg in consumer:\n        print(msg.value)\n        print(\"-\" * 100)\n```\nWhat's Next?\nNow that you are all set, we recommend you check out our Tutorials and\nCommunity Tutorials sections, where you'll find various examples of\nregression, classification, and time series predictions with MindsDB.\nTo learn more about MindsDB itself, follow the guide on\nMindsDB database structure. Also, don't miss out on the\nremaining pages from the SQL API section, as they explain a common SQL\nsyntax with examples.",
    "tag": "mindsdb"
  },
  {
    "title": "1. Select your service for MindsDB",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/connect/connect-mariadb-skysql.mdx",
    "content": "\ntitle: MariaDB SkySQL Setup Guide with MindsDB\nsidebarTitle: MariaDB SkySQL\n\nFind more information on MariaDB Sky SQL here\n1. Select your service for MindsDB\nIf you haven't already, identify the service to be enabled with MindsDB and make\nsure it is running. Otherwise skip to step 2.\n\n\n2. Add MindsDB to your service Allowlist\nAccess to MariaDB SkySQL services is\nrestricted on a per-service basis.\nAdd the following IP addresses to allow MindsDB to connect to your MariaDB\nservice, do this by clicking on the cog icon and navigating to Security Access.\nIn the dialog, input as prompted \u2013 one by one \u2013 the following IPs:\n`18.220.205.95\n3.19.152.46\n52.14.91.162`\n\n\n3. Download your service .pem file\nA\ncertificate authority chain\n(.pem file) must be provided for proper TLS certificate validation.\nFrom your selected service, click on the world globe icon (Connect to service).\nIn the Login Credentials section, click Download. The `aws_skysql_chain.pem`\nfile will download onto your machine.\n\n\n4. Publically Expose your service .pem File\nSelect secure storage for the `aws_skysql_chain.pem` file that allows a working\npublic URL or localpath.\n\n\n5. Link MindsDB to your MariaDB SkySQL Service\nTo print the query template, select Add Data in either the top or side\nnavigation and choose MariaDB SkySQL from the list. Fill in the values and run\nquery to complete the setup.\n\n\nHere are the codes:\n\n\n\n``````sql Template\nCREATE DATABASE maria_datasource            --- display name for the database\nWITH ENGINE = 'MariaDB',                      --- name of the MindsDB handler\nPARAMETERS = {\n  \"host\": \" \",                              --- host IP address or URL\n  \"port\": ,                                 --- port used to make TCP/IP connection\n  \"database\": \" \",                          --- database name\n  \"user\": \" \",                              --- database user\n  \"password\": \" \",                          --- database password\n  \"ssl\": True/False,                        --- optional, the `ssl` parameter value indicates whether SSL is enabled (`True`) or disabled (`False`)\n  \"ssl_ca\": {                               --- optional, SSL Certificate Authority\n    \"path\": \" \"                                 --- either \"path\" or \"url\"\n  },\n  \"ssl_cert\": {                             --- optional, SSL certificates\n    \"url\": \" \"                                  --- either \"path\" or \"url\"\n  },\n  \"ssl_key\": {                              --- optional, SSL keys\n    \"path\": \" \"                                 --- either \"path\" or \"url\"\n  }\n};\n```\n\n```sql Example for MariaDB SkySQL Service\nCREATE DATABASE skysql_datasource\nWITH ENGINE = 'MariaDB',\nPARAMETERS = {\n  \"host\": \"mindsdbtest.mdb0002956.db1.skysql.net\",\n  \"port\": 5001,\n  \"database\": \"mindsdb_data\",\n  \"user\": \"DB00007539\",\n  \"password\": \"password\",\n  --- here, the SSL certificate is required\n  \"ssl-ca\": {\n    \"url\": \"https://mindsdb-web-builds.s3.amazonaws.com/aws_skysql_chain.pem\"\n  }\n};\n```\n```\n\n\n\nWhat's Next?\nNow that you are all set, we recommend you check out our Tutorials and\nCommunity Tutorials sections, where you'll find various examples of\nregression, classification, and time series predictions with MindsDB.\nTo learn more about MindsDB itself, follow the guide on\nMindsDB database structure. Also, don't miss out on the\nremaining pages from the SQL API section, as they explain a common SQL\nsyntax with examples.\nHave fun!\n\nFrom Our Community\nCheck out the tutorial created by our community:\n\nLet's connect Mindsdb with Mariadb shell Locally and Predict the Mobile Price\n  by Atharva Shirdhankar\n",
    "tag": "mindsdb"
  },
  {
    "title": "How to Use the MindsDB SQL Editor",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/connect/mindsdb_editor.mdx",
    "content": "\ntitle: MindsDB SQL Editor\nsidebarTitle: MindsDB SQL Editor\n\nMindsDB provides a SQL Editor, so you don't need to download additional SQL clients to connect to MindsDB.\nHow to Use the MindsDB SQL Editor\nThere are two ways you can use the Editor, as below.\n\n\n\n\n```After setting up the MindsDB using [Docker](/setup/self-hosted/docker), or pip\non\n[Linux](/setup/self-hosted/pip/linux)/[Windows](/setup/self-hosted/pip/windows)/[MacOS](/setup/self-hosted/pip/macos),\nor pip via [source code](/setup/self-hosted/pip/source), go to your terminal and\nexecute the following:\n\n```bash\npython -m mindsdb\n```\n\nOn execution, we get:\n\n```bash\n...\n2022-05-06 14:07:04,599 - INFO -  - GUI available at http://127.0.0.1:47334/\n...\n```\n\nImmediately after, your browser automatically opens the MindsDB SQL Editor. In\ncase if it doesn't, visit the URL\n[`http://127.0.0.1:47334/`](http://127.0.0.1:47334/) in your browser of\npreference.\n```\n\n\n\n\n    Go to the MindsDB Cloud and log in to your account. The Editor is the first page you see after logging in.\n  \n\nHere is a sneak peek of the MindsDB SQL Editor:\n\nWhat's Next?\nNow that you are all set, we recommend you check out our Tutorials and\nCommunity Tutorials sections, where you'll find various examples of\nregression, classification, and time series predictions with MindsDB.\nTo learn more about MindsDB itself, follow the guide on\nMindsDB database structure. Also, don't miss out on the\nremaining pages from the SQL API section, as they explain a common SQL\nsyntax with examples.",
    "tag": "mindsdb"
  },
  {
    "title": "Description",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/rest/sql.mdx",
    "content": "\ntitle: SQL Queries\nopenapi: POST /api/sql/query\nsidebarTitle: SQL Queries\n\nDescription\nThis API provides a REST endpoint for executing the SQL queries. Note:\n\nThis endpoint is a HTTP POST method.\nThis endpoint accept data via `application/json` request body. \nThe only required key is the `query` which has the SQL statement value.\n\nBy default, the open source version doesn't provide Authentication methods as MindsDB Cloud. \nOn MindsDB cloud you need to set the `cookie` field in the header of request e.g  `{session=273trgsehgrui3i2riurwehe}`. \n\n\nBody\n\nString that contains the SQL query that needs to be executed. \n\nResponse\n \nA list with the column names returned\n\n \nThe database where the query is executed\n\n \nThe actual data returned by the query in case of the table response type\n\n \nThe type of the response table | error | ok\n\n\n```shell Shell\ncurl --request POST \\\n     --url https://cloud.mindsdb.com/api/sql/query \\\n     --header 'Content-Type: application/json' \\\n     --cookie '{session=273trgsehgrui3i2riurwehe}'\\\n     --data '\n{\n     \"query\": \"SELECT * FROM example_db.demo_data.home_rentals LIMIT 10;\"\n}\n```\n`python Python\nimport requests\nurl = 'https://cloud.mindsdb.com/api/sql/query'\ncookies = '{session=273trgsehgrui3i2riurwehe}'\nresp = requests.post(url, json={'query': \n                    'SELECT * FROM example_db.demo_data.home_rentals LIMIT 10;', cookies=cookies})`\n\n\n`json Response\n {\n    \"column_names\": [\n        \"sqft\",\n        \"rental_price\"\n    ],\n    \"context\": {\n        \"db\": \"mindsdb\"\n    },\n    \"data\": [\n        [\n            917,\n            3901\n        ],\n        [\n            194,\n            2042\n        ]\n    ],\n    \"type\": \"table\"\n    }`",
    "tag": "mindsdb"
  },
  {
    "title": "Description",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/rest/projects/get-projects.mdx",
    "content": "\ntitle: Get All Projects\nopenapi: \"GET /projects\"\nsidebarTitle: Get All Projects\n\nDescription\nThis API provides a REST endpoint for listing projects. \n\nBy default, the open source version doesn't provide Authentication methods as MindsDB Cloud. \nOn MindsDB cloud you need to set the `cookie` field in the header of request e.g  `{session=273trgsehgrui3i2riurwehe}`. \n\nResponse\n\n\n\n`shell Shell\ncurl --request GET \\\n     --url https://cloud.mindsdb.com/api/projects/ \\\n     --cookie '{session=273trgsehgrui3i2riurwehe}'`\n`python Python\nimport requests\nurl = 'https://cloud.mindsdb.com/api/projects/'\ncookies = '{session=273trgsehgrui3i2riurwehe}'\nresp = requests.get(url, cookies=cookies})`\n\n\n`json Response\n[\n    {\n        \"name\": \"mindsdb\"\n    }\n]`",
    "tag": "mindsdb"
  },
  {
    "title": "Description",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/rest/models/list-models.mdx",
    "content": "\ntitle: Get All Models\nopenapi: \"GET /projects/{projectName}/models\"\nsidebarTitle: Get All Models\n\nDescription\nThis API provides a REST endpoint for getting a list of models.\n\nBy default, the open source version doesn't provide Authentication methods as MindsDB Cloud. \nOn MindsDB cloud you need to set the `cookie` field in the header of request e.g  `{session=273trgsehgrui3i2riurwehe}`. \n\n\n`shell Shell\ncurl --request GET \\\n     --url https://cloud.mindsdb.com/api/projects/{projectName}/models \\\n     --cookie '{session=273trgsehgrui3i2riurwehe}'\\`\n`python Python\nimport requests\nurl = 'https://cloud.mindsdb.com/api/projects/{projectName}/models'\ncookies = '{session=273trgsehgrui3i2riurwehe}'\nresp = requests.get(url, cookies=cookies})`\n\n\n`json Response\n[\n    {\n        \"accuracy\": 0.999,\n        \"active\": true,\n        \"created_at\": \"2023-01-19 17:02:58\",\n        \"current_phase\": null,\n        \"data_source\": null,\n        \"error\": null,\n        \"fetch_data_query\": \"SELECT * FROM demo_data.home_rentals\",\n        \"id\": 1,\n        \"is_active\": null,\n        \"mindsdb_version\": \"23.1.3.1\",\n        \"name\": \"home_rentals_model\",\n        \"predict\": \"rental_price\",\n        \"status\": \"complete\",\n        \"training_time\": \"0:00:54\",\n        \"update\": \"up_to_date\",\n        \"version\": 1\n    }\n]`",
    "tag": "mindsdb"
  },
  {
    "title": "Description",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/rest/models/query-model.mdx",
    "content": "\ntitle: Query a Model\nopenapi: \"POST /projects/{projectName}/models/{modelName}/predict\"\nsidebarTitle: Query a Model\n\nDescription\nThis API provides a REST endpoint for making predictions. \n\nBy default, the open source version doesn't provide Authentication methods as MindsDB Cloud. \nOn MindsDB cloud you need to set the `cookie` field in the header of request e.g  `{session=273trgsehgrui3i2riurwehe}`. \n\n\n```shell Shell\ncurl --request POST \\\n     --url https://cloud.mindsdb.com/api/projects/{projectName}/models/{modelName}/predict \\\n     --header 'Content-Type: application/json' \\\n     --cookie '{session=273trgsehgrui3i2riurwehe}'\\\n     --data '\n{\n   'data': [\n       {'sqft': '1000'},\n       {'sqft': '500'},\n    ],\n   'params': {\n}\n}\n```\n`python Python\nimport requests\nurl = 'https://cloud.mindsdb.com/projects/{projectName}/models/{modelName}/predict'\ncookies = '{session=273trgsehgrui3i2riurwehe}'\ndata = { 'data': [{'sqft': '1000'}, {'sqft': '500'}],'params': {}}\nresp = requests.post(url, data=data, cookies=cookies})`\n\n\n`json Response\n[\n    {\n        \"__mindsdb_row_id\": null,\n        \"days_on_market\": null,\n        \"location\": null,\n        \"neighborhood\": null,\n        \"number_of_bathrooms\": null,\n        \"number_of_rooms\": null,\n        \"rental_price\": 2847,\n        \"rental_price_anomaly\": null,\n        \"rental_price_confidence\": 0.99,\n        \"rental_price_explain\": \"{\\\"predicted_value\\\": 2847, \\\"confidence\\\": 0.99, \\\"anomaly\\\": null, \\\"truth\\\": null, \\\"confidence_lower_bound\\\": 2730, \\\"confidence_upper_bound\\\": 2964}\",\n        \"rental_price_max\": 2964,\n        \"rental_price_min\": 2730,\n        \"rental_price_original\": null,\n        \"select_data_query\": null,\n        \"sqft\": 823,\n        \"when_data\": null\n    }\n]`",
    "tag": "mindsdb"
  },
  {
    "title": "Prerequisites",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/feature-eng.mdx",
    "content": "\ntitle: Feature Engineering in MindsDB\nsidebarTitle: Feature Engineering\n\nThe more data you have, the more accurate predictions you get.\nWe recommend you provide the predictor with as many historical data rows and data columns as possible to make your predictions even more accurate. The examples presented here prove this hypothesis.\nIf you want to follow the examples, please sign up for the MindsDB Cloud account here.\nPrerequisites\nThe base table is available in the `example_db` integration in the MindsDB Cloud Editor. In order to be able to use it, you must first create a database like this:\n`sql\nCREATE DATABASE example_db\nWITH ENGINE = \"postgres\",\nPARAMETERS = {\n    \"user\": \"demo_user\",\n    \"password\": \"demo_password\",\n    \"host\": \"3.220.66.106\",\n    \"port\": \"5432\",\n    \"database\": \"demo\"\n};`\nOn execution, we get:\n`sql\nQuery OK, 0 rows affected (x.xxx sec)`\nOnce that's done, you can run the following commands with us.\nExample: Adding More Data Columns\nIntroduction\nHere, we'll create several predictors using the same table, increasing the number of data columns with each step.\nWe start with the base table and create a predictor based on it. Then we add two columns to our base table and again create a predictor based on the enhanced table. At last, we add another two columns and create a predictor.\nBy comparing the accuracies of the predictors, we'll find that more data results in more accurate predictions.\nLet's get started.\nLet's Run the Codes\nHere, we go through the codes for the base table and enhanced base tables simultaneously.\nData Setup\nLet's prepare and verify the data. Here, we create the views and query them to ensure the input for the predictors is in order.\n\n\n\n\n```Let's start by querying the data from the `example_db.demo_data.used_car_price` table, which is our base table.\n\n```sql\nSELECT *\nFROM example_db.demo_data.used_car_price\nLIMIT 5;\n```\n\nOn execution, we get:\n\n```sql\n+-----+----+-----+------------+-------+--------+---+----+----------+\n|model|year|price|transmission|mileage|fueltype|tax|mpg |enginesize|\n+-----+----+-----+------------+-------+--------+---+----+----------+\n| A1  |2017|12500|Manual      |15735  |Petrol  |150|55.4|1.4       |\n| A6  |2016|16500|Automatic   |36203  |Diesel  |20 |64.2|2         |\n| A1  |2016|11000|Manual      |29946  |Petrol  |30 |55.4|1.4       |\n| A4  |2017|16800|Automatic   |25952  |Diesel  |145|67.3|2         |\n| A3  |2019|17300|Manual      |1998   |Petrol  |145|49.6|1         |\n+-----+----+-----+------------+-------+--------+---+----+----------+\n```\n\nWhere:\n\n| Name           | Description                                                 |\n|----------------|-------------------------------------------------------------|\n| `model`        | Model of the car.                                           |\n| `year`         | Year of production.                                         |\n| `price`        | Price of the car.                                           |\n| `transmission` | Transmission (`Manual`, or `Automatic`, or `Semi-Auto`).    |\n| `mileage`      | Mileage of the car.                                         |\n| `fueltype`     | Fuel type of the car.                                       |\n| `tax`          | Tax.                                                        |\n| `mpg`          | Miles per gallon.                                           |\n| `enginesize`   | Engine size of the car.                                     |\n```\n\n\n\n\n\n\n```Let's create a view based on the `example_db.demo_data.used_car_price` table, and add two more columns. Please note that we replace the `mpg` column with the `kml` column.\n\nThe added columns are:<br></br>\n- the `kml` column, calculated from the `mpg` column using the formula like in the query below, stands for `kilometers per liter`,<br></br>\n- the `years_old` column, calculated by subtracting car's year from the current date, stands for car's age.<br></br>\n\n```sql\nCREATE VIEW used_car_price_plus_2_columns (\nSELECT * FROM example_db (\n    SELECT \n    model, \n    year, \n    price, \n    transmission, \n    mileage, \n    fueltype, \n    tax,\n    enginesize, \n    ROUND(CAST((mpg / 2.3521458) AS numeric), 1) AS kml, -- mpg (miles per galon) is replaced with kml (kilometers per liter)\n    (date_part('year', CURRENT_DATE)-year) AS years_old -- age of a car\n    FROM demo_data.used_car_price\n)\n);\n```\n\nOn execution, we get:\n\n```sql\nQuery OK, 0 rows affected (x.xxx sec)\n```\n\nLet's query the newly created view.\n\n```sql\nSELECT *\nFROM mindsdb.used_car_price_plus_2_columns\nLIMIT 5;\n```\n\nOn execution, we get:\n\n```sql\n+-----+----+-----+------------+-------+--------+---+----+----------+----+---------+\n|model|year|price|transmission|mileage|fueltype|tax|mpg |enginesize|kml |years_old|\n+-----+----+-----+------------+-------+--------+---+----+----------+----+---------+\n| A1  |2017|12500|Manual      |15735  |Petrol  |150|55.4|1.4       |23.6|5        |\n| A6  |2016|16500|Automatic   |36203  |Diesel  |20 |64.2|2         |27.3|6        |\n| A1  |2016|11000|Manual      |29946  |Petrol  |30 |55.4|1.4       |23.6|6        |\n| A4  |2017|16800|Automatic   |25952  |Diesel  |145|67.3|2         |28.6|5        |\n| A3  |2019|17300|Manual      |1998   |Petrol  |145|49.6|1         |21.1|3        |\n+-----+----+-----+------------+-------+--------+---+----+----------+----+---------+\n```\n```\n\n\n\n\n\n\n```Let's create a view based on the `example_db.demo_data.used_car_price` table, and add four more columns. Please note that we replace the `mpg` column with the `kml` column.\n\nThe added columns are:<br></br>\n- the `kml` column, calculated from the `mpg` column using the formula like in the query below, stands for `kilometers per liter`,<br></br>\n- the `years_old` column, calculated by subtracting car's year from the current date, stands for car's age,<br></br>\n- the `units_to_sell` column, calculated using the `OVER` and `PARTITION BY` clauses, indicates how many units of a certain car model are sold in a year,<br></br>\n- the `tax_div_price` column, calculated by dividing the `tax` column by the `price` column.<br></br>\n\n```sql\nCREATE VIEW used_car_price_plus_another_2_columns (\nSELECT * FROM example_db (\n    SELECT \n    model, \n    year, \n    price, \n    transmission, \n    mileage, \n    fueltype, \n    tax,\n    enginesize, \n    ROUND(CAST((mpg / 2.3521458) AS numeric), 1) AS kml, -- mpg (miles per galon) is replaced with kml (kilometers per liter)\n    (date_part('year', CURRENT_DATE)-year) AS years_old, -- age of a car\n    COUNT(*) OVER (PARTITION BY model, year) AS units_to_sell, -- how many units of a certain model are sold in a year\n    ROUND((CAST(tax AS decimal) / price), 3) AS tax_div_price -- value of tax divided by price of a car\n    FROM demo_data.used_car_price\n)\n);\n```\n\nOn execution, we get:\n\n```sql\nQuery OK, 0 rows affected (x.xxx sec)\n```\n\nLet's query the newly created view.\n\n```sql\nSELECT *\nFROM mindsdb.used_car_price_plus_another_2_columns\nLIMIT 5;\n```\n\nOn execution, we get:\n\n```sql\n+-----+----+-----+------------+-------+--------+---+----+----------+----+---------+-------------+-------------+\n|model|year|price|transmission|mileage|fueltype|tax|mpg |enginesize|kml |years_old|units_to_sell|tax_div_price|\n+-----+----+-----+------------+-------+--------+---+----+----------+----+---------+-------------+-------------+\n| A1  |2010|9990 |Automatic   |38000  |Petrol  |125|53.3|1.4       |22.7|12       |1            |0.013        |\n| A1  |2011|6995 |Manual      |65000  |Petrol  |125|53.3|1.4       |22.7|11       |5            |0.018        |\n| A1  |2011|6295 |Manual      |107000 |Petrol  |125|53.3|1.4       |22.7|11       |5            |0.020        |\n| A1  |2011|4250 |Manual      |116000 |Diesel  |20 |70.6|1.6       |30.0|11       |5            |0.005        |\n| A1  |2011|6475 |Manual      |45000  |Diesel  |0  |70.6|1.6       |30.0|11       |5            |0.000        |\n+-----+----+-----+------------+-------+--------+---+----+----------+----+---------+-------------+-------------+\n```\n```\n\n\n\n\n\nDropping a View If you want to drop a view, run the command `DROP VIEW view_name;`.\n\nCreating Predictors\nNow, we create predictors based on the `example_db.demo_data.used_car_price` table and its extensions.\n\n\n\n\n``````sql\nCREATE MODEL mindsdb.price_predictor\nFROM example_db\n(SELECT * FROM demo_data.used_car_price)\nPREDICT price;\n```\n\nOn execution, we get:\n\n```sql\nQuery OK, 0 rows affected (x.xxx sec)\n```\n```\n\n\n\n\n\n\n``````sql\n\nCREATE MODEL mindsdb.price_predictor_plus_2_columns\nFROM mindsdb\n    (SELECT * FROM used_car_price_plus_2_columns)\nPREDICT price;\n```\n\nOn execution, we get:\n\n```sql\nQuery OK, 0 rows affected (x.xxx sec)\n```\n```\n\n\n\n\n\n\n``````sql\nCREATE MODEL mindsdb.price_predictor_plus_another_2_columns\nFROM mindsdb\n    (SELECT * FROM used_car_price_plus_another_2_columns)\nPREDICT price;\n```\n\nOn execution, we get:\n\n```sql\nQuery OK, 0 rows affected (x.xxx sec)\n```\n```\n\n\n\n\n\nDropping a Predictor If you want to drop a predictor, run the command `DROP MODEL predictor_name;`.\n\nPredictor Status\nFinally, let's check the predictor status whose value is `generating` at first, then `training`, and at last, `complete`.\n\n\n\n\n``````sql\nSELECT *\nFROM mindsdb.models\nWHERE name='price_predictor';\n```\n\nOn execution, we get:\n\n```sql\n+---------------+--------+--------+---------+-------------+---------------+------+--------------------------------------+----------------+\n|name           |status  |accuracy|predict  |update_status|mindsdb_version|error |select_data_query                     |training_options|\n+---------------+--------+--------+---------+-------------+---------------+------+--------------------------------------+----------------+\n|price_predictor|complete|0.963   |price    |up_to_date   |22.10.2.1      |[NULL]|SELECT * FROM demo_data.used_car_price|                |\n+---------------+--------+--------+---------+-------------+---------------+------+--------------------------------------+----------------+\n```\n```\n\n\n\n\n\n\n``````sql\nSELECT *\nFROM mindsdb.models\nWHERE name='price_predictor_plus_2_columns';\n```\n\nOn execution, we get:\n\n```sql\n+------------------------------+--------+--------+---------+-------------+---------------+------+-------------------------------------------+----------------+\n|name                          |status  |accuracy|predict  |update_status|mindsdb_version|error |select_data_query                          |training_options|\n+------------------------------+--------+--------+---------+-------------+---------------+------+-------------------------------------------+----------------+\n|price_predictor_plus_2_columns|complete|0.965   |price    |up_to_date   |22.10.2.1      |[NULL]|SELECT * FROM used_car_price_plus_2_columns|                |\n+------------------------------+--------+--------+---------+-------------+---------------+------+-------------------------------------------+----------------+\n```\n```\n\n\n\n\n\n\n``````sql\nSELECT *\nFROM mindsdb.models\nWHERE name='price_predictor_plus_another_2_columns';\n```\n\nOn execution, we get:\n\n```sql\n+--------------------------------------+--------+--------+---------+-------------+---------------+------+---------------------------------------------------+----------------+\n|name                                  |status  |accuracy|predict  |update_status|mindsdb_version|error |select_data_query                                  |training_options|\n+--------------------------------------+--------+--------+---------+-------------+---------------+------+---------------------------------------------------+----------------+\n|price_predictor_plus_another_2_columns|complete|0.982   |price    |up_to_date   |22.10.2.1      |[NULL]|SELECT * FROM used_car_price_plus_another_2_columns|                |\n+--------------------------------------+--------+--------+---------+-------------+---------------+------+---------------------------------------------------+----------------+\n```\n```\n\n\n\n\nAccuracy Comparison\nOnce the training process of all three predictors completes, we see the accuracy values.\n\nFor the base table, we get an accuracy value of `0.963`.\nFor the base table with two more data columns, we get an accuracy value of `0.965`. The accuracy value increased, as expected.\nFor the base table with four more data columns, we get an accuracy value of `0.982`. The accuracy value increased again, as expected.\n\nTrue vs Predicted Price Comparison\nLet's compare how close the predicted price values are to the true price.\n`sql\n+-------+-------+---------------+-----------+-----------+--------------+----------------+----------------+---------------+\n| model | year  | transmission  | fueltype  | mileage   | true_price   | pred_price_1   | pred_price_2   | pred_price_3  |\n+-------+-------+---------------+-----------+-----------+--------------+----------------+----------------+---------------+\n| A1    | 2017  | Manual        | Petrol    | 7620      | 14440        | 17268          | 17020          | 14278         |\n| A6    | 2016  | Automatic     | Diesel    | 20335     | 18982        | 17226          | 17935          | 19016         |\n| A3    | 2018  | Semi-Auto     | Diesel    | 9058      | 19900        | 25641          | 23008          | 21286         |\n+-------+-------+---------------+-----------+-----------+--------------+----------------+----------------+---------------+`\nThe prices predicted by the third predictor, having the highest accuracy value, are the closest to the true price, as expected.\nExample: Joining Data Tables\nIntroduction\nWe start by creating a predictor from the `car_sales` table. Then, we add more data by joining the `car_sales` and `car_info` tables. We create a predictor based on the `car_sales_info` view.\nLet's get started.\nLet's Run the Codes\nHere, we go through the codes using partial tables and the full table after joining the data.\nData Setup\nHere is the `car_sales` table:\n`sql\nSELECT *\nFROM example_db.demo_data.car_sales\nLIMIT 5;`\nOn execution, we get:\n`sql\n+-----+----+-----+------------+-------+--------+---+\n|model|year|price|transmission|mileage|fueltype|tax|\n+-----+----+-----+------------+-------+--------+---+\n| A1  |2017|12500|Manual      |15735  |Petrol  |150|\n| A6  |2016|16500|Automatic   |36203  |Diesel  |20 |\n| A1  |2016|11000|Manual      |29946  |Petrol  |30 |\n| A4  |2017|16800|Automatic   |25952  |Diesel  |145|\n| A3  |2019|17300|Manual      |1998   |Petrol  |145|\n+-----+----+-----+------------+-------+--------+---+`\nWhere:\n| Name           | Description                                                 |\n|----------------|-------------------------------------------------------------|\n| `model`        | Model of the car.                                           |\n| `year`         | Year of production.                                         |\n| `price`        | Price of the car.                                           |\n| `transmission` | Transmission (`Manual`, or `Automatic`, or `Semi-Auto`).    |\n| `mileage`      | Mileage of the car.                                         |\n| `fueltype`     | Fuel type of the car.                                       |\n| `tax`          | Tax.                                                        |\nAnd here is the `car_info` table:\n`sql\nSELECT *\nFROM example_db.demo_data.car_info\nLIMIT 5;`\nOn execution, we get:\n`sql\n+-----+----+------------+---------+-----+----------+\n|model|year|transmission|fueltype |mpg  |enginesize|\n+-----+----+------------+---------+-----+----------+\n| A1  |2010|Automatic   |Petrol   |53.3 |1.4       |\n| A1  |2011|Manual      |Diesel   |70.6 |1.6       |\n| A1  |2011|Manual      |Petrol   |53.3 |1.4       |\n| A1  |2012|Automatic   |Petrol   |50.6 |1.4       |\n| A1  |2012|Manual      |Diesel   |72.95|1.7       |\n+-----+----+------------+---------+-----+----------+`\nWhere:\n| Name           | Description                                                 |\n|----------------|-------------------------------------------------------------|\n| `model`        | Model of the car.                                           |\n| `year`         | Year of production.                                         |\n| `transmission` | Transmission (`Manual`, or `Automatic`, or `Semi-Auto`).    |\n| `fueltype`     | Fuel type of the car.                                       |\n| `mpg`          | Miles per gallon.                                           |\n| `enginesize`   | Engine size of the car.                                     |\nLet's join the `car_sales` and `car_info` tables on the `model`, `year`, `transmission`, and `fueltype` columns.\n`sql\nSELECT * FROM example_db\n(\n  SELECT s.*, i.mpg, i.enginesize\n  FROM demo_data.car_sales s\n  JOIN demo_data.car_info i\n  ON s.model=i.model\n  AND s.year=i.year\n  AND s.transmission=i.transmission\n  AND s.fueltype=i.fueltype\n)\nLIMIT 5;`\n\nNested `SELECT` Statements Please note that we use the nested `SELECT` statement in order to trigger native query at the MindsDB Cloud Editor. Here, the `example_db` database is a PostgreSQL database, so we trigger PostgreSQL-native syntax.\n\nOn execution, we get:\n`sql\n+-----+----+-----+------------+-------+--------+---+----+----------+\n|model|year|price|transmission|mileage|fueltype|tax|mpg |enginesize|\n+-----+----+-----+------------+-------+--------+---+----+----------+\n| A1  |2010|9990 |Automatic   |38000  |Petrol  |125|53.3|1.4       |\n| A1  |2011|4250 |Manual      |116000 |Diesel  |20 |70.6|1.6       |\n| A1  |2011|6475 |Manual      |45000  |Diesel  |0  |70.6|1.6       |\n| A1  |2011|6295 |Manual      |107000 |Petrol  |125|53.3|1.4       |\n| A1  |2011|7495 |Manual      |60700  |Petrol  |125|53.3|1.4       |\n+-----+----+-----+------------+-------+--------+---+----+----------+`\nNow, we create a view based on the `JOIN` query:\n`sql\nCREATE VIEW car_sales_info \n(\n    SELECT * FROM example_db\n    (\n    SELECT s.*, i.mpg, i.enginesize\n    FROM demo_data.car_sales s\n    JOIN demo_data.car_info i\n    ON s.model=i.model\n    AND s.year=i.year\n    AND s.transmission=i.transmission\n    AND s.fueltype=i.fueltype\n    )\n);`\nOn execution, we get:\n`sql\nQuery OK, 0 rows affected (x.xxx sec)`\nLet's verify the view by selecting from it.\n`sql\nSELECT *\nFROM mindsdb.car_sales_info\nLIMIT 5;`\nOn execution, we get:\n`sql\n+-----+----+-----+------------+-------+--------+---+----+----------+\n|model|year|price|transmission|mileage|fueltype|tax|mpg |enginesize|\n+-----+----+-----+------------+-------+--------+---+----+----------+\n| A1  |2010|9990 |Automatic   |38000  |Petrol  |125|53.3|1.4       |\n| A1  |2011|4250 |Manual      |116000 |Diesel  |20 |70.6|1.6       |\n| A1  |2011|6475 |Manual      |45000  |Diesel  |0  |70.6|1.6       |\n| A1  |2011|6295 |Manual      |107000 |Petrol  |125|53.3|1.4       |\n| A1  |2011|7495 |Manual      |60700  |Petrol  |125|53.3|1.4       |\n+-----+----+-----+------------+-------+--------+---+----+----------+`\nCreating Predictors\nLet's create a predictor with the `car_sales` table as input data.\n`sql\nCREATE MODEL mindsdb.price_predictor_car_sales\nFROM example_db\n  (SELECT * FROM demo_data.car_sales)\nPREDICT price;`\nOn execution, we get:\n`sql\nQuery OK, 0 rows affected (x.xxx sec)`\nNow, let's create a predictor for the table that is a `JOIN` between the `car_sales` and `car_info` tables.\n`sql\nCREATE MODEL mindsdb.price_predictor_car_sales_info\nFROM mindsdb\n  (SELECT * FROM car_sales_info)\nPREDICT price;`\nOn execution, we get:\n`sql\nQuery OK, 0 rows affected (x.xxx sec)`\nPredictor Status\nNext, we check the status of both predictors.\nWe start with the predictor based on the partial table.\n`sql\nSELECT *\nFROM mindsdb.models\nWHERE name='price_predictor_car_sales';`\nOn execution, we get:\n`sql\n+-------------------------+--------+--------+---------+-------------+---------------+------+---------------------------------+----------------+\n|name                     |status  |accuracy|predict  |update_status|mindsdb_version|error |select_data_query                |training_options|\n+-------------------------+--------+--------+---------+-------------+---------------+------+---------------------------------+----------------+\n|price_predictor_car_sales|complete|0.912   |price    |up_to_date   |22.10.2.1      |[NULL]|SELECT * FROM demo_data.car_sales|                |\n+-------------------------+--------+--------+---------+-------------+---------------+------+---------------------------------+----------------+`\nAnd now, for the predictor based on the full table.\n`sql\nSELECT *\nFROM mindsdb.models\nWHERE name='price_predictor_car_sales_info';`\nOn execution, we get:\n`sql\n+------------------------------+--------+--------+---------+-------------+---------------+------+----------------------------+----------------+\n|name                          |status  |accuracy|predict  |update_status|mindsdb_version|error |select_data_query           |training_options|\n+------------------------------+--------+--------+---------+-------------+---------------+------+----------------------------+----------------+\n|price_predictor_car_sales_info|complete|0.912   |price    |up_to_date   |22.10.2.1      |[NULL]|SELECT * FROM car_sales_info|                |\n+------------------------------+--------+--------+---------+-------------+---------------+------+----------------------------+----------------+`\nAccuracy Comparison",
    "tag": "mindsdb"
  },
  {
    "title": "What is Feature Importance?",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/feature-importance.mdx",
    "content": "\ntitle: Feature Importance of MindsDB and Lightwood\nsidebarTitle: Feature Importance\n\nMindsDB together with the Lightwood ML engine provide the feature importance tool.\nWhat is Feature Importance?\nFeature importance is a useful tool for obtaining explanations about how any given machine learning model generally works. Simply put, it assigns a relative numerical score to each input feature so that a user understands what parts of the input are used to a greater or lesser degree by the model when generating predictions.\nWhile there are a few variations of this technique, Lightwood offers the permutation-based variant.\nPermutation Feature Importance\nThe procedure consists of splitting data into the training and validation sets. Once the ML model is trained with the former, the latter is used to, among other things, find out the importance scores for each feature.\nThe algorithm is rather simple. We iterate over all the input features and randomly shuffle the information within each of them one by one without shuffling the rest of the input features. Then, the model generates predictions for this altered input as it normally would for any other input.\nOnce we have predictions for all the shuffled variations of the validation dataset, we can evaluate accuracy metrics that are of interest to the user, such as mean absolute error for regression tasks, and compare them against the value obtained for the original dataset, which acts as a reference value. Based on the lost accuracy, we finally assign a numerical score that reflects this impact and report it as the importance of this column for the model.\nFor edge cases where a feature is completely irrelevant (no lost accuracy if the feature is absent) or absolutely critical (accuracy drops to the minimum possible value if the feature is absent), the importance score is 0.0 and 1.0, respectively. However, the user should be careful to note that these scores do not model intra-feature dependencies or correlations, meaning that it is not wise to interpret the scores as independent from each other, as any feature that is correlated with another highly-scored feature will present a high score, too.\nHow to Use the Permutation Feature Importance\nYou can customize the behavior of this analysis module from the MindsDB SQL editor via the USING key. For example, if you want to consider all rows in the validation dataset, rather than clip it to some default value, here is an example:\n`sql\n...\nUSING\nengine = 'lightwood',\nanalysis_blocks = [\n  {\"module\": \"PermutationFeatureImportance\", \n   \"args\": {\"row_limit\": 0}} -- all validation data is used \n];`\nOnce you train a model, use the `DESCRIBE model_name;` command to see the reported importance scores.\nExample\nLet's use the following data to create a model:\n`sql\nSELECT *\nFROM example_db.demo_data.home_rentals\nLIMIT 5;`\nOn execution, we get:\n`sql\n+---------------+-------------------+----+--------+--------------+--------------+------------+\n|number_of_rooms|number_of_bathrooms|sqft|location|days_on_market|neighborhood  |rental_price|\n+---------------+-------------------+----+--------+--------------+--------------+------------+\n|2              |1                  |917 |great   |13            |berkeley_hills|3901        |\n|0              |1                  |194 |great   |10            |berkeley_hills|2042        |\n|1              |1                  |543 |poor    |18            |westbrae      |1871        |\n|2              |1                  |503 |good    |10            |downtown      |3026        |\n|3              |2                  |1066|good    |13            |thowsand_oaks |4774        |\n+---------------+-------------------+----+--------+--------------+--------------+------------+`\nNow we create a model using the `USING` clause as shown in the previous chapter.\n`sql\nCREATE MODEL home_rentals_model\nFROM example_db\n    (SELECT * FROM demo_data.home_rentals)\nPREDICT rental_price\nUSING\n    engine = 'lightwood',\n    analysis_blocks = [\n        {\"module\": \"PermutationFeatureImportance\", \n        \"args\": {\"row_limit\": 0}}\n    ];`\nOn execution, we get:\n`sql\nQuery successfully completed`\nHere is how you can monitor the status of the model:\n`sql\nSELECT status\nFROM mindsdb.models\nWHERE name = 'home_rentals_model';`\nOnce the status is complete, we can query the model.\n`sql\nSELECT d.sqft, d.neighborhood, d.days_on_market,\n       m.rental_price AS predicted_price, m.rental_price_explain\nFROM mindsdb.home_rentals_model AS m\nJOIN example_db.demo_data.home_rentals AS d\nLIMIT 5;`\nOn execution, we get:\n`sql\n+----+--------------+--------------+---------------+---------------------------------------------------------------------------------------------------------------------------------------------+\n|sqft|neighborhood  |days_on_market|predicted_price|rental_price_explain                                                                                                                         |\n+----+--------------+--------------+---------------+---------------------------------------------------------------------------------------------------------------------------------------------+\n|917 |berkeley_hills|13            |3886           |{\"predicted_value\": 3886, \"confidence\": 0.99, \"anomaly\": null, \"truth\": null, \"confidence_lower_bound\": 3805, \"confidence_upper_bound\": 3967}|\n|194 |berkeley_hills|10            |2007           |{\"predicted_value\": 2007, \"confidence\": 0.99, \"anomaly\": null, \"truth\": null, \"confidence_lower_bound\": 1925, \"confidence_upper_bound\": 2088}|\n|543 |westbrae      |18            |1865           |{\"predicted_value\": 1865, \"confidence\": 0.99, \"anomaly\": null, \"truth\": null, \"confidence_lower_bound\": 1783, \"confidence_upper_bound\": 1946}|\n|503 |downtown      |10            |3020           |{\"predicted_value\": 3020, \"confidence\": 0.99, \"anomaly\": null, \"truth\": null, \"confidence_lower_bound\": 2938, \"confidence_upper_bound\": 3101}|\n|1066|thowsand_oaks |13            |4748           |{\"predicted_value\": 4748, \"confidence\": 0.99, \"anomaly\": null, \"truth\": null, \"confidence_lower_bound\": 4667, \"confidence_upper_bound\": 4829}|\n+----+--------------+--------------+---------------+---------------------------------------------------------------------------------------------------------------------------------------------+`\nHere is how you can check the importance scores for all columns:\n`sql\nDESCRIBE home_rentals_model;`\nOn execution, we get:\n`sql\n+------------------+----------------------------------------------------------------------------------------------------------------------+----------------+-------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------+\n|accuracies        |column_importance                                                                                                     |outputs         |inputs                                                                                     |model                                                                                                                                               |\n+------------------+----------------------------------------------------------------------------------------------------------------------+----------------+-------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------+\n|{\"r2_score\":0.999}|{\"days_on_market\":0.09,\"location\":0.042,\"neighborhood\":0,\"number_of_bathrooms\":0,\"number_of_rooms\":0.292,\"sqft\":0.999}|[\"rental_price\"]|[\"number_of_rooms\",\"number_of_bathrooms\",\"sqft\",\"location\",\"days_on_market\",\"neighborhood\"]|encoders --> dtype_dict --> dependency_dict --> model --> problem_definition --> identifiers --> imputers --> analysis_blocks --> accuracy_functions|\n+------------------+----------------------------------------------------------------------------------------------------------------------+----------------+-------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------+`",
    "tag": "mindsdb"
  },
  {
    "title": "Features",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/data-insights.mdx",
    "content": "\ntitle: Data Insights\nsidebarTitle: Data Insights\n\nData Insights is a data visualization feature of the MindsDB Cloud editor.\nIt lets you explore the queried data by initially displaying and analyzing a\nsubset of the first ten rows. You can choose to analyze a full dataset by\nclicking the `Full Data Analysis` button. The analysis presents the distribution\nof your data aggregated by column.\n\n\n\nThe data used here comes from one of our tutorials. For details, click\nhere.\nBefore you see the Data Insights pane, you must run a `SELECT` query on your\ndataset. Let's have a look at the available features.\nFeatures\nDistribution of Data per Column\nWhen opening the Data Insights pane, you see the distribution of data of each\noutput dataset column. Initially, the visualization and analysis of the first\nten rows is shown, as below.\n\n\n\nThere is one histogram per column that depicts the column name, data types of\nthe distribution, and the distribution itself.\n`Potential Bias` Flag\nTo see the `Potential Bias` flag, enter a full-screen mode of the Data Insights\npane.\n\n\n\nHere, the `location` column exhibits potential bias, as there are more `great`\ncolumn values than `good` or `poor` column values. Such cases are typically\nflagged. However, it does not necessarily mean that there is a problem with the\ndataset.\nThe `Potential Bias` flag is used when data does not distribute normally or\nuniformly, likely over-representing or under-representing some values. This may\nbe normal, hence, bias is only potential.\n`Missing Values` Flag\nTo see the `Missing Values` flag, enter a full-screen mode of the Data Insights\npane.\nThis flag indicates the proportion of missing values in a column. Columns with a\nhigh percentage of missing values are not useful for modeling purposes. Hence,\nit is recommended to pay attention to the `Missing Values` flag and try to\nmitigate it whenever possible, as it indicates the degrading quality of your\ndata.\nHovering Over the Histogram\nWhen hovering over the histogram, you get the information on a particular column\nvalue and how many of such values are present in a column. The format is\n`(column_value, count)`.\n\n\n\nIt is helpful to determine the exact data value counts from the histograms.\nFull Data Analysis\nLet's do a full data analysis step by step.\nFirst, we need to query data for analysis in the MindsDB Cloud editor. Please\nnote that you need to query your dataset without using a `LIMIT` keyword to be\nable to perform a complete data analysis.\n`sql\nSELECT *\nFROM example_db.demo_data.home_rentals;`\nOn execution, we get:\n`sql\n+---------------+-------------------+----+--------+--------------+--------------+------------+\n|number_of_rooms|number_of_bathrooms|sqft|location|days_on_market|neighborhood  |rental_price|\n+---------------+-------------------+----+--------+--------------+--------------+------------+\n|2              |1                  |917 |great   |13            |berkeley_hills|3901        |\n|0              |1                  |194 |great   |10            |berkeley_hills|2042        |\n|1              |1                  |543 |poor    |18            |westbrae      |1871        |\n|2              |1                  |503 |good    |10            |downtown      |3026        |\n|3              |2                  |1066|good    |13            |thowsand_oaks |4774        |\n+---------------+-------------------+----+--------+--------------+--------------+------------+`\nNow, open the Data Insights pane by clicking the `Data Insights` button to the\nright of the output table. Initially, it shows the analysis of the first ten\nrows of the output table.\n\n\n\nTo perform a complete analysis of your data, you can either go to a full-screen\nmode or stay in a pane mode and click on the `Full Data Analysis` button. Below\nis the complete data analysis.\n\n\n\nAlso, whenever your dataset changes, you can click on the\n`Refresh Data Analysis` button to update the data visualization and analysis.\nWhat's Next?\nWant to do more exploratory data analysis in MindsDB? We\u2019re collecting feedback\nto develop even more data visualization features. Let us know what you'd like to\nsee as part of Data Insights.\n\nLink to the feedback form \nThe link to our feedback form will be shared\n  soon.\n\n\nFrom Our Community\nCheck out the articles and video guides created by our community:\n\n\nArticle on How To Maintain ML-Analytics Cycle With Headless BI\n  by Jan Kadlec\n\n\nVideo guide on MindsDB Data Insights by\n  @akhilcoder\n\n",
    "tag": "mindsdb"
  },
  {
    "title": "The `information_schema` Database",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/table-structure.mdx",
    "content": "\ntitle: MindsDB Information Schema\nsidebarTitle: MindsDB Information Schema\n\nOn start-up, MindsDB consists of one system database (`information_schema`), one default project (`mindsdb`), and two base tables (`models` and `models_versions`) that belong to the default project.\nYou can verify it by running the following SQL commands:\n`sql\nSHOW [FULL] DATABASES;`\nOn execution, we get:\n`sql\n+----------------------+---------+--------+\n| Database             | TYPE    | ENGINE |\n+----------------------+---------+--------+\n| information_schema   | system  | [NULL] |\n| mindsdb              | project | [NULL] |\n| files                | data    | files  |\n+----------------------+---------+--------+`\nAnd:\n`sql\nSHOW [FULL] TABLES;`\nOn execution, we get:\n`sql\n+----------------------+-------------+\n| Tables_in_mindsdb    | Table_type  |\n+----------------------+-------------+\n| models               | BASE TABLE  |\n| models_versions      | BASE TABLE  |\n+----------------------+-------------+`\nThe `information_schema` Database\nThe `information_schema` database contains all the system tables, such as `databases`, `tables`, `columns`, `ml_engines`, etc.\nYou can query for any system information using this query template:\n`sql\nSELECT *\nFROM information_schema.table_name;`\nDon't forget to replace table_name with the table of your interest.\nLet's query for the following:\n\nThe `databases` table lists all the databases including their name, type, and engine.\n\n`sql\n  SELECT *\n  FROM information_schema.databases;`\nOn execution, we get:\n`sql\n  +------------------+-------+--------+\n  |NAME              |TYPE   |ENGINE  |\n  +------------------+-------+--------+\n  |information_schema|system |[NULL]  |\n  |mindsdb           |project|[NULL]  |\n  |my_new_project    |project|[NULL]  |\n  |files             |data   |files   |\n  +------------------+-------+--------+`\n\nThe `handlers` table lists all the available ML handlers.\n\n`sql\n  SELECT *\n  FROM information_schema.handlers;\n  -- or\n  SHOW HANDLERS;`\nOn execution, we get:\n`sql\n  +------------------+------------+---------------------------------+-------+------------------------------------------------------------------------------+----------------+----------------------+------+\n  |NAME              |TITLE       |DESCRIPTION                      |VERSION|CONNECTION_ARGS                                                               |IMPORT_SUCCESS  |IMPORT_ERROR          |FIELD8|\n  +------------------+------------+---------------------------------+-------+------------------------------------------------------------------------------+----------------+----------------------+------+\n  |merlion           |Merlion     |MindsDB handler for Merlion      |0.0.1  |[NULL]                                                                        |true            |[NULL]                |      |\n  |byom              |BYOM        |MindsDB handler for BYOM         |0.0.1  |{'model_code': {'type': 'path', 'description': 'The path name to model code'}}|true            |[NULL]                |      |\n  |ludwig            |Ludwig      |MindsDB handler for Ludwig AutoML|0.0.2  |[NULL]                                                                        |false           |No module named 'dask'|      |\n  |lightwood         |Lightwood   |[NULL]                           |1.0.0  |[NULL]                                                                        |true            |[NULL]                |      |\n  |huggingface       |Hugging Face|MindsDB handler for Higging Face |0.0.1  |[NULL]                                                                        |true            |[NULL]                |      |\n  +------------------+------------+---------------------------------+-------+------------------------------------------------------------------------------+----------------+----------------------+------+`\n\nThe `ml_engines` table lists all the ML engines. These are the instantiated ML handlers. Check out our docs on the CREATE ML _ENGINE command to learn more.\n\n`sql\n  SELECT *\n  FROM information_schema.ml_engines;\n  -- or\n  SHOW ML_ENGINES;`\nOn execution, we get:\n`sql\n  +------------------+-----------+----------------+\n  |NAME              |HANDLER    |CONNECTION_DATA |\n  +------------------+-----------+----------------+\n  |huggingface       |huggingface|{'password': ''}|\n  |lightwood         |lightwood  |{'password': ''}|\n  +------------------+-----------+----------------+`\nThe `mindsdb` Project\nYou create models and views within projects. The default project is `mindsdb`. But you can create your projects using the `CREATE DATABASE` statement, as below:\n`sql\nCREATE DATABASE my_new_project;`\nHere is how to create a model within your project:\n`sql\nCREATE MODEL my_new_project.my_model\nFROM integration_name\n    (SELECT * FROM table_name)\nPREDICT target;`\nAnd here is how to create a view within your project:\n`sql\nCREATE VIEW my_new_project.my_view (\n    SELECT *\n    FROM integration_name.table_name\n);`\nPlease replace the integration_name and table_name placeholders with your database name and your table name respectively.\n\nWhat If Names of a Model and a View are the Same?\n    Please note that if you use the same name for a model and a view stored in one project, then MindsDB adds `_view` to the view name.\n\nNow you can verify that the model and view are within your project:\n`sql\nSHOW FULL TABLES FROM my_new_project;`\nOn execution, we get:\n`sql\n+------------------------------+-------------+\n| Tables_in_my_new_project     | Table_type  |\n+------------------------------+-------------+\n| models                       | BASE TABLE  |\n| models_versions              | BASE TABLE  |\n| my_model                     | MODEL       |\n| my_view                      | VIEW        |\n+------------------------------+-------------+`\nThe `models` and `models_versions` Tables\nThe `mindsdb` project and every project that you create contain these two tables by default:\n\n\nThe `models` table stores information about the models within the project, such as `name`, `status`, `accuracy`, and more.\n\n\nThe `models_versions` table stores information about all present and past versions of each model.\n\n\nFor more information on the `models` and `models_versions` tables, visit our docs on the PROJECT entity.\nThe `files` Database\nIt is another default database that stores all the files uploaded by you to MindsDB Cloud.",
    "tag": "mindsdb"
  },
  {
    "title": "Connect your Database to MindsDB",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/native-queries.mdx",
    "content": "\ntitle: How to Run Native Queries in MindsDB\nsidebarTitle: Native Queries\n\nThe underlying database engine of MindsDB is MySQL. However, you can run queries native to your database engine within MindsDB.\nConnect your Database to MindsDB\nTo run queries native to your database, you must first connect your database to MindsDB using the `CREATE DATABASE` statement.\n`sql\nCREATE DATABASE example_db\nWITH ENGINE = \"postgres\",\nPARAMETERS = {\n    \"user\": \"demo_user\",\n    \"password\": \"demo_password\",\n    \"host\": \"3.220.66.106\",\n    \"port\": \"5432\",\n    \"database\": \"demo\"\n};`\nHere we connect the `example_db` database, which is a PostgreSQL database.\nRun Queries Native to your Database\nOnce we have our PostgreSQL database connected, we can run PostgreSQL-native queries.\nQuerying\nTo run PostgreSQL-native code, we must nest it within the `SELECT` statement like this:\n`sql\nSELECT * FROM example_db (\n    SELECT \n        model, \n        year, \n        price, \n        transmission, \n        mileage, \n        fueltype, \n        mpg, -- miles per galon\n        ROUND(CAST((mpg / 2.3521458) AS numeric), 1) AS kml, -- kilometers per liter\n        (date_part('year', CURRENT_DATE)-year) AS years_old, -- age of a car\n        COUNT(*) OVER (PARTITION BY model, year) AS units_to_sell, -- how many units of a certain model are sold in a year\n        ROUND((CAST(tax AS decimal) / price), 3) AS tax_div_price -- value of tax divided by price of a car\n    FROM demo_data.used_car_price\n);`\nOn execution, we get:\n`sql\n+-----+----+-----+------------+-------+--------+----+----+---------+-------------+-------------+\n|model|year|price|transmission|mileage|fueltype|mpg |kml |years_old|units_to_sell|tax_div_price|\n+-----+----+-----+------------+-------+--------+----+----+---------+-------------+-------------+\n| A1  |2010|9990 |Automatic   |38000  |Petrol  |53.3|22.7|12       |1            |0.013        |\n| A1  |2011|6995 |Manual      |65000  |Petrol  |53.3|22.7|11       |5            |0.018        |\n| A1  |2011|6295 |Manual      |107000 |Petrol  |53.3|22.7|11       |5            |0.02         |\n| A1  |2011|4250 |Manual      |116000 |Diesel  |70.6|30  |11       |5            |0.005        |\n| A1  |2011|6475 |Manual      |45000  |Diesel  |70.6|30  |11       |5            |0            |\n+-----+----+-----+------------+-------+--------+----+----+---------+-------------+-------------+`\nThe first line (`SELECT * FROM example_db`) informs MindsDB that we select from a PostgreSQL database. After that, we nest a PostgreSQL code within brackets.\nCreating Views\nWe can create a view based on a native query.\n`sql\nCREATE VIEW cars FROM example_db (\n        SELECT \n            model, \n            year, \n            price, \n            transmission, \n            mileage, \n            fueltype, \n            mpg, -- miles per galon\n            ROUND(CAST((mpg / 2.3521458) AS numeric), 1) AS kml, -- kilometers per liter\n            (date_part('year', CURRENT_DATE)-year) AS years_old, -- age of a car\n            COUNT(*) OVER (PARTITION BY model, year) AS units_to_sell, -- how many units of a certain model are sold in a year\n            ROUND((CAST(tax AS decimal) / price), 3) AS tax_div_price -- value of tax divided by price of a car\n        FROM demo_data.used_car_price\n);`\nOn execution, we get:\n```sql\nQuery OK, 0 rows affected (x.xxx sec)",
    "tag": "mindsdb"
  },
  {
    "title": "Working with `PROJECTS`",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/project.mdx",
    "content": "\ntitle: The PROJECT Entity\nsidebarTitle: MindsDB Projects\n\nMindsDB introduces the `PROJECT` entity that lets you create projects to store your ML experiments. In SQL words each project is a new database.\nWorking with `PROJECTS`\nCreating `PROJECTS`\nYou can create projects to store your models, making the structure like this:\n`project_alpha\n\u251c\u2500 models\n\u251c\u2500 models_versions\n\u251c\u2500 model_a\n\u251c\u2500 model_b\nproject_beta\n\u251c\u2500 models\n\u251c\u2500 models_versions\n\u251c\u2500 model_c`\nHere is how you create a project:\n`sql\nCREATE DATABASE project_alpha;`\nViewing `PROJECTS`\nThere are two ways you can list all your databases and projects:\n\n\nUse the `SHOW DATABASES` command:\n`sql\nSHOW DATABASES;`\nOn execution, we get:\n`sql\n+----------------------+\n| Database             |\n+----------------------+\n| information_schema   |\n| mindsdb              |\n| project_alpha        |\n| project_beta         |\n| files                |\n+----------------------+`\n\n\nUse the `SHOW FULL DATABASES` command to get more details:\n`sql\nSHOW FULL DATABASES;`\nOn execution, we get:\n`sql\n+----------------------+----------+-----------+\n| Database             | TYPE     | ENGINE    |\n+----------------------+----------+-----------+\n| information_schema   | system   | [NULL]    |\n| mindsdb              | project  | [NULL]    |\n| project_alpha        | project  | [NULL]    |\n| project_beta         | project  | [NULL]    |\n| files                | data     | files     |\n+----------------------+----------+-----------+`\n\n\nDropping `PROJECTS`\nHere is how you can remove a project:\n`sql\nDROP DATABASE project_alpha;`\nOn execution, we get:\n`sql\nQuery OK, 0 rows affected (x.xxx sec)`\n\nCannot Drop a Project\nPlease note that if your project stores at least one table, it cannot be removed. In this case, you should first drop all the tables, such as models and views, belonging to this project, and then, you can remove the project.\n   Please see the Example section for details.\n\nWorking with `MODELS`\nCreating `MODELS`\nHere is how you create a model within the project:\n`sql\nCREATE MODEL project_alpha.model_a\nFROM integration_name\n    (SELECT * FROM table_name)\nPREDICT target;`\nOn execution, we get:\n`sql\nQuery OK, 0 rows affected (x.xxx sec)`\nViewing `MODELS`\nTo see all the models from all projects, run the command below.\n`sql\nSHOW MODELS;`\nOn execution, we get:\n`sql\n+---------+--------------+--------+--------+---------+-------------+---------------+------+------------------------+--------------------+\n|NAME     |PROJECT       |STATUS  |ACCURACY|PREDICT  |UPDATE_STATUS|MINDSDB_VERSION|ERROR |SELECT_DATA_QUERY       |TRAINING_OPTIONS    |\n+---------+--------------+--------+--------+---------+-------------+---------------+------+------------------------+--------------------+\n|model_a  |project_alpha |complete|0.999   |target   |up_to_date   |22.10.2.1      |[NULL]|SELECT * FROM table_name|{'target': 'target'}|\n|model_b  |project_alpha |complete|0.999   |target   |up_to_date   |22.10.2.1      |[NULL]|SELECT * FROM table_name|{'target': 'target'}|\n|model_c  |project_beta  |complete|0.999   |target   |up_to_date   |22.10.2.1      |[NULL]|SELECT * FROM table_name|{'target': 'target'}|\n+---------+--------------+--------+--------+---------+-------------+---------------+------+------------------------+--------------------+`\nAnd if you want to list all the models from a defined project, run either of the commands below.\n`sql\nSHOW MODELS \nFROM project_alpha;\n-- or\nSELECT * \nFROM project_alpha.models;`\nOn execution, we get:\n`sql\n+---------+--------------+--------+--------+---------+-------------+---------------+------+------------------------+--------------------+\n|NAME     |PROJECT       |STATUS  |ACCURACY|PREDICT  |UPDATE_STATUS|MINDSDB_VERSION|ERROR |SELECT_DATA_QUERY       |TRAINING_OPTIONS    |\n+---------+--------------+--------+--------+---------+-------------+---------------+------+------------------------+--------------------+\n|model_a  |project_alpha |complete|0.999   |target   |up_to_date   |22.10.2.1      |[NULL]|SELECT * FROM table_name|{'target': 'target'}|\n|model_b  |project_alpha |complete|0.999   |target   |up_to_date   |22.10.2.1      |[NULL]|SELECT * FROM table_name|{'target': 'target'}|\n+---------+--------------+--------+--------+---------+-------------+---------------+------+------------------------+--------------------+`\nHere is how to run a detailed search:\n`sql\nSHOW MODELS \nFROM project_alpha \nLIKE 'model_a' \nWHERE status='complete';`\nOn execution, we get:\n`sql\n+---------+--------------+--------+--------+---------+-------------+---------------+------+------------------------+--------------------+\n|NAME     |PROJECT       |STATUS  |ACCURACY|PREDICT  |UPDATE_STATUS|MINDSDB_VERSION|ERROR |SELECT_DATA_QUERY       |TRAINING_OPTIONS    |\n+---------+--------------+--------+--------+---------+-------------+---------------+------+------------------------+--------------------+\n|model_a  |project_alpha |complete|0.999   |target   |up_to_date   |22.10.2.1      |[NULL]|SELECT * FROM table_name|{'target': 'target'}|\n+---------+--------------+--------+--------+---------+-------------+---------------+------+------------------------+--------------------+`\nDropping `MODELS`\nTo drop a model, run this command:\n`sql\nDROP MODEL project_alpha.model_a;`\nOn execution, we get:\n`sql\nQuery OK, 0 rows affected (x.xxx sec)`\nWorking with `MODELS_VERSION`\nThere is a `models_versions` table for each project that stores all the versions of your models.\nHere is how to query for all model versions from all the projects:\n`sql\nSELECT * \nFROM information_schema.models_versions;`\nOn execution, we get:\n`sql\n+-------+-------------+------+-------+--------+--------+-------+-------------+---------------+------+------------------------+--------------------+\n|NAME   |PROJECT      |ACTIVE|VERSION|STATUS  |ACCURACY|PREDICT|UPDATE_STATUS|MINDSDB_VERSION|ERROR |SELECT_DATA_QUERY       |TRAINING_OPTIONS    |\n+-------+-------------+------+-------+--------+--------+-------+-------------+---------------+------+------------------------+--------------------+\n|model_a|project_alpha|true  |1      |complete|0.999   |target |up_to_date   |22.10.2.1      |[NULL]|SELECT * FROM table_name|{'target': 'target'}|\n|model_b|project_alpha|true  |1      |complete|0.999   |target |up_to_date   |22.10.2.1      |[NULL]|SELECT * FROM table_name|{'target': 'target'}|\n|model_c|project_beta |true  |1      |complete|0.999   |target |up_to_date   |22.10.2.1      |[NULL]|SELECT * FROM table_name|{'target': 'target'}|\n+-------+-------------+------+-------+--------+--------+-------+-------------+---------------+------+------------------------+--------------------+`\n\nExample\n\n\n```If there is more training data available, you don't need to recreate your model. Instead, use the `RETRAIN` command.\n\n```sql\nRETRAIN project_alpha.model_b;\n```\n\nAfter the retraining process completes, here is what you get:\n\n```sql\nSELECT * \nFROM information_schema.models_versions;\n```\n\nOn execution, we get:\n\n```sql\n+-------+-------------+------+-------+--------+--------+-------+-------------+---------------+------+------------------------+--------------------+\n|NAME   |PROJECT      |ACTIVE|VERSION|STATUS  |ACCURACY|PREDICT|UPDATE_STATUS|MINDSDB_VERSION|ERROR |SELECT_DATA_QUERY       |TRAINING_OPTIONS    |\n+-------+-------------+------+-------+--------+--------+-------+-------------+---------------+------+------------------------+--------------------+\n|model_a|project_alpha|true  |1      |complete|0.999   |target |up_to_date   |22.10.2.1      |[NULL]|SELECT * FROM table_name|{'target': 'target'}|\n|model_b|project_alpha|false |1      |complete|0.999   |target |up_to_date   |22.10.2.1      |[NULL]|SELECT * FROM table_name|{'target': 'target'}|\n|model_b|project_alpha|true  |2      |complete|0.999   |target |up_to_date   |22.10.2.1      |[NULL]|SELECT * FROM table_name|{'target': 'target'}|\n|model_c|project_beta |true  |1      |complete|0.999   |target |up_to_date   |22.10.2.1      |[NULL]|SELECT * FROM table_name|{'target': 'target'}|\n+-------+-------------+------+-------+--------+--------+-------+-------------+---------------+------+------------------------+--------------------+\n```\n\nNow, the `model_b` model has two records storing its two versions, out of which one is active.\n```\n\n\n\nYou can also query for model versions of a project using this `SELECT` statement:\n`sql\nSELECT * FROM project_alpha.models_versions;`\nOn execution, we get:\n`sql\n+-------+-------------+------+-------+--------+--------+-------+-------------+---------------+------+------------------------+--------------------+\n|NAME   |PROJECT      |ACTIVE|VERSION|STATUS  |ACCURACY|PREDICT|UPDATE_STATUS|MINDSDB_VERSION|ERROR |SELECT_DATA_QUERY       |TRAINING_OPTIONS    |\n+-------+-------------+------+-------+--------+--------+-------+-------------+---------------+------+------------------------+--------------------+\n|model_a|project_alpha|true  |1      |complete|0.999   |target |up_to_date   |22.10.2.1      |[NULL]|SELECT * FROM table_name|{'target': 'target'}|\n|model_b|project_alpha|false |1      |complete|0.999   |target |up_to_date   |22.10.2.1      |[NULL]|SELECT * FROM table_name|{'target': 'target'}|\n|model_b|project_alpha|true  |2      |complete|0.999   |target |up_to_date   |22.10.2.1      |[NULL]|SELECT * FROM table_name|{'target': 'target'}|\n+-------+-------------+------+-------+--------+--------+-------+-------------+---------------+------+------------------------+--------------------+`\nWorking with `TABLES`\nThe models that you create with the `CREATE MODEL` command are simple tables within a project. Therefore, you can use the `SHOW [FULL] TABLES` commands to query for them.\nHere is how to query for tables from all databases/projects/schemas:\n`sql\nSELECT table_schema, table_name, table_type\nFROM information_schema.tables \nWHERE table_type IN ('BASE TABLE', 'MODEL');`\nOn execution, we get:\n`sql\n+--------------+----------------+------------+\n|table_schema  |table_name      |table_type\n+--------------+----------------+------------+\n|mindsdb       |models          |BASE TABLE  |\n|mindsdb       |models_versions |BASE TABLE  |\n|project_alpha |models          |BASE TABLE  |\n|project_alpha |models_versions |BASE TABLE  |\n|project_beta  |models          |BASE TABLE  |\n|project_beta  |models_versions |BASE TABLE  |\n|project_alpha |model_a         |MODEL       |\n|project_alpha |model_b         |MODEL       |\n|project_beta  |model_c         |MODEL       |\n+--------------+----------------+------------+`\n\nDefault Tables\n\n\n```Please note that each project contains two tables by default. These are the `models` table and the `models_versions` table.\n```\n\n\n\nThere are also shortcut commands to query for the tables:\n\n\nQuerying for tables from the default project:\n`sql\nSHOW TABLES;`\nOn execution, we get:\n`sql\n+---------------------+\n|Tables_in_mindsdb    |\n+---------------------+\n|models               |\n|models_versions      |\n+---------------------+`\nOr, to get more details:\n`sql\nSHOW FULL TABLES;`\nOn execution, we get:\n`sql\n+---------------------+-----------+\n|Tables_in_mindsdb    |Table_type |\n+---------------------+-----------+\n|models               |BASE TABLE |\n|models_versions      |BASE TABLE |\n+---------------------+-----------+`\n\n\n\nHow to Set a Default Project\n\n\n```  The default project is set to `mindsdb`. If you want to change it, run the `USE project_name;` command.\n```\n\n\n\n\n\nQuerying for tables from a defined project:\n`sql\nSHOW TABLES FROM project_alpha;`\nOn execution, we get:\n`sql\n+-------------------------+\n|Tables_in_project_alpha  |\n+-------------------------+\n|models                   |\n|models_versions          |\n|model_a                  |\n|model_b                  |\n+-------------------------+`\nOr, to get more details:\n`sql\nSHOW FULL TABLES FROM project_alpha;`\nOn execution, we get:\n`sql\n+-------------------------+-----------+\n|Tables_in_project_alpha  |Table_type |\n+-------------------------+-----------+\n|models                   |BASE TABLE |\n|models_versions          |BASE TABLE |\n|model_a                  |MODEL      |\n|model_b                  |MODEL      |\n+-------------------------+-----------+`\n\n\nExample\nLet's create a project.\n`sql\nCREATE DATABASE my_project;`\nTo verify that the project was created successfully, let's run the command below.\n`sql\nSHOW FULL DATABASES;`\nOn execution, we get:\n`sql\n+------------------+-------+------+\n|Database          |TYPE   |ENGINE|\n+------------------+-------+------+\n|information_schema|system |[NULL]|\n|mindsdb           |project|[NULL]|\n|my_project        |project|[NULL]|\n|files             |data   |files |\n+------------------+-------+------+`\n\nDefault Databases\n\n\n```Please note that `information_schema`, `mindsdb`, and `files` are the default databases. For more information, please visit our docs on [MindsDB default structure](/sql/table-structure/).\n```\n\n\n\nNow we create a model within the project.\n`sql\nCREATE MODEL my_project.my_model\nFROM example_db\n    (SELECT * FROM demo_data.home_rentals)\nPREDICT rental_price;`\nAlso, let's create a view.\n`sql\nCREATE VIEW my_project.my_view (\n    SELECT * \n    FROM example_db.demo_data.home_rentals\n);`\nHere is what we have in the `my_project` project.\n`sql\nSHOW TABLES FROM my_project;`\nOn execution, we get:\n`sql\n+--------------------+\n|Tables_in_my_project|\n+--------------------+\n|models              |\n|models_versions     |\n|my_model            |\n|my_view             |\n+--------------------+`\nLet's try to delete our project.\n`sql\nDROP DATABASE my_project;`\nOn execution, we get:\n`sql\nProject 'my_project' can not be deleted, because it contains tables: my_model, my_view`\nUsers should remove all project content before dropping a project.\n`sql\nDROP MODEL my_project.my_model;\nDROP VIEW my_project.my_view;`\nNow we can proceed to drop a project.\n```sql\nDROP DATABASE my_project;",
    "tag": "mindsdb"
  },
  {
    "title": "Description",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/api/join-on.mdx",
    "content": "\ntitle: JOIN ... ON Statement\nsidebarTitle: Two or More Data Tables\n\nDescription\nThe `JOIN` statement joins two tables `ON` a defined column. It is a regular `JOIN` used throughout SQL.\nSyntax\nHere is the syntax:\n`sql\nSELECT t1.column_name, t2.column_name, ...\nFROM integration_name.table_name [AS] t1\nJOIN integration_name.table_name [AS] t2\nON t1.column_name = t2.column_name;`\nOn execution, we get:\n`sql\n+-----------------+-----------------+\n| t1.column_name  | t2.column_name  |\n+-----------------+-----------------+\n| t1.value        | t2.value        |\n+-----------------+-----------------+`\nWhere:\n| Name                                | Description                                     |\n| ----------------------------------- | ----------------------------------------------- |\n| `t1.column_name`                    | Name of the column from the first table.        |\n| `t2.column_name`                    | Name of the column from the second table.       |\n| `integration_name.table_name`       | Name of the table used in the `JOIN` operation. |\n\nNested `JOINs`\nMindsDB provides you with two categories of `JOINs`. One is the `JOIN` statement which combines the data table with the model table in order to fetch bulk predictions. Another is the regular `JOIN` used throughout SQL. Please note that only the latter one requires the `ON` clause.\nYou can nest these types of `JOINs` as follows:\n`sql\nSELECT * FROM (\n    SELECT *\n    FROM project_name.model_table AS m\n    JOIN integration_name.data_table AS d;\n) AS t1\nJOIN (\n    SELECT *\n    FROM project_name.model_table AS m\n    JOIN integration_name.data_table AS d;\n) AS t2\nON t1.column_name = t2.column_name;`\n\nExample 1\nLet's use the following data to see how the different types of `JOINs` work.\nThe `pets` table that stores pets:\n`sql\n+------+-------+\n|pet_id|name   |\n+------+-------+\n|1     |Moon   |\n|2     |Ripley |\n|3     |Bonkers|\n|4     |Star   |\n|5     |Luna   |\n|6     |Lake   |\n+------+-------+`\nAnd the `owners` table that stores pets' owners:\n`sql\n+--------+-------+------+\n|owner_id|name   |pet_id|\n+--------+-------+------+\n|1       |Amy    |4     |\n|2       |Bob    |1     |\n|3       |Harry  |5     |\n|4       |Julia  |2     |\n|5       |Larry  |3     |\n|6       |Henry  |0     |\n+--------+-------+------+`\n`JOIN` or `INNER JOIN`\nThe `JOIN` or `INNER JOIN` command joins the rows of the `owners` and `pets` tables wherever there is a match. For example, a pet named Lake does not have an owner, so it'll be left out.\n`sql\nSELECT *\nFROM files.owners o\n[INNER] JOIN files.pets p\nON o.pet_id = p.pet_id;`\nOn execution, we get:\n`sql\n+--------+-------+------+------+-------+\n|owner_id|name   |pet_id|pet_id|name   |\n+--------+-------+------+------+-------+\n|1       |Amy    |4     |4     |Star   |\n|2       |Bob    |1     |1     |Moon   |\n|3       |Harry  |5     |5     |Luna   |\n|4       |Julia  |2     |2     |Ripley |\n|5       |Larry  |3     |3     |Bonkers|\n+--------+-------+------+------+-------+`\nAs in standard SQL, you can use the `WHERE` clause to filter the output data.\n`sql\nSELECT *\nFROM files.owners o\n[INNER] JOIN files.pets p\nON o.pet_id = p.pet_id\nWHERE o.name = 'Amy'\nOR o.name = 'Bob';`\nOn execution, we get:\n`sql\n+--------+-------+------+------+-------+\n|owner_id|name   |pet_id|pet_id|name   |\n+--------+-------+------+------+-------+\n|1       |Amy    |4     |4     |Star   |\n|2       |Bob    |1     |1     |Moon   |\n+--------+-------+------+------+-------+`\n`LEFT JOIN`\nThe `LEFT JOIN` command joins the rows of two tables such that all rows from the left table, even the ones with no match, show up. Here, the left table is the `owners` table.\n`sql\nSELECT *\nFROM files.owners o\nLEFT JOIN files.pets p\nON o.pet_id = p.pet_id;`\nOn execution, we get:\n`sql\n+--------+-------+------+------+-------+\n|owner_id|name   |pet_id|pet_id|name   |\n+--------+-------+------+------+-------+\n|1       |Amy    |4     |4     |Star   |\n|2       |Bob    |1     |1     |Moon   |\n|3       |Harry  |5     |5     |Luna   |\n|4       |Julia  |2     |2     |Ripley |\n|5       |Larry  |3     |3     |Bonkers|\n|6       |Henry  |0     |[NULL]|[NULL] |\n+--------+-------+------+------+-------+`\n`RIGHT JOIN`\nThe `RIGHT JOIN` command joins the rows of two tables such that all rows from the right table, even the ones with no match, show up. Here, the right table is the `pets` table.\n`sql\nSELECT *\nFROM files.owners o\nRIGHT JOIN files.pets p\nON o.pet_id = p.pet_id;`\nOn execution, we get:\n`sql\n+--------+-------+------+------+-------+\n|owner_id|name   |pet_id|pet_id|name   |\n+--------+-------+------+------+-------+\n|2       |Bob    |1     |1     |Moon   |\n|4       |Julia  |2     |2     |Ripley |\n|5       |Larry  |3     |3     |Bonkers|\n|1       |Amy    |4     |4     |Star   |\n|3       |Harry  |5     |5     |Luna   |\n|[NULL]  |[NULL] |[NULL]|6     |Lake   |\n+--------+-------+------+------+-------+`\n`FULL JOIN` or `FULL OUTER JOIN`\nThe `FULL [OUTER] JOIN` command joins the rows of two tables such that all rows from both tables, even the ones with no match, show up.\n`sql\nSELECT *\nFROM files.owners o\nFULL [OUTER] JOIN files.pets p\nON o.pet_id = p.pet_id;`\nOn execution, we get:\n`sql\n+--------+------+------+------+-------+---------+\n|owner_id|name  |pet_id|pet_id|name   |animal_id|\n+--------+------+------+------+-------+---------+\n|1       |Amy   |4     |4     |Star   |2        |\n|2       |Bob   |1     |1     |Moon   |1        |\n|3       |Harry |5     |5     |Luna   |2        |\n|4       |Julia |2     |2     |Ripley |1        |\n|5       |Larry |3     |3     |Bonkers|3        |\n|6       |Henry |0     |[NULL]|[NULL] |[NULL]   |\n|[NULL]  |[NULL]|[NULL]|6     |Lake   |4        |\n+--------+------+------+------+-------+---------+`\nExample 2\nMore than two tables can be joined subsequently.\nLet's use another table called `animals`:\n`sql\n+---------+-------+\n|animal_id|name   |\n+---------+-------+\n|1        |Dog    |\n|2        |Cat    |\n|3        |Hamster|\n|4        |Fish   |\n+---------+-------+`\nNow we can join all three tables.\n`sql\nSELECT *\nFROM files.owners o\nRIGHT JOIN files.pets p ON o.pet_id = p.pet_id\nJOIN files.animals a ON p.animal_id = a.animal_id;`\nOn execution, we get:\n```sql\n+--------+-------+------+------+-------+---------+---------+-------+\n|owner_id|name   |pet_id|pet_id|name   |animal_id|animal_id|name   |\n+--------+-------+------+------+-------+---------+---------+-------+\n|2       |Bob    |1     |1     |Moon   |1        |1        |Dog    |\n|4       |Julia  |2     |2     |Ripley |1        |1        |Dog    |\n|5       |Larry  |3     |3     |Bonkers|3        |3        |Hamster|\n|1       |Amy    |4     |4     |Star   |2        |2        |Cat    |\n|3       |Harry  |5     |5     |Luna   |2        |2        |Cat    |\n|[NULL]  |[NULL] |[NULL]|6     |Lake   |4        |4        |Fish   |\n+--------+-------+------+------+-------+---------+---------+-------+",
    "tag": "mindsdb"
  },
  {
    "title": "Creating a Model",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/api/manage-models-versions.mdx",
    "content": "\ntitle: How to Use and Manage Model Versions in MindsDB\nsidebarTitle: Managing Model Versions\n\nCreating a Model\nTo create a model, use the CREATE MODEL statement.\n`sql\nCREATE MODEL mindsdb.home_rentals\nFROM example_db\n    (SELECT * FROM demo_data.home_rentals)\nPREDICT rental_price\nUSING engine = 'lightwood',\n      tag = 'my model';`\nNow, your model has one version. You can verify by querying the `models_versions` table.\n`sql\nSELECT *\nFROM mindsdb.models_versions\nWHERE name = 'home_rentals';`\nOn execution, we get:\n`sql\n+------------+-------+------+-------+--------+--------+------------+-------------+---------------+------+------------------------------------+---------------------------------------+--------+\n|NAME        |PROJECT|ACTIVE|VERSION|STATUS  |ACCURACY|PREDICT     |UPDATE_STATUS|MINDSDB_VERSION|ERROR |SELECT_DATA_QUERY                   |TRAINING_OPTIONS                       |TAG     |\n+------------+-------+------+-------+--------+--------+------------+-------------+---------------+------+------------------------------------+---------------------------------------+--------+\n|home_rentals|mindsdb|true  |1      |complete|0.999   |rental_price|up_to_date   |22.11.3.2      |[NULL]|SELECT * FROM demo_data.home_rentals|{'target': 'rental_price', 'using': {}}|my model|\n+------------+-------+------+-------+--------+--------+------------+-------------+---------------+------+------------------------------------+---------------------------------------+--------+`\nRetraining a Model\nTo retrain a model, use the RETRAIN statement.\n`sql\nRETRAIN mindsdb.home_rentals;`\nLet's query for the model versions again.\n`sql\nSELECT *\nFROM mindsdb.models_versions\nWHERE name = 'home_rentals';`\nOn execution, we get:\n`sql\n+------------+-------+------+-------+--------+--------+------------+-------------+---------------+------+------------------------------------+---------------------------------------+--------+\n|NAME        |PROJECT|ACTIVE|VERSION|STATUS  |ACCURACY|PREDICT     |UPDATE_STATUS|MINDSDB_VERSION|ERROR |SELECT_DATA_QUERY                   |TRAINING_OPTIONS                       |TAG     |\n+------------+-------+------+-------+--------+--------+------------+-------------+---------------+------+------------------------------------+---------------------------------------+--------+\n|home_rentals|mindsdb|false |1      |complete|0.999   |rental_price|up_to_date   |22.11.3.2      |[NULL]|SELECT * FROM demo_data.home_rentals|{'target': 'rental_price', 'using': {}}|my model|\n|home_rentals|mindsdb|true  |2      |complete|0.999   |rental_price|up_to_date   |22.11.3.2      |[NULL]|SELECT * FROM demo_data.home_rentals|{'target': 'rental_price', 'using': {}}|my model|\n+------------+-------+------+-------+--------+--------+------------+-------------+---------------+------+------------------------------------+---------------------------------------+--------+`\nUsing Active Model Version\nTo use the currently active model version, run this query:\n`sql\nSELECT *\nFROM mindsdb.home_rentals AS p\nJOIN example_db.demo_data.home_rentals AS d;`\nUsing Specific Model Version\nTo use a specific model version, even if it is set to inactive, run this query:\n`sql\nSELECT *\nFROM mindsdb.home_rentals.1 AS p\nJOIN example_db.demo_data.home_rentals AS d;`\nSetting Model Version as Active\nTo set a specific model version as active, run the `UPDATE` statement:\n`sql\nUPDATE mindsdb.models_versions\nSET active = 1\nWHERE version = 1\nAND name = 'home_rentals';`\nDeleting Specific Model Version\nTo delete a specific model version, run the `DELETE FROM` statement:\n`sql\nDELETE FROM mindsdb.models_versions\nWHERE version = 2\nAND name = 'home_rentals';`\nPlease note that you cannot delete the version that is active.\nDeleting All Model Versions\nTo delete all models version, run the `DROP MODEL` statement:\n```sql\nDROP MODEL mindsdb.home_rentals;",
    "tag": "mindsdb"
  },
  {
    "title": "Description",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/api/join.mdx",
    "content": "\ntitle: JOIN Statement\nsidebarTitle: AI Table with Data Table\n\nDescription\nThe `JOIN` clause combines rows from the database table and the model table on a column defined in its implementation. It is used to make bulk predictions, as shown in the examples.\nSyntax\nHere is the syntax:\n`sql\nSELECT t.column_name, p.column_name, ...\nFROM integration_name.table_name [AS] t\nJOIN project_name.model_name [AS] p;`\nOn execution, we get:\n`sql\n+-----------------+-----------------+\n| t.column_name   | p.column_name   |\n+-----------------+-----------------+\n| t.value         | p.value         |\n+-----------------+-----------------+`\nWhere:\n| Name                                | Description                                                         |\n| ----------------------------------- | ------------------------------------------------------------------- |\n| `integration_name.table_name`       | Name of the data source table used as input for making predictions. |\n| `project_name.model_name`           | Name of the model table used to make predictions.                   |\n| `p.value`                           | Predicted value stored in the output table.                         |\nExample 1\nLet's join the `home_rentals` table with the `home_rentals_model` model using this statement:\n`sql\nSELECT t.rental_price AS real_price, \n       m.rental_price AS predicted_price,\n       t.number_of_rooms,  t.number_of_bathrooms,\n       t.sqft, t.location, t.days_on_market \nFROM example_db.demo_data.home_rentals AS t \nJOIN mindsdb.home_rentals_model AS m \nLIMIT 20;`\nOn execution, we get:\n`sql\n+------------+-----------------+-----------------+---------------------+------+----------+----------------+\n| real_price | predicted_price | number_of_rooms | number_of_bathrooms | sqft | location | days_on_market |\n+------------+-----------------+-----------------+---------------------+------+----------+----------------+\n| 3901       | 3886            | 2               | 1                   | 917  | great    | 13             |\n| 2042       | 2007            | 0               | 1                   | 194  | great    | 10             |\n| 1871       | 1865            | 1               | 1                   | 543  | poor     | 18             |\n| 3026       | 3020            | 2               | 1                   | 503  | good     | 10             |\n| 4774       | 4748            | 3               | 2                   | 1066 | good     | 13             |\n| 4382       | 4388            | 3               | 2                   | 816  | poor     | 25             |\n| 2269       | 2272            | 0               | 1                   | 461  | great    | 6              |\n| 2284       | 2272            | 1               | 1                   | 333  | great    | 6              |\n| 5420       | 5437            | 3               | 2                   | 1124 | great    | 9              |\n| 5016       | 4998            | 3               | 2                   | 1204 | good     | 7              |\n| 1421       | 1427            | 0               | 1                   | 538  | poor     | 43             |\n| 3476       | 3466            | 2               | 1                   | 890  | good     | 6              |\n| 5271       | 5255            | 3               | 2                   | 975  | great    | 6              |\n| 3001       | 2993            | 2               | 1                   | 564  | good     | 13             |\n| 4682       | 4692            | 3               | 2                   | 953  | good     | 10             |\n| 1783       | 1738            | 1               | 1                   | 493  | poor     | 24             |\n| 1548       | 1543            | 1               | 1                   | 601  | poor     | 47             |\n| 1492       | 1491            | 0               | 1                   | 191  | good     | 12             |\n| 2431       | 2419            | 0               | 1                   | 511  | great    | 1              |\n| 4237       | 4257            | 3               | 2                   | 916  | poor     | 36             |\n+------------+-----------------+-----------------+---------------------+------+----------+----------------+`\nExample 2\nLet's query a time series model using this statement:\n`sql\nSELECT m.saledate as date,\n       m.ma AS forecast\nFROM mindsdb.house_sales_model AS m \nJOIN example_db.demo_data.house_sales AS t\nWHERE t.saledate > LATEST\nAND t.type = 'house'\nLIMIT 4;`\nOn execution, we get:\n```sql\n+----------+------------------+\n|date      |forecast          |\n+----------+------------------+\n|2019-12-31|517506.31349071994|\n|2019-12-31|627822.6592658638 |\n|2019-12-31|953426.9545788583 |\n|2019-12-31|767252.4205039773 |\n+----------+------------------+",
    "tag": "mindsdb"
  },
  {
    "title": "Description",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/api/update.mdx",
    "content": "\ntitle: UPDATE FROM SELECT Statement\nsidebarTitle: UPDATE\n\nDescription\nThe `UPDATE FROM SELECT` statement updates data in existing table. The data comes from a subselect query.\nIt can be used as alternative to 'create table' and 'insert into' for store predictions in distinct columns of existing rows\nSyntax\nHere is an example:\n`sql\nUPDATE\n    int2.table2\nSET\n    predicted = source.result,\nFROM\n (\n     SELECT p.result, p.prod_id, p.shop_id\n      FROM int1.table1 as t\n     JOIN mindsdb.pred1 as p\n ) AS source\nWHERE\n    prod_id = source.prod_id\n    and shop_id = source.shop_id`\nAnd the steps followed by the syntax:\n\nIt executes query from 'FROM' block to get the output data.\n  In our example it is join of table table1 (from integration int1) with predictor pred1.\n  It also can be select from integration\nsource is the alias for fetched data\nthen it updates table2 from int2 using conditions from `WHERE` block and fields for update from `SET` block\nunder the hood it splits input data to rows and execute this query for every row:\n  `sql\n  UPDATE\n      table2\n  SET\n      predicted = <row.result>,\n  WHERE\n      prod_id = <row.prod_id>\n      and shop_id = <row.shop_id>`\n\nNote: in `WHERE` block it is better to use primary key for table\nor set of rows that can be a primary key (and identifies every row in table).\nOtherwise, it can lead to unexpected results when one row in destination table was updated several times",
    "tag": "mindsdb"
  },
  {
    "title": "Description",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/api/insert.mdx",
    "content": "\ntitle: INSERT INTO Statement\nsidebarTitle: INSERT INTO\n\nDescription\nThe `INSERT INTO` statement inserts data into a table. The data comes from a subselect query. It is commonly used to input prediction results into a database table.\nSyntax\nHere is the syntax:\n`sql\nINSERT INTO integration_name.table_name\n    (SELECT ...);`\nPlease note that the destination table (`integration_name.table_name`) must\nexist and contain all the columns where the data is to be inserted.\nAnd the steps followed by the syntax:\n\nIt executes a subselect query to get the output dataset.\nIt uses the `INSERT INTO` statement to insert the output of the\n  `(SELECT ...)` query into the `integration_name.table_name` table.\n\nOn execution, we get:\n`sql\nQuery OK, 0 row(s) updated - x.xxxs`\nExample\nWe want to save the prediction results into the `int1.tbl1` table.\nHere is the schema structure used throughout this example:\n`bash\nint1\n\u2514\u2500\u2500 tbl1\nmindsdb\n\u2514\u2500\u2500 predictor_name\nint2\n\u2514\u2500\u2500 tbl2`\nWhere:\n| Name             | Description                                                                           |\n| ---------------- | ------------------------------------------------------------------------------------- |\n| `int1`           | Integration where the table that stores prediction results resides.                   |\n| `tbl1`           | Table that stores prediction results.                                                 |\n| `predictor_name` | Name of the model.                                                                    |\n| `int2`           | Integration where the data source table used in the inner `SELECT` statement resides. |\n| `tbl2`           | Data source table used in the inner `SELECT` statement.                               |\nLet's execute the query.\n`sql\nINSERT INTO int1.tbl1 (\n    SELECT *\n    FROM int2.tbl2 AS ta\n    JOIN mindsdb.predictor_name AS tb\n    WHERE ta.date > '2015-12-31'\n);`\nOn execution, we get:\n```sql\nQuery OK, 0 row(s) updated - x.xxxs",
    "tag": "mindsdb"
  },
  {
    "title": "`DESCRIBE MODEL ... FEATURES` Statement",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/api/describe.mdx",
    "content": "\ntitle: DESCRIBE MODEL Statement\nsidebarTitle: DESCRIBE MODEL\n\nThe `DESCRIBE MODEL` statement is used to display the attributes of an existing model.\n`DESCRIBE MODEL ... FEATURES` Statement\nDescription\nThe `DESCRIBE MODEL mindsdb.predictor_name.features` statement displays how the model encoded the data before the training process.\nSyntax\nHere is the syntax:\n`sql\nDESCRIBE MODEL mindsdb.predictor_name.features;`\nOn execution, we get:\n`sql\n+--------------+-------------+--------------+-------------+\n| column       | type        | encoder      | role        |\n+--------------+-------------+--------------+-------------+\n| column_name  | column_type | encoder_used | column_role |\n+--------------+-------------+--------------+-------------+`\nWhere:\n| Name                 | Description                                           |\n| -------------------- | ----------------------------------------------------- |\n| `predictor_name`     | Name of the model to be described.                    |\n| `column`             | Data columns that were used to create the model.      |\n| `type`               | Data type of the column.                              |\n| `encoder`            | Encoder type used for the column.                     |\n| `role`               | Role of the column (`feature` or `target`).           |\nExample\nLet's look at an example using the `home_rentals_model` model.\n`sql\nDESCRIBE MODEL mindsdb.home_rentals_model.features;`\nOn execution, we get:\n`sql\n+---------------------+-------------+----------------+---------+\n| column              | type        | encoder        | role    |\n+---------------------+-------------+----------------+---------+\n| number_of_rooms     | categorical | OneHotEncoder  | feature |\n| number_of_bathrooms | binary      | BinaryEncoder  | feature |\n| sqft                | integer     | NumericEncoder | feature |\n| location            | categorical | OneHotEncoder  | feature |\n| days_on_market      | integer     | NumericEncoder | feature |\n| neighborhood        | categorical | OneHotEncoder  | feature |\n| rental_price        | integer     | NumericEncoder | target  |\n+---------------------+-------------+----------------+---------+`\nHere the `rental_price` column is the `target` column to be predicted. As for the `feature` columns, these are used to train the ML model to predict the value of the `rental_price` column.\n`DESCRIBE MODEL ... MODEL` Statement\nDescription\nThe `DESCRIBE MODEL mindsdb.predictor_name.model` statement displays the performance of the candidate models.\nSyntax\nHere is the syntax:\n`sql\nDESCRIBE MODEL mindsdb.predictor_name.model;`\nOn execution, we get:\n`sql\n+-----------------+-------------+---------------+-----------+---------------------+\n| name            | performance | training_time | selected  | accuracy_functions  |\n+-----------------+-------------+---------------+-----------+---------------------+\n| candidate_model | performance | training_time | selected  | accuracy_functions  |\n+-----------------+-------------+---------------+-----------+---------------------+`\nWhere:\n| Name                       | Description                                                                                                                                                                                                                                                                                                                            |\n| -------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `predictor_name`           | Name of the model to be described.                                                                                                                                                                                                                                                                                                     |\n| `name`                     | Name of the candidate model.                                                                                                                                                                                                                                                                                                           |\n| `performance`              | Accuracy value from 0 to 1, depending on the type of the model.                                                                                                                                                                                                                                                                        |\n| `training_time`            | Time elapsed for the training of the model.                                                                                                                                                                                                                                                                                            |\n| `selected`                 | `1` for the best performing model and `0` for the rest.                                                                                                                                                                                                                                                                                |\n| `accuracy_functions`       | It stores the `r2_score` value for regression predictions, the `balanced_accuracy_score` value for classification predictions, and the `bounded_ts_accuracy` value for time series predictions. The values vary between 0 and 1, where 1 indicates a perfect predictor, based on results obtained for a held out portion of data.      |\nExample\nLet's look at an example using the `home_rentals_model` model.\n`sql\nDESCRIBE MODEL mindsdb.home_rentals_model.model;`\nOn execution, we get:\n`sql\n+------------+--------------------+----------------------+----------+---------------------+\n| name       | performance        | training_time        | selected | accuracy_functions  |\n+------------+--------------------+----------------------+----------+---------------------+\n| Neural     | 0.9861694189913056 | 3.1538941860198975   | 0        | ['r2_score']        |\n| LightGBM   | 0.9991920992432087 | 15.671080827713013   | 1        | ['r2_score']        |\n| Regression | 0.9983390488042778 | 0.016761064529418945 | 0        | ['r2_score']        |\n+------------+--------------------+----------------------+----------+---------------------+`\nThe value of the `accuracy_functions` column is stored in the `performance` column. For example, the `r2_score` value of the `Neural` model is `0.9861694189913056`.\n`DESCRIBE MODEL ... ENSEMBLE` Statement\nDescription\nThe `DESCRIBE MODEL mindsdb.predictor_name.ensemble` statement displays the parameters used to select the best candidate model.\nSyntax\nHere is the syntax:\n`sql\nDESCRIBE MODEL mindsdb.predictor_name.ensemble;`\nOn execution, we get:\n`sql\n+-----------------+\n| ensemble        |\n+-----------------+\n| {JSON}          |\n+-----------------+`\nWhere:\n| Name                  | Description                                                                                |\n| --------------------- | ------------------------------------------------------------------------------------------ |\n| `predictor_name`      | Name of the model to be described.                                                         |\n| `ensemble`            | Object of the JSON type describing the parameters used to select the best candidate model. |\nExample\nLet's look at an example using the `home_rentals_model` model.\n`sql\nDESCRIBE MODEL mindsdb.home_rentals_model.ensemble;`\nOn execution, we get:\n`sql\n+----------------------------------------------------------------------+\n| ensemble                                                             |\n+----------------------------------------------------------------------+\n| {                                                                    |\n| \"encoders\": {                                                        |\n|   \"rental_price\": {                                                  |\n|     \"module\": \"NumericEncoder\",                                      |\n|     \"args\": {                                                        |\n|       \"is_target\": \"True\",                                           |\n|       \"positive_domain\": \"$statistical_analysis.positive_domain\"     |\n|     }                                                                |\n|   },                                                                 |\n|   \"number_of_rooms\": {                                               |\n|     \"module\": \"OneHotEncoder\",                                       |\n|     \"args\": {}                                                       |\n|   },                                                                 |\n|   \"number_of_bathrooms\": {                                           |\n|     \"module\": \"BinaryEncoder\",                                       |\n|     \"args\": {}                                                       |\n|   },                                                                 |\n|   \"sqft\": {                                                          |\n|     \"module\": \"NumericEncoder\",                                      |\n|     \"args\": {}                                                       |\n|   },                                                                 |\n|   \"location\": {                                                      |\n|     \"module\": \"OneHotEncoder\",                                       |\n|     \"args\": {}                                                       |\n|   },                                                                 |\n|   \"days_on_market\": {                                                |\n|     \"module\": \"NumericEncoder\",                                      |\n|     \"args\": {}                                                       |\n|   },                                                                 |\n|   \"neighborhood\": {                                                  |\n|     \"module\": \"OneHotEncoder\",                                       |\n|     \"args\": {}                                                       |\n|   }                                                                  |\n| },                                                                   |\n| \"dtype_dict\": {                                                      |\n|   \"number_of_rooms\": \"categorical\",                                  |\n|   \"number_of_bathrooms\": \"binary\",                                   |\n|   \"sqft\": \"integer\",                                                 |\n|   \"location\": \"categorical\",                                         |\n|   \"days_on_market\": \"integer\",                                       |\n|   \"neighborhood\": \"categorical\",                                     |\n|   \"rental_price\": \"integer\"                                          |\n| },                                                                   |\n| \"dependency_dict\": {},                                               |\n| \"model\": {                                                           |\n|   \"module\": \"BestOf\",                                                |\n|   \"args\": {                                                          |\n|     \"submodels\": [                                                   |\n|       {                                                              |\n|         \"module\": \"Neural\",                                          |\n|         \"args\": {                                                    |\n|           \"fit_on_dev\": true,                                        |\n|           \"stop_after\": \"$problem_definition.seconds_per_mixer\",     |\n|           \"search_hyperparameters\": true                             |\n|         }                                                            |\n|       },                                                             |\n|       {                                                              |\n|         \"module\": \"LightGBM\",                                        |\n|         \"args\": {                                                    |\n|           \"stop_after\": \"$problem_definition.seconds_per_mixer\",     |\n|           \"fit_on_dev\": true                                         |\n|         }                                                            |\n|       },                                                             |\n|       {                                                              |\n|         \"module\": \"Regression\",                                      |\n|         \"args\": {                                                    |\n|           \"stop_after\": \"$problem_definition.seconds_per_mixer\"      |\n|         }                                                            |\n|       }                                                              |\n|     ],                                                               |\n|     \"args\": \"$pred_args\",                                            |\n|     \"accuracy_functions\": \"$accuracy_functions\",                     |\n|     \"ts_analysis\": null                                              |\n|   }                                                                  |\n| },                                                                   |\n| \"problem_definition\": {                                              |\n|   \"target\": \"rental_price\",                                          |\n|   \"pct_invalid\": 2,                                                  |\n|   \"unbias_target\": true,                                             |\n|   \"seconds_per_mixer\": 57024.0,                                      |\n|   \"seconds_per_encoder\": null,                                       |\n|   \"expected_additional_time\": 8.687719106674194,                     |\n|   \"time_aim\": 259200,                                                |\n|   \"target_weights\": null,                                            |\n|   \"positive_domain\": false,                                          |\n|   \"timeseries_settings\": {                                           |\n|     \"is_timeseries\": false,                                          |\n|     \"order_by\": null,                                                |\n|     \"window\": null,                                                  |\n|     \"group_by\": null,                                                |\n|     \"use_previous_target\": true,                                     |\n|     \"horizon\": null,                                                 |\n|     \"historical_columns\": null,                                      |\n|     \"target_type\": \"\",                                               |\n|     \"allow_incomplete_history\": true,                                |\n|     \"eval_cold_start\": true,                                         |\n|     \"interval_periods\": []                                           |\n|   },                                                                 |\n|   \"anomaly_detection\": false,                                        |\n|   \"use_default_analysis\": true,                                      |\n|   \"ignore_features\": [],                                             |\n|   \"fit_on_all\": true,                                                |\n|   \"strict_mode\": true,                                               |\n|   \"seed_nr\": 420                                                     |\n| },                                                                   |\n| \"identifiers\": {},                                                   |\n| \"accuracy_functions\": [                                              |\n|   \"r2_score\"                                                         |\n| ]                                                                    |\n|}                                                                     |\n+----------------------------------------------------------------------+`\n\nNeed More Info?\n    If you need more information on how to `DESCRIBE MODEL` or understand the results, feel free to ask us on the community Slack workspace.",
    "tag": "mindsdb"
  },
  {
    "title": "Description",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/api/select-files.mdx",
    "content": "\ntitle: SELECT FROM files Statement\nsidebarTitle: SELECT FROM files\n\nDescription\nThe `SELECT * FROM files.file_name` statement is used to select data from a file.\nFirst, you upload a file to the MindsDB Cloud Editor by following\nthis guide. And then, you can\nCREATE MODEL from the uploaded file.\nSyntax\nHere is the syntax:\n`sql\nSELECT *\nFROM files.file_name;`\nOn execution, we get:\n`sql\n+--------+--------+--------+--------+\n| column | column | column | column |\n+--------+--------+--------+--------+\n| value  | value  | value  | value  |\n+--------+--------+--------+--------+`\nWhere:\n| Name          | Description                                                                                         |\n| ------------- | --------------------------------------------------------------------------------------------------- |\n| `file_name`   | Name of the file uploaded to the MindsDB Cloud Editor by following this guide. |\n| `column`      | Name of the column from the file.                                                                   |\nExample\nOnce you uploaded your file by following this guide, you\ncan query it like a table.\n`sql\nSELECT *\nFROM files.home_rentals\nLIMIT 10;`\nOn execution, we get:\n`sql\n+-----------------+---------------------+-------+----------+----------------+---------------+--------------+--------------+\n| number_of_rooms | number_of_bathrooms | sqft  | location | days_on_market | initial_price | neighborhood | rental_price |\n+-----------------+---------------------+-------+----------+----------------+---------------+--------------+--------------+\n| 0               | 1                   | 484,8 | great    | 10             | 2271          | south_side   | 2271         |\n| 1               | 1                   | 674   | good     | 1              | 2167          | downtown     | 2167         |\n| 1               | 1                   | 554   | poor     | 19             | 1883          | westbrae     | 1883         |\n| 0               | 1                   | 529   | great    | 3              | 2431          | south_side   | 2431         |\n| 3               | 2                   | 1219  | great    | 3              | 5510          | south_side   | 5510         |\n| 1               | 1                   | 398   | great    | 11             | 2272          | south_side   | 2272         |\n| 3               | 2                   | 1190  | poor     | 58             | 4463          | westbrae     | 4123.812     |\n| 1               | 1                   | 730   | good     | 0              | 2224          | downtown     | 2224         |\n| 0               | 1                   | 298   | great    | 9              | 2104          | south_side   | 2104         |\n| 2               | 1                   | 878   | great    | 8              | 3861          | south_side   | 3861         |\n+-----------------+---------------------+-------+----------+----------------+---------------+--------------+--------------+`\nNow let's create a predictor using the uploaded file. You can learn more about\nthe CREATE MODEL command here.\n`sql\nCREATE MODEL mindsdb.home_rentals_model\nFROM files\n    (SELECT * from home_rentals)\nPREDICT rental_price;`\nOn execution, we get:\n```sql\nQuery OK, 0 rows affected (x.xxx sec)",
    "tag": "mindsdb"
  },
  {
    "title": "Description",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/api/select.mdx",
    "content": "\ntitle: SELECT Statement\nsidebarTitle: SELECT\n\nDescription\nThe `SELECT` statement fetches predictions from the model table. The data is returned on the fly and the result set is not persisted.\nBut there are ways to save predictions data! You can save your predictions as a view using the CREATE VIEW statement. Please note that a view is a saved query and does not store data like a table. Another way is to insert your predictions into a table using the INSERT INTO statement.\nSyntax\nSingle Prediction\nHere is the syntax for fetching a single prediction from the model table:\n`sql\nSELECT target_name, target_name_explain\nFROM mindsdb.predictor_name\nWHERE column_name = value \nAND column_name = value;`\n\nGrammar Matters\n\n\n```Here are some points to keep in mind while writing queries in MindsDB:<br/>\n&nbsp;&nbsp;&nbsp;1. The `column_name = value` pairs may be joined by `AND` or `OR` keywords.<br/>\n&nbsp;&nbsp;&nbsp;2. Do not use any quotations for numerical values.<br/>\n&nbsp;&nbsp;&nbsp;3. Use single quotes for strings.\n&nbsp;&nbsp;&nbsp;4. The tables and column names are case sensitive.\n```\n\n\n\nOn execution, we get:\n`sql\n+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+\n| target_name       | target_name_explain                                                                                                                           |\n+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+\n| predicted_value   | {\"predicted_value\": 4394, \"confidence\": 0.99, \"anomaly\": null, \"truth\": null, \"confidence_lower_bound\": 4313, \"confidence_upper_bound\": 4475} |\n+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+`\nWhere:\n| Name                                     | Description                                                                                                                                                                          |\n| ---------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| `target_name`                            | Name of the column to be predicted.                                                                                                                                                  |\n| `target_name_explain`                    | Object of the JSON type that contains the `predicted_value` and additional information such as `confidence`, `anomaly`, `truth`, `confidence_lower_bound`, `confidence_upper_bound`. |\n| `predictor_name`                         | Name of the model used to make the prediction.                                                                                                                                       |\n| `WHERE column_name = value AND ...`      | `WHERE` clause used to pass the data values for which the prediction is made.                                                                                                        |\nBulk Predictions\nHere is the syntax for making predictions in bulk by joining the data source table with the model table:\n`sql\nSELECT m.target_name, t.column_name, ...\nFROM integration_name.table_name AS t\nJOIN mindsdb.predictor_name AS m;`\nOn execution, we get:\n`sql\n+----------------------+-------------+\n| target_name          | column_name |\n+----------------------+-------------+\n| predicted_value1     | value1      |\n| predicted_value2     | value2      |\n| predicted_value3     | value3      |\n| predicted_value4     | value4      |\n| predicted_value5     | value5      |\n+----------------------+-------------+`\nWhere:\n| Name                                   | Description                                                                                                                         |\n| -------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------|\n| `m.target_name`                        | Name of the column to be predicted. The `m.` in front indicates that this column comes from the `mindsdb.predictor_name` table.     |\n| `t.column_name, ...`                   | Columns from the data source table (`integration_name.table_name`) that you want to see in the output.                              |\n| `integration_name.table_name`          | Data source table that is joined with the model table (`mindsdb.predictor_name`).                                                   |\n| `predictor_name`                       | Name of the model used to make predictions.                                                                                         |\nPlease note that in the case of bulk predictions, we do not pass the data values for which the prediction is made. It is because bulk predictions use all data available in the data source table.\nExample\nSingle Prediction\nLet's predict the `rental_price` value using the `home_rentals_model` model for the property having `sqft=823`, `location='good'`, `neighborhood='downtown'`, and `days_on_market=10`.\n`sql\nSELECT sqft, location, neighborhood, days_on_market, rental_price, rental_price_explain\nFROM mindsdb.home_rentals_model1\nWHERE sqft=823\nAND location='good'\nAND neighborhood='downtown'\nAND days_on_market=10;`\nOn execution, we get:\n`sql\n+-------+----------+--------------+----------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------+\n| sqft  | location | neighborhood | days_on_market | rental_price | rental_price_explain                                                                                                                          |\n+-------+----------+--------------+----------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------+\n| 823   | good     | downtown     | 10             | 4394         | {\"predicted_value\": 4394, \"confidence\": 0.99, \"anomaly\": null, \"truth\": null, \"confidence_lower_bound\": 4313, \"confidence_upper_bound\": 4475} |\n+-------+----------+--------------+----------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------+`\nBulk Predictions\nNow let's make bulk predictions to predict the `rental_price` value using the `home_rentals_model` model joined with the data source table.\n`sql\nSELECT t.sqft, t.location, t.neighborhood, t.days_on_market, t.rental_price AS real_price,\n       m.rental_price AS predicted_rental_price\nFROM example_db.demo_data.home_rentals AS t\nJOIN mindsdb.home_rentals_model AS m\nLIMIT 5;`\nOn execution, we get:\n`sql\n+-------+----------+-----------------+----------------+--------------+-----------------------------+\n| sqft  | location | neighborhood    | days_on_market | real_price   | predicted_rental_price      |\n+-------+----------+-----------------+----------------+--------------+-----------------------------+\n| 917   | great    | berkeley_hills  | 13             | 3901         | 3886                        |\n| 194   | great    | berkeley_hills  | 10             | 2042         | 2007                        |\n| 543   | poor     | westbrae        | 18             | 1871         | 1865                        |\n| 503   | good     | downtown        | 10             | 3026         | 3020                        |\n| 1066  | good     | thowsand_oaks   | 13             | 4774         | 4748                        |\n+-------+----------+-----------------+----------------+--------------+-----------------------------+`\nSelect from integration\nSimple select\nIn this example query contains only tables from one integration \nand therefore will be sent to integration database\n(integration name will be cut from table name)\n`sql\nSELECT location, max(sqft)\nFROM example_db.demo_data.home_rentals \nGROUP BY location\nLIMIT 5;`\nRaw select from integration\nIt is also possible to send raw query to integration. \nIt can be useful when query to integration can not be parsed as sql\nSyntax:\n`sql\nSELECT ... FROM integration_name ( query goes here )`\nExample of select from mongo integration using mongo query\n`sql\nSELECT * FROM mongo (\n db.house_sales2.find().limit(1) \n)`\nComplex queries\n\nSubselect on data from integration.\n\nIt can be useful in cases when integration engine doesn't support some functions, for example grouping.\nIn that case all data from raw select are passed to mindsdb and then subselect performs on them inside mindsdb\n`sql\nSELECT type, max(bedrooms), last(MA)\nFROM mongo (\n db.house_sales2.find().limit(300) \n) GROUP BY 1`\n\nUnions\n\nIt is possible to use `UNION` / `UNION ALL` operators.\nIt this case every subselect from union will be fetched and merged to one result-set on mindsdb side  \n```sql\n SELECT \n  data.time as date, data.target\n FROM datasource.table_name as data\nUNION ALL\n SELECT\n  model.time as date, model.target as target\n FROM mindsdb.model as model \n  JOIN datasource.table_name as t\n WHERE t.time > LATEST AND t.group = 'value';",
    "tag": "mindsdb"
  },
  {
    "title": "Description",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/api/adjust.mdx",
    "content": "\ntitle: ADJUST Statement\nsidebarTitle: ADJUST\n\nDescription\nThe `ADJUST` statement lets you retrain a model with additional training data.\nImagine you have a model that was trained with a certain dataset. Now there is more training data available and you wish to retrain this model with a new dataset. The `ADJUST` statement lets you partially retrain the model, so it takes less time and resources than the RETRAIN statement. In the machine learning literature, this is also referred to as fine-tuning a model.\nSyntax\nHere is the syntax:\n`sql\nADJUST project_name.model_name\nFROM integration_name\n    (SELECT column_name, ... FROM table_name)\n[USING\n    key = value,\n    ...];`\nWhere:\n| Expressions                                 | Description                                                                                                        |\n| ------------------------------------------- | ------------------------------------------------------------------------------------------------------------------ |\n| `project_name`                              | Name of the project where the model resides.                                                                       |\n| `model_name`                                | Name of the model to be retrained.                                                                                 |\n| `integration_name`                          | Name of the integration created using the CREATE DATABASE statement or file upload.    |\n| `(SELECT column_name, ... FROM table_name)` | Selecting additional data to be used for retraining.                                                               |\n| `USING key = value`                         | Optional. The `USING` clause lets you pass multiple parameters to the `ADJUST` statement.                          |\n\nModel Versions\nEvery time the model is retrained/adjusted, its new version is created with an incremented version number. Unless overridden, the most recent version becomes active when training completes.\nYou can query for all model versions like this:\n`sql\nSELECT *\nFROM project_name.models_versions;`\nFor more information on managing model versions, check out our docs here.\n\nExample 1: Regression Model\nIn this example, we use our sample PostgreSQL database. You can connect to it like this:\n`sql\nCREATE DATABASE example_db\nWITH ENGINE = \"postgres\",\nPARAMETERS = {\n    \"user\": \"demo_user\",\n    \"password\": \"demo_password\",\n    \"host\": \"3.220.66.106\",\n    \"port\": \"5432\",\n    \"database\": \"demo\"\n    };`\nFirst, we create and train the model using a subset of the `home_rentals` data, considering properties that have been on the market less than 10 days.\n`sql\nCREATE MODEL mindsdb.adjust_home_rentals_model\nFROM example_db\n    (SELECT * \n    FROM demo_data.home_rentals \n    WHERE days_on_market < 10)\nPREDICT rental_price;`\nOn execution, we get:\n`sql\nQuery successfully completed`\nWe can check its status using this command:\n`sql\nSELECT *\nFROM mindsdb.models\nWHERE name = 'adjust_home_rentals_model';`\nOnce the status is complete, we can query for predictions.\n`sql\nSELECT rental_price, rental_price_explain \nFROM mindsdb.adjust_home_rentals_model\nWHERE sqft = 1000\nAND location = 'great'\nAND neighborhood = 'berkeley_hills'\nAND number_of_rooms = 2\nAND number_of_bathrooms = 1\nAND days_on_market = 40;`\nOn execution, we get:\n`sql\n+---------------+-----------------------------------------------------------------------------------------------------------------------------------------------+\n| rental_price  | rental_price_explain                                                                                                                          |\n+---------------+-----------------------------------------------------------------------------------------------------------------------------------------------+\n| 2621          | {\"predicted_value\": 2621, \"confidence\": 0.99, \"anomaly\": null, \"truth\": null, \"confidence_lower_bound\": 2523, \"confidence_upper_bound\": 2719} |\n+---------------+-----------------------------------------------------------------------------------------------------------------------------------------------+`\nLet\u2019s adjust this model with more training data. Now we consider properties that have been on the market for 10 or more days.\n`sql\nADJUST mindsdb.adjust_home_rentals_model\nFROM example_db\n    (SELECT * \n    FROM demo_data.home_rentals \n    WHERE days_on_market >= 10);`\nOn execution, we get:\n`sql\nQuery successfully completed`\nTo check the status and versions of the model, run this command:\n`sql\nSELECT name, engine, project, active, version, status\nFROM mindsdb.models_versions\nWHERE name = 'adjust_home_rentals_model';`\nOn execution, we get:\n`sql\n+---------------------------+-----------+---------+--------+---------+----------+\n| name                      | engine    | project | active | version | status   |\n+---------------------------+-----------+---------+--------+---------+----------+\n| adjust_home_rentals_model | lightwood | mindsdb | false  | 1       | complete |\n| adjust_home_rentals_model | lightwood | mindsdb | true   | 2       | complete |\n+---------------------------+-----------+---------+--------+---------+----------+`\nPlease note that the longer the property is on the market, the lower its rental price. Hence, we can expect the `rental_price` prediction to be lower.\n`sql\nSELECT rental_price, rental_price_explain \nFROM mindsdb.adjust_home_rentals_model\nWHERE sqft = 1000\nAND location = 'great'\nAND neighborhood = 'berkeley_hills'\nAND number_of_rooms = 2\nAND number_of_bathrooms = 1\nAND days_on_market = 40;`\nOn execution, we get:\n`sql\n+---------------+-----------------------------------------------------------------------------------------------------------------------------------------------+\n| rental_price  | rental_price_explain                                                                                                                          |\n+---------------+-----------------------------------------------------------------------------------------------------------------------------------------------+\n| 2055          | {\"predicted_value\": 2055, \"confidence\": 0.99, \"anomaly\": null, \"truth\": null, \"confidence_lower_bound\": 1957, \"confidence_upper_bound\": 2153} |\n+---------------+-----------------------------------------------------------------------------------------------------------------------------------------------+`\nExample 2: Classification Model\nIn this example, we again use our sample PostgreSQL database.\nFirst, we create and train the model using a subset of the `customer_churn` data, considering only female customers.\n`sql\nCREATE MODEL mindsdb.adjust_customer_churn_model\nFROM example_db\n    (SELECT *\n    FROM demo_data.customer_churn\n    WHERE gender = 'Female')\nPREDICT churn;`\nOn execution, we get:\n`sql\nQuery successfully completed`\nWe can check its status using this command:\n`sql\nSELECT *\nFROM mindsdb.models\nWHERE name = 'adjust_customer_churn_model';`\nOnce the status is complete, we can query for predictions.\n`sql\nSELECT churn, churn_explain\nFROM mindsdb.adjust_customer_churn_model\nWHERE seniorcitizen = 0\nAND partner = 'Yes'\nAND dependents = 'No'\nAND tenure = 1\nAND phoneservice = 'No'\nAND multiplelines = 'No phone service'\nAND internetservice = 'DSL';`\nOn execution, we get:\n`sql\n+--------+------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| churn  | churn_explain                                                                                                                                              |\n+--------+------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| No     | {\"predicted_value\": \"No\", \"confidence\": 0.9887640449438202, \"anomaly\": null, \"truth\": null, \"probability_class_No\": 0.934, \"probability_class_Yes\": 0.066} |\n+--------+------------------------------------------------------------------------------------------------------------------------------------------------------------+`\nLet\u2019s adjust this model with more training data. Now we also consider male customers.\n`sql\nADJUST mindsdb.adjust_customer_churn_model\nFROM example_db\n    (SELECT *\n    FROM demo_data.customer_churn\n    WHERE gender = 'Male');`\nOn execution, we get:\n`sql\nQuery successfully completed`\nTo check the status and versions of the model, run this command:\n`sql\nSELECT name, engine, project, active, version, status\nFROM mindsdb.models_versions\nWHERE name = 'adjust_customer_churn_model';`\nOn execution, we get:\n`sql\n+-----------------------------+-----------+---------+--------+---------+----------+\n| name                        | engine    | project | active | version | status   |\n+-----------------------------+-----------+---------+--------+---------+----------+\n| adjust_customer_churn_model | lightwood | mindsdb | false  | 1       | complete |\n| adjust_customer_churn_model | lightwood | mindsdb | true   | 2       | complete |\n+-----------------------------+-----------+---------+--------+---------+----------+`\nLet\u2019s query for a prediction again.\n`sql\nSELECT churn, churn_explain\nFROM mindsdb.adjust_customer_churn_model\nWHERE seniorcitizen = 0\nAND partner = 'Yes'\nAND dependents = 'No'\nAND tenure = 1\nAND phoneservice = 'No'\nAND multiplelines = 'No phone service'\nAND internetservice = 'DSL';`\nOn execution, we get:\n`sql\n+--------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| churn  | churn_explain                                                                                                                                                |\n+--------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| No     | {\"predicted_value\": \"No\", \"confidence\": 0.9887640449438202, \"anomaly\": null, \"truth\": null, \"probability_class_No\": 0.9294, \"probability_class_Yes\": 0.0706} |\n+--------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+`",
    "tag": "mindsdb"
  },
  {
    "title": "Description",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/api/retrain.mdx",
    "content": "\ntitle: RETRAIN Statement\nsidebarTitle: RETRAIN\n\nDescription\nThe `RETRAIN` statement is used to retrain the already trained predictors with the new data. The predictor is updated to leverage the new data in optimizing its predictive capabilities.\nRetraining takes at least as much time as the training process of the predictor did because now the dataset used to retrain has new or updated data.\nSyntax\nHere is the syntax:\n`sql\nRETRAIN project_name.predictor_name\n[FROM integration_name\n    (SELECT column_name, ... FROM table_name)\nPREDICT target_name\nUSING engine = 'engine_name',\n      tag = 'tag_name',\n      active = 0/1];`\nOn execution, we get:\n`sql\nQuery OK, 0 rows affected (0.058 sec)`\nWhere:\n| Expressions                                     | Description                                                                                                                                               |\n| ----------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `project_name`                                  | Name of the project where the model resides.                                                                                                              |\n| `predictor_name`                                | Name of the model to be retrained.                                                                                                                        |\n| `integration_name`                              | Optional. Name of the integration created using the CREATE DATABASE statement or file upload. |\n| `(SELECT column_name, ... FROM table_name)`     | Optional. Selecting data to be used for training and validation.                                                                                          |\n| `target_column`                                 | Optional. Column to be predicted.                                                                                                                         |\n| `engine_name`                                   | You can optionally provide an ML engine, based on which the model is retrained.                                                                           |\n| `tag_name`                                      | You can optionally provide a tag that is visible in the `training_options` column of the `mindsdb.models` table.                                          |\n| `active`                                        | Optional. Setting it to `0` causes the retrained version to be inactive. And setting it to `1` causes the retrained version to be active.                 |\n\nModel Versions\n    Every time the model is retrained, its new version is created with the incremented version number.\n\n\n```You can query for all model versions like this:\n\n```sql\nSELECT *\nFROM project_name.models_versions;\n```\n\nFor more information on managing model versions, check out our [docs here](/sql/api/manage-models-versions/).\n```\n\n\n\nWhen to `RETRAIN` the Model?\nIt is advised to `RETRAIN` the predictor whenever the `update_status` column value from the `mindsdb.models` table is set to `available`.\nHere is when the `update_status` column value is set to `available`:\n\nWhen the new version of MindsDB is available that causes the predictor to become obsolete.\nWhen the new data is available in the table that was used to train the predictor.\n\nTo find out whether you need to retrain your model, query the `mindsdb.models` table and look for the `update_status` column.\nHere are the possible values of the `update_status` column:\n| Name          | Description                                                                          |\n| ------------- | ------------------------------------------------------------------------------------ |\n| `available`   | It indicates that the model should be updated.                                       |\n| `updating`    | It indicates that the retraining process of the model takes place.                   |\n| `up_to_date`  | It indicates that your model is up to date and does not need to be retrained.        |\nLet's run the query.\n`sql\nSELECT name, update_status\nFROM mindsdb.models\nWHERE name = 'predictor_name';`\nOn execution, we get:\n`sql\n+------------------+---------------+\n| name             | update_status |\n+------------------+---------------+\n| predictor_name   | up_to_date    |\n+------------------+---------------+`\nWhere:\n| Name               | Description                                                  |\n| ------------------ | ------------------------------------------------------------ |\n| `predictor_name`   | Name of the model to be retrained.                           |\n| `update_status`    | Column informing whether the model needs to be retrained.    |\nExample\nLet's look at an example using the `home_rentals_model` model.\nFirst, we check the status of the predictor.\n`sql\nSELECT name, update_status\nFROM mindsdb.models\nWHERE name = 'home_rentals_model';`\nOn execution, we get:\n`sql\n+--------------------+---------------+\n| name               | update_status |\n+--------------------+---------------+\n| home_rentals_model | available     |\n+--------------------+---------------+`\nThe `available` value of the `update_status` column informs us that we should retrain the model.\n`sql\nRETRAIN mindsdb.home_rentals_model;`\nOn execution, we get:\n`sql\nQuery OK, 0 rows affected (0.058 sec)`\nNow, let's check the status again.\n`sql\nSELECT name, update_status\nFROM mindsdb.models\nWHERE name = 'home_rentals_model';`\nOn execution, we get:\n`sql\n+--------------------+---------------+\n| name               | update_status |\n+--------------------+---------------+\n| home_rentals_model | updating      |\n+--------------------+---------------+`\nAnd after the retraining process is completed:\n`sql\nSELECT name, update_status\nFROM mindsdb.models\nWHERE name = 'home_rentals_model';`\nOn execution, we get:\n```sql\n+--------------------+---------------+\n| name               | update_status |\n+--------------------+---------------+\n| home_rentals_model | up_to_date    |\n+--------------------+---------------+",
    "tag": "mindsdb"
  },
  {
    "title": "Description",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/api/drop.mdx",
    "content": "\ntitle: DROP MODEL Statement\nsidebarTitle: DROP MODEL\n\nDescription\nThe `DROP MODEL` statement deletes the model table.\nSyntax\nHere is the syntax:\n`sql\nDROP MODEL predictor_name;`\nOn execution, we get:\n`sql\nQuery OK, 0 rows affected (0.058 sec)`\nWhere:\n| Name               | Description                      |\n| ------------------ | -------------------------------- |\n| `predictor_name`   | Name of the model to be deleted. |\nExample\nLet's list all the available predictor tables.\n`sql\nSELECT name\nFROM mindsdb.models;`\nOn execution, we get:\n`sql\n+---------------------+\n| name                |\n+---------------------+\n| other_model         |\n| home_rentals_model  |\n+---------------------+`\nNow we delete the `home_rentals_model` table.\n`sql\nDROP MODEL home_rentals_model;`\nOn execution, we get:\n`sql\nQuery OK, 0 rows affected (0.058 sec)`\nWe can check if the deletion was successful by querying the `mindsdb.models`\ntable again.\n`sql\nSELECT name\nFROM mindsdb.models;`\nOn execution, we get:\n```sql\n+---------------------+\n| name                |\n+---------------------+\n| other_model         |\n+---------------------+",
    "tag": "mindsdb"
  },
  {
    "title": "Filtering Spam Emails",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/tutorials/spam_emails_tutorial.mdx",
    "content": "\ntitle: Email Spam\nCommuntiy Author: PWiederspan\nPre-requisites\nThis tutorial will be easier to follow if you have first read:\n\nGetting Started Guide\nGetting Started with Cloud\n\nAnd have downloaded this dataset.\nFiltering Spam Emails\nAnyone with an email address has experienced it, you open your inbox to find a random email promising a cash reward if you click now. Or maybe it\u2019s sneakier, a tracking link for a package you don\u2019t remember ordering. Spam emails are annoying for anyone, they clutter your inbox and send unnecessary push notifications, but they can be security risks as well. Whether a malicious tracking link in an official looking email, or a phishing scam asking you to confirm credentials, Spam emails are a serious concern for any IT department. Luckily, modern email clients have robust spam filters which remove potentially harmful emails from the inbox and label them as spam. Many major email providers use Machine Learning to analyze incoming emails and determine whether or not they are spam. Let's take a look at how MindsDB can be used to do this from a dataset of about 4600 emails.\nUpload the Dataset File\nIf you are planning on using MindsDB to make predictions based on your own dataset, you probably have that data in a database, in which case you would want to follow the steps described in the Getting Started Guide, mentioned in the prerequisite section, to connect MindsDB to your database.\nFor this tutorial we will be using the MindsDB cloud editor. If you haven\u2019t familiarized yourself with it, now is a great time to do so.\nFrom the main page in the editor, select the \u201cUpload File\u201d button in the top right corner. Then from the new page, browse local files and select the correct Spam Emails CSV, name it appropriately (in this tutorial we use spam_predict), and select the \u201cSave and Continue\u201d button.\nWhen the upload is complete it should take you back to the main editor page with something like this in the query box:\n```sql\n--- MindsDB ships with a filesystem database called 'files'\n--- Each file you uploaded is saved as a table there.\n\n--- You can always check the list tables in files as follows:\nSHOW TABLES FROM files;\n```\nSelecting the \u201cRun\u201d button in the top left corner, or pressing Shift+Enter, will run the query and you should see a list of all files uploaded in the bottom section.\nCreate A Predictor\nFor this particular dataset each row represents a single email, with each column representing the frequency of a given word or symbol in that email. There are also columns for the total, average, and longest run of capital letters throughout the email.\nFirst we need to switch to using the mindsdb database.\n`sql\nUSE mindsdb;`\nWe will start by training a model with the CREATE PREDICTOR command using all columns from the dataset we uploaded.\n`sql\nCREATE PREDICTOR mindsdb.spam_predictor\nFROM files\n    (SELECT * FROM spam_predict)\nPREDICT Spam;`\nWe can check the status of the model by querying the name we used as the predictor. As this is a larger dataset, the status may show \u201cGenerating\u201d or \u201cTraining\u201d for a few minutes. Re-run the query to refresh.\n`sql\nSELECT *\nFROM mindsdb.predictors\nWHERE name='spam_predictor';`\nWhen the model has finished training the status will change to \"complete\".\nOn execution, we get:\n`sql\n+----------------+----------+--------------------+-------------+------------------+------------------+-------+--------------------+------------------+\n| name           | status   | accuracy           | predict     | update_status    | mindsdb_version  | error | select_data_query  | training_options |\n+----------------+----------+--------------------+-------------+------------------+------------------+-------+--------------------+------------------+\n| spam_predictor | complete | 0.9580387730450108 | Spam        | up_to_date       | 22.4.2.1         | null  |                    |                  |\n+----------------+----------+--------------------+-------------+------------------+------------------+-------+--------------------+------------------+`\nYou have just created and trained the Machine Learning Model with approximately 96% accuracy!\nMake Predictions\nWe will now begin to make predictions about whether a given email is spam using the model we just trained.\nThe generic syntax to make predictions is:\n`sql\nSELECT t.column_name1, t.column_name2\nFROM integration_name.table AS t\nJOIN mindsdb.predictor_name AS p\nWHERE t.column_name IN (value1, value2, ...);`\nFor our prediction we will query the spam, spam_confidence, and spam_explain columns from the spam_predictor model we trained. To simulate a test email we will provide values for several in the WHERE clause representing various word frequencies and capital letter runs.\n`sql\nSELECT spam, spam_confidence, spam_explain\nFROM mindsdb.spam_predictor\nWHERE word_freq_internet=2.5\nAND word_freq_email=2\nAND word_freq_credit=0.9\nAND word_freq_money=0.9\nAND word_freq_report=1.2\nAND word_freq_free=1.2\nAND word_freq_your=0.9\nAND word_freq_all=2.4\nAND capital_run_length_average = 20\nAND word_freq_mail=1\nAND capital_run_total=20\nAND capital_run_longest=20;`\nOn execution, we get:\n`sql\n+-------+-------------------+-------------------------------------------------------------------------------------------+\n| spam  | spam_confidence   | spam_explain                                                                              |\n+-------+-------------------+-------------------------------------------------------------------------------------------+\n| 1     | 0.956989247311828 | {\"predicted_value\": \"1\", \"confidence\": 0.956989247311828, \"anomaly\": null, \"truth\": null} |\n+-------+-------------------+-------------------------------------------------------------------------------------------+`\nWith this output we can see that MindsDB predicts that this is a spam email with approximately 96% accuracy.\nYou can change these values and add additional word frequency columns to the query as you see fit to get different predictions.\nBatch Predictions\nIn the example above, we saw how we can use MindsDB to determine if a single email is spam, however, in a lot of cases we want to determine if any email in an inbox is spam. We can do batch predictions using the JOIN command. By joining our spam_predictor table on a query of our dataset, we can see MindsDB prediction for each row alongside the actual spam value for each email.\nThe generic syntax to do this is:\n`sql\nSELECT t.column_name1, t.column_name2\nFROM integration_name.table AS t\nJOIN mindsdb.predictor_name AS p\nWHERE t.column_name IN (value1, value2, ...);`\nAs we are using the original dataset, which contains 58 columns, we will only include a few of them. The important column for the purposes of this tutorial is the \u201cspam\u201d column for both tables, which we can use to check the predictions accuracy.\n`sql\nSELECT t.word_freq_internet, t.word_freq_email, t.word_freq_credit,\n       t.word_freq_money, t.word_freq_report, t.capital_run_length_longest,\n       t.spam, p.spam AS predicted_spam\nFROM files.spam_predict AS t\nJOIN mindsdb.spam_predictor AS p;`\nOn execution, we get:\n`sql\n+--------------------+-----------------+------------------+-----------------+------------------+---------------------------+-------+----------------+\n| word_freq_internet | word_freq_email | word_freq_credit | word_freq_money | word_freq_report | capital_run_length_longest| spam  | spam_predictor |\n+--------------------+-----------------+------------------+-----------------+------------------+---------------------------+-------+----------------+\n| 0                  | 0               | 0                | 0               | 0                | 9989                      | 1     | 1              |\n| 0                  | 0               | 0                | 0               | 0                | 99                        | 1     | 1              |\n| 0                  | 0.21            | 0                | 0               | 0                | 99                        | 1     | 1              |\n| 0                  | 0               | 0                | 0               | 0                | 99                        | 1     | 1              |\n| 0                  | 0               | 0                | 0               | 0                | 99                        | 1     | 1              |\n+--------------------+-----------------+------------------+-----------------+------------------+---------------------------+-------+----------------+`\nWith the ability to scroll through all 4600 rows, broken up 100 per page. You can now see predictions for a whole dataset!\nWhat we learned\nIn this tutorial we learned how to :\n\nUpload a dataset into MindsDB Cloud\nCreate a MindsDB Predictor\nTrain our Model using MindsDB\nMake individual predictions by providing sample data\n",
    "tag": "mindsdb"
  },
  {
    "title": "Pre-requisites",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/tutorials/heart-disease.mdx",
    "content": "\ntitle: Heart Disease\nDataset: Heart Disease Data\nCommuntiy Author: Mohd Talha\nCardiovascular disease remains the leading cause of morbidity and mortality\naccording to the\nNational Center for Health Statistics\nin the United States, and consequently, early diagnosis is of paramount\nimportance. Machine learning technology, a subfield of artificial intelligence,\nis enabling scientists, clinicians and patients to detect it in the earlier\nstages and therefore save lives.\nUntil now, building, deploying and maintaining applied machine learning\nsolutions was a complicated and expensive task, because it required skilled\npersonnel and expensive tools. But not only that. A traditional machine learning\nproject requires building integrations with data and applications, that is not\nonly a technical but also an organizational challenge. So what if machine\nlearning can become a part of the standard tools that are already in use?\nThis is exactly the problem that MindsDB is solving. It makes machine learning\neasy to use by automating and integrating it into the mainstream instruments for\ndata and analytics, namely databases and business intelligence software. It adds\nan AI \u201cbrain\u201d to databases so that they can learn automatically from existing\ndata, allowing you to generate and visualize predictions using standard data\nquery language like SQL. Lastly, MindsDB is open-source, and anyone can use it\nfor free.\nIn this article, we will show step by step how to use MindsDB inside databases\nto predict the risk of heart disease for patients.\nYou can follow this tutorial by connecting to your own database and using\ndifferent data - the same workflow applies to most machine learning use cases.\nLet's get started!\nPre-requisites\nIf you want to install MindsDB locally, check out the installation guide for\nDocker or\nPyPi and you can\nfollow this tutorial. If you are OK with using MindsDB cloud, then simply\ncreate a free account and you will be up and\nrunning in just one minute.\nSecond, you will need to have a mysql client like DBeaver, MySQL Workbench etc.\ninstalled locally to connect to the MindsDB MySQL API. MindsDB contains a SQL\nEditor which can be accessed on MindsDB cloud or the URL 127.0.0.1:47334/.\nData Overview\nFor the example of this tutorial, we will use the heart disease dataset\navailable publicly in Kaggle.\nEach row represents a patient and we will train a machine learning model to help\nus predict if the patient is classified as a heart disease patient. Below is a\nshort description of each feature inside the data.\n\nage - In Years\nsex - 1 = Male; 0 = Female\ncp - chest pain type (4 values)\ntrestbps - Resting blood pressure (in mm Hg on admission to the hospital)\nchol - Serum cholesterol in mg/dl\nfbs - Fasting blood sugar > 120 mg/dl (1 = true; 0 = false)\nrestecg - Resting electrocardiographic results\nthalach - Maximum heart rate achieved\nexang - Exercise induced angina (1 = yes; 0 = no)\noldpeak - ST depression induced by exercise relative to rest\nslope - the slope of the peak exercise ST segment\nca - Number of major vessels (0-3) colored by fluoroscopy\nthal - 1 = normal; 2 = fixed defect; 3 = reversible defect\ntarget - 1 or 0 (This is what we will predict)\n\nHow to use MindsDB\nMindsDB allows you to automatically create & train machine learning models from\nthe data in your database that you have connected to in the previous step.\nMindsDB works via MySQL wire protocol, which means you can do all these steps\nthrough SQL commands. When it comes to making predictions, SQL queries become\neven handier, because you can easily make them straight from your existing\napplications or Business Intelligence tools that already speak SQL. The ML\nmodels are available to use immediately after being trained as if they were\nvirtual database tables (a concept called \u201cAI Tables\u201d). So, let\u2019s see how it\nworks.\nConnect to your data\nFirst, we need to connect MindsDB to the database where the Heart Disease data\nis stored:\n\nAccess MindsDB GUI on either cloud or local via the URL 127.0.0.1:47334/\nOn the default page, select the button `Add Data` or alternatively select the\n  plug icon on the left sidebar.\nThe 'Select your data source' page will populate for you to choose your\n  database type. For this tutorial we will be selecting the postgres database\n  button.\n\n\n\nOnce you have selected the database type,the page will automatically navigate\n  to the SQL Editor where the syntax to create a database connection will\n  automatically populate for you to enter the required parameters.\n\nThe required parameters are:\n\nCREATE DATABASE display_name --- display name for database.\nWITH ENGINE = \"postgres\", --- name of the mindsdb handler\nPARAMETERS =\n\"user\": \" \" - Your database user.\n\"password\": \" \" - Your password.\n\"host\": \" \" - host, it can be an ip or an url.\n\"port\": \"5432\" - common port is 5432.\n\"database\": \" \" - The name of your database (Optional)\n\n\nSelect the `Run` button or Shift+Enter to execute the syntax. Once the Database\nconnection is created the console will display a message 'Query successfully\ncompleted'.\nCreate a machine learning model.\nWe can create a machine learning predictive model by using simple SQL statements\nexecuted in the SQL Editor.\nTo create and train a new machine learning model we will need to use the CREATE\nPREDICTOR statement:\n`sql\nCREATE PREDICTOR mindsdb.predictor_name\nFROM integration_name\n    (SELECT column_name, column_name2 FROM table_name)\nPREDICT column_name;`\nThe required values that we need to provide are:\n\npredictor_name (string): The name of the model\nintegration_name (string): The name of the connection to your database.\ncolumn_name (string): The feature you want to predict.\n\nTo train the model that will predict the risk of heart disease as target we will\nrun:\n`sql\nCREATE PREDICTOR patients_target\nFROM mindsdb_predictions\n    (SELECT * FROM heart_disease)\nPREDICT target;`\nSelect the `Run` button or Shift+Enter to execute the syntax. Once the machine\nlearning model is created the console will display a message 'Query successfully\ncompleted'. \nWhat we did here was to create a predictor called `patients_target`to predict\nthe presence of heart disease as `target`. The model has started training. To\ncheck if the training has finished you can SELECT the model name from the\npredictors table:\n`sql\nSELECT *\nFROM mindsdb.predictors\nWHERE name='patients_target';`\n\nThe complete status means that the model training has successfully finished.\nUsing SQL Statements to make predictions\nThe next steps would be to query the model and predict the heart disease risk.\nLet\u2019s imagine a patient. This patient\u2019s age is 30, she has a cholesterol level\nof 177 mg/dl, with slope of the peak exercise ST segment as 2, and thal as 2.\nAdd all of this information to the `WHERE` clause.\n`sql\nSELECT target AS prediction,\n       target_confidence AS confidence,\n       target_explain AS info\nFROM mindsdb.patients_target\nWHERE age=30\nAND chol=177\nAND slope=2\nAND thal=2;`\n\nWith a confidence of around 83%, MindsDB predicted a high risk of heart disease\nfor this patient.\nThe above example shows how you can make predictions for a single patient. But\nwhat if you have a table in your database with many patients\u2019 diagnosis data,\nand you want to make predictions for them in bulk? For this purpose, you can\njoin the predictor with such a table.\n`sql\nSELECT *\nFROM mindsdb_predictions.heart_disease AS t\nJOIN mindsdb.patients_target AS tb\nWHERE t.thal=2;`\n\n\n\nNow you can even connect the output table to your BI tool and for more\nconvenient visualization of the results using graphs or pivots.\nConclusion\nIn this tutorial, you have seen how easy it is to apply machine learning for\nyour predictive needs. MindsDB's innovative open-source technology is making it\neasy to leverage machine learning for people who are not experts in this field.\nHowever, MindsDB is a great tool for ML practitioners as well: if you are a\nskilled data scientist, you could also benefit from the convenience of deploying\ncustom machine learning solutions within databases by building & configuring\nmodels manually through a declarative syntax called\nJSON-AI.\nThere are other interesting ML use cases where MindsDB is positioned extremely\nwell, like multivariate time-series and real-time data streams, so feel free to",
    "tag": "mindsdb"
  },
  {
    "title": "Introduction",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/tutorials/customer-churn.mdx",
    "content": "\ntitle: Predicting Customer Churn with MindsDB\nsidebarTitle: Customer Churn\n\nIntroduction\nIn this tutorial, we'll create and train a machine learning model, or as we call\nit, an `AI Table` or a `predictor`. By querying the model, we'll predict the\nprobability of churn for new customers of a telecom company.\nMake sure you have access to a working MindsDB installation, either locally or\nat MindsDB Cloud.\nIf you want to learn how to set up your account at MindsDB Cloud, follow\nthis guide. Another way is to set up\nMindsDB locally using\nDocker or\nPython.\nLet's get started.\nData Setup\nConnecting the Data\nThere are a couple of ways you can get the data to follow through with this\ntutorial.\n\n\n    You can connect to a demo database that we've prepared for you. It contains the data used throughout this tutorial (the `example_db.demo_data.customer_churn` table).\n\n\n``````sql\nCREATE DATABASE example_db\nWITH ENGINE = \"postgres\",\nPARAMETERS = {\n    \"user\": \"demo_user\",\n    \"password\": \"demo_password\",\n    \"host\": \"3.220.66.106\",\n    \"port\": \"5432\",\n    \"database\": \"demo\"\n};\n```\n\nNow you can run queries directly on the demo database. Let's preview the data that we'll use to train our predictor.\n\n```sql\nSELECT *\nFROM example_db.demo_data.customer_churn\nLIMIT 10;\n```\n```\n\n\n\n\n    You can download the CSV data file here and upload it via MindsDB SQL Editor.\n\n\n```Follow [this guide](/sql/create/file/) to find out how to upload a file to MindsDB.\n\nNow you can run queries directly on the file as if it were a table. Let's preview the data that we'll use to train our predictor.\n\n```sql\nSELECT *\nFROM files.churn\nLIMIT 10;\n```\n```\n\n\n\n\n\nPay Attention to the Queries \nFrom now on, we'll use the\n`files.churn` file as a table. Make sure you replace it with\n`example_db.demo_data.customer_churn` if you connect the data as a database.\n\nUnderstanding the Data\nWe use the customer churn dataset, where each row is one customer, to predict\nwhether the customer is going to stop using the company products.\nBelow is the sample data stored in the `files.churn` table.\n`sql\n+----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+-------------------------+--------------+------------+-----+\n|customerID|gender|SeniorCitizen|Partner|Dependents|tenure|PhoneService|MultipleLines   |InternetService|OnlineSecurity|OnlineBackup|DeviceProtection|TechSupport|StreamingTV|StreamingMovies|Contract      |PaperlessBilling|PaymentMethod            |MonthlyCharges|TotalCharges|Churn|\n+----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+-------------------------+--------------+------------+-----+\n|7590-VHVEG|Female|0            |Yes    |No        |1     |No          |No phone service|DSL            |No            |Yes         |No              |No         |No         |No             |Month-to-month|Yes             |Electronic check         |29.85         |29.85       |No   |\n|5575-GNVDE|Male  |0            |No     |No        |34    |Yes         |No              |DSL            |Yes           |No          |Yes             |No         |No         |No             |One year      |No              |Mailed check             |56.95         |1889.5      |No   |\n|3668-QPYBK|Male  |0            |No     |No        |2     |Yes         |No              |DSL            |Yes           |Yes         |No              |No         |No         |No             |Month-to-month|Yes             |Mailed check             |53.85         |108.15      |Yes  |\n|7795-CFOCW|Male  |0            |No     |No        |45    |No          |No phone service|DSL            |Yes           |No          |Yes             |Yes        |No         |No             |One year      |No              |Bank transfer (automatic)|42.3          |1840.75     |No   |\n|9237-HQITU|Female|0            |No     |No        |2     |Yes         |No              |Fiber optic    |No            |No          |No              |No         |No         |No             |Month-to-month|Yes             |Electronic check         |70.7          |151.65      |Yes  |\n+----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+-------------------------+--------------+------------+-----+`\nWhere:\n| Column             | Description                                                                                                             | Data Type           | Usage   |\n| ------------------ | ----------------------------------------------------------------------------------------------------------------------- | ------------------- | ------- |\n| `CustomerId`       | The identification number of a customer.                                                                                | `character varying` | Feature |\n| `Gender`           | The gender of a customer.                                                                                               | `character varying` | Feature |\n| `SeniorCitizen`    | It indicates whether the customer is a senior citizen (`1`) or not (`0`).                                               | `integer`           | Feature |\n| `Partner`          | It indicates whether the customer has a partner (`Yes`) or not (`No`).                                                  | `character varying` | Feature |\n| `Dependents`       | It indicates whether the customer has dependents (`Yes`) or not (`No`).                                                 | `character varying` | Feature |\n| `Tenure`           | Number of months the customer has been staying with the company.                                                        | `integer`           | Feature |\n| `PhoneService`     | It indicates whether the customer has a phone service (`Yes`) or not (`No`).                                            | `character varying` | Feature |\n| `MultipleLines`    | It indicates whether the customer has multiple lines (`Yes`) or not (`No`, `No phone service`).                         | `character varying` | Feature |\n| `InternetService`  | Customer\u2019s internet service provider (`DSL`, `Fiber optic`, `No`).                                                      | `character varying` | Feature |\n| `OnlineSecurity`   | It indicates whether the customer has online security (`Yes`) or not (`No`, `No internet service`).                     | `character varying` | Feature |\n| `OnlineBackup`     | It indicates whether the customer has online backup (`Yes`) or not (`No`, `No internet service`).                       | `character varying` | Feature |\n| `DeviceProtection` | It indicates whether the customer has device protection (`Yes`) or not (`No`, `No internet service`).                   | `character varying` | Feature |\n| `TechSupport`      | It indicates whether the customer has tech support (`Yes`) or not (`No`, `No internet service`).                        | `character varying` | Feature |\n| `StreamingTv`      | It indicates whether the customer has streaming TV (`Yes`) or not (`No`, `No internet service`).                        | `character varying` | Feature |\n| `StreamingMovies`  | It indicates whether the customer has streaming movies (`Yes`) or not (`No`, `No internet service`).                    | `character varying` | Feature |\n| `Contract`         | The contract term of the customer (`Month-to-month`, `One year`, `Two year`).                                           | `character varying` | Feature |\n| `PaperlessBilling` | It indicates whether the customer has paperless billinig (`Yes`) or not (`No`).                                         | `character varying` | Feature |\n| `PaymentMethod`    | Customer\u2019s payment method (`Electronic check`, `Mailed check`, `Bank transfer (automatic)`, `Credit card (automatic)`). | `character varying` | Feature |\n| `MonthlyCharges`   | The monthly charge amount.                                                                                              | `money`             | Feature |\n| `TotalCharges`     | The total amount charged to the customer.                                                                               | `money`             | Feature |\n| `Churn`            | It indicates whether the customer churned (`Yes`) or not (`No`).                                                        | `character varying` | Label   |\n\nLabels and Features\nA label is a column whose values will be predicted (the y variable in simple\nlinear regression).\nA feature is a column used to train the model (the x variable in simple\nlinear regression).\n\nTraining a Predictor\nLet's create and train the machine learning model. For that, we use the\nCREATE MODEL statement and specify the\ninput columns used to train `FROM` (features) and what we want to\n`PREDICT` (labels).\n`sql\nCREATE MODEL mindsdb.customer_churn_predictor\nFROM files\n  (SELECT * FROM churn)\nPREDICT Churn;`\nWe use all of the columns as features, except for the `Churn` column, whose\nvalues will be predicted.\nStatus of a Predictor\nA predictor may take a couple of minutes for the training to complete. You can\nmonitor the status of the predictor by using this SQL command:\n`sql\nSELECT status\nFROM mindsdb.models\nWHERE name='customer_churn_predictor';`\nIf we run it right after creating a predictor, we get this output:\n`sql\n+------------+\n| status     |\n+------------+\n| generating |\n+------------+`\nA bit later, this is the output:\n`sql\n+----------+\n| status   |\n+----------+\n| training |\n+----------+`\nAnd at last, this should be the output:\n`sql\n+----------+\n| status   |\n+----------+\n| complete |\n+----------+`\nNow, if the status of our predictor says `complete`, we can start making\npredictions!\nMaking Predictions\nMaking a Single Prediction\nYou can make predictions by querying the predictor as if it were a table. The\nSELECT statement lets you make predictions for the label\nbased on the chosen features.\n`sql\nSELECT Churn, Churn_confidence, Churn_explain\nFROM mindsdb.customer_churn_predictor\nWHERE SeniorCitizen=0\nAND Partner='Yes'\nAND Dependents='No'\nAND tenure=1\nAND PhoneService='No'\nAND MultipleLines='No phone service'\nAND InternetService='DSL';`\nOn execution, we get:\n`sql\n+-------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Churn | Churn_confidence    | Churn_explain                                                                                                                                                    |\n+-------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Yes   | 0.7752808988764045  | {\"predicted_value\": \"Yes\", \"confidence\": 0.7752808988764045, \"anomaly\": null, \"truth\": null, \"probability_class_No\": 0.4756, \"probability_class_Yes\": 0.5244}    |\n+-------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+`\nTo get more accurate predictions, we should provide as much data as possible in\nthe `WHERE` clause. Let's run another query.\n`sql\nSELECT Churn, Churn_confidence, Churn_explain\nFROM mindsdb.customer_churn_predictor\nWHERE SeniorCitizen=0\nAND Partner='Yes'\nAND Dependents='No'\nAND tenure=1\nAND PhoneService='No'\nAND MultipleLines='No phone service'\nAND InternetService='DSL'\nAND Contract='Month-to-month'\nAND MonthlyCharges=29.85\nAND TotalCharges=29.85\nAND OnlineBackup='Yes'\nAND OnlineSecurity='No'\nAND DeviceProtection='No'\nAND TechSupport='No'\nAND StreamingTV='No'\nAND StreamingMovies='No'\nAND PaperlessBilling='Yes'\nAND PaymentMethod='Electronic check';`\nOn execution, we get:\n`sql\n+-------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Churn | Churn_confidence    | Churn_explain                                                                                                                                                    |\n+-------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Yes   | 0.8202247191011236  | {\"predicted_value\": \"Yes\", \"confidence\": 0.8202247191011236, \"anomaly\": null, \"truth\": null, \"probability_class_No\": 0.4098, \"probability_class_Yes\": 0.5902}    |\n+-------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+`\nMindsDB predicted the probability of this customer churning with confidence of\naround 82%. The previous query predicted it with confidence of around 79%. So\nproviding more data improved the confidence level of predictions.\nMaking Batch Predictions\nAlso, you can make bulk predictions by joining a data table with your predictor\nusing JOIN.\n`sql\nSELECT t.customerID, t.Contract, t.MonthlyCharges, m.Churn\nFROM files.churn AS t\nJOIN mindsdb.customer_churn_predictor AS m\nLIMIT 100;`\nOn execution, we get:\n`sql\n+----------------+-------------------+------------------+---------+\n| customerID     | Contract          | MonthlyCharges   | Churn   |\n+----------------+-------------------+------------------+---------+\n| 7590-VHVEG     | Month-to-month    | 29.85            | Yes     |\n| 5575-GNVDE     | One year          | 56.95            | No      |\n| 3668-QPYBK     | Month-to-month    | 53.85            | Yes     |\n| 7795-CFOCW     | One year          | 42.3             | No      |\n| 9237-HQITU     | Month-to-month    | 70.7             | Yes     |\n+----------------+-------------------+------------------+---------+`\nWhat's Next?\nHave fun while trying it out yourself!\n\nBookmark MindsDB repository on GitHub.\nSign up for a free MindsDB account.\nEngage with the MindsDB community on\n  Slack or\n  GitHub to ask questions and\n  share your ideas and thoughts.\n\nIf this tutorial was helpful, please give us a GitHub star",
    "tag": "mindsdb"
  },
  {
    "title": "Pre-requisites",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/tutorials/bodyfat.mdx",
    "content": "\ntitle: Determining Body Fat Percentage\nsidebarTitle: Body Fat Percentage\n\nDataset:\nBody fat prediction\nCommuntiy Author: Contip\nMachine Learning powered data analysis can be performed quickly and efficiently\nby MindsDB to enable individuals to make accurate predictions for certain\nmetrics based on a variety of associated values. MindsDB enables you to make\npredictions automatically using just SQL commands, all the ML workflow is\nautomated, and abstracted as virtual \u201cAI tables\u201d in your database so you may\nstart getting insights from forecasts right away. In this tutorial, we'll be\nusing MindsDB and a MySQL database to predict body fat percentage based on\nseveral body part measurement criteria.\nPre-requisites\nBefore you start make sure that you've:\n\nVisted Getting Started Guide\nVisited Getting Started with Cloud\nDownloaded the dataset. The dataset being used for this tutorial. Get it from\n  Kaggle.\n\nData Overview\nFor this tutorial, we'll be using the Body Fat Prediction dataset available at\nKaggle. Each\nrow represents one person and we'll train an ML model to help us predict an\nindividual's body fat percentage using MindsDB. Below is a short description of\neach feature of the data:\n\nDensity: Individual's body density as determined by underwater weighing\n  (float)\nBodyFat: The individual's determined body fat percentage (float). This is what\n  we want to predict\nAge: Age of the individual (int)\nWeight: Weight of the individual in pounds (float)\nHeight: Height of the individual in inches (float)\nNeck: Circumference of the individual's neck in cm (float)\nChest: Circumference of the individual's chest in cm (float)\nAbdomen: Circumference of the individual's abdomen in cm (float)\nHip: Circumference of the individual's hips in cm (float)\nThigh: Circumference of the individual's thigh in cm (float)\nKnee: Circumference of the individual's knee in cm (float)\nAnkle: Circumference of the individual's ankle in cm (float)\nBiceps: Circumference of the individual's biceps in cm (float)\nForearm: Circumference of the individual's forearm in cm (float)\nWrist: Circumference of the individual's wrist in cm (float)\n\nAdd data to MindsDB GUI\nMindsDB has a functionality to uploud your data file directly via the GUI where\nyou can immediately query the data and create a machine learning model.\nThe following is the steps to upload your data directly to MindsDB:\n\nAccess the MindsDB GUI via cloud or local via the URL 127.0.0.1:47334/.\nSelect the button `Add data` or select the plug icon on the left side bar.\nThe page will navigate to 'Select your data source'. Select the option\n  'Files'.\n\n\n\nSelect the tab under 'Import a file'. Please note the dataset files should not\n  exceed the maximum size limit which is 10MB.\nProvide a name for the data file which will be saved as a table.\n\nOnce you have successfully uploaded the file, you can query the data from the\nfiles table to ensure the information pulls through.\nRun the following syntax:\n`sql\nSELECT *\nFROM files.bodyfat\nLIMIT 10;`\n\nOnce you have confirmed the file has successfully uploaded and the data can be\nretrieved, we can move on to creating a predictor.\nCreate and train a machine learning model.\nWith the CREATE PREDICTOR statement, we can create a machine learning model:\n`sql\nCREATE PREDICTOR mindsdb.predictor_name\nFROM files\n        (SELECT column_name, column_name2 FROM file_name)\nPREDICT column_name;`\nThe required values that we need to provide are: \u200b\n\npredictor_name (string): The name of the model\nintegration_name (string): The name of the connection to your database.\ncolumn_name (string): The feature you want to predict.\n\nFor our case, we'll enter the following syntax:\n`sql\nCREATE PREDICTOR bodyfat_predictor\nFROM files\n        (SELECT * FROM bodyfat)\nPREDICT Bodyfat;`\nSelect the `Run` button or select Shift+Enter to run the syntax. Once is is\nsuccessful you will receive a message in the console 'Query successfully\ncompleted'.\nYou should see output similar to the following:\n\nAt this point, the predictor will immediately begin training. Check the status\nof the training by entering the command:\n`sql\nSELECT *\nFROM mindsdb.predictors\nWHERE name='bodyfat_predictor';`\nWhen complete, you should see output similar to the following:\n\nAs you can see, the predictor training has been completed with an accuracy of\napproximately 99%. At this point, you have successfully trained an ML model for\nour Body Fat Prediction dataset!\nUsing SQL Commands to Make Predictions\nNow, we can query the model and make predictions based on our input data by\nusing SQL statements.\nLet's imagine an individual aged 25, with a body density of 1.08, a weight of\n170lb, a height of 70in, a neck circumference of 38.1cm, a chest circumference\nof 103.5cm, an abdomen circumference of 85.4cm, a hip circumference of 102.2cm,\na thigh circumference of 63.0cm, a knee circumference of 39.4cm, an ankle\ncircumference of 22.8cm, a biceps circumference of 33.3cm, a forearm\ncircumference of 28.7cm, and a wrist circumference of 18.3cm. We can predict\nthis person's body fat percentage by issuing the following command:\n`sql\nSELECT BodyFat, BodyFat_confidence, BodyFat_explain\nFROM mindsdb.bodyfat_predictor\nWHERE Density=1.08\nAND Age=25\nAND Weight=170\nAND Height=70\nAND Neck=38.1\nAND Chest=103.5\nAND Abdomen=85.4\nAND Hip=102.2\nAND Thigh=63.0\nAND Knee=39.4\nAND Ankle=22.8\nAND Biceps=33.3\nAND Forearm=28.7\nAND Wrist=18.3;`\nThis should return output similar to:\n\nAs you can see, with around 99% confidence, MindsDB predicted the body fat\npercentage for this individual at 8.97%. You can at this point feel free to\nalter the prospective individual's bodypart measurement parameters and make\nadditional prediction queries if you'd like.\nMaking Batch Predictions using the JOIN syntax\nThe above example showed how to make predictions for a single individual's\nbodyfat, but what if you had a table of bodypart measurements for a number of\nindividuals, and wanted to make predictions for them all? This is possible using\nthe JOIN command, which allows for the\ncombining of rows from a database table and the prediction model table on a\nrelated column.\nThe basic syntax to use the JOIN syntax is:\n`sql\nSELECT t.column_name1, t.column_name2\nFROM integration_name.table AS t\nJOIN mindsdb.predictor_name AS p\nWHERE t.column_name IN (value1, value2, ...);`\nFor our purposes, we'll re-use the original data set, taking the Age, Density,\nWeight, Height, Neck circumference, Chest circumference, Abdomen circumference,\nand Hip circumference fields. We'll also include the original BodyFat percentage\nto compare our predicted values against the originals. Execute the following\ncommand:\n`sql\nSELECT t.Age, t.Density, t.Weight, t.Height, t.Neck,\n       t.Chest, t.Abdomen, t.Hip, t.BodyFat, p.BodyFat AS predicted_BodyFat\nFROM bodyfat_integration.bodyfat AS t\nJOIN mindsdb.bodyfat_predictor AS p\nLIMIT 5;`\nThis should return an output table similar to the following:\n\nAs you can see, a prediction has been generated for each row in the input table.\nAdditionally, our predicted bodyfat percentages align closely with the original\nvalues! Note that even though we chose only to display the Age, Density, Weight,\nHeight, Neck, Chest, Abdomen, and Hip measurements in this example, the\npredicted_BodyFat field was determined by taking into consideration all of the\ndata fields in the original bodyfat table (as this table was JOINed with the\nbodyfat_predictor table, from which we selected the specified fields). In order\nto make predictions based ONLY on the specified fields, we would have to create\na new table containing only those fields, and JOIN that with the\nbodyfat_predictor table!\nYou are now done with the tutorial! \ud83c\udf89\nPlease feel free to try it yourself. Sign up for a\nfree MindsDB account to get up and running in 5\nminutes, and if you need any help, feel free to ask in\nSlack\nor Github.\nFor more tutorials like this check out",
    "tag": "mindsdb"
  },
  {
    "title": "Forecast Bitcoin price using MindsDB",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/tutorials/bitcoin-forecasting.mdx",
    "content": "Forecast Bitcoin price using MindsDB\nLevel: Easy\nDataset: Coinbase 2017-2018 Bitcoin data\nBitcoin is a digital currency that uses blockchain technology, Bitcoin can be sent from user to user on the peer-to-peer Bitcoin network without the need for intermediaries. Note that this is just a task for fun so use it at your own risk.\nIn this tutorial, you will learn how to forecast Bitcoin using MindsDB. And all you need to know is just SQL. Behind the scenes, MindsDB will create the complete machine learning workflow, like determine, normalize & encode the data, train & test the model, etc. But we don\u2019t need to bother with all this complexity. Of course, if you want to, you can tune things manually inside MindsDB with a declarative syntax called JSON-AI, but we will not cover it in this article.\nDISCLAIMER: Please note that predicting Bitcoin price is just an example for showing MindsDB technology and you are solely responsible for any results you may get in real life, if you use this information for real trading purposes. Please note, that you can also follow this tutorial with other data you have.\nPre-requisites\nFirst, you need MindsDB installed. If you want to use MindsDB locally, you need to install MindsDB with\nDocker or Python. \nHowever, if you want to use MindsDB without installing it locally, you can use Cloud Mindsdb. \nIn this tutorial, I'm using MindsDB Cloud, because it is easy to set up in just 2 minutes and it has a great free tier.\nSecond, you need a MySQL client to connect to MindsDB MYSQL API.\nConnect your database\nYou must first connect MindsDB to the database where the record is stored. \nIn the left navigation, click  Database, click  ADD DATABASE. \nAnd you must provide all the necessary parameters to connect to the database.\n\n\nSupported Database - select the database that you want to connect to\nIntegrations Name - add a name to the integration, here I'm using 'mysql' but you can name it as you like\nDatabase - the database name\nHost - database hostname\nPort - database port\nUsername - database user\nPassword - user's password\n\nThen, click on CONNECT.\nThe next step is to use the MySQL client to connect to MindsDB\u2019s MySQL API, train a new model, and make a prediction.\nConnect to MindsDB\u2019s MySQL API\nIn this tutorial I'm using MySQL command-line client, but you can also follow up with the one that works the best for you, like Dbeaver.\nThe first step is to use the MindsDB Cloud user to connect to the MindsDB MySQL API, using this command:\n\nYou need to specify the hostname and user name explicitly, as well as a password for connecting. Click enter and you are connected to MindsDB API.\n\nIf you have an authentication error, please make sure you are providing the email address you have used to create an account on MindsDB Cloud.\nData\nNow, let's show the databases.\n\nThere are 4 databases, and the MySQL database is the database that I've connected to MindsDB.\nLet's check the MySQL database.\n\nThere are 3 tables, and in this tutorial, we will use the Bitcoin table.\nAnd let's check what is inside this table.\n\nThese tables have 5 columns: date, open price, the highest price of the day, lowest price of the day, and close price. \nThe column we want to forecast is close price.\nCreate the model\nNow, to create the model let's move to MindsDB database. and let's see what's inside.\n\nThere are 2 tables, predictors, and commands. Predictors contain your predictors record, and commands contain your last commands used.\nTo train a new machine learning model we will need to CREATE PREDICTOR as a new record inside the predictors table, and using this command:\n`sql\nCREATE PREDICTOR mindsdb.predictor_name\nFROM integration_name\n(SELECT column_name, column_name2 FROM table_name) as ds_name\nPREDICT column_target as column_alias\nORDER BY column_orderby\nWINDOW num_window\nHORIZON num_horizon\nUSING {\"is_timeseries\": \"Yes\"}`\nThe values that we need to provide are:\n\npredictor_name (string) - The name of the model.\nintegration_name (string) - The name of the connection to your database.\nds_name (string) - the name of the dataset you want to create, it's optional if you don't specify this value MindsDB will generate by itself.\ncolumn_target (string) - The feature you want to predict.\ncolumn_alias - Alias name of the feature you want to predict.\ncolumn_orderby - The column to order the data, for time series this should be the date/time column.\nnum_window - keyword specifies the number of rows to \"look back\" into when making a prediction after the rows are ordered by the order_by column and split into groups. \nThis could be used to specify something like \"Always use the previous 10 rows\".\nnum_horizon - keyword specifies the number of future predictions. \n\nSo, use this command to create the models:\n\nIf there's no error, that means your model is created and training. To see if your model is finished, use this command:\n`sql\nSELECT * FROM mindsdb.predictors WHERE name = predictor_name;`\nAnd values that we need to provide are:\n\npredictor_name (string) - The name of the model.\n\n\nIf the model is finished, it will look like this. The model has been created! and the accuracy is 99%!\nCreate the prediction\nNow you are in the last step of this tutorial, creating the prediction. To create a prediction you can use this command:\n`sql\nSELECT target_variable, target_variable_explain FROM model_table \nWHERE when_data={\"column3\": \"value\", \"column2\": \"value\"};`\nAnd you need to set these values:\n- target_variable - The original value of the target variable.\n- target_variable_confidence - Model confidence score.\n- target_variable_explain - JSON object that contains additional information as confidence_lower_bound, confidence_upper_bound, anomaly, truth.\n- when_data - The data to make the predictions from(WHERE clause params).\n\nFinally, we have created a Bitcoin forecasting model using SQL and MindsDB. Yayyy!\nConclusions",
    "tag": "mindsdb"
  },
  {
    "title": "Pre-requisites",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/tutorials/mushrooms.mdx",
    "content": "\ntitle: Mushrooms Hunting\nDataset: Mushrooms\nCommunity Author:\nChandre Tosca Van Der Westhuizen\nMushrooms are a fleshy sporocarp of fungi which can either be edible or\npoisonous. Its usage dates back centuries with ancient Greek, Chinese and\nAfrican cultures. They can have high nutritional value and medicinal properties\nwhich provide great health benefits. On the other side,some of these fungi can\nbe toxic and consuming the wrong mushroom can have deadly consequences. It is\nimportant for industries across the world, like the food and health sector, to\nidentify which type of mushrooms are edible and which are poisonous.\nWe will explore how MindsDB's machine learning predictive model can make it\neasier classifying mushrooms and predicting which is safe to consume and which\ncan make you ill.\nPre-requisites\nTo ensure you can complete all the steps, make sure you have access to the\nfollowing tools:\n\nA MindsDB instance. Check out the installation guide for\n   Docker or\n   PyPi. You can also\n   use MindsDB Cloud.\nDownloaded the dataset. You can get it from\n   Kaggle\nOptional: Access to ngrok. You can check the installation details at the\n   ngrok website.\n\nData Overview\nThe dataset consists of the following information:\nAttribute Information: (classes: edible=e, poisonous=p)\n\n\n```-  cap_shape: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s\n-  cap_surface: fibrous=f,grooves=g,scaly=y,smooth=s\n-  cap_color: brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y\n-  bruises: bruises=t,no=f\n-  odor: almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s\n-  gill_attachment: attached=a,descending=d,free=f,notched=n\n-  gill_spacing: close=c,crowded=w,distant=d\n-  gill_size: broad=b,narrow=n\n-  gill_color: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y\n-  stalk_shape: enlarging=e,tapering=t\n-  stalk_root: bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=?\n-  stalk_surface_above_ring: fibrous=f,scaly=y,silky=k,smooth=s\n-  stalk_surface_below_ring: fibrous=f,scaly=y,silky=k,smooth=s\n-  stalk_color_above_ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n-  stalk_color_below_ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n-  veil_type: partial=p,universal=u\n-  veil_color: brown=n,orange=o,white=w,yellow=y\n-  ring_number: none=n,one=o,two=t\n-  ring_type: cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z\n-  spore_print_color: black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y\n-  population: abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y\n-  habitat: grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d\n```\n\n\nDatabase connection to MindsDB\nTo establish a database connection we will access MindsDB's GUI. MindsDB has a\nSQL Editor on Cloud and local via the URL 127.0.0.1:47334/.\nFirst, we need to connect MindsDB to the database where the Mushrooms data is\nstored:\n\nAccess MindsDB GUI on either cloud or the URL 127.0.0.1:47334/\nOn the default page, select the button `Add Data` or alternatively select the\n  plug icon on the left sidebar\nThe 'Select your data source' page will populate for you to choose your\n  database type. For this tutorial we will be selecting the postgres database\n  button.\n\n\n\nOnce you have selected the database type,the page will automatically navigate\n  to the SQL Editor where the syntax to create a database connection will\n  automatically populate for you to enter the required parameters.\n\nThe required parameters are:\n\nCREATE DATABASE display_name --- display name for database.\nWITH ENGINE = \"postgres\", --- name of the mindsdb handler\nPARAMETERS =\n\"user\": \" \", - Your database user.\n\"password\": \" \", - Your password.\n\"host\": \" \", - host, it can be an ip or an url.\n\"port\": \"5432\", - common port is 5432.\n\"database\": \" \" - The name of your database (Optional)\n\n\nSelect the `Run` button or Shift+Enter to execute the syntax. Once the Database\nconnection is created the console will display a message 'Query successfully\ncompleted'. \u200b\n\nPlease note that some database connections require running a Ngrok tunnel to\nestablish a connection. Run the ngrok command in a terminal:\n`bash\nngrok tcp [db-port]`\nfor example,if your port number is 5433 you will see a similar output:\n`bash\nSession Status                online\nAccount                       myaccount (Plan: Free)\nVersion                       2.3.40\nRegion                        United States (us)\nWeb Interface                 http://127.0.0.1:4040\nForwarding                    tcp://6.tcp.ngrok.io:14789 -> localhost:5433`\nThe forwarded address information will be required when connecting to\nMindsDB's GUI. Select and copy the 'Forwarding' information, in this case it\nis `6.tcp.ngrok.io:14789`, where 6.tcp.ngrok.io will be used for the host\nparameter and 14789 as the port number.\n\nOnce the database integration is successful we can query the table from the\ndatabase to ensure the data pulls through on MindsDB.\n\nCreate a machine learning model.\nNow we are ready to create our own predictor. We will start by using the SQL\nEditor to execute simple SQL syntax to create and train a machine learning\npredictive model.\nThe predictor we will and be trained to determine if a mushroom is edible or\npoisonous.The following CREATE PREDICTOR statement is used to create predictors:\n`sql\nCREATE PREDICTOR mindsdb.predictor_name\nFROM integration_name\n    (SELECT column_name, column_name2 FROM table_name)\nPREDICT column_name;`\nThe required values that we need to provide are: \u200b\n\npredictor_name (string): The name of the model\nintegration_name (string): The name of the connection to your database.\ncolumn_name (string): The feature you want to predict.\n\nUse the following query to create a predictor that will predict the\n`target_class` for the specific field parameters.\n`sql\nCREATE PREDICTOR mushroom_predictor\nFROM mindsdb_predictions\n    (SELECT * FROM mushrooms)\nPREDICT class;`\nSelect the `Run` button or Shift+Enter to execute the syntax. Once the predictor\nis created the console will display a message 'Query successfully completed'.\n\nThe predictor was created successfully and has started training. To check the\nstatus of the model, use the below query.\n`sql\nSELECT *\nFROM mindsdb.predictors\nWHERE name='mushroom_predictor';`\nAfter the predictor has finished training, you will see a similar output. Note\nthat MindsDB does model testing for you automatically, so you will immediately\nsee if the predictor is accurate enough.\n\nThe predictor has completed its training, indicated by the status column, and\nshows the accuracy of the model. You can revisit training new predictors to\nincrease accuracy by changing the query to better suit the dataset i.e. omitting\ncertain columns etc.\nGood job! We have successfully created and trained a predictive model \u2728\nMake predictions\nIn this section you will learn how to make predictions using your trained model.\nNow we will use the trained model to make predictions using a SQL syntax.\nUse the following query using mock data with the predictor.\n`sql\nSELECT class\nFROM mindsdb.mushroom_predictor\nWHERE cap_shape='x'\nAND cap_surface='s'\nAND cap_color='n'\nAND bruises='t'\nAND odor='p'\nAND gill_attachment='f'\nAND gill_spacing='c'\nAND gill_size='n'\nAND gill_color='k'\nAND stalk_shape='e'\nAND stalk_root='e'\nAND stalk_surface_above_ring='s'\nAND stalk_surface_below_ring='s'\nAND stalk_color_above_ring='w'\nAND stalk_color_below_ring='w'\nAND veil_type='p'\nAND veil_color='w'\nAND ring_number='o'\nAND ring_type='p'\nAND spore_print_color='k'\nAND population='s'\nAND habitat='u';`\nThe result:\n\nThe machine learning model has predicted that the mushroom is poisonous.\nWe have successfully created and trained a model that can predict if a model is\nedible or poisonous.\nWant to try it out for yourself? Sign up for a\nfree MindsDB account\nand join our community! Engage with MindsDB community on\nSlack\nor Github to ask questions,\nshare and express ideas and thoughts!\nFor more check out other",
    "tag": "mindsdb"
  },
  {
    "title": "Connect your database to MindsDB GUI",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/tutorials/diabetes.mdx",
    "content": "\ntitle: Diabetes\n\n\nDataset: Diabetes Data\nCommuntiy Author: Chandre Tosca Van Der Westhuizen\nDiabetes is a metabolic disease that causes high blood sugar and if left\nuntreated can damage your nerves, eyes, kidneys and other organs. It is known as\na silent killer, as recent studies have shown that by the year 2040 the world's\ndiabetic patients will reach 642 million. The need to analyze vast medical data\nto assist in the diagnoses, treatment and management of illnesses is increasing\nfor the medical community. With the rapid development of machine learning, it\nhas been applied to many aspects of medical health and is transforming the\nhealthcare system.\nThe vitality to intelligently transform information into valuable knowledge\nthrough machine learning has become more present in biosciences. With the use of\npredictive models, MindsDB can assist in classifying diabetic and non-diabetic\npatients or those who pose a high risk. This is just a small showcase on how\nMindsDB's machine learning will be able to assist in vastly enhancing the reach\nof illnesses, thereby making it more efficient and can revolutionize businesses\nand most importantly the health care system.\nIn this tutorial we will be exploring how we can use a machine learning model to classify negative and positive cases for diabetes. MindsDB allows you to train your model from CSV data directly, however for this tutorial you will:\n\nEstablish a connection between your database and MindsDB via MindsDB\n   GUI(Cloud and local instance).\nAllow connections to the database using Ngrok.\nCreate a machine learning model using SQL.\nMake a prediction.\n\nConnect your database to MindsDB GUI\nMindsDB has a functionality to upload your dataset directly to MindsDB. However\nin this tutorial, you will be shown how to connect your database to MindsDB\ncloud and local instance.\nFor this example we will be using a local postgres database, therefore we will\nconnect using an ngrok tunnel.\nRunning a Ngrok Tunnel\nTo make our database publicly avaiable, we will use `ngrok`.\nThe following command can be run in docker or a terminal on your device to set\nup a ngrok tunnel.\n`bash\nngrok tcp [db-port]`\nFor this example the port number used is 5432.\nYou should see a similar output:\n`bash\nSession Status                online\nAccount                       chandre (Plan: Free)\nVersion                       2.3.40\nRegion                        United States (us)\nWeb Interface                 http://127.0.0.1:4040\nForwarding                    tcp://4.tcp.ngrok.io:13096 -> localhost:5432`\nThe information required is by the forwarded address, next to 'Forwarding'\nselect and copy `4.tcp.ngrok.io:13096`. Once you have copied the information,\nyou can add it to the information requested by the MindsDB GUI which we will get\nto in a moment.\nFor the next steps we will log into the MindsDB cloud interface and local gui.\nMindsDB Cloud is perfect if you are unable to install MindsDB on your device. If\nyou are not registered yet, feel free to follow the below guide. If you have\nalready registered, skip the next steps to connect your database.\nMindsDB GUI- Cloud MySQL Editor & MySQL CLI\nThe next step will be to connect your database to MindsDB.\nYou can visit this link and follow the\nsteps to create a MindsDB Cloud account. Access the MySQL Editor\nhere.\nIf you are using MindsDB through Docker, you can run the command below to start\nMindsDB:\n`bash\ndocker run -p 47334:47334 -p 47335:47335 mindsdb/mindsdb`\nOnce you are connected,head over to your browser and in the browser tab type in\nthe host number `127.0.0.1:47334` to access the local MindsDB GUI.\nEstablish connection between your database and MindsDB GUI\nOnce you have accessed the GUI, you will be able to select a database type and\nenter the required parameters:\n\nOn the user interface,select the plug icon on the left sidebar.\nA page will populate with different datasources to select from. For this\n  tutorial we are choosing PostgreSQL.\n\n\n\nThe page will automatically direct to the SQL editor defaulting the syntax\n  parameters required to create a connection with PostgreSQL:\nCREATE DATABASE : This will be the chosen display name for your database.\n    For this tutorial we will choose airbyte.\nWITH ENGINE : name of the mindsdb handler,in this example it will be\n    Postgres. PARAMETERS =\n\"user\": Your database user. for this tutorial it is postgres\n\"password\": Your password.\n\"host\":host, it can be an ip or an url. For this tutorial it will be the\n    forwarding address 4.tcp.ngrok.io\n\"port\": \"13096\",common port is 5432, for this tutorial we will use 15076.\n\"database\":The name of your database (optional). For this tutorial we will\n    be using postgres.\n\nSelect the button RUN or select Shift+Enter to execute the query.\n\nYou are now done with connecting MindsDB to your database! \ud83d\ude80\nCreate a predictor\nNow we are ready to create our own predictor! We will start by using the MySQL\nAPI to connect to MindsDB and with a single SQL command create a predictor.\nThe predictor we will create will be trained to determine negative and positive\ncases for diabetes.\nUse the following query to create a predictor that will predict the `Class`\n(positive or negative) for the specific field parameters.\n`sql\nCREATE PREDICTOR diabetes_predictor\nFROM mindsdb_prediction\n    (SELECT * FROM diabetes)\nPREDICT class;`\nSelect the `RUN` button,alternatively select Shift+Enter to execute the query.\nYou will receive the message 'Query successfully completed' if the machine\nlearning model is successfully created.\n\nThe predictor was created successfully and has started training. To check the\nstatus of the model, use the below query.\n`sql\nSELECT *\nFROM mindsdb.predictors\nWHERE name='diabetes_predictor';`\nAfter the predictor has finished training, you will see a similar output. Note\nthat MindsDB does model testing for you automatically, so you will immediately\nsee if the predictor is accurate enough.\n\nThe predictor has completed its training, indicated by the status column, and\nshows the accuracy of the model. You can revisit training new predictors to\nincrease accuracy by changing the query to better suit the dataset i.e. omitting\ncertain columns etc.\nGood job! We have successfully created and trained a predictive model \u2728\nMake predictions\nIn this section you will learn how to make predictions using your trained model.\nNow we will use the trained model to make predictions using a SQL query\nUse the following query using mock data with the predictor.\n`sql\nSELECT Class\nFROM mindsdb.diabetes_predictor\nWHERE number_of_times_pregnant=0\nAND plasma_glucose_concentration=135.0\nAND diastolic_blood_pressure=65.0\nAND triceps_skin_fold_thickness=30\nAND two_Hour_serum_insulin=0\nAND body_mass_index=23.5\nAND diabetes_pedigree_function=0.366\nAND age=31;`\nMindsDB will provide you with results similar to below:\n\nThe machine learning model predicted the diabetic class to be negative.\nViola! We have successfully created and trained a model and made our own\nprediction. How easy and amazing is MindsDB? \ud83c\udf89\nWant to try it out?\n\nSign up for a\n  free MindsDB account\nJoin MindsDB community on\n  Slack\n  and GitHub to ask questions,\n",
    "tag": "mindsdb"
  },
  {
    "title": "Manufacturing process quality",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/tutorials/process-quality.mdx",
    "content": "\ntitle: Process Quality\nDataset: Mining process Data\nCommuntiy Author: pixpack\nPre-requisites\nBefore you start make sure that you've:\n\nVisited Getting Started Guide\nVisited Getting Started with Cloud\nDownloaded the dataset. You can get it from\n   Kaggle.\nOptional- Visual studio.\n\nManufacturing process quality\nPredicting process result quality is a common task in manufacturing analytics.\nManufacturing plants commonly use quality predictions to gain a competitive edge\nover their competitors, improve their products or increase their customers\nsatisfaction. MindsDB is a tool that can help you solve quality prediction\ntasks easily and effectively using machine learning. MindsDB abstracts\nML models as virtual \u201cAI Tables\u201d in databases and you can make predictions just\nusing normal SQL commands.\nIn this tutorial you will learn how to predict the quality of a mining process\nusing MindsDB.\nUpload a file\n\nFix headers:\n`sed -e 's/ /_/g' -e 's/\\(.*\\)/\\L\\1/' -e 's/%_//g' MiningProcess_Flotation_Plant_Database.csv > fixed_headers.csv`\n     (for Linux/Unix)\nedit headers manually: change `space` to `underscore`, upper case to lower\n     case, remove `%` from headers (for Windows)\nAccess MindsDB GUI on local via URL 127.0.0.1:47334/.\nIf you are using MindsDB Cloud, make sure to add the file to a database and\n   create a database connection to MindsDB's GUI. Cloud has a file size limit of\n   10MB, therefore for this dataset it would be best to either use MindsDB local\n   instance or if on Cloud\n   connect via a database.\nSelect the `Add data` button or the plug icon on the left side bar.\nThe page will navigate to the 'Select your data source' page. Select 'Files'.\nUnder 'Import a file' select the tab 'Click here to browse local files' and\n   select your data file.\nOnce the file is uploaded 100%, provide a name for the data file that will be\n   saved as a table.\nSelect the button Save and Continue.\n\nYou can query the file that has been uploaded to see that the data does pull\nthrough.\n`sql\nSELECT *\nFROM files.file_name;`\nConnect to MindsDB SQL Sever\nMindsDB's GUI has a SQL Editor that allows you to create and train predictors\nand also makes predictions. However you can also do this via the mysql client in\na local terminal by accessing MindsDB SQL Server.\n1.\n`sql\nmysql -h cloud.mindsdb.com --port 3306 -u username@email.com -p`\n2.\n`sql\nUSE mindsdb;`\nCreate a predictor\nIn this section you will connect to MindsDB with the MySql API and create a\nPredictor. It is in MindsDB terms a machine learning model, but all its\ncomplexity is automated and abstracted as a virtual \u201cAI Table\u201d. If you are an ML\nexpert and want to tweak the model, MindsDB also allows you that (please refer\nto documentation).\nUse the following query to create a Predictor that will foretell the\nsilica_concentrate at the end of our mining process.\n\nThe row number is limited to 5000 to speed up training but you can keep the\nwhole dataset.\n\n`sql\nCREATE PREDICTOR mindsdb.process_quality_predictor\nFROM files (\n    SELECT iron_feed, silica_feed, starch_flow, amina_flow, ore_pulp_flow,\n           ore_pulp_ph, ore_pulp_density,flotation_column_01_air_flow,\n           flotation_column_02_air_flow, flotation_column_03_air_flow,\n           flotation_column_04_air_flow, flotation_column_05_air_flow,\n           flotation_column_06_air_flow,flotation_column_07_air_flow,\n           flotation_column_01_level, flotation_column_02_level,\n           flotation_column_03_level, flotation_column_04_level,\n           flotation_column_05_level, flotation_column_06_level,\n           flotation_column_07_level, iron_concentrate, silica_concentrate\n    FROM process_quality\n    LIMIT 5000\n) PREDICT silica_concentrate;`\nOn execution, we get:\n`sql\nQuery OK, 0 rows affected (2 min 27.52 sec)`\nNow the Predictor will begin training. You can check the status with the\nfollowing query.\n`sql\nSELECT *\nFROM mindsdb.predictors\nWHERE name='process_quality_predictor';`\nOn execution, we get:\n`sql\n+-----------------------------+----------+----------+--------------------+-------------------+------------------+\n| name                        | status   | accuracy | predict            | select_data_query | training_options |\n+-----------------------------+----------+----------+--------------------+-------------------+------------------+\n| process_quality_predictor   | complete | 1        | silica_concentrate |                   |                  |\n+-----------------------------+----------+----------+--------------------+-------------------+------------------+\n1 row in set (0.28 sec)`\nAs you can see the accuracy of the model is 1 (i.e. 100%). This is the result of\nusing a limited dataset of 5000 rows. In reality when using the whole dataset,\nyou will probably see a more reasonable accuracy.\nYou are now done with creating the predictor! \u2728\nMake predictions\nIn this section you will learn how to make predictions using your trained model.\nTo run a prediction against new or existing data, you can use the following\nquery.\n`sql\nSELECT silica_concentrate, silica_concentrate_confidence, silica_concentrate_explain\nFROM mindsdb.process_quality_predictor\nWHERE iron_feed=48.81\nAND silica_feed=25.31\nAND starch_flow=2504.94\nAND amina_flow=309.448\nAND ore_pulp_flow=377.6511682692\nAND ore_pulp_ph=10.0607\nAND ore_pulp_density=1.68676;`\nOn execution, we get:\n`sql\n+--------------------+-------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+\n| silica_concentrate | silica_concentrate_confidence | Info                                                                                                                                            |\n+--------------------+-------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+\n| 1.68               | 0.99                          | {\"predicted_value\": \"1.68\", \"confidence\": 0.99, \"confidence_lower_bound\": null, \"confidence_upper_bound\": null, \"anomaly\": null, \"truth\": null} |\n+--------------------+-------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+\n1 row in set (0.81 sec)`\nAs you can see, the model predicted the `silica concentrate` for our data point.\nAgain we can see a very high confidence due to the limited dataset. When making\npredictions you can include different fields. As you can notice, we have only\nincluded the first 7 fields of our dataset. You are free to test different\ncombinations.\nIn the previous example, we have made a prediction for a single data point. In a\nreal scenario, you might want to make predictions on multiple data points. In\nthis case, MindsDB allows you to Join this other table with the Predictor. In\nresult, you will get another table as an output with a predicted value as one of\nits columns.\nLet\u2019s see how to make batch predictions.\nUse the following command to create the batch prediction.\n`sql\nSELECT\n    collected_data.iron_feed,\n    collected_data.silica_feed,\n    collected_data.starch_flow,\n    collected_data.amina_flow,\n    collected_data.ore_pulp_flow,\n    collected_data.ore_pulp_ph,\n    collected_data.ore_pulp_density,\n    predictions.silica_concentrate_confidence AS confidence,\n    predictions.silica_concentrate AS predicted_silica_concentrate\nFROM process_quality_integration.process_quality AS collected_data\nJOIN mindsdb.process_quality_predictor AS predictions\nLIMIT 5;`\nAs you can see below, the predictor has made multiple predictions for each data\npoint in the `collected_data` table! You can also try selecting other fields to\nget more insight on the predictions. See the\nJOIN clause documentation for more\ninformation.\n`sql\n+-----------+-------------+-------------+------------+---------------+-------------+------------------+------------+------------------------------+\n| iron_feed | silica_feed | starch_flow | amina_flow | ore_pulp_flow | ore_pulp_ph | ore_pulp_density | confidence | predicted_silica_concentrate |\n+-----------+-------------+-------------+------------+---------------+-------------+------------------+------------+------------------------------+\n| 58.84     | 11.46       | 3277.34     | 564.209    | 403.242       | 9.88472     | 1.76297          | 0.99       | 2.129567174379606            |\n| 58.84     | 11.46       | 3333.59     | 565.308    | 401.016       | 9.88543     | 1.76331          | 0.99       | 2.129548423407259            |\n| 58.84     | 11.46       | 3400.39     | 565.674    | 399.551       | 9.88613     | 1.76366          | 0.99       | 2.130100408285386            |\n| 58.84     | 11.46       | 3410.55     | 563.843    | 397.559       | 9.88684     | 1.764            | 0.99       | 2.1298757513510136           |\n| 58.84     | 11.46       | 3408.98     | 559.57     | 401.719       | 9.88755     | 1.76434          | 0.99       | 2.130438907683961            |\n+-----------+-------------+-------------+------------+---------------+-------------+------------------+------------+------------------------------+`\nWhat's Next?\nHave fun while trying it out yourself!\n\nBookmark MindsDB repository on GitHub.\nSign up for a free MindsDB account.\nEngage with the MindsDB community on\n  Slack or\n  GitHub to ask questions and\n  share your ideas and thoughts.\n\nIf this tutorial was helpful, please give us a GitHub star",
    "tag": "mindsdb"
  },
  {
    "title": "Introduction",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/tutorials/home-rentals.mdx",
    "content": "\ntitle: Predicting Home Rental Prices with MindsDB\nsidebarTitle: Home Rentals\n\nIntroduction\nIn this tutorial, we'll create and train a machine learning model, or as we call\nit, an `AI Table` or a `predictor`. By querying the model, we'll predict the\nrental prices of the properties based on their attributes, such as the number of\nrooms, area, or neighborhood.\nMake sure you have access to a working MindsDB installation, either locally or\nat MindsDB Cloud.\nIf you want to learn how to set up your account at MindsDB Cloud, follow\nthis guide. Another way is to set up\nMindsDB locally using\nDocker or\nPython.\nLet's get started.\nData Setup\nConnecting the Data\nThere are a couple of ways you can get the data to follow through with this\ntutorial.\n\n\n    You can connect to a demo database that we've prepared for you. It contains the data used throughout this tutorial (the `example_db.demo_data.home_rentals` table).\n\n\n``````sql\nCREATE DATABASE example_db\nWITH ENGINE = \"postgres\",\nPARAMETERS = {\n    \"user\": \"demo_user\",\n    \"password\": \"demo_password\",\n    \"host\": \"3.220.66.106\",\n    \"port\": \"5432\",\n    \"database\": \"demo\"\n};\n```\n\nNow you can run queries directly on the demo database. Let's preview the data that we'll use to train our predictor.\n\n```sql\nSELECT *\nFROM example_db.demo_data.home_rentals\nLIMIT 10;\n```\n```\n\n\n\n\n    You can download the CSV data file here and upload it via MindsDB SQL Editor.\n\n\n```Follow [this guide](/sql/create/file/) to find out how to upload a file to MindsDB.\n\nNow you can run queries directly on the file as if it were a table. Let's preview the data that we'll use to train our predictor.\n\n```sql\nSELECT *\nFROM files.home_rentals\nLIMIT 10;\n```\n```\n\n\n\n\n\nPay Attention to the Queries From now on, we'll use the\n  `example_db.demo_data.home_rentals` table. Make sure you replace it with\n  `files.home_rentals` if you connect the data as a file.\n\nUnderstanding the Data\nWe use the home rentals dataset, where each row is one property, to predict the\n`rental_price` column value for all the newly added properties.\nBelow is the sample data stored in the `example_db.demo_data.home_rentals`\ntable.\n`sql\n+-----------------+---------------------+------+----------+----------------+----------------+--------------+\n| number_of_rooms | number_of_bathrooms | sqft | location | days_on_market | neighborhood   | rental_price |\n+-----------------+---------------------+------+----------+----------------+----------------+--------------+\n|               2 |                   1 |  917 | great    |             13 | berkeley_hills |         3901 |\n|               0 |                   1 |  194 | great    |             10 | berkeley_hills |         2042 |\n|               1 |                   1 |  543 | poor     |             18 | westbrae       |         1871 |\n|               2 |                   1 |  503 | good     |             10 | downtown       |         3026 |\n|               3 |                   2 | 1066 | good     |             13 | thowsand_oaks  |         4774 |\n+-----------------+---------------------+------+----------+----------------+----------------+--------------+`\nWhere:\n| Column                | Description                                                              | Data Type           | Usage   |\n| --------------------- | ------------------------------------------------------------------------ | ------------------- | ------- |\n| `number_of_rooms`     | Number of rooms in a property `[0,1,2,3]`.                               | `integer`           | Feature |\n| `number_of_bathrooms` | Number of bathrooms in a property `[1,2]`.                               | `integer`           | Feature |\n| `sqft`                | Area of a property in square feet.                                       | `integer`           | Feature |\n| `location`            | Rating of the location of a property `[poor, great, good]`.              | `character varying` | Feature |\n| `days_on_market`      | Number of days a property has been on the market.                        | `integer`           | Feature |\n| `neighborhood`        | Neighborhood `[alcatraz_ave, westbrae, ..., south_side, thowsand_oaks]`. | `character varying` | Feature |\n| `rental_price`        | Rental price of a property in USD.                                       | `integer`           | Label   |\n\nLabels and Features\nA label is a column whose values will be predicted (the y variable in simple\nlinear regression). A feature is a column used to train the model (the\nx variable in simple linear regression).\n\nTraining a Predictor\nLet's create and train the machine learning model. For that, we use the\nCREATE MODEL statement and specify the\ninput columns used to train `FROM` (features) and what we want to\n`PREDICT` (labels).\n`sql\nCREATE MODEL mindsdb.home_rentals_model\nFROM example_db\n  (SELECT * FROM demo_data.home_rentals)\nPREDICT rental_price;`\nWe use all of the columns as features, except for the `rental_price` column,\nwhose values will be predicted.\nStatus of a Predictor\nA predictor may take a couple of minutes for the training to complete. You can\nmonitor the status of the predictor by using this SQL command:\n`sql\nSELECT status\nFROM mindsdb.models\nWHERE name='home_rentals_model';`\nIf we run it right after creating a predictor, we get this output:\n`sql\n+------------+\n| status     |\n+------------+\n| generating |\n+------------+`\nA bit later, this is the output:\n`sql\n+----------+\n| status   |\n+----------+\n| training |\n+----------+`\nAnd at last, this should be the output:\n`sql\n+----------+\n| status   |\n+----------+\n| complete |\n+----------+`\nNow, if the status of our predictor says `complete`, we can start making\npredictions!\nMaking Predictions\nMaking a Single Prediction\nYou can make predictions by querying the predictor as if it were a table. The\nSELECT statement lets you make predictions for the label\nbased on the chosen features.\n`sql\nSELECT rental_price, rental_price_explain\nFROM mindsdb.home_rentals_model\nWHERE sqft = 823\nAND location='good'\nAND neighborhood='downtown'\nAND days_on_market=10;`\nOn execution, we get:\n`sql\n+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------+\n| rental_price | rental_price_explain                                                                                                                          |\n+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------+\n| 4394         | {\"predicted_value\": 4394, \"confidence\": 0.99, \"anomaly\": null, \"truth\": null, \"confidence_lower_bound\": 4313, \"confidence_upper_bound\": 4475} |\n+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------+`\nMaking Batch Predictions\nAlso, you can make bulk predictions by joining a data table with your predictor\nusing JOIN.\n`sql\nSELECT t.rental_price AS real_price,\n       m.rental_price AS predicted_price,\n       t.number_of_rooms,  t.number_of_bathrooms, t.sqft, t.location, t.days_on_market\nFROM example_db.demo_data.home_rentals AS t\nJOIN mindsdb.home_rentals_model AS m\nLIMIT 100;`\nOn execution, we get:\n`sql\n+------------+-----------------+-----------------+---------------------+------+----------+----------------+\n| real_price | predicted_price | number_of_rooms | number_of_bathrooms | sqft | location | days_on_market |\n+------------+-----------------+-----------------+---------------------+------+----------+----------------+\n| 3901       | 3886            | 2               | 1                   | 917  | great    | 13             |\n| 2042       | 2007            | 0               | 1                   | 194  | great    | 10             |\n| 1871       | 1865            | 1               | 1                   | 543  | poor     | 18             |\n| 3026       | 3020            | 2               | 1                   | 503  | good     | 10             |\n| 4774       | 4748            | 3               | 2                   | 1066 | good     | 13             |\n+------------+-----------------+-----------------+---------------------+------+----------+----------------+`\nWhat's Next?\nHave fun while trying it out yourself!\n\nBookmark MindsDB repository on GitHub.\nSign up for a free MindsDB account.\nEngage with the MindsDB community on\n  Slack or\n  GitHub to ask questions and\n  share your ideas and thoughts.\n\nIf this tutorial was helpful, please give us a GitHub star",
    "tag": "mindsdb"
  },
  {
    "title": "Introduction",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/tutorials/eeg-forecasting.mdx",
    "content": "\ntitle: Forecasting Eye State from Electroencephalogram Readings with MindsDB\nsidebarTitle: Brain Activity\n\nIntroduction\nIn this tutorial, we'll create and train a machine learning model, or as we call\nit, an `AI Table` or a `predictor`. By querying the model, we'll produce\ncategorical forecasts for a multivariate time series.\nMake sure you have access to a working MindsDB installation, either locally or\nat MindsDB Cloud.\nIf you want to learn how to set up your account at MindsDB Cloud, follow\nthis guide. Another way is to set up\nMindsDB locally using\nDocker or\nPython.\nLet's get started.\nData Setup\nConnecting the Data\nThere are a couple of ways you can get the data to follow through with this\ntutorial.\nConnecting as a database\n\n\n```You can connect to a demo database that we've prepared for you. It contains the data used throughout this tutorial (the `example_db.demo_data.eeg_eye` table).\n\n```sql\nCREATE DATABASE example_db\n    WITH ENGINE = \"postgres\",\n    PARAMETERS = {\n        \"user\": \"demo_user\",\n        \"password\": \"demo_password\",\n        \"host\": \"3.220.66.106\",\n        \"port\": \"5432\",\n        \"database\": \"demo\"\n};\n```\n\nNow you can run queries directly on the demo database. Let's preview the data that we'll use to train our predictor.\n\n```sql\nSELECT *\nFROM example_db.demo_data.eeg_eye\nLIMIT 10;\n```\n```\n\n\nConnecting as a file\n\n\n```The dataset we use in this tutorial is the UCI's *EEG Eye State* dataset. You can download it [here](https://archive.ics.uci.edu/ml/datasets/EEG+Eye+State) in the `ARFF` format that shoud be converted to the `CSV` format before uploading it via [MindsDB SQL Editor](/connect/mindsdb_editor/).\n\nFollow [this guide](/sql/create/file/) to find out how to upload a file to MindsDB.\n\nNow you can run queries directly on the file as if it were a table. Let's preview the data that we'll use to train our predictor.\n\n```sql\nSELECT *\nFROM files.eeg_eye\nLIMIT 10;\n```\n```\n\n\n\n  Pay Attention to the Queries\" From now on, we'll use the `files.eeg_eye` file\n  as a table. Make sure you replace it with `example_db.demo_data.eeg_eye` if\n  you connect the data as a database.\n\nUnderstanding the Data\nWe use the UCI's EEG Eye State dataset, where each row contains data of one\nelectroencephalogram (EEG) reading plus the current state of the patient's eye,\nwhere `0` indicates open eye and `1` indicates closed eye. We want to know ahead\nof time when the eye state will change, so we predict the `eyeDetection` column.\nBelow is the sample data stored in the `files.eeg_eye` table.\n`sql\n+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+--------------+\n| AF3      | F7       | F3       | FC5      | T7       | P7       | O1       | O2       | P8       | T8       | FC6      | F4       | F8       | AF4      | eyeDetection |\n+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+--------------+\n| 4329.23  | 4009.23  | 4289.23  | 4148.21  | 4350.26  | 4586.15  | 4096.92  | 4641.03  | 4222.05  | 4238.46  | 4211.28  | 4280.51  | 4635.9   | 4393.85  | 0            |\n| 4324.62  | 4004.62  | 4293.85  | 4148.72  | 4342.05  | 4586.67  | 4097.44  | 4638.97  | 4210.77  | 4226.67  | 4207.69  | 4279.49  | 4632.82  | 4384.1   | 0            |\n| 4327.69  | 4006.67  | 4295.38  | 4156.41  | 4336.92  | 4583.59  | 4096.92  | 4630.26  | 4207.69  | 4222.05  | 4206.67  | 4282.05  | 4628.72  | 4389.23  | 0            |\n| 4328.72  | 4011.79  | 4296.41  | 4155.9   | 4343.59  | 4582.56  | 4097.44  | 4630.77  | 4217.44  | 4235.38  | 4210.77  | 4287.69  | 4632.31  | 4396.41  | 0            |\n| 4326.15  | 4011.79  | 4292.31  | 4151.28  | 4347.69  | 4586.67  | 4095.9   | 4627.69  | 4210.77  | 4244.1   | 4212.82  | 4288.21  | 4632.82  | 4398.46  | 0            |\n+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+--------------+`\nWhere:\n| Column                                                                    | Description                                                                               | Data Type | Usage   |\n| ------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------- | --------- | ------- |\n| `AF3`,`F7`,`F3`,`FC5`,`T7`,`P7`,`O1`,`O2`,`P8`,`T8`,`FC6`,`F4`,`F8`,`AF4` | The EEG measurement data.                                                                 | `float`   | Feature |\n| `eyeDetectin`                                                             | The state of the patient's eye where `0` indicates open eye and `1` indicates closed eye. | `binary`  | Label   |\n\nLabels and Features\nA label is a column whose values will be predicted (the y variable in simple\nlinear regression).\nA feature is a column used to train the model (the x variable in simple\nlinear regression).\n\nTraining a Predictor\nLet's create and train the machine learning model. For that, we use the\nCREATE MODEL statement and specify the\ninput columns used to train `FROM` (features) and what we want to\n`PREDICT` (labels).\nThe `eyeDetection` column is our target variable. The interesting thing about\nthis example is that we aim to forecast labels that are not strictly\nnumerical. Even though this example is simple (because the variable is a binary\ncategory), this can easily be generalized to more than two categories.\nWe order the measurements by the `Timestamps` column that shows readings\nfrequency of approximately 8 miliseconds.\n`sql\nCREATE MODEL mindsdb.eeg_eye_forecast\nFROM files\n  (SELECT * FROM eeg_eye)\nPREDICT eyeDetection\nORDER BY Timestamps\nWINDOW 50\nHORIZON 10;`\nAs the sampling frequency is 8 ms, this predictor is trained using a historical\ncontext of roughly 400 ms (`(50 * 8) = 400 [ms]`) to predict the following 80 ms\n(`(10 * 8) = 80 [ms]`).\nStatus of a Predictor\nA predictor may take a couple of minutes for the training to complete. You can\nmonitor the status of the predictor by using this SQL command:\n`sql\nSELECT status\nFROM mindsdb.models\nWHERE name='eeg_eye_forecast';`\nIf we run it right after creating a predictor, we get this output:\n`sql\n+------------+\n| status     |\n+------------+\n| generating |\n+------------+`\nA bit later, this is the output:\n`sql\n+----------+\n| status   |\n+----------+\n| training |\n+----------+`\nAnd at last, this should be the output:\n`sql\n+----------+\n| status   |\n+----------+\n| complete |\n+----------+`\nNow, if the status of our predictor says `complete`, we can start making\npredictions!\nMaking Predictions\nYou can make predictions by querying the predictor joined with the data table.\nThe SELECT statement lets you make predictions for the\nlabel based on the chosen features for a given time period. Usually, you want to\nknow what happens right after the latest training data point that was fed. We\nhave a special keyword for that, the `LATEST` keyword.\nLet's run a query to get predictions for the next `HORIZON` timesteps into\nthe future, which in this case is roughly 80 miliseconds.\n`sql\nSELECT m.Timestamps as timestamps,\n       m.eyeDetection as eye_status\nFROM files.eeg_eye as t\nJOIN mindsdb.eeg_eye_forecast as m\nWHERE t.timestamps > LATEST\nLIMIT 10;`\nOn execution, we get:\n```sql\n```\nThat's it. We can now `JOIN` any set of `WINDOW` rows worth of\nmeasurements with this predictor, and forecasts will be emitted to help us\nexpect a change in the state of the patient's eye based on the EEG readings.\nAlternate Problem Framings\nIt is also possible to reframe this task as a normal forecasting scenario\nwhere the variable is numeric. There are a few options here. It boils down to\nwhat the broader scenario is and what format would maximize the value of any\nspecific prediction.\nFor example, a simple mapping of `eye is open` to `0` and `eye is closed` to `1`\nwould be enough to replicate the above behavior.\nWe could also explore other options. With some data transformations on the data\nlayer, we could get a countdown to the next change in state, effectively\npredicting a date if we cast this back into the timestamp domain.\nWhat's Next?\nHave fun while trying it out yourself!\n\nBookmark MindsDB repository on GitHub.\nSign up for a free MindsDB account.\nEngage with the MindsDB community on\n  Slack or\n  GitHub to ask questions and\n  share your ideas and thoughts.\n\nIf this tutorial was helpful, please give us a GitHub star",
    "tag": "mindsdb"
  },
  {
    "title": "Using MindsDB Machine Learning to Solve a Real-World **time series** Problem",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/tutorials/mindsdb-superset-snowflake.mdx",
    "content": "Using MindsDB Machine Learning to Solve a Real-World time series Problem\nLet\u2019s use these powerful AI tables in a real-world scenario. (if you are not familiar with AI-Tables, you can learn about them in here. \nImagine that you are a data analyst at the Chicago Transit Authority. Every day, you need to optimize the number of buses per route to avoid overcrowded or empty buses. You need machine learning to forecast the number of rides per bus, per route, and by time of day. The data you have looks like the table below with route_id, timestamp, number of rides, and day-type (W = weekend)\n\nThis is a difficult machine learning problem that is common in databases. A timestamp indicates that we are dealing with the time-series problem. The data is further complicated by the type of day (day-type) the row contains and this is called multivariate. Additionally, there is high-cardinality as each route will have multiple row entries each with different timestamps, rides, and day types.\nLet\u2019s see how we can use machine learning with MindsDB to optimize the number of buses per route and visualize the results.\nSet Up MindsDB\nFirst things first! You need to connect your database to MindsDB. One of the easy ways to do so is to create a MindsDB cloud account. If you prefer to deploy MindsDB locally, please refer to installation instructions via Docker or PyPI.\nOnce an account is created you can connect to Snowflake using standard parameters like database name (in this case the Chicago Transit Authority), host, port, username, password, etc.\n\nConnect MindsDB to the Data for model training\nMindsDB works through a MySQL Wire protocol. Therefore, you can connect to it using any MySQL client. Here, we\u2019ll use the DBeaver database client and can see the Snowflake databases we are connected to.\n\nStep 1: Getting the Training Data\nWe start by getting the training data from the database that we connected to our MindsDB cloud account. It is always good to first make sure that all the databases are present and the connections correct.\n`sql\nshow databases;`\n\nMindsDB comes with some built-in databases as follows:\n\nINFORMATION_SCHEMA stores information about MindsDB,\nMINDSDB stores metadata about the predictors and allows access to the created predictors as tables,\nDATASOURCE for connecting to data or uploading files.\n\nThe SNF database is the database of the Chicago Transit Authority that we connected. It provides us with the training data. Let\u2019s check it.\n`sql\nSELECT *\nFROM CHICAGO_TRANSIT_AUTHORITY.PUBLIC.CTA_BUS_RIDES_LATEST\nLIMIT 100;`\n\nThe training data consists of the number of rides per bus route and day. For example, on 2001-07-03, there were 7354 rides on bus route 3.\nYou can download the dataset here and execute the SQL commands along with the tutorial!\nStep 2: Training the Predictive Model\nLet\u2019s move on to the next step, which is training the predictive model. For that, we\u2019ll use the MINDSDB database.\n`sql\nuse mindsdb;\nshow tables`\n\nMINDSDB database comes with the predictors and commands tables. The predictors table lets us see the status of our predictive models. For example, assuming that we have already trained our predictive model for forecasting the number of rides, we\u2019ll see the following.\n`sql\nSELECT name, status FROM MINDSDB.PREDICTORS;`\n\nThe process of training a predictive model using MindsDB is as simple as creating a view or a table.\n`sql\nCREATE PREDICTOR mindsdb.rides_forecaster_demo FROM snf (\nSELECT ROUTE, RIDES, DATE\nFROM CHICAGO_TRANSIT_AUTHORITY.PUBLIC.CTA_BUS_RIDES_LATEST WHERE DATE > '2020-01-01') \nPREDICT RIDES ORDER BY DATE GROUP BY ROUTE\nWINDOW 10 HORIZON 7;`\nLet\u2019s discuss the statement above. We create a predictor table using the `CREATE PREDICTOR` statement and specifying the database from which the training data comes. The code in `yellow` selects the filtered training data. After that, we use the `PREDICT` keyword to define the column whose data we want to forecast.\nNext, there are standard SQL clauses, such as `ORDER BY, GROUP BY, WINDOW, and HORIZON`. We use the `ORDER BY` clause and the DATE column as its argument. By doing so, we emphasize that we deal with a time-series problem. We order the rows by date. The `GROUP BY` clause divides the data into partitions. Here, each of them relates to a particular bus route. We take into account just the last ten rows for every given prediction. Hence, we use `WINDOW` 10. To prepare the forecast of the number of bus rides for the next week, we define `HORIZON` 7.\nNow, you can execute the CREATE PREDICTOR statement and wait until your predictive model is complete. The MINDSDB.PREDICTORS table stores its name as rides_forecaster_demo and its status as training. Once your predictive model is ready, the status changes to complete.\nStep 3: Getting the Forecasts\nWe are ready to go to the last step, i.e., using the predictive model to get future data. One way is to query the rides_forecaster_demo predictive model directly. Another way is to join this predictive model table to the table with historical data before querying it.\nWe consider a time-series problem. Therefore, it is better to join our predictive model table to the table with historical data.\n`sql\nSELECT tb.ROUTE, tb.RIDES AS PREDICTED_RIDES\nFROM snf.PUBLIC.CTA_BUS_RIDES_LATEST AS ta\nJOIN mindsdb.rides_forecaster_demo AS tb \nWHERE ta.ROUTE = \"171\" AND ta.DATE > LATEST\nLIMIT 7;`\nLet\u2019s analyze it. We join the table that stores historical data (i.e., snf.PUBLIC.CTA_BUS_RIDES_LATEST) to our predictive model table (i.e., mindsdb.rides_forecaster_demo). The queried information is the route and the predicted number of rides per route. And the usage of the condition ta.DATE > LATEST (provided by MindsDB) ensures that we get the future number of rides per route.\nLet\u2019s run the query above to forecast the number of rides for route 171 in the next seven days.\n\nNow we know the number of rides for route 171 in the next seven days. We could do it in the same way for all the other routes.\nThanks to the special SQL syntax that includes CREATE PREDICTOR, PREDICT, and > LATEST, MindsDB makes it straightforward to run predictors on our chosen data.\nNow, let\u2019s visualize our predictions.\nVisualizing the Results using Apache Superset\nApache Superset is a modern, open-source data exploration and visualization platform designed for all data personas in an organization. Superset ships with a powerful SQL editor and a no-code chart builder experience. Superset ships with support for most SQL databases out of the box and over 50 visualization types.\nYou can connect to the Snowflake database or your MindsDB database that has a Snowflake connection within. Upon starting up your Superset workspace, your earlier defined database connection is ready to use! So you have access to the Chicago Transit Authority data, as well as to the predictions made by MindsDB.\nVisualizing Data\nThe two data sets that we are relevant for visualization are the stops_by_route and forecasts data sets. The stops_by_route data set contains the exact location of each bus stop for each bus route. And the forecasts data set stores the actual and predicted number of rides, confidence interval, and lower and upper bounds of prediction, per route and timestamp.\nSuperset lets us visualize the stops_by_route data set as follows.\n\nEvery bus route has a different color. Also, there is volatility associated with each bus route. Let\u2019s publish this chart to a new dashboard by clicking the +Save button, then switch to the Save as tab, and then type in \u201cRoutes Dashboard\u201d in the Add to Dashboard field.\nNow, let\u2019s craft a time-series line chart to visualize actual vs predicted riders. Let\u2019s look at the chart that presents the actual number of bus riders (in blue) and the predicted number of bus rides (in purple).\n\nPredictions made by MindsDB closely resemble the actual data, except for a short time during March 2020 when the large-scale lockdowns took place. There we see a sudden drop in the number of bus rides. But MindsDB took some time to cope with this new reality and adjust its predictions.\nLastly, let\u2019s add a data zoom to this chart for end-users to zoom in on specific date ranges. Click the Customize tab and then click Data Zoom to enable it. Then, click the + Save button and publish to the same \u201cRoutes Dashboard\u201d.\nLet\u2019s head over to the dashboard now and customize it to make it more dynamic and explorable. Click Dashboards in the top nav bar and then select \u201cRoutes Dashboard\u201d from the list of dashboards. You can rearrange the chart positions by clicking the pencil icon, dragging the corners of the chart objects, and then clicking Save.\n\nLet\u2019s add some dashboard filters to this dashboard so dashboard consumers can filter the charts down to specific bus routes and volatility values. Click the right arrow (->) to pop open the filter tray. Then select the pencil icon to start editing this dashboard\u2019s filters. Create the following filters with appropriate filter names:\n\nA Value filter on the route column from the forecasts table.\nA Numerical range filter on the volatility column from the stops_by_route table.\n\nClick Save to publish these filters.\n\n\nLet\u2019s give these filters for a test ride! Use the routes filter to only show information for routes 1, 100, and 1001.\n\nWe could zoom in to see the time during the first large-scale lockdowns in March 2020. For these particular routes, the predictions made by MindsDB are not so far off.\n\nNow, let\u2019s use our volatility filter to view only the routes with volatility values greater than 55.\n\nConclusions: Powerful forecasting with MindsDB, your database, and Superset\nThe combination of MindsDB and your database covers all the phases of the ML lifecycle. And Superset helps you to visualize the data in any form of diagrams, charts, or dashboards.\n\nMindsDB provides easy-to-use predictive models through AI Tables. You can create these predictive models using SQL statements and feeding the input data. Also, you can query them the same way you query a table. The easiest way to get started with Superset is with the free tier for Preset Cloud, a hassle-free and fully hosted cloud service for Superset.\nWe encourage you to try some predictions with your own data, so please sign up for a free MindsDB cloud account and if you need any help with MindsDB, feel free to ask our Slack and Github communities.\nWhat's Next?\nHave fun while trying it out yourself!\n\nBookmark MindsDB repository on GitHub.\nSign up for a free MindsDB account.\nEngage with the MindsDB community on Slack or GitHub to ask questions and share your ideas and thoughts.\n",
    "tag": "mindsdb"
  },
  {
    "title": "Pre-requisites",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/tutorials/insurance-cost-prediction.mdx",
    "content": "\ntitle: Predict Insurance Cost using MindsDB\nsidebarTitle: Insurance Cost\n\nDataset: Medical Cost Personal Data\nCommuntiy Author: Kinie K Kusuma\nPre-requisites\nFirst, you need MindsDB installed. So please make sure you've visited\nGetting Started Guide and Getting Started with Cloud.\nYou may start to use MindsDB by installing it locally or you can use the\nCloud service. Let\u2019s use the cloud for this\ntutorial. Second, you need a MySQL client to connect to the MindsDB MySQL API.\nCan you accurately predict insurance costs?\nIn this tutorial, you will learn how to predict insurance costs using MindsDB.\nThis tutorial is very easy because you don't need to learn any machine learning\nalgorithms, all you need to know is just SQL.\nThe process looks like the following: First we will connect MindsDB to a\ndatabase with past data so it can learn from it We will use a single SQL command\nthat will tell MindsDB to train its predictor We will use the standard SQL\nSelect statement to get predictions from AI Tables in MindsDB. Like if this data\nalready exists!\nMindsDB will execute a complete Machine Learning workflow behind the scenes, it\nwill determine data types for each column, normalize and encode it, train and\ntest ML model. All this happens automatically, so it is very cool! Those who\nwant to get their hands dirty with manual hyperparameters optimization, you can\nalso do that with MindsDB using a declarative syntax called JSON-AI.\nSo let's look at how it works using a real use case. For the demo purpose we\nwill use a public dataset from Kaggle, but you are free to follow this tutorial\nwith your own data.\nConnect your database\nFirst, you need to connect MindsDB to the database where the data is stored.\nOpen MindsDB GUI and in the left navigation click on Database, then click on the\nADD DATABASE. Here, you need to provide all of the required parameters for\nconnecting to the database.\n\n\nSupported Database - select the database that you want to connect to\nIntegrations Name - add a name to the integration, here I'm using 'mysql' but\n  you can name it differently\nDatabase - the database name\nHost - database hostname\nPort - database port\nUsername - database user\nPassword - user's password\n\nThen, click on CONNECT.\nThe next step is to use the MySQL client to connect to MindsDB\u2019s MySQL API,\ntrain a new model, and make a prediction.\nConnect to MindsDB\u2019s MySQL API\nHere I'm using MySQL command-line client, but you can also follow up with the\none that works the best for you, like Dbeaver.\nThe first step is to use the MindsDB Cloud user to connect to the MindsDB MySQL\nAPI, using this command:\n\nYou need to specify the hostname and user name explicitly, as well as a password\nfor connecting. Click enter and you are connected to MindsDB API.\n\nIf you have an authentication error, please make sure you are providing the\nemail address you have used to create an account on MindsDB Cloud.\nData\nNow, let's show the databases.\n\nThere are 4 databases, and the MySQL database is the database that I've\nconnected to MindsDB.\nLet's check the MySQL database.\n\nThere are 3 tables, and because the tutorial is about insurance cost prediction,\nwe will use the insurance table.\nLet's check what is inside this table.\n\nSo, these tables have 7 columns:\n\nage: The age of the person (integer)\nsex: Gender (male or female)\nbmi: Body mass index is a value derived from the mass and height of a person.\n  The BMI is defined as the body mass divided by the square of the body height,\n  and is expressed in units of kg/m\u00b2, resulting from mass in kilograms and\n  height in meters (float)\nchildren: The number of children (integer)\nsmoker: Indicator if the person smoke (yes or no)\nregion: Region where the insured lives (southeast, northeast, southwest or\n  northwest)\ncharges: The insurance cost, this is the target of prediction (float)\n\nCreate the model\nNow, to create the model, let's move to the MindsDB database, and see what's\ninside.\n\nThere are 2 tables, predictors, and commands. Predictors contain your predictors\nrecord, and commands contain your last commands used.\nTo train a new machine learning model we will need to CREATE PREDICTOR as a new\nrecord inside the predictors table, and using this command:\n`sql\nCREATE PREDICTOR mindsdb.predictor_name\nFROM integration_name\n    (SELECT column_name, column_name2 FROM table_name)\nPREDICT column_name;`\nThe values that we need to provide are:\n\npredictor_name (string) - The name of the model.\nintegration_name (string) - The name of the connection to your database.\nds_name (string) - the name of the dataset you want to create, it's optional\n  if you don't specify this value MindsDB will generate by itself.\ncolumn_name (string) - The feature you want to predict.\ncolumn_alias - Alias name of the feature you want to predict.\n\nSo, use this command to create the models:\n`sql\nCREATE PREDICTOR insurance_cost_predictor\nFROM insurance_costs\n    (SELECT * FROM insurance)\nPREDICT charges;`\n\nIf there's no error, that means your model is created and training has started.\nTo see if your model is finished, use this command:\n`sql\nSELECT *\nFROM mindsdb.predictors\nWHERE name = predictor_name;`\nAnd values that we need to provide are:\n\npredictor_name (string) - The name of the model.\n\n`sql\nSELECT *\nFROM mindsdb.predictors\nWHERE name='insurance_cost_predictor';`\n\nIf the predictor is ready, it will look like this. The model has been created\nand trained! The reported accuracy is 75%. If you want to have more control over\nthe model, head to lightwood.io to see how that can be customized.\nMake prediction\nNow you are in the last step of this tutorial, making the prediction. To make a\nprediction you can use this command:\n`sql\nSELECT target_variable, target_variable_explain\nFROM model_table\nWHERE column3=\"value\"\nAND column2=value;`\nYou need to set these values:\n\ntarget_variable - The original value of the target variable.\ntarget_variable_confidence - Model confidence score.\ntarget_variable_explain - JSON object that contains additional information as\n  confidence_lower_bound, confidence_upper_bound, anomaly, truth.\nwhen_data - The data to make the predictions from(WHERE clause params).\n\n`sql\nSELECT charges, charges_confidence, charges_explain AS info\nFROM insurance_cost_predictor\nWHERE age=20\nAND sex='male'\nAND bmi=33.20\nAND children=0\nAND smoker='no'\nAND region='southeast';`\n\nFinally, we have trained an insurance model using SQL and MindsDB.\nConclusions\nAs you can see it is very easy to start making predictions with machine learning\neven without being a data scientist! Feel free to check this yourself, MindsDB\nhas an option of a\nfree cloud account",
    "tag": "mindsdb"
  },
  {
    "title": "Create a predictor",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/tutorials/crop-prediction.mdx",
    "content": "\ntitle: Crop Recommendation\nDataset: Crop recommendation \nData\nCommuntiy Author: pixpack\nModern agriculture is becoming very dependent on technology. From advanced\nmachinery to specially selected crops. All the technology produces a lot of data\nthat can be used for better adjustment of the farming process. One use case of\nmachine learning in agriculture could be the selection of the best crop for a\nspecific field to maximize the potential yield. Such problems are often called\nClassification Problems in machine learning. With MindsDB you can easily\nmake automated machine learning predictions straight from your existing\ndatabase. Even without advanced ML engineering skills, you can start leveraging\npredictive models that help you make better business decisions.\nIn this tutorial, you will learn how to predict the best crop type based on field parameters using MindsDB.\nPre-requisites\nBefore you start make sure you have:\n\n\nAccess to MindsDB. In this tutorial, we will use\n   MindsDB Cloud GUI. If you want you can also\n   deploy mindsdb on your premises, Check out the installation guide for\n   Docker or\n   PyPi.\n\n\nDownloaded the dataset. You can get it from\n   Kaggle.\n\n\nAdd your file to MindsDB\nMindsDB can integrates with many databases, in most scenarios your data will be\nstored in a database, if you decide to load this dataset into your database of\nchoice, please follow instructions here as to how to connect mindsdb to your\ndatabase.\nIn this tutorial, we will be adding the dataset directly to MindsDB's GUI. For\nthis example MindsDB Cloud GUI will be used.If you\nneed to create an account you can find the guide on how to do it\nhere. Alternatively, you can also use\nMindsDB's local deployment and access the GUI in your browser with\n127.0.0.1:47334.\nThe first step will be to access MindsDB cloud where we will also make use of\nthe SQL Editor:\n\nOnce you are logged onto the Cloud GUI,navigate to either the `Add Data`\n  button or the plug icon on the left side bar to select it.\nThe screen will navigate to the `Select your datasource` page, select the\n  option Files.\n\n\n\nSelect Import Files.\n\n\n\nClick on `Import a File` to select your dataset from your local drive. Your\n  dataset should be a maximum size of 10MB.\nUnder `Table name` type in the name you would like to give your dataset which\n  will be stored in MindsDB files.\n\nOnce the dataset has been successfully uploaded into a table, you can query the\ndataset directly from the table.\nIn the SQL Editor,type in the following syntax and select the button `Run` or\nShift+Enter to execute the code:\n`sql\nSELECT *\nFROM files.crops;`\n\nThis confirms that the dataset has been successfully uploaded with all its rows.\nCreate a predictor\nNow we can create a machine learning model with `crops` columns serving as\nfeatures, and MindsDB takes care of the rest of the ML workflow automatically.\nThere is a way to get your hands into the insides of the model to fine tune it,\nbut we will not cover it in this tutorial.\nIn the SQL Editor, type in the below syntax to create and train a machine\nlearning predictive model:\n`sql\nCREATE PREDICTOR crop_predictor\nFROM files\n    (SELECT * FROM crops)\nPREDICT label;`\nSelect the button `Run` or Shift+Enter to execute the code. If the predictor is\nsuccessfully created the console will display a message\n`Query successfully completed`.\n\nNow the predictor will begin training. You can check the status of the predictor\nwith the following query.\n`sql\nSELECT *\nFROM mindsdb.predictors\nWHERE name='crop_predictor';`\nAfter the predictor has finished training, you will see a similar output. Note\nthat MindsDB does model testing for you automatically, so you will immediately\nsee if the predictor is accurate enough.\n\nYou are now done with creating the predictor! \u2728\nMake predictions\nIn this section you will learn how to make predictions using your trained model.\nTo run a prediction against new or existing data, you can use the following\nquery.\n`sql\nSELECT label\nFROM mindsdb.crop_predictor\nWHERE N = 77\nAND P = 52\nAND K = 17\nAND temperature = 24\nAND humidity = 20.74\nAND ph = 5.71\nAND rainfall = 75.82;`\n\nAs we have used a real data point from our dataset we can verify the prediction.\n`text\nN,  P,  K,  temperature,  humidity,   ph,           rainfall,     label\n77, 52, 17, 24.86374934,  65.7420046, 5.714799723,  75.82270467,  maize`\nAs you can see, the model correctly predicted the most appropriate crop type for\nour field.\nOK, we made a prediction using a single query, but what if you want to make a\nbatch prediction for a large set of data in your database? In this case, MindsDB\nallows you to Join this other table with the Predictor. In result, you will get\nanother table as an output with a predicted value as one of its columns.\nLet\u2019s see how it works.\nUse the following command to create the batch prediction and execute with the\n`Run` button.\n`sql\nSELECT\n    collected_data.N,\n    collected_data.P,\n    collected_data.K,\n    collected_data.temperature,\n    collected_data.humidity,\n    collected_data.ph,\n    collected_data.rainfall,\n    predictions.label AS predicted_crop_type\nFROM crops_integration.crops AS collected_data\nJOIN mindsdb.crop_predictor AS predictions\nLIMIT 5;`\nAs you can see below, the predictor has made multiple predictions for each data\npoint in the `collected_data` table. You can also try selecting other fields to\nget more insight on the predictions. See the\nJOIN clause documentation for more\ninformation.\n\nYou are now done with the tutorial! \ud83c\udf89\nPlease feel free to try it yourself. Sign up for a\nfree MindsDB account\nto get up and running in 5 minutes, and if you need any help, feel free to ask\nin\nSlack\nor Github.\nFor more check out other",
    "tag": "mindsdb"
  },
  {
    "title": "AI Tables Intro",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/tutorials/ai-tables.mdx",
    "content": "AI Tables Intro\nThere is an ongoing transformational shift within the modern business world from the \u201cwhat happened and why\u201d based on historical data analysis to the \u201cwhat will we predict can happen and how can we make it happen\u201d based on machine learning predictive modeling.\n\nThe success of your predictions depends both on the data you have available and the models you train this data on. Data Scientists and Data Engineers need best-in-class tools to prepare the data for feature engineering, the best training models, and the best way of deploying, monitoring, and managing these implementations for optimal prediction confidence.\nMachine Learning (ML) Lifecycle\nThe ML lifecycle can be represented as a process that consists of the data preparation phase, modeling phase, and deployment phase. The diagram below presents all the steps included in each of the stages.\n\nCompanies looking to implement machine learning have found their current solutions require substantial amounts of data preparation, cleaning, and labeling, plus hard to find machine learning/AI data scientists to conduct feature engineering; build, train, and optimize models; assemble, verify, and deploy into production; and then monitor in real-time, improve, and refine. Machine learning models require multiple iterations with existing data to train. Additionally, extracting, transforming, and loading (ETL) data from one system to another is complicated, leads to multiple copies of information, and is a compliance and tracking nightmare.\nA recent study has shown it takes 64% of companies a month, to over a year, to deploy a machine learning model into production\u00b9. Leveraging existing databases and automating the feature engineering, building, training, and optimization of models, assembling them, and deploying them into production is called AutoML and has been gaining traction within enterprises for enabling non-experts to use machine learning models for practical applications.\n\nMindsDB brings machine learning to existing SQL databases with a concept called AI Tables. AI Tables integrate the machine learning models as virtual tables inside a database, create predictions, and can be queried with simple SQL statements. Almost instantly, time series, regression, and classification predictions can be done directly in your database.\nDeep Dive into the AI Tables\nLet\u2019s consider the following income table that stores the income and debt values.\n`sql\nSELECT income, debt FROM income_table;`\n\nA simple visualization of the data present in the income table is as follows.\n\nQuerying the income table to get the debt value for a particular income value results in the following.\n`sql\nSELECT income, debt FROM income\nWHERE income = 80000;`\n\n\nBut what happens when we query the table for income value that is not present?\n`sql\nSELECT income, debt FROM income WHERE income = 90000;`\n\n\nWhen a table doesn\u2019t have an exact match the query will return a null value. This is where the AI Tables come into play!\nLet\u2019s create a debt model that allows us to approximate the debt value for any income value. We\u2019ll train this debt model using the income table\u2019s data.\n`sql\nCREATE PREDICTOR mindsdb.debt_model FROM income_table PREDICT debt;`\nMindsDB provides the CREATE PREDICTOR statement. When we execute this statement, the predictive model works in the background, automatically creating a vector representation of the data that can be visualized as follows.\n\nLet\u2019s now look for the debt value of some random income value. To get the approximated debt value, we query the debt_model and not the income table.\n`sql\nSELECT income, debt FROM debt_model WHERE income = 90120;`\n",
    "tag": "mindsdb"
  },
  {
    "title": "Bring Your Own Model",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/tutorials/byom.mdx",
    "content": "Bring Your Own Model\nIntroduction\nMindsDB allows you to integrate your own machine learning models into it.\nIn order to do this your model will require some sort of API wrapper, for now we have two API specifications we support: MLflow and Ray Serve.\nThe former supports importing already trained models and predicting with them from mindsdb. The later supports both training and predicting with external models.\nIn order to use custom models there are three mandatory arguments one must past inside the `USING` statement:\n- `url.predict`, this is the url to call for getting predictions from your model\n- `format`, this can be either `mlflow` or `ray_serve`\n- `dtype_dict`, this is a JSON specifying all columns expected by their models, and their respective data types. For now, the mapping supports data types used by lightwood, our AutoML library.\nThere's an additional optional argument if you want to train the model via MindsDB (only for Ray Serve):\n- `url.train`, which is the endpoint that will be called to train your model\n\n1. Ray Serve\n1.1 Simple example - Logistic regression\nRay serve is a simple high-throughput service that can wrap over your own ml models. In this example, we will train and predict with an external scikit-learn model. First, let's look at the actual model wrapped inside a class that complies with the above requirements:\n```python\nimport ray\nfrom fastapi import Request, FastAPI\nfrom ray import serve\nimport time\nimport pandas as pd\nimport json\nfrom sklearn.linear_model import LogisticRegression\napp = FastAPI()\nray.init()\nserve.start(detached=True)\nasync def parse_req(request: Request):\n    data = await request.json()\n    target = data.get('target', None)\n    di = json.loads(data['df'])\n    df = pd.DataFrame(di)\n    return df, target\n@serve.deployment(route_prefix=\"/my_model\")\n@serve.ingress(app)\nclass MyModel:\n    @app.post(\"/train\")\n    async def train(self, request: Request):\n        df, target = await parse_req(request)\n        feature_cols = list(set(list(df.columns)) - set([target]))\n        self.feature_cols = feature_cols\n        X = df.loc[:, self.feature_cols]\n        Y = list(df[target])\n        self.model = LogisticRegression()\n        self.model.fit(X, Y)\n        return {'status': 'ok'}\n\n\n```@app.post(\"/predict\")\nasync def predict(self, request: Request):\n    df, _ = await parse_req(request)\n    X = df.loc[:, self.feature_cols]\n    predictions = self.model.predict(X)\n    pred_dict = {'prediction': [float(x) for x in predictions]}\n    return pred_dict\n```\n\n\nMyModel.deploy()\nwhile True:\n    time.sleep(1)\n```\nThe important bits here are having train and predict endpoints.\nThe `train` endpoint accept two parameters in the JSON sent via POST:\n- `df` -- a serialized dictionary that can be converted into a pandas dataframe\n- `target` -- the name of the target column\nThe `predict` endpoint needs only one parameter:\n- `df` -- a serialized dictionary that can be converted into a pandas dataframe\nThe training endpoints must return a JSON that contains the keys `status` set to `ok`. The predict endpoint must return a dictionary containing the `prediction` key, storing the predictions. Additional keys can be returned for confidence and confidence intervals.\nOnce you start this RayServe-wrapped model you can train it using a query like this one:\n`sql\nCREATE PREDICTOR mindsdb.byom_ray_serve\nFROM mydb (\n    SELECT number_of_rooms, initial_price, rental_price \n    FROM test_data.home_rentals\n) \nPREDICT number_of_rooms\nUSING\nurl.train = 'http://127.0.0.1:8000/my_model/train',\nurl.predict = 'http://127.0.0.1:8000/my_model/predict',\ndtype_dict={\"number_of_rooms\": \"categorical\", \"initial_price\": \"integer\", \"rental_price\": \"integer\"},\nformat='ray_server';`\nAnd you can query predictions as usual, either by conditioning on a subset of input colums:\n`sql\nSELECT * FROM byom_ray_serve WHERE initial_price=3000 AND rental_price=3000;`\nOr by `JOINING` to do batch predicions:\n`sql\nSELECT tb.number_of_rooms, t.rental_price FROM mydb.test_data.home_rentals AS t JOIN mindsdb.byom_ray_serve AS tb WHERE t.rental_price > 5300;`\nPlease note that, if your model is behind a reverse proxy (e.g. nginx) you might have to increase the maximum limit for POST requests in order to receive the training data. MindsDB itself can send as much as you'd like and has been stress-tested with over a billion rows.\n1.2. Example - Keras NLP model\nFor this example, we will consider a natural language processing (NLP) task where we want to train a neural network with Keras to detect if a tweet is related to a natural disaster (fires, earthquakes, etc.). Please download this dataset to follow the example.\nThe code for the model here is a bit more complex than in section 1.1, but the same rules apply: we create a Ray Server based service that wraps around a Kaggle NLP Model which can be trained and then used for predictions:\n`python\nimport re\nimport time\nimport json\nimport string\nimport requests\nfrom collections import Counter, defaultdict\n\u200b\nimport ray\nfrom ray import serve\n\u200b\nimport gensim\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom nltk.util import ngrams\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom fastapi import Request, FastAPI\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\n\u200b\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\nfrom tensorflow.keras.initializers import Constant\nfrom tensorflow.keras.optimizers import Adam\n\u200b\napp = FastAPI()\nstop = set(stopwords.words('english'))\n\u200b\n\u200b\nasync def parse_req(request: Request):\n    data = await request.json()\n    target = data.get('target', None)\n    di = json.loads(data['df'])\n    df = pd.DataFrame(di)\n    return df, target\n\u200b\n\u200b\n@serve.deployment(route_prefix=\"/nlp_kaggle_model\")\n@serve.ingress(app)\nclass Model:\n    MAX_LEN = 100\n    GLOVE_DIM = 50\n    EPOCHS = 10\n\u200b\n    def __init__(self):\n        self.model = None\n\u200b\n    @app.post(\"/train\")\n    async def train(self, request: Request):\n        df, target = await parse_req(request)\n\u200b\n        target_arr = df.pop(target).values\n        df = self.preprocess_df(df)\n        train_corpus = self.create_corpus(df)\n\u200b\n        self.embedding_dict = {}\n        with open('./glove.6B.50d.txt', 'r') as f:\n            for line in f:\n                values = line.split()\n                word = values[0]\n                vectors = np.asarray(values[1:], 'float32')\n                self.embedding_dict[word] = vectors\n        f.close()\n\u200b\n        self.tokenizer_obj = Tokenizer()\n        self.tokenizer_obj.fit_on_texts(train_corpus)\n\u200b\n        sequences = self.tokenizer_obj.texts_to_sequences(train_corpus)\n        tweet_pad = pad_sequences(sequences, maxlen=self.__class__.MAX_LEN, truncating='post', padding='post')\n        df = tweet_pad[:df.shape[0]]\n\u200b\n        word_index = self.tokenizer_obj.word_index\n        num_words = len(word_index) + 1\n        embedding_matrix = np.zeros((num_words, self.__class__.GLOVE_DIM))\n\u200b\n        for word, i in tqdm(word_index.items()):\n            if i > num_words:\n                continue\n\u200b\n            emb_vec = self.embedding_dict.get(word)\n            if emb_vec is not None:\n                embedding_matrix[i] = emb_vec\n\u200b\n        self.model = Sequential()\n        embedding = Embedding(num_words,\n                              self.__class__.GLOVE_DIM,\n                              embeddings_initializer=Constant(embedding_matrix),\n                              input_length=self.__class__.MAX_LEN,\n                              trainable=False)\n        self.model.add(embedding)\n        self.model.add(SpatialDropout1D(0.2))\n        self.model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n        self.model.add(Dense(1, activation='sigmoid'))\n\u200b\n        optimzer = Adam(learning_rate=1e-5)\n        self.model.compile(loss='binary_crossentropy', optimizer=optimzer, metrics=['accuracy'])\n\u200b\n        X_train, X_test, y_train, y_test = train_test_split(df, target_arr, test_size=0.15)\n        self.model.fit(X_train, y_train, batch_size=4, epochs=self.__class__.EPOCHS, validation_data=(X_test, y_test), verbose=2)\n\u200b\n        return {'status': 'ok'}\n\u200b\n    @app.post(\"/predict\")\n    async def predict(self, request: Request):\n        df, _ = await parse_req(request)\n\u200b\n        df = self.preprocess_df(df)\n        test_corpus = self.create_corpus(df)\n\u200b\n        sequences = self.tokenizer_obj.texts_to_sequences(test_corpus)\n        tweet_pad = pad_sequences(sequences, maxlen=self.__class__.MAX_LEN, truncating='post', padding='post')\n        df = tweet_pad[:df.shape[0]]\n\u200b\n        y_pre = self.model.predict(df)\n        y_pre = np.round(y_pre).astype(int).flatten().tolist()\n        sub = pd.DataFrame({'target': y_pre})\n\u200b\n        pred_dict = {'prediction': [float(x) for x in sub['target'].values]}\n        return pred_dict\n\u200b\n    def preprocess_df(self, df):\n        df = df[['text']]\n        df['text'] = df['text'].apply(lambda x: self.remove_URL(x))\n        df['text'] = df['text'].apply(lambda x: self.remove_html(x))\n        df['text'] = df['text'].apply(lambda x: self.remove_emoji(x))\n        df['text'] = df['text'].apply(lambda x: self.remove_punct(x))\n        return df\n\u200b\n    def remove_URL(self, text):\n        url = re.compile(r'https?://\\S+|www\\.\\S+')\n        return url.sub(r'', text)\n\u200b\n    def remove_html(self, text):\n        html = re.compile(r'<.*?>')\n        return html.sub(r'', text)\n\u200b\n    def remove_punct(self, text):\n        table = str.maketrans('', '', string.punctuation)\n        return text.translate(table)\n\u200b\n    def remove_emoji(self, text):\n        emoji_pattern = re.compile(\"[\"\n                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                                   u\"\\U00002702-\\U000027B0\"\n                                   u\"\\U000024C2-\\U0001F251\"\n                                   \"]+\", flags=re.UNICODE)\n        return emoji_pattern.sub(r'', text)\n\u200b\n    def create_corpus(self, df):\n        corpus = []\n        for tweet in tqdm(df['text']):\n            words = [word.lower() for word in word_tokenize(tweet) if ((word.isalpha() == 1) & (word not in stop))]\n            corpus.append(words)\n        return corpus\n\u200b\n\u200b\nif __name__ == '__main__':\n\u200b\n    ray.init()\n    serve.start(detached=True)\n\u200b\n    Model.deploy()\n\u200b\n    while True:\n        time.sleep(1)`\nWe need access to the training data, so we'll create a table called `nlp_kaggle_train` to load the dataset that the original model uses.\nAnd ingest it into a table with the following schema:\n`sql\nid INT,\nkeyword VARCHAR(255),\nlocation VARCHAR(255),\ntext VARCHAR(5000),\ntarget INT`\nNote: specifics of the schema and how to ingest the csv will vary depending on your database.\nNext, we can register and train the above custom model using the following query:\n`sql\nCREATE PREDICTOR mindsdb.byom_ray_serve_nlp\nFROM maria (\n    SELECT text, target\n    FROM test.nlp_kaggle_train\n) PREDICT target\nUSING\nurl.train = 'http://127.0.0.1:8000/nlp_kaggle_model/train',\nurl.predict = 'http://127.0.0.1:8000/nlp_kaggle_model/predict',\ndtype_dict={\"text\": \"rich_text\", \"target\": \"integer\"},\nformat='ray_server';`\nTraining will take a while given that this model is a neural network rather than a simple logistic regression. You can check its status with the query `SELECT * FROM mindsdb.predictors WHERE name = 'byom_ray_serve_nlp';`, much like you'd do with a \"normal\" MindsDB predictor.\nOnce the predictor's status becomes `trained` we can query it for predictions as usual:\n`sql\nSELECT * FROM mindsdb.byom_ray_serve_nlp WHERE text='The tsunami is coming, seek high ground';`\nWhich would, hopefully, output `1`. Alternatively, we can try out this tweet to expect `0` as an output:\n`sql\nSELECT * FROM mindsdb.byom_ray_serve_nlp WHERE text='This is lovely dear friend';`\nIf your results do not match this example, it could help to train the model for a longer amount of epochs.\n\n2. MLFlow\n2.1 Simple example - Logistic Regression\nMLFlow is a tool that you can use to train and serve models, among other features like organizing experiments, tracking metrics, etc.\nGiven there is no way to train an MLflow-wrapped model using its API, you will have to train your models outside of MindsDB by pulling your data manually (i.e. with a script), ideally using a MLflow run or experiment.\nThe first step would be to create a script where you train a model and save it using one of the saving methods that MLflow exposes. For this example, we will use the model in this simple tutorial where the method is `mlflow.sklearn.log_model` (here), given that the model is built with scikit-learn.\nOnce trained, you need to make sure the model is served and listening for input in a URL of your choice (note, this can mean your model can run on a different machine than the one executing MindsDB). Let's assume this URL to be `http://localhost:5000/invocations` for now.\nThis means you would execute the following command in your terminal, from the directory where the model was stored:\n`mlflow models serve --model-uri runs:/<run-id>/model`\nWith `<run-id>` given in the output of the command `python train.py` used for actually training the model.\nNext, we're going to bring this model into MindsDB:\n`sql\nCREATE PREDICTOR mindsdb.byom_mlflow \nPREDICT `1`  -- `1` is the target column name\nUSING \nurl.predict='http://localhost:5000/invocations', \nformat='mlflow', \ndata_dtype={\"0\": \"integer\", \"1\": \"integer\"}`\nWe can now run predictions as usual, by using the `WHERE` statement or joining on a data table with an appropriate schema:\n`sql\nSELECT `1` FROM byom_mlflow WHERE `0`=2;`\n2.2. Advanced example - Keras NLP model\nSame use case as in section 1.2, be sure to download the dataset to reproduce the steps here. In this case, we will take a look at the best practices when your model needs custom data preprocessing code (which, realistically, will be fairly common).\nThe key difference is that we now need to use the `mlflow.pyfunc` module to both 1) save the model using `mlflow.pyfunc.save_model` and 2) subclass `mlflow.pyfunc.PythonModel` to wrap the model in an MLflow-compatible way that will enable our custom inference logic to be called.\nSaving the model\nIn the same script where you train the model (which you can find in the final section of 2.2) there should be a call at the end where you actually use mlflow to save every produced artifact:\n`python\nmlflow.pyfunc.save_model(\n    path=\"nlp_kaggle\",\n    python_model=Model(),\n    conda_env=conda_env,\n    artifacts=artifacts\n)`\nHere, `artifacts` will be a dictionary with all expected produced outputs when running the training phase. In this case, we want both a model and a tokenizer to preprocess the input text. On the other hand, `conda_env` specifies the environment under which your model should be executed once served in a self-contained conda environment, so it should include all required packages and dependencies. For this example, they look like this:\n```python\nthese will be accessible inside the Model() wrapper\nartifacts = {\n    'model': model_path,\n    'tokenizer_path': tokenizer_path,\n}\nspecs for environment that will be created when serving the model\nconda_env = {\n    'name': 'nlp_keras_env',\n    'channels': ['defaults'],\n    'dependencies': [\n        'python=3.8',\n        'pip',\n        {\n            'pip': [\n                'mlflow',\n                'tensorflow',\n                'cloudpickle',\n                'nltk',\n                'pandas',\n                'numpy',\n                'scikit-learn',\n                'tqdm',\n            ],\n        },\n    ],\n}\n```\nFinally, to actually store the model you need to provide the wrapper class that will 1) load all produced artifacts into an accessible \"context\" and 2) implement all required inference logic:\n```python\nclass Model(mlflow.pyfunc.PythonModel):\n    def load_context(self, context):\n        # we use paths in the context to load everything\n        self.model_path = context.artifacts['model']\n        self.model = load_model(self.model_path)\n        with open(context.artifacts['tokenizer_path'], 'rb') as f:\n            self.tokenizer = pickle.load(f)\n\n\n```def predict(self, context, model_input):\n    # preprocess input, tokenize, pad, and call the model\n    df = preprocess_df(model_input)\n    corpus = create_corpus(df)\n    sequences = self.tokenizer.texts_to_sequences(corpus)\n    tweet_pad = pad_sequences(sequences, \n                              maxlen=MAX_LEN, \n                              truncating='post', \n                              padding='post')\n    df = tweet_pad[:df.shape[0]]\n\n    y_pre = self.model.predict(df)\n    y_pre = np.round(y_pre).astype(int).flatten().tolist()\n\n    return list(y_pre)\n```\n\n\n```\nAs you can see, here we are loading multiple artifacts and using them to guarantee the input data will be in the same format that was used when training. Ideally, you would abstract this even further into a single `preprocess` method that is called both at training time and inference time.\nFinally, serving is simple. Go to the directory where you called the above script, and execute `mlflow models serve --model-uri\u00a0./nlp_kaggle`.\nAt this point, the rest is essentially the same as in the previous example. You can link the MLflow model with these SQL statements:\n`sql\nCREATE PREDICTOR mindsdb.byom_mlflow_nlp\nPREDICT `target`\nUSING \n    url.predict='http://localhost:5000/invocations',\n    format='mlflow',\n    dtype_dict={\"text\": \"rich text\", \"target\": \"binary\"};`\nTo get predictions, you can directly pass input data using the `WHERE` clause:\n`sql\nSELECT target\nFROM mindsdb.byom_mlflow_nlp\nWHERE text='The tsunami is coming, seek high ground';`\nOr you can `JOIN` with a data table. For this, you should ensure the table actually exists and that the database it belongs to has been connected to your MindsDB instance. For more details, refer to the same steps in the Ray Serve example (section 1.2).\n`sql\nSELECT\n    ta.text,\n    tb.target as predicted\nFROM db_byom.test.nlp_kaggle_test as ta\nJOIN mindsdb.byom_mlflow_nlp as tb;`\nFull Script\nFinally, for reference, here's the full script that trains and saves the model. The model is exactly the same as in section 1.2, so it may seem familiar.\n```python\nimport re\nimport pickle\nimport string\nimport mlflow.pyfunc\nimport nltk\nimport tqdm\nimport sklearn\nimport tensorflow\nimport cloudpickle\nimport numpy as np\nimport pandas as pd\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.initializers import Constant\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.models import load_model\nstop = set(stopwords.words('english'))\nMAX_LEN = 100\nGLOVE_DIM = 50\nEPOCHS = 10\ndef preprocess_df(df):\n    df = df[['text']]\n    funcs = [remove_URL, remove_html, remove_emoji, remove_punct]\n    for fn in funcs:\n        df['text'] = df['text'].apply(lambda x: fn(x))\n    return df\ndef remove_URL(text):\n    url = re.compile(r'https?://\\S+|www.\\S+')\n    return url.sub(r'', text)\ndef remove_html(text):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'', text)\ndef remove_punct(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\ndef create_corpus(df):\n    corpus = []\n    for tweet in tqdm.tqdm(df['text']):\n        words = [word.lower() for word in word_tokenize(tweet) if ((word.isalpha() == 1) & (word not in stop))]\n        corpus.append(words)\n    return corpus\nclass Model(mlflow.pyfunc.PythonModel):\n\n\n```def load_context(self, context):\n\n    self.model_path = context.artifacts['model']\n    with open(context.artifacts['tokenizer_path'], 'rb') as f:\n        self.tokenizer = pickle.load(f)\n    self.model = load_model(self.model_path)\n\ndef predict(self, context, model_input):\n\n    df = preprocess_df(model_input)\n    corpus = create_corpus(df)\n    sequences = self.tokenizer.texts_to_sequences(corpus)\n    tweet_pad = pad_sequences(sequences, maxlen=MAX_LEN, truncating='post', padding='post')\n    df = tweet_pad[:df.shape[0]]\n\n    y_pre = self.model.predict(df)\n    y_pre = np.round(y_pre).astype(int).flatten().tolist()\n\n    return list(y_pre)\n```\n\n\nif name == 'main':\n    train_model = True\n\n\n```model_path = './'\ntokenizer_path = './tokenizer.pkl'\nrun_name = 'test_run'\nmlflow_pyfunc_model_path = \"nlp_kaggle\"\nmlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n\nif train_model:\n\n    # preprocess data\n    df = pd.read_csv('./train.csv')\n    target = df[['target']]\n    target_arr = target.values\n    df = preprocess_df(df)\n    train_corpus = create_corpus(df)\n\n    # load embeddings\n    embedding_dict = {}\n    with open('./glove.6B.50d.txt', 'r') as f:\n        for line in f:\n            values = line.split()\n            word = values[0]\n            vectors = np.asarray(values[1:], 'float32')\n            embedding_dict[word] = vectors\n    f.close()\n\n    # generate and save tokenizer\n    tokenizer_obj = Tokenizer()\n    tokenizer_obj.fit_on_texts(train_corpus)\n\n    with open(tokenizer_path, 'wb') as f:\n        pickle.dump(tokenizer_obj, f)\n\n    # tokenize and pad\n    sequences = tokenizer_obj.texts_to_sequences(train_corpus)\n    tweet_pad = pad_sequences(sequences, maxlen=MAX_LEN, truncating='post', padding='post')\n    df = tweet_pad[:df.shape[0]]\n\n    word_index = tokenizer_obj.word_index\n    num_words = len(word_index) + 1\n    embedding_matrix = np.zeros((num_words, GLOVE_DIM))\n\n    # fill embedding matrix\n    for word, i in tqdm.tqdm(word_index.items()):\n        if i > num_words:\n            continue\n\n        emb_vec = embedding_dict.get(word)\n        if emb_vec is not None:\n            embedding_matrix[i] = emb_vec\n\n    X_train, X_test, y_train, y_test = train_test_split(df, target_arr, test_size=0.15)\n\n    # generate model\n    model = Sequential()\n    embedding = Embedding(num_words,\n                          GLOVE_DIM,\n                          embeddings_initializer=Constant(embedding_matrix),\n                          input_length=MAX_LEN,\n                          trainable=False)\n    model.add(embedding)\n    model.add(SpatialDropout1D(0.2))\n    model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n    model.add(Dense(1, activation='sigmoid'))\n\n    optimzer = Adam(learning_rate=1e-5)\n    model.compile(loss='binary_crossentropy', optimizer=optimzer, metrics=['accuracy'])\n\n    # train and save\n    model.fit(X_train, y_train, batch_size=4, epochs=EPOCHS, validation_data=(X_test, y_test), verbose=2)\n    model.save(model_path)\n\n# save in mlflow format\nartifacts = {\n    'model': model_path,\n    'tokenizer_path': tokenizer_path,\n}\n\nconda_env = {\n    'channels': ['defaults'],\n    'dependencies': [\n        'python=3.8',\n        'pip',\n        {\n            'pip': [\n                'mlflow',\n                'tensorflow=={}'.format(tensorflow.__version__),\n                'cloudpickle=={}'.format(cloudpickle.__version__),\n                'nltk=={}'.format(nltk.__version__),\n                'pandas=={}'.format(pd.__version__),\n                'numpy=={}'.format(np.__version__),\n                'scikit-learn=={}'.format(sklearn.__version__),\n                'tqdm=={}'.format(tqdm.__version__)\n            ],\n        },\n    ],\n    'name': 'nlp_keras_env'\n}\n\n# Save and register the MLflow Model\nwith mlflow.start_run(run_name=run_name) as run:\n    mlflow.pyfunc.save_model(\n        path=mlflow_pyfunc_model_path,\n        python_model=Model(),\n        conda_env=conda_env,\n        artifacts=artifacts)\n\n    result = mlflow.register_model(\n        f\"runs:/{run.info.run_id}/{mlflow_pyfunc_model_path}\",\n        f\"{mlflow_pyfunc_model_path}\"\n    )\n```\n\n",
    "tag": "mindsdb"
  },
  {
    "title": "Introduction",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/tutorials/house-sales-forecasting.mdx",
    "content": "\ntitle: Forecasting Quarterly House Sales with MindsDB\nsidebarTitle: House Sales\n\nIntroduction\nIn this tutorial, we'll create and train a machine learning model, or as we\ncall it, an `AI Table` or a `predictor`. By querying the model, we'll predict the\nreal estate sales using a multivariate time series strategy.\nMake sure you have access to a working MindsDB installation, either locally or\nat MindsDB Cloud.\nIf you want to learn how to set up your account at MindsDB Cloud, follow\nthis guide. Another way is to set up\nMindsDB locally using\nDocker or\nPython.\nLet's get started.\nData Setup\nConnecting the Data\nThere are a couple of ways you can get the data to follow through with this\ntutorial.\nConnecting as a database\n\n\n```You can connect to a demo database that we've prepared for you. It contains the data used throughout this tutorial (the `example_db.demo_data.house_sales` table).\n\n```sql\nCREATE DATABASE example_db\n    WITH ENGINE = \"postgres\",\n    PARAMETERS = {\n        \"user\": \"demo_user\",\n        \"password\": \"demo_password\",\n        \"host\": \"3.220.66.106\",\n        \"port\": \"5432\",\n        \"database\": \"demo\"\n};\n```\n\nNow you can run queries directly on the demo database. Let's preview the data that we'll use to train our predictor.\n\n```sql\nSELECT *\nFROM example_db.demo_data.house_sales\nLIMIT 10;\n```\n```\n\n\nConnecting as a file\n\n\n```The dataset we use in this tutorial is the pre-processed version of the *House Property Sales* data. You can download [the `CSV` data file here](https://www.kaggle.com/datasets/htagholdings/property-sales) (we use the *ma_lga_12345.csv* file) and upload it via [MindsDB SQL Editor](/connect/mindsdb_editor/).\n\nFollow [this guide](/sql/create/file/) to find out how to upload a file to MindsDB.\n\nNow you can run queries directly on the file as if it were a table. Let's preview the data that we'll use to train our predictor.\n\n```sql\nSELECT *\nFROM files.house_sales\nLIMIT 10;\n```\n```\n\n\n\nPay Attention to the Queries\nFrom now on, we'll use the `files.house_sales` file as a table. Make sure\nyou replace it with `example_db.demo_data.house_sales` if you connect the data\nas a database.\n\nUnderstanding the Data\nWe use the house sales dataset, where each row is one house or one unit, to\npredict the `MA` column values. It tracks quarterly moving averages (`MA`) of\nhouse sales aggregated by real estate type and the number of bedrooms in each\nlisting.\nBelow is the sample data stored in the `files.house_sales` table.\n`sql\n+----------+------+-----+--------+\n|saledate  |MA    |type |bedrooms|\n+----------+------+-----+--------+\n|30/09/2007|441854|house|2       |\n|31/12/2007|441854|house|2       |\n|31/03/2008|441854|house|2       |\n|30/06/2016|430880|unit |2       |\n|30/09/2016|430654|unit |2       |\n+----------+------+-----+--------+`\nWhere:\n| Column     | Description                                                         | Data Type           | Usage   |\n| ---------- | ------------------------------------------------------------------- | ------------------- | ------- |\n| `saledate` | The date of sale.                                                   | `date`              | Feature |\n| `MA`       | Moving average of the historical median price of the house or unit. | `integer`           | Label   |\n| `type`     | Type of property (`house` or `unit`).                               | `character varying` | Feature |\n| `bedrooms` | Number of bedrooms.                                                 | `integer`           | Feature |\n!!!Info \"Labels and Features\" A label is a column whose values will be\npredicted (the y variable in simple linear regression). A feature is a\ncolumn used to train the model (the x variable in simple linear regression).\nTraining a Predictor\nLet's create and train the machine learning model. For that, we use the\nCREATE MODEL statement and specify the\ninput columns used to train `FROM` (features) and what we want to\n`PREDICT` (labels).\n`sql\nCREATE MODEL mindsdb.house_sales_predictor\nFROM files\n  (SELECT * FROM house_sales)\nPREDICT MA\nORDER BY saledate\nGROUP BY bedrooms, type\n-- the target column to be predicted stores one row per quarter\nWINDOW 8      -- using data from the last two years to make forecasts (last 8 rows)\nHORIZON 4;    -- making forecasts for the next year (next 4 rows)`\nWe use all of the columns as features, except for the `MA` column, whose values\nwill be predicted.\nMindsDB makes it simple so that we don't need to repeat the predictor creation\nprocess for every group, that is, for every distinct number of bedrooms or for\nevery distinct type of real estate. Instead, we just group by both the\n`bedrooms` and `type` columns, and the predictor learns from all series and\nenables forecasts for all of them!\nStatus of a Predictor\nA predictor may take a couple of minutes for the training to complete. You can\nmonitor the status of the predictor by using this SQL command:\n`sql\nSELECT status\nFROM mindsdb.models\nWHERE name='house_sales_predictor';`\nIf we run it right after creating a predictor, we get this output:\n`sql\n+------------+\n| status     |\n+------------+\n| generating |\n+------------+`\nA bit later, this is the output:\n`sql\n+----------+\n| status   |\n+----------+\n| training |\n+----------+`\nAnd at last, this should be the output:\n`sql\n+----------+\n| status   |\n+----------+\n| complete |\n+----------+`\nNow, if the status of our predictor says `complete`, we can start making\npredictions!\nMaking Predictions\nYou can make predictions by querying the predictor joined with the data table.\nThe SELECT statement lets you make predictions for the\nlabel based on the chosen features for a given time period. Usually, you want to\nknow what happens right after the latest training data point that was fed. We\nhave a special keyword for that, the `LATEST` keyword.\n`sql\nSELECT m.saledate AS date, m.MA AS forecast, MA_explain\nFROM mindsdb.house_sales_predictor AS m\nJOIN files.house_sales AS t\nWHERE t.saledate > LATEST\nAND t.type = 'house'\nAND t.bedrooms = 2\nLIMIT 4;`\nOn execution, we get:\n`sql\n+-------------+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| date        | forecast          | MA_explain                                                                                                                                                                                    |\n+-------------+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| 2019-12-31  | 441413.5849598734 | {\"predicted_value\": 441413.5849598734, \"confidence\": 0.99, \"anomaly\": true, \"truth\": null, \"confidence_lower_bound\": 440046.28237074096, \"confidence_upper_bound\": 442780.88754900586}        |\n| 2020-04-01  | 443292.5194586229 | {\"predicted_value\": 443292.5194586229, \"confidence\": 0.9991, \"anomaly\": null, \"truth\": null, \"confidence_lower_bound\": 427609.3325864327, \"confidence_upper_bound\": 458975.7063308131}        |\n| 2020-07-02  | 443292.5194585953 | {\"predicted_value\": 443292.5194585953, \"confidence\": 0.9991, \"anomaly\": null, \"truth\": null, \"confidence_lower_bound\": 424501.59192981094, \"confidence_upper_bound\": 462083.4469873797}       |\n| 2020-10-02  | 443292.5194585953 | {\"predicted_value\": 443292.5194585953, \"confidence\": 0.9991, \"anomaly\": null, \"truth\": null, \"confidence_lower_bound\": 424501.59192981094, \"confidence_upper_bound\": 462083.4469873797}       |\n+-------------+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+`\nPlease note that in the SELECT statement, we select\n`m.saledate` instead of `t.saledate` because we make predictions for future\ndates that are not in the data table.\nNow, try changing the `type` column value to unit, or the `bedrooms` column\nvalue to any number between 1 to 5, and check how the forecasts vary. This is\nbecause MindsDB recognizes each grouping as being its own different time series.\nWhat's Next?\nHave fun while trying it out yourself!\n\nBookmark MindsDB repository on GitHub.\nSign up for a free MindsDB account.\nEngage with the MindsDB community on\n  Slack or\n  GitHub to ask questions and\n  share your ideas and thoughts.\n\nIf this tutorial was helpful, please give us a GitHub star",
    "tag": "mindsdb"
  },
  {
    "title": "Description",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/create/predictor.mdx",
    "content": "\ntitle: CREATE MODEL Statement\nsidebarTitle: MODEL\n\nDescription\nThe `CREATE MODEL` statement creates and trains a machine learning (ML) model.\n\n    Please note that the `CREATE MODEL` statement is equivalent to the `CREATE PREDICTOR` statement.\n    We are transitioning to the `CREATE MODEL` statement, but the `CREATE PREDICTOR` statement still works.\n\nSyntax\nOverview\nHere is the full syntax:\n```sql\nCREATE MODEL project_name.predictor_name\n[FROM integration_name\n    (SELECT [sequential_column,] [partition_column,] column_name, ...\n     FROM table_name)]\nPREDICT target_column\n[ORDER BY sequential_column]\n[GROUP BY partition_column]\n[WINDOW int]\n[HORIZON int]\n[USING engine = 'engine_name',\n       tag = 'tag_name'];\n```\nWhere:\n| Expressions                                     | Description                                                                                                                                     |\n| ----------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------- |\n| `project_name`                                  | Name of the project where the model is created. By default, the `mindsdb` project is used.                                                      |\n| `predictor_name`                                | Name of the model to be created.                                                                                                                |\n| `integration_name`                              | Name of the integration created using the CREATE DATABASE statement or file upload.            |\n| `(SELECT column_name, ... FROM table_name)`     | Selecting data to be used for training and validation.                                                                                          |\n| `target_column`                                 | Column to be predicted.                                                                                                                         |\n| `ORDER BY sequential_column`                    | Used in time series models. The column by which time series is ordered. It can be a date or anything that defines the sequence of events.                                                                                                                                                                        |\n| `GROUP BY partition_column`                     | Used in time series models. It is optional. The column by which rows that make a partition are grouped. For example, if you want to forecast the inventory for all items in the store, you can partition the data by `product_id`, so each distinct `product_id` has its own time series.                        |\n| `WINDOW int`                                    | Used in time series models. The number of rows to look back at when making a prediction. It comes after the rows are ordered by the column defined in `ORDER BY` and split into groups by the column(s) defined in `GROUP BY`. The `WINDOW 10` syntax could be interpreted as \"Always use the previous 10 rows\". |\n| `HORIZON int`                                   | Used in time series models. It is optional. It defines the number of future predictions (it is 1 by default). However, the `HORIZON` parameter, besides defining the number of predictions, has an impact on the training procedure when using the Lightwood ML backend. For example, different mixers are selected depending on whether the `HORIZON` value is one or greater than one. |\n| `engine_name`                                   | You can optionally provide an ML engine, based on which the model is created.                                                                   |\n| `tag_name`                                      | You can optionally provide a tag that is visible in the `training_options` column of the `mindsdb.models` table.                                |\nRegression Models\nHere is the syntax for regression models:\n`sql\nCREATE MODEL project_name.predictor_name\nFROM integration_name\n    (SELECT column_name, ... FROM table_name)\nPREDICT target_column\n[USING engine = 'engine_name',\n       tag = 'tag_name'];`\n\nPlease note that the `FROM` clause is mandatory here.\n\nThe `target_column` that will be predicted is a numerical value. The prediction values are not limited to a defined set of values, but can be any number from the given range of numbers.\nClassification Models\nHere is the syntax for classification models:\n`sql\nCREATE MODEL project_name.predictor_name\nFROM integration_name\n    (SELECT column_name, ... FROM table_name)\nPREDICT target_column\n[USING engine = 'engine_name',\n       tag = 'tag_name'];`\n\nPlease note that the `FROM` clause is mandatory here.\n\nThe `target_column` that will be predicted is a string value. The prediction values are limited to a defined set of values, such as `Yes` and `No`.\nTime Series Models\nHere is the syntax for time series models:\n```sql\nCREATE MODEL project_name.predictor_name\nFROM integration_name\n    (SELECT sequential_column, partition_column,\n            other_column, target_column\n     FROM table_name)\nPREDICT target_column\nORDER BY sequential_column\n[GROUP BY partition_column]\nWINDOW int\n[HORIZON int]\n[USING engine = 'engine_name',\n       tag = 'tag_name'];\n```\n\nPlease note that the `FROM` clause is mandatory here.\n\nDue to the nature of time series forecasting, you need to use the JOIN statement and join the data table with the model table to get predictions.\nNLP Models\nHere is the syntax for using external models within MindsDB:\n`sql\nCREATE MODEL project_name.model_name\nPREDICT PRED\nUSING\n  engine = 'engine_name',\n  task = 'task_name',\n  model_name = 'hub_model_name',\n  input_column = 'input_column_name',\n  labels = ['label1', 'label2'];`\n\nPlease note that you don't need to define the `FROM` clause here. Instead, the `input_column` is defined in the `USING` clause.\n\nIt allows you to bring an external model, for example, from the Hugging Face model hub, and use it within MindsDB.\nExample\nRegression Models\nHere is an example for regression models that uses data from a database:\n`sql\nCREATE MODEL mindsdb.home_rentals_model\nFROM example_db\n  (SELECT * FROM demo_data.home_rentals)\nPREDICT rental_price\nUSING engine = 'lightwood',\n      tag = 'my home rentals model';`\nOn execution, we get:\n`sql\nQuery OK, 0 rows affected (x.xxx sec)`\nVisit our tutorial on regression models to see the full example.\nClassification Models\nHere is an example for classification models that uses data from a file:\n`sql\nCREATE MODEL mindsdb.customer_churn_predictor\nFROM files\n  (SELECT * FROM churn)\nPREDICT Churn\nUSING engine = 'lightwood',\n      tag = 'my customers model';`\nOn execution, we get:\n`sql\nQuery OK, 0 rows affected (x.xxx sec)`\nVisit our tutorial on classification models to see the full example.\nTime Series Models\nHere is an example for time series models that uses data from a file:\n`sql\nCREATE MODEL mindsdb.house_sales_predictor\nFROM files\n  (SELECT * FROM house_sales)\nPREDICT MA\nORDER BY saledate\nGROUP BY bedrooms\n-- the target column to be predicted stores one row per quarter\nWINDOW 8      -- using data from the last two years to make forecasts (last 8 rows)\nHORIZON 4;    -- making forecasts for the next year (next 4 rows)`\nOn execution, we get:\n`sql\nQuery OK, 0 rows affected (x.xxx sec)`\nVisit our tutorial on time series models to see the full example.\nNLP Models\nHere is an example for the Hugging Face model:\n`sql\nCREATE MODEL mindsdb.spam_classifier\nPREDICT PRED\nUSING\n  engine = 'huggingface',\n  task = 'text-classification',\n  model_name = 'mrm8488/bert-tiny-finetuned-sms-spam-detection',\n  input_column = 'text_spammy',\n  labels = ['ham', 'spam'];`\nOn execution, we get:\n`sql\nQuery OK, 0 rows affected (x.xxx sec)`\nVisit our page no how to bring Hugging Face models into MindsDB for more details.\n\n\nChecking Model Status\n\n\n```After you run the `CREATE MODEL` statement, you can check the status of the training process by querying the `mindsdb.models` table.\n\n```sql\nSELECT *\nFROM mindsdb.models\nWHERE name = 'predictor_name';\n```\n\nOn execution, we get:\n\n```sql\n+---------------+-----------------------------------+----------------------------------------+-----------------------+-------------+---------------+-----+-----------------+----------------+\n|name           |status                             |accuracy                                |predict                |update_status|mindsdb_version|error|select_data_query|training_options|\n+---------------+-----------------------------------+----------------------------------------+-----------------------+-------------+---------------+-----+-----------------+----------------+\n|predictor_name |generating or training or complete |number depending on the accuracy metric |column_to_be_predicted |up_to_date   |22.7.5.0       |     |                 |                |\n+---------------+-----------------------------------+----------------------------------------+-----------------------+-------------+---------------+-----+-----------------+----------------+\n```\n```\n\n\n\nTuning the Lightwood ML Engine Features\nDescription\nIn MindsDB, the underlying AutoML models are based on the Lightwood engine by default. This library generates models automatically based on the data and declarative problem definition. But the default configuration can be overridden using the `USING` statement that provides an option to configure specific parameters of the training process.\nIn the upcoming version of MindsDB, it will be possible to choose from more ML frameworks. Please note that the Lightwood engine is used by default.\nSyntax\nHere is the syntax:\n`sql\nCREATE MODEL project_name.predictor_name\nFROM integration_name\n    (SELECT column_name, ... FROM table_name)\nPREDICT target_column\nUSING parameter_key = 'parameter_value';`\n`encoders` Key\nIt grants access to configure how each column is encoded. By default, the AutoML engine tries to get the best match for the data.\n`sql\n...\nUSING encoders.column_name.module = 'value';`\nTo learn more about `encoders` and their options, visit the Lightwood documentation page on encoders.\n`model` Key\nIt allows you to specify the type of machine learning algorithm to learn from the encoder data.\n`sql\n...\nUSING model.args = {\"key\": value};`\nHere are the model options:\n| Model                                                               | Description                                                                                                                                                                                                                                                                         |\n| -------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| BaseMixer         | It is a base class for all mixers.                                                                                                                                                                                                                                                  |\n| LightGBM           | This mixer configures and uses LightGBM for regression or classification tasks depending on the problem definition.                                                                                                                                                                 |\n| LightGBMArray | This mixer consists of several LightGBM mixers in regression mode aimed at time series forecasting tasks.                                                                                                                                                                           |\n| NHitsMixer       | This mixer is a wrapper around an MQN-HITS deep learning model.                                                                                                                                                                                                                     |\n| Neural               | This mixer trains a fully connected dense network from concatenated encoded outputs of each feature in the dataset to predict the encoded output.                                                                                                                                   |\n| NeuralTs           | This mixer inherits from Neural mixer and should be used for time series forecasts.                                                                                                                                   |\n| ProphetMixer   | This mixer is a wrapper around the popular time series library Prophet.                                                                                                                                                                      |                                        |\n| RandomForest   | This mixer supports both regression and classification tasks. It inherits from sklearn.ensemble.RandomForestRegressor and sklearn.ensemble.RandomForestClassifier.                                                                                                                                            |\n| Regression       | This mixer inherits from scikit-learn\u2019s Ridge class.                                                                                                                                           |\n| SkTime               | This mixer is a wrapper around the popular time series library sktime.                                                                                                                                                                                                              |\n| Unit                   | This is a special mixer that passes along whatever prediction is made by the target encoder without modifications. It is used for single-column predictive scenarios that may involve complex and/or expensive encoders (e.g. free-form text classification with transformers).     |\n| XGBoostMixer   | This mixer is a good all-rounder, due to the generally great performance of tree-based ML algorithms for supervised learning tasks with tabular data.     |\nTo learn more about all the `model` options, visit the Lightwood documentation page on mixers.\nOther Keys Supported by Lightwood in JsonAI\nThe most common use cases of configuring predictors use `encoders` and `model` keys explained above. To see all the available keys, check out the Lightwood documentation page on JsonAI.\nExample\nHere we use the `home_rentals` dataset and specify particular `encoders` for some columns and a LightGBM `model`.\n`sql\nCREATE MODEL mindsdb.home_rentals_model\nFROM example_db\n    (SELECT * FROM demo_data.home_rentals)\nPREDICT rental_price\nUSING\n    encoders.location.module = 'CategoricalAutoEncoder',\n    encoders.rental_price.module = 'NumericEncoder',\n    encoders.rental_price.args.positive_domain = 'True',\n    model.args = {\"submodels\": [\n                    {\"module\": \"LightGBM\",\n                     \"args\": {\n                          \"stop_after\": 12,\n                          \"fit_on_dev\": true\n                          }\n                    }\n                ]};`\nOn execution, we get:\n```sql\nQuery OK, 0 rows affected (x.xxx sec)",
    "tag": "mindsdb"
  },
  {
    "title": "What's Next?",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/create/file.mdx",
    "content": "\ntitle: Upload a File to MindsDB\nsidebarTitle: Upload a File to MindsDB\n\nFollow the steps below to upload a file to MindsDB.\n\nLog in to your MindsDB Cloud account to\n   open the MindsDB Editor.\nNavigate to `Add data` section by clicking the `Add data` button located in\n   the top right corner.\n\n\n\n\n\nChoose the `Files` tab.\n\n\n\n\n\nChoose the `Import File` option.\nUpload a file (here it is `house_sales.csv`), name a table used to store the\n   file data (here it is `house_sales`), and click the `Save and Continue`\n   button.\n\n\n\n\nWhat's Next?\nNow, you are ready to create a predictor from a file. Make sure to check out\nthis guide",
    "tag": "mindsdb"
  },
  {
    "title": "Description",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/create/ml-engine.mdx",
    "content": "\ntitle: CREATE ML_ENGINE Statement\nsidebarTitle: ML_ENGINE\n\nYou can create machine learning (ML) engines based on the ML handlers available in MindsDB. If you can't find the ML handler of your interest, you can always contribute by building a new ML handler.\nDescription\nThe `CREATE ML_ENGINE` command creates an ML engine that uses one of the available ML handlers.\nSyntax\nBefore creating an ML engine, make sure that the ML handler of your interest is available by querying for the ML handlers.\n`sql\nSELECT *\nFROM information_schema.handlers;\n-- or \nSHOW HANDLERS;`\n\nCan't Find an ML Handler?\n    If you can't find the ML handler of your interest, you can contribute by building a new ML handler.\n\nIf you find the ML handler of your interest, then you can create an ML engine using this command:\n`sql\nCREATE ML_ENGINE ml_engine_name\nFROM handler_name\n[USING argument_key = argument_value];`\nPlease replace `ml_engine_name`, `handler_name`, and optionally, `argument_key` and `argument_value` with the real values.\nTo verify that your ML engine was successfully created, run the command below:\n`sql\nSELECT *\nFROM information_schema.ml_engines;\n-- or \nSHOW ML_ENGINES;`\nIf you want to drop an ML engine, run the command below:\n`sql\nDROP ML_ENGINE ml_engine_name;`\nExample\nLet's check what ML handlers are currently available:\n`sql\nSHOW HANDLERS;`\nOn execution, we get:\n`sql\n+------------------+------------+---------------------------------+-------+------------------------------------------------------------------------------+---------------+----------------------+------+\n|NAME              |TITLE       |DESCRIPTION                      |VERSION|CONNECTION_ARGS                                                               |IMPORT_SUCCESS |IMPORT_ERROR          |FIELD8|\n+------------------+------------+---------------------------------+-------+------------------------------------------------------------------------------+---------------+----------------------+------+\n|merlion           |Merlion     |MindsDB handler for Merlion      |0.0.1  |[NULL]                                                                        |true           |[NULL]                |      |\n|byom              |BYOM        |MindsDB handler for BYOM         |0.0.1  |{'model_code': {'type': 'path', 'description': 'The path name to model code'}}|true           |[NULL]                |      |\n|ludwig            |Ludwig      |MindsDB handler for Ludwig AutoML|0.0.2  |[NULL]                                                                        |false          |No module named 'dask'|      |\n|lightwood         |Lightwood   |[NULL]                           |1.0.0  |[NULL]                                                                        |true           |[NULL]                |      |\n|huggingface       |Hugging Face|MindsDB handler for Higging Face |0.0.1  |[NULL]                                                                        |true           |[NULL]                |      |\n+------------------+------------+---------------------------------+-------+------------------------------------------------------------------------------+---------------+----------------------+------+`\nHere we create an ML engine using the Lightwood handler.\n`sql\nCREATE ML_ENGINE my_lightwood_engine\nFROM lightwood;`\nOn execution, we get:\n`sql\nQuery successfully completed`\nNow let's verify that our ML engine exists.\n`sql\nSHOW ML_ENGINES;`\nOn execution, we get:\n`sql\n+-------------------+-----------+----------------+\n|NAME               |HANDLER    |CONNECTION_DATA |\n+-------------------+-----------+----------------+\n|huggingface        |huggingface|{'password': ''}|\n|lightwood          |lightwood  |{'password': ''}|\n|my_lightwood_engine|lightwood  |{'password': ''}|\n+-------------------+-----------+----------------+`\nPlease note that we haven't used any arguments while creating the ML engine. The `USING` clause is optional, as it depends on the ML handler whether it requires/allows some arguments or not.\nAfter creating your ML engine, you can create a model like this:\n`sql\nCREATE MODEL my_model\nFROM integration_name\n    (SELECT * FROM table_name)\nUSING engine = 'my_lightwood_engine';`",
    "tag": "mindsdb"
  },
  {
    "title": "Description",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/create/table.mdx",
    "content": "\ntitle: CREATE TABLE Statement\nsidebarTitle: TABLE\n\nDescription\nThe `CREATE TABLE` statement creates a table and fills it with a subselect query output. It is usually used to materialize prediction results as tables.\nSyntax\nYou can use the usual `CREATE TABLE` statement:\n`sql\nCREATE TABLE integration_name.table_name\n    (SELECT ...);`\nOr the `CREATE OR REPLACE TABLE` statement:\n`sql\nCREATE OR REPLACE TABLE integration_name.table_name\n    (SELECT ...);`\nHere are the steps followed by the syntax:\n\nIt executes a subselect query to get the output data.\nIn the case of the `CREATE OR REPLACE TABLE` statement, the\n  `integration_name.table_name` table is dropped before recreating it.\nIt (re)creates the `integration_name.table_name` table inside the\n  `integration_name` integration.\nIt uses the INSERT INTO statement to insert the\n  output of the `(SELECT ...)` query into the\n  `integration_name.table_name`.\n\nOn execution, we get:\n`sql\nQuery OK, 0 rows affected (x.xxx sec)`\nExample\nWe want to save the prediction results into the `int1.tbl1` table.\nHere is the schema structure used throughout this example:\n`bash\nint1\n\u2514\u2500\u2500 tbl1\nmindsdb\n\u2514\u2500\u2500 predictor_name\nint2\n\u2514\u2500\u2500 tbl2`\nWhere:\n| Name             | Description                                                                           |\n| ---------------- | ------------------------------------------------------------------------------------- |\n| `int1`           | Integration where the table that stores prediction results resides.                   |\n| `tbl1`           | Table that stores prediction results.                                                 |\n| `predictor_name` | Name of the model.                                                                    |\n| `int2`           | Integration where the data source table used in the inner `SELECT` statement resides. |\n| `tbl2`           | Data source table used in the inner `SELECT` statement.                               |\nLet's execute the query.\n`sql\nCREATE OR REPLACE TABLE int1.tbl1 (\n    SELECT *\n    FROM int2.tbl2 AS ta\n    JOIN mindsdb.predictor_name AS tb\n    WHERE ta.date > '2015-12-31'\n);`\nOn execution, we get:\n```sql\nQuery OK, 0 rows affected (x.xxx sec)",
    "tag": "mindsdb"
  },
  {
    "title": "Description",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/create/jobs.mdx",
    "content": "\ntitle: CREATE JOB Statement\nsidebarTitle: JOB\n\nDescription\nThe `CREATE JOB` statement lets you schedule the execution of queries by providing relevant parameters, such as start date, end date, or repetition frequency.\nSyntax\nWith the following syntax, you can create jobs, delete jobs, and query for jobs and their history.\nCreating a Job\nHere is the syntax:\n`sql\nCREATE JOB [project_name.]job_name [AS] (\n   <mindsdb_sql_query_1>[; <mindsdb_sql_query_2>]\n)\n[START <date>]\n[END <date>]\n[EVERY [number] <period>];`\nWhere:\n| Expression                                          | Description                                                                                                                                                         |\n| --------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `[project_name.]job_name`                           | Name of the job preceded by an optional project name where the job is to be created.                                                                                |\n| `<mindsdb_sql_query_1>[; <mindsdb_sql_query_2>]`    | One or more queries separated by `;` to be executed by the job.                                                                                                     |\n| `[START <date>]`                                    | Optional. The date when the job starts its periodical or one-time execution. If not set, it is the current system date.                                             |\n| `[END <date>]`                                      | Optional. The date when the job ends its periodical or one-time execution. If it is not set (and the repetition rules are set), then the job repeats forever.       |\n| `[EVERY [number] <period>]`                         | Optional. The repeatition rules for the job. If not set, the job runs once, not considering the end date value. If the `number` value is not set, it defaults to 1. |\n\nAvailable `<date>` formats\nHere are the supported `<date>` formats:\n- `'%Y-%m-%d %H:%M:%S'`\n- `'%Y-%m-%d'`\nPlease note that the default time zone is UTC.\n\n\nAvailable `<period>` values\nAnd the supported `<period>` values:\n- `minute` / `minutes` / `min`\n- `hour` / `hours`\n- `day` / `days`\n- `week` / `weeks`\n- `month` / `months`\n\nDeleting a Job\nHere is the syntax for deleting a job:\n`sql\nDROP JOB [project_name.]job_name;`\nThe `project_name` value is optional. The `job_name` value indicates the job to be deleted.\nLet's look at some examples:\n`sql\nDROP JOB my_project.retrain_and_save_job;`\nHere we drop the `retrain_and_save_job` that resides in the `my_project` project.\nAnd another example:\n`sql\nDROP JOB create_table_job;`\nHere we drop the `create_table_job` job that resides in the current project.\nTo learn more about projects in MindsDB, visit our docs here.\nQuerying Jobs\nHere is how we can view all jobs in the current project:\n`sql\nSELECT *\nFROM jobs;`\nOn execution, we get:\n`sql\n+------------------------------------+---------+----------------------------+----------------------------+----------------------------+---------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| NAME                               | PROJECT | START_AT                   | END_AT                     | NEXT_RUN_AT                | SCHEDULE_STR  | QUERY                                                                                                                                                                                                                                   |\n+------------------------------------+---------+----------------------------+----------------------------+----------------------------+---------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| drop_model                         | mindsdb | 2023-04-01 00:00:00.000000 | [NULL]                     | 2023-04-01 00:00:00.000000 | [NULL]        | DROP MODEL mindsdb.home_rentals_model                                                                                                                                                                                                   |\n| retrain_model_and_save_predictions | mindsdb | 2023-02-15 19:19:43.210122 | 2023-04-01 00:00:00.000000 | 2023-02-15 19:19:43.210122 | every 2 days  | RETRAIN mindsdb.home_rentals_model USING join_learn_process = true; INSERT INTO my_integration.rentals (SELECT m.rental_price, m.rental_price_explain FROM mindsdb.home_rentals_model AS m JOIN example_db.demo_data.home_rentals AS d) |\n| save_predictions                   | mindsdb | 2023-02-15 19:19:48.545580 | [NULL]                     | 2023-02-15 19:19:48.545580 | every hour    | CREATE TABLE my_integration.`result_{{START_DATETIME}}` (SELECT m.rental_price, m.rental_price_explain FROM mindsdb.home_rentals_model AS m JOIN example_db.demo_data.home_rentals AS d)                                                |\n+------------------------------------+---------+----------------------------+----------------------------+----------------------------+---------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+`\nWe can also view jobs from a defined project:\n`sql\nSELECT *\nFROM my_project.jobs;`\nOr from all projects at once:\n`sql\nSELECT *\nFROM information_schema.jobs;`\nQuerying Jobs History\nYou can query the history of jobs similar to querying for jobs.\nHere is how we can view all jobs history in the current project:\n`sql\nSELECT *\nFROM jobs_history;`\nOn execution, we get:\n`sql\n+------------------------------------+---------+----------------------------+----------------------------+----------------------------+--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| NAME                               | PROJECT | START_AT                   | END_AT                     | NEXT_RUN_AT                | ERROR  | QUERY                                                                                                                                                                                                                                   |\n+------------------------------------+---------+----------------------------+----------------------------+----------------------------+--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| retrain_model_and_save_predictions | mindsdb | 2023-02-15 19:19:43.210122 | 2023-04-01 00:00:00.000000 | 2023-02-15 19:19:43.210122 | [NULL] | RETRAIN mindsdb.home_rentals_model USING join_learn_process = true; INSERT INTO my_integration.rentals (SELECT m.rental_price, m.rental_price_explain FROM mindsdb.home_rentals_model AS m JOIN example_db.demo_data.home_rentals AS d) |\n| save_predictions                   | mindsdb | 2023-02-15 19:19:48.545580 | [NULL]                     | 2023-02-15 19:19:48.545580 | [NULL] | CREATE TABLE my_integration.`result_{{START_DATETIME}}` (SELECT m.rental_price, m.rental_price_explain FROM mindsdb.home_rentals_model AS m JOIN example_db.demo_data.home_rentals AS d)                                                |\n+------------------------------------+---------+----------------------------+----------------------------+----------------------------+--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+`\nPlease note that the `drop_model` job is not in the `jobs_history` table because it didn't start yet.\nWe can also view jobs history from a defined project:\n`sql\nSELECT *\nFROM my_project.jobs_history;`\nOr from all projects at once:\n`sql\nSELECT *\nFROM information_schema.jobs_history;`\nAdditional Configuration\nHere is the template of the `config.json` file that you can pass as a parameter when starting your local MindsDB instance:\n`bash\n\"jobs\": {\n        \"disable\": true,\n        \"check_interval\": 30\n      }`\nThe `disable` parameter defines whether the scheduler is active (`true`) or not (`false`). By default, in the MindsDB Cloud Editor, the scheduler is active.\nThe `check_interval` parameter defines the interval in seconds between consecutive checks of the scheduler table. By default, in the MindsDB Cloud Editor, it is 30 seconds.\nYou can modify the default configuration in your local MindsDB installation by creating a `config.json` file and starting MindsDB with this file as a parameter. You can find detailed instructions here.\nExample 1\nIn this example, we create a job in the current project to retrain the `home_rentals_model` model and insert predictions into the `rentals` table.\n```sql\nCREATE JOB retrain_model_and_save_predictions (\nRETRAIN mindsdb.home_rentals_model\n   USING\n      join_learn_process = true;\nINSERT INTO my_integration.rentals (\n      SELECT m.rental_price, m.rental_price_explain\n      FROM mindsdb.home_rentals_model AS m\n      JOIN example_db.demo_data.home_rentals AS d\n   )\n)\nEND '2023-04-01 00:00:00'\nEVERY 2 days;\n```\n\nPlease note that the `join_learn_process` parameter in the `USING` clause of the RETRAIN statement ensures that the retraining process completes before inserting predictions into a table. In general, this parameter is used to prevent several retrain processes from running simultaneously.\n\nThe `retrain_model_and_save_predictions` job starts its execution on the current system date and ends on the 1st of April 2023. The job is executed every 2 days.\nExample 2\nIn this example, the job creates a table named as `result_{{START_DATETIME}}` and inserts predictions into it.\n```sql\nCREATE JOB save_predictions (\nCREATE TABLE my_integration.`result_{{START_DATETIME}}` (\n      SELECT m.rental_price, m.rental_price_explain\n      FROM mindsdb.home_rentals_model AS m\n      JOIN example_db.demo_data.home_rentals AS d\n   )\n)\nEVERY hour;\n```\n\nPlease note that the uniqueness of the created table name is ensured here by using the `{{START_DATETIME}}` variable that is replaced at runtime by the date and time of the current run.\nYou can use the following variables for this purpose:\n- `PREVIOUS_START_DATETIME` is replaced by date and time of the previous run of this job.\n- `START_DATETIME` is replaced by date and time of the current job run.\n- `START_DATE` is replaced by date of the current job run.\n\nThe `save_predictions` job starts its execution on the current system date and repeats every 2 hours until it is manually disabled.\nExample 3\nIn this example, we create a job to drop the `home_rentals_model` model scheduled on the 1st of April 2023.\n```sql\nCREATE JOB drop_model (\nDROP MODEL mindsdb.home_rentals_model\n) \nSTART '2023-04-01';\n```",
    "tag": "mindsdb"
  },
  {
    "title": "Description",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/create/databases.mdx",
    "content": "\ntitle: CREATE DATABASE Statement\nsidebarTitle: DATABASE\n\nDescription\nMindsDB lets you connect to your favorite databases, data warehouses, data lakes, etc., via the `CREATE DATABASE` command.\nThe MindsDB SQL API supports creating connections to integrations by passing the\nconnection parameters specific per integration. You can find more in the\nSupported Integrations chapter.\nSyntax\nLet's review the syntax for the `CREATE DATABASE` command.\n`sql\nCREATE DATABASE datasource_name\n[WITH] [ENGINE [=] engine_name] [,]\n[PARAMETERS [=] {\n  \"key\": \"value\",\n  ...\n}];`\nOn execution, we get:\n`sql\nQuery OK, 0 rows affected (x.xxx sec)`\nWhere:\n| Name                | Description                                                                       |\n| ------------------- | --------------------------------------------------------------------------------- |\n| `datasource_name`   | Identifier for the data source to be created.                                     |\n| `engine_name`       | Engine to be selected depending on the database connection.                       |\n| `PARAMETERS`        | `{\"key\": \"value\"}` object with the connection parameters specific for each engine. |\n\nSQL Commands Resulting in the Same Output Please note that the\nkeywords/statements enclosed within square brackets are optional. Also, by\ndefault, the engine is `mindsdb` if not provided otherwise. That yields the\nfollowing SQL commands to result in the same output.\n`sql\nCREATE DATABASE db;\nCREATE DATABASE db ENGINE 'mindsdb';\nCREATE DATABASE db ENGINE = 'mindsdb';\nCREATE DATABASE db WITH ENGINE 'mindsdb';\nCREATE DATABASE db WITH ENGINE = 'mindsdb';`\n\nExample\nConnecting a Data Source\nHere is an example of how to connect to a MySQL database.\n`sql\nCREATE DATABASE mysql_datasource\nWITH ENGINE = 'mariadb',\nPARAMETERS = {\n  \"user\": \"root\",\n  \"port\": 3307,\n  \"password\": \"password\",\n  \"host\": \"127.0.0.1\",\n  \"database\": \"my_database\"\n};`\nOn execution, we get:\n`sql\nQuery OK, 0 rows affected (8.878 sec)`\nListing Linked Databases\nYou can list all the linked databases using the command below.\n`sql\nSHOW DATABASES;`\nOn execution, we get:\n`sql\n+--------------------+\n| Database           |\n+--------------------+\n| information_schema |\n| mindsdb            |\n| files              |\n| mysql_datasource   |\n+--------------------+`\nMaking your Local Database Available to MindsDB\nWhen connecting your local database to MindsDB Cloud, you should expose the\nlocal database server to be publicly accessible. It is easy to accomplish using\nNgrok Tunnel. The free tier offers all you need to get\nstarted.\nThe installation instructions are easy to follow. Head over to the\ndownloads page and choose your operating system.\nFollow the instructions for installation.\nThen create a free account at Ngrok to get\nan auth token that you can use to configure your Ngrok instance.\nOnce installed and configured, run the following command to obtain the host and\nport for your localhost at `port-number`.\n`bash\nngrok tcp port-number`\nHere is an example. Assuming that you run a PostgreSQL database at\n`localhost:5432`, use the following command:\n`bash\nngrok tcp 5432`\nOn execution, we get:\n`bash\nSession Status                online\nAccount                       myaccount (Plan: Free)\nVersion                       2.3.40\nRegion                        United States (us)\nWeb Interface                 http://127.0.0.1:4040\nForwarding                    tcp://4.tcp.ngrok.io:15093 -> localhost 5432`\nNow you can access your local database at `4.tcp.ngrok.io:15093` instead of\n`localhost:5432`.\nSo to connect your local database to the MindsDB GUI, use the `Forwarding`\ninformation. The host is `4.tcp.ngrok.io`, and the port is `15093`.\nProceed to create a database connection in the MindsDB GUI by executing the\n`CREATE DATABASE` statement with the host and port number obtained from\nNgrok.\n`sql\nCREATE DATABASE psql_datasource\nWITH ENGINE = 'postgres',\nPARAMETERS = {\n  \"user\": \"postgres\",\n  \"port\": 15093,\n  \"password\": \"password\",\n  \"host\": \"4.tcp.ngrok.io\",\n  \"database\": \"postgres\"\n};`\nPlease note that the Ngrok tunnel loses connection when stopped or canceled. To\nreconnect your local database to MindsDB, you should create an Ngrok tunnel\nagain. In the free tier, Ngrok changes the host and port values each time you\nlaunch the program, so you need to reconnect your database in the MindsDB Cloud\nby passing the new host and port values obtained from Ngrok.\nBefore resetting the database connection, drop the previously connected data\nsource using the `DROP DATABASE` statement.\n`sql\nDROP DATABASE psql_datasource;`\nAfter dropping the data source and reconnecting your local database, you can use\nthe predictors that you trained using the previously connected data source.\nHowever, if you have to `RETRAIN` your predictors, please ensure the database\nconnection has the same name you used when creating the predictor to avoid\nfailing to retrain.\nSupported Integrations",
    "tag": "mindsdb"
  },
  {
    "title": "Description",
    "source": "https://github.com/mindsdb/mindsdb/tree/staging/docs/sql/create/view.mdx",
    "content": "\ntitle: CREATE VIEW Statement\nsidebarTitle: VIEW\n\nDescription\nThe `CREATE VIEW` statement creates a view, which is a saved `SELECT` statement executed every time we call this view, as opposed to a table that stores its data in columns and rows.\nIn MindsDB, the `CREATE VIEW` statement is commonly used to create AI Tables. An AI Table is a virtual table created by joining the data source table with the prediction model.\nSyntax\nHere is the syntax:\n`sql\nCREATE VIEW project_name.view_name AS (\n    SELECT a.column_name, ...,\n           p.model_column AS model_column\n    FROM integration_name.table_name AS a\n    JOIN mindsdb.predictor_name AS p\n);`\nOn execution, we get:\n`sql\nQuery OK, 0 rows affected (x.xxx sec)`\nWhere:\n| Name                                  | Description                                                                              |\n| ------------------------------------- | ---------------------------------------------------------------------------------------- |\n| `project_name`                        | Name of the project to store the view.                                                   |\n| `view_name`                           | Name of the view.                                                                        |\n| `a.column_name, ...`                  | Columns of the data source table that are the input for the model to make predictions.   |\n| `p.model_column`                      | Name of the target column to be predicted.                                               |\n| `integration_name.table_name`         | Data source table name along with the integration where it resides.                      |\n| `predictor_name`                      | Name of the model.                                                                       |\nExample\nBelow is the query that creates and trains the `home_rentals_model` model to predict the `rental_price` value. The inner `SELECT` statement provides all real estate listing data used to train the model.\n`sql\nCREATE MODEL mindsdb.home_rentals_model\nFROM integration\n    (SELECT * FROM house_rentals_data)\nPREDICT rental_price;`\nOn execution, we get:\n`sql\nQuery OK, 0 rows affected (x.xxx sec)`\nNow, we can JOIN the `home_rentals_data` table with the `home_rentals_model` model to make predictions. By creating a view (using the `CREATE VIEW` statement) that is based on the `SELECT` statement joining the data and model tables, we create an AI Table.\nHere, the `SELECT` statement joins the data source table and the model table. The input data for making predictions consists of the `sqft`, `number_of_bathrooms`, and `location` columns. These are joined with the `rental_price` column that stores predicted values.\n`sql\nCREATE VIEW mindsdb.home_rentals_predictions AS (\n    SELECT\n        a.sqft,\n        a.number_of_bathrooms,\n        a.location,\n        p.rental_price AS price\n    FROM integration.home_rentals_data AS a\n    JOIN mindsdb.home_rentals_model AS p\n);`\nOn execution, we get:\n`sql\nQuery OK, 0 rows affected (x.xxx sec)`\n\nDataset for Training and Dataset for Joining\n\n\n```In this example, we used the same dataset (`integration.home_rentals_data`) for training the model (see the `CREATE MODEL` statement above) and for joining with the model to make predictions (see the `CREATE VIEW` statement above). It doesn't happen like that in real-world scenarios.\nNormally, you use the old data to train the model, and then you join the new data with this model to make predictions.\n\nConsider the `old_data` dataset that stores data from the years 2019-2021 and the `new_data` dataset that stores data from the year 2022.\n\nWe train the model with the `old_data` dataset like this:\n\n```sql\nCREATE MODEL mindsdb.data_model\nFROM integration\n    (SELECT * FROM old_data)\nPREDICT column;\n```\n\nNow, having the `data_model` model trained using the `old_data` dataset, we can join this model with the `new_data` dataset to make predictions like this:\n\n```sql\nCREATE VIEW mindsdb.data_predictions AS (\n    SELECT\n        a.column1,\n        a.column2,\n        a.column3,\n        p.column AS predicted_column\n    FROM integration.new_data AS a\n    JOIN mindsdb.data_model AS p\n);\n```\n```\n\n\n\nUSING VIEW\nExamples to use view\n\nComplex select on view (it is grouping in this example).\n\n`sql\nSELECT type, last(bedrooms) \nFROM mindsdb.house_v\nGROUP BY 1`\n\nCreating predictor from view\n\n`sql\nCREATE MODEL house_sales_model\nFROM mindsdb (\n  SELECT * FROM house_v\n) PREDICT ma\nORDER BY saledate\nGROUP BY bedrooms, type\nWINDOW 1 HORIZON 4`\n\nUsing predictor with view\n\n`sql\nSELECT * FROM mindsdb.house_v\nJOIN mindsdb.house_sales_model\nWHERE house_v.saledate > latest`\n\nFrom Our Community\nCheck out the article created by our community:\n\nArticle on Creating Views with MindsDB\n  by Rutam Prita Mishra\n",
    "tag": "mindsdb"
  }
]