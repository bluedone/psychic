[
  {
    "title": "Get Started",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/introduction.md",
    "content": "\ntitle: Introduction\nslug: /\ndescription:\n  QuestDB is a relational column-oriented database designed for real-time\n  analytics on time series data.\n\nQuestDB is a relational column-oriented database designed for time series and\nevent data. It uses SQL with extensions for time series to assist with real-time\nanalytics. These pages cover core concepts of QuestDB, including setup steps,\nusage guides, and reference documentation for syntax, APIs and configuration.\nGet Started\nThis section explains how to install and run QuestDB using one of the following\nmethods:\n\nDocker for repeatable, portable and scalable\n  installations\nBinaries for direct downloads to run on Linux,\n  macOS or Windows\nHomebrew for running QuestDB on macOS\nCreate your first database provides a\n  guide once QuestDB is running\nLearn more is a selection of useful concepts\n  and guides for new QuestDB users\n\nDevelop\nThis section describes how to connect to QuestDB using a variety of tools and\nprogramming languages through our various network endpoints.\n\nConnect to the database through our various\n  endpoints. Learn which protocol is best for different use cases\nInsert data using the InfluxDB Line Protocol,\n  PostgreSQL wire protocol or our HTTP REST API\n\nQuery data with SQL queries via the PostgreSQL\n  Wire Protocol or exported to JSON or CSV via our HTTP REST API\n\n\nWeb Console for quick SQL queries, charting and\n  CSV upload/export functionality\n\n\nGuides\n\nMigrating from InfluxDB\nLarge CSV import (COPY SQL)\nSmall CSV import (REST API)\nModifying Data\nTimestamps and time zones\nOut-of-order commit lag\n\nDeployment\n\nAWS Marketplace AMI\nKubernetes\nGoogle Cloud Platform\nDigitalOcean Droplet\n\nOperations\nThis section contains resources for managing QuestDB instances and has dedicated\npages for the following topics:\n\nDesign for performance for\n  configuring table settings to improve performance\nCapacity planning for configuring server\n  settings and system resources\nData retention strategy to delete old data\n  and save disk space\nHealth monitoring endpoint for\n  determining the status of the instance\nBackup and restore using filesystem and\n  point-in-time backup functionality. Notes for cloud providers.\n\nThird-party tools\nThis section describes how to integrate QuestDB with third-party tools and\nutilities for collecting metrics and visualizing data:\n\nGrafana instructions for connecting QuestDB\n  as a datasource for building visualizations and dashboards\nMindsDB tutorial for connecting QuestDB to MindsDB for running models with Machine Learning capabilities.\nKafka guide for ingesting data from\n  Kafka topics into QuestDB\nPandas for ingesting Pandas dataframes via\n  ILP\nPrometheus monitoring and alerting\nRedpanda instructions for a Kafka\n  compatible data streaming tool\nTelegraf guide for collecting system\n  metrics, specifying QuestDB as an output and visualizing the results\n\nConcepts\nThis section describes the architecture of QuestDB, how it stores and queries\ndata, and introduces features and capabilities unique to the system.\n\nStorage model describes how QuestDB stores\n  records and partitions within tables\nDesignated timestamp is a core feature\n  that enables time-oriented language capabilities and partitioning\nSQL extensions allow performant time series\n  analysis with a concise syntax\nJIT compiler to speed up `WHERE` clauses\nPartitions can be used to perform time-based\n  queries more efficiently\nSymbol type makes storing and retrieving repetitive\n  strings efficient\nIndexes can be used for faster read access on\n  specific columns\nGeospatial data with geohashes\nRoot directory describes the\n  directory contents of QuestDB for storage and configuration\n\nReference\nThis section contains the reference documentation for the following categories:\nAPIs\n\nREST\nPostgres\nInfluxDB\nJava (embedded)\n\nCommand-line options\nThe following resource provides info on options that may be passed to QuestDB\nwhen starting services:\n\nCommand-line options for starting and\n  running QuestDB from an executable\n\nConfiguration\nThe configuration page shows all the properties\nthat can be used to customize QuestDB.\nILP Client libraries\n\nClients overview\nJava ILP client\n\nData Types\nThe data types page lists the datatypes that\ncan be used in QuestDB.\nFunctions\n\nAggregate\nAnalytic\nBinary\nBoolean\nConditional\nDate and time\nMeta\nNumeric\nRandom value generator\nRow generator\nSpatial\nText\nTimestamp generator\nTimestamp\nTrigonometric\n\nOperators\n\nBitwise\nPattern matching\nSpatial\n\nSQL\n\nSQL Execution order\nData types\nALTER TABLE ADD COLUMN\nALTER TABLE RENAME COLUMN\nALTER TABLE DROP COLUMN\nALTER TABLE ATTACH PARTITION\nALTER TABLE DETACH PARTITION\nALTER TABLE DROP PARTITION\nALTER TABLE SET PARAM\nALTER TABLE ALTER COLUMN ADD INDEX\nALTER TABLE ALTER COLUMN DROP INDEX\nALTER TABLE ALTER COLUMN CACHE | NOCACHE\nALTER TABLE SET TYPE\nALTER TABLE RESUME WAL\nBACKUP\nCASE\nCAST\nCOPY\nCREATE TABLE\nDISTINCT\nEXPLAIN\nFILL\nDROP TABLE\nGROUP BY\nINSERT\nJOIN\nLATEST ON\nLIMIT\nORDER BY\nREINDEX\nRENAME TABLE\nSAMPLE BY\nSELECT\nSHOW\nSNAPSHOT\nTRUNCATE TABLE\nUNION EXCEPT INTERSECT\nVACUUM TABLE\nWHERE\nWITH\n\nSupport\nFor hints on diagnosing common configuration issues, see the following\nresources:\n\nTroubleshooting FAQ guide with solutions for\n  various HW & SW configuration issues\nList of OS error codes page with the\n  list of Operating System error codes\n\nWe are happy to help with any question you may have, particularly to help you\noptimize the performance of your application. Feel free to reach out using the\nfollowing channels:\n\nRaise an issue on GitHub\nJoin the Community Slack\nQuestDB on Stack Overflow\n",
    "tag": "questdb"
  },
  {
    "title": "Insert data",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/develop/insert-data.md",
    "content": "Insert data\nimport Tabs from \"@theme/Tabs\"\nimport TabItem from \"@theme/TabItem\"\nimport { RemoteRepoExample } from \"@theme/RemoteRepoExample\"\nThis page shows how to insert data into QuestDB using different programming\nlanguages and tools.\nInfluxDB Line Protocol is the recommended primary\ningestion method in QuestDB and is recommended for high-performance\napplications.\nFor transactional data inserts, use the\nPostgreSQL wire protocol.\nFor operational (ad-hoc) data ingestion, the Web Console makes\nit easy to upload CSV files and insert via SQL statements. You can also perform\nthese same actions via the HTTP REST API. For\nlarge CSV import (database migrations), use SQL\n`COPY`.\nIn summary, these are the different options:\n\nInfluxDB Line Protocol\nHigh performance.\nOptional automatic timestamps.\nOptional integrated authentication.\nClient libraries in various programming\n    languages.\nPostgreSQL wire protocol\nSQL `INSERT` statements, including parameterized queries.\nUse `psql` on the command line.\nInteroperability with third-party tools and libraries.\nWeb Console\nCSV upload.\nSQL `INSERT` statements.\nSQL `COPY` for large CSV import.\nHTTP REST API\nCSV upload.\nSQL `INSERT` statements.\nUse `curl` on the command line.\n\nInfluxDB Line Protocol\nThe InfluxDB Line Protocol (ILP) is a text protocol over TCP on port 9009.\nIt is a one-way protocol to insert data, focusing on simplicity and performance.\nHere is a summary table showing how it compares with other ways to insert data\nthat we support:\n| Protocol                 | Record Insertion Reporting       | Data Insertion Performance          |\n| :----------------------- | :------------------------------- | :---------------------------------- |\n| InfluxDB Line Protocol   | Server logs; Disconnect on error | Best                            |\n| CSV upload via HTTP REST | Configurable                     | Very Good                           |\n| SQL `INSERT` statements  | Transaction-level                | Good                                |\n| SQL `COPY` statements    | Transaction-level                | Suitable for one-off data migration |\nThis interface is the preferred ingestion method as it provides the following\nbenefits:\n\nHigh-throughput ingestion\nRobust ingestion from multiple sources into tables with dedicated systems for\n  reducing congestion\nConfigurable commit-lag for out-of-order data via\n  server configuration\n  settings\n\nWith sufficient client-side validation, the lack of errors to the client and\nconfirmation isn't necessarily a concern: QuestDB will log out any issues and\ndisconnect on error. The database will process any valid lines up to that point\nand insert rows.\nOn the InfluxDB line protocol page, you may\nfind additional details on the message format, ports and authentication.\nThe Telegraf guide helps you configure a\nTelegraf agent to collect and send metrics to QuestDB via ILP.\n:::tip\nThe ILP client libraries provide more\nuser-friendly ILP clients for a growing number of languages.\n:::\nExamples\nThese examples send a few rows of input. These use client libraries as well as\nraw TCP socket connections, when a client library is not available.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```ruby\nrequire 'socket'\nHOST = 'localhost'\nPORT = 9009\nReturns the current time in nanoseconds\ndef time_in_nsec\n    now = Time.now\n    return now.to_i * (10 ** 9) + now.nsec\nend\nbegin\n    s = TCPSocket.new HOST, PORT\n    # Single record insert\n    s.puts \"trades,name=client_timestamp value=12.4 #{time_in_nsec}\\n\"\n    # Omitting the timestamp allows the server to assign one\n    s.puts \"trades,name=client_timestamp value=12.4\\n\"\n    # Streams of readings must be newline-delimited\n    s.puts \"trades,name=client_timestamp value=12.4\\n\" +\n            \"trades,name=client_timestamp value=11.4\\n\"\nrescue SocketError => ex\n    puts ex.inspect\nensure\n    s.close() if s\nend\n```\n\n\n```php\n\n```\n\n\nTimestamps\nProviding a timestamp is optional. If one isn't provided, the server will\nautomatically assign the server's system time as the row's timestamp value.\nTimestamps are interpreted as the number of nanoseconds from 1st Jan 1970 UTC,\nunless otherwise configured. See `cairo.timestamp.locale` and\n`line.tcp.timestamp` configuration options.\nILP Datatypes and Casts\nStrings vs Symbols\nStrings may be recorded as either the `STRING` type or the `SYMBOL` type.\nInspecting a sample ILP we can see how a space `' '` separator splits `SYMBOL`\ncolumns to the left from the rest of the columns.\n`text\ntable_name,col1=symbol_val1,col2=symbol_val2 col3=\"string val\",col4=10.5\n                                            \u252c\n                                            \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 separator`\nIn this example, columns `col1` and `col2` are strings written to the database\nas `SYMBOL`s, whilst `col3` is written out as a `STRING`.\n`SYMBOL`s are strings with which are automatically\ninterned by the database on a\nper-column basis. You should use this type if you expect the string to be\nre-used over and over, such as is common with identifiers.\nFor one-off strings use `STRING` columns which aren't interned.\nCasts\nQuestDB types are a superset of those supported by ILP. This means that when\nsending data you should be aware of the performed conversions.\nSee:\n\nQuestDB Types in SQL\nILP types and cast conversion tables\n\nConstructing well-formed messages\nDifferent library implementations will perform different degrees content\nvalidation upfront before sending messages out. To avoid encountering issues,\nfollow these guidelines:\n\n\nAll strings must be UTF-8 encoded.\n\n\nColumns should only appear once per row.\n\n\nSymbol columns must be written out before other columns.\n\n\nTable and column names can't have invalid characters. These should not\n  contain `?`, `.`,`,`, `'`, `\"`, `\\`, `/`, `:`, `(`, `)`, `+`, `-`, `*`, `%`,\n  `~`,`' '` (space), `\\0` (nul terminator),\n  ZERO WIDTH NO-BREAK SPACE.\n\n\nWrite timestamp column via designated API, or at the end of the message if\n  you are using raw sockets. If you have multiple timestamp columns write\n  additional ones as column values.\n\n\nDon't change column type between rows.\n\n\nSupply timestamps in order. These need to be at least equal to previous\n  ones in the same table, unless using the out of order feature. This is not\n  necessary if you use the out-of-order\n  feature.\n\n\nErrors in Server Logs\nQuestDB will always log any ILP errors in its\nserver logs.\nHere is an example error from the server logs caused when a line attempted to\ninsert a `STRING` into a `SYMBOL` column.\n`text\n2022-04-13T13:35:19.784654Z E i.q.c.l.t.LineTcpConnectionContext [3968] could not process line data [table=bad_ilp_example, msg=cast error for line protocol string [columnWriterIndex=0, columnType=SYMBOL], errno=0]\n2022-04-13T13:35:19.784670Z I tcp-line-server scheduling disconnect [fd=3968, reason=0]`\nInserting NULL values\nTo insert a NULL value, skip the column (or symbol) for that row.\nFor example:\n`text\ntable1 a=10.5 1647357688714369403\ntable1 b=1.25 1647357698714369403`\nWill insert as:\n| a      | b      | timestamp                   |\n| :----- | :----- | --------------------------- |\n| 10.5   | NULL | 2022-03-15T15:21:28.714369Z |\n| NULL | 1.25   | 2022-03-15T15:21:38.714369Z |\nIf you don't immediately see data\nIf you don't see your inserted data, this is usually down to one of two things:\n\n\nYou prepared the messages, but forgot to call `.flush()` or similar in your\n  client library, so no data was sent.\n\n\nThe internal timers and buffers within QuestDB did not commit the data yet.\n  For development (and development only), you may want to tweak configuration\n  settings to commit data more frequently.\n  `ini title=server.conf\n  cairo.max.uncommitted.rows=1`\n  Refer to\n  ILP's commit strategy\n  documentation for more on these configuration settings.\n\n\nAuthentication\nILP can additionally provide authentication. This is an optional feature which\nis documented here.\nThird-party Library Compatibility\nUse our own client libraries and/or protocol\ndocumentation: Clients intended to work with InfluxDB will not work with\nQuestDB.\nPostgreSQL wire protocol\nQuestDB also supports the same wire protocol as PostgreSQL, allowing you to\nconnect and query the database with various third-party pre-existing client\nlibraries and tools.\nYou can connect to TCP port `8812` and use both `INSERT` and `SELECT` SQL\nqueries.\nPostgreSQL wire protocol is better suited for applications inserting via SQL\nprogrammatically as it provides parameterized queries, which avoid SQL injection\nissues.\n:::tip\nInfluxDB Line Protocol is the recommended primary\ningestion method in QuestDB. SQL `INSERT` statements over the PostgreSQL offer\nfeedback and error reporting, but have worse overall performance.\n:::\nHere are a few examples demonstrating SQL `INSERT` queries:\n\n\nCreate the table:\n`shell\npsql -h localhost -p 8812 -U admin -d qdb \\\n    -c \"CREATE TABLE IF NOT EXISTS t1 (name STRING, value INT);\"`\nInsert row:\n`shell\npsql -h localhost -p 8812 -U admin -d qdb -c \"INSERT INTO t1 VALUES('a', 42)\"`\nQuery back:\n`shell\npsql -h localhost -p 8812 -U admin -d qdb -c \"SELECT * FROM t1\"`\nNote that you can also run `psql` from Docker without installing the client\nlocally:\n`docker run -it --rm --network=host -e PGPASSWORD=quest \\\n    postgres psql ....`\n\n\nThis example uses the\npsychopg3 adapter.\nTo install the client library, use `pip`:\n`shell\npython3 -m pip install \"psycopg[binary]\"`\n```python\nimport psycopg as pg\nimport time\nConnect to an existing QuestDB instance\nconn_str = 'user=admin password=quest host=127.0.0.1 port=8812 dbname=qdb'\nwith pg.connect(conn_str, autocommit=True) as connection:\n\n\n```# Open a cursor to perform database operations\n\nwith connection.cursor() as cur:\n\n    # Execute a command: this creates a new table\n\n    cur.execute('''\n      CREATE TABLE IF NOT EXISTS test_pg (\n          ts TIMESTAMP,\n          name STRING,\n          value INT\n      ) timestamp(ts);\n      ''')\n\n    print('Table created.')\n\n    # Insert data into the table.\n\n    for x in range(10):\n\n        # Converting datetime into millisecond for QuestDB\n\n        timestamp = time.time_ns() // 1000\n\n        cur.execute('''\n            INSERT INTO test_pg\n                VALUES (%s, %s, %s);\n            ''',\n            (timestamp, 'python example', x))\n\n    print('Rows inserted.')\n\n    #Query the database and obtain data as Python objects.\n\n    cur.execute('SELECT * FROM trades_pg;')\n    records = cur.fetchall()\n    for row in records:\n        print(row)\n```\n\n\nthe connection is now closed\n```\n\n\n```java\npackage com.myco;\nimport java.sql.*;\nimport java.util.Properties;\nclass App {\n  public static void main(String[] args) throws SQLException {\n    Properties properties = new Properties();\n    properties.setProperty(\"user\", \"admin\");\n    properties.setProperty(\"password\", \"quest\");\n    properties.setProperty(\"sslmode\", \"disable\");\n\n\n```final Connection connection = DriverManager.getConnection(\n  \"jdbc:postgresql://localhost:8812/qdb\", properties);\nconnection.setAutoCommit(false);\n\nfinal PreparedStatement statement = connection.prepareStatement(\n  \"CREATE TABLE IF NOT EXISTS trades (\" +\n  \"    ts TIMESTAMP, date DATE, name STRING, value INT\" +\n  \") timestamp(ts);\");\nstatement.execute();\n\ntry (PreparedStatement preparedStatement = connection.prepareStatement(\n    \"INSERT INTO TRADES  VALUES (?, ?, ?, ?)\")) {\n  preparedStatement.setTimestamp(\n    1,\n    new Timestamp(io.questdb.std.Os.currentTimeMicros()));\n  preparedStatement.setDate(2, new Date(System.currentTimeMillis()));\n  preparedStatement.setString(3, \"abc\");\n  preparedStatement.setInt(4, 123);\n  preparedStatement.execute();\n}\nSystem.out.println(\"Done\");\nconnection.close();\n```\n\n\n}\n}\n```\n\n\nThis example uses the pg package which\nallows for quickly building queries using Postgres wire protocol. Details on the\nuse of this package can be found on the\nnode-postgres documentation.\nThis example uses naive `Date.now() * 1000` inserts for Timestamp types in\nmicrosecond resolution. For accurate microsecond timestamps, the\nprocess.hrtime.bigint()\ncall can be used.\n```javascript\n\"use strict\"\nconst { Client } = require(\"pg\")\nconst start = async () => {\n  const client = new Client({\n    database: \"qdb\",\n    host: \"127.0.0.1\",\n    password: \"quest\",\n    port: 8812,\n    user: \"admin\",\n  })\n  await client.connect()\nconst createTable = await client.query(\n    \"CREATE TABLE IF NOT EXISTS trades (\" +\n      \"    ts TIMESTAMP, date DATE, name STRING, value INT\" +\n      \") timestamp(ts);\",\n  )\n  console.log(createTable)\nlet now = new Date().toISOString()\n  const insertData = await client.query(\n    \"INSERT INTO trades VALUES($1, $2, $3, $4);\",\n    [now, now, \"node pg example\", 123],\n  )\n  await client.query(\"COMMIT\")\nconsole.log(insertData)\nfor (let rows = 0; rows < 10; rows++) {\n    // Providing a 'name' field allows for prepared statements / bind variables\n    now = new Date().toISOString()\n    const query = {\n      name: \"insert-values\",\n      text: \"INSERT INTO trades VALUES($1, $2, $3, $4);\",\n      values: [now, now, \"node pg prep statement\", rows],\n    }\n    await client.query(query)\n  }\n  await client.query(\"COMMIT\")\nconst readAll = await client.query(\"SELECT * FROM trades\")\n  console.log(readAll.rows)\nawait client.end()\n}\nstart()\n  .then(() => console.log(\"Done\"))\n  .catch(console.error)\n```\n\n\nThis example uses the pgx driver and toolkit for\nPostgreSQL in Go. More details on the use of this toolkit can be found on the\nGitHub repository for pgx.\n```go\npackage main\nimport (\n  \"context\"\n  \"fmt\"\n  \"log\"\n  \"time\"\n\"github.com/jackc/pgx/v4\"\n)\nvar conn *pgx.Conn\nvar err error\nfunc main() {\n  ctx := context.Background()\n  conn, _ = pgx.Connect(ctx, \"postgresql://admin:quest@localhost:8812/qdb\")\n  defer conn.Close(ctx)\n// text-based query\n  _, err := conn.Exec(ctx,\n    (\"CREATE TABLE IF NOT EXISTS trades (\" +\n     \"    ts TIMESTAMP, date DATE, name STRING, value INT\" +\n     \") timestamp(ts);\"))\n  if err != nil {\n    log.Fatalln(err)\n  }\n// Prepared statement given the name 'ps1'\n  _, err = conn.Prepare(ctx, \"ps1\", \"INSERT INTO trades VALUES($1,$2,$3,$4)\")\n  if err != nil {\n    log.Fatalln(err)\n  }\n// Insert all rows in a single commit\n  tx, err := conn.Begin(ctx)\n  if err != nil {\n    log.Fatalln(err)\n  }\nfor i := 0; i < 10; i++ {\n    // Execute 'ps1' statement with a string and the loop iterator value\n    _, err = conn.Exec(\n      ctx,\n      \"ps1\",\n      time.Now(),\n      time.Now().Round(time.Millisecond),\n      \"go prepared statement\",\n      i + 1)\n    if err != nil {\n      log.Fatalln(err)\n    }\n  }\n// Commit the transaction\n  err = tx.Commit(ctx)\n  if err != nil {\n    log.Fatalln(err)\n  }\n// Read all rows from table\n  rows, err := conn.Query(ctx, \"SELECT * FROM trades\")\n  fmt.Println(\"Reading from trades table:\")\n  for rows.Next() {\n    var name string\n    var value int64\n    var ts time.Time\n    var date time.Time\n    err = rows.Scan(&ts, &date, &name, &value)\n    fmt.Println(ts, date, name, value)\n  }\nerr = conn.Close(ctx)\n}\n```\n\n\nThe following example shows how to use parameterized queries and prepared\nstatements using the rust-postgres\nclient.\n```rust\nuse postgres::{Client, NoTls, Error};\nuse chrono::{Utc};\nuse std::time::SystemTime;\nfn main() -> Result<(), Error> {\n    let mut client = Client::connect(\"postgresql://admin:quest@localhost:8812/qdb\", NoTls)?;\n\n\n```// Basic query\nclient.batch_execute(\n  \"CREATE TABLE IF NOT EXISTS trades ( \\\n      ts TIMESTAMP, date DATE, name STRING, value INT \\\n  ) timestamp(ts);\")?;\n\n// Parameterized query\nlet name: &str = \"rust example\";\nlet val: i32 = 123;\nlet utc = Utc::now();\nlet sys_time = SystemTime::now();\nclient.execute(\n    \"INSERT INTO trades VALUES($1,$2,$3,$4)\",\n    &[&utc.naive_local(), &sys_time, &name, &val],\n)?;\n\n// Prepared statement\nlet mut txn = client.transaction()?;\nlet statement = txn.prepare(\"INSERT INTO trades VALUES ($1,$2,$3,$4)\")?;\nfor value in 0..10 {\n    let utc = Utc::now();\n    let sys_time = SystemTime::now();\n    txn.execute(&statement, &[&utc.naive_local(), &sys_time, &name, &value])?;\n}\ntxn.commit()?;\n\nprintln!(\"import finished\");\nOk(())\n```\n\n\n}\n```\n\n\nWeb Console\nQuestDB ships with an embedded Web Console running\nby default on port `9000`.\n```questdb-sql title='Creating a table and inserting some data'\nCREATE TABLE takeaway_order (ts TIMESTAMP, id SYMBOL, status SYMBOL)\n  TIMESTAMP(ts);\nINSERT INTO takeaway_order VALUES (now(), 'order1', 'placed');\nINSERT INTO takeaway_order VALUES (now(), 'order2', 'placed');\n```\nSQL statements can be written in the code editor and executed by clicking the\nRun button. Note that the web console runs a single statement at a time.\nFor inserting bulk data or migrating data from other databases, see\nlarge CSV import.\nHTTP REST API\nQuestDB exposes a REST API for compatibility with a wide range of libraries and\ntools. The REST API is accessible on port `9000` and has the following\ninsert-capable entrypoints:\n| Entrypoint                                 | HTTP Method | Description                             | API Docs                                                     |\n| :----------------------------------------- | :---------- | :-------------------------------------- | :----------------------------------------------------------- |\n| /imp      | POST        | Import CSV data                         | Reference      |\n| /exec?query=.. | GET         | Run SQL Query returning JSON result set | Reference |\nFor details such as content type, query parameters and more, refer to the\nREST API docs.\n`/imp`: Uploading Tabular Data\n:::tip\nInfluxDB Line Protocol is the recommended primary\ningestion method in QuestDB. CSV uploading offers insertion feedback and error\nreporting, but has worse overall performance.\nSee `/imp`'s atomicity query\nparameter to customize behavior on error.\n:::\nLet's assume you want to upload the following data via the `/imp` entrypoint:\n\n\n`csv title=data.csv\ncol1,col2,col3\na,10.5,True\nb,100,False\nc,,True`\n\n\n| col1 | col2   | col3    |\n| :--- | :----- | :------ |\n| a    | 10.5   | true  |\n| b    | 100    | false |\n| c    | NULL | true  |\n\n\nYou can do so via the command line using `cURL` or programmatically via HTTP\nAPIs in your scripts and applications.\nBy default, the response is designed to be human-readable. Use the `fmt=json`\nquery argument to obtain a response in JSON. You can also specify the schema\nexplicitly. See the second example in Python for these features.\n\n\nThis example imports a CSV file with automatic schema detection.\n`shell title=\"Basic import with table name\"\ncurl -F data=@data.csv http://localhost:9000/imp?name=table_name`\nThis example overwrites an existing table and specifies a timestamp format and a\ndesignated timestamp column. For more information on the optional parameters to\nspecify timestamp formats, partitioning and renaming tables, see the\nREST API documentation.\n`bash title=\"Providing a user-defined schema\"\ncurl \\\n-F schema='[{\"name\":\"ts\", \"type\": \"TIMESTAMP\", \"pattern\": \"yyyy-MM-dd - HH:mm:ss\"}]' \\\n-F data=@weather.csv 'http://localhost:9000/imp?overwrite=true&timestamp=ts'`\n\n\nThis first example shows uploading the `data.csv` file with automatic schema\ndetection.\n```python\nimport sys\nimport requests\ncsv = {'data': ('my_table', open('./data.csv', 'r'))}\nhost = 'http://localhost:9000'\ntry:\n    response = requests.post(host + '/imp', files=csv)\n    print(response.text)\nexcept requests.exceptions.RequestException as e:\n    print(f'Error: {e}', file=sys.stderr)\n```\nThe second example creates a CSV buffer from Python objects and uploads them\nwith a custom schema. Note UTF-8 encoding.\nThe `fmt=json` parameter allows us to obtain a parsable response, rather than a\ntabular response designed for human consumption.\n```python\nimport io\nimport csv\nimport requests\nimport pprint\nimport json\ndef to_csv_str(table):\n    output = io.StringIO()\n    csv.writer(output, dialect='excel').writerows(table)\n    return output.getvalue().encode('utf-8')\ndef main():\n    table_name = 'example_table2'\n    table = [\n        ['col1', 'col2', 'col3'],\n        ['a',    10.5,   True],\n        ['b',    100,    False],\n        ['c',    None,   True]]\n\n\n```table_csv = to_csv_str(table)\nprint(table_csv)\nschema = json.dumps([\n    {'name': 'col1', 'type': 'SYMBOL'},\n    {'name': 'col2', 'type': 'DOUBLE'},\n    {'name': 'col3', 'type': 'BOOLEAN'}])\nresponse = requests.post(\n    'http://localhost:9000/imp',\n    params={'fmt': 'json'},\n    files={\n        'schema': schema,\n        'data': (table_name, table_csv)}).json()\n\n# You can parse the `status` field and `error` fields\n# of individual columns. See Reference/API/REST docs for details.\npprint.pprint(response)\n```\n\n\nif name == 'main':\n    main()\n```\n\n\n```javascript\nconst fetch = require(\"node-fetch\")\nconst FormData = require(\"form-data\")\nconst fs = require(\"fs\")\nconst HOST = \"http://localhost:9000\"\nasync function run() {\n  const form = new FormData()\nform.append(\"data\", fs.readFileSync(__dirname + \"/data.csv\"), {\n    filename: fileMetadata.name,\n    contentType: \"application/octet-stream\",\n  })\ntry {\n    const r = await fetch(`${HOST}/imp`, {\n      method: \"POST\",\n      body: form,\n      headers: form.getHeaders(),\n    })\n\n\n```console.log(r)\n```\n\n\n} catch (e) {\n    console.error(e)\n  }\n}\nrun()\n```\n\n\n```go\npackage main\nimport (\n  \"bytes\"\n  \"fmt\"\n  \"io\"\n  \"io/ioutil\"\n  \"log\"\n  \"mime/multipart\"\n  \"net/http\"\n  \"net/url\"\n  \"os\"\n)\nfunc main() {\n  u, err := url.Parse(\"http://localhost:9000\")\n  checkErr(err)\n  u.Path += \"imp\"\n  url := fmt.Sprintf(\"%v\", u)\n  fileName := \"/path/to/data.csv\"\n  file, err := os.Open(fileName)\n  checkErr(err)\ndefer file.Close()\nbuf := new(bytes.Buffer)\n  writer := multipart.NewWriter(buf)\n  uploadFile, _ := writer.CreateFormFile(\"data\", \"data.csv\")\n  _, err = io.Copy(uploadFile, file)\n  checkErr(err)\n  writer.Close()\nreq, err := http.NewRequest(http.MethodPut, url, buf)\n  checkErr(err)\n  req.Header.Add(\"Content-Type\", writer.FormDataContentType())\nclient := &http.Client{}\n  res, err := client.Do(req)\n  checkErr(err)\ndefer res.Body.Close()\nbody, err := ioutil.ReadAll(res.Body)\n  checkErr(err)\nlog.Println(string(body))\n}\nfunc checkErr(err error) {\n  if err != nil {\n    panic(err)\n  }\n}\n```\n\n\n`/exec`: SQL `INSERT` Query\nThe `/exec` entrypoint takes a SQL query and returns results as JSON.\nWe can use this for quick SQL inserts too, but note that there's no support for\nparameterized queries that are necessary to avoid SQL injection issues.\n:::tip\nPrefer the PostgreSQL interface if you are\ngenerating sql programmatically.\nPrefer ILP if you need high-performance inserts.\n:::\n\n\n```shell\nCreate Table\ncurl -G \\\n  --data-urlencode \"query=CREATE TABLE IF NOT EXISTS trades(name STRING, value INT)\" \\\n  http://localhost:9000/exec\nInsert a row\ncurl -G \\\n  --data-urlencode \"query=INSERT INTO trades VALUES('abc', 123456)\" \\\n  http://localhost:9000/exec\n```\n\n\n```python\nimport sys\nimport requests\nimport json\nhost = 'http://localhost:9000'\ndef run_query(sql_query):\n    query_params = {'query': sql_query, 'fmt' : 'json'}\n    try:\n        response = requests.get(host + '/exec', params=query_params)\n        json_response = json.loads(response.text)\n        print(json_response)\n    except requests.exceptions.RequestException as e:\n        print(f'Error: {e}', file=sys.stderr)\ncreate table\nrun_query(\"CREATE TABLE IF NOT EXISTS trades (name STRING, value INT)\")\ninsert row\nrun_query(\"INSERT INTO trades VALUES('abc', 123456)\")\n```\n\n\nThe `node-fetch` package can be installed using `npm i node-fetch`.\n```javascript\nconst fetch = require(\"node-fetch\")\nconst HOST = \"http://localhost:9000\"\nasync function createTable() {\n  try {\n    const query = \"CREATE TABLE IF NOT EXISTS trades (name STRING, value INT)\"\n\n\n```const response = await fetch(\n  `${HOST}/exec?query=${encodeURIComponent(query)}`,\n)\nconst json = await response.json()\n\nconsole.log(json)\n```\n\n\n} catch (error) {\n    console.log(error)\n  }\n}\nasync function insertData() {\n  try {\n    const query = \"INSERT INTO trades VALUES('abc', 123456)\"\n\n\n```const response = await fetch(\n  `${HOST}/exec?query=${encodeURIComponent(query)}`,\n)\nconst json = await response.json()\n\nconsole.log(json)\n```\n\n\n} catch (error) {\n    console.log(error)\n  }\n}\ncreateTable().then(insertData)\n```\n\n\n```go\npackage main\nimport (\n  \"fmt\"\n  \"io/ioutil\"\n  \"log\"\n  \"net/http\"\n  \"net/url\"\n)\nfunc main() {\n  u, err := url.Parse(\"http://localhost:9000\")\n  checkErr(err)\nu.Path += \"exec\"\n  params := url.Values{}\n  params.Add(\"query\", `CREATE TABLE IF NOT EXISTS\n      trades (name STRING, value INT);\n    INSERT INTO\n      trades\n    VALUES(\n      \"abc\",\n      123456\n    );`)\n  u.RawQuery = params.Encode()\n  url := fmt.Sprintf(\"%v\", u)\nres, err := http.Get(url)\n  checkErr(err)\ndefer res.Body.Close()\nbody, err := ioutil.ReadAll(res.Body)\n  checkErr(err)\nlog.Println(string(body))\n}\nfunc checkErr(err error) {\n  if err != nil {\n    panic(err)\n  }\n}\n```\n\n",
    "tag": "questdb"
  },
  {
    "title": "Query data",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/develop/query-data.md",
    "content": "Query data\nimport Tabs from \"@theme/Tabs\"\nimport TabItem from \"@theme/TabItem\"\nThis page describes how to query data from QuestDB using different programming\nlanguages and tools.\nFor ad-hoc SQL queries, including CSV download and charting use the web console.\nApplications can choose between the HTTP REST API which returns JSON or use\nthe PostgreSQL wire protocol.\nHere are all your options:\n\nWeb Console\nSQL `SELECT` statements.\nDownload query results as CSV.\nChart query results.\nPostgreSQL wire protocol\nSQL `SELECT` statements.\nUse `psql` on the command line.\nInteroperability with third-party tools and libraries.\nHTTP REST API\nSQL `SELECT` statements as JSON or CSV.\nResult paging.\n\nWeb Console\nQuestDB ships with an embedded Web Console running by default on port `9000`.\nimport Screenshot from \"@theme/Screenshot\"\n\n\n\nTo query data from the web console, SQL statements can be written in the code\neditor and executed by clicking the Run button.\n```questdb-sql title='Listing tables and querying a table'\nSHOW TABLES;\nSELECT * FROM my_table;\n--Note that `SELECT * FROM` is optional\nmy_table;\n```\nAside from the Code Editor, the Web Console includes a data visualization panel\nfor viewing query results as tables or graphs and an Import tab for uploading\ndatasets as CSV files. For more details on these components and general use of\nthe console, see the Web Console page.\nPostgreSQL wire protocol\nYou can query data using the Postgres endpoint\nthat QuestDB exposes which is accessible by default via port `8812`. Examples in\nmultiple languages are shown below. To learn more, check out our docs about \nPostgres compatibility and tools.\n\n\n```python\nimport psycopg2\nconnection = None\ncursor = None\ntry:\n    connection = psycopg2.connect(\n        user='admin',\n        password='quest',\n        host='127.0.0.1',\n        port='8812',\n        database='qdb')\n    cursor = connection.cursor()\n    postgreSQL_select_Query = 'SELECT x FROM long_sequence(5);'\n    cursor.execute(postgreSQL_select_Query)\n    print('Selecting rows from test table using cursor.fetchall')\n    mobile_records = cursor.fetchall()\n\n\n```print(\"Print each row and it's columns values\")\nfor row in mobile_records:\n    print(\"y = \", row[0], \"\\n\")\n```\n\n\nexcept (Exception, psycopg2.Error) as error:\n    print(\"Error while fetching data from PostgreSQL\", error)\nfinally:\n    if cursor:\n        cursor.close()\n    if connection:\n        connection.close()\n    print(\"PostgreSQL connection is closed\")\n```\n\n\n```java\npackage com.myco;\nimport java.sql.*;\nimport java.util.Properties;\npublic class App {\n    public static void main(String[] args) throws SQLException {\n        Properties properties = new Properties();\n        properties.setProperty(\"user\", \"admin\");\n        properties.setProperty(\"password\", \"quest\");\n        properties.setProperty(\"sslmode\", \"disable\");\n\n\n```    final Connection connection = DriverManager.getConnection(\n        \"jdbc:postgresql://localhost:8812/qdb\", properties);\n    try (PreparedStatement preparedStatement = connection.prepareStatement(\n            \"SELECT x FROM long_sequence(5);\")) {\n        try (ResultSet rs = preparedStatement.executeQuery()) {\n            while (rs.next()) {\n                System.out.println(rs.getLong(1));\n            }\n        }\n    }\n    connection.close();\n}\n```\n\n\n}\n```\n\n\n```javascript\n\"use strict\"\nconst { Client } = require(\"pg\")\nconst start = async () => {\n  const client = new Client({\n    database: \"qdb\",\n    host: \"127.0.0.1\",\n    password: \"quest\",\n    port: 8812,\n    user: \"admin\",\n  })\n  await client.connect()\nconst res = await client.query(\"SELECT x FROM long_sequence(5);\")\nconsole.log(res.rows)\nawait client.end()\n}\nstart().catch(console.error)\n```\n\n\n```go\npackage main\nimport (\n  \"database/sql\"\n  \"fmt\"\n_ \"github.com/lib/pq\"\n)\nconst (\n  host     = \"localhost\"\n  port     = 8812\n  user     = \"admin\"\n  password = \"quest\"\n  dbname   = \"qdb\"\n)\nfunc main() {\n  connStr := fmt.Sprintf(\n    \"host=%s port=%d user=%s password=%s dbname=%s sslmode=disable\",\n    host, port, user, password, dbname)\n  db, err := sql.Open(\"postgres\", connStr)\n  checkErr(err)\n  defer db.Close()\n// Currently, we do not support queries with bind parameters in Go\n  rows, err := db.Query(\"SELECT x FROM long_sequence(5);\")\n  checkErr(err)\n  defer rows.Close()\nfor rows.Next() {\n    var num string\n    err = rows.Scan(&num)\n    checkErr(err)\n    fmt.Println(num)\n  }\nerr = rows.Err()\n  checkErr(err)\n}\nfunc checkErr(err error) {\n  if err != nil {\n    panic(err)\n  }\n}\n```\n\n\n```c\n// compile with\n// g++ libpq_example.c -o libpq_example.exe  -I pgsql\\include -L dev\\pgsql\\lib\n// -std=c++17  -lpthread -lpq\ninclude \ninclude \ninclude \nvoid do_exit(PGconn *conn) {\n  PQfinish(conn);\n  exit(1);\n}\nint main() {\n  PGconn conn = PQconnectdb(\n      \"host=localhost user=admin password=quest port=8812 dbname=testdb\");\n  if (PQstatus(conn) == CONNECTION_BAD) {\n    fprintf(stderr, \"Connection to database failed: %s\\n\",\n            PQerrorMessage(conn));\n    do_exit(conn);\n  }\n  PGresult res = PQexec(conn, \"SELECT x FROM long_sequence(5);\");\n  if (PQresultStatus(res) != PGRES_TUPLES_OK) {\n    printf(\"No data retrieved\\n\");\n    PQclear(res);\n    do_exit(conn);\n  }\n  int rows = PQntuples(res);\n  for (int i = 0; i < rows; i++) {\n    printf(\"%s\\n\", PQgetvalue(res, i, 0));\n  }\n  PQclear(res);\n  PQfinish(conn);\n  return 0;\n}\n```\n\n\n```csharp\nusing Npgsql;\nstring username = \"admin\";\nstring password = \"quest\";\nstring database = \"qdb\";\nint port = 8812;\nvar connectionString = $@\"host=localhost;port={port};username={username};password={password};\ndatabase={database};ServerCompatibilityMode=NoTypeLoading;\";\nawait using NpgsqlConnection connection = new NpgsqlConnection(connectionString);\nawait connection.OpenAsync();\nvar sql = \"SELECT x FROM long_sequence(5);\";\nawait using NpgsqlCommand command = new NpgsqlCommand(sql, connection);\nawait using (var reader = await command.ExecuteReaderAsync()) {\n    while (await reader.ReadAsync())\n    {\n        var x = reader.GetInt64(0);\n    }\n}\n```\n\n\n`ruby\nrequire 'pg'\nbegin\n    conn =PG.connect( host: \"127.0.0.1\", port: 8812, dbname: 'qdb', \n                      user: 'admin', password: 'quest' )\n    rows = conn.exec 'SELECT x FROM long_sequence(5);'\n    rows.each do |row|\n        puts row\n    end\nrescue PG::Error => e\n     puts e.message\nensure\n    conn.close if conn\nend`\n\n\n```php\ngetMessage(), \"\\n\";\n} finally {\n        if (!is_null($db_conn)) {\n                pg_close($db_conn);\n        }\n}\n\n?>\n```\n\n\nHTTP REST API\nQuestDB exposes a REST API for compatibility with a wide range of libraries and\ntools. The REST API is accessible on port `9000` and has the following\nquery-capable entrypoints:\n|Entrypoint                                 |HTTP Method|Description                            |API Docs                                                    |\n|:------------------------------------------|:----------|:--------------------------------------|:-----------------------------------------------------------|\n|/exp?query=..   |GET        |Export SQL Query as CSV                |Reference     |\n|/exec?query=..|GET        |Run SQL Query returning JSON result set|Reference|\nFor details such as content type, query parameters and more, refer to the\nREST API docs.\n`/exp`: SQL Query to CSV\nThe `/exp` entrypoint allows querying the database with a SQL select query and\nobtaining the results as CSV.\nFor obtaining results in JSON, use `/exec` instead, documented next.\n\n\n`bash\ncurl -G --data-urlencode \\\n    \"query=SELECT * FROM example_table2 LIMIT 3\" \\\n    http://localhost:9000/exp`\n`csv\n\"col1\",\"col2\",\"col3\"\n\"a\",10.5,true\n\"b\",100.0,false\n\"c\",,true`\n\n\n```python\nimport requests\nresp = requests.get(\n    'http://localhost:9000/exp',\n    {\n        'query': 'SELECT * FROM example_table2',\n        'limit': '3,6'   # Rows 3, 4, 5\n    })\nprint(resp.text)\n```\n`csv\n\"col1\",\"col2\",\"col3\"\n\"d\",20.5,true\n\"e\",200.0,false\n\"f\",,true`\n\n\n`/exec`: SQL Query to JSON\nThe `/exec` entrypoint takes a SQL query and returns results as JSON.\nThis is similar to the `/exec` entry point which returns results as CSV.\nQuerying Data\n\n\n\n`shell\ncurl -G \\\n  --data-urlencode \"query=SELECT x FROM long_sequence(5);\" \\\n  http://localhost:9000/exec`\nThe JSON response contains the original query, a `\"columns\"` key with the schema\nof the results, a `\"count\"` number of rows and a `\"dataset\"` with the results.\n`json\n{\n    \"query\": \"SELECT x FROM long_sequence(5);\",\n    \"columns\": [\n        {\"name\": \"x\", \"type\": \"LONG\"}],\n    \"dataset\": [\n        [1],\n        [2],\n        [3],\n        [4],\n        [5]],\n    \"count\": 5\n}`\n\n\n```python\nimport sys\nimport requests\nhost = 'http://localhost:9000'\nsql_query = \"select * from long_sequence(10)\"\ntry:\n    response = requests.get(\n        host + '/exec',\n        params={'query': sql_query}).json()\n    for row in response['dataset']:\n        print(row[0])\nexcept requests.exceptions.RequestException as e:\n    print(f'Error: {e}', file=sys.stderr)\n```\n\n\n```javascript\nconst fetch = require(\"node-fetch\")\nconst HOST = \"http://localhost:9000\"\nasync function run() {\n  try {\n    const query = \"SELECT x FROM long_sequence(5);\"\n\n\n```const response = await fetch(`${HOST}/exec?query=${encodeURIComponent(query)}`)\nconst json = await response.json()\n\nconsole.log(json)\n```\n\n\n} catch (error) {\n    console.log(error)\n  }\n}\nrun()\n```\n\n\n```go\npackage main\nimport (\n  \"fmt\"\n  \"io/ioutil\"\n  \"log\"\n  \"net/http\"\n  \"net/url\"\n)\nfunc main() {\n  u, err := url.Parse(\"http://localhost:9000\")\n  checkErr(err)\nu.Path += \"exec\"\n  params := url.Values{}\n  params.Add(\"query\", \"SELECT x FROM long_sequence(5);\")\n  u.RawQuery = params.Encode()\n  url := fmt.Sprintf(\"%v\", u)\nres, err := http.Get(url)\n  checkErr(err)\ndefer res.Body.Close()\nbody, err := ioutil.ReadAll(res.Body)\n  checkErr(err)\nlog.Println(string(body))\n}\nfunc checkErr(err error) {\n  if err != nil {\n    panic(err)\n  }\n}\n```\n\n",
    "tag": "questdb"
  },
  {
    "title": "Connect",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/develop/connect.md",
    "content": "Connect\nYou can interact with a QuestDB database by connecting to one of its various\nnetwork endpoints.\n|Network Endpoint|Port|Inserting & modifying data*|Querying data|\n|:---------------|:---|:-------------------------------------|:------------|\n|Web Console|9000|SQL `INSERT`, `UPDATE`, CSV|SQL `SELECT`, charting|\n|InfluxDB Line Protocol|9009|High performance bulk insert|-|\n|PostgreSQL wire protocol|8812|SQL `INSERT`, `UPDATE`|SQL `SELECT`|\n|HTTP REST API|9000|SQL `INSERT`, `UPDATE`, CSV|SQL `SELECT`, CSV|\n`*` `UPDATE` is available from QuestDB 6.4.\n:::note\nAll network ports may be configured.\n:::\nWeb console\nThe web console is a general admin and query\ninterface.\nIt's great for quickly trying things out. You can also chart your query results.\nConnect your web browser to http://[server-address]:9000/. When running locally,\nthis will be `http://localhost:9000/`.\nimport Screenshot from \"@theme/Screenshot\"\n\n\n\nInfluxDB Line Protocol\nThe fastest way to insert data into QuestDB is using the InfluxDB Line\nProtocol (ILP).\nIt is an insert-only protocol that bypasses SQL `INSERT` statements achieving\nhigher throughput.\n`shell\nreadings,city=London temperature=23.2 1465839830100400000\\n\nreadings,city=London temperature=23.6 1465839830100700000\\n\nreadings,make=Honeywell temperature=23.2,humidity=0.443 1465839830100800000\\n`\nOur ILP tutorial covers\ningesting data with various client libraries.\nFor a more in-depth understanding, see our\nprotocol documentation.\nPostgreSQL wire protocol\nFor SQL, we support the same wire protocol as PostgreSQL, allowing you to\nconnect and query the database with various third-party pre-existing client\nlibraries and tools.\n```python\nimport psycopg2\nconnection = None\ntry:\n    connection = psycopg2.connect(\n        user=\"admin\",\n        password=\"quest\",\n        host=\"127.0.0.1\",\n        port=\"8812\",\n        database=\"qdb\")\nfinally:\n    if (connection):\n        connection.close()\n```\nSee how you can connect through the PostgreSQL wire protocol from\ndifferent programming languages to:\n\nInsert data\nQuery data\n\nHTTP REST API\nThe HTTP interface that hosts the web console also provides a REST API for\nimporting data, exporting data and querying.\n`shell\ncurl -F data=@data.csv http://localhost:9000/imp`\nFind out how to:\n\nInsert data\n",
    "tag": "questdb"
  },
  {
    "title": "Accessing the Web Console",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/develop/web-console.md",
    "content": "\ntitle: Web Console\ndescription:\n  How to use the Web Console in QuestDB for importing, querying, and visualizing\n  data.\n\nThe Web Console is a client that allows you to interact with QuestDB. It\nprovides UI tools to query data and visualize the results in a table or plot.\nimport Screenshot from \"@theme/Screenshot\"\n\nAccessing the Web Console\nThe Web Console will be available at `http://[server-address]:9000`. When\nrunning locally, this will be `http://localhost:9000`.\nLayout\n\nSystem tables in Schema explorer\nIt is possible to hide QuestDB system tables (`telemetry` and\n`telemetry_config`) in Schema explorer by setting up the following configuration\noption in a server.conf\nfile:\n`bash title=\"/var/lib/questdb/conf/server.conf\"\ntelemetry.hide.tables=true`\nCode editor\nThe default panel shown in the web console is the code editor which allows you\nto write and run SQL queries.\nShortcuts\n|Command       |Action                                                                      |\n|:-------------|:---------------------------------------------------------------------------|\n|Run query     |`f9` or `ctrl/cmd + enter`                                                  |\n|Locate cursor |`f2`, use this to focus the SQL editor on your cursor in order to locate it |\nBehaviour\nAs you can write multiple SQL commands separated by a semicolon, the Web Console\nuses the following logic to decide which queries to execute:\n\nCheck if a query or part of a query is highlighted. If yes, it will be\n  executed, otherwise:\nCheck if the cursor is within a SQL statement. If yes, the statement will be\n  executed, otherwise:\nCheck if the cursor is on the same line as a SQL statement and after the\n  semicolon. If yes, this statement will be executed, finally:\nIf the cursor is on a line that does not contain a SQL statement, the next\n  encountered statement will be executed. If there is no statement after the\n  cursor, the previous statement will be used.\n\nVisualizing results\nYou can run a query and click on the `Chart` button. This will display the chart\neditor. You can then choose chart type, for example `line` and then press\n`Draw`.\nDownloading results\nYou can download the query result by clicking the `CSV` button. This file will\nbe useful to test the import functionality below.\nNotification panel\nThe panel at the bottom of the web console shows the status of the most-recent\nquery. This panel can be toggled by clicking the up-arrow icon on the right of\nthe panel and shows the last 20 messages and notifications after query\nexecution.\n\nImport\nThe import tab can be accessed by clicking this icon on the left-side navigation\nmenu:\n\nImport details\nDescription of the fields in the import details table\n|Column       |Description                                                                                    |\n|:------------|:----------------------------------------------------------------------------------------------|\n|`File name`  |Name of the imported file. If imported from copy & paste, an automatically-generated file name |\n|`Size`       |Size of the imported file                                                                      |\n|`Total rows` |Number of rows successfully imported                                                           |\n|`Failed rows`|Number of rows that failed to import                                                           |\n|`Header row` |Whether the dataset has been recognized to have a header row or not                            |\n|`Status`     |Status of the import. See import statuses                                  |\nImport statuses\nDescription of the import statuses\n|Status              |Description                                                                                                                                                                                     |\n|:-------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n|`importing`         |Data is currently being imported                                                                                                                                                                |\n|`failed`            |Import failed, no data was imported                                                                                                                                                             |\n|`imported in [time]`|Import is finished. The completion time is displayed next to the status                                                                                                                         |\n|`exists`            |You are trying to import a file that already exists. To import it regardless, you can either append or override. See importing again for a more exhaustive description|\nAmending the schema\nAlthough the schema is automatically detected, you can amend the type of any\ncolumn of an imported data set using the following steps:\n\nClick on the file you want to amend in the Import screen. The schema will be\n  displayed in a table in the lower-half of the screen.\nClick the column which type you want to change. A drop-down list allows for\n  selecting a type for the column.\nYou will then need to re-trigger the import.\n\n\nCustom import\nYou can amend the import behavior with the following options. This will trigger\nto import the data again.\n|Option |Name                         |Description                                                        |\n|:------|:----------------------------|:------------------------------------------------------------------|\n|`A`    |Append                       |Uploaded data will be appended to the end of the table             |\n|`O`    |Override                     |Uploaded data will override existing data in the table             |\n|`LEV`  |Skip lines with extra values |Skips rows that contain values that don't align with the schema    |\n|`H`    |Header row                   |Flag whether the first row should be considered to be a header row |\nTo start the import, click the following button:\n\nProviding an asset path\nIt's possible to provide an asset path if QuestDB is being run from somewhere\nthat is not the server root. In this case, create a `.env` file in the UI\ndirectory of QuestDB and provide the path to web console assets as follows:\n`bash\nASSET_PATH=/path/to/questdb/ui`\nAn\nexample dotenv\nfile is provided which can be renamed to `.env` and placed in QuestDB's UI",
    "tag": "questdb"
  },
  {
    "title": "Create Table",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/develop/update-data.md",
    "content": "\ntitle: Update data\ndescription:\n  This page demonstrates how to update time series data in QuestDB from\n  NodeJS, Java, Python and cURL. The examples show how to use the REST and Postgres APIs.\n\nimport Tabs from \"@theme/Tabs\"\nimport TabItem from \"@theme/TabItem\"\nThis page shows how to update existing data in QuestDB using different programming\nlanguages and tools. There are two main methods for updating data:\n\nPostgres wire protocol for compatibility with a\n  range of clients\nREST API provides access to QuestDB via HTTP\n\nPrerequisites\nThis page assumes that QuestDB is running and accessible. QuestDB can be run\nusing either Docker, the\nBinaries or\nHomebrew for macOS users.\nPostgres compatibility\nYou can query data using the Postgres endpoint\nthat QuestDB exposes. This is accessible via port `8812` by default. More\ninformation on the Postgres wire protocol implementation with details on\nsupported features can be found on the\nPostgres API reference page.\n\n\n\n\nThis example uses the pg package which\nallows for quickly building queries using Postgres wire protocol. Details on the\nuse of this package can be found on the\nnode-postgres documentation.\nThis example uses naive `Date.now() * 1000` inserts for Timestamp types in\nmicrosecond resolution. For accurate microsecond timestamps, the\nprocess.hrtime.bigint() call can be used.\n```javascript\n\"use strict\"\nconst { Client } = require(\"pg\")\nconst start = async () => {\n  const client = new Client({\n    database: \"qdb\",\n    host: \"127.0.0.1\",\n    password: \"quest\",\n    port: 8812,\n    user: \"admin\",\n    options: \"-c statement_timeout=300000\"\n  })\n  await client.connect()\nconst createTable = await client.query(\n    \"CREATE TABLE IF NOT EXISTS trades (ts TIMESTAMP, date DATE, name STRING, value INT) timestamp(ts);\"\n  )\n  console.log(createTable)\nfor (let rows = 0; rows < 10; rows++) {\n    // Providing a 'name' field allows for prepared statements / bind variables\n    let now = new Date().toISOString()\n    const query = {\n      name: \"insert-values\",\n      text: \"INSERT INTO trades VALUES($1, $2, $3, $4);\",\n      values: [now, now, \"node pg prep statement\", rows],\n    }\n    await client.query(query)\n  }\nconst updateData = await client.query(\n          \"UPDATE trades SET name = 'update example', value = 123 WHERE value > 7;\"\n  )\n  console.log(updateData)\nawait client.query(\"COMMIT\")\nconst readAll = await client.query(\"SELECT * FROM trades\")\n  console.log(readAll.rows)\nawait client.end()\n}\nstart()\n  .then(() => console.log(\"Done\"))\n  .catch(console.error)\n```\n\n\n```java\npackage com.myco;\nimport java.sql.*;\nimport java.util.Properties;\nclass App {\n  public static void main(String[] args) throws SQLException {\n    Properties properties = new Properties();\n    properties.setProperty(\"user\", \"admin\");\n    properties.setProperty(\"password\", \"quest\");\n    properties.setProperty(\"sslmode\", \"disable\");\n    properties.setProperty(\"options\", \"-c statement_timeout=300000\");\n\n\n```final Connection connection = DriverManager.getConnection(\"jdbc:postgresql://localhost:8812/qdb\", properties);\nconnection.setAutoCommit(false);\n\nfinal PreparedStatement statement = connection.prepareStatement(\"CREATE TABLE IF NOT EXISTS trades (ts TIMESTAMP, date DATE, name STRING, value INT) timestamp(ts);\");\nstatement.execute();\n\ntry (PreparedStatement preparedStatement = connection.prepareStatement(\"INSERT INTO trades VALUES (?, ?, ?, ?)\")) {\n  preparedStatement.setTimestamp(1, new Timestamp(io.questdb.std.Os.currentTimeMicros()));\n  preparedStatement.setDate(2, new Date(System.currentTimeMillis()));\n  preparedStatement.setString(3, \"abc\");\n  preparedStatement.setInt(4, 123);\n  preparedStatement.execute();\n}\n\nfinal PreparedStatement statement = connection.prepareStatement(\"UPDATE trades SET value = value + 1000;\");\nstatement.execute();\n\nSystem.out.println(\"Done\");\nconnection.close();\n```\n\n\n}\n}\n```\n\n\nThis example uses the psycopg2 database\nadapter which does not support prepared statements (bind variables). This\nfunctionality is on the roadmap for the antecedent\npsychopg3 adapter.\n```python\nimport psycopg2 as pg\nimport datetime as dt\ntry:\n    connection = pg.connect(user=\"admin\",\n                            password=\"quest\",\n                            host=\"127.0.0.1\",\n                            port=\"8812\",\n                            database=\"qdb\",\n                            options='-c statement_timeout=300000')\n    cursor = connection.cursor()\n\n\n```# text-only query\ncursor.execute(\"CREATE TABLE IF NOT EXISTS trades (ts TIMESTAMP, date DATE, name STRING, value INT) timestamp(ts);\")\n\n# insert 10 records\nfor x in range(10):\n  now = dt.datetime.utcnow()\n  date = dt.datetime.now().date()\n  cursor.execute(\"\"\"\n    INSERT INTO trades\n    VALUES (%s, %s, %s, %s);\n    \"\"\", (now, date, \"python example\", x))\n# commit records\nconnection.commit()\n\n# update records\ncursor.execute(\"UPDATE trades SET value = value + 100;\")\n\ncursor.execute(\"SELECT * FROM trades;\")\nrecords = cursor.fetchall()\nfor row in records:\n    print(row)\n```\n\n\nfinally:\n    if (connection):\n        cursor.close()\n        connection.close()\n        print(\"Postgres connection is closed\")\n```\n\n\nREST API\nQuestDB exposes a REST API for compatibility with a wide range of libraries and\ntools. The REST API is accessible on port `9000` and has the following\nentrypoints:\n\n`/imp` - import data\n`/exec` - execute an SQL statement\n\nMore details on the use of these entrypoints can be found on the\nREST API reference page.\n`/imp` endpoint\nThe `/imp` endpoint does not allow for updating data.\n`/exec` endpoint\nAlternatively, the `/exec` endpoint can be used to create a table and the\n`INSERT` statement can be used to populate it with values:\n\n\n\n\n```shell\nCreate Table\ncurl -G \\\n  --data-urlencode \"query=CREATE TABLE IF NOT EXISTS trades(name STRING, value INT)\" \\\n  http://localhost:9000/exec\nInsert a row\ncurl -G \\\n  --data-urlencode \"query=INSERT INTO trades VALUES('abc', 123456)\" \\\n  http://localhost:9000/exec\nUpdate a row\ncurl -G \\\n  --data-urlencode \"query=UPDATE trades SET value = 9876 WHERE name = 'abc'\" \\\n  http://localhost:9000/exec\n```\n\n\nThe `node-fetch` package can be installed using `npm i node-fetch`.\n```javascript\nconst fetch = require(\"node-fetch\")\nconst HOST = \"http://localhost:9000\"\nasync function createTable() {\n  try {\n    const query = \"CREATE TABLE IF NOT EXISTS trades (name STRING, value INT)\";\n\n\n```const response = await fetch(`${HOST}/exec?query=${encodeURIComponent(query)}`)\nconst json = await response.json()\n\nconsole.log(json)\n```\n\n\n} catch (error) {\n    console.log(error)\n  }\n}\nasync function insertData() {\n  try {\n    const query = \"INSERT INTO trades VALUES('abc', 123456)\"\n\n\n```const response = await fetch(`${HOST}/exec?query=${encodeURIComponent(query)}`)\nconst json = await response.json()\n\nconsole.log(json)\n```\n\n\n} catch (error) {\n    console.log(error)\n  }\n}\nasync function updateData() {\n  try {\n    const query = \"UPDATE trades SET value = 9876 WHERE name = 'abc'\"\n\n\n```const response = await fetch(`${HOST}/exec?query=${encodeURIComponent(query)}`)\nconst json = await response.json()\n\nconsole.log(json)\n```\n\n\n} catch (error) {\n    console.log(error)\n  }\n}\ncreateTable().then(insertData).then(updateData)\n```\n\n\n```python\nimport requests\nimport json\nhost = 'http://localhost:9000'\ndef run_query(sql_query):\n  query_params = {'query': sql_query, 'fmt' : 'json'}\n  try:\n    response = requests.get(host + '/exec', params=query_params)\n    json_response = json.loads(response.text)\n    print(json_response)\n  except requests.exceptions.RequestException as e:\n    print(\"Error: %s\" % (e))\ncreate table\nrun_query(\"CREATE TABLE IF NOT EXISTS trades (name STRING, value INT)\")\ninsert row\nrun_query(\"INSERT INTO trades VALUES('abc', 123456)\")\nupdate row\nrun_query(\"UPDATE trades SET value = 9876 WHERE name = 'abc'\")\n```\n\n\nWeb Console\nBy default, QuestDB has an embedded Web Console running at\nhttp://[server-address]:9000. When running locally, this is accessible at\n`http://localhost:9000`. The Web Console can be used to\nexplore table schemas, visualizing query results as tables or graphs, and\nimporting datasets from CSV files. For details on these components, refer to the",
    "tag": "questdb"
  },
  {
    "title": "How do I delete a row?",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/troubleshooting/faq.md",
    "content": "\ntitle: FAQ\ndescription: FAQ for QuestDB troubleshooting.\n\nThe following document contains common hardware and software configuration\nissues met when running QuestDB, as well as solutions to them.\nHow do I delete a row?\nSee our guide on modifying data.\nHow do I convert a `STRING` column to a `SYMBOL` or vice versa?\nThe SQL `UPDATE` keyword can be used to change the data type of a column. The\nsame approach can also be used to increase the\ncapacity of a SYMBOL column that is\nundersized.\nThe steps are as follows:\n\nAdd a new column to the table and define the desired data type.\nStop data ingestion and increase\n   SQL query timeout,\n   `query.timeout.sec`, as `UPDATE` may take a while to complete. Depending on\n   the size of the column, we recommend to increase the value significantly: the\n   default is 60 seconds and it may be reasonable to increase it to one hour.\n   Restart the instance after changing the configuration, to activate the\n   change.\nUse `UPDATE` to copy the existing column content to the new column. Now, the\n   column has the correct content with the new data type.\nDelete the old column.\nRename the new column accordingly. For example, to change `old_col` from\n   `STRING` to `SYMBOL` for table `my_table`:\n\n`questdb-sql\nALTER TABLE my_table ADD COLUMN new_col SYMBOL;\nUPDATE my_table SET new_col = old_col;\nALTER TABLE my_table DROP COLUMN old_col;\nALTER TABLE my_table RENAME COLUMN new_col TO old_col;`\nWhy do I get `table busy` error messages when inserting data over PostgreSQL wire protocol?\nYou may get `table busy [reason=insert]` or similar errors when running `INSERT`\nstatements concurrently on the same table. This means that the table is locked\nby inserts issued from another SQL connection or other client protocols for data\nimport, like ILP over TCP or CSV over HTTP. To reduce the chances of getting\nthis error, try using auto-commit to keep the transaction as short as possible.\nWe're also considering adding automatic insert retries on the database side, but\nfor now, it is safe to handle this error on the client side and retry the\ninsert.\nWhy do I see `could not open read-write` messages when creating a table or inserting rows?\nLog messages may appear like the following:\n`2022-02-01T13:40:11.336011Z I i.q.c.l.t.LineTcpMeasurementScheduler could not create table [tableName=cpu, ex=could not open read-write\n...\nio.questdb.cairo.CairoException: [24] could not open read-only [file=/root/.questdb/db/cpu/service.k]`\nThe machine may have insufficient limits for the maximum number of open files.\nTry checking the `ulimit` value on your machine. Refer to\ncapacity planning page\nfor more details.\nWhy do I see `errno=12` mmap messages in the server logs?\nLog messages may appear like the following:\n`2022-02-01T13:40:10.636014Z E i.q.c.l.t.LineTcpConnectionContext [8655] could not process line data [table=test_table, msg=could not mmap  [size=248, offset=0, fd=1766, memUsed=314809894008, fileLen=8192], errno=12]`\nThe machine may have insufficient limits of memory map areas a process may have.\nTry checking and increasing the `vm.max_map_count` value on your machine. Refer\nto\ncapacity planning\npage for more details.\nHow do I avoid duplicate rows with identical fields?\nWe have an open\nfeature request to optionally de-duplicate rows\ninserted with identical fields. Until then, you need to\nmodify the data after it's inserted and use a\n`GROUP BY` query to identify duplicates.\nCan I query by time?\nYes! When using the `WHERE` statement to define the time range for a query, the\nIN keyword allows\nmodifying the range and interval of the search. The range can be tuned to a\nsecond resolution.\nFor example, the following query search for daily records between 9:15 to 16:00\ninclusively from Jan 1 2000 for 365 days. The original timestamp,\n2000-01-01T09:15, is extended for 405 minutes to cover the range. This range is\nrepeated every day for 365 times:\n```questdb-sql\nSELECT timestamp, col1 FROM 'table1' WHERE timestamp IN`2000-01-01T09:15;405m;1d;365';",
    "tag": "questdb"
  },
  {
    "title": "Where to find error codes",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/troubleshooting/os-error-codes.md",
    "content": "\ntitle: List of OS error codes\ndescription:\n  List of OS error codes that may be reported by QuestDB running on Linux and\n  Windows.\n\nThe following document contains a partial list of Operating System (OS) error\ncodes that can be reported when running QuestDB and brief descriptions for them.\nWhere to find error codes\nQuestDB includes OS error codes into the `[<code>]` part of the exception\nmessage written to the error logs:\n`io.questdb.cairo.CairoException: [24] could not open read-only [file=/root/.questdb/db/cpu/service.k]`\nThe above message reports error code 24 which is \"Too many open files\" on Linux.\nSome error log messages may also include `errno=<code>` key/value pair:\n`2022-02-01T13:40:10.636014Z E i.q.c.l.t.LineTcpConnectionContext [8655] could not process line data [table=test_table, msg=could not mmap  [size=248, offset=0, fd=1766, memUsed=314809894008, fileLen=8192], errno=12]`\nThe above message reports error code 12 which is \"Out of memory\" on Linux.\nLinux error codes\n| Error number | Error name      | Description                                      |\n| ------------ | --------------- | ------------------------------------------------ |\n| 1            | EPERM           | Operation not permitted.                         |\n| 2            | ENOENT          | No such file or directory.                       |\n| 3            | ESRCH           | No such process.                                 |\n| 4            | EINTR           | Interrupted system call.                         |\n| 5            | EIO             | I/O error.                                       |\n| 6            | ENXIO           | No such device or address.                       |\n| 7            | E2BIG           | Argument list too long.                          |\n| 8            | ENOEXEC         | Exec format error.                               |\n| 9            | EBADF           | Bad file number.                                 |\n| 10           | ECHILD          | No child processes.                              |\n| 11           | EAGAIN          | Try again.                                       |\n| 12           | ENOMEM          | Out of memory.                                   |\n| 13           | EACCES          | Permission denied.                               |\n| 14           | EFAULT          | Bad address.                                     |\n| 15           | ENOTBLK         | Block device required.                           |\n| 16           | EBUSY           | Device or resource busy.                         |\n| 17           | EEXIST          | File exists.                                     |\n| 18           | EXDEV           | Cross-device link.                               |\n| 19           | ENODEV          | No such device.                                  |\n| 20           | ENOTDIR         | Not a directory.                                 |\n| 21           | EISDIR          | Is a directory.                                  |\n| 22           | EINVAL          | Invalid argument.                                |\n| 23           | ENFILE          | File table overflow.                             |\n| 24           | EMFILE          | Too many open files.                             |\n| 25           | ENOTTY          | Not a typewriter.                                |\n| 26           | ETXTBSY         | Text file busy.                                  |\n| 27           | EFBIG           | File too large.                                  |\n| 28           | ENOSPC          | No space left on device.                         |\n| 29           | ESPIPE          | Illegal seek.                                    |\n| 30           | EROFS           | Read-only file system.                           |\n| 31           | EMLINK          | Too many links.                                  |\n| 32           | EPIPE           | Broken pipe.                                     |\n| 33           | EDOM            | Math argument out of domain of func.             |\n| 34           | ERANGE          | Math result not representable.                   |\n| 35           | EDEADLK         | Resource deadlock would occur.                   |\n| 36           | ENAMETOOLONG    | File name too long.                              |\n| 37           | ENOLCK          | No record locks available.                       |\n| 38           | ENOSYS          | Function not implemented.                        |\n| 39           | ENOTEMPTY       | Directory not empty.                             |\n| 40           | ELOOP           | Too many symbolic links encountered.             |\n| 42           | ENOMSG          | No message of desired type.                      |\n| 43           | EIDRM           | Identifier removed.                              |\n| 44           | ECHRNG          | Channel number out of range.                     |\n| 45           | EL2NSYNC        | Level 2 not synchronized.                        |\n| 46           | EL3HLT          | Level 3 halted.                                  |\n| 47           | EL3RST          | Level 3 reset.                                   |\n| 48           | ELNRNG          | Link number out of range.                        |\n| 49           | EUNATCH         | Protocol driver not attached.                    |\n| 50           | ENOCSI          | No CSI structure available.                      |\n| 51           | EL2HLT          | Level 2 halted.                                  |\n| 52           | EBADE           | Invalid exchange.                                |\n| 53           | EBADR           | Invalid request descriptor.                      |\n| 54           | EXFULL          | Exchange full.                                   |\n| 55           | ENOANO          | No anode.                                        |\n| 56           | EBADRQC         | Invalid request code.                            |\n| 57           | EBADSLT         | Invalid slot.                                    |\n| 59           | EBFONT          | Bad font file format.                            |\n| 60           | ENOSTR          | Device not a stream.                             |\n| 61           | ENODATA         | No data available.                               |\n| 62           | ETIME           | Timer expired.                                   |\n| 63           | ENOSR           | Out of streams resources.                        |\n| 64           | ENONET          | Machine is not on the network.                   |\n| 65           | ENOPKG          | Package not installed.                           |\n| 66           | EREMOTE         | Object is remote.                                |\n| 67           | ENOLINK         | Link has been severed.                           |\n| 68           | EADV            | Advertise error.                                 |\n| 69           | ESRMNT          | Srmount error.                                   |\n| 70           | ECOMM           | Communication error on send.                     |\n| 71           | EPROTO          | Protocol error.                                  |\n| 72           | EMULTIHOP       | Multihop attempted.                              |\n| 73           | EDOTDOT         | RFS specific error.                              |\n| 74           | EBADMSG         | Not a data message.                              |\n| 75           | EOVERFLOW       | Value too large for defined data type.           |\n| 76           | ENOTUNIQ        | Name not unique on network.                      |\n| 77           | EBADFD          | File descriptor in bad state.                    |\n| 78           | EREMCHG         | Remote address changed.                          |\n| 79           | ELIBACC         | Can not access a needed shared library.          |\n| 80           | ELIBBAD         | Accessing a corrupted shared library.            |\n| 81           | ELIBSCN         | .lib section in a.out corrupted.                 |\n| 82           | ELIBMAX         | Attempting to link in too many shared libraries. |\n| 83           | ELIBEXEC        | Cannot exec a shared library directly.           |\n| 84           | EILSEQ          | Illegal byte sequence.                           |\n| 85           | ERESTART        | Interrupted system call should be restarted.     |\n| 86           | ESTRPIPE        | Streams pipe error.                              |\n| 87           | EUSERS          | Too many users.                                  |\n| 88           | ENOTSOCK        | Socket operation on non-socket.                  |\n| 89           | EDESTADDRREQ    | Destination address required.                    |\n| 90           | EMSGSIZE        | Message too long.                                |\n| 91           | EPROTOTYPE      | Protocol wrong type for socket.                  |\n| 92           | ENOPROTOOPT     | Protocol not available.                          |\n| 93           | EPROTONOSUPPORT | Protocol not supported.                          |\n| 94           | ESOCKTNOSUPPORT | Socket type not supported.                       |\n| 95           | EOPNOTSUPP      | Operation not supported on transport endpoint.   |\n| 96           | EPFNOSUPPORT    | Protocol family not supported.                   |\n| 97           | EAFNOSUPPORT    | Address family not supported by protocol.        |\n| 98           | EADDRINUSE      | Address already in use.                          |\n| 99           | EADDRNOTAVAIL   | Cannot assign requested address.                 |\n| 100          | ENETDOWN        | Network is down.                                 |\n| 101          | ENETUNREACH     | Network is unreachable.                          |\n| 102          | ENETRESET       | Network dropped connection because of reset.     |\n| 103          | ECONNABORTED    | Software caused connection abort.                |\n| 104          | ECONNRESET      | Connection reset by peer.                        |\n| 105          | ENOBUFS         | No buffer space available.                       |\n| 106          | EISCONN         | Transport endpoint is already connected.         |\n| 107          | ENOTCONN        | Transport endpoint is not connected.             |\n| 108          | ESHUTDOWN       | Cannot send after transport endpoint shutdown.   |\n| 109          | ETOOMANYREFS    | Too many references: cannot splice.              |\n| 110          | ETIMEDOUT       | Connection timed out.                            |\n| 111          | ECONNREFUSED    | Connection refused.                              |\n| 112          | EHOSTDOWN       | Host is down.                                    |\n| 113          | EHOSTUNREACH    | No route to host.                                |\n| 114          | EALREADY        | Operation already in progress.                   |\n| 115          | EINPROGRESS     | Operation now in progress.                       |\n| 116          | ESTALE          | Stale NFS file handle.                           |\n| 117          | EUCLEAN         | Structure needs cleaning.                        |\n| 118          | ENOTNAM         | Not a XENIX named type file.                     |\n| 119          | ENAVAIL         | No XENIX semaphores available.                   |\n| 120          | EISNAM          | Is a named type file.                            |\n| 121          | EREMOTEIO       | Remote I/O error.                                |\n| 122          | EDQUOT          | Quota exceeded.                                  |\n| 123          | ENOMEDIUM       | No medium found.                                 |\n| 124          | EMEDIUMTYPE     | Wrong medium type.                               |\n| 125          | ECANCELED       | Operation Canceled.                              |\n| 126          | ENOKEY          | Required key not available.                      |\n| 127          | EKEYEXPIRED     | Key has expired.                                 |\n| 128          | EKEYREVOKED     | Key has been revoked.                            |\n| 129          | EKEYREJECTED    | Key was rejected by service.                     |\n| 130          | EOWNERDEAD      | Owner died.                                      |\n| 131          | ENOTRECOVERABLE | State not recoverable.                           |\nWindows error codes\nA complete list of Windows error codes may be found here.\n| Error number | Error name                    | Description                                                                                  |\n| ------------ | ----------------------------- | -------------------------------------------------------------------------------------------- |\n| 1            | ERROR_INVALID_FUNCTION        | Incorrect function.                                                                          |\n| 2            | ERROR_FILE_NOT_FOUND          | The system cannot find the file specified.                                                   |\n| 3            | ERROR_PATH_NOT_FOUND          | The system cannot find the path specified.                                                   |\n| 4            | ERROR_TOO_MANY_OPEN_FILES     | The system cannot open the file.                                                             |\n| 5            | ERROR_ACCESS_DENIED           | Access is denied.                                                                            |\n| 6            | ERROR_INVALID_HANDLE          | The handle is invalid.                                                                       |\n| 7            | ERROR_ARENA_TRASHED           | The storage control blocks were destroyed.                                                   |\n| 8            | ERROR_NOT_ENOUGH_MEMORY       | Not enough storage is available to process this command.                                     |\n| 9            | ERROR_INVALID_BLOCK           | The storage control block address is invalid.                                                |\n| 10           | ERROR_BAD_ENVIRONMENT         | The environment is incorrect.                                                                |\n| 11           | ERROR_BAD_FORMAT              | An attempt was made to load a program with an incorrect format.                              |\n| 12           | ERROR_INVALID_ACCESS          | The access code is invalid.                                                                  |\n| 13           | ERROR_INVALID_DATA            | The data is invalid.                                                                         |\n| 14           | ERROR_OUTOFMEMORY             | Not enough storage is available to complete this operation.                                  |\n| 15           | ERROR_INVALID_DRIVE           | The system cannot find the drive specified.                                                  |\n| 16           | ERROR_CURRENT_DIRECTORY       | The directory cannot be removed.                                                             |\n| 17           | ERROR_NOT_SAME_DEVICE         | The system cannot move the file to a different disk drive.                                   |\n| 18           | ERROR_NO_MORE_FILES           | There are no more files.                                                                     |\n| 19           | ERROR_WRITE_PROTECT           | The media is write protected.                                                                |\n| 20           | ERROR_BAD_UNIT                | The system cannot find the device specified.                                                 |\n| 21           | ERROR_NOT_READY               | The device is not ready.                                                                     |\n| 22           | ERROR_BAD_COMMAND             | The device does not recognize the command.                                                   |\n| 23           | ERROR_CRC                     | Data error (cyclic redundancy check).                                                        |\n| 24           | ERROR_BAD_LENGTH              | The program issued a command but the command length is incorrect.                            |\n| 25           | ERROR_SEEK                    | The drive cannot locate a specific area or track on the disk.                                |\n| 26           | ERROR_NOT_DOS_DISK            | The specified disk or diskette cannot be accessed.                                           |\n| 27           | ERROR_SECTOR_NOT_FOUND        | The drive cannot find the sector requested.                                                  |\n| 28           | ERROR_OUT_OF_PAPER            | The printer is out of paper.                                                                 |\n| 29           | ERROR_WRITE_FAULT             | The system cannot write to the specified device.                                             |\n| 30           | ERROR_READ_FAULT              | The system cannot read from the specified device.                                            |\n| 31           | ERROR_GEN_FAILURE             | A device attached to the system is not functioning.                                          |\n| 32           | ERROR_SHARING_VIOLATION       | The process cannot access the file because it is being used by another process.              |\n| 33           | ERROR_LOCK_VIOLATION          | The process cannot access the file because another process has locked a portion of the file. |\n| 34           | ERROR_WRONG_DISK              | The wrong diskette is in the drive. Insert %2 (Volume Serial Number: %3) into drive %1.      |\n| 36           | ERROR_SHARING_BUFFER_EXCEEDED | Too many files opened for sharing.                                                           |\n| 38           | ERROR_HANDLE_EOF              | Reached the end of the file.                                                                 |\n| 39           | ERROR_HANDLE_DISK_FULL        | The disk is full.                                                                            |\n| 87           | ERROR_INVALID_PARAMETER       | The parameter is incorrect.                                                                  |\n| 112          | ERROR_DISK_FULL               | The disk is full.                                                                            |\n| 123          | ERROR_INVALID_NAME            | The file name, directory name, or volume label syntax is incorrect.                          |\n| 1450         | ERROR_NO_SYSTEM_RESOURCES     | Insufficient system resources exist to complete the requested service.                       |\nMacOS error codes\n| Error number | Error name        | Description                                      |\n| ------------ | ----------------- | ------------------------------------------------ |\n| 0            | Base              | Undefined error: 0                               |\n| 1            | EPERM             | Operation not permitted                          |\n| 2            | ENOENT            | No such file or directory                        |\n| 3            | ESRCH             | No such process                                  |\n| 4            | EINTR             | Interrupted system call                          |\n| 5            | EIO               | Input/output error                               |\n| 6            | ENXIO             | Device not configured                            |\n| 7            | E2BIG             | Argument list too long                           |\n| 8            | ENOEXEC           | Exec format error                                |\n| 9            | EBADF             | Bad file descriptor                              |\n| 10           | ECHILD            | No child processes                               |\n| 11           | EDEADLK           | Resource deadlock avoided                        |\n| 12           | ENOMEM            | Cannot allocate memory                           |\n| 13           | EACCES            | Permission denied                                |\n| 14           | EFAULT            | Bad address                                      |\n| 15           | ENOTBLK           | Block device required                            | \n| 16           | EBUSY             | Device busy                                      |\n| 17           | EEXIST            | File exists                                      |\n| 18           | EXDEV             | Cross-device link                                |\n| 19           | ENODEV            | Operation not supported by device                |\n| 20           | ENOTDIR           | Not a directory                                  |\n| 21           | EISDIR            | Is a directory                                   |\n| 22           | EINVAL            | Invalid argument                                 |\n| 23           | ENFILE            | Too many open files in system                    |\n| 24           | EMFILE            | Too many open files                              |\n| 25           | ENOTTY            | Inappropriate ioctl for device                   |\n| 26           | ETXTBSY           | Text file busy                                   |\n| 27           | EFBIG             | File too large                                   |\n| 28           | ENOSPC            | No space left on device                          |\n| 29           | ESPIPE            | Illegal seek                                     |\n| 30           | EROFS             | Read-only file system                            |\n| 31           | EMLINK            | Too many links                                   |\n| 32           | EPIPE             | Broken pipe                                      |\n| 33           | EDOM              | Numerical argument out of domain                 |\n| 34           | ERANGE            | Result too large                                 |\n| 35           | EAGAIN            | Resource temporarily unavailable                 |\n| 36           | EINPROGRESS       | Operation now in progress                        | \n| 37           | EALREADY          | Operation already in progress                    |\n| 38           | ENOTSOCK          | Socket operation on non-socket                   |\n| 39           | EDESTADDRREQ      | Destination address required                     |\n| 40           | EMSGSIZE          | Message too long                                 |\n| 41           | EPROTOTYPE        | Protocol wrong type for socket                   |\n| 42           | ENOPROTOOPT       | Protocol not available                           |\n| 43           | EPROTONOSUPPORT   | Protocol not supported                           |\n| 44           | ESOCKTNOSUPPORT   | Socket type not supported                        |\n| 45           | ENOTSUP           | Operation not supported                          |\n| 46           | EPFNOSUPPORT      | Protocol family not supported                    |\n| 47           | EAFNOSUPPORT      | Address family not supported by protocol family  |\n| 48           | EADDRINUSE        | Address already in use                           |\n| 49           | EADDRNOTAVAIL     | Can\u2019t assign requested address                   |\n| 50           | ENETDOWN          | Network is down                                  |\n| 51           | ENETUNREACH       | Network is unreachable                           |\n| 52           | ENETRESET         | Network dropped connection on reset              |\n| 53           | ECONNABORTED      | Software caused connection abort                 |\n| 54           | ECONNRESET        | Connection reset by peer                         |\n| 55           | ENOBUFS           | No buffer space available                        |\n| 56           | EISCONN           | Socket is already connected                      |\n| 57           | ENOTCONN          | Socket is not connected                          |\n| 58           | ESHUTDOWN         | Can\u2019t send after socket shutdown                 |\n| 59           | ETOOMANYREFS      | Too many references: can\u2019t splice                |\n| 60           | ETIMEDOUT         | Operation timed out                              |\n| 61           | ECONNREFUSED      | Connection refused                               |\n| 62           | ELOOP             | Too many levels of symbolic links                |\n| 63           | ENAMETOOLONG      | File name too long                               |\n| 64           | EHOSTDOWN         | Host is down                                     |\n| 65           | EHOSTUNREACH      | No route to host                                 |\n| 66           | ENOTEMPTY         | Directory not empty                              |\n| 67           | EPROCLIM          | Too many processes                               |\n| 68           | EUSERS            | Too many users                                   |\n| 69           | EDQUOT            | Disc quota exceeded                              |\n| 70           | ESTALE            | Stale NFS file handle                            |\n| 71           | EREMOTE           | Too many levels of remote in path                |\n| 72           | EBADRPC           | RPC struct is bad                                |\n| 73           | ERPCMISMATCH      | RPC version wrong                                |\n| 74           | EPROGUNAVAIL      | RPC prog. not avail                              |\n| 75           | EPROGMISMATCH     | Program version wrong                            |\n| 76           | EPROCUNAVAIL      | Bad procedure for program                        |\n| 77           | ENOLCK            | No locks available                               |\n| 78           | ENOSYS            | Function not implemented                         |\n| 79           | EFTYPE            | Inappropriate file type or format                |\n| 80           | EAUTH             | Authentication error                             |\n| 81           | ENEEDAUTH         | Need authenticator                               |\n| 82           | EPWROFF           | Device power is off                              |\n| 83           | EDEVERR           | Device error                                     |\n| 84           | EOVERFLOW         | Value too large to be stored in data type        |\n| 85           | EBADEXEC          | Bad executable                                   |\n| 86           | EBADARCH          | Bad CPU type in executable                       |\n| 87           | ESHLIBVERS        | Shared library version mismatch                  |\n| 88           | EBADMACHO         | Malformed Macho file                             |\n| 89           | ECANCELED         | Operation canceled                               |\n| 90           | EIDRM             | Identifier removed                               |\n| 91           | ENOMSG            | No message of desired type                       |\n| 92           | EILSEQ            | Illegal byte sequence                            |\n| 93           | ENOATTR           | Attribute not found                              |\n| 94           | EBADMSG           | Bad message                                      |\n| 95           | EMULTIHOP         | EMULTIHOP (Reserved)                             |\n| 96           | ENODATA           | No message available on STREAM                   |\n| 97           | ENOLINK           | ENOLINK (Reserved)                               |\n| 98           | ENOSR             | No STREAM resources                              |\n| 99           | ENOSTR            | Not a STREAM                                     |\n| 100          | EPROTO            | Protocol error                                   |\n| 101          | ETIME             | STREAM ioctl timeout                             |\n| 102          | EOPNOTSUPP        | Operation not supported on socket                |\n| 103          | ENOPOLICY         | Policy not found                                 |\n| 104          | ENOTRECOVERABLE   | State not recoverable                            |\n| 105          | EOWNERDEAD        | Previous owner died                              |",
    "tag": "questdb"
  },
  {
    "title": "Prometheus metrics endpoint",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/operations/health-monitoring.md",
    "content": "\ntitle: Health monitoring\ndescription:\n  How to configure health monitoring for querying the status of a QuestDB\n  instance using an embedded server, Prometheus metrics and Alertmanager.\n\nThis document describes the options available for monitoring the health of a\nQuestDB instance. There are options for minimal health checks via a `min` server\nwhich provides a basic 'up/down' check, or detailed metrics in Prometheus format\nexposed via an HTTP endpoint.\nPrometheus metrics endpoint\nPrometheus is an open-source systems monitoring and alerting toolkit. Prometheus\ncollects and stores metrics as time series data, i.e. metrics information is\nstored with the timestamp at which it was recorded, alongside optional key-value\npairs called labels.\nQuestDB exposes a `/metrics` endpoint which provides internal system metrics in\nPrometheus format. To use this functionality and get started with example\nconfiguration, refer to the\nPrometheus documentation.\nMin health server\nREST APIs will often be situated behind a load balancer that uses a monitor URL\nfor its configuration. Having a load balancer query the QuestDB REST endpoints\n(on port `9000` by default) will cause internal logs to become excessively\nnoisy. Additionally, configuring per-URL logging would increase server latency.\nTo provide a dedicated health check feature that would have no performance knock\non other system components, we opted to decouple health checks from the REST\nendpoints used for querying and ingesting data. For this purpose, a `min` HTTP\nserver runs embedded in a QuestDB instance and has a separate log and thread\npool configuration.\nThe configuration section for the `min` HTTP server is available in the\nminimal HTTP server reference.\nThe `min` server is enabled by default and will reply to any `HTTP GET` request\nto port `9003`:\n`shell title=\"GET health status of local instance\"\ncurl -v http://127.0.0.1:9003`\nThe server will respond with an HTTP status code of `200`, indicating that the\nsystem is operational:\n```shell title=\"200 'OK' response\"\n*   Trying 127.0.0.1...\n* TCP_NODELAY set\n* Connected to 127.0.0.1 (127.0.0.1) port 9003 (#0)\n\nGET / HTTP/1.1\nHost: 127.0.0.1:9003\nUser-Agent: curl/7.64.1\nAccept: /\n< HTTP/1.1 200 OK\n< Server: questDB/1.0\n< Date: Tue, 26 Jan 2021 12:31:03 GMT\n< Transfer-Encoding: chunked\n< Content-Type: text/plain\n<\n* Connection #0 to host 127.0.0.1 left intact\n```\n\nPath segments are ignored which means that optional paths may be used in the URL\nand the server will respond with identical results, e.g.:\n`shell title=\"GET health status with arbitrary path\"\ncurl -v http://127.0.0.1:9003/status`\n:::info\nThe `/metrics` path segment is reserved for metrics exposed in Prometheus\nformat. For more details, see the\nPrometheus documentation.\n:::\nUnhandled error detection\nWhen metrics subsystem is\nenabled\non the database, the health endpoint checks the occurrences of unhandled,\ncritical errors since the database start and, if any of them were detected, it\nreturns HTTP 500 status code. The check is based on the\n`questdb_unhandled_errors_total` metric.\nWhen metrics subsystem is disabled, the health check endpoint always returns\nHTTP 200 status code.\nAvoiding CPU starvation\nOn systems with\n8 Cores and less,\ncontention for threads might increase the latency of health check service\nresponses. If you are in a situation where a load balancer thinks QuestDB\nservice is dead with nothing apparent in QuestDB logs, you may need to configure\na dedicated thread pool for the health check service. For more reference, see\nthe",
    "tag": "questdb"
  },
  {
    "title": "Background",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/operations/data-retention.md",
    "content": "\ntitle: Data retention\ndescription:\n  How to employ a data retention strategy to delete old data and save disk space\n\nBackground\nThe nature of time-series data is that the relevance of information diminishes\nover time. If stale data is no longer required, users can delete old data from\nQuestDB to either save disk space or adhere to a data retention policy. This is\nachieved in QuestDB by removing data partitions from a table.\nThis page provides a high-level overview of partitioning with examples to drop\ndata by date. For more details on partitioning, see the\npartitioning page.\nStrategy for data retention\nA simple approach to removing stale data is to drop data that has been\npartitioned by time. A table must have a\ndesignated timestamp assigned and a\npartitioning strategy specified during a `CREATE TABLE` operation to achieve\nthis.\n:::note\nUsers cannot alter the partitioning strategy after a table is created.\n:::\nTables can be partitioned by one of the following:\n\n`YEAR`\n`MONTH`\n`DAY`\n`HOUR`\n\n`questdb-sql title=\"Creating a table and partitioning by DAY\"\nCREATE TABLE my_table(ts TIMESTAMP, symb SYMBOL, price DOUBLE) timestamp(ts)\nPARTITION BY DAY;`\nDropping partitions\n:::caution\nUse `DROP PARTITION` with care, as QuestDB cannot recover data from dropped\npartitions.\n:::\nTo drop partitions, users can use the\nALTER TABLE DROP PARTITION\nsyntax. Partitions may be dropped by:\n\n`DROP PARTITION LIST` - specifying a comma-separated list of partitions to\n  drop\n\n```questdb-sql\n  --Delete a partition\n  ALTER TABLE my_table DROP PARTITION LIST '2021-01-01';\n--Delete a list of two partitions\n  ALTER TABLE my_table DROP PARTITION LIST '2021-01-01', '2021-01-02';\n  ```\n\n`WHERE timestamp =` - exact date matching by timestamp\n\n`questdb-sql\n  ALTER TABLE my_table DROP PARTITION\n  WHERE timestamp = to_timestamp('2021-01-01', 'yyyy-MM-dd');`\n\n`WHERE timestamp <` - using comparison operators (`<` / `>`) to delete by time\n  range relative to a timestamp. Note that the `now()` function may be used to\n  automate dropping of partitions relative to the current time, i.e.:\n\n`questdb-sql\n  --Drop partitions older than 30 days\n  WHERE timestamp < dateadd('d', -30, now())`\nUsage notes:\n\nThe most chronologically recent partition cannot be deleted\nArbitrary partitions may be dropped, which means they may not be the oldest\n  chronologically. Depending on the types of queries users are performing on a\n  dataset, it may not be desirable to have gaps caused by dropped partitions.\n\nExample\nThe following example demonstrates how to create a table with partitioning and\nto drop partitions based on time. This example produces 5 days' worth of data\nwith one incrementing `LONG` value inserted per hour.\n```questdb-sql title=\"Create a partitioned table and generate data\"\nCREATE TABLE my_table (timestamp TIMESTAMP, x LONG) timestamp(timestamp)\nPARTITION BY DAY;\nINSERT INTO my_table\nSELECT timestamp_sequence(\n    to_timestamp('2021-01-01T00:00:00', 'yyyy-MM-ddTHH:mm:ss'),100000L * 36000), x\nFROM long_sequence(120);\n```\nFor reference, the following functions are used to generate the example data:\n\ntimestamp sequence\n  with 1 hour stepping\nrow generator with\n  `long_sequence()` function which creates a `x:long` column\n\nThe result of partitioning is visible when listing as directories on disk:\n`bash title=\"path/to/<QuestDB-root>/db\"\nmy_table\n\u251c\u2500\u2500 2021-01-01\n\u251c\u2500\u2500 2021-01-02\n\u251c\u2500\u2500 2021-01-03\n\u251c\u2500\u2500 2021-01-04\n\u2514\u2500\u2500 2021-01-05`\nPartitions can be dropped using the following query:\n```\n--Delete days before 2021-01-03\nALTER TABLE my_table DROP PARTITION\nWHERE timestamp < to_timestamp('2021-01-03', 'yyyy-MM-dd');",
    "tag": "questdb"
  },
  {
    "title": "Optimizing queries",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/operations/design-for-performance.md",
    "content": "\ntitle: Design for performance\ndescription: How to plan and configure database to optimize performance.\n\nTo optimize the performance of a QuestDB instance, it is important to adjust\nsystem and table configuration according to the nature of the data. This page\nlists out common configurations that users should take into account when testing\ndata using QuestDB.\nTo monitor various metrics of the QuestDB instances, refer to the\nPrometheus monitoring page or the\nHealth monitoring page.\nRefer to Capacity planning for deployment\nconsiderations.\nOptimizing queries\nThe following section describes the underlying aspects to consider when\nformulating queries.\nRow serialization\nRow serialization and deserialization has a cost on both client and server. The\nQuestDB Web Console limits fetching to 10,000 dataset. When fetching a large\n(10K+) dataset via a single query using other methods, consider using\npagination, hence multiple queries instead.\nChoosing a schema\nThis section provides some hints for choosing the right schema for a dataset\nbased on the storage space that types occupy in QuestDB.\nPartitioning\nWhen creating tables, a partitioning strategy is recommended in order to be able\nto enforce a data retention policy to save disk space, and for optimizations on\nthe number of concurrent file reads performed by the system. For more\ninformation on this topic, see the following resources:\n\npartitions page which provides a general overview\n  of this concept\ndata retention guide provides further\n  details on partitioning tables with examples on how to drop partitions by time\n  range\n\nRecords per partition\nThe number of records per partition should factor into the partitioning strategy\n(`YEAR`, `MONTH`, `DAY`, `HOUR`). Having too many records per partition or\nhaving too few records per partition and having query operations across too many\npartitions has the result of slower query times. A general guideline is that\nroughly between 1 million and 100 million records is optimal per partition.\nSymbols\nSymbols are a data type that is recommended to be used\nfor strings that are repeated often in a dataset. The benefit of using this data\ntype is lower storage requirements than regular strings and faster performance\non queries as symbols are internally stored as `int` values.\n:::info\nOnly symbols can be indexed in QuestDB. Although\nmultiple indexes can be specified for a table, there would be a performance\nimpact on the rate of ingestion.\n:::\nThe following example shows the creation of a table with a `symbol` type that\nhas multiple options passed for performance optimization.\n`questdb-sql\nCREATE TABLE my_table(\n    symb SYMBOL CAPACITY 1048576 NOCACHE INDEX CAPACITY 512,\n    s STRING,\n    ts TIMESTAMP\n) timestamp(ts) PARTITION BY DAY;`\nThis example adds a `symbol` type with:\n\ncapacity specified to estimate how many unique symbol values to expect\ncaching disabled which allows dealing with larger value counts\nindex for the symbol column with a storage block value\n\nA full description of the options used above for `symbol` types can be found in\nthe CREATE TABLE page.\nSymbol caching\nSymbol cache enables the use of on-heap\ncache for reads and can enhance performance. However, the cache size grows as\nthe number of distinct value increases, and the size of the cached symbol may\nhinder query performance.\nWe recommend that users check the JVM and GC metrics via\nPrometheus monitoring before taking one\nof the following steps:\n\nDisabling the symbol cache. See\n  Usage of symbols for server-wide\n  and table-wide configuration options.\nIncreasing the JVM heap size using the `-Xmx` argument.\n\nSymbol capacity\nSymbol capacity should be the same or\nslightly larger than the count of distinct symbol values.\nUndersized symbol columns slow down query performance. Similarly, there is a\nperformance impact when symbol is not used for its designed way, most commonly\nassigning `symbol` to columns with a unique value per row. It is crucial to\nchoose a suitable data type based on the\nnature of the dataset.\nIndex\nAppropriate us of indexes provides faster read access\nto a table. However, indexes have a noticeable cost in terms of disk space and\ningestion rate - we recommend starting with no indexes and adding them later,\nonly if they appear to improve query performance. Refer to\nIndex trade-offs for more information.\nNumbers\nThe storage space that numbers occupy can be optimized by choosing `byte`,\n`short`, and `int` data types appropriately. When values are not expected to\nexceed the limit for that particular type, savings on disk space can be made.\nSee also Data types for more details.\n| type    | storage per value | numeric range                            |\n| :------ | :---------------- | :--------------------------------------- |\n| byte    | 8 bits            | -128 to 127                              |\n| short   | 16 bits           | -32768 to 32767                          |\n| int     | 32 bits           | -2147483648 to 2147483647                |\n| float   | 32 bits           | Single precision IEEE 754 floating point |",
    "tag": "questdb"
  },
  {
    "title": "bind to all IP addresses on port 9009",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/operations/capacity-planning.md",
    "content": "\ntitle: Capacity planning\ndescription:\n  How to plan and configure system resources, database configuration, and client\n  application code available to QuestDB to ensure that server operation\n  continues uninterrupted.\n\nCapacity planning should be considered as part of the requirements of deploying\nQuestDB to forecast CPU, memory, network capacity, and a combination of these\nelements, depending on the expected demands of the system. This page describes\nconfiguring these system resources with example scenarios that align with both\nedge cases and common setup configurations.\nMost of the configuration settings referred to below except for OS settings are\nconfigured in QuestDB by either a `server.conf` configuration file or as\nenvironment variables. For more details on applying configuration settings in\nQuestDB, refer to the configuration page.\nTo monitor various metrics of the QuestDB instances, refer to the\nPrometheus monitoring page or the\nHealth monitoring page.\nStorage and filesystem\nThe following sections describe aspects to consider regarding the storage of\ndata and file systems.\nSupported filesystem\nQuestDB officially supports the following filesystems:\n\nEXT4\nAPFS\nNTFS\nOVERLAYFS (used by Docker)\n\nOther file systems supporting\nmmap feature may work with\nQuestDB but they should not be used in production, as QuestDB does not run tests\non them.\nWhen an unsupported file system is used, QuestDB logs show the following\nwarning:\n`-> UNSUPPORTED (SYSTEM COULD BE UNSTABLE)\"`\n:::caution\nUsers can't use NFS or similar distributed filesystems directly with a\nQuestDB database.\n:::\nWrite amplification\nWhen ingesting out-of-order data, high disk write rate combined with high write\namplification may slow down the performance.\nFor data ingestion over PGWire, or as a further step for ILP ingestion, smaller\ntable partitions maybe reduce the write\namplification. This applies to tables with partition directories exceeding a few\nhundred MBs on disk. For example, partition by day can be reduced to by hour,\npartition by month to by day, and so on.\n:::note\n\nIn QuestDB the write amplification is calculated by the\n  metrics:\n  `questdb_physically_written_rows_total` / `questdb_committed_rows_total`.\nPartitions are defined when a table is created. Refer to\n  CREATE TABLE for more information.\n\n:::\nCPU and RAM configuration\nThis section describes configuration strategies based on the forecast behavior\nof the database.\nRAM size\nWe recommend having at least 8GB of RAM for basic workloads and 32GB for more\nadvanced ones.\nFor relatively small datasets, typically a few to a few dozen GB, if the need\nfor reads is high, performance can benefit from maximizing the use of the OS\npage cache. Users may consider increasing available RAM to improve the speed of\nread operations.\nMemory page size configuration\nFor frequent out-of-order (O3) writes over high number of columns/tables, the\nperformance may be impacted by the size of the memory page being too big as this\nincreases the demand for RAM. The memory page, `cairo.o3.column.memory.size`, is\nset to 8M by default. This means that the table writer uses 16MB (2x8MB) RAM per\neach column when it receives O3 writes. Decreasing the value in the interval of\n[128K, 8M] based on the number of columns used may improve O3 write performance.\nCPU cores\nBy default, QuestDB attempts to use all available CPU cores.\nThe guide on shared worker configuration details how to\nchange the default setting. Assuming that the disk does not have bottleneck for\noperations, the throughput of read-only queries scales proportionally with the\nnumber of available cores. As a result, a machine with more cores will provide\nbetter query performance.\nShared workers\nIn QuestDB, there are worker pools which can help separate CPU-load between\nsub-systems.\n:::caution\nIn case if you are configuring thread pool sizes manually, the total number of\nthreads to be used by QuestDB should not exceed the number of available CPU\ncores.\n:::\nThe number of worker threads shared across the application can be configured as\nwell as affinity to pin processes to specific CPUs by ID. Shared worker threads\nservice SQL execution subsystems and, in the default configuration, every other\nsubsystem. More information on these settings can be found on the\nshared worker configuration page.\nQuestDB will allocate CPU resources differently depending on how many CPU cores\nare available. This default can be overridden via configuration. We recommend at\nleast 4 cores for basic workloads and 16 for advanced ones.\n8 CPU cores or less\nQuestDB will configure a shared worker pool to handle everything except the\nInfluxDB line protocol (ILP) writer which gets a dedicated CPU core. The worker\ncount is calculated as follows:\n$(cpuAvailable) - (line.tcp.writer.worker.count)$\nMinimal size of the shared worker pool is 2, even on a single-core machine.\n16 CPU cores or less\nILP I/O Worker pool is configured to use 2 CPU cores to speed up ingestion and\nthe ILP Writer is using 1 core. The shared worker pool is handling everything\nelse and is configured using this formula:\n$(cpuAvailable) - 1 - (line.tcp.writer.worker.count) - (line.tcp.io.worker.count)$\nFor example, with 16 cores, the shared pool will have 12 threads:\n$16-1-2-1$\n17 CPU cores and more\nThe ILP I/O Worker pool is configured to use 6 CPU cores to speed up ingestion\nand the ILP Writer is using 1 core. The shared worker pool is handling\neverything else and is configured using this formula:\n$(cpuAvailable) - 2 - (line.tcp.writer.worker.count) - (line.tcp.io.worker.count)$\nFor example, with 32 cores, the shared pool will have 23 threads:\n$32-2-6-1$\nWriter page size\nThe default page size for writers is 16MB. In cases where there are a large\nnumber of small tables, using 16MB to write a maximum of 1MB of data, for\nexample, is a waste of OS resources. To changes the default value, set the\n`cairo.writer.data.append.page.size` value in `server.conf`:\n`ini title=\"server.conf\"\ncairo.writer.data.append.page.size=1M`\nInfluxDB over TCP\nWe have\na documentation page\ndedicated to capacity planning for ILP ingestion.\nInfluxDB over UDP\n:::note\nThe UDP receiver is deprecated since QuestDB version 6.5.2. We recommend the\nTCP receiver instead.\n:::\nGiven a single client sending data to QuestDB via InfluxDB line protocol over\nUDP, the following configuration can be applied which dedicates a thread for a\nUDP writer and specifies a CPU core by ID:\n`ini title=\"server.conf\"\nline.udp.own.thread=true\nline.udp.own.thread.affinity=1`\nPostgres\nGiven clients sending data to QuestDB via Postgres interface, the following\nconfiguration can be applied which sets a dedicated worker and pins it with\n`affinity` to a CPU by core ID:\n`ini title=\"server.conf\"\npg.worker.count=4\npg.worker.affinity=1,2,3,4`\nNetwork Configuration\nFor InfluxDB line, PGWire and HTTP protocols, there are a set of configuration\nsettings relating to the number of clients that may connect, the internal I/O\ncapacity and connection timeout settings. These settings are configured in the\n`server.conf` file in the format:\n`ini\n<protocol>.net.connection.<config>`\nWhere `<protocol>` is one of:\n\n`http` - HTTP connections\n`pg` - PGWire protocol\n`line.tcp` - InfluxDB line protocol over TCP\n\nAnd `<config>` is one of the following settings:\n| key       | description                                                                                                                                                                                                                |\n| :-------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `limit`   | The number of simultaneous connections to the server. This value is intended to control server memory consumption.                                                                                                         |\n| `timeout` | Connection idle timeout in milliseconds. Connections are closed by the server when this timeout lapses.                                                                                                                    |\n| `hint`    | Applicable only for Windows, where TCP backlog limit is hit. For example Windows 10 allows max of 200 connection. Even if limit is set higher, without hint=true it won't be possible to connect more than 200 connection. |\n| `sndbuf`  | Maximum send buffer size on each TCP socket. If value is -1 socket send buffer remains unchanged from OS default.                                                                                                          |\n| `rcvbuf`  | Maximum receive buffer size on each TCP socket. If value is -1, the socket receive buffer remains unchanged from OS default.                                                                                               |\nFor example, this is configuration for Linux with relatively low number of\nconcurrent connections:\n```ini title=\"server.conf InfluxDB line protocol network example configuration for moderate number of concurrent connections\"\nbind to all IP addresses on port 9009\nline.tcp.net.bind.to=0.0.0.0:9009\nmaximum of 30 concurrent connection allowed\nline.tcp.net.connection.limit=30\nnothing to do here, connection limit is quite low\nline.tcp.net.connection.hint=false\nconnections will time out after 60s of no activity\nline.tcp.net.connection.timeout=60000\nreceive buffer is 4Mb to accomodate large messages\nline.tcp.net.rcvbuf=4m\n```\nLet's assume you would like to configure InfluxDB line protocol for large number\nof concurrent connection on Windows:\n```ini title=\"server.conf InfluxDB line protocol network example configuration for large number of concurrent connections on Windows\"\nbind to specific NIC on port 9009, NIC is identified by IP address\nline.tcp.net.bind.to=10.75.26.3:9009\nlarge number of concurrent connections\nline.tcp.net.connection.limit=400\nWindows will not allow 400 client to connect unless we use the \"hint\"\nline.tcp.net.connection.hint=true\nconnections will time out after 30s of no activity\nline.tcp.net.connection.timeout=30000\nreceive buffer is 1Mb because messages are small, smaller buffer will\nreduce memory usage, 400 connection times 1MB = 400MB RAM is required to handle input\nline.tcp.net.rcvbuf=1m\n```\nFor reference on the defaults of the `http` and `pg` protocols, refer to the\nserver configuration page.\nPooled connection\nConnection pooling should be used for any production-ready use of PGWire or ILP\nover TCP.\nThe maximum number of pooled connections is configurable,\n(`pg.connection.pool.capacity` for PGWire and\n(`line.tcp.connection.pool.capacity` for ILP over TCP. The default number of\nconnections for both interfaces is 64. Users should avoid using too many\nconnections.\nOS configuration\nThis section describes approaches for changing system settings on the host\nQuestDB is running on when system limits are reached due to maximum open files\nor virtual memory areas. QuestDB passes operating system errors to its logs\nunchanged and as such, changing the following system settings should only be\ndone in response to such OS errors.\nMaximum open files\nThe storage model of QuestDB has the benefit that most data structures relate\nclosely to the file system, with columnar data being stored in its own `.d` file\nper partition. In edge cases with extremely large tables, frequent out-of-order\ningestion, or high number of table partitions, the number of open files may hit\na user or system-wide maximum limit and can cause unpredictable behavior.\nThe following commands allow for checking current user and system limits for\nmaximum number of open files:\n```bash title=\"checking ulimit\"\nSoft limit\nulimit -Sn\nHard limit\nulimit -Hn\n```\nSetting system-wide open file limit:\nTo increase this setting and have the configuration persistent, the limit on the\nnumber of concurrently open files can be changed in `/etc/sysctl.conf`:\n`ini title=\"/etc/sysctl.conf\"\nfs.file-max=262144`\nTo confirm that this value has been correctly configured, reload `sysctl` and\ncheck the current value:\n```bash\nreload configuration\nsysctl -p\nquery current settings\nsysctl fs.file-max\n```\nMax virtual memory areas limit\nIf the host machine has insufficient limits of map areas, this may result in out\nof memory exceptions. To increase this value and have the configuration\npersistent, mapped memory area limits can be changed in `/etc/sysctl.conf`:\n`ini title=\"/etc/sysctl.conf\"\nvm.max_map_count=262144`\nEach mapped area needs kernel memory, and it's recommended to have around 128\nbytes available per 1 map count.\n```bash\nreload configuration\nsysctl -p\nquery current settings\ncat /proc/sys/vm/max_map_count",
    "tag": "questdb"
  },
  {
    "title": "this will add crontab record that will run trigger at backup every-day at 01:00 AM",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/operations/backup.md",
    "content": "\ntitle: Backup and restore\nsidebar_label: Backup and restore\ndescription:\n  Details and resources which describe how to perform database backup and\n  restore operations for a QuestDB instance using point-in-time backups and\n  filesystem images.\n\nThis document provides practical details of using the point-in-time backup\nfunctionality in QuestDB along with filesystem backup as means to prevent data\nloss. Alongside backup details, this document describes how to restore from\nbackups and hints for performing filesystem backups on common cloud providers.\nQuestDB provides two strategies for creating backups:\n\nPoint-in-time (PIT) backup\nFilesystem backup\n\nLimitations\nQuestDB officially supports the following filesystems:\n\nEXT4\nAPFS\nNTFS\nOVERLAYFS (used by Docker)\n\nOther file systems supporting\nmmap feature may work with\nQuestDB but they should not be used in production, as QuestDB does not run tests\non them.\n:::caution\n\n\nA backup includes the contents of the database up to the point of executing a\nbackup. Any data inserted while a backup is underway is not stored as part of\nthe backup.\n\n\nUsers can't use NFS or a similar distributed filesystem directly with QuestDB,\n  but users may copy a backup to such a filesystem after a backup has been made.\n\n\n:::\nCreating a point-in-time backup\nWhen creating a point-in-time (PIT) backup in QuestDB, you can specify that the\nwhole database or specific tables should be backed up. This process will create\na backup in a directory specified by the user in the `cairo.sql.backup.root`\nconfiguration key. For more details on passing\nconfiguration in this manner, see the\nserver configuration\ndocumentation.\n`ini title=\"/path/to/server.conf\"\ncairo.sql.backup.root=/path/to/backup/dir`\nA backup can then be triggered via SQL command and\nthe backup is complete as soon as the SQL query has finished executing:\n`questdb-sql\n-- backup whole database\nBACKUP database;\n-- backup a specific table\nBACKUP table my_table;`\nNote that calling `BACKUP TABLE <table_name>` will only copy table data and\nmetadata to the destination folder. This form of backup will not copy entire\ndatabase configuration files required to perform a complete database restore.\nAlternatively, the REST API\ncan be used to execute the SQL for a database backup:\n`bash title=\"Backing up a database via curl\"\ncurl -G --data-urlencode \"query=BACKUP database;\" \\\n  http://localhost:9000/exec`\nCreating a filesystem backup (disk snapshot)\n:::caution\nTo run a reliable filesystem backup without database downtime, you should use\n`SNAPSHOT PREPARE`/`SNAPSHOT COMPLETE`\nSQL statements.\n:::\nThe most common ways to perform cloud-native filesystem snapshots are described\nin the following resources, which rely on similar steps but have minor\ndifferences in terminology and services:\n\nAWS -\n  creating EBS snapshots\nAzure -\n  creating snapshots of a virtual hard disk\nGCP - working\n  with persistent disk snapshots\n\nRestoring from a backup\nIn order to restore a backup, the QuestDB executable must be provided with the\ndirectory location of an existing backup as the root directory. This can\ndone via the `-d` flag as `-d /path/to/backup` when starting up QuestDB.\n`bash\njava -p /path/to/questdb-<version>.jar \\\n     -m io.questdb/io.questdb.ServerMain \\\n     -d /path/to/backup_directory`\nUsers who are starting QuestDB via `systemd` or the official AWS AMI may refer\nto the\nsystemd file\nfor reference. To verify that database information has been successfully\nimported, check logs via `journalctl -u questdb` which will contain a list\nexisting tables.\nDocker instances may have a backup directory mounted to the root directory as\nfollows:\n`bash\ndocker run \\\n -p 9000:9000  -p 9009:9009 \\\n -p 8812:8812 -p 9003:9003 \\\n -v \"/path/to/backup_directory:/root/.questdb/\" questdb/questdb`\n:::info\nThe database backup must contain database metadata files and directories (`db`,\n`config` etc.). The contents of these directories is described in more detail in\nthe root directory documentation.\n:::\nExamples\nThe following example sets up a cronjob which triggers a daily backup via REST\nAPI:\n```bash\nthis will add crontab record that will run trigger at backup every-day at 01:00 AM\ncopy paste this into server terminal\ncrontab -l | { cat; echo \"0 1 * * * /usr/bin/curl --silent -G --data-urlencode 'query=BACKUP database;' http://localhost:9000/exec &>/dev/null\"; } | crontab -\n```\nThis example shows how to compress a backup using the `tar` utility. An archive\nfile `questdb_backup.tar.gz` will be created in the directory that the command\nis run:\n`bash\ntar -zcvf questdb_backup.tar.gz /path/to/backup`\nThe backup file can be expanded using the same utility:\n```bash\ntar -xf questdb_backup.tar.gz",
    "tag": "questdb"
  },
  {
    "title": "Storage model",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/operations/updating-data.md",
    "content": "\ntitle: Updating data\nsidebar_label: Updating data\ndescription:\n  How the UPDATE statement is implemented in QuestDB.\n\nThis document describes how the UPDATE statement works in QuestDB and what happens\nunder the hood when an update is executed.\nStorage model\nTo be able to understand how table rows are updated in QuestDB, first we\nneed to have an idea of how the data is stored. The documentation contains\ndetailed descriptions of the storage model and\nthe directory layout\nbut if we quickly want to summarize it:\n- Each table has its own folder in the db root, the directory is named after the table\n- Partitions are manifested as subdirectories under the folder which represents the table\n- The actual data is stored in column files inside these subdirectories\n- Column files store data ordered by the designated timestamp and they are\nappend-only. This goes naturally with time series data, just think about market\ndata where the price of different financial instruments are tracked during the\ntrading day, for example\nColumn versions\nSince the data is stored in order and the files are append-only updating it is not\nstraightforward. We took the optimistic approach and assumed that past data\nwill never have to change. This is great for read performance.\nHowever, sometimes you may need to amend data which has been recorded incorrectly\nbecause of a bug or for any other reason.\nWe could break our append-only model and start accessing different parts of the\ncolumn files to fix incorrect data. The problem we would face with is inconsistent\nreads. Readers running queries on the table would not be happy as they could see\nsome part of the data updated but not others.\nThe solution is to make the update transactional and copy-on-write. Basically\na new column file is created when processing the UPDATE statement. All readers are\nlooking at a previous consistent view of the data from an older column file while the\nUPDATE is in progress. Readers can find the latest committed version of column files\nbased on a record stored in a metadata file. When the update is completed and a new\ncolumn version is available for the readers, this metadata record gets updated as part\nof the commit. After metadata has changed newly submitted SELECT queries will see the\nupdated data.\nThe copy-on-write approach gives us data consistency and good performance at a price,\ndisk usage will increase. When sizing disk space we should account for extra storage\nto make sure UPDATE statements have enough headroom. Only those column files will get\na new version where data is actually changing. For example, if only a single column\nis updated in a single partition of a table, then only a single column file will be\nrewritten.\nPlease, also check the following guide on modifying data\nin QuestDB for additional information.\nVacuum updated columns\nWhen a column is updated, the new version of the column is written to disk and a background \ntask starts to vacuum redundant column files. The term Vacuum originates from Postgres, it means\nthe collection of garbage and release of disk space. The Vacuum task checks periodically if\nolder column versions are still used by readers and deletes unused files.\nVacuum runs automatically and there is also a VACUUM TABLE\nSQL command to trigger it. \nLimitations\nCurrent implementation of the UPDATE operation rewrites the column files by copying\nrecords in their existing order from the previous version, and replacing the value if\nit needs changing. As a result the designated timestamp cannot be updated.\nModifying the designated timestamp would lead to rewriting history of the time series.\nRecords would need to be reordered, this could even mean moving rows in between\ndifferent partitions. We may remove this limitation in the future if there is enough",
    "tag": "questdb"
  },
  {
    "title": "TYPE questdb_json_queries_total counter",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/third-party-tools/prometheus.md",
    "content": "\ntitle: Prometheus monitoring and alerting\nsidebar_label: Prometheus\ndescription:\n  This document describes how to monitor QuestDB metrics such as memory\n  consumption using the Prometheus metrics endpoint, and how to log alerts to\n  Prometheus Alertmanager.\n\nimport InterpolateReleaseData from \"../../src/components/InterpolateReleaseData\"\nimport CodeBlock from \"@theme/CodeBlock\"\nPrometheus is an open-source systems monitoring and alerting toolkit. Prometheus\ncollects and stores metrics as time series data, i.e. metrics information is\nstored with the timestamp at which it was recorded, alongside optional key-value\npairs called labels.\nUsers can measure the internal status of a QuestDB instance via an HTTP endpoint\nexposed by QuestDB at port `9003`. This document describes how to enable metrics\nvia this endpoint, how to configure Prometheus to scrape metrics from a QuestDB\ninstance, and how to enable alerting from QuestDB to Prometheus Alertmanager.\nPrerequisites\n\n\nQuestDB must be running and accessible. You can do so from\n  Docker, the\n  binaries, or\n  Homebrew for macOS users.\n\n\nPrometheus can be installed using\n  homebrew,\n  Docker, or directly as a binary. For more\n  details, refer to the official Prometheus\n  installation instructions.\n\n\nAlertmanager can be run using\n  Docker or\n  Quay, or can be built\n  from source by following the\n  build instructions on GitHub.\n\n\nScraping Prometheus metrics from QuestDB\nQuestDB has a `/metrics` HTTP endpoint on port `9003` to expose Prometheus\nmetrics. Before being able to query metrics, they must be enabled via the\n`metrics.enabled` key in server configuration:\n`ini title=\"/path/to/server.conf\"\nmetrics.enabled=true`\nWhen running QuestDB via Docker, port `9003` must be exposed and the metrics\nconfiguration can be enabled via the `QDB_METRICS_ENABLED` environment variable:\n (\n    \n      {`docker run \\\\\n  -e QDB_METRICS_ENABLED=TRUE \\\\\n  -p 8812:8812 -p 9000:9000 -p 9003:9003 -p 9009:9009 \\\\\n  -v \"$(pwd):/var/lib/questdb\" \\\\\n  questdb/questdb:${release.name}`}\n    \n  )}\n/>\nTo verify that metrics are being exposed correctly by QuestDB, navigate to\n`http://<questdb_ip>:9003/metrics` in a browser, where `<questdb_ip>` is the IP\naddress of an instance, or execute a basic curl like the following example:\n```bash title=\"Given QuestDB running at 127.0.0.1\"\ncurl http://127.0.0.1:9003/metrics\nTYPE questdb_json_queries_total counter\nquestdb_json_queries_total 0\nTYPE questdb_memory_tag_MMAP_DEFAULT gauge\nquestdb_memory_tag_MMAP_DEFAULT 77872\nTYPE questdb_memory_malloc_count gauge\nquestdb_memory_malloc_count 659\n...\n```\nTo configure Prometheus to scrape these metrics, provide the QuestDB instance IP\nand port `9003` as a target. The following example configuration file\n`questdb.yml` assumes there is a running QuestDB instance on localhost\n(127.0.0.1) with port `9003` available:\n```shell title=\"questdb.yml\"\nglobal:\n  scrape_interval: 5s\n  external_labels:\n    monitor: 'questdb'\nscrape_configs:\n  - job_name: 'questdb'\n    scrape_interval: 5s\n    static_configs:\n      - targets: ['127.0.0.1:9003']\n```\nStart Prometheus and pass this configuration on launch:\n`bash\nprometheus --config.file=questdb.yml`\nPrometheus should be available on `0.0.0.0:9090` and navigating to\n`http://0.0.0.0:9090/targets` should show that QuestDB is being scraped\nsuccessfully:\nimport Screenshot from \"@theme/Screenshot\"\n\nIn the graphing tab of Prometheus (`http://0.0.0.0:9090/graph`), autocomplete\ncan be used to graph QuestDB-specific metrics which are all prefixed with\n`questdb_`:\n\nThe following metrics are available:\n| Metric                                   | Type    | Description                                                                                                                                                                                                                                                   |\n| :--------------------------------------- | :------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| `questdb_commits_total`                  | counter | Number of total commits of all types (in-order and out-of-order) executed on the database tables.                                                                                                                                                             |\n| `questdb_o3_commits_total`               | counter | Number of total out-of-order (O3) commits executed on the database tables.                                                                                                                                                                                    |\n| `questdb_committed_rows_total`           | counter | Number of total rows committed to the database tables.                                                                                                                                                                                                        |\n| `questdb_physically_written_rows_total`  | counter | Number of total rows physically written to disk. Greater than `committed_rows` when out-of-order ingestion is enabled. Write amplification is `questdb_physically_written_rows_total / questdb_committed_rows_total`. |\n| `questdb_rollbacks_total`                | counter | Number of total rollbacks executed on the database tables.                                                                                                                                                                                                    |\n| `questdb_json_queries_total`             | counter | Number of total REST API queries, including retries.                                                                                                                                                                                                              |\n| `questdb_json_queries_completed`         | counter | Number of successfully executed REST API queries.                                                                                                                                                                                                                 |\n| `questdb_unhandled_errors_total`         | counter | Number of total unhandled errors occurred in the database. Such errors usually mean a critical service degradation in one of the database subsystems.                                                                                                         |\n| `questdb_jvm_major_gc_count`             | counter | Number of times major JVM garbage collection was triggered.                                                                                                                                                                                                                 |\n| `questdb_jvm_major_gc_time`              | counter | Total time spent on major JVM garbage collection in milliseconds.                                                                                                                                                                                             |\n| `questdb_jvm_minor_gc_count`             | counter | Number of times minor JVM garbage collection pause was triggered.                                                                                                                                                                                                          |\n| `questdb_jvm_minor_gc_time`              | counter | Total time spent on minor JVM garbage collection pauses in milliseconds.                                                                                                                                                                                      |\n| `questdb_jvm_unknown_gc_count`           | counter | Number of times JVM garbage collection of unknown type was triggered. Non-zero values of this metric may be observed only on some, non-mainstream JVM implementations.                                                                                                                                                                                                               |\n| `questdb_jvm_unknown_gc_time`            | counter | Total time spent on JVM garbage collection of unknown type in milliseconds. Non-zero values of this metric may be observed only on some, non-mainstream JVM implementations.                                                                                                                                                                                           |\n| `questdb_memory_tag_MMAP_DEFAULT`        | gauge   | Amount of memory allocated for mmaped files.                                                                                                                                                                                                                  |\n| `questdb_memory_tag_NATIVE_DEFAULT`      | gauge   | Amount of allocated untagged native memory.                                                                                                                                                                                                                   |\n| `questdb_memory_tag_MMAP_O3`             | gauge   | Amount of memory allocated for O3 mmapped files.                                                                                                                                                                                                              |\n| `questdb_memory_tag_NATIVE_O3`           | gauge   | Amount of memory allocated for O3.                                                                                                                                                                                                                            |\n| `questdb_memory_tag_NATIVE_RECORD_CHAIN` | gauge   | Amount of memory allocated for SQL record chains.                                                                                                                                                                                                             |\n| `questdb_memory_tag_MMAP_TABLE_WRITER`   | gauge   | Amount of memory allocated for table writer mmapped files.                                                                                                                                                                                                    |\n| `questdb_memory_tag_NATIVE_TREE_CHAIN`   | gauge   | Amount of memory allocated for SQL tree chains.                                                                                                                                                                                                               |\n| `questdb_memory_tag_MMAP_TABLE_READER`   | gauge   | Amount of memory allocated for table reader mmapped files.                                                                                                                                                                                                    |\n| `questdb_memory_tag_NATIVE_COMPACT_MAP`  | gauge   | Amount of memory allocated for SQL compact maps.                                                                                                                                                                                                              |\n| `questdb_memory_tag_NATIVE_FAST_MAP`     | gauge   | Amount of memory allocated for SQL fast maps.                                                                                                                                                                                                                 |\n| `questdb_memory_tag_NATIVE_LONG_LIST`    | gauge   | Amount of memory allocated for long lists.                                                                                                                                                                                                                    |\n| `questdb_memory_tag_NATIVE_HTTP_CONN`    | gauge   | Amount of memory allocated for HTTP connections.                                                                                                                                                                                                              |\n| `questdb_memory_tag_NATIVE_PGW_CONN`     | gauge   | Amount of memory allocated for PGWire connections.                                                                                                                                                                                                           |\n| `questdb_memory_tag_MMAP_INDEX_READER`   | gauge   | Amount of memory allocated for index reader mmapped files.                                                                                                                                                                                                    |\n| `questdb_memory_tag_MMAP_INDEX_WRITER`   | gauge   | Amount of memory allocated for index writer mmapped files.                                                                                                                                                                                                    |\n| `questdb_memory_tag_MMAP_INDEX_SLIDER`   | gauge   | Amount of memory allocated for indexed column view mmapped files.                                                                                                                                                                                             |\n| `questdb_memory_tag_NATIVE_REPL`         | gauge   | Amount of memory mapped for replication tasks.                                                                                                                                                                                                                |\n| `questdb_memory_free_count`              | gauge   | Number of times native memory was freed.                                                                                                                                                                                                                      |\n| `questdb_memory_mem_used`                | gauge   | Current amount of allocated native memory.                                                                                                                                                                                                                    |\n| `questdb_memory_malloc_count`            | gauge   | Number of times native memory was allocated.                                                                                                                                                                                                                  |\n| `questdb_memory_realloc_count`           | gauge   | Number of times native memory was reallocated.                                                                                                                                                                                                                |\n| `questdb_memory_rss`                     | gauge   | Resident Set Size (Linux/Unix) / Working Set Size (Windows).                                                                                                                                                                                                  |\n| `questdb_memory_jvm_free`                | gauge   | Current amount of free Java memory heap in bytes.                                                                                                                                                                                                             |\n| `questdb_memory_jvm_total`               | gauge   | Current size of Java memory heap in bytes.                                                                                                                                                                                                                    |\n| `questdb_memory_jvm_max`                 | gauge   | Maximum amount of Java heap memory that can be allocated in bytes.                                                                                                                                                                                            |\n| `questdb_json_queries_cached`            | gauge   | Number of current cached REST API queries.                                                                                                                                                                                                                 |\n| `questdb_pg_wire_select_queries_cached`  | gauge   | Number of current cached PGWire `SELECT` queries.                                                                                                                                                                                                        |\n| `questdb_pg_wire_update_queries_cached`  | gauge   | Number of current cached PGWire `UPDATE` queries.                                                                                                                                                                                                        |\nAll of the above metrics are volatile, i.e. they're collected since the current\ndatabase start.\nConfiguring Prometheus Alertmanager\nQuestDB includes a log writer that sends any message logged at critical level\n(by default) to Prometheus\nAlertmanager over a\nTCP/IP socket connection. To configure this writer, add it to the `writers`\nconfig alongside other log writers. Details on logging configuration can be\nfound on the\nserver configuration documentation.\nAlertmanager may be started via Docker with the following command:\n`docker run -p 127.0.0.1:9093:9093 --name alertmanager quay.io/prometheus/alertmanager`\nTo discover the IP address of this container, run the following command which\nspecifies `alertmanager` as the container name:\n`bash\ndocker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' alertmanager`\nTo run QuestDB and point it towards Alertmanager for alerting, first create a\nfile `./conf/log.conf` with the following contents. `172.17.0.2` in this case is\nthe IP address of the docker container for alertmanager that was discovered by\nrunning the `docker inspect` command above.\n```ini title=\"./conf/log.conf\"\nWhich writers to enable\nwriters=stdout,alert\nstdout\nw.stdout.class=io.questdb.log.LogConsoleWriter\nw.stdout.level=INFO\nPrometheus Alerting\nw.alert.class=io.questdb.log.LogAlertSocketWriter\nw.alert.level=CRITICAL\nw.alert.alertTargets=172.17.0.2:9093\n```\nStart up QuestDB in Docker using the following command:\n`bash \"Mounting a volume with the log.conf file\"\ndocker run \\\n  -p 9000:9000 -p 8812:8812 -p 9009:9009 -p 9003:9003 \\\n  -v \"$(pwd)::/var/lib/questdb\" \\\n  questdb/questdb:6.1.3`\nWhen alerts are successfully triggered, QuestDB logs will indicate the sent and\nreceived status:\n`txt\n2021-12-14T18:42:54.222967Z I i.q.l.LogAlertSocketWriter Sending: 2021-12-14T18:42:54.122874Z I i.q.l.LogAlertSocketWriter Sending: 2021-12-14T18:42:54.073978Z I i.q.l.LogAlertSocketWriter Received [0] 172.17.0.2:9093: {\"status\":\"success\"}\n2021-12-14T18:42:54.223377Z I i.q.l.LogAlertSocketWriter Received [0] 172.17.0.2:9093: {\"status\":\"success\"}`\n:::info\nThe template used by QuestDB for alerts is user-configurable and is described in\nmore detail in the\nserver configuration",
    "tag": "questdb"
  },
  {
    "title": "Prerequisites",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/third-party-tools/redpanda.md",
    "content": "\ntitle: Redpanda\ndescription:\n  Guide for using Redpanda with QuestDB via the QuestDB Kafka connector\n\nRedpanda is an open-source, Kafka-compatible streaming\nplatform that uses C++ and Raft to replace Java and Zookeeper. Since it is Kafka\ncompatible, it can be used with the\nQuestDB Kafka connector,\nproviding an alternative data streaming option.\nPrerequisites\n\nDocker\nA local JDK installation\nA running QuestDB instance\n\nConfigure and start Redpanda\nThe Redpanda\nQuick start guide\nprovides a `docker-compose.yaml` file that can be used. Copy and paste the\ncontent into into a file named `docker-compose.yml` on your local filesystem:\n```yaml title=\"docker-compose.yml\"\nversion: \"3.7\"\nname: redpanda-quickstart\nnetworks:\n  redpanda_network:\n    driver: bridge\nvolumes:\n  redpanda-0: null\nservices:\n  redpanda-0:\n    command:\n      - redpanda\n      - start\n      - --kafka-addr\n      - internal://0.0.0.0:9092,external://0.0.0.0:19092\n      # use the internal addresses to connect to the Redpanda brokers'\n      # from inside the same Docker network.\n      #\n      # use the external addresses to connect to the Redpanda brokers'\n      # from outside the Docker network.\n      #\n      # address the broker advertises to clients that connect to the Kafka API.\n      - --advertise-kafka-addr\n      - internal://redpanda-0:9092,external://localhost:19092\n      - --pandaproxy-addr\n      - internal://0.0.0.0:8082,external://0.0.0.0:18082\n      # address the broker advertises to clients that connect to PandaProxy.\n      - --advertise-pandaproxy-addr\n      - internal://redpanda-0:8082,external://localhost:18082\n      - --schema-registry-addr\n      - internal://0.0.0.0:8081,external://0.0.0.0:18081\n      # Redpanda brokers use the RPC API to communicate with eachother internally.\n      - --rpc-addr\n      - redpanda-0:33145\n      - --advertise-rpc-addr\n      - redpanda-0:33145\n      # tells Seastar (the framework Redpanda uses under the hood) to use 1 core on the system.\n      - --smp 1\n      # the amount of memory to make available to Redpanda.\n      - --memory 1G\n      # the amount of memory that's left for the Seastar subsystem.\n      # For development purposes this is set to 0.\n      - --reserve-memory 0M\n      # Redpanda won't assume it has all of the provisioned CPU\n      # (to accommodate Docker resource limitations).\n      - --overprovisioned\n      # enable logs for debugging.\n      - --default-log-level=debug\n    image: docker.redpanda.com/vectorized/redpanda:v22.3.11\n    container_name: redpanda-0\n    volumes:\n      - redpanda-0:/var/lib/redpanda/data\n    networks:\n      - redpanda_network\n    ports:\n      - 18081:18081\n      - 18082:18082\n      - 19092:19092\n      - 19644:9644\n  console:\n    container_name: redpanda-console\n    image: docker.redpanda.com/vectorized/console:v2.1.1\n    networks:\n      - redpanda_network\n    entrypoint: /bin/sh\n    command: -c 'echo \"$$CONSOLE_CONFIG_FILE\" > /tmp/config.yml; /app/console'\n    environment:\n      CONFIG_FILEPATH: /tmp/config.yml\n      CONSOLE_CONFIG_FILE: |\n        kafka:\n          brokers: [\"redpanda-0:9092\"]\n          schemaRegistry:\n            enabled: true\n            urls: [\"http://redpanda-0:8081\"]\n        redpanda:\n          adminApi:\n            enabled: true\n            urls: [\"http://redpanda-0:9644\"]\n    ports:\n      - 8080:8080\n    depends_on:\n      - redpanda-0\n```\nOnce the file is saved, run the following command to start a single Redpanda\nbroker inside Docker and expose Redpanda to your host machine:\n`shell\ndocker compose up`\nIt also start the\nRedpanda web UI.\nDownload Apache Kafka\nDownload\nApache Kafka\nand unzip the file.\nThis step is required as Redpanda does not have its own Kafka Connect\nequivalent.\nDownload the QuestDB Kafka connector\nDownload\nthe QuestDB Kafka connector,\nunder the zip archive named `kafka-questdb-connector-<version>-bin.zip`.\n:::tip\nYou can automate downloading the latest connector package by running this\ncommand:\n`shell\ncurl -s https://api.github.com/repos/questdb/kafka-questdb-connector/releases/latest |\njq -r '.assets[]|select(.content_type == \"application/zip\")|.browser_download_url'|\nwget -qi -`\n:::\nUnzip the connector - it has a directory with 2 JARs: Copy these JARs into\n/path/to/kafka/lib:\n`shell\nunzip kafka-questdb-connector-*-bin.zip\ncd kafka-questdb-connector\ncp ./*.jar /path/to/kafka/libs`\nThere should be already a lot of other JAR files. That's how you can tell you\nare in the right directory.\nConfigure properties\nGo to /path/to/kafka/config - there should be already quite a few *.property\nfiles. Create a new file: `questdb-connector.properties` with the following\nlines:\n`json title=\"questdb-connector.properties\"\nname=questdb-sink\nconnector.class=io.questdb.kafka.QuestDBSinkConnector\nhost=localhost:9009\ntopics=example-topic\ntable=example_table\ninclude.key=false\nvalue.converter=org.apache.kafka.connect.json.JsonConverter\nvalue.converter.schemas.enable=false\nkey.converter=org.apache.kafka.connect.storage.StringConverter`\nIn addition, pointing the open `connect-standalone.properties` and replace:\n`json\nbootstrap.servers=localhost:9092`\nwith the Redpanda broker URL:\n`json\nbootstrap.servers=127.0.0.1:19092`\nStart Kafka Connect\nNavigate to the Kafka Connect folder and then run:\n`shell\n./bin/connect-standalone.sh config/connect-standalone.properties config/questdb-connector.properties`\nNow the Kafka Connect is initiated.\nSend a message\nOpen the Redpand UI topic page, `http://127.0.0.1:8080/topics`.It should display `example-topic`:\n\nIf the topic is not there then refresh a few times.\nSelect `example-topic` to expand more details and click `Actions` -->\n`Publish Message`:\n\nPaste the following message into the message box:\n`json\n{ \"firstname\": \"Arthur\", \"lastname\": \"Dent\", \"age\": 42 }`\n\nThen, click 'Publish'.\nSee result from QuestDB\nGo to QuestDB web console at `http://localhost:9000/`. Run a `SELECT` query:\n`questdb-sql\nSELECT * FROM example_table;`\nThe message is delivered to QuestDB:\n\nSee also\n\nQuestDB Kafka Connector reference manual\n",
    "tag": "questdb"
  },
  {
    "title": "-- INPUT PLUGINS -- #",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/third-party-tools/telegraf.md",
    "content": "\ntitle: Telegraf\ndescription:\n  Learn how to use Telegraf to collect system metrics and send this data to\n  QuestDB.\n\nTelegraf is a client for\ncollecting metrics from many inputs and has support for sending it on to various\noutputs. It is plugin-driven for the collection and delivery of data, so it is\neasily configurable and customizable. Telegraf is compiled as a standalone\nbinary, which means there are no external dependencies required to manage.\nQuestDB supports ingesting from Telegraf over TCP. This page provides examples\nfor collecting CPU and memory usage metrics using Telegraf and sends these metrics\nto a locally-running QuestDB instance for querying and visualization.\nPrerequisites\n\n\nQuestDB must be running and accessible. You can do so from\n  Docker, the\n  binaries, or\n  Homebrew for macOS users.\n\n\nTelegraf can be installed using\n  homebrew,\n  docker, or directly as a binary. For more\n  details, refer to the official Telegraf\n  installation instructions.\n\n\nConfiguring Telegraf\nAs Telegraf is a plugin-driven agent, the configuration file provided when\nTelegraf is launched will determine which metrics to collect, if and how\nprocessing of the metrics should be performed, and the destination outputs.\nThe default location that Telegraf can pick up configuration files is\n`/usr/local/etc/` on macOS and `/etc/telegraf/` on Linux. After installation,\ndefault configuration files are in the following locations:\n\nHomebrew install: `/usr/local/etc/telegraf.conf`\nLinux, Deb and RPM: `/etc/telegraf/telegraf.conf`\n\nFull configuration files for writing over TCP are provided below and can\nbe placed in these directories and picked up by Telegraf. To view a\ncomprehensive configuration file with example inputs and outputs, the following\ncommand can generate an example:\n`telegraf -sample-config > example.conf`\nExample Inputs\nThe examples on this page will use input plugins that read CPU and memory usage\nstatistics of the host machine and send this to the outputs specified in the\nconfiguration file. The following snippet includes code comments which describe\nthe inputs in more detail:\n```shell title=\"Example inputs sending host data to QuestDB\"\n...\n-- INPUT PLUGINS --\n[[inputs.cpu]]\n  # Read metrics about cpu usage\n  ## Whether to report per-cpu stats or not\n  percpu = true\n  ## Whether to report total system cpu stats or not\n  totalcpu = true\n  ## If true, collect raw CPU time metrics\n  collect_cpu_time = false\n  ## If true, compute and report the sum of all non-idle CPU states\n  report_active = false\nRead metrics about memory usage\n[[inputs.mem]]\n  # no customisation\n```\nWriting to QuestDB over TCP\nQuestDB expects influx line protocol messages over TCP on port `9009`. To change\nthe default port, see the\nInfluxDB line protocol (TCP)\nsection of the server configuration page.\nCreate a new file named `questdb_tcp.conf` in one of the locations Telegraf can\nload configuration files from and paste the following example:\n```shell title=\"/path/to/telegraf/config/questdb_tcp.conf\"\nConfiguration for Telegraf agent\n[agent]\n  ## Default data collection interval for all inputs\n  interval = \"5s\"\n  hostname = \"qdb\"\n-- OUTPUT PLUGINS --\n[[outputs.socket_writer]]\n  # Write metrics to a local QuestDB instance over TCP\n  address = \"tcp://127.0.0.1:9009\"\n-- INPUT PLUGINS --\n[[inputs.cpu]]\n  percpu = true\n  totalcpu = true\n  collect_cpu_time = false\n  report_active = false\n[[inputs.mem]]\n  # no customisation\n```\nRun Telegraf and specify this config file with TCP writer settings:\n`shell\ntelegraf --config questdb_tcp.conf`\nTelegraf should report the following if configured correctly:\n`bash\n2021-01-29T12:11:32Z I! Loaded inputs: cpu mem\n2021-01-29T12:11:32Z I! Loaded aggregators:\n2021-01-29T12:11:32Z I! Loaded processors:\n2021-01-29T12:11:32Z I! Loaded outputs: socket_writer\n...`\nVerifying the integration\n\n\nNavigate to the QuestDB Web Console at\n   `http://127.0.0.1:9000/`. The Schema Navigator in the\n   top left should display two new tables:\n\n\n`cpu` generated from `inputs.cpu`\n\n\n`mem` generated from `inputs.mem`\n\n\nType `cpu` in the query editor and click RUN\n\n\nThe `cpu` table will have a column for each metric collected by the Telegraf\nplugin for monitoring memory:\nimport Screenshot from \"@theme/Screenshot\"\n\nGraphing system CPU\nTo create a graph that visualizes CPU usage over time, run the following example\nquery:\n`SELECT\navg(usage_system) cpu_average,\nmax(usage_system) cpu_max,\ntimestamp\nFROM cpu SAMPLE BY 1m;`\nSelect the Chart tab and set the following values:\n\nChart type line\nLabels timestamp\nSeries cpu_average and cpu_max\n\n<Screenshot\n  alt=\"Graphing CPU metrics using the QuestDB Web Console\"\n  src=\"/img/docs/telegraf/cpu_stats_chart.png\"\n  width={745}\n  height={375}",
    "tag": "questdb"
  },
  {
    "title": "Prerequisites",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/third-party-tools/pandas.md",
    "content": "\ntitle: Pandas\ndescription:\n  Guide for using Pandas with QuestDB via the official QuestDB Python client\n  library\n\nPandas is a fast, powerful, flexible, and\neasy-to-use open-source data analysis and manipulation tool, built on top of the\nPython programming language. The\nQuestDB Python client\nprovides native support for ingesting Pandas dataframes via the InfluxDB Line Protocol (ILP).\nPrerequisites\n\nQuestDB must be running and accessible. You can do so from\n  Docker, the\n  binaries, or\n  Homebrew for macOS users.\nPython 3.8 or later\nPandas \npyarrow\nNumPy\n\nOverview\nThe QuestDB Python client implements the `dataframe()` method to transform Pandas DataFrames into QuestDB-flavored ILP messages.\nThe following example shows how to insert data from a Pandas DataFrame to the `trades` table:\n```python\nfrom questdb.ingress import Sender, IngressError\nimport sys\nimport pandas as pd\ndef example(host: str = 'localhost', port: int = 9009):\n    df = pd.DataFrame({\n            'pair': ['USDGBP', 'EURJPY'],\n            'traded_price': [0.83, 142.62],\n            'qty': [100, 400],\n            'limit_price': [0.84, None],\n            'timestamp': [\n                pd.Timestamp('2022-08-06 07:35:23.189062', tz='UTC'),\n                pd.Timestamp('2022-08-06 07:35:23.189062', tz='UTC')]})\n    try:\n        with Sender(host, port) as sender:\n            sender.dataframe(\n                df,\n                table_name='trades',  # Table name to insert into.\n                symbols=['pair'],  # Columns to be inserted as SYMBOL types.\n                at='timestamp')  # Column containing the designated timestamps.\n\n\n```except IngressError as e:\n    sys.stderr.write(f'Got error: {e}\\n')\n```\n\n\nif name == 'main':\n    example()\n```\nSee also\nFor detailed documentation, please see:\n\nSender.dataframe()\nBuffer.dataframe()\n",
    "tag": "questdb"
  },
  {
    "title": "Prerequisites",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/third-party-tools/mindsdb.md",
    "content": "\ntitle: MindsDB\ndescription:\n  Guide for getting started in Machine Learning with MindsDB and QuestDB\n\nMindsDB provides Machine Learning capabilities to enable\npredictive questions about your data. With MindsDB:\n\nDevelopers can quickly add AI capabilities to their applications.\nData scientists can streamline MLOps by deploying ML models as AI Tables.\nData analysts can easily make forecasts on complex data, such as multivariate\n  time-series with high cardinality, and visualize these in BI tools like\n  Grafana, and Tableau.\n\nCombining both MindsDB and QuestDB provides unbound prediction ability with\nSQL.\nThis guide describes how to pre-process data in QuestDB and then access these\ndata from MindsDB to produce powerful ML models.\nPrerequisites\n\ndocker: To create an image and run the container.\nmysql: The client we use to interact with MindsDB\n  (`mysql -h 127.0.0.1 --port 47335 -u mindsdb -p`). Alternatively, use\n  MindsDB web console at `http://localhost:47334/` instead.\nCurl: To upload data to QuestDB from a local\n  CSV file.\n\nInstructions\nThe following are the overall steps to connect MindsDB and QuestDB:\n\nBuild a Docker image and spawn a container to run MindsDB and QuestDB\n   together.\nAdd QuestDB as a datasource to MindsDB using a SQL Statement.\nCreate a table and add data for a simple ML use case using QuestDB's web\n   console.\nConnect to MindsDB using `mysql` as a client and write some SQL.\n\nWe have put together all the files needed in\nGH.\nRunning the Docker container\nClone the\nrepository for this tutorial. The\nDockerfile\nallows us to build an image with the following command:\n`shell\ndocker build -t questdb/mindsdb:latest .`\nThen, start the service container `qmdb` with the following command:\n`shell\ndocker run --rm \\\n    -p 8812:8812 \\\n    -p 9009:9009 \\\n    -p 9000:9000 \\\n    -p 8888:8888 \\\n    -p 47334:47334 \\\n    -p 47335:47335 \\\n    -d \\\n    --name qmdb \\\n    questdb/mindsdb:latest`\nThe container is run as user `quest`. It takes about 10 seconds to become\nresponsive, logs can be followed in the terminal:\n`shell\ndocker logs -f qmdb\n...\nhttp API: starting...\nmysql API: starting...\nmongodb API: starting...\n...\nmongodb API: started on 47336\nmysql API: started on 47335\nhttp API: started on 47334`\nThe container has these mount points:\n\n/home/quest: User home directory.\n~/questdb/: QuestDB's root directory.\n~/questdb/db/: QuestDB's data root directory.\n~/backups/: Directory for backups.\n~/csv/: Directory for the `COPY` operation.\n~/mindsdb/storage/: MindsDB's data root directory.\n\nThe container is running `Debian GNU/Linux 11 (bullseye)` and exposes these\nports:\n\n9000: QuestDB Web Console\n8812: QuestDB pg-wire\n9009: QuestDB ILP ingress line protocol\n47334: MindsDB WebConsole\n47335: MindsDB mysql API\n47336: MindsDB mongodb API\n\nAdding data to QuestDB\nThere are different ways to\ninsert data to QuestDB.\nSQL\nWe can access QuestDB's web console at `http://localhost:9000`.\nRun the following SQL query to create a simple table:\n`questdb-sql\nCREATE TABLE IF NOT EXISTS house_rentals_data (\n    number_of_rooms INT,\n    number_of_bathrooms INT,\n    sqft INT,\n    location SYMBOL,\n    days_on_market INT,\n    initial_price FLOAT,\n    neighborhood SYMBOL,\n    rental_price FLOAT,\n    ts TIMESTAMP\n) TIMESTAMP(ts) PARTITION BY YEAR;`\nWe could populate table house_rentals_data with random data:\n`questdb-sql\nINSERT INTO house_rentals_data SELECT * FROM (\n    SELECT\n        rnd_int(1,6,0),\n        rnd_int(1,3,0),\n        rnd_int(180,2000,0),\n        rnd_symbol('great', 'good', 'poor'),\n        rnd_int(1,20,0),\n        rnd_float(0) * 1000,\n        rnd_symbol('alcatraz_ave', 'berkeley_hills', 'downtown', 'south_side', 'thowsand_oaks', 'westbrae'),\n        rnd_float(0) * 1000 + 500,\n        timestamp_sequence(\n            to_timestamp('2021-01-01', 'yyyy-MM-dd'),\n            14400000000L\n        )\n    FROM long_sequence(100)\n);`\nCURL command\nThe\ndata CSV file\ncan be downloaded to a local folder and uploaded to QuestDB using the following command:\n`shell\ncurl -F data=@sample_house_rentals_data.csv \"http://localhost:9000/imp?forceHeader=true&name=house_rentals_data\"`\nEither way, this gives us 100 data points, one every 4 hours, from\n`2021-01-16T12:00:00.000000Z` (QuestDB's timestamps are UTC with microsecond\nprecision).\nConnect to MindsDB\nWe can connect to MindsDB with a standard mysql-wire-protocol compliant client\n(no password, hit ENTER):\n`shell\nmysql -h 127.0.0.1 --port 47335 -u mindsdb -p`\nAlternatively, we can use MindsDB web console at `http://localhost:47334`:\nFrom the terminal or the MindsDB web console, run the following command to check\nthe available databases:\n`sql\nSHOW DATABASES;`\nQuestDB is not shown in the result:\n`shell\n+--------------------+\n| Database           |\n+--------------------+\n| mindsdb            |\n| files              |\n| information_schema |\n+--------------------+`\nTo see QuestDB as a database we need to add it to MindsDB:\n`sql\nCREATE DATABASE questdb\n    WITH ENGINE = \"questdb\",\n    PARAMETERS = {\n        \"user\": \"admin\",\n        \"password\": \"quest\",\n        \"host\": \"questdb\",\n        \"port\": \"8812\",\n        \"database\": \"questdb\"\n    };`\nThen, run `SHOW DATABASES;` should display both MindsDB and QuestDB:\n`shell\n+--------------------+\n| Database           |\n+--------------------+\n| mindsdb            |\n| files              |\n| questdb            |\n| information_schema |\n+--------------------+`\n`questdb`\nThis is a read-only view on our QuestDB instance. We can query it leveraging the\nfull power of QuestDB's unique SQL syntax because statements are sent from\nMindsDB to QuestDB without interpreting them. It only works for SELECT\nstatements:\n`sql\nSELECT * FROM questdb(\n  SELECT\n        ts, neighborhood,\n            sum(days_on_market) DaysLive,\n            min(rental_price) MinRent,\n            max(rental_price) MaxRent,\n            avg(rental_price) AvgRent\n    FROM house_rentals_data\n    WHERE ts BETWEEN '2021-01-08' AND '2021-01-10'\n    SAMPLE BY 1d FILL (0, 0, 0, 0)\n);`\nThe result should be something like this:\n`shell\n+--------------+----------------+----------+----------+----------+--------------------+\n| ts           | neighborhood   | DaysLive | MinRent  | MaxRent  | AvgRent            |\n+--------------+----------------+----------+----------+----------+--------------------+\n| 1610064000.0 | south_side     | 19       | 1285.338 | 1285.338 | 1285.338134765625  |\n| 1610064000.0 | downtown       | 7        | 1047.14  | 1047.14  | 1047.1396484375    |\n| 1610064000.0 | berkeley_hills | 17       | 727.52   | 727.52   | 727.5198974609375  |\n| 1610064000.0 | westbrae       | 36       | 1038.358 | 1047.342 | 1042.85009765625   |\n| 1610064000.0 | thowsand_oaks  | 5        | 1067.319 | 1067.319 | 1067.318603515625  |\n| 1610064000.0 | alcatraz_ave   | 0        | 0.0      | 0.0      | 0.0                |\n| 1610150400.0 | south_side     | 10       | 694.403  | 694.403  | 694.4031982421875  |\n| 1610150400.0 | downtown       | 16       | 546.798  | 643.204  | 595.0011291503906  |\n| 1610150400.0 | berkeley_hills | 4        | 1256.49  | 1256.49  | 1256.4903564453125 |\n| 1610150400.0 | westbrae       | 0        | 0.0      | 0.0      | 0.0                |\n| 1610150400.0 | thowsand_oaks  | 0        | 0.0      | 0.0      | 0.0                |\n| 1610150400.0 | alcatraz_ave   | 14       | 653.924  | 1250.477 | 952.2005004882812  |\n| 1610236800.0 | south_side     | 0        | 0.0      | 0.0      | 0.0                |\n| 1610236800.0 | downtown       | 9        | 1357.916 | 1357.916 | 1357.9158935546875 |\n| 1610236800.0 | berkeley_hills | 0        | 0.0      | 0.0      | 0.0                |\n| 1610236800.0 | westbrae       | 0        | 0.0      | 0.0      | 0.0                |\n| 1610236800.0 | thowsand_oaks  | 0        | 0.0      | 0.0      | 0.0                |\n| 1610236800.0 | alcatraz_ave   | 0        | 0.0      | 0.0      | 0.0                |\n+--------------+----------------+----------+----------+----------+--------------------+`\nBeyond SELECT statements, for instance when we need to save the results of a\nquery into a new table, we need to use QuestDB's web console available at\nlocalhost:9000:\n`questdb-sql\nCREATE TABLE sample_query_results AS (\n    SELECT\n        ts,\n        neighborhood,\n        sum(days_on_market) DaysLive,\n        min(rental_price) MinRent,\n        max(rental_price) MaxRent,\n        avg(rental_price) AvgRent\n    FROM house_rentals_data\n    WHERE ts BETWEEN '2021-01-08' AND '2021-01-10'\n    SAMPLE BY 1d FILL (0, 0, 0, 0)\n) TIMESTAMP(ts) PARTITION BY MONTH;`\n`mindsdb`\nContains the metadata tables necessary to create ML models:\n`sql\nUSE mindsdb;\nSHOW TABLES;`\n`shell\n+-------------------+\n| Tables_in_mindsdb |\n+-------------------+\n| models            |\n| models_versions   |\n+-------------------+`\nCreate a predictor model\nWe can create a predictor model `mindsdb.home_rentals_model_ts` to predict the\n`rental_price` for a neighborhood considering the past 20 days, and no\nadditional features:\n`sql\nCREATE PREDICTOR mindsdb.home_rentals_model_ts FROM questdb (\n    SELECT\n        neighborhood,\n        rental_price,\n        ts\n    FROM house_rentals_data\n)\nPREDICT rental_price ORDER BY ts GROUP BY neighborhood\nWINDOW 20 HORIZON 1;`\nThis triggers MindsDB to create/train the model based on the full data available\nfrom QuestDB's table `house_rentals_data` (100 rows) as a time series on the\ncolumn `ts`.\nWhen status is complete, the model is ready for use; otherwise, we simply wait\nwhile we observe MindsDB's logs.\nCreating/training a model will take time proportional to the number of features,\ni.e. cardinality of the source table as defined in the inner SELECT of the\nCREATE MODEL statement, and the size of the corpus, i.e. number of rows. The\nmodel is a table in MindsDB:\n`sql\nSHOW TABLES;`\nThe new table is displayed:\n`shell\n+-----------------------+\n| Tables_in_mindsdb     |\n+-----------------------+\n| models                |\n| models_versions       |\n| home_rentals_model_ts |\n+-----------------------+`\nDescribe the predictor model\nWe can get more information about the trained model, how was the accuracy\ncalculated or which columns are important for the model by executing the\n`DESCRIBE MODEL` statement:\n`sql\nDESCRIBE MODEL mindsdb.home_rentals_model_ts;`\n`shell\n*************************** 1. row ***************************\n        accuracies: {'complementary_smape_array_accuracy':0.859}\n           outputs: ['rental_price']\n            inputs: ['neighborhood', 'ts', '__mdb_ts_previous_rental_price']\n        datasource: home_rentals_model_ts\n             model: encoders --> dtype_dict --> dependency_dict --> model --> problem_definition --> identifiers --> imputers --> accuracy_functions`\nOr, to see how the model encoded the data prior to training we can execute:\n`sql\nDESCRIBE MODEL mindsdb.home_rentals_model_ts.features;`\n`shell\n+--------------+-------------+------------------+---------+\n| column       | type        | encoder          | role    |\n+--------------+-------------+------------------+---------+\n| neighborhood | categorical | OneHotEncoder    | feature |\n| rental_price | float       | TsNumericEncoder | target  |\n| ts           | datetime    | ArrayEncoder     | feature |\n+--------------+-------------+------------------+---------+`\nAdditional information about the models and how they can be customized can be\nfound on the Lightwood docs.\nQuery MindsDB for predictions\nThe latest `rental_price` value per neighborhood in table\n`questdb.house_rentals_data` can be obtained directly from QuestDB executing\nquery:\n`sql\nSELECT * FROM questdb (\n    SELECT\n        neighborhood,\n        rental_price,\n        ts\n    FROM house_rentals_data\n    LATEST BY neighborhood\n);`\n`shell\n+----------------+--------------+--------------+\n| neighborhood   | rental_price | ts           |\n+----------------+--------------+--------------+\n| thowsand_oaks  | 1150.427     | 1610712000.0 |   (2021-01-15 12:00:00.0)\n| south_side     | 726.953      | 1610784000.0 |   (2021-01-16 08:00:00.0)\n| downtown       | 568.73       | 1610798400.0 |   (2021-01-16 12:00:00.0)\n| westbrae       | 543.83       | 1610841600.0 |   (2021-01-17 00:00:00.0)\n| berkeley_hills | 559.928      | 1610870400.0 |   (2021-01-17 08:00:00.0)\n| alcatraz_ave   | 1268.529     | 1610884800.0 |   (2021-01-17 12:00:00.0)\n+----------------+--------------+--------------+`\nTo predict the next value:\n`sql\nSELECT\n    tb.ts,\n    tb.neighborhood,\n    tb.rental_price as predicted_rental_price,\n    tb.rental_price_explain as explanation\nFROM questdb.house_rentals_data AS ta\nJOIN mindsdb.home_rentals_model_ts AS tb\nWHERE ta.ts > LATEST;`\n`shell\n+---------------------+----------------+------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| ts                  | neighborhood   | predicted_rental_price | explanation                                                                                                                                                                              |\n+---------------------+----------------+------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| 2021-01-17 00:00:00 | downtown       |      877.3007391233444 | {\"predicted_value\": 877.3007391233444, \"confidence\": 0.9991, \"anomaly\": null, \"truth\": null, \"confidence_lower_bound\": 379.43294697022424, \"confidence_upper_bound\": 1375.1685312764646} |\n| 2021-01-19 08:00:00 | westbrae       |      923.1387395936794 | {\"predicted_value\": 923.1387395936794, \"confidence\": 0.9991, \"anomaly\": null, \"truth\": null, \"confidence_lower_bound\": 385.8327438509463, \"confidence_upper_bound\": 1460.4447353364124}  |\n| 2021-01-15 16:00:00 | thowsand_oaks  |      1418.678199780345 | {\"predicted_value\": 1418.678199780345, \"confidence\": 0.9991, \"anomaly\": null, \"truth\": null, \"confidence_lower_bound\": 1335.4600013965369, \"confidence_upper_bound\": 1501.8963981641532} |\n| 2021-01-17 12:00:00 | berkeley_hills |      646.5979284300436 | {\"predicted_value\": 646.5979284300436, \"confidence\": 0.9991, \"anomaly\": null, \"truth\": null, \"confidence_lower_bound\": 303.253838410034, \"confidence_upper_bound\": 989.9420184500532}    |\n| 2021-01-18 12:00:00 | south_side     |       1422.69481363723 | {\"predicted_value\": 1422.69481363723, \"confidence\": 0.9991, \"anomaly\": null, \"truth\": null, \"confidence_lower_bound\": 129.97617491441304, \"confidence_upper_bound\": 2715.413452360047}   |\n| 2021-01-18 04:00:00 | alcatraz_ave   |      1305.009073065412 | {\"predicted_value\": 1305.009073065412, \"confidence\": 0.9991, \"anomaly\": null, \"truth\": null, \"confidence_lower_bound\": 879.0232742685288, \"confidence_upper_bound\": 1730.994871862295}   |\n+---------------------+----------------+------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+`\nStop the container\nTo terminate the container, run:\n`shell\ndocker stop qmdb`\n\nMindsDB GitHub\nMindsDB Documentation\n",
    "tag": "questdb"
  },
  {
    "title": "Prerequisites",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/third-party-tools/grafana.md",
    "content": "\ntitle: Grafana\ndescription: Guide for time series data visualization with QuestDB and Grafana\n\nGrafana is a popular observability and monitoring\napplication used to visualize data and help with time series data analysis. It\nhas an extensive ecosystem of widgets and plugins. QuestDB supports connecting\nto Grafana via the Postgres endpoint.\nPrerequisites\n\nGrafana should be installed and\n  running.\nQuestDB should be installed and running via\n  Docker\n\nConfigure database\nTo avoid unnecessary memory usage, it is recommended to disable QuestDB's `SELECT`\nquery cache by setting the property `pg.select.cache.enabled=false` in your\n`server.conf`. This is because Grafana does not use prepared statements when\nsending the queries and the query cache becomes much less efficient.\nAdd a data source\n\nOpen Grafana's UI (by default available at `http://localhost:3000`)\nGo to the `Configuration` section and click on `Data sources`\nClick `Add data source`\nChoose the `PostgreSQL` plugin and configure it with the following settings:\n\n`bash\nhost: localhost:8812\ndatabase: qdb\nuser: admin\npassword: quest\nSSL mode: disable`\n\nWhen adding a panel, use the \"text edit mode\" by clicking the pencil icon and\n   adding a query\n\nGlobal variables\nUse\nglobal variables\nto simplify queries with dynamic elements such as date range filters.\n`$__timeFilter(timestamp)`\nThis variable allows filtering results by sending a start-time and end-time to\nQuestDB. This expression evaluates to:\n`questdb-sql\ntimestamp BETWEEN\n    '2018-02-01T00:00:00Z' AND '2018-02-28T23:59:59Z'`\n`$__interval`\nThis variable calculates a dynamic interval based on the time range applied to\nthe dashboard. By using this function, the sampling interval changes\nautomatically as the user zooms in and out of the panel.\nExample query\n`questdb-sql\nSELECT\n  pickup_datetime AS time,\n  avg(trip_distance) AS distance\nFROM taxi_trips\nWHERE $__timeFilter(pickup_datetime)\nSAMPLE BY $__interval;`\nKnown issues\nFor alert queries generated by certain Grafana versions, the macro\n`$__timeFilter(timestamp)` produces timestamps with nanosecond precision, while\nthe expected precision is millisecond precision. As a result, the alert queries\nare not compatible with QuestDB and lead to an `Invalid date` error. To resolve\nthis, we recommend the following workaround:\n```questdb-sql\nSELECT\n  pickup_datetime AS time,\n  avg(trip_distance) AS distance\nFROM taxi_trips\nWHERE pickup_datetime BETWEEN cast($__unixEpochFrom()1000000L as timestamp) and cast($__unixEpochTo()1000000L as timestamp)\n```\nSee Grafana issues for more\ninformation.\nSee also",
    "tag": "questdb"
  },
  {
    "title": "QuestDB Kafka connector",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/third-party-tools/kafka/overview.md",
    "content": "\ntitle: Ingestion from Kafka Overview\nsidebar_label: Overview\ndescription: Apache Kafka integration overview.\n\nIngesting data from Apache Kafka to QuestDB is a common use case. Possible\nstrategies are as the following:\n\nQuestDB Kafka connector: The\n   recommended strategy for connecting to Kafka using ILP and Kafka Connect.\nJDBC connector: A generic connector\n   using Kafka Connect.\nWrite a dedicated program to read data from Kafka and write to QuestDB.\nUse a stream processing engine.\n\nEach strategy has different trade-offs. The rest of this page discusses each\nstrategy and aims to guide advanced users.\nQuestDB Kafka connector\nQuestDB has developed a QuestDB Kafka connector for Kafka. The connector is\nbuilt on top of the Kafka Connect framework and uses the\nInflux Line Protocol (ILP)\nfor communication with QuestDB. Kafka Connect handles concerns such as fault\ntolerance and serialization. It also provides facilities for message\ntransformations, filtering, etc. ILP ensures operational simplicity and\nexcellent performance: it can insert 100,000s rows per second.\nThis is the recommended strategy for most users.\nJDBC connector\nSimilar to the QuestDB Kafka connector, the JDBC connector also uses the Kafka\nConnect framework. However, instead of using a dedicated ILP, it relies on a\ngeneric JDBC binary and QuestDB\nPostgreSQL protocol compatibility.\nIt requires objects in Kafka to have associated schema and overall it is more\ncomplex to set up and run. Compared to the QuestDB Kafka connector, the JDBC\nconnector has significantly lower performance, but the following advantages:\n\nJDBC insertion allows higher consistency guarantees than the fire-and-forget\n  ILP method used by the QuestDB Kafka connector.\nVarious Kafka-as-a-Service providers often have the JDBC connector\n  pre-packaged.\n\nThis strategy is recommended when the QuestDB Kafka connector cannot be used for\nsome reason.\nDedicated program\nWriting a dedicated program reading from Kafka topics and writing to QuestDB\ntables offers great flexibility: The program can do arbitrary data\ntransformations and filtering, including stateful operations. On the other hand:\nIt's the most complex strategy to implement. You'll have to deal with different\nserialization formats, handle failures, etc. This strategy is recommended for\nvery advanced use cases only. This is not recommended for most users.\nStream processing engine\nStream processing engine provides a middle ground between writing a dedicated\nprogram and using one of the connectors. Engines such as\nApache Flink provide rich API for data\ntransformations, enrichment, and filtering; at the same time, they can help you\nwith shared concerns such as fault-tolerance and serialization. However, they\noften have a non-trivial learning curve. QuestDB offers a\nconnector for Apache Flink.\nIt is the recommended strategy if you are an existing Flink user, and you need",
    "tag": "questdb"
  },
  {
    "title": "Prerequisites",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/third-party-tools/kafka/jdbc.md",
    "content": "\ntitle: JDBC connector\ndescription:\n  JDBC driver support in QuestDB allows for ingesting messages from a Kafka\n  topic via Kafka Connect.\n\nSupport for the JDBC driver means that data can easily be exported from a Kafka\ntopic and ingested directly to QuestDB by means of Kafka Connect.\nThis article assumes that users have successfully set up an installation of\nKafka and are ready to start exporting messages to QuestDB.\nPrerequisites\nYou will need the following:\n\nKafka\nA running QuestDB instance\n\nConfigure Kafka\nThe following binaries must be available to Kafka:\n\nKafka Connect JDBC binary\nPostgreSQL JDBC driver\n\nTo download these files, visit the\nKafka Connect JDBC\npage which provides CLI installation and a direct download of the required\n`.jar` files. Select the ZIP file for download, unzip the contents of the\narchive and copy the required `.jar` files to the Kafka `libs` directory:\n`shell\nunzip confluentinc-kafka-connect-jdbc-10.0.1.zip\ncd confluentinc-kafka-connect-jdbc-10.0.1\ncp kafka-connect-jdbc-10.0.1.jar /path/to/kafka_2.13-2.6.0/libs\ncp postgresql-42.2.10.jar /path/to/kafka_2.13-2.6.0/libs`\nA configuration file `/path/to/kafka/config/connect-jdbc.properties` must be\ncreated for Kafka Connect in standalone mode. The Postgres connection URL must\nmatch the destination QuestDB instance and a topic can be specified under the\n`topics={mytopic}` key. This example guide uses a topic `example-topic` and the\nPostgres server is running on the default port `8812`.\nCreate a file `config/connect-jdbc.properties` with the following contents:\n```shell\nname=local-jdbc-sink\nconnector.class=io.confluent.connect.jdbc.JdbcSinkConnector\nconnection.url=jdbc:postgresql://127.0.0.1:8812/qdb?useSSL=false\nconnection.user=admin\nconnection.password=quest\ntopics=example-topic\ninsert.mode=insert\ndialect.name=PostgreSqlDatabaseDialect\npk.mode=none\nauto.create=true\n```\nStart Kafka\nThe commands listed in this section must be run from the Kafka home directory\nand in the order shown below.\nStart the Kafka Zookeeper used to coordinate the server:\n`shell\nbin/zookeeper-server-start.sh  config/zookeeper.properties`\nStart a Kafka server:\n`shell\nbin/kafka-server-start.sh  config/server.properties`\nStart Kafka Connect:\n`shell\nbin/connect-standalone.sh config/connect-standalone.properties config/connect-jdbc.properties`\nPublish messages\nMessages can be published via the console producer script:\n`shell\nbin/kafka-console-producer.sh --topic example-topic --bootstrap-server localhost:9092`\nA `>` greater-than symbol indicates that a messages can be published to the\nexample topic from the interactive session. Paste the following minified JSON as\na single line to publish messages and create the table `example-topic` in the\nQuestDB instance:\n\n`json\n{\"schema\":{\"type\":\"struct\",\"fields\":[{\"type\":\"boolean\",\"optional\":false,\"field\":\"flag\"},{\"type\":\"int8\",\"optional\":false,\"field\":\"id8\"},{\"type\":\"int16\",\"optional\":false,\"field\":\"id16\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"id32\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"id64\"},{\"type\":\"float\",\"optional\":false,\"field\":\"idFloat\"},{\"type\":\"double\",\"optional\":false,\"field\":\"idDouble\"},{\"type\":\"string\",\"optional\":true,\"field\":\"msg\"}],\"optional\":false,\"name\":\"msgschema\"},\"payload\":{\"flag\":false,\"id8\":222,\"id16\":222,\"id32\":222,\"id64\":222,\"idFloat\":222,\"idDouble\":333,\"msg\":\"hi\"}}`\n\nVerify the integration\nTo verify that the data has been ingested into the `example-topic` table, the\nfollowing request to QuestDB's `/exp` REST endpoint can be made to export the\ntable contents via CURL:\n`shell\ncurl -G \\\n  --data-urlencode \"query=select * from 'example-topic'\" \\\n  http://localhost:9000/exp`\nThe expected response based on the example JSON message published above will be\nthe following:\n`csv\n\"flag\",\"id8\",\"id16\",\"id32\",\"id64\",\"idFloat\",\"idDouble\",\"msg\"\nfalse,-34,-34,222,222,222.0000,333.0,\"hi\"`\nJSON format\nThe JSON object sent in the example above has the following structure containing\n`schema` and `payload` objects:\n```json\n{\n  \"schema\": {\n    \"type\": \"struct\",\n    \"fields\": [\n      {\n        \"type\": \"boolean\",\n        \"optional\": false,\n        \"field\": \"flag\"\n      },\n      {\n        \"type\": \"int8\",\n        \"optional\": false,\n        \"field\": \"id8\"\n      },\n      {\n        \"type\": \"int16\",\n        \"optional\": false,\n        \"field\": \"id16\"\n      },\n      {\n        \"type\": \"int32\",\n        \"optional\": false,\n        \"field\": \"id32\"\n      },\n      {\n        \"type\": \"int64\",\n        \"optional\": false,\n        \"field\": \"id64\"\n      },\n      {\n        \"type\": \"float\",\n        \"optional\": false,\n        \"field\": \"idFloat\"\n      },\n      {\n        \"type\": \"double\",\n        \"optional\": false,\n        \"field\": \"idDouble\"\n      },\n      {\n        \"type\": \"string\",\n        \"optional\": true,\n        \"field\": \"msg\"\n      }\n    ],\n    \"optional\": false,\n    \"name\": \"msgschema\"\n  },\n  \"payload\": {\n    \"flag\": false,\n    \"id8\": 222,\n    \"id16\": 222,\n    \"id32\": 222,\n    \"id64\": 222,\n    \"idFloat\": 222,\n    \"idDouble\": 333,\n    \"msg\": \"hi\"\n  }\n}",
    "tag": "questdb"
  },
  {
    "title": "Integration guide",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/third-party-tools/kafka/questdb-kafka.md",
    "content": "\ntitle: QuestDB Kafka connector\ndescription:\n  QuestDB ships a QuestDB Kafka connector for ingesting messages from Kafka via\n  the ILP protocol.\n\nQuestDB ships a\nQuestDB Kafka connector\nfor fast ingestion from Kafka into QuestDB.\nThis page has the following main sections:\n\nA QuestDB Kafka connector integration guide\nA configuration manual for the connector\nFAQ\n\nIntegration guide\nThis guide shows the steps to use the QuestDB Kafka connector to read JSON data\nfrom Kafka topics and write them as rows into a QuestDB table.\nPrerequisites\nYou will need the following:\n\nKafka\nA running QuestDB instance\n\n\nA local\n  JDK installation\n\n\n\nConfigure Kafka\nBefore starting Kafka, the following steps must be completed:\n\nDownload the connector file.\n\nThe Apache Kafka distribution contains the Kafka Connect framework, but the\nQuestDB-specific components need to be downloaded from\nthe QuestDB Kafka connector GH page,\nunder the zip archive named `kafka-questdb-connector-<version>-bin.zip`.\n\nCopy the file to the Kafka `libs` directory.\n\nOnce downloaded, unzip the contents of the archive and copy the required `.jar`\nfiles to the Kafka `libs` directory:\n`shell\nunzip kafka-questdb-connector-*-bin.zip\ncd kafka-questdb-connector\ncp ./*.jar /path/to/kafka_2.13-2.6.0/libs`\n:::tip\nYou can automate downloading the latest connector package by running this\ncommand:\n`shell\ncurl -s https://api.github.com/repos/questdb/kafka-questdb-connector/releases/latest |\njq -r '.assets[]|select(.content_type == \"application/zip\")|.browser_download_url'|\nwget -qi -`\n:::\n\nSet the configuration file.\n\nA configuration file `/path/to/kafka/config/questdb-connector.properties` must\nbe created for Kafka Connect in the standalone mode. The host and port of the\nrunning QuestDB server must be defined. A topic can be specified under the\n`topics={mytopic}` key.\nThe example below creates a configuration file. It assumes a running QuestDB\nserver on the ILP default port, `9009`, creates a reader from a Kafka topic,\n`example-topic`, and writes into a QuestDB table, `example_table`:\n`shell title=\"Create a configuration file\"\nname=questdb-sink\nconnector.class=io.questdb.kafka.QuestDBSinkConnector\nhost=localhost:9009\ntopics=example-topic\ntable=example_table\ninclude.key=false\nvalue.converter=org.apache.kafka.connect.json.JsonConverter\nvalue.converter.schemas.enable=false\nkey.converter=org.apache.kafka.connect.storage.StringConverter`\nStart Kafka\nThe commands listed in this section must be run from the Kafka home directory\nand in the order shown below.\n\nStart the Kafka Zookeeper used to coordinate the server:\n\n`shell\nbin/zookeeper-server-start.sh  config/zookeeper.properties`\n\nStart a Kafka server:\n\n`shell\nbin/kafka-server-start.sh  config/server.properties`\n\nStart the QuestDB Kafka connector:\n\n`shell\nbin/connect-standalone.sh config/connect-standalone.properties config/questdb-connector.properties`\nPublish messages\nMessages can be published via the console producer script:\n`shell\nbin/kafka-console-producer.sh --topic example-topic --bootstrap-server localhost:9092`\nA greater-than symbol, `>`, indicates that a message can be published to the\nexample topic from the interactive session. Paste the following minified JSON as\na single line to publish the message and create the table `example-topic` in the\nQuestDB instance:\n\n`json\n{\"firstname\": \"Arthur\", \"lastname\": \"Dent\", \"age\": 42}`\n\nVerify the integration\nTo verify that the data has been ingested into the `example-topic` table, the\nfollowing request to QuestDB's `/exp` REST API endpoint can be made to export\nthe table contents via the curl command:\n`shell\ncurl -G \\\n  --data-urlencode \"query=select * from 'example_table'\" \\\n  http://localhost:9000/exp`\nThe expected response based on the example JSON message published above will be\nsimilar to the following:\n`csv\n\"firstname\",\"age\",\"lastname\",\"timestamp\"\n\"Arthur\",42,\"Dent\",\"2022-11-01T13:11:55.558108Z\"`\nIf you can see the expected result then congratulations, you have successfully\ncreated and executed your first Kafka to QuestDB pipeline! \ud83c\udf89\nAdditional sample projects\nYou can find additional sample projects on the\nQuestDB Kafka connector\nGithub project page. It includes a\nsample integration\nwith Debezium for\nChange Data Capture from\nPostgreSQL.\nConfiguration manual\nThis section lists configuration options as well as further information about\nthe Kafka Connect connector.\nConfiguration Options\nThe connector supports the following configuration options:\n| Name                              | Type      | Example                                                     | Default            | Meaning                                                    |\n| --------------------------------- | --------- | ----------------------------------------------------------- | ------------------ | ---------------------------------------------------------- |\n| topics                            | `string`  | orders                                                      | N/A                | Topics to read from                                        |\n| key.converter                     | `string`  | org.apache.kafka.connect.storage.StringConverter | N/A                | Converter for keys stored in Kafka                         |\n| value.converter                   | `string`  | org.apache.kafka.connect.json.JsonConverter      | N/A                | Converter for values stored in Kafka                       |\n| host                              | `string`  | localhost:9009                                              | N/A                | Host and port where QuestDB server is running              |\n| table                             | `string`  | my_table                                                    | Same as Topic name | Target table in QuestDB                                    |\n| key.prefix                        | `string`  | from_key                                                    | key                | Prefix for key fields                                      |\n| value.prefix                      | `string`  | from_value                                                  | N/A                | Prefix for value fields                                    |\n| skip.unsupported.types | `boolean` | false                                                       | false              | Skip unsupported types                                     |\n| timestamp.field.name   | `string`  | pickup_time                                                 | N/A                | Designated timestamp field name                            |\n| timestamp.units                   | `string`  | micros                                                      | auto               | Designated timestamp field units                           |\n| include.key                       | `boolean` | false                                                       | true               | Include message key in target table                        |\n| symbols                           | `string`  | instrument,stock                                            | N/A                | Comma separated list of columns that should be symbol type |\n| doubles                           | `string`  | volume,price                                                | N/A                | Comma separated list of columns that should be double type |\n| username                          | `string`  | user1                                                       | admin              | User name for QuestDB. Used only when token is non-empty   |\n| token                             | `string`  | QgHCOyq35D5HocCMrUGJinEsjEscJlC                  | N/A                | Token for QuestDB authentication                           |\n| tls                               | `boolean` | true                                                        | false              | Use TLS for QuestDB connection                             |\nHow does the connector work?\nThe connector reads data from Kafka topics and writes it to QuestDB tables via\nILP. The connector converts each field in the Kafka message to a column in the\nQuestDB table. Structures and maps are flatted into columns.\nExample: Consider the following Kafka message:\n`json\n{\n  \"firstname\": \"John\",\n  \"lastname\": \"Doe\",\n  \"age\": 30,\n  \"address\": {\n    \"street\": \"Main Street\",\n    \"city\": \"New York\"\n  }\n}`\nThe connector will create a table with the following columns:\n| firstname string | lastname string | age long | address_street string | address_city string |\n| --------------------------- | -------------------------- | ------------------- | -------------------------------- | ------------------------------ |\n| John                        | Doe                        | 30                  | Main Street                      | New York                       |\nSupported serialization formats\nThe connector does not deserialize data independently. It relies on Kafka\nConnect converters. The connector has been tested predominantly with JSON, but\nit should work with any converter, including Avro. Converters can be configured\nusing `key.converter` and `value.converter` options, both are included in the\nConfiguration options table above.\nDesignated timestamps\nThe connector supports\ndesignated timestamps.\nIf the message contains a timestamp field, the connector can use it as a\ntimestamp for the row. The field name must be configured using the\n`timestamp.field.name` option. The field must either be an integer or a\ntimestamp. When the field is set to an integer, the connector will autodetect\nits units. This works for timestamps after 04/26/1970, 5:46:40 PM. The units can\nalso be configured explicitly using the `timestamp.units` configuration, which\nsupports the following values:\n\n`nanos`\n`micros`\n`millis`\n`auto` (default)\n\nSymbol\nQuestDB supports a special type called\nsymbol. Use the `symbols`\nconfiguration option to specify which columns should be created as the `symbol`\ntype.\nNumeric type inference for floating point type\nWhen a configured Kafka Connect deserializer provides a schema, the connector\nuses it to determine column types. If a schema is unavailable, the connector\ninfers the type from the value. This might produce unexpected results for\nfloating point numbers, which may be interpreted as `long` initially and\ngenerates an error.\nConsider this example:\n`json\n{\n  \"instrument\": \"BTC-USD\",\n  \"volume\": 42\n}`\nKafka Connect JSON converter deserializes the `volume` field as a `long` value.\nThe connector sends it to the QuestDB server as a `long` value. If the target\ntable does not have a column `volume`, the database creates a `long` column. If\nthe next message contains a floating point value for the `volume` field, the\nconnector sends it to QuestDB as a `double` value. This causes an error because\nthe existing column `volume` is of type `long`.\nTo avoid this problem, the connector can be configured to send selected numeric\ncolumns as `double` regardless of the actual initial input value. Use the\n`doubles` configuration option to specify which columns should the connector\nalways send as the `double` type.\nTarget table considerations\nWhen a target table does not exist in QuestDB, it will be created automatically.\nThis is the recommended approach for development and testing.\nIn production, it's recommended to use the SQL\nCREATE TABLE keyword,\nbecause it gives you more control over the table schema, allowing per-table\npartitioning, creating indexes, etc.\nFAQ\n\nDoes this connector work with Schema Registry? \n\n\n\nThe Connector works independently of the serialization strategy used. It relies\non Kafka Connect converters to deserialize data. Converters can be configured\nusing `key.converter` and `value.converter` options, see the configuration\nsection above.\n\n\n\n\nI'm getting this error:\n\"org.apache.kafka.connect.errors.DataException: JsonConverter with schemas.enable requires 'schema' and 'payload' fields and may not contain additional fields. If you are trying to deserialize plain JSON data, set schemas.enable=false in your converter configuration.\"\n\n\n\nThis error means that the connector is trying to deserialize data using a\nconverter that expects a schema. The connector does not require schemas, so you\nneed to configure the converter to not expect a schema. For example, if you are\nusing a JSON converter, you need to set `value.converter.schemas.enable=false`\nor `key.converter.schemas.enable=false` in the connector configuration.\n\n\n\n\nDoes this connector work with Debezium?\n\n\n\nYes, it's been tested with Debezium as a source and a\n[sample project](https://github.com/questdb/kafka-questdb-connector/tree/main/kafka-questdb-connector-samples/stocks)\nis available. Bear in mind that QuestDB is meant to be used as an append-only\ndatabase; hence, updates should be translated as new inserts. The connector\nsupports Debezium's `ExtractNewRecordState` transformation to extract the new\nstate of the record. The transformation by default drops DELETE events, so there\nis no need to handle them explicitly.\n\n\n\n\nQuestDB is a time series database, how does it fit into Change Data\nCapture via Debezium?\n\n\n\nQuestDB works with Debezium just great! This is the recommended pattern:\nTransactional applications use a relational database to store the current state\nof the data. QuestDB is used to store the history of changes. Example: Imagine\nyou have a PostgreSQL table with the most recent stock prices. Whenever a stock\nprice changes, an application updates the PostgreSQL table. Debezium captures\neach UPDATE/INSERT and pushes it as an event to Kafka. Kafka Connect QuestDB\nconnector reads the events and inserts them into QuestDB. In this way,\nPostgreSQL will have the most recent stock prices and QuestDB will have the\nhistory of changes. You can use QuestDB to build a dashboard with the most\nrecent stock prices and a chart with the history of changes.\n\n\n\n\nHow I can select which fields to include in the target table?\n\n\n\nUse the ReplaceField transformation to remove unwanted fields. For example, if\nyou want to remove the `address` field, you can use the following configuration:\n\n```json\n{\n  \"name\": \"questdb-sink\",\n  \"config\": {\n    \"connector.class\": \"io.questdb.kafka.QuestDBSinkConnector\",\n    \"host\": \"localhost:9009\",\n    \"topics\": \"Orders\",\n    \"table\": \"orders_table\",\n    \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n    \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n    \"transforms\": \"removeAddress\",\n    \"transforms.removeAddress.type\": \"org.apache.kafka.connect.transforms.ReplaceField$Value\",\n    \"transforms.removeAddress.blacklist\": \"address\"\n  }\n}\n```\n\nSee\n[ReplaceField documentation](https://docs.confluent.io/platform/current/connect/transforms/replacefield.html#replacefield)\nfor more details.\n\n\n\n\nI need to run Kafka Connect on Java 8, but the connector says it requires\nJava 11. What should I do? \n\n\n\nThe Kafka Connect-specific part of the connectors works with Java 8. The\nrequirement for Java 11 is coming from QuestDB client itself. The zip archive\ncontains 2 JARs: `questdb-kafka-connector-VERSION.jar` and\n`questdb-VERSION.jar`. You can replace the latter with\n`questdb-VERSION-jdk8.jar` from the\n[Maven central](https://mvnrepository.com/artifact/org.questdb/questdb/6.5.4-jdk8).\nPlease note that this setup is not officially supported, and you may encounter\nissues. If you do, please report them to us.\n\n\n\nSee also\n\nChange Data Capture with QuestDB and Debezium\n",
    "tag": "questdb"
  },
  {
    "title": "sql-execution-order.md",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/concept/sql-execution-order.md",
    "content": "\ntitle: SQL execution order\nsidebar_label: SQL execution order\ndescription:\n  Execution order for SQL clauses in QuestDB. This covers the SQL keywords you\n  may already be familiar with as well as extensions to the language that are\n  unique to QuestDB.\n\nQuestDB attempts to implement standard ANSI SQL. We also try to be compatible\nwith PostgreSQL, although parts of this are a work in progress. QuestDB\nimplements these clauses which have the following execution order:\n\nFROM\nON\nJOIN\nWHERE\nLATEST ON\nGROUP BY (optional)\nWITH\nHAVING (implicit)\nSELECT\nDISTINCT\nORDER BY\nLIMIT\n\nWe have also implemented sub-queries that users may execute at any part of a\nquery that mentions a table name. The sub-query implementation adds almost zero\nexecution cost to SQL. We encourage the use of sub-queries as they add flavors\nof functional language features to traditional SQL.\nFor more information on the SQL extensions in QuestDB which deviate from ANSI\nSQL and PostgreSQL, see the",
    "tag": "questdb"
  },
  {
    "title": "Index creation and deletion",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/concept/indexes.md",
    "content": "\ntitle: Indexes\nsidebar_label: Indexes\ndescription:\n  Explanation on how indexes work as well as the pros and cons that you need to\n  be aware of when using them.\n\nAn index stores the row locations for each value of the target column in order\nto provide faster read access. It allows you to bypass full table scans by\ndirectly accessing the relevant rows during queries with `WHERE` conditions.\nIndexing is available for symbol columns. Index support\nfor other types will be added over time.\nIndex creation and deletion\nThe following are ways to index a `symbol` column:\n\nAt table creation time using\n  CREATE TABLE\nUsing\n  ALTER TABLE ALTER COLUMN ADD INDEX\n  to index an existing `symbol` column\n\nTo delete an index:\n\nALTER TABLE ALTER COLUMN DROP INDEX\n\nHow indexes work\nIndex creates a table of row locations for each distinct value for the target\nsymbol. Once the index is created, inserting data into\nthe table will update the index. Lookups on indexed values will be performed in\nthe index table directly which will provide the memory locations of the items,\nthus avoiding unnecessary table scans.\nHere is an example of a table and its index table.\n`shell\nTable                                       Index\n|Row ID | Symbol    | Value |             | Symbol     | Row IDs       |\n| 1     | A         | 1     |             | A          | 1, 2, 4       |\n| 2     | A         | 0     |             | B          | 3             |\n| 3     | B         | 1     |             | C          | 5             |\n| 4     | A         | 1     |\n| 5     | C         | 0     |`\n`INSERT INTO Table values(B, 1);` would trigger two updates: one for the Table,\nand one for the Index.\n`shell\nTable                                       Index\n|Row ID | Symbol    | Value |             | Symbol     | Row IDs       |\n| 1     | A         | 1     |             | A          | 1, 2, 4       |\n| 2     | A         | 0     |             | B          | 3, 6          |\n| 3     | B         | 1     |             | C          | 5             |\n| 4     | A         | 1     |\n| 5     | C         | 0     |\n| 6     | B         | 1     |`\nIndex capacity\nWhen a symbol column is indexed, an additional index capacity can be defined\nto specify how many row IDs to store in a single storage block on disk:\n\nServer-wide setting: `cairo.index.value.block.size` with a default of `256`\nColumn-wide setting: The\n  index option for\n  `CREATE TABLE`\nColumn-wide setting:\n  ALTER TABLE COLUMN ADD INDEX\n\nFewer blocks used to store row IDs achieves better performance. At the same time\nover-sizing the setting will result in higher than necessary disk space usage.\n:::note\n\nThe index capacity and\n  symbol capacity are different\n  settings.\nThe index capacity value should not be changed, unless an user is aware of all\n  the implications.\n\n:::\nAdvantages\nIndex allows you to greatly reduce the complexity of queries that span a subset\nof an indexed column, typically when using `WHERE` clauses.\nConsider the following query applied to the above table\n`SELECT sum(Value) FROM Table WHERE Symbol='A';`\n\nWithout Index, the query engine would scan the whole table in order to\n  perform the query. It will need to perform 6 operations (read each of the 6\n  rows once).\nWith Index, the query engine will first scan the index table, which is\n  considerably smaller. In our example, it will find A in the first row. Then,\n  the query engine would check the values at the specific locations 1, 2, 4 in\n  the table to read the corresponding values. As a result, it would only scan\n  the relevant rows in the table and leave irrelevant rows untouched.\n\nTrade-offs\n\n\nStorage space: The index will maintain a table with each distinct symbol\n  value and the locations where these symbols can be found. As a result, there\n  is a small cost of storage associated with indexing a symbol field.\n\n\nIngestion performance: Each new entry in the table will trigger an entry\n  in the Index table. This means that any write will now require two write\n  operations, and therefore take twice as long.\n\n\nExamples\nTable with index\nAn example of `CREATE TABLE` command creating a table with an index capacity of\n128:\n`questdb-sql\nCREATE TABLE my_table(symb SYMBOL, price DOUBLE, ts TIMESTAMP),\n  INDEX (symb CAPACITY 128) timestamp(ts);\n-- equivalent to\nCREATE TABLE my_table(symb SYMBOL INDEX CAPACITY 128, price DOUBLE, ts TIMESTAMP),\n  timestamp(ts);`\nIndex capacity\nConsider an example table with 200 unique stock symbols and 1,000,000,000\nrecords over time. The index will have to store 1,000,000,000 / 200 row IDs for\neach symbol, i.e. 5,000,000 per symbol.\n\nIf the index capacity is set to 1,048,576 in this case, QuestDB will use 5\n  blocks to store the row IDs.\nIf the index capacity is set to 1,024 in this case, the block count will be\n",
    "tag": "questdb"
  },
  {
    "title": "`conf` directory",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/concept/root-directory-structure.md",
    "content": "\ntitle: Root directory structure\nsidebar_label: Root directory\ndescription: Contents of the  folder explained.\n\nimport Tabs from \"@theme/Tabs\"\nimport TabItem from \"@theme/TabItem\"\nQuestDB creates the following file structure in its `root_directory`:\n`filestructure\nquestdb\n\u251c\u2500\u2500 conf\n\u251c\u2500\u2500 db\n\u251c\u2500\u2500 log\n\u251c\u2500\u2500 public\n\u2514\u2500\u2500 snapshot (optional)`\nBy default, QuestDB's root directory will be the following:\n\n\n\n\n`shell\n$HOME/.questdb`\n\n\nPath on Macs with Apple Silicon (M1 or M2) chip:\n`shell\n/opt/homebrew/var/questdb`\nPath on Macs with Intel chip:\n`shell\n/usr/local/var/questdb`\n\n\n`shell\nC:\\Windows\\System32\\qdbroot`\n\n\n`conf` directory\nContains configuration files for QuestDB:\n`filestructure\n\u251c\u2500\u2500 conf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 date.formats\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 mime.types\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 server.conf`\n| file           | description                                                                                                    |\n| -------------- | -------------------------------------------------------------------------------------------------------------- |\n| `date.formats` | A list of date formats in plain text.                                                                          |\n| `mime.types`   | Mapping file used by the HTTP server to map file extension to response type when an user downloads a file.     |\n| `server.conf`  | Server configuration file. Find out more in the server configuration section. |\n`db` directory\nThis directory contains all the files related to database tables. It is\norganised as follows:\n\nEach table has its own `table_directory` under `root_directory/db/table_name`\nWithin a `table_directory`, each partition has its\n  own `partition_directory`.\nWithin each `partition directory`, each column has its own `column_file`, for\n  example `mycolumn.d`\nIf a given column has an index, then there will also\n  be an `index_file`, for example `mycolumn.k`\n\nThe table also stores metadata in `_meta` files:\n```filestructure\n\u251c\u2500\u2500 db\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Table\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 Partition 1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 _archive\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 column1.d\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 column2.d\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 column2.k\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 ...\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 Partition 2\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 _archive\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 column1.d\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 column2.d\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 column2.k\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 ...\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 _meta\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 _txn\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 _cv\n\u2502\u00a0\u00a0 \u2514\u2500\u2500  table_1.lock\n```\nIf the table is not partitioned, data is stored in a directory called `default`:\n`filestructure\n\u251c\u2500\u2500 db\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Table\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 default\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 _archive\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 column1.d\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 column2.d\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 column2.k\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 ...\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 _meta\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 _txn\n\u2502\u00a0\u00a0 \u2514\u2500\u2500  table_1.lock`\nFor a WAL table, the table structure contains one or more `wal` folders and a\n`seq` folder representing the Sequencer:\n`wal table filestructure\n\u251c\u2500\u2500 db\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Table\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 Partition 1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 _archive\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 column1.d\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 column2.d\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 column2.k\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 ...\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 Partition 2\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 _archive\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 column1.d\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 column2.d\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 column2.k\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 ...\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 txn_seq\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 _meta\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 _txnlog\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 _wal_index.d\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 wal1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 0\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502    \u00a0  \u251c\u2500\u2500 _meta\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502    \u00a0  \u251c\u2500\u2500 _event\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502    \u00a0  \u251c\u2500\u2500 column1.d\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502    \u00a0  \u251c\u2500\u2500 column2.d\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502       \u2514\u2500\u2500 ...\n|   |   |\u00a0\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 wal2\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 0\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502   \u2502\u00a0  \u251c\u2500\u2500 _meta\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502   \u2502\u00a0  \u251c\u2500\u2500 _event\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502   \u2502\u00a0  \u251c\u2500\u2500 column1.d\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502   \u2502\u00a0  \u251c\u2500\u2500 column2.d\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502   \u2502\u00a0  \u2514\u2500\u2500 ...\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0  \u00a0\u00a0 \u251c\u2500\u2500 _meta\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 _event\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 column1.d\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 column2.d\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 ...\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 _meta\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 _txn\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 _cv\n\u2502\u00a0\u00a0 |`\n:::caution\nAs tempting as it may be to delete partitions by manually removing the\ndirectories from the file system, we really discourage this. The partitions are\norganised with metadata and deleting them directly could corrupt the table. We\nrecommend you use\nALTER TABLE DROP PARTITION for\nthis effect.\n:::\n`log` directory\nContains the log files for QuestDB:\n`filestructure\n\u251c\u2500\u2500 log\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 stdout-2020-04-15T11-59-59.txt\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 stdout-2020-04-12T13-31-22.txt`\nLog files look like this:\n`shell\n2020-04-15T16:42:32.879970Z I i.q.c.TableReader new transaction [txn=2, transientRowCount=1, fixedRowCount=1, maxTimestamp=1585755801000000, attempts=0]\n2020-04-15T16:42:32.880051Z I i.q.g.FunctionParser call to_timestamp('2020-05-01:15:43:21','yyyy-MM-dd:HH:mm:ss') -> to_timestamp(Ss)\n2020-04-15T16:42:32.880657Z I i.q.c.p.WriterPool >> [table=`table_1`, thread=12]\n2020-04-15T16:42:32.881330Z I i.q.c.AppendMemory truncated and closed [fd=32]\n2020-04-15T16:42:32.881448Z I i.q.c.AppendMemory open /usr/local/var/questdb/db/table_1/2020-05/timestamp.d [fd=32, pageSize=16777216]\n2020-04-15T16:42:32.881708Z I i.q.c.AppendMemory truncated and closed [fd=33]\n2020-04-15T16:42:32.881830Z I i.q.c.AppendMemory open /usr/local/var/questdb/db/table_1/2020-05/temperature.d [fd=33, pageSize=16777216]\n2020-04-15T16:42:32.882092Z I i.q.c.AppendMemory truncated and closed [fd=34]\n2020-04-15T16:42:32.882210Z I i.q.c.AppendMemory open /usr/local/var/questdb/db/table_1/2020-05/humidity.d [fd=34, pageSize=16777216]\n2020-04-15T16:42:32.882248Z I i.q.c.TableWriter switched partition to '/usr/local/var/questdb/db/table_1/2020-05'\n2020-04-15T16:42:32.882571Z I i.q.c.p.WriterPool << [table=`table_1`, thread=12]\n2020-04-15T16:44:33.245144Z I i.q.c.AppendMemory truncated and closed [fd=32]\n2020-04-15T16:44:33.245418Z I i.q.c.AppendMemory truncated and closed [fd=33]\n2020-04-15T16:44:33.245712Z I i.q.c.AppendMemory truncated and closed [fd=34]\n2020-04-15T16:44:33.246096Z I i.q.c.ReadWriteMemory truncated and closed [fd=30]\n2020-04-15T16:44:33.246217Z I i.q.c.ReadOnlyMemory closed [fd=31]\n2020-04-15T16:44:33.246461Z I i.q.c.TableWriter closed 'table_1'\n2020-04-15T16:44:33.246492Z I i.q.c.p.WriterPool closed [table=`table_1`, reason=IDLE, by=12]\n2020-04-15T16:44:33.247184Z I i.q.c.OnePageMemory closed [fd=28]\n2020-04-15T16:44:33.247239Z I i.q.c.ReadOnlyMemory closed [fd=27]\n2020-04-15T16:44:33.247267Z I i.q.c.TableReader closed 'table_1'\n2020-04-15T16:44:33.247287Z I i.q.c.p.ReaderPool closed 'table_1' [at=0:0, reason=IDLE]\n2020-04-15T16:44:39.763406Z I http-server disconnected [ip=127.0.0.1, fd=24]\n2020-04-15T16:44:39.763729Z I i.q.c.h.HttpServer pushed`\n`public` directory\nContains the web files for the Web Console:\n`filestructure\n\u2514\u2500\u2500 public\n    \u251c\u2500\u2500 assets\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 console-configuration.json\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 favicon.png\n    \u251c\u2500\u2500 index.html\n    \u251c\u2500\u2500 qdb.js\n    \u251c\u2500\u2500 qdb.css\n    \u2514\u2500\u2500 ...`\n`snapshot` directory\nCreated when a filesystem (disk) snapshot is\ncollected. Contains table metadata file copies.\n`tmp` directory\nCreated when a COPY SQL command is run for a\npartitioned table and no value is set for the `cairo.sql.copy.work.root`\nconfiguration setting. Contains temporary import files like indexes or temporary",
    "tag": "questdb"
  },
  {
    "title": "Properties",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/concept/designated-timestamp.md",
    "content": "\ntitle: Designated timestamp\nsidebar_label: Designated timestamp\ndescription:\n  How designated timestamps are implemented and why it is an important\n  functionality for time series.\n\nQuestDB offers the option to elect a column as a designated timestamp. This\nallows you to specify which column the tables will be indexed by in order to\nleverage time-oriented language features and high-performance functionalities.\nA designated timestamp is elected by using the\ntimestamp(columnName) function:\n\nduring a CREATE TABLE operation\nduring a SELECT operation\n  (`dynamic timestamp`)\nwhen ingesting data via ILP, for tables that do not already exist in QuestDB,\n  partitions are applied automatically by day by default with a `timestamp`\n  column\n\n:::info\n\n\nChecking if tables contain a designated timestamp column can be done via the\n  `tables()` and `table_columns()` functions which are described in the\n  meta functions documentation page.\n\n\nThe native timestamp format used by QuestDB is a Unix timestamp in microsecond\n  resolution. See\n  Timestamps in QuestDB\n  for more details.\n\n\n:::\nProperties\n\nOnly a column of type `timestamp` can be elected as a designated timestamp.\nOnly one column can be elected for a given table.\n\nOut-of-order policy\nAs of version 6.0.0, QuestDB supports the ingestion of records that are\nout-of-order (O3) by time. QuestDB detects and adjusts data ingestion for O3\ndata automatically and no manual configuration is required.\nAdvantages\nElecting a designated timestamp allows you to:\n\nPartition tables by time range. For more information, see the\n  partitions reference.\nUse time series joins such as `ASOF JOIN`. For more information, see the\n",
    "tag": "questdb"
  },
  {
    "title": "Properties",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/concept/write-ahead-log.md",
    "content": "\ntitle: Write-Ahead Log (WAL)\nsidebar_label: Write-Ahead Log\ndescription:\n  Documentation for properties of a WAL table and comparison with its non-WAL\n  counterpart.\n\nimport Screenshot from \"@theme/Screenshot\"\nFrom QuestDB 7.0, we add a new approach to ingest data using a write-ahead log\n(WAL). This page introduces the properties of a WAL-enabled table (WAL table)\nand compares it with a non-WAL table. It also contains a summary of key\ncomponents and relevant functions as well as SQL keywords.\nProperties\nA WAL table must be partitioned. It permits the\nfollowing concurrent transactions:\n\nData ingestion through different interfaces\nData modifications\nTable schema changes\n\nThe following configurations and keywords enable and create WAL tables:\n\n\nWAL table creation is enabled by the following methods:\n\n\nTable-wide configuration via\n    CREATE TABLE:\n\n`WAL` generates a WAL table.\n`BYPASS WAL` generates a non-WAL table.\n\n\n\nServer-wide configuration `cairo.wal.enabled.default`: When\n    `cairo.wal.enabled.default` is set to `true`,\n    CREATE TABLE SQL keyword generates\n    WAL tables without `WAL` and `BYPASS WAL`.\n\n\nParallel threads to apply WAL data to the table storage can be configured, see\n  WAL table configuration\n  for more details.\n\n\nComparison\nThe following table highlights the main difference between a WAL and a non-WAL\ntable:\n| WAL table                                                                                  | Non-WAL table                                                                                                             |\n| ------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------- |\n| Concurrent data ingestion via multiple interfaces                                          | ILP data locks the table for ingestion, concurrent data ingestion via other interfaces is not allowed - `Table Busy`error |\n| Unconstrained concurrent DDLs and DMLs                                                     | Concurrent DDLs and DMLs for ILP interface only                                                                           |\n| Asynchronous operations - in rare situations there may be slight delays in data visibility | Synchronous operations - no-wait commits                                                                                  |\n| Improved data freshness for `DROP` and `RENAME` of the table with a system-wide lock       | No change                                                                                                                 |\n| Some impacts on existing operations                                        | No change                                                                                                                 |\nLimitations\n:::note\nWe are working to reduce the limitations.\n:::\nFor a WAL table, the following existing operations may have different behaviors\nto a non-WAL table:\n\n\nUPDATE:\n\n\nNo row count returned\n\n\nNo support for `JOIN`\n\n\n`ALTER TABLE`:\n\n\nADD COLUMN can only add 1\n    column per statement\n\n\nNon-structural operations may fail silently. These are partition-level and\n    configuration operations:\n\nATTACH PARTITION\nDETACH PARTITION\nDROP PARTITION\nSET PARAM\n\n\n\nKey components\nA WAL table uses the following components to manage concurrent commit requests:\n\n\n`WAL`: dedicated APIs for each ingestion interface. When data is ingested via\n  multiple interfaces, dedicated `WALs` ensure that the table is not locked for\n  one interface only.\n\n\nSequencer: centrally manages transactions, providing a single source of truth.\n  The sequencer generates unique `txn` numbers as identifications to\n  transactions and keeps a log tracking the allocation. This log is called\n  `TransactionLog` and is stored in a meta file called `_txnlog`. See\n  root directory for\n  more information.\n\n\nWAL apply job: collects the commit requests based on the unique `txn` numbers\n  and sends them to the `TableWriter` to be committed.\n\n\n`TableWriter`: updates the database and resolves any out-of-order data writes.\n  Each committed transaction is given a `txn` number.\n\n\n\n\nChecking WAL configurations\nThe following table metadata functions are useful to check WAL table settings:\n\ntables() shows general table\n  metadata, including whether a table is a WAL table or not.\nwal_tables() shows WAL-table\n  status.\nALTER TABLE RESUME WAL restarts\n  suspended transactions.\n\n",
    "tag": "questdb"
  },
  {
    "title": "SQL extensions",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/concept/sql-extensions.md",
    "content": "\ntitle: SQL extensions\ndescription:\n  QuestDB attempts to implement standard ANSI SQL with time-based extensions for\n  convenience. This document describes SQL extensions in QuestDB and how users\n  can benefit from them.\n\nQuestDB attempts to implement standard ANSI SQL. We also try to be compatible\nwith PostgreSQL, although parts of this are a work in progress. This page\npresents the main extensions we bring to SQL and the main differences that one\nmight find in SQL but not in QuestDB's dialect.\nSQL extensions\nWe have extended SQL to support our data storage model and simplify semantics of\ntime series analytics.\nLATEST ON\nLATEST ON is a clause introduced to help find\nthe latest entry by timestamp for a given key or combination of keys as part of\na `SELECT` statement.\n`questdb-sql title=\"LATEST ON customer ID and currency\"\nSELECT * FROM balances\nWHERE balance > 800\nLATEST ON ts PARTITION BY customer_id, currency;`\nSAMPLE BY\nSAMPLE BY is used for time-based\naggregations with an efficient syntax.\nThe short query below will return the simple average balance from a list of\naccounts by one month buckets.\n`questdb-sql title=\"SAMPLE BY one month buckets\"\nSELECT avg(balance) FROM accounts SAMPLE BY 1M`\nTimestamp search\nTimestamp search can be performed with regular operators, e.g `>`, `<=` etc.\nHowever, QuestDB provides a\nnative notation which is faster\nand less verbose.\n`questdb-sql title=\"Results in a given year\"\nSELECT * FROM scores WHERE ts IN '2018';`\nDifferences from standard SQL\nSELECT * FROM is optional\nIn QuestDB, using `SELECT * FROM` is optional, so `SELECT * FROM my_table;` will\nreturn the same result as `my_table;`. While adding `SELECT * FROM` makes SQL\nlook more complete, there are examples where omitting these keywords makes\nqueries a lot easier to read.\n`questdb-sql title=\"Optional use of SELECT * FROM\"\nmy_table;\n-- equivalent to:\nSELECT * FROM my_table;`\nGROUP BY is optional\nThe `GROUP BY` clause is optional and can be omitted as the QuestDB optimizer\nderives group-by implementation from the `SELECT` clause. In standard SQL, users\nmight write a query like the following:\n`questdb-sql\nSELECT a, b, c, d, sum(e) FROM tab GROUP BY a, b, c, d;`\nHowever, enumerating a subset of `SELECT` columns in the `GROUP BY` clause is\nredundant and therefore unnecessary. The same SQL in QuestDB SQL-dialect can be\nwritten as:\n`questdb-sql\nSELECT a, b, c, d, sum(e) FROM tab;`\nImplicit HAVING\nLet's look at another more complex example using `HAVING` in standard SQL:\n`questdb-sql\nSELECT a, b, c, d, sum(e)\nFROM tab\nGROUP BY a, b, c, d\nHAVING sum(e) > 100;`\nIn QuestDB's dialect, featherweight sub-queries come to the rescue to create a\nsmaller, more readable query, without unnecessary repetitive aggregations.\n`HAVING` functionality can be obtained implicitly as follows:\n```questdb-sql\n(SELECT a, b, c, d, sum(e) s FROM tab) WHERE s > 100;",
    "tag": "questdb"
  },
  {
    "title": "Properties",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/concept/partitions.md",
    "content": "\ntitle: Partitions\nsidebar_label: Partitions\ndescription:\n  Overview of QuestDB's partition system for time-series. This is an important\n  feature that will help you craft more efficient queries.\n\nQuestDB offers the option to partition tables by intervals of time. Data for\neach interval is stored in separate sets of files.\nimport Screenshot from \"@theme/Screenshot\"\n\nProperties\n\nAvailable partition intervals are `NONE`, `YEAR`, `MONTH`, `DAY`, and `HOUR`.\nDefault behavior is `PARTITION BY NONE` when using CREATE TABLE and `PARTITION BY DAY` via ILP ingestion.\nPartitions are defined at table creation. For more information, refer to\n  CREATE TABLE section.\nThe naming convention for partition directories is as follows:\n\n| Table Partition | Partition format |\n| --------------- | ---------------- |\n| `HOUR`          | `YYYY-MM-DD-HH`  |\n| `DAY`           | `YYYY-MM-DD`     |\n| `MONTH`         | `YYYY-MM`        |\n| `YEAR`          | `YYYY`           |\n:::info\nPartitioning is only possible on tables which have a designated timestamp. For\nmore information on designated timestamps, refer to the\ndesignated timestamp section.\n:::\nAdvantages\n\nReduced disk IO for timestamp interval searches. This is because our SQL\n  optimiser leverages partitioning.\nSignificantly improved calculations and seek times. This is achieved by\n  leveraging the chronology and relative immutability of data for previous\n  partitions.\nPhysical separation of data files. This makes it easily to implement file\n  retention policies or extract certain intervals.\n\nStorage example\nEach partition effectively is a directory on the host machine corresponding to\nthe partitioning interval. In the example below, we assume a table `trips` that\nhas been partitioned using `PARTITION BY MONTH`.\n`[quest-user trips]$ dir\n2017-03     2017-10   2018-05     2019-02\n2017-04     2017-11   2018-06     2019-03\n2017-05     2017-12   2018-07     2019-04\n2017-06     2018-01   2018-08   2019-05\n2017-07     2018-02   2018-09   2019-06\n2017-08     2018-03   2018-10\n2017-09     2018-04   2018-11`\nEach partition on the disk contains the column data files of the corresponding\ntimestamp interval.\n```\n[quest-user 2019-06]$ dir\n_archive    cab_type.v              dropoff_latitude.d     ehail_fee.d\ncab_type.d  congestion_surcharge.d  dropoff_location_id.d  extra.d\ncab_type.k  dropoff_datetime.d      dropoff_longitude.d    fare_amount.d",
    "tag": "questdb"
  },
  {
    "title": "Advantages of `symbol` types",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/concept/symbol.md",
    "content": "\ntitle: Symbol\nsidebar_label: Symbol\ndescription:\n  Documentation for usage of the symbol data type in QuestDB. This type is used\n  to store repetitive strings in order to enable optimizations on storage and\n  search.\n\nQuestDB introduces a data type called `symbol`; a data structure used to store\nrepetitive strings. Internally, `symbol` types are stored as a table of integers\nand their corresponding string values.\nThis page presents the concept, optional setting, and their indication for\n`symbol` types.\nAdvantages of `symbol` types\n\nGreatly improved query performance as string operations compare and write\n  `int` types instead of `string`.\nGreatly improved storage efficiency as `int` maps to `string` types.\nUnobtrusive to the user because SQL execution has the same result as handling\n  string values.\nReduced complexity of database schemas by removing the need for explicit\n  additional tables or joins.\n\nProperties\n\nSymbol tables are stored separately from column data.\nFast conversion from `string` to `int` and vice-versa when reading or writing\n  data.\nColumns defined as `symbol` types support indexing.\nBy default, QuestDB caches `symbol` types in memory for improved query speed\n  and ILP ingestion speed. The setting is configurable.\n\nUsage of `symbols`\n`Symbol` columns\nColumns can be specified as `SYMBOL` using\nCREATE TABLE, similar to other types:\n`questdb-sql title=\"Create table with a SYMBOL type\"\nCREATE TABLE my_table\n  (symb SYMBOL CAPACITY 128 NOCACHE, price DOUBLE, ts TIMESTAMP)\ntimestamp(ts);`\nThe following additional symbol settings are defined, either globally as part of\nthe server configuration or locally when a\ntable is created:\n\n\nSymbol capacity: Optional setting used to indicate how many distinct\n  values this column is expected to have. Based on the value used, the data\n  structures will resize themselves when necessary, to allow QuestDB to function\n  correctly. Underestimating the symbol value count may result in drop of\n  performance whereas over-estimating may result in higher disk space and memory\n  consumption. Symbol capacity is also used to set the initial symbol cache size\n  when the cache is enabled.\n\n\nServer-wide setting: `cairo.default.symbol.capacity` with a default of `256`\n\n\nColumn-wide setting: The\n    CAPACITY option for\n    `CREATE TABLE`\n\n\nCache: Optional setting specifying whether a symbol should be cached. When\n  a `symbol` column is cached, QuestDB will use a Java heap-based hash table to\n  resolve symbol values and keys. When a column has a large number of distinct\n  symbol values (over 100,000, for example), the heap impact might be\n  significant and may cause OutOfMemory errors, depending on the heap size. Not\n  caching leverages a memory-mapped structure which can deal with larger value\n  counts but is slower.\n\n\nServer-wide setting: `cairo.default.symbol.cache.flag` with a default of\n    `true`\n\nColumn-wide setting when a table is created: The\n    CACHE | NOCACHE keyword\n    for `CREATE TABLE`\n\nSymbols for column indexing\n`Symbols` may also be indexed for faster query execution. See",
    "tag": "questdb"
  },
  {
    "title": "Append model",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/concept/storage-model.md",
    "content": "\ntitle: Storage model\nsidebar_label: Storage model\ndescription:\n  Overview of QuestDB's column-based storage model. It ensures table level\n  atomicity and durability while keeping low overhead for maximum performance.\n\nQuestDB uses a column-based storage model. Data is stored in tables with\neach column stored in its own file and its own native format. New data is\nappended to the bottom of each column to allow data to be organically retrieved\nin the same order that it was ingested.\nAppend model\nQuestDB appends one column at a time and each one is updated using the same\nmethod. The tail of column file is mapped into the memory page in RAM and the\ncolumn append is effectively a memory write at an address. Once the memory page\nis exhausted it is unmapped and a new page is mapped.\nThis method ensures minimum resource churn and consistent append latency.\nimport Screenshot from \"@theme/Screenshot\"\n\nRead model\nTable columns are randomly accessible. Columns with fixed size data types are\nread by translating the record number into a file offset by a simple bit shift.\nThe offset in the column file is then translated into an offset in a lazily\nmapped memory page, where the required value is read from.\n\nConsistency and durability\nQuestDB ensures table level isolation and consistency by applying table\nupdates atomically. Updates to a table are applied in the context of a table\ntransaction which is either committed or rolled back in an atomic operation.\nQueries that are concurrent with table updates are consistent in the sense that\nthey will return data either as it was before or after the table transaction was\ncommitted \u2014 no intermediate uncommitted data will be shown in a query result.\nTo guarantee atomicity, each table maintains a `last_committed_record_count`\nin a separate file. By convention, any table reader will never read more records\nthan the transaction count. This enables the isolation property: where\nuncommitted data cannot be read. Since uncommitted data is appended directly to\nthe table, the transaction size is only limited by the available disk space.\nOnce all data is appended, QuestDB `commit()` ensures that the transaction count\nis updated atomically both in multi-threaded and multi-process environments. It\ndoes so lock-free to ensure minimal impact on concurrent reads.\nThe consistency assurance of the data stored is limited to QuestDB\nauto-repairing abnormally terminated transactions. We do not yet support\nuser-defined constraints, checks and triggers.\nBy default QuestDB relies on OS-level data durability leaving the OS to\nwrite dirty pages to disk. Data durability can be also configured with\n`commit()` optionally being able to invoke `msync()` with a choice of\nsynchronous or asynchronous IO. The `msync()` calls are made for column files\nonly, so while the `sync`/`async` commit modes improve the overall durability,\nthey don't guarantee durability in the face of OS errors or power loss.\n\nSummary\nThe QuestDB storage model uses memory mapped files and cross-process atomic\ntransaction updates as a low overhead method of inter-process communication.\nData committed by one process can be instantaneously read by another process,\neither randomly (via queries) or incrementally (as a data queue). QuestDB provides\na variety of reader implementations.\n",
    "tag": "questdb"
  },
  {
    "title": "Queries eligible for JIT compilation",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/concept/jit-compiler.md",
    "content": "\ntitle: JIT compiler\nsidebar_label: JIT compiler\ndescription:\n  Documentation for usage of the just-in-time (JIT) SQL compiler in QuestDB. JIT\n  compilation enhances the performance of the system to run SQL queries which\n  contain filters with arithmetical expressions.\n\nQuestDB includes a JIT compiler which is run on queries (and sub-queries) that\nperform a full scan over a table or table partitions. The main goal behind this\nfeature is to improve performance for filters with arithmetical expressions. To\ndo so, the JIT compiler emits machine code with a single function that may also\nuse SIMD (vector) instructions.\nFor details on the implementation, motivation, and internals of this feature,\nsee our article about SQL JIT compilation.\nThis post describes our storage model, how we built a JIT compiler for SQL and\nour plans for improving it in future.\nQueries eligible for JIT compilation\nThe types of queries that are eligible for performance improvements via JIT\ncompilation are those which contain `WHERE` clauses. Here are some examples\nwhich are supported based on the `cpu-only` data set from the\nTime Series Benchmark Suite\nuse case:\n```questdb-sql\n-- basic filtering in WHERE clauses\nSELECT count(), max(usage_user) FROM cpu WHERE usage_user > 75;\n-- sub-queries\nSELECT * FROM cpu\nWHERE usage_user > 75\nAND (region = 'us-west-1' OR region = 'us-east-1');\n```\nJIT compiler usage\nThe JIT compiler is enabled by default for QuestDB 6.3 onwards. If you wish to\ndisable it, change the `cairo.sql.jit.mode` setting in the\nserver configuration file from `on` to `off`:\n`ini title=\"path/to/server.conf\"\ncairo.sql.jit.mode=off`\nEmbedded API users are able to enable or disable the compiler globally by\nproviding their `CairoConfiguration` implementation. Alternatively, JIT\ncompilation can be changed for a single query by using the\n`SqlExecutionContext#setJitMode` method. The latter may look like the following:\n`java\nfinal CairoConfiguration configuration = new DefaultCairoConfiguration(temp.getRoot().getAbsolutePath());\ntry (CairoEngine engine = new CairoEngine(configuration)) {\n    final SqlExecutionContextImpl ctx = new SqlExecutionContextImpl(engine, 1);\n    // Enable SQL JIT compiler\n    ctx.setJitMode(SqlJitMode.JIT_MODE_ENABLED);\n    // Subsequent query execution (called as usual) with have JIT enabled\n    try (SqlCompiler compiler = new SqlCompiler(engine)) {\n        try (RecordCursorFactory factory = compiler.compile(\"abc\", ctx).getRecordCursorFactory()) {\n            try (RecordCursor cursor = factory.getCursor(ctx)) {\n                // ...\n            }\n        }\n    }\n}`\nServer logs should contain references to `SQL JIT compiler mode`:\n`log\n2021-12-16T09:25:34.472450Z A server-main SQL JIT compiler mode: on`\nDue to certain limitations noted below, JIT compilation won't take place for all\nqueries. To understand whether JIT compilation took place for a query, one will\nsee something similar in the server logs:\n`log\n2021-12-16T09:35:01.738910Z I i.q.g.SqlCompiler plan [q=`select-group-by count() count from (select [usage_user] from cpu timestamp (timestamp) where usage_user > 75)`, fd=73]\n2021-12-16T09:35:01.742777Z I i.q.g.SqlCodeGenerator JIT enabled for (sub)query [tableName=cpu, fd=73]`\nKnown limitations\nThe current implementation of the JIT SQL compiler has a number of limitations:\n\nOnly x86-64 CPUs are currently supported.\nVectorized filter execution requires AVX2 instruction set.\nFilters with any SQL function, such as `now()`, or `abs()`, or `round()`, are\n  not supported.\nFilters with any pseudo-function or operator, such as `in()` on symbol column,\n  or `between` on non-designated timestamp column, or `within` on geohash\n  column, are not supported.\nOnly the following arithmetic operations are allowed to be present in the\n  filter: `+`, `-`, `*`, `/`.\nOnly filters with fixed-size columns are supported: BOOLEAN, BYTE, GEOBYTE,\n  SHORT, GEOSHORT, CHAR, INT, GEOINT, SYMBOL, FLOAT, LONG, GEOLONG, DATE,\n",
    "tag": "questdb"
  },
  {
    "title": "Inserting the following line",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/concept/geohashes.md",
    "content": "\ntitle: Geospatial data\ndescription:\n  This document describes how to work with geohashes as geospatial types in\n  QuestDB, including hints on converting back and forth from latitude and\n  longitude, inserting via SQL, InfluxDB line protocol, CSV, and more.\n\nQuestDB adds support for working with geospatial data through a `geohash` type.\nThis page describes how to use geohashes, with an overview of the syntax,\nincluding hints on converting from latitude and longitude, inserting via SQL,\nInfluxDB line protocol, and via Java embedded API.\nTo facilitate working with this data type,\nspatial functions and\noperators have been added to help with\nfiltering and generating data.\nGeohash description\nA geohash is a convenient way of expressing a location using a short\nalphanumeric string, with greater precision obtained with longer strings. The\nbasic idea is that the Earth is divided into grids of defined size, and each\narea is assigned a unique id called its Geohash. For a given location on Earth,\nwe can convert latitude and longitude as\nthe approximate center point\nof a grid represented by a geohash string. This string is the Geohash and will\ndetermine which of the predefined regions the point belongs to.\nIn order to be compact, base32\nis used as a representation of Geohashes, and is therefore comprised of:\n\nall decimal digits (0-9) and\nalmost all of the alphabet (case-insensitive) except \"a\", \"i\", \"l\", \"o\".\n\nThe following figure illustrates how increasing the length of a geohash result\nin a higher-precision grid size:\nimport Screenshot from \"@theme/Screenshot\"\n\nQuestDB geohash type\nGeohash column types are represented in QuestDB as `geohash(<precision>)`.\nPrecision is specified in the format `n{units}` where `n` is a numeric\nmultiplier and `units` may be either `c` for char or `b` for bits (`c` being\nshorthand for 5 x `b`).\nThe following example shows basic usage of geohashes by creating a column of 5\n`char` precision, 29 bits of precision, and inserting geohash values into these\ncolumns:\n`questdb-sql\nCREATE TABLE geo_data (g5c geohash(5c), g29b geohash(29b));\nINSERT INTO geo_data VALUES(#u33d8, ##10101111100101111111101101101)\n-- Querying by geohash\nSELECT * FROM geo_data WHERE g5c = #u33d8;`\nIt's not possible to store variable size geohashes within a column, therefore\nthe size and precision of a geohash must be known beforehand. Shorter-precision\ngeohashes cannot be inserted into longer-precision columns as all bits are\nsignificant. Details on the size of geohashes is described in the\ngeohash precision section below. Additionally,\n`NULL` is supported as a separate value for geohash columns of all precision.\nGeohash literals\nGeohashes have a literal syntax which starts with the hash `#` symbol followed\nby up to 12 chars, i.e.:\n`questdb-sql\nINSERT INTO my_geo_data VALUES(#u33, #u33d8b12)`\nGeohash literals with a single hash (`#`) may include a suffix in the format\n`/{bits}` where `bits` is the number of bits from 1-60 to allow for further\ngranularity of the geohash size. This is useful if a specific precision is\ndesired on the column size, but the values being inserted are using a char\nnotation:\n`questdb-sql\n-- insert a 5-bit geohash into a 4 bit column\nINSERT INTO my_geo_data VALUES(#a/4)\n-- insert a 20-bit geohash into an 18 bit column\nINSERT INTO my_geo_data VALUES(#u33d/18)`\nThe binary equivalent of geohashes may be expressed with two hash symbols (`##`)\nfollowed by up to 60 bits:\n`questdb-sql\nINSERT INTO my_geo_data VALUES(##0111001001001001000111000110)`\nImplicit casts from strings to literal geohashes is possible, but less efficient\nas string conversion to geohash must be performed:\n`questdb-sql\nINSERT INTO my_geo_data VALUES(#u33, #u33d8b12)\n-- equivalent to\nINSERT INTO my_geo_data VALUES('u33', 'u33d8b12')`\n`NULL` values reserve 1 bit which means 8-bit geohashes are stored in 9-bits as\n`short`s internally.\nSpecifying geohash precision\nThe size of the `geohash` type may be:\n\n1 to 12 chars or\n1 to 60 bits\n\nThe following table shows all options for geohash precision using `char`s and\nthe calculated area of the grid the geohash refers to:\n| Type           | Example         | Area              |\n| -------------- | --------------- | ----------------- |\n| `geohash(1c)`  | `#u`            | 5,000km \u00d7 5,000km |\n| `geohash(2c)`  | `#u3`           | 1,250km \u00d7 625km   |\n| `geohash(3c)`  | `#u33`          | 156km \u00d7 156km     |\n| `geohash(4c)`  | `#u33d`         | 39.1km \u00d7 19.5km   |\n| `geohash(5c)`  | `#u33d8`        | 4.89km \u00d7 4.89km   |\n| `geohash(6c)`  | `#u33d8b`       | 1.22km \u00d7 0.61km   |\n| `geohash(7c)`  | `#u33d8b1`      | 153m \u00d7 153m       |\n| `geohash(8c)`  | `#u33d8b12`     | 38.2m \u00d7 19.1m     |\n| `geohash(9c)`  | `#u33d8b121`    | 4.77m \u00d7 4.77m     |\n| `geohash(10c)` | `#u33d8b1212`   | 1.19m \u00d7 0.596m    |\n| `geohash(11c)` | `#u33d8b12123`  | 149mm \u00d7 149mm     |\n| `geohash(12c)` | `#u33d8b121234` | 37.2mm \u00d7 18.6mm   |\nFor geohashes with size determined by `b` for bits, the following table compares\nthe precision of some geohashes with units expressed in bits compared to chars:\n| Type (char)    | Equivalent to  |\n| -------------- | -------------- |\n| `geohash(1c)`  | `geohash(5b)`  |\n| `geohash(6c)`  | `geohash(30b)` |\n| `geohash(12c)` | `geohash(60b)` |\nCasting geohashes\nExplicit casts are not necessary, but given certain constraints, it may be\nrequired to cast from strings to geohashes. Empty strings are cast as `null` for\ngeohash values which are stored in the column with all bits set:\n`questdb-sql\nINSERT INTO my_geo_data VALUES(cast({my_string} as geohash(8c))`\nIt may be desirable to cast as geohashes in the circumstance where a table with\na desired schema should be created such as the following query. Note that the\nuse of `WHERE 1 != 1` means that no rows are inserted, only the table schema is\nprepared:\n`questdb-sql\nCREATE TABLE new_table AS\n(SELECT cast(null AS geohash(4c)) gh4c)\nFROM source_table WHERE 1 != 1`\nGeohash types can be cast from higher to lower precision, but not from lower to\nhigher precision:\n`questdb-sql\n-- The following cast is valid:\nCAST(#123 as geohash(1c))\n-- Invalid (low-to-high precision):\nCAST(#123 as geohash(4c))`\nSQL examples\nThe following queries create a table with two `geohash` type columns of varying\nprecision and insert geohashes as string values:\n`questdb-sql\nCREATE TABLE my_geo_data (g1c geohash(1c), g8c geohash(8c));\nINSERT INTO my_geo_data values(#u, #u33d8b12);`\nLarger-precision geohashes are truncated when inserted into smaller-precision\ncolumns, and inserting smaller-precision geohases into larger-precision columns\nproduces an error, i.e.:\n`questdb-sql\n-- SQL will execute successfully with 'u33d8b12' truncated to 'u'\nINSERT INTO my_geo_data values(#u33d8b12, #eet531sq);\n-- ERROR as '#e' is too short to insert into 8c_geohash column\nINSERT INTO my_geo_data values(#u, #e);`\nPerforming geospatial queries is done by checking if geohash values are equal to\nor within other geohashes. Consider the following table:\n`questdb-sql\nCREATE TABLE geo_data\n  (ts timestamp,\n  device_id symbol,\n  g1c geohash(1c),\n  g8c geohash(8c)),\nindex(device_id) timestamp(ts);`\n:::info\nThe `within` operator may only be used when all symbol columns in the query are\nindexed.\n:::\nThis creates a table with a `symbol` type column as an identifier and we can\ninsert values as follows:\n`questdb-sql\nINSERT INTO geo_data values(now(), 'device_1', #u, #u33d8b12);\nINSERT INTO geo_data values(now(), 'device_1', #u, #u33d8b18);\nINSERT INTO geo_data values(now(), 'device_2', #e, #ezzn5kxb);\nINSERT INTO geo_data values(now(), 'device_1', #u, #u33d8b1b);\nINSERT INTO geo_data values(now(), 'device_2', #e, #ezzn5kxc);\nINSERT INTO geo_data values(now(), 'device_3', #e, #u33dr01d);`\nThis table contains the following values:\n| ts                          | device_id | g1c | g8c      |\n| --------------------------- | --------- | --- | -------- |\n| 2021-09-02T14:20:04.669312Z | device_1  | u   | u33d8b12 |\n| 2021-09-02T14:20:06.553721Z | device_1  | u   | u33d8b12 |\n| 2021-09-02T14:20:07.095639Z | device_1  | u   | u33d8b18 |\n| 2021-09-02T14:20:07.721444Z | device_2  | e   | ezzn5kxb |\n| 2021-09-02T14:20:08.241489Z | device_1  | u   | u33d8b1b |\n| 2021-09-02T14:20:08.807707Z | device_2  | e   | ezzn5kxc |\n| 2021-09-02T14:20:09.280980Z | device_3  | e   | u33dr01d |\nWe can check if the last-known location of a device is a specific geohash with\nthe following query which will return an exact match based on geohash:\n`questdb-sql\nSELECT * FROM geo_data WHERE g8c = #u33dr01d LATEST ON ts PARTITION BY device_id`\n| ts                          | device_id | g1c | g8c      |\n| --------------------------- | --------- | --- | -------- |\n| 2021-09-02T14:20:09.280980Z | device_3  | e   | u33dr01d |\nFirst and last functions\nThe use of `first()` and `last()` functions within geospatial queries has been\nsignificantly optimized so that common types of queries relating to location are\nimproved. This means that queries such as \"last-known location\" by indexed\ncolumn for a given time range or sample bucket is specifically optimized for\nquery speed over large datasets:\n`questdb-sql\nSELECT ts, last(g8c) FROM geo_data WHERE device_id = 'device_3';\n-- first and last locations sample by 1 hour:\nSELECT ts, last(g8c), first(g8c) FROM geo_data\nWHERE device_id = 'device_3' sample by 1h;`\nWithin operator\nThe `within` operator can be used as a prefix match to evaluate if a geohash is\nequal to or is within a larger grid. The following query will return the most\nrecent entries by device ID if the `g8c` column contains a geohash within\n`u33d`:\n`questdb-sql title=\"LATEST BY usage\"\nSELECT * FROM geo_data\nWHERE g8c within(#u33d)\nLATEST ON ts PARTITION BY device_id;`\n:::info\nThe `within` operator can only be applied in `LATEST BY` queries and all symbol\ncolumns within the query must be indexed.\n:::\n| ts                          | device_id | g1c | g8c      |\n| --------------------------- | --------- | --- | -------- |\n| 2021-09-02T14:20:08.241489Z | device_1  | u   | u33d8b1b |\n| 2021-09-02T14:20:09.280980Z | device_3  | e   | u33dr01d |\nFor more information on the use of this operator, see the\nspatial operators reference.\nJava embedded usage\nGeohashes are inserted into tables via Java (embedded) QuestDB instance through\nthe selected `Writer`'s `putGeoHash` method. The `putGeoHash` method accepts\n`LONG` values natively with the destination precision. Additionally,\n`GeoHashes.fromString` may be used for string conversion, but comes with some\nperformance overhead as opposed to `long` values directly.\nDepending on whether the table is a WAL table\nor not, the following components may be used:\n\n`TableWriter` is used to write data directly into a table.\n`WalWriter` is used to write data into a WAL-enabled table via WAL.\n`TableWriterAPI` is used for both WAL and non-WAL tables, as it requests the\n  suitable `Writer` based on the table metadata.\n\n```java title=\"TableWriter\"\n// Insert data into a non-WAL table:\nfinal TableToken tableToken = engine.getTableToken(\"geohash_table\");\ntry (TableWriter writer = engine.getTableWriter(ctx.getCairoSecurityContext(), tableToken, \"test\")) {\n  for (int i = 0; i < 10; i++) {\n      TableWriter.Row row = writer.newRow();\n      row.putSym(0, \"my_device\");\n      // putGeoStr(columnIndex, hash)\n      row.putGeoStr(1, \"u33d8b1b\");\n      // putGeoHashDeg(columnIndex, latitude, longitude)\n      row.putGeoHashDeg(2, 48.669, -4.329)\n      row.append();\n  }\n  writer.commit();\n}\n```\n```java title=\"WalWriter\"\n// Insert data into a WAL table:\nfinal TableToken tableToken = engine.getTableToken(\"geohash_table\");\ntry (WalWriter writer = engine.getWalWriter(ctx.getCairoSecurityContext(), tableToken)) {\n    for (int i = 0; i < 10; i++) {\n        TableWriter.Row row = writer.newRow();\n        row.putSym(0, \"my_device\");\n        // putGeoStr(columnIndex, hash)\n        row.putGeoStr(1, \"u33d8b1b\");\n        // putGeoHashDeg(columnIndex, latitude, longitude)\n        row.putGeoHashDeg(2, 48.669, -4.329)\n        row.append();\n    }\n    writer.commit();\n\n\n```// Apply WAL to the table\ntry (ApplyWal2TableJob walApplyJob = new ApplyWal2TableJob(engine, 1, 1)) {\n    while (walApplyJob.run(0));\n}\n```\n\n\n}\n```\n```java title=\"TableWriterAPI\"\n// Insert table into either a WAL or a non-WAL table:\nfinal TableToken tableToken = engine.getTableToken(\"geohash_table\");\ntry (TableWriterAPI writer = engine.getTableWriterAPI(ctx.getCairoSecurityContext(), tableToken, \"test\")) {\n    for (int i = 0; i < 10; i++) {\n        TableWriter.Row row = writer.newRow();\n        row.putSym(0, \"my_device\");\n        // putGeoStr(columnIndex, hash)\n        row.putGeoStr(1, \"u33d8b1b\");\n        // putGeoHashDeg(columnIndex, latitude, longitude)\n        row.putGeoHashDeg(2, 48.669, -4.329)\n        row.append();\n    }\n    writer.commit();\n\n\n```// Apply WAL to the table\ntry (ApplyWal2TableJob walApplyJob = new ApplyWal2TableJob(engine, 1, 1)) {\n    while (walApplyJob.run(0));\n}\n```\n\n\n}\n```\nReading geohashes via Java is done by means of the following methods:\n\n`Record.getGeoByte(columnIndex)`\n`Record.getGeoShort(columnIndex)`\n`Record.getGeoInt(columnIndex)`\n`Record.getGeoLong(columnIndex)`\n\nTherefore it's necessary to know the type of the column beforehand through\ncolumn metadata by index:\n`java\nColumnType.tagOf(TableWriter.getMetadata().getColumnType(columnIndex));`\nInvoking the method above will return one of the following:\n\n`ColumnType.GEOBYTE`\n`ColumnType.GEOSHORT`\n`ColumnType.GEOINT`\n`ColumnType.GEOLONG`\n\nFor more information and detailed examples of using table readers and writers,\nsee the Java API documentation.\nInfluxDB Line Protocol\nGeohashes may also be inserted via InfluxDB Line Protocol (ILP) by the following\nsteps:\n\nCreate a table with columns of geohash type beforehand:\n\n`questdb-sql\nCREATE TABLE tracking (ts timestamp, geohash geohash(8c));`\n\nInsert via ILP using the `geohash` field:\n\n`bash\ntracking geohash=\"46swgj10\"`\n:::info\nThe ILP parser does not support geohash literals, only strings. This means that\ntable columns of type `geohash` type with the desired precision must exist\nbefore inserting rows with this protocol.\nIf a value cannot be converted or is omitted it will be set as `NULL`\n:::\nInserting geohashes with larger precision than the column it is being inserted\ninto will result in the value being truncated, for instance, given a column with\n`8c` precision:\n```bash\nInserting the following line\ngeo_data geohash=\"46swgj10r88k\"\nEquivalent to truncating to this value:\ngeo_data geohash=\"46swgj10\"\n```\nCSV import\nGeohashes may also be inserted via\nREST API. In order to perform inserts in\nthis way;\n\nCreate a table with columns of geohash type beforehand:\n\n`questdb-sql\nCREATE TABLE tracking (ts timestamp, geohash geohash(8c));`\nNote that you may skip this step, if you specify column types in the `schema`\nJSON object.\n\nImport the CSV file via REST API using the `geohash` field:\n\n`bash\ncurl -F data=@tracking.csv 'http://localhost:9000/imp?name=tracking'`\nThe `tracking.csv` file's contents may look like the following:\n`csv\nts,geohash\n17/01/2022 01:02:21,46swgj10`\nJust like ILP, CSV import supports geohash strings only, so the same\nrestrictions apply.\nPostgres\nGeohashes may also be used over Postgres wire protocol as other data types. The\nPython example below demonstrates how to connect to QuestDB over postgres wire,\ninsert and query geohashes:\n:::info\nWhen querying geohash values over Postgres wire protocol, QuestDB always returns\ngeohashes in text mode (i.e. as strings) as opposed to binary\n:::\n```python\nimport psycopg2 as pg\nimport datetime as dt\ntry:\n    connection = pg.connect(user=\"admin\",\n                            password=\"quest\",\n                            host=\"127.0.0.1\",\n                            port=\"8812\",\n                            database=\"qdb\")\n    cursor = connection.cursor()\n\n\n```cursor.execute(\"\"\"CREATE TABLE IF NOT EXISTS geo_data\n  (ts timestamp, device_id symbol index, g1c geohash(1c), g8c geohash(8c))\n  timestamp(ts);\"\"\")\n\ncursor.execute(\"INSERT INTO geo_data values(now(), 'device_1', 'u', 'u33d8b12');\")\ncursor.execute(\"INSERT INTO geo_data values(now(), 'device_1', 'u', 'u33d8b18');\")\ncursor.execute(\"INSERT INTO geo_data values(now(), 'device_2', 'e', 'ezzn5kxb');\")\ncursor.execute(\"INSERT INTO geo_data values(now(), 'device_3', 'e', 'u33dr01d');\")\n# commit records\nconnection.commit()\n\nprint(\"Data in geo_data table:\")\ncursor.execute(\"SELECT * FROM geo_data;\")\nrecords = cursor.fetchall()\nfor row in records:\n    print(row)\n\nprint(\"Records within 'u33d' geohash:\")\ncursor.execute(\"SELECT * FROM geo_data WHERE g8c within(#u33d) LATEST ON ts PARTITION BY device_id;\")\nrecords = cursor.fetchall()\nfor row in records:\n    print(row)\n```\n\n\nfinally:\n    if (connection):\n        cursor.close()\n        connection.close()\n        print(\"QuestDB connection closed\")",
    "tag": "questdb"
  },
  {
    "title": "The files are moved to the same folder.",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/guides/importing-data.md",
    "content": "\ntitle: CSV import via COPY SQL\nsidebar_label: Large CSV import (COPY SQL)\ndescription:\n  This document describes how to load large CSV data using COPY SQL keyword.\n\nThe COPY SQL command is the preferred way to import\nlarge CSV files into partitioned tables. It should be used to migrate data from\nanother database into QuestDB. This guide describes the method of migrating data\nto QuestDB via CSV files. For the time being this is the only way to migrate\ndata from other databases into QuestDB.\nThis guide is applicable for QuestDB version 6.5 and higher.\n:::caution\nFor partitioned tables, the best `COPY` performance can be achieved only on a\nmachine with a local, physically attached SSD. It is possible to use a network\nblock storage, such as an AWS EBS volume to perform the operation, with the\nfollowing impact:\n\nUsers need to configure the maximum IOPS and throughput setting values for the\n  volume.\nThe required import time is likely to be 5-10x longer.\n\n:::\nPrepare the import\nPreparation is key. Import is a multi-step process, which consists of:\n\nExport the existing database as CSV files\nEnable and configure `COPY` command to be optimal for the system\nPrepare target schema in QuestDB\n\nExport the existing database\nExport data using one CSV file per table. Make sure to export a column, which\ncan be used as timestamp. Data in CSV is not expected to be in any particular\norder. If it is not possible to export the table as one CSV, export multiple\nfiles and concatenate these files before importing into QuestDB.\nConcatenate multiple CSV files\nThe way to concatenate files depends on whether the CSV files have headers.\nFor CSV files without headers, concatenation is straightforward:\n\nimport Tabs from \"@theme/Tabs\"\nimport TabItem from \"@theme/TabItem\"\n\n\n\n`shell\nls *.csv | xargs cat > singleFile.csv`\n\n\n`shell\nls *.csv | xargs cat > singleFile.csv`\n\n\n```shell\n$TextFiles = Get-Item C:\\Users\\path\\to\\csv*.csv\nThe files are moved to the same folder.\n$TextFiles foreach { Add-Content -Value $(Get-Content $_) -Path C:\\Users\\path\\to\\csv\\singleFile.csv}\n```\n\n\nFor CSV files with headers, concatenation can be tricky. You could manually\nremove the first line of the files before concatenating, or use some smart\ncommand line to concatenate and remove the headers. A good alternative is using\nthe open source tool\ncsvstack.\nThis is how you can concatenate multiple CSV files using csvstack:\n`shell\ncsvstack *.csv > singleFile.csv`\nThings to know about `COPY`\n\n\n`COPY` is disabled by default, as a security precaution. Configuration is\n  required.\n\n\n`COPY` is more efficient when source and target disks are different.\n\n\n`COPY` is parallel when target table is partitioned.\n\n\n`COPY` is serial when target table is non-partitioned, out-of-order\n  timestamps will be rejected.\n\n\n`COPY` cannot import data into non-empty table.\n\n\n`COPY` indexes CSV file; reading indexed CSV file benefits hugely from disk\n  IOPS. We recommend using NVME.\n\n\n`COPY` imports one file at a time; there is no internal queuing system yet.\n\n\nCOPY reference\n\n\nConfigure `COPY`\n\nEnable `COPY` and configure `COPY`\n  directories to suit your server.\n`cairo.sql.copy.root` must be set for `COPY` to work.\n\nCreate the target table schema\nIf you know the target table schema already, you can\nskip this section.\nQuestDB could analyze the input file and \"guess\" the schema. This logic is\nactivated when target table does not exist.\nTo have QuestDB help with determining file schema, it is best to work with a\nsub-set of CSV. A smaller file allows us to iterate faster if iteration is\nrequired.\nLet's assume we have the following CSV:\n`csv \"weather.csv\"\n\"locationId\",\"timestamp\",\"windDir\",\"windSpeed\",\"windGust\",\"cloudCeiling\",\"skyCover\",\"visMiles\",\"tempF\",\"dewpF\",\"rain1H\",\"rain6H\",\"rain24H\",\"snowDepth\"\n1,\"2010-07-05T00:23:58.981263Z\",3050,442,512,,\"OBS\",11.774906006761,-5,-31,58.228032196984,70.471606345673,77.938252342637,58\n2,\"2017-10-10T10:13:55.246046Z\",900,63,428,5487,\"BKN\",4.958601701089,-19,-7,4.328016420894,36.020659549374,97.821114441800,41\n3,\"2010-03-12T11:17:13.727137Z\",2880,299,889,371,\"BKN\",10.342717709226,46,81,9.149518425127,20.229637391479,20.074738007931,80\n4,\"2018-08-21T15:42:23.107543Z\",930,457,695,4540,\"OBS\",13.359184086767,90,-47,33.346163208862,37.501996055160,58.316836760009,13\n...`\n\nExtract the first 1000 line to `test_file.csv` (assuming both files are in\n   the `cairo.sql.copy.root` directory):\n\n`shell\nhead -1000 weather.csv > test_file.csv`\n\nUse a simple `COPY` command to import `test_file.csv` and define the table\n   name:\n\n`questdb-sql\n   COPY weather from 'test_file.csv' WITH HEADER true;`\nTable `weather` is created and it quickly returns an id of asynchronous import\nprocess running in the background:\n| id               |\n| ---------------- |\n| 5179978a6d7a1772 |\n\n\nIn the Web Console right click table and select `Copy Schema to Clipboard` -\n   this copies the schema generated by the input file analysis.\n\n\nPaste the table schema to the code editor:\n\n\n`questdb-sql\n   CREATE TABLE 'weather' (\n     timestamp TIMESTAMP,\n     windDir INT,\n     windSpeed INT,\n     windGust INT,\n     cloudCeiling INT,\n     skyCover STRING,\n     visMiles DOUBLE,\n     tempF INT,\n     dewpF INT,\n     rain1H DOUBLE,\n     rain6H DOUBLE,\n     rain24H DOUBLE,\n     snowDepth INT\n   );`\n\nIdentify the correct schema:\n\n5.1. The generated schema may not be completely correct. Check the log table\n   and log file to resolve common errors using the id (see also\n   Track import progress\n   and FAQ):\n`questdb-sql\n   SELECT * FROM sys.text_import_log WHERE id = '5179978a6d7a1772' ORDER BY ts DESC;`\n| ts                          | id               | table   | file                       | phase | status   | message | rows_handled | rows_imported | errors |\n| --------------------------- | ---------------- | ------- | -------------------------- | ----- | -------- | ------- | ------------ | ------------- | ------ |\n| 2022-08-08T16:38:06.262706Z | 5179978a6d7a1772 | weather | test_file.csvtest_file.csv |       | finished |         | 999          | 999           | 0      |\n| 2022-08-08T16:38:06.226162Z | 5179978a6d7a1772 | weather | test_file.csvtest_file.csv |       | started  |         |              |               | 0      |\nCheck `rows_handled`, `rows_imported`, and `message` for any errors and amend\nthe schema as required.\n5.2. Drop the table and re-import `test_file.csv` using the updated schema.\n\nRepeat the steps to narrow down to a correct schema.\n\nThe process may require either truncating:\n`questdb-sql\n   TRUNCATE TABLE table_name;`\nor dropping the target table:\n`questdb-sql\n   DROP TABLE table_name;`\n\nClean up: Once all the errors are resolved, copy the final schema, drop the\n   small table.\nMake sure table is correctly partitioned. The final schema in our example\n   should look like this:\n\n`questdb-sql\n   CREATE TABLE 'weather' (\n     timestamp TIMESTAMP,\n     windDir INT,\n     windSpeed INT,\n     windGust INT,\n     cloudCeiling INT,\n     skyCover STRING,\n     visMiles DOUBLE,\n     tempF INT,\n     dewpF INT,\n     rain1H DOUBLE,\n     rain6H DOUBLE,\n     rain24H DOUBLE,\n     snowDepth INT\n   ) TIMESTAMP (timestamp) partition by DAY;`\n\nReady for import: Create an empty table using the final schema.\n\nImport CSV\nOnce an empty table is created in QuestDB using the correct schema, import can\nbe initiated with:\n`questdb-sql\nCOPY weather FROM 'weather.csv' WITH HEADER true TIMESTAMP 'timestamp' FORMAT 'yyyy-MM-ddTHH:mm:ss.SSSUUUZ';`\nIt quickly returns id of asynchronous import process running in the background:\n| id               |\n| :--------------- |\n| 55020329020b446a |\nTrack import progress\n`COPY` returns an id for querying the log table (`sys.text_import_log`), to\nmonitor the progress of ongoing import:\n`questdb-sql\nSELECT * FROM sys.text_import_log WHERE id = '55020329020b446a';`\n| ts                          | id               | table   | file        | phase                  | status   | message | rows_handled | rows_imported | errors |\n| :-------------------------- | ---------------- | ------- | ----------- | ---------------------- | -------- | ------- | ------------ | ------------- | ------ |\n| 2022-08-03T14:00:40.907224Z | 55020329020b446a | weather | weather.csv | null                   | started  | null    | null         | null          | 0      |\n| 2022-08-03T14:00:40.910709Z | 55020329020b446a | weather | weather.csv | analyze_file_structure | started  | null    | null         | null          | 0      |\n| 2022-08-03T14:00:42.370563Z | 55020329020b446a | weather | weather.csv | analyze_file_structure | finished | null    | null         | null          | 0      |\n| 2022-08-03T14:00:42.370793Z | 55020329020b446a | weather | weather.csv | boundary_check         | started  | null    | null         | null          | 0      |\nLooking at the log from the newest to the oldest might be more convenient:\n`questdb-sql\nSELECT * FROM sys.text_import_log WHERE id = '55020329020b446a' ORDER BY ts DESC;`\nOnce import successfully ends the log table should contain a row with a 'null'\nphase and 'finished' status :\n| ts                          | id               | table   | file        | phase | status   | message | rows_handled | rows_imported | errors |\n| :-------------------------- | ---------------- | ------- | ----------- | ----- | -------- | ------- | ------------ | ------------- | ------ |\n| 2022-08-03T14:10:59.198672Z | 55020329020b446a | weather | weather.csv | null  | finished |         | 300000000    | 300000000     | 0      |\nImport into non-partitioned tables uses single-threaded implementation (serial\nimport) that reports only start and finish records in the status table. Given an\nordered CSV file `weather1mil.csv`, when importing, the log table shows:\n| ts                          | id               | table   | file            | phase | status   | message | rows_handled | rows_imported | errors |\n| :-------------------------- | ---------------- | ------- | --------------- | ----- | -------- | ------- | ------------ | ------------- | ------ |\n| 2022-08-03T15:00:40.907224Z | 42d31603842f771a | weather | weather1mil.csv | null  | started  | null    | null         | null          | 0      |\n| 2022-08-03T15:01:20.000709Z | 42d31603842f771a | weather | weather1mil.csv | null  | finished | null    | 999999       | 999999        | 0      |\nThe log table contains only coarse-grained, top-level data. Import phase run\ntimes vary a lot (e.g. `partition_import` often takes 80% of the whole import\nexecution time), and therefore\nthe server log provides an alternative\nto follow more details of import:\n`log title=\"import log\"\n2022-08-03T14:00:40.907224Z I i.q.c.t.ParallelCsvFileImporter started [importId=5502031634e923b2, phase=analyze_file_structure, file=`C:\\dev\\tmp\\weather.csv`, workerCount=10]\n2022-08-03T14:00:40.917224Z I i.q.c.p.WriterPool >> [table=`weather`, thread=43]\n2022-08-03T14:00:41.440049Z I i.q.c.t.ParallelCsvFileImporter finished [importId=5502031634e923b2, phase=analyze_file_structure, file=`C:\\dev\\tmp\\weather.csv`, duration=0s, errors=0]\n2022-08-03T14:00:41.440196Z I i.q.c.t.ParallelCsvFileImporter started [importId=5502031634e923b2, phase=boundary_check, file=`C:\\dev\\tmp\\weather.csv`, workerCount=10]\n2022-08-03T14:01:18.853212Z I i.q.c.t.ParallelCsvFileImporter finished [importId=5502031634e923b2, phase=boundary_check, file=`C:\\dev\\tmp\\weather.csv`, duration=6s, errors=0]\n2022-08-03T14:01:18.853303Z I i.q.c.t.ParallelCsvFileImporter started [importId=5502031634e923b2, phase=indexing, file=`C:\\dev\\tmp\\weather.csv`, workerCount=10]\n2022-08-03T14:01:18.853516Z I i.q.c.t.ParallelCsvFileImporter temporary import directory [path='E:\\dev\\tmp\\weather\\]\n2022-08-03T14:01:42.612302Z I i.q.c.t.CsvFileIndexer finished chunk [chunkLo=23099021813, chunkHi=26948858785, lines=29999792, errors=0]\n2022-08-03T14:01:42.791789Z I i.q.c.t.CsvFileIndexer finished chunk [chunkLo=11549510915, chunkHi=15399347885, lines=30000011, errors=0]`\nIf the ON ERROR option is set to `ABORT`,\nimport stops on the first error and the error is logged. Otherwise, all errors\nare listed in the log.\nThe reference to the error varies depending on the phase of an import:\n\nIn the indexing phase, if an error occurs, the absolute input file line is\n  referenced:\n\n`log\n2022-08-08T11:50:24.319675Z E i.q.c.t.CsvFileIndexer could not parse timestamp [line=999986, column=1]`\n\nIn the data import phase, if an error occurs, the log references the offset as\n  related to the start of the file.\n\n`log\n2022-08-08T12:19:56.828792Z E i.q.c.t.TextImportTask type syntax [type=INT, offset=5823, column=0, value='CMP2']`\nThe errored rows can then be extracted for further investigation.\n\nFAQ\n\nWhat happens in a database crash or OS reboot?\n\n\n\nIf reboot/power loss happens while partitions are being attached, then table\nmight be left with incomplete data. Please truncate table before re-importing\nwith:\n\n```questdb-sql\nTRUNCATE TABLE table_name;\n```\n\nIf reboot/power loss happens before any partitions being attached, the import\nshould not be affected.\n\n\n\n\nI'm getting \"COPY is disabled ['cairo.sql.copy.root' is not set?]\" error message\n\n\n\nPlease set `cairo.sql.copy.root` setting, restart the instance and try again.\n\n\n\n\nI'm getting \"could not create temporary import work directory [path='somepath', errno=-1]\" error message\n\n\n\nPlease make sure that the `cairo.sql.copy.root` and `cairo.sql.copy.work.root`\nare valid paths pointing to existing directories.\n\n\n\n\nI'm getting \"[2] could not open read-only [file=somepath]\" error message\n\n\n\nPlease check that import file path is valid and accessible to QuestDB instance\nusers.\n\nIf you are running QuestDB using Docker, please check if the directory mounted\nfor storing source CSV files is identical to the one `cairo.sql.copy.root`\nproperty or `QDB_CAIRO_SQL_COPY_ROOT` environment variable points to.\n\nFor example, the following command can start a QuestDB instance:\n\n```shell\ndocker run -p 9000:9000 \\\n-v \"/tmp/questdb:/var/lib/questdb\" \\\n-v \"/tmp/questdb/my_input_root:/tmp/questdb_import\" \\\n-e QDB_CAIRO_SQL_COPY_ROOT=/tmp/questdb_wrong \\\nquestdb/questdb\n```\n\nHowever, running:\n\n```questdb-sql\nCOPY weather from 'weather_example.csv' WITH HEADER true;\n```\n\nResults in the \"[2] could not open read-only\n[file=/tmp/questdb_wrong/weather_example.csv]\" error message.\n\n\n\n\nI'm getting \"column count mismatch [textColumnCount=4, tableColumnCount=3, table=someTable]\" error message\n\n\n\nThere are more columns in input file than in the existing target table. Please\nremove column(s) from input file or add them to the target table schema.\n\n\n\n\nI'm getting \"timestamp column 'ts2' not found in file header\" error message\n\n\n\nEither input file is missing header or timestamp column name given in `COPY`\ncommand is invalid. Please add file header or fix timestamp option.\n\n\n\n\nI'm getting \"column is not a timestamp [no=0, name='ts']\" error message\n\n\n\nTimestamp column given by the user or (if header is missing) assumed based on\ntarget table schema is of a different type.  \nPlease check timestamp column name in input file header or make sure input file\ncolumn order matches that of target table.\n\n\n\n\nI'm getting \"target table must be empty [table=t]\" error message\n\n\n\n`COPY` doesn't yet support importing into partitioned table with existing data.\n\nPlease truncate table before re-importing with:\n\n```questdb-sql\nTRUNCATE TABLE table_name;\n```\n\nor import into another empty table and then use `INSERT INTO SELECT`:\n\n```questdb-sql\nINSERT INTO table_name batch 100000\nSELECT * FROM other_table;\n```\n\nto copy data into original target table.\n\n\n\n\nI'm getting \"io_uring error\" error message\n\n\n\nIt's possible that you've hit a IO_URING-related kernel error.  \nPlease set `cairo.iouring.enabled` setting to false, restart QuestDB instance,\nand try again.\n\n\n\n\nI'm getting \"name is reserved\" error message\n\n\n\nThe table you're trying import into is in bad state (metadata is incomplete).\n\nPlease either drop the table with:\n\n```questdb-sql\nDROP TABLE table_name;\n```\n\nand recreate the table or change the table name in the `COPY` command.\n\n\n\n\nI'm getting \"Unable to process the import request. Another import request may be in progress.\" error message\n\n\n\nOnly one import can be running at a time.\n\nEither cancel running import with:\n\n```questdb-sql\nCOPY 'paste_import_id_here' CANCEL;\n```\n\nor wait until the current import is finished.\n\n\n\n\nImport finished but table is (almost) empty\n\n\n\nPlease check the latest entries in log table:\n\n```questdb-sql\nSELECT * FROM sys.text_import_log LIMIT -10;\n```\n\nIf \"errors\" column is close to number of records in the input file then it may\nmean:\n\n- `FORMAT` option of `COPY` command or auto-detected format doesn't match\n  timestamp column data in file\n- Other column(s) can't be parsed and `ON ERROR SKIP_ROW` option was used\n- Input file is unordered and target table has designated timestamp but is not\n  partitioned\n\nIf none of the above causes the error, please check the log file for messages\nlike:\n\n```log\n2022-08-08T11:50:24.319675Z E i.q.c.t.CsvFileIndexer could not parse timestamp [line=999986, column=1]\n```\n\nor\n\n```log\n2022-08-08T12:19:56.828792Z E i.q.c.t.TextImportTask type syntax [type=INT, offset=5823, column=0, value='CMP2']\n```\n\nthat should explain why rows were rejected. Note that in these examples, the\nformer log message mentions the absolute input file line while the latter is\nreferencing the offset as related to the start of the file.\n\n\n\n\nImport finished but table column names are `f0`, `f1`, ...\n\n\n\nInput file misses header and target table does not exist, so columns received\nsynthetic names . You can rename them with the `ALTER TABLE` command:\n\n```questdb-sql\nALTER TABLE table_name RENAME COLUMN f0 TO ts;\n```\n\n\n",
    "tag": "questdb"
  },
  {
    "title": "correct order",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/guides/importing-data-rest.md",
    "content": "\ntitle: CSV import via REST API\nsidebar_label: Small CSV import (REST API)\ndescription:\n  This document describes how to load CSV data and specify text loader\n  configuration for timestamp and date parsing\n\nThe REST API provides an `/imp` endpoint exposed on port `9000` by default. This\nendpoint allows streaming tabular text data directly into a table, supporting\nCSV, TAB and pipe (`|`) delimited inputs with optional headers. Data types and\nstructures are detected automatically, but additional configurations can be\nprovided to improve automatic detection.\n:::note\nThe REST API is better suited when the following conditions are true:\n\nRegular uploads of small batches of data into the same table.\nThe file batches do not contain overlapping periods (they contain distinct\n  days/weeks/months). Otherwise, the import performance will be impacted.\n\nFor database migrations, or uploading one large CSV file into QuestDB, users may\nconsider using the `COPY` SQL command. See\nCOPY command documentation and\nGuide on CSV import for more details.\n:::\nImporting compressed files\nIt is possible to upload compressed files directly without decompression:\n`bash\ngzip -cd compressed_data.tsv.gz | curl -v -F data=@- 'http://localhost:9000/imp'`\nThe `data=@-` value instructs `curl` to read the file contents from `stdin`.\nSpecifying a schema during CSV import\nA `schema` JSON object can be provided with POST requests to `/imp` while\ncreating tables via CSV import. This allows for more control over user-defined\npatterns for timestamps, or for explicitly setting types during column-creation.\nThe following example demonstrates basic usage, in this case, that the\n`ticker_name` column should be parsed as `SYMBOL` type instead of `STRING`:\n`bash\ncurl \\\n  -F schema='[{\"name\":\"ticker_name\", \"type\": \"SYMBOL\"}]' \\\n  -F data=@trades.csv 'http://localhost:9000/imp'`\nIf a timestamp column (`ts`) in this CSV file has a custom or non-standard\ntimestamp format, this may be included with the call as follows:\n`bash\ncurl \\\n  -F schema='[ \\\n    {\"name\":\"ts\", \"type\": \"TIMESTAMP\", \"pattern\": \"yyyy-MM-dd - HH:mm:ss\"}, \\\n    {\"name\":\"ticker_name\", \"type\": \"SYMBOL\"} \\\n  ]' \\\n  -F data=@trades.csv 'http://localhost:9000/imp'`\nFor nanosecond-precision timestamps such as\n`2021-06-22T12:08:41.077338934Z`, a pattern can be provided in the following\nway:\n`bash\ncurl \\\n  -F schema='[ \\\n    {\"name\":\"ts\", \"type\": \"TIMESTAMP\", \"pattern\": \"yyyy-MM-ddTHH:mm:ss.SSSUUUNNNZ\"} \\\n  ]' \\\n  -F data=@my_file.csv \\\n  http://localhost:9000/imp`\nMore information on the patterns for timestamps can be found on the\ndate and time functions\npage.\n:::note\nThe `schema` object must precede the `data` object in calls to this REST\nendpoint. For example:\n```bash\ncorrect order\ncurl -F schema='{my_schema_obj}' -F data=@my_file.csv http://localhost:9000/imp\nincorrect order\ncurl -F data=@my_file.csv -F schema='{my_schema_obj}' http://localhost:9000/imp\n```\n:::\nText loader configuration\nQuestDB uses a `text_loader.json` configuration file which can be placed in the\nserver's `conf` directory. This file does not exist by default, but has the\nfollowing implicit settings:\n`json title=\"conf/text_loader.json\"\n{\n  \"date\": [\n    {\n      \"format\": \"dd/MM/y\"\n    },\n    {\n      \"format\": \"yyyy-MM-dd HH:mm:ss\"\n    },\n    {\n      \"format\": \"yyyy-MM-ddTHH:mm:ss.SSSz\",\n      \"locale\": \"en-US\",\n      \"utf8\": false\n    },\n    {\n      \"format\": \"MM/dd/y\"\n    }\n  ],\n  \"timestamp\": [\n    {\n      \"format\": \"yyyy-MM-ddTHH:mm:ss.SSSUUUz\",\n      \"utf8\": false\n    }\n  ]\n}`\nExample\nGiven a CSV file which contains timestamps in the format\n`yyyy-MM-dd - HH:mm:ss.SSSUUU`, the following text loader configuration will\nprovide the correct timestamp parsing:\n`json title=\"conf/text_loader.json\"\n{\n  \"date\": [\n    {\n      \"format\": \"dd/MM/y\"\n    },\n    {\n      \"format\": \"yyyy-MM-dd HH:mm:ss\"\n    },\n    {\n      \"format\": \"yyyy-MM-ddTHH:mm:ss.SSSz\",\n      \"locale\": \"en-US\",\n      \"utf8\": false\n    },\n    {\n      \"format\": \"MM/dd/y\"\n    }\n  ],\n  \"timestamp\": [\n    {\n      \"format\": \"yyyy-MM-ddTHH:mm:ss.SSSUUUz\",\n      \"utf8\": false\n    },\n    {\n      \"format\": \"yyyy-MM-dd - HH:mm:ss.SSSUUU\",\n      \"utf8\": false\n    }\n  ]\n}`\nThe CSV data can then be loaded via POST request, for example, using cURL:\n`curl\ncurl -F data=@weather.csv 'http://localhost:9000/imp'`\nFor more information on the `/imp` entry point, refer to the",
    "tag": "questdb"
  },
  {
    "title": "Timestamps in QuestDB",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/guides/working-with-timestamps-timezones.md",
    "content": "\ntitle: Timestamps and time zones\ndescription:\n  This document describes how to work with time zones in QuestDB, including\n  hints how to convert timestamps to UTC, to a specific time zone or by a UTC\n  offset value.\n\nWhen working with timestamped data, it may be necessary to convert timestamp\nvalues to or from UTC, or to offset timestamp values by a fixed duration. The\nfollowing sections describe how QuestDB handles timestamps natively, how to use\nbuilt-in functions for working with time zone conversions, and general hints for\nworking with time zones in QuestDB.\nTimestamps in QuestDB\nThe native timestamp format used by QuestDB is a Unix timestamp in microsecond\nresolution. Although timestamps in nanoseconds will be parsed, the output will\nbe truncated to microseconds. QuestDB does not store time zone information\nalongside timestamp values and therefore it should be assumed that all\ntimestamps are in UTC.\nThe following example shows how a Unix timestamp in microseconds may be passed\ninto a timestamp column directly:\n`questdb-sql\nCREATE TABLE my_table (ts timestamp, col1 int) timestamp(ts);\nINSERT INTO my_table VALUES(1623167145123456, 12);\nmy_table;`\n| ts                          | col1 |\n| :-------------------------- | :--- |\n| 2021-06-08T15:45:45.123456Z | 12   |\nTimestamps may also be inserted as strings in the following way:\n`questdb-sql\nINSERT INTO my_table VALUES('2021-06-08T16:45:45.123456Z', 13);\nmy_table;`\n| ts                          | col1 |\n| :-------------------------- | :--- |\n| 2021-06-08T15:45:45.123456Z | 12   |\n| 2021-06-08T16:45:45.123456Z | 13   |\nWhen inserting timestamps into a table, it is also possible to use\ntimestamp units\nto define the timestamp format, in order to process trailing zeros in exported\ndata sources such as PostgreSQL:\n```questdb-sql\nINSERT INTO my_table VALUES(to_timestamp('2021-06-09T16:45:46.123456789', 'yyyy-MM-ddTHH:mm:ss.N+'), 14);\n-- Passing 9-digit nanosecond into QuestDB, this is equal to:\nINSERT INTO my_table VALUES(to_timestamp('2021-06-10T16:45:46.123456789', 'yyyy-MM-ddTHH:mm:ss.SSSUUUN'), 14);\nmy_table;\n```\nThe output maintains microsecond resolution:\n| ts                          | col1 |\n| :-------------------------- | :--- |\n| 2021-06-08T15:45:45.123456Z | 12   |\n| 2021-06-08T16:45:45.123456Z | 13   |\n| 2021-06-09T16:45:46.123456Z | 14   |\nQuestDB's internal time zone database\nIn order to simplify working with time zones, QuestDB uses\nthe tz time zone database which is\nstandard in the Java ecosystem. This time zone database is used internally in\ntime zone lookup and in operations relating to timestamp value conversion to and\nfrom time zones.\nFor this reason, a time zone may be referenced by abbreviated name, by full time\nzone name or by UTC offset:\n| Abbreviation | Time zone name   | UTC offset |\n| :----------- | :--------------- | :--------- |\n| EST          | America/New_York | -05:00     |\nReferring to time zones\nIt's strongly advised not to use the three-letter ID or abbreviation for\ntime zones for the following reason:\n\nThe same abbreviation is often used for multiple time zones (for example,\n\"CST\" could be U.S. \"Central Standard Time\" and \"China Standard Time\"), and\nthe Java platform can then only recognize one of them\n\nTherefore, choosing a geographic region which observes a time zone\n(`\"America/New_York\"`, `\"Europe/Prague\"`) or a UTC offset value (`\"+02:00\"`) is\nmore reliable when referring to time zones. Instructions for converting to and\nfrom time zones are described in the\nConverting timestamps to and from time zones\nsection below.\nThe current QuestDB time zone database uses the English locale but support\nfor additional locales may be added in future. Referring to time zones which are\noutdated or not recognized results in a `invalid timezone name` error. The\nfollowing resources may be used for hints how to refer to time zones by ID or\noffset:\n\nThe official list maintained by IANA\nJava's\n  getAvailableZoneIds\n  method\nWiki entry on tz database time zones\n  (this is a convenient reference, but may not be 100% accurate)\n\n:::info\nUsers should be aware that the time zone database contains both current and\nhistoric transitions for various time zones. Therefore time zone conversions\nmust take the historic time zone transitions into account based on the timestamp\nvalues.\n:::\nUpdates to the time zone database\nThe upstream project updates past time zones as new information becomes\navailable. These changes are typically related to daylight saving time (DST)\nstart and end date transitions and, on rare occasions, time zone name changes.\nThe tz database version used by QuestDB is determined by the JDK version used at\nbuild time and therefore updates to the time zone database are directly\ninfluenced by this JDK version. To find the JDK version used by a QuestDB build,\nrun the following SQL:\n`questdb-sql\nSELECT build()`\n| build                                                                                              |\n| -------------------------------------------------------------------------------------------------- |\n| Build Information: QuestDB 6.0.3, JDK 11.0.7, Commit Hash a6afbadb9b9419d47cca1bf86fa13fdadf08bda4 |\nConverting timestamps to and from time zones\nFor convenience, QuestDB includes two functions for time zone conversions on\ntimestamp values.\n\nto_timezone()\nto_utc()\n\nThese functions are used to convert a Unix timestamp, or a string equivalent\ncast to timestamp as follows:\n`questdb-sql\nSELECT to_timezone(1623167145000000, 'Europe/Berlin')`\n| to_timezone                 |\n| :-------------------------- |\n| 2021-06-08T17:45:45.000000Z |\n`questdb-sql\nSELECT to_utc(1623167145000000, 'Europe/Berlin')`\n| to_utc                      |\n| :-------------------------- |\n| 2021-06-08T13:45:45.000000Z |\nUsing UTC offset for conversions\nThe to_timezone() and\nto_utc() functions may use UTC\noffset for converting timestamp values. In some cases, this can be more reliable\nthan string or time zone ID conversion given historic changes to time zone names\nor transitions. The following example takes a Unix timestamp in microseconds and\nconverts it to a time zone `+2` hours offset from UTC:\n`questdb-sql\nSELECT to_timezone(1213086329000000, '+02:00')`\n| to_timezone                 |\n| :-------------------------- |\n| 2008-06-10T10:25:29.000000Z |\n`questdb-sql\nSELECT to_utc('2008-06-10T10:25:29.000000Z', '+02:00')`\n| to_timezone                 |\n| :-------------------------- |",
    "tag": "questdb"
  },
  {
    "title": "Overview",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/guides/influxdb-migration.md",
    "content": "\ntitle: Migrating from InfluxDB\ndescription:\n  This document describes details about steps to migrate your database from\n  InfluxDB to QuestDB.\n\nThis page describes the steps to importing data from InfluxDB OSS or InfluxDB\nCloud to QuestDB.\nOverview\nData stored in InfluxDB needs to be exported in order to import it into QuestDB.\nThere are two ways of exporting data from InfluxDB:\n\nRun a SQL query using InfluxDB API and get results in JSON\nRun the `inspect` command and get results in ILP\n\nThe first approach is simpler and might suffice if you are migrating a small to\nmoderate dataset. For larger datasets it is advised to use the second option.\nSQL query for importing small tables\n:::note\nThis approach is recommended only for small to moderate datasets.\n:::\nWhen using InfluxDB API to run a SQL query, results will be in JSON. Since we\ncannot import JSON files directly into QuestDB, we will need to convert the\nresults into CSV. There are many ways to do so, and one of those is using the\njq JSON processor.\nTo run the SQL query you will need to have an\nAPI token.\nThe below is an example to query a table using the SQL API endpoint and convert\nthe results to CSV:\n`shell\ncurl --get http://localhost:8086/query --header \"Authorization: Token zuotzwwBxbXIor4_D-t-BMEpJj1nAj2DGqNSshTUyHUcX0DMjI6fiBv_pgeW-xxpnAwgEVG0uJAucEaJKtvpJA==\" --data-urlencode \"db=bench\" --data-urlencode \"q=SELECT * from readings LIMIT 1000;\" | jq '.results[].series[].values[] | @csv'`\nThe resulting CSV can be then\nimported into QuestDB.\nThe inspect command and ILP for importing datasets at scale\nTo move data from InfluxDB into QuestDB at scale, it is best to use the\n`influxd inspect` command to export the data, as the\nexport-lp\nsubcommand allows exporting all time-structured merge tree (TSM) data in a\nbucket as ILP messages in a big text file.\nThe text file can then be inserted into QuestDB. This assumes you are migrating\nfrom self-managed InfluxDB and have access to execute the `inspect` command.\nFor InfluxDB Cloud users, the first step should be\nexporting the data from cloud to InfluxDB OSS\nbefore following the instructions.\nInstructions\nGenerate admin token\nMake sure you have an admin token generated and set the env variable\nexport\u00a0`INFLUX_TOKEN`.\nFor example:\n`shell\nexport INFLUX_TOKEN=xyoczzmBxbXIor4_D-t-BMEpJj1nAj2DGqNSshTUyHUcX0DMjI6fiBv_pgeW-xxpnAwgEVG0uJAucEaJKtvpJA==`\nFind out your org_id and bucket_id\nYou need to know your org_id and bucket_id you will export. If you don\u2019t know\nthem you can\nissue\u00a0`influx org list`\u00a0and\u00a0`influx bucket list --org-id YOUR_ID`\u00a0to find those\nvalues.\nExport the bucket contents\nNow you can just export the bucket contents by using `inspect export-lp` command\nand by defining a destination folder:\n`shell\ninfluxd inspect export-lp --bucket-id YOUR_BUCKET_ID --output-path /var/tmp/influx/outputfolder`\nPlease note the step above can take a while. As an example, it took almost an\nhour for a 160G bucket on a mid-AWS EC2 instance.\nConnect to QuestDB\nConnect to your QuestDB instance and issue a\nCREATE TABLE statement. This is not\ntechnically needed as once you start streaming data, your table will be\nautomatically created. However, this step is recommended because this allows\nfine tuning some parameters such as column types or partition.\nSince the data is already in ILP format, there is no need to use the official\nQuestDB client libraries for ingestion.\nYou only need to connect via a socket to your instance and stream row by row.\nThe below is an example Python code streaming the instance:\n```python\nimport socket\nimport sys\nsock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\ndef send_utf8(msg):\n    print(msg)\n    sock.sendall(msg.encode())\nif name == 'main':\n    try:\n        sock.connect(('localhost', 9009))\n        with open(\"ilp_influx_export.ilp\") as infile:\n            for line in infile:\n                print(line)\n                send_utf8(line)\n    except socket.error as e:\n        sys.stderr.write(f'Got error: {e}')\n    sock.close()\n```\nTransform data in QuestDB\nSince InfluxDB exports only one metric for each ILP line, this means that if you\nare storing more than one metric for the same series, one row will create\nmultiple ILP lines with one valid metric value and the other metrics shown as\nNULL. Therefore, we recommend transforming your data in QuestDB.\nFor example, if you query a table with several metrics:\n`questdb-sql\nSELECT * FROM diagnostics WHERE timestamp = '2016-01-01T00:00:00.000000Z' AND driver='Andy' AND name='truck_150')`\nYour result may be something like this:\n\nA way to solve this is to execute a SQL query grouping data by all the\ndimensions and selecting the maximum values for all the metrics:\n`questdb-sql\nSELECT\n  timestamp,\n  device_version,\n  driver,\n  fleet,\n  model,\n  name,\n  max(current_load) AS current_load,\n  max(fuel_capacity) AS fuel_capacity,\n  max(fuel_state) AS fuel_state,\n  max(load_capacity) AS load_capacity,\n  max(nominal_fuel_consumption) AS nominal_fuel_consumption,\n  max(status) AS status\nFROM\n  diagnostics;`\nThis produces aggregated rows containing all the metrics for each dimension\ngroup:\n\nYou can use the INSERT keyword to output the\nprocessed result into a new table.\nSee also\n\nComparing InfluxDB, TimescaleDB, and QuestDB time series databases\n",
    "tag": "questdb"
  },
  {
    "title": "The commit lag value is set in milliseconds.",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/guides/out-of-order-commit-lag.md",
    "content": "\ntitle: Optimizing ingestion with commit lag configuration\nsidebar_label: Out-of-order commit lag\ndescription:\n  This document describes server configuration parameters for out-of-order\n  commit-lag along with details when and why these settings should be applied\nimage: /img/guides/out-of-order-commit-lag/o3-data.jpeg\n\n:::note\nDeprecated content\nThis page applies to QuestDB 6.5.5 and earlier versions. From\nQuestDB 6.6 onwards, the\ndatabase adjusts relevant settings automatically and provides maximum ingestion\nspeed.\n:::\nThe QuestDB commit lag configuration provides options to optimize data ingestion\nefficiency, when:\n\ningesting data over InfluxDB Line Protocol (ILP), and\nreceiving out-of-order (O3) data.\n\nAs of software version 6.0, QuestDB adds support for O3 data ingestion. The skew\nand latency of out-of-order data are likely to be relatively constant, so users\nmay configure ingestion based on the characteristics of the data for optimum\nthroughput.\nThis page explains the concept of commit lag and\nthe way to configure it.\nWhat is a commit lag?\nMost real-time O3 data patterns are caused by the delivery mechanism and\nhardware jitter, and therefore the timestamp distribution tends to be contained\nby some boundary.\nimport Screenshot from \"@theme/Screenshot\"\n\nIf any new timestamp value has a high probability to arrive within 10 seconds of\nthe previously received value, the boundary for this data is `10 seconds` and we\nname this commit lag or just lag.\nData received within the commit lag value is kept in memory only, and invisible\nfrom queries. Our out-of-order algorithm detects and prioritizes the data using\nan optimized processing path to commit later. Once committed, the data is\nvisible for queries.\nBenefits of commit lag configuration\nQuestDB stores all table data physically sorted by designated timestamp. When\nall data is received in a chronological order, this operation is\nstraight-forward. If QuestDB ingests a row with a designated timestamp earlier\nthan the latest timestamp already committed for one table, this row is\nout-of-order and QuestDB's engine needs to re-sort the existing data on disk.\nThe operation to ingest O3 data is therefore costly.\nFor optimal ingestion performance, the number of O3 data commits should be\nminimized. Configuring the commit lag based on data usage is therefore a way to\noptimize data ingestion, by accumulating data for a set period of time, sorting\nthe collected data in order, and committing it to memory. Although data may not\nbe immediately visible as a result of the commit lag setting, the overall data\ningestion efficiency can be improved.\nCommit lag and commit timing\nCommit lag is a user configurable value. On the server level configuration, the\nvalue is defined in milliseconds by `cairo.commit.lag`. Commit lag has an impact\non the timing of commit, as the value is combined with other parameters for\nILP commit strategy.\nThe `cairo.commit.lag` value is applied each time when a commit happens. As a\nresult, data older than the lag value will be committed and become visible.\nWhen to change the commit lag value\nThe commit lag value should be considered together with\n`cairo.max.uncommitted.rows` as part of the\nILP commit strategy.\nThe default configuration for the server is as follows:\n```ini title=\"Defaults\"\ncairo.commit.lag=300000\nThe commit lag value is set in milliseconds.\ncairo.max.uncommitted.rows=500000\n```\nUsers should modify these parameters based on any known or expected pattern for:\n\nThe length of time by which most records are late\nThe frequency of incoming records and the expected throughput\nThe freshness of the data used in queries\n\nTo minimize the number of O3 data commits, if the throughput is low and\ntimestamps are expected to be consistently delayed up to 30 seconds, the\nfollowing configuration settings can be applied:\n`ini title=\"server.conf\"\ncairo.commit.lag=30000\ncairo.max.uncommitted.rows=500`\nFor high-throughput scenarios, a lower commit lag value combined with a larger\nnumber of uncommitted rows may be more appropriate. The following settings would\nassume a throughput of ten thousand records per second with a likely maximum of\n1 second lateness for timestamp values:\n`ini title=\"server.conf\"\ncairo.commit.lag=1000\ncairo.max.uncommitted.rows=10000`\nHow to configure O3 ingestion\nQuestDB provides the following O3 data ingestion configuration options; users\ncan choose the most suitable configuration based on their specific case:\n\n\nServer-wide configuration:\n\n\n`cairo.commit.lag` in\n    Cairo engine\n\n\nTable configuration:\n\n\nSetting table parameters via SQL using\n    SET PARAM\n\nCreating table with parameters via SQL using\n    WITH\n\nSQL `INSERT AS SELECT` with batch size and lag:\n    Inserting query results\n\n\nImport configuration:\n\nOut-of-order CSV import\n\nOut-of-order CSV import\nIt is possible to set `commitLag` and `maxUncommittedRows` via REST API when\nimporting data via the `/imp` endpoint. The `commitLag` unit is microsecond. The\nfollowing example imports a file which contains out-of-order records. The\n`timestamp` and `partitionBy` parameters must be provided for commit lag and\nmax uncommitted rows to have any effect:\n```shell\ncurl -F data=@weather.csv \\\n'http://localhost:9000/imp?&timestamp=ts&partitionBy=DAY&commitLag=120000000&maxUncommittedRows=10000'\nThe commitLag value is set in microseconds.",
    "tag": "questdb"
  },
  {
    "title": "Upgrading QuestDB",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/guides/v6-migration.md",
    "content": "\ntitle: Version 6.0 migration\ndescription:\n  This document describes details about automatic upgrades with QuestDB version\n  6.0 and instructions for manually reverting tables for compatibility with\n  earlier QuestDB versions.\n\nRelease 6.0 introduces breaking changes in table transaction files. An automated\nconversion process has been included in the release which will migrate table\ntransaction files to use the new format. The following sections describe the\nautomated upgrade process with notes for manually downgrading tables for\ncompatibility with older versions.\nUpgrading QuestDB\nWhen QuestDB v6.0 starts up, and tables from older QuestDB versions are\ndetected, a migration to the new transaction file format will run automatically.\nThe migration scans for the existence of tables within the QuestDB storage\ndirectory and upgrades transaction (`_txn`) files for each table. All other\ntable data is untouched by the upgrade.\nIf the migration fails for a table, an error message will be printed in the\nQuestDB logs on startup. QuestDB will not terminate, but tables which have not\nbeen successfully upgraded cannot be used for querying or writing.\nStarting QuestDB again will trigger another attempt to migrate tables using an\nolder transaction file format.\nReverting transaction files\nDuring the upgrade process, `_txn` files are backed up and renamed using the\nformat `_txn.v417`. Users who wish to revert the table migration can downgrade\ntables by following these steps:\n\ndelete the folder `/path/to/questdb/db/_upgrade.d`\nfor each table, rename `_txn.v417` to `_txn`\n\nTable downgrade example\nThis section illustrates how to revert transaction files to a format used by\nQuestDB versions earlier than 6.0. Given storage directories for two table\n`example_table` and `sensors`:\n`bash title=\"path/to/qdb\"\n\u251c\u2500\u2500 conf\n\u251c\u2500\u2500 db\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 _tab_index.d\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 _upgrade.d\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 example_table\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 2021\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 tempF.d\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ...\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 visMiles.d\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 _meta\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 _txn\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 _txn.v417\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 sensors\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 2021\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 device_id.d\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 ...\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 temperature.d\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 _meta\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 _txn\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 _txn.v417\n\u2514\u2500\u2500 public`\nThe tables may be downgraded in the following manner:\n`bash\nrm db/_upgrade.d\nmv db/example_table/_txn.v417 db/example_table/_txn\nmv db/sensors/_txn.v417 db/sensors/_txn`\nAfter these steps have been completed, QuestDB v5.x may be started and the table\ndata will be loaded as usual.\nBreaking SQL changes\nRelease 6.0.1 contains breaking changes relating to SQL syntax to simplify\nworking with `TIMESTAMP` types and for improved compatibility with ANSI SQL\nexpectations.\n:::info\nFor more information on these changes, see the 6.0.1 software version\nrelease notes on GitHub.\n:::\nTo illustrate how timestamps are handled, a table `my_table` containing 48\nrecords with timestamps every hour beginning at `00:00:00` on `2020-01-01` will\nbe used in the following examples:\n|timestamp                  |\n|:--------------------------|\n|2020-01-01T00:00:00.000000Z|\n|2020-01-01T01:00:00.000000Z|\n|2020-01-01T02:00:00.000000Z|\n|...                        |\n|2020-01-01T23:00:00.000000Z|\n|2020-01-02T00:00:00.000000Z|\n|2020-01-02T01:00:00.000000Z|\n|...                        |\n|2020-01-02T23:00:00.000000Z|\nTimestamp string equality\nThe following example SQL uses a `WHERE` clause to evaluate if records match\nusing string equality.\n`questdb-sql title=\"Timestamp string equality\"\nSELECT * FROM my_table\nWHERE timestamp = '2020-01-01'`\nThe result will be 1 record with exact match of `2020-01-01T00:00:00.000000Z`.\nIn other words, the string `2020-01-01` does not represent an interval, but a\nsingle `TIMESTAMP` data point of `2020-01-01T00:00:00.000000Z`\n|timestamp                  |\n|:--------------------------|\n|2020-01-01T00:00:00.000000Z|\nBefore software version `6.0.1`, this would result in 24 records of all hours\nduring date '2020-01-01'\n|timestamp                  |\n|:--------------------------|\n|2020-01-01T00:00:00.000000Z|\n|2020-01-01T01:00:00.000000Z|\n|2020-01-01T02:00:00.000000Z|\n|...                        |\n|2020-01-01T23:00:00.000000Z|\nIn order to use the old semantics, the query must use the `IN` keyword instead\nof `=`:\n`questdb-sql title=\"Timestamp string equality using IN\"\nSELECT * FROM my_table\nWHERE timestamp IN '2020-01-01'`\nTimestamp string comparison\nTimestamps may also be compared using `>` greater-than and `<` less-than\noperators. The following example SQL uses a `>` greater-than operator to\nevaluate if records occur later than a timestamp provided as a string:\n`questdb-sql title=\"Timestamp string equality\"\nSELECT * FROM my_table\nWHERE timestamp > '2020-01-01'`\nThe results are 47 records which have timestamps strictly greater than\n`2020-01-01T00:00:00.000000Z`. The string `2020-01-01` does not represent an\ninterval, but a single `TIMESTAMP` data point of `2020-01-01T00:00:00.000000Z`:\n|timestamp                  |\n|:--------------------------|\n|2020-01-01T01:00:00.000000Z|\n|...                        |\n|2020-01-02T23:00:00.000000Z|\nBefore software version `6.0.1`, this would result in 24 records, one for each\nhour during the date `2020-01-02`:\n|timestamp                  |\n|:--------------------------|\n|2020-01-02T00:00:00.000000Z|\n|...                        |\n|2020-01-02T23:00:00.000000Z|\nIn order to use the old semantics, the query must use `>=` instead of `>`, and\n`<=` instead of `<`:\n`questdb-sql title=\"Greater than or equal to a string timestamp\"\nSELECT * FROM my_table\nWHERE timestamp >= '2020-01-02'`\nTimestamp IN list\nThe `IN` keyword is used to check equality with a list of 2 elements:\n`questdb-sql title=\"Timestamp IN string list\"\nSELECT * FROM my_table\nWHERE timestamp IN ('2020-01-01T00:00:00.000000Z', '2020-01-02T00:00:00.000000Z')`\nThe result is two records matching exactly `2020-01-01T00:00:00.000000Z` and\n`2020-01-02T00:00:00.000000Z`\n|timestamp                  |\n|:--------------------------|\n|2020-01-02T00:00:00.000000Z|\n|2020-01-02T00:00:00.000000Z|\nBefore software version `6.0.1`, this would result in 25 records, one for each\nhour during the date `2020-01-01` and the `00:00:00` data point on `2020-01-02`:\n|timestamp                  |\n|:--------------------------|\n|2020-01-02T00:00:00.000000Z|\n|...                        |\n|2020-01-02T00:00:00.000000Z|\nIn order to use the old semantics, the `BETWEEN` keyword should be used:\n```questdb-sql title=\"Timestamp string equality using BETWEEN\"\nSELECT * FROM my_table\nWHERE timestamp BETWEEN '2020-01-01T00:00:00.000000Z' AND '2020-01-02T00:00:00.000000Z'",
    "tag": "questdb"
  },
  {
    "title": "Modify data",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/guides/modifying-data.md",
    "content": "Modify data\nQuestDB is a timeseries database optimized to append data.\nFor best performance, design your application to avoid having to frequently\nedit existing records.\nThe UPDATE statement is available in QuestDB\nsince version 6.4, `DELETE` is also planned to be included in upcoming releases.\nHowever, they are intended for correcting data that was inserted incorrectly\nor should have never been inserted in the first place (for example as part\nof data administration tasks).\nThese are three alternatives to `UPDATE` and `DELETE` you may consider:\n\n\nAppend newest state: Insert a newer state to replace\n  an older one: This has the added advantage that you can query back\n  in time to a previous state. It is also the basis of organizing data for\n  bi-temporality.\n\n\nReplace a table: Create a new table with the new data you\n  need, drop the old one and rename.\n\n\nDelete by dropping partitions: Create your\n  timeseries tables with partitions, then delete the ones you no longer need.\n\n\nAppend newest state\nUsing the timestamp field\nHere's a worked example using the timestamp column:\n```questdb-sql\nCREATE TABLE takeaway_order (\n    ts TIMESTAMP,\n    id SYMBOL,\n    status SYMBOL)\n        timestamp(ts);\nINSERT INTO takeaway_order VALUES (now(), 'order1', 'placed');\nINSERT INTO takeaway_order VALUES (now(), 'order2', 'placed');\nINSERT INTO takeaway_order VALUES (now(), 'order1', 'cooking');\nINSERT INTO takeaway_order VALUES (now(), 'order1', 'in-transit');\nINSERT INTO takeaway_order VALUES (now(), 'order1', 'arrived');\nINSERT INTO takeaway_order VALUES (now(), 'order3', 'placed');\nINSERT INTO takeaway_order VALUES (now(), 'order3', 'cooking');\nINSERT INTO takeaway_order VALUES (now(), 'order3', 'in-transit');\n```\nWe join the latest timestamp of an order id against the rest of the data to\nobtain full details.\n`questdb-sql\nWITH\n    ts_takeaway_order AS (\n        SELECT\n            max(ts) AS ts,\n            id\n        FROM\n            takeaway_order GROUP BY id)\nSELECT\n    o.*\nFROM\n    ts_takeaway_order ts_o\n    INNER JOIN 'takeaway_order' o\n    ON ts_o.ts = o.ts`\nThis results in the latest state for each order:\n|timestamp ts             |id symbol|status symbol|\n|:--------------------------|:----------|:--------------|\n|2022-04-07T15:33:43.944922Z|order1     |arrived        |\n|2022-04-07T15:33:37.370694Z|order2     |placed         |\n|2022-04-07T15:33:50.829323Z|order3     |in-transit     |\nUsing dedicated fields\nIf timestamps don't work for you here, you can also use an extra integer column\ncalled `version`, an extra boolean `deleted` column or similar.\nReplace Table\nAnother alternative is to:\n* Select only the data you want from an existing table into a new temporary one.\n* Drop the original table.\n* Rename the temporary table to the original table's name.\n```questdb-sql\nCREATE TABLE mytable_copy AS (\n    SELECT * FROM mytable WHERE column_value != 42\n) TIMESTAMP(ts) PARTITION BY DAY;\nDROP TABLE mytable;\nRENAME table mytable_copy TO mytable;\n```\nDelete by Dropping Partitions\nWhen you create tables with a timestamp, you may organise them into\npartitions using the\nCREATE TABLE .. PARTITION BY\nSQL statement.\nYou may then use the ALTER TABLE DROP PARTITION",
    "tag": "questdb"
  },
  {
    "title": "Prerequisites",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/deployment/google-cloud-platform.md",
    "content": "\ntitle: Google Cloud Platform\ndescription:\n  This document describes how to deploy QuestDB on Google Cloud platform using a\n  Compute Engine VM with additional details on configuring networking rules\n\nThis guide describes how to run QuestDB on a Compute Engine instance on Google\nCloud platform with details on how to enable networking on various interfaces by\nmeans of firewall rules.\nThis guide uses the official QuestDB Docker image during VM instance creation to\nsimplify setup steps for a quick, robust deployment. The networking rules below\nshow how to make ports for PostgreSQL wire protocol and REST API publicly\naccessible or by whitelisted IP.\nPrerequisites\n\nA Google Cloud Platform\n  (GCP) account and a GCP Project\nThe\n  Compute Engine API\n  must be enabled for the corresponding Google Cloud Platform project\n\nCreate a Compute Engine VM\n\nIn the Google Cloud Console, navigate to\n   Compute Engine and\n   click Create Instance\n\nimport Screenshot from \"@theme/Screenshot\"\n\n\nGive the instance a name, this example uses `questdb-europe-west3`\nChoose a Region and Zone, this example uses\n   `europe-west3 (Frankfurt)` and the default zone\nChoose a machine configuration, a general-purpose instance is `e2-medium`\n   with 4GB memory\nEnable the checkbox under Container and provide the latest QuestDB Docker\n   image:\n\n`text\n   questdb/questdb:latest`\nGiven the steps so far, the VM Instance configuration page should look like the\nfollowing:\n\nBefore creating the instance, assign a Network tag so that a firewall rule\nfor networking can be easily applied to instances of the same type.\n\nExpand the menu item Management, security, disks, networking, sole\n   tenancy towards the bottom of the page\nIn the Networking panel add a Network tag to identify the instance,\n   this example uses `questdb`\nLaunch the instance by clicking Create\n\n\nCreate a firewall rule\n\nNavigate to the\n   Firewalls configuration\n   page under VPC network -> Firewalls\nAdd the target tag `questdb`\nChoose an IP range that this rule applies to, this example uses `0.0.0.0/0`\n   (i.e. any IP)\nIn the Protocols and ports section, enable `8812` and `9000` for TCP.\nClick create\n\n\nAll VM instances on Compute Engine within this account which have the Network\ntag `questdb` will have this firewall rule applied.\n:::info\nThe configuration above allows networking from any IP address for the selected\nports. A more secure approach would be to only allow incoming connections from\nwhitelisted IPs.\nThe ports we have opened are\n\n`9000` for the REST API and Web Console\n`8812` for PostgreSQL wire protocol\n\n:::\nVerify the deployment\nTo verify the instance state, navigate to Compute Engine ->\nVM Instances. A status\nindicator should show the instance as running:\n\nTo verify that the QuestDB deployment is operating as expected:\n\nCopy the External IP of the instance\nNavigate to `<external_ip>:9000` in a browser\n\nThe Web Console should be visible:\n\nAlternatively, a request may be sent against the REST API exposed on port 9000:\n```bash\ncurl -G \\\n  --data-urlencode \"query=SELECT * FROM telemetry_config\" \\\n  :9000/exec",
    "tag": "questdb"
  },
  {
    "title": "Prerequisites",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/deployment/kubernetes.md",
    "content": "\ntitle: Run QuestDB on Kubernetes\nsidebar_label: Kubernetes\ndescription:\n  This document describes how to deploy QuestDB using a Kubernetes cluster by\n  means of official Helm charts maintained by the QuestDB project\n\nYou can deploy QuestDB in a Kubernetes cluster using a\nStatefulSet\nand a\npersistent volume.\nWe distribute QuestDB via Helm on\nArtifactHub.\nPrerequisites\n\nHelm\nKubernetes CLI\nminikube\n\nGet the QuestDB Helm chart\nUsing the Helm client, add the official Helm chart repository:\n`shell\nhelm repo add questdb https://helm.questdb.io/`\nUpdate the Helm index:\n`shell\nhelm repo update`\nRun QuestDB\nStart a local cluster using `minikube`:\n`shell\nminikube start`\nThen install the chart:\n`shell\nhelm install my-questdb questdb/questdb`\nFinally, use the Kubernetes CLI to get the pod name:\n`shell\nkubectl get pods`\nResult:\n| NAME         | READY | STATUS  | RESTARTS | AGE   |\n| ------------ | ----- | ------- | -------- | ----- |\n| my-questdb-0 | 1/1   | Running | 1        | 9m59s |\nQuerying QuestDB locally\nIn order to run queries against your local instance of QuestDB, you can use port\nforwarding:\n`shell\nkubectl port-forward my-questdb-0 9000`\nThe following ports may also be used:\n\n9000: REST API and\n  Web Console\n8812: Postgres\n",
    "tag": "questdb"
  },
  {
    "title": "pg.password=...",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/deployment/aws-official-ami.md",
    "content": "\ntitle: Launch the official QuestDB AMI via the AWS Marketplace\nsidebar_label: AWS Marketplace AMI\ndescription:\n  This document describes how to launch the official AWS Marketplace AMI with\n  QuestDB installed and how to access and secure the instance on Amazon Web\n  Services\n\nAWS Marketplace is a digital catalog with software listings from independent\nsoftware vendors that runs on AWS. This guide describes how to launch QuestDB\nvia the AWS Marketplace using the official listing. This document also describes\nusage instructions after you have launched the instance, including hints for\nauthentication, the available interfaces, and tips for accessing the REST API\nand web console.\nPrerequisites\n\nAn Amazon Web Services account\n\nLaunching QuestDB on the AWS Marketplace\nThe QuestDB listing can be found in the AWS Marketplace under the databases\ncategory. To launch a QuestDB instance:\n\nNavigate to the\n   QuestDB listing\nClick Continue to Subscribe and subscribe to the offering\nConfigure a version, an AWS region and click Continue to Launch\nChoose an instance type and network configuration and click Launch\n\nAn information panel displays the ID of the QuestDB instance with launch\nconfiguration details and hints for locating the instance in the EC2 console.\nQuestDB configuration\nThe server configuration file is at the following location on the AMI:\n`bash\n/var/lib/questdb/conf/server.conf`\nFor details on the server properties and using this file, see the\nserver configuration documentation.\nThe default ports used by QuestDB interfaces are as follows:\n\nWeb console & REST API is available on port `9000`\nPostgreSQL wire protocol available on `8812`\nInfluxDB line protocol `9009` (TCP and UDP)\nHealth monitoring & Prometheus `/metrics` `9003`\n\nPostgres credentials\nGenerated credentials can found in the server configuration file:\n`bash\n/var/lib/questdb/conf/server.conf`\nThe default Postgres username is `admin` and a password is randomly generated\nduring startup:\n`ini\npg.user=admin\npg.password=...`\nInfluxDB line protocol credentials\nThe credentials for InfluxDB line protocol can be found at\n`bash\n/var/lib/questdb/conf/full_auth.json`\nFor details on authentication using this protocol, see the\nInfluxDB line protocol authentication guide.\nDisabling authentication\nIf you would like to disable authentication for Postgres wire protocol or\nInfluxDB line protocol, comment out the following lines in the server\nconfiguration file:\n```ini title=\"/var/lib/questdb/conf/server.conf\"\npg.password=...\nline.tcp.auth.db.path=conf/auth.txt\n```\nDisabling interfaces\nInterfaces may be disabled completely with the following configuration:\n```ini title=\"/var/lib/questdb/conf/server.conf\"\ndisable postgres\npg.enabled=false\ndisable InfluxDB line protocol over TCP and UDP\nline.tcp.enabled=false\nline.udp.enabled=false\ndisable HTTP (web console and REST API)\nhttp.enabled=false\n```\nThe HTTP interface may alternatively be set to readonly:\n```ini title=\"/var/lib/questdb/conf/server.conf\"\nset HTTP interface to readonly\nhttp.security.readonly=true",
    "tag": "questdb"
  },
  {
    "title": "pg.password=...",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/deployment/digitalocean.md",
    "content": "\ntitle: Launch QuestDB on DigitalOcean\nsidebar_label: DigitalOcean Droplet\ndescription:\n  This document describes how to launch DigitalOcean droplet with QuestDB\n\nDigitalOcean is a platform with software listings from independent vendors that\nrun on cloud resources. This guide describes how to launch QuestDB via the\nDigitalOcean marketplace using the official listing. This document also\ndescribes usage instructions after you have launched the instance, including\nhints for authentication, the available interfaces, and tips for accessing the\nREST API and web console.\nPrerequisites\nThe prerequisites for deploying QuestDB on DigitalOcean are as follows:\n\nA DigitalOcean account (sign up using\n  the QuestDB referral link for 100 USD free\n  credit)\nBasic `shell` knowledge for executing commands on the DigitalOcean droplet\n\nCreate a QuestDB Droplet\nDigitalOcean has a marketplace which offers 1-Click Apps reviewed by their\nstaff. QuestDB is available on the marketplace recently, so setup using this\nmethod is preferred:\n\nNavigate to the\n   QuestDB listing\n   on DigitalOcean\nClick Create QuestDB Droplet\nSelect the basic plan for your Droplet (4GB RAM is recommended)\n\nimport Screenshot from \"@theme/Screenshot\"\n\n\nChoose a region closest to you\nAt the Authentication section, enter your SSH public key, or set a\n   password\nSet a hostname for the droplet such as `questdb-demo`\nLeave all other settings with their defaults, and click Create Droplet at\n   the bottom of the page\n\n\nAfter 30 seconds, QuestDB should be ready to use. To validate that we set\neverything up successfully, copy the Droplet's IP address by clicking on it and\nnavigate to `http://<IP ADDRESS>:9000/` where `<IP ADDRESS>` is the IP address\nyou just copied. The interactive console should load and we can start querying\nthe database and inserting data.\nQuestDB droplet configuration\nThe server configuration file is at the following location on the droplet:\n`bash\n/home/questdb/server.conf`\nFor details on the server properties and using this file, see the\nserver configuration documentation.\nThe default ports used by QuestDB interfaces are as follows:\n\nWeb console & REST API is available on port `9000`\nPostgreSQL wire protocol available on `8812`\nInfluxDB line protocol `9009` (TCP and UDP)\nHealth monitoring & Prometheus `/metrics` `9003`\n\nQuestDB Credentials\nCredentials may be configured in the server configuration file:\n`bash\n/home/questdb/server.conf`\nThe default Postgres credentials should be changed:\n`ini\npg.user=...\npg.password=...`\nFor details on authentication using InfluxDB line protocol, see the\nInfluxDB line protocol authentication guide.\nDisabling authentication\nIf you would like to disable authentication for Postgres wire protocol or\nInfluxDB line protocol, comment out the following lines in the server\nconfiguration file:\n```ini title=\"/home/questdb/server.conf\"\npg.password=...\nline.tcp.auth.db.path=conf/auth.txt\n```\nDisabling interfaces\nInterfaces may be disabled completely with the following configuration:\n```ini title=\"/home/questdb/server.conf\"\ndisable postgres\npg.enabled=false\ndisable InfluxDB line protocol over TCP and UDP\nline.tcp.enabled=false\nline.udp.enabled=false\ndisable HTTP (web console and REST API)\nhttp.enabled=false\n```\nThe HTTP interface may alternatively be set to readonly:\n```ini title=\"/home/questdb/server.conf\"\nset HTTP interface to readonly\nhttp.security.readonly=true\n```\nAPI creation\nIn addition to creating a Droplet from the QuestDB 1-Click App via the control\npanel, you can also\nuse the DigitalOcean API.\nAs an example, to create a 4GB QuestDB Droplet in the SFO2 region, you can use\nthe following curl command. You\u2019ll need to either save your API access token to\nan environment variable or substitute it into the command below.\n```bash\ncurl -X POST -H 'Content-Type: application/json' \\\n     -H 'Authorization: Bearer '$TOKEN'' -d \\\n    '{\"name\":\"choose_a_name\",\"region\":\"sfo2\",\"size\":\"s-2vcpu-4gb\",\"image\":\"questdb-20-04\"}' \\\n    \"https://api.digitalocean.com/v2/droplets\"",
    "tag": "questdb"
  },
  {
    "title": "Supported elements / components",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/__guidelines/markdown.md",
    "content": "\ntitle: Markdown\nThis page describes elements and components from\nMDX that we use and elements that we avoid.\nSupported elements / components\nWe use the following elements.\n\nNo more than one block per page\n\nNote\n:::note\nThis is something important the user should be aware of. But no danger.\n:::\nTip\n:::tip\nCan be used to highlight nice tricks on a very occasional basis.\n:::\nCaution\n:::caution\nGives a warning about something dangerous.\n:::\nTable\n| table | table |\n| ----- | ----- |\n| value | value |\n| value | value |\n\"questdb-sql\" code block\nTitle is optional.\n`questdb-sql title=\"title\"\nSELECT * FROM users;`\nMulti language code block\nimport Tabs from \"@theme/Tabs\"\nimport TabItem from \"@theme/TabItem\"\n\n\n`questdb-sql\nSELECT * FROM users;`\n\n\n`shell\ncurl -G \"http://localhost:13005/exec\"`\n\n\n`java\nfinal CairoEngine engine = new CairoEngine();`\n\n\n\"Script\" code block\nUse the `script` language. The title is optional.\n`shell\nsome shell command`\nElements / components to avoid\nInfo\n:::info\nNot used because hard to differentiate from note\n:::\nWarning\n:::warning\nWarning\n:::\nQuote",
    "tag": "questdb"
  },
  {
    "title": "New page",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/__guidelines/naming-convention.md",
    "content": "\ntitle: Naming convention\nOur conventions improve consistency and ensure that we have optimal SEO.\nNew page\nFile name\n`file-name.md` (kebab-case, no uppercase here, the same rule applies to blog\nposts)\nTitle\n`A descriptive title` (No Pascal Case)\nMarkdown attributes\n\n`id`: please do NOT set the `id`, Docusaurus will automatically compute it\n  from the file path and name.\n`sidebar_label`: `Custom name`, do not set if it is the same as title,\n  Docusaurus will automatically fallback to it.\n\nImages\nImages should always be inserted in markdown, not HTML:\n`shell\n[Image description](path/to/img.jpg)`\nThe description is important, it will populate the `alt` attribute which\nimproves SEO.\nLinks\nLinks should always be inserted in markdown, not HTML:\n`shell\n[link text](https://path/to/resource.html)`\n:::caution\nPlease use meaningful wording for the link text, content such as \"here\" or\n\"click here\" is forbidden since it will negatively impact SEO.",
    "tag": "questdb"
  },
  {
    "title": "Content",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/__guidelines/content-hierarchy.md",
    "content": "\ntitle: Content hierarchy\nDocumentation should follow a hierarchy, this is true both for the content and\nhow titles are organized. In most cases, you can refer to a template and reuse\nthe hierarchy exposed there. When you write a page that does not derive from a\ntemplate, please follow the guidelines exposed here.\nContent\nWhen you need to show a command, please show it at the top of your page as much\nas possible. This will ensure that users can easily copy/paste code without\nhaving to scan the whole page.\nIt is okay to be very descriptive and thorough, however not every detail should\nhave the same weight. If you are documenting a function with many arguments,\nplease start with the most common ones first, gradually defining the ones that\npeople are less likely to use.\nTitles\nPages need to start with text, not a title. Titles should always follow the\nfollowing hierarchy:\n`shell\nh1 (title) > h2 (##) > h3 (###) > h4 (####)`\nThis will improve readability and SEO. Example:\n```markdown\nThe first title should be H2\nThen there should be some text\nThen further titles should be H3\nThen ideally some text. Subsequent titles can then be used\nFor example H4\nor even\nFor example H5\netc\n```\nBad practices\n\nRepetitive subtitles\n",
    "tag": "questdb"
  },
  {
    "title": "sql-code-blocks.md",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/__guidelines/sql-code-blocks.md",
    "content": "\ntitle: SQL code blocks\nChecklist\n\n[ ] Use the `questdb-sql` language\n[ ] Keywords are uppercase\n[ ] Types are uppercase\n[ ] Column names are camelCase\n[ ] Table names are camelCase\n[ ] Function names are lowercase\n[ ] Statements finish with `;`\n\nFormatting\n\nAlways write explicit `SELECT * FROM table` instead of `table` with the\n  exception of pages describing the shorthand expression.\n`timestamp` is not a valid column name. Neither are any type or function\n  names.\nChar returns are allowed either (1) after a `,` (2) after a `SQL keyword` (3)\n  after opening or closing a bracket `(`,`)`\n\nExamples\n`questdb-sql\nSELECT * FROM tableName;`\n`questdb-sql\nSELECT columnName, min(columnName) FROM tableName;`\n`questdb-sql\nCREATE TABLE tableName(columnName TYPE, columnName TYPE) timestamp(columnName) PARTITION BY DAY;`\n`questdb-sql\nSELECT cast(columnName AS INT) FROM tableName;`\n`questdb-sql\nSELECT columnName, min(columnName)\nFROM tableName\nWHERE columnName > 3;`\n`questdb-sql\nSELECT\ncolumnName,\nmin(columnName),\nmax(columnName)\nFROM table WHERE columnName > 3;`\n```questdb-sql\nCREATE TABLE tableName AS(\n...\n);",
    "tag": "questdb"
  },
  {
    "title": "function_name",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/__guidelines/template/function.md",
    "content": "\ntitle: FUNCTION\nsidebar_label: Function\n\nFunction reference pages list one or several (when closely related to each\nother) functions. For this reason, it the section should start with a short\nparagraph describing the class of functions described in this page and what they\nare used for. \nfunction_name\n\nfunction_name(`arg1`,`arg2`) - does this and that. \nArguments\n\n`arg1` is [data_type].\n`arg2` is is [data_type].\nReturn value\nReturn value type is [data_type].\nDescription\n`function_name` requires a longer explanation of the logic because it is a bit\ncomplex. In this case, it is good to describe it with some text. It should start\nby a plain-text explanation. Then branches should be expressed as bullet points.\nFor example a given `argument` might have various behaviors depending on value\n\n`true` makes it do this. It means that this happens and that might happen.\n`false` makes the function do that. So `function_name` may have this behavior.\n\nExamples\nBasic usage\n`questdb-sql title=\"Example description - Scalar result\"\nSELECT function_name(arg1,arg2) FROM table;`\n| function_name |\n| ------------- |\n| true          |\nWith optional arguments\n`questdb-sql title=\"Example description - Table result\"\nSELECT function_name(arg1,arg2,opt1) FROM table;`\n| a     | b   | function_name | function_name |\n| ----- | --- | ------------- | ------------- |\n| true  | 47  | true          | 47            |\n| false | 53  | false         | 53            |\nWith null\n`questdb-sql title=\"Example description - Series result\"\nSELECT function_name(arg1,arg2) FROM table;`\n| a     | b   | function_name | function_name |\n| ----- | --- | ------------- | ------------- |\n| true  | 47  | true          | 47            |\n| ...   | ... | ...           | ...           |\n| false | 53  | false         | 53            |\nfunction_name_2\n\nfunction_name(`arg1`,`arg2`) - does this and that. \nArguments\n\n`arg1` is [data_type].\n`arg2` is is [data_type].\nReturn value\nReturn value type is [data_type].\nDescription\n`function_name` requires a longer explanation of the logic because it is a bit\ncomplex. In this case, it is good to describe it with some text. It should start\nby a plain-text explanation. Then branches should be expressed as bullet points.\nFor example a given `argument` might have various behaviors depending on value\n\n`true` makes it do this. It means that this happens and that might happen.\n`false` makes the function do that. So `function_name` may have this behavior.\n\nExamples\nAt minimum, examples need a descriptive title. An optional description can be\nused to.\nBasic usage\n`questdb-sql title=\"Example description - Scalar result\"\nSELECT function_name(arg1,arg2) FROM table;`\n| function_name |\n| ------------- |\n| true          |\nWith optional arguments\n`questdb-sql title=\"Example description - Table result\"\nSELECT function_name(arg1,arg2) FROM table;`\n| a     | b   | function_name | function_name |\n| ----- | --- | ------------- | ------------- |\n| true  | 47  | true          | 47            |\n| false | 53  | false         | 53            |\nWith null\n`questdb-sql title=\"Example description - Series result\"\nSELECT function_name(arg1,arg2) FROM table;`\n| a     | b   | function_name | function_name |\n| ----- | --- | ------------- | ------------- |\n| true  | 47  | true          | 47            |\n| ...   | ... | ...           | ...           |",
    "tag": "questdb"
  },
  {
    "title": "Syntax",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/__guidelines/template/sql.md",
    "content": "\ntitle: KEYWORD\nsidebar_label: SQL\n\n`KEYWORD` does something interesting.\nSyntax\n\nSome arguments may need explanations.\n\n`this` does that and is useful\n`this` does that and is also useful\n\nExamples\nExamples are introduced by a title text and an optional description. Ideally\nthey are made clear enough so that neither text nor output example is required.\nThere should be many examples describing various situations.\nBasic usage\n`questdb-sql title=\"This example does that\"\nSELECT a FROM b;`\nAggregations\n`questdb-sql title=\"Do something interesting\"\nSELECT a FROM b;`\nJoining tables\n`questdb-sql title=\"Something else, equally interesting\"\nSELECT a FROM b;`\nWith dynamic designated timestamp\n```questdb-sql title=\"This is only interesting for certain users\"\nSELECT a FROM b;",
    "tag": "questdb"
  },
  {
    "title": "First step",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/__guidelines/template/guide.md",
    "content": "\ntitle: How to \"xyz\"\nsidebar_label: Guide\n\nHow To Guides should have a short description of what they intend to cover. If\nyour How To has a corresponding section for reference elsewhere in the docs,\nmake sure to include it here, or in a 'References' section. References\nFirst step\nBeginning steps. Include any code in code blocks. Make sure that the code is\nrunnable, and actually works. Each snippet should be executable as one entity -\nno several commands into one single snippet.\n`shell\n/bin/bash -f shell-code-here`\nSecond step\nBreak things up into sections to make it easy to follow.\n:::note\nUse Notes to call out important bits of information.\n:::\nMore links\ninclude links to other relevant documentation.",
    "tag": "questdb"
  },
  {
    "title": "Options",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/command-line-options.md",
    "content": "\ntitle: Command-line options\ndescription: Command-line options reference documentation.\n\nQuestDB may be started, stopped and passed configuration options from the\ncommand line. On Windows, the QuestDB server can also start an\ninteractive session.\nOptions\nThe following sections describe the options that may be passed to QuestDB when\nstarting the server from the command line.\n\nimport Tabs from \"@theme/Tabs\"\nimport TabItem from \"@theme/TabItem\"\n\n\n\n`shell\n./questdb.sh [start|stop|status] [-d dir] [-f] [-t tag]`\n\n\n`shell\nquestdb [start|stop|status] [-d dir] [-f] [-t tag]`\n\n\n`shell\nquestdb.exe [start|stop|status|install|remove] \\\n  [-d dir] [-f] [-j JAVA_HOME] [-t tag]`\n\n\nStart\n`start` - starts QuestDB as a service.\n| Option | Description                                                                                                                                                                                                          |\n| ------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `-d`   | Expects a `dir` directory value which is a folder that will be used as QuestDB's root directory. For more information and the default values, see the default root section below.         |\n| `-t`   | Expects a `tag` string value which will be as a tag for the service. This option allows users to run several QuestDB services and manage them separately. If this option is omitted, the default tag will be `questdb`. |\n| `-f`   | Force re-deploying the Web Console. Without this option, the Web Console is cached and deployed only when missing.                                                                                                   |\n| `-j`   | Windows only! This option allows to specify a path to `JAVA_HOME`.                                                                                                                                               |\n:::info\nWhen running multiple QuestDB services, a tag must be used to disambiguate\nbetween services for `start` and `stop` commands. There will be conflicting\nports and root directories if only the tag flag is specified when starting\nmultiple services. Each new service should have its own config file or should\nbe started with separate port and root directory options.\n:::\n:::info\nWhen running QuestDB as Windows service you can check status in both:\n- Windows Event Viewer - look for events with \"QuestDB\" source in Windows Logs | Application .\n- service log file - `$dataDir\\log\\service-%Y-%m-%dT%H-%M-%S.txt` (default is `C:\\Windows\\System32\\qdbroot\\log\\service-%Y-%m-%dT%H-%M-%S.txt` )\n:::\n\n\n\n\n`shell\n./questdb.sh start [-d dir] [-f] [-t tag]`\n\n\n`shell\nquestdb start [-d dir] [-f] [-t tag]`\n\n\n`shell\nquestdb.exe start [-d dir] [-f] [-j JAVA_HOME] [-t tag]`\n\n\nDefault root directory\nBy default, QuestDB's root directory\nwill be the following:\n\n\n\n\n`shell\n$HOME/.questdb`\n\n\nPath on Macs with Apple Silicon (M1 or M2) chip:\n`shell\n/opt/homebrew/var/questdb`\nPath on Macs with Intel chip:\n`shell\n/usr/local/var/questdb`\n\n\n`shell\nC:\\Windows\\System32\\qdbroot`\n\n\nStop\n`stop` - stops a service.\n| Option | Description                                                                                                        |\n| ------ | ------------------------------------------------------------------------------------------------------------------ |\n| `-t`   | Expects a `tag` string value which to stop a service by tag. If this is omitted, the default tag will be `questdb` |\n\n\n\n\n`shell\n./questdb.sh stop`\n\n\n`shell\nquestdb stop`\n\n\n`shell\nquestdb.exe stop`\n\n\nStatus\n`status` - shows the status for a service.\n| Option | Description                                                                                                    |\n| ------ | -------------------------------------------------------------------------------------------------------------- |\n| `-t`   | Expects a `tag` string value which to stop a service by tag. If this is omitted, the default will be `questdb` |\n\n\n\n\n`shell\n./questdb.sh status`\n\n\n`shell\nquestdb status`\n\n\n`shell\nquestdb.exe status`\n\n\nInstall (Windows)\n`install` - installs the Windows QuestDB service. The service will start\nautomatically at startup.\n`shell\nquestdb.exe install`\nRemove (Windows)\n`remove` - removes the Windows QuestDB service. It will no longer start at\nstartup.\n`shell\nquestdb.exe remove`\nInteractive session (Windows)\nYou can start QuestDB interactively by running `questdb.exe`. This will launch\nQuestDB interactively in the active `Shell` window. QuestDB will be stopped when\nthe Shell is closed.\nDefault root directory\nWhen started interactively, QuestDB's root directory defaults to the `current`\ndirectory.\nStop\nTo stop, press Ctrl+C in the terminal or close it",
    "tag": "questdb"
  },
  {
    "title": "list of configured writers",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/configuration.md",
    "content": "\ntitle: Configuration\ndescription: Server configuration keys reference documentation.\n\nThis page describes methods for configuring QuestDB server settings.\nConfiguration can be set either:\n\nIn the `server.conf` configuration file available in the\n  root directory\nUsing environment variables\n\nWhen a key is absent from both the configuration file and the environment\nvariables, the default value is used. Configuration of logging is handled\nseparately and details of configuring this behavior can be found at the\nlogging section below.\nEnvironment variables\nAll settings in the configuration file can be set or overridden using\nenvironment variables. If a key is set in both the `server.conf` file and via an\nenvironment variable, the environment variable will take precedence and the\nvalue in the server configuration file will be ignored.\nTo make these configuration settings available to QuestDB via environment\nvariables, they must be in the following format:\n`shell\nQDB_<KEY_OF_THE_PROPERTY>`\nWhere `<KEY_OF_THE_PROPERTY>` is equal to the configuration key name. To\nproperly format a `server.conf` key as an environment variable it must have:\n\n`QDB_` prefix\nuppercase characters\nall `.` period characters replaced with `_` underscore\n\nFor example, the server configuration key for shared workers must be passed as\ndescribed below:\n| `server.conf` key     | env var                   |\n| --------------------- | ------------------------- |\n| `shared.worker.count` | `QDB_SHARED_WORKER_COUNT` |\n:::note\nQuestDB applies these configuration changes on startup and a running instance\nmust be restarted in order for configuration changes to take effect\n:::\nExamples\nThe following configuration property customizes the number of worker threads\nshared across the application:\n`shell title=\"conf/server.conf\"\nshared.worker.count=5`\n`shell title=\"Customizing the worker count via environment variable\"\nexport QDB_SHARED_WORKER_COUNT=5`\nDocker\nThis section describes how to configure QuestDB server settings when running\nQuestDB in a Docker container. A command to run QuestDB via Docker with default\ninterfaces is as follows:\n`shell title=\"Example of running docker container with built-in storage\"\ndocker run -p 9000:9000 \\\n -p 9009:9009 \\\n -p 8812:8812 \\\n -p 9003:9003 \\\n questdb/questdb`\nThis publishes the following ports:\n\n`-p 9000:9000` - REST API and\n  Web Console\n`-p 9009:9009` - InfluxDB line protocol\n`-p 8812:8812` - Postgres wire protocol\n`-p 9003:9003` -\n  Min health server and Prometheus metrics\n\nThe examples in this section change the default HTTP and REST API port from\n`9000` to `4000` for illustrative purposes, and demonstrate how to publish this\nport with a non-default property.\nEnvironment variables\nServer configuration can be passed to QuestDB running in Docker by using the\n`-e` flag to pass an environment variable to a container:\n`bash\ndocker run -p 4000:4000 -e QDB_HTTP_BIND_TO=0.0.0.0:4000 questdb/questdb`\nMounting a volume\nA server configuration file can be provided by mounting a local directory in a\nQuestDB container. Given the following configuration file which overrides the\ndefault HTTP bind property:\n`shell title=\"./server.conf\"\nhttp.bind.to=0.0.0.0:4000`\nRunning the container with the `-v` flag allows for mounting the current\ndirectory to QuestDB's `conf` directory in the container. With the server\nconfiguration above, HTTP ports for the web console and REST API will be\navailable on `localhost:4000`:\n`bash\ndocker run -v \"$(pwd):/var/lib/questdb/conf\" -p 4000:4000 questdb/questdb`\nTo mount the full root directory of QuestDB when running in a Docker container,\nprovide a the configuration in a `conf` directory:\n`shell title=\"./conf/server.conf\"\nhttp.bind.to=0.0.0.0:4000`\nMount the current directory using the `-v` flag:\n`bash\ndocker run -v \"$(pwd):/var/lib/questdb/\" -p 4000:4000 questdb/questdb`\nThe current directory will then have data persisted to disk:\n`bash title=\"Current directory contents\"\n\u251c\u2500\u2500 conf\n\u2502  \u2514\u2500\u2500 server.conf\n\u251c\u2500\u2500 db\n\u2514\u2500\u2500 public`\nKeys and default values\nThis section lists the configuration keys available to QuestDB by topic or\nsubsystem. Parameters for specifying buffer and memory page sizes are provided\nin the format `n<unit>`, where `<unit>` can be one of the following:\n\n`m` for MB\n`k` for kB\n\nFor example:\n`ini title=\"Setting maximum send buffer size to 2MB per TCP socket\"\nhttp.net.connection.sndbuf=2m`\nShared worker\nShared worker threads service SQL execution subsystems and (in the default\nconfiguration) every other subsystem.\n| Property                  | Default | Description                                                                                                                                                  |\n| ------------------------- | ------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| shared.worker.count       |         | Number of worker threads shared across the application. Increasing this number will increase parallelism in the application at the expense of CPU resources. |\n| shared.worker.affinity    |         | Comma-delimited list of CPU ids, one per thread specified in `shared.worker.count`. By default, threads have no CPU affinity.                                |\n| shared.worker.haltOnError | false   | Toggle whether worker should stop on error.                                                                                                                  |\nMinimal HTTP server\nThis server runs embedded in a QuestDB instance by default and enables health\nchecks of an instance via HTTP. It responds to all requests with a HTTP status\ncode of `200` unless the QuestDB process dies.\n:::info\nPort `9003` also provides a `/metrics` endpoint with Prometheus metrics exposed.\nExamples of how to use the min server and Prometheus endpoint can be found on\nthe health monitoring page.\n:::\n| Property                        | Default      | Description                                                                                                                                                                                                                          |\n| ------------------------------- | ------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| http.min.enabled                | true         | Enable or disable Minimal HTTP server.                                                                                                                                                                                               |\n| http.min.bind.to                | 0.0.0.0:9003 | IPv4 address and port of the server. `0` means it will bind to all network interfaces, otherwise the IP address must be one of the existing network adapters.                                                                        |\n| http.min.net.connection.limit   | 4            | Active connection limit.                                                                                                                                                                                                             |\n| http.min.net.connection.timeout | 300000       | Idle connection timeout is milliseconds.                                                                                                                                                                                             |\n| http.min.net.connection.hint    | false        | Windows specific flag to overcome OS limitations on TCP backlog size.                                                                                                                                                                |\n| http.min.worker.count           |              | By default, minimal HTTP server uses shared thread pool for CPU core count 16 and below. It will use dedicated thread for core count above 16. When `0`, the server will use the shared pool. Do not set pool size to more than `1`. |\n| http.min.worker.affinity        |              | Core number to pin thread to.                                                                                                                                                                                                        |\nHTTP server\nThis section describes configuration settings for the Web Console available by\ndefault on port `9000`. For details on the use of this component, refer to the\nweb console documentation page.\n| Property                                     | Default      | Description                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| -------------------------------------------- | ------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| http.enabled                                 | true         | Enable or disable HTTP server.                                                                                                                                                                                                                                                                                                                                                                                                         |\n| http.bind.to                                 | 0.0.0.0:9000 | IP address and port of HTTP server. A value of `0` means that the HTTP server will bind to all network interfaces. You can specify IP address of any individual network interface on your system.                                                                                                                                                                                                                                      |\n| http.net.connection.limit                    | 64           | The maximum number permitted for simultaneous TCP connection to the HTTP server. The rationale of the value is to control server memory consumption.                                                                                                                                                                                                                                                                                   |\n| http.net.connection.timeout                  | 300000       | TCP connection idle timeout in milliseconds. Connection is closed by HTTP server when this timeout lapses.                                                                                                                                                                                                                                                                                                                             |\n| http.net.connection.sndbuf                   | 2M           | Maximum send buffer size on each TCP socket. If this value is `-1`, the socket send buffer size remains unchanged from the OS defaults.                                                                                                                                                                                                                                                                                                |\n| http.net.connection.rcvbuf                   | 2M           | Maximum receive buffer size on each TCP socket. If this value is `-1`, the socket receive buffer size remains unchanged from the OS defaults.                                                                                                                                                                                                                                                                                          |\n| http.net.connection.hint                     | false        | Windows specific flag to overcome OS limitations on TCP backlog size                                                                                                                                                                                                                                                                                                                                                                   |\n| http.connection.pool.initial.capacity        | 4            | Initial size of pool of reusable objects that hold connection state. The pool should be configured to maximum realistic load so that it does not resize at runtime.                                                                                                                                                                                                                                                                    |\n| http.connection.string.pool.capacity         | 128          | Initial size of the string pool shared by the HTTP header and multipart content parsers.                                                                                                                                                                                                                                                                                                                                               |\n| http.multipart.header.buffer.size            | 512          | Buffer size in bytes used by the HTTP multipart content parser.                                                                                                                                                                                                                                                                                                                                                                        |\n| http.multipart.idle.spin.count               | 10000        | How long the code accumulates incoming data chunks for column and delimiter analysis.                                                                                                                                                                                                                                                                                                                                                  |\n| http.receive.buffer.size                     | 1M           | Size of receive buffer.                                                                                                                                                                                                                                                                                                                                                                                                                |\n| http.request.header.buffer.size              | 64K          | Size of internal buffer allocated for HTTP request headers. The value is rounded up to the nearest power of 2. When HTTP requests contain headers that exceed the buffer size server will disconnect the client with HTTP error in server log.                                                                                                                                                                                         |\n| http.response.header.buffer.size             | 32K          | Size of the internal response buffer. The value will be rounded up to the nearest power of 2. The buffer size should be large enough to accommodate max size of server response headers.                                                                                                                                                                                                                                               |\n| http.worker.count                            | 0            | Number of threads in private worker pool. When `0`, HTTP server will be using shared worker pool of the server. Values above `0` switch on private pool.                                                                                                                                                                                                                                                                               |\n| http.worker.affinity                         |              | Comma separated list of CPU core indexes. The number of items in this list must be equal to the worker count.                                                                                                                                                                                                                                                                                                                          |\n| http.worker.haltOnError                      | false        | Changing the default value is strongly discouraged. Flag that indicates if worker thread must shutdown on unhandled error.                                                                                                                                                                                                                                                                                                         |\n| http.send.buffer.size                        | 2M           | Size of the internal send buffer. Larger buffer sizes result in fewer I/O interruptions the server is making at the expense of memory usage per connection. There is a limit of send buffer size after which increasing it stops being useful in terms of performance. 2MB seems to be optimal value.                                                                                                                                  |\n| http.static.index.file.name                  | index.html   | Name of index file for the Web Console.                                                                                                                                                                                                                                                                                                                                                                                                |\n| http.frozen.clock                            | false        | Sets the clock to always return zero. This configuration parameter is used for internal testing.                                                                                                                                                                                                                                                                                                                                       |\n| http.allow.deflate.before.send               | false        | Flag that indicates if Gzip compression of outgoing data is allowed.                                                                                                                                                                                                                                                                                                                                                                   |\n| http.keep-alive.timeout                      | 5            | Used together with `http.keep-alive.max` to set the value of HTTP `Keep-Alive` response header. This instructs browser to keep TCP connection open. Has to be `0` when `http.version` is set to `HTTP/1.0`.                                                                                                                                                                                                                            |\n| http.keep-alive.max                          | 10000        | See `http.keep-alive.timeout`. Has to be `0` when `http.version` is set to `HTTP/1.0`.                                                                                                                                                                                                                                                                                                                                                 |\n| http.static.public.directory                 | public       | The name of directory for public web site.                                                                                                                                                                                                                                                                                                                                                                                             |\n| http.text.date.adapter.pool.capacity         | 16           | Size of date adapter pool. This should be set to the anticipated maximum number of `DATE` fields a text input can have. The pool is assigned to connection state and is reused alongside of connection state object.                                                                                                                                                                                                                   |\n| http.text.json.cache.limit                   | 16384        | JSON parser cache limit. Cache is used to compose JSON elements that have been broken up by TCP protocol. This value limits the maximum length of individual tag or tag value.                                                                                                                                                                                                                                                         |\n| http.text.json.cache.size                    | 8192         | Initial size of JSON parser cache. The value must not exceed `http.text.json.cache.limit` and should be set to avoid cache resizes at runtime.                                                                                                                                                                                                                                                                                         |\n| http.text.max.required.delimiter.stddev      | 0.1222d      | The maximum standard deviation value for the algorithm that calculates text file delimiter. Usually when text parser cannot recognise the delimiter it will log the calculated and maximum standard deviation for the delimiter candidate.                                                                                                                                                                                             |\n| http.text.max.required.line.length.stddev    | 0.8          | Maximum standard deviation value for the algorithm that classifies input as text or binary. For the values above configured stddev input will be considered binary.                                                                                                                                                                                                                                                                    |\n| http.text.metadata.string.pool.capacity      | 128          | The initial size of pool for objects that wrap individual elements of metadata JSON, such as column names, date pattern strings and locale values.                                                                                                                                                                                                                                                                                     |\n| http.text.roll.buffer.limit                  | 4M           | The limit of text roll buffer. See `http.text.roll.buffer.size` for description.                                                                                                                                                                                                                                                                                                                                                       |\n| http.text.roll.buffer.size                   | 1024         | Roll buffer is a structure in the text parser that holds a copy of a line that has been broken up by TCP. The size should be set to the maximum length of text line in text input.                                                                                                                                                                                                                                                     |\n| http.text.analysis.max.lines                 | 1000         | Number of lines to read on CSV import for heuristics which determine column names & types. Lower line numbers may detect CSV schemas quicker, but possibly with less accuracy. 1000 lines is the maximum for this value.                                                                                                                                                                                                               |\n| http.text.lexer.string.pool.capacity         | 64           | The initial capacity of string fool, which wraps `STRING` column types in text input. The value should correspond to the maximum anticipated number of STRING columns in text input.                                                                                                                                                                                                                                                   |\n| http.text.timestamp.adapter.pool.capacity    | 64           | Size of timestamp adapter pool. This should be set to the anticipated maximum number of `TIMESTAMP` fields a text input can have. The pool is assigned to connection state and is reused alongside of connection state object.                                                                                                                                                                                                         |\n| http.text.utf8.sink.size                     | 4096         | Initial size of UTF-8 adapter sink. The value should correspond the maximum individual field value length in text input.                                                                                                                                                                                                                                                                                                               |\n| http.json.query.connection.check.frequency   | 1000000      | Changing the default value is strongly discouraged. The value to throttle check if client socket has been disconnected.                                                                                                                                                                                                                                                                                                            |\n| http.json.query.float.scale                  | 4            | The scale value of string representation of `FLOAT` values.                                                                                                                                                                                                                                                                                                                                                                            |\n| http.json.query.double.scale                 | 12           | The scale value of string representation of `DOUBLE` values.                                                                                                                                                                                                                                                                                                                                                                           |\n| http.query.cache.enabled                     | true         | Enable or disable the query cache. Cache capacity is `number_of_blocks * number_of_rows`.                                                                                                                                                                                                                                                                                                                                              |\n| http.query.cache.block.count                 | 4            | Number of blocks for the query cache.                                                                                                                                                                                                                                                                                                                                                                                                  |\n| http.query.cache.row.count                   | 16           | Number of rows for the query cache.                                                                                                                                                                                                                                                                                                                                                                                                    |\n| http.security.readonly                       | false        | Forces HTTP read only mode when `true`, disabling commands which modify the data or data structure, e.g. INSERT, UPDATE, or CREATE TABLE.                                                                                                                                                                                                                                                                                              |\n| http.security.max.response.rows              | 2^63-1       | Limit the number of response rows over HTTP.                                                                                                                                                                                                                                                                                                                                                                                           |\n| http.security.interrupt.on.closed.connection | true         | Switch to enable termination of SQL processing if the HTTP connection is closed. The mechanism affects performance so the connection is only checked after `circuit.breaker.throttle` calls are made to the check method. The mechanism also reads from the input stream and discards it since some HTTP clients send this as a keep alive in between requests, `circuit.breaker.buffer.size` denotes the size of the buffer for this. |\n| circuit.breaker.throttle                     | 2000000      | Number of internal iterations such as loops over data before checking if the HTTP connection is still open                                                                                                                                                                                                                                                                                                                             |\n| circuit.breaker.buffer.size                  | 32           | Size of buffer to read from HTTP connection. If this buffer returns zero and the HTTP client is no longer sending data, SQL processing will be terminated.                                                                                                                                                                                                                                                                             |\n| http.server.keep.alive                       | true         | If set to `false`, the server will disconnect the client after completion of each request.                                                                                                                                                                                                                                                                                                                                             |\n| http.version                                 | HTTP/1.1     | Protocol version, other supported value is `HTTP/1.0`.                                                                                                                                                                                                                                                                                                                                                                                 |\nCairo engine\nThis section describes configuration settings for the Cairo SQL engine in\nQuestDB.\n| Property                                       | Default           | Description                                                                                                                                                                                                              |\n|------------------------------------------------| ----------------- |--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| query.timeout.sec                              | 60                | A global timeout (in seconds) for long-running queries.                                                                                                                                                                  |\n| cairo.max.uncommitted.rows                     | 500000            | Maximum number of uncommitted rows per table, when the number of pending rows reaches this parameter on a table, a commit will be issued.                                                                                |\n| cairo.commit.lag (QuestDB 6.5.5 and earlier)   | 5 minutes         | Expected maximum time lag for out-of-order rows in milliseconds.                                                                                                                                                         |\n| cairo.o3.max.lag (QuestDB 6.6 and later)       | 10 minutes        | The maximum size of in-memory buffer in milliseconds. The buffer is allocated dynamically through analysing the shape of the incoming data, and `o3MaxLag` is the upper limit.                                           |\n| cairo.o3.min.lag (QuestDB 6.6 and later)       | 1 second          | The minimum size of in-memory buffer in milliseconds. The buffer is allocated dynamically through analysing the shape of the incoming data, and `o3MinLag` is the lower limit.                                           |\n| cairo.sql.backup.root                          | null              | Output root directory for backups.                                                                                                                                                                                       |\n| cairo.sql.backup.dir.datetime.format           | null              | Date format for backup directory.                                                                                                                                                                                        |\n| cairo.sql.backup.dir.tmp.name                  | tmp               | Name of tmp directory used during backup.                                                                                                                                                                                |\n| cairo.sql.backup.mkdir.mode                    | 509               | Permission used when creating backup directories.                                                                                                                                                                        |\n| cairo.snapshot.instance.id                     | empty string      | Instance id to be included into disk snapshots.                                                                                                                                                                          |\n| cairo.snapshot.recovery.enabled                | true              | When `false`, disables snapshot recovery on database start.                                                                                                                                                              |\n| cairo.root                                     | db                | Directory for storing db tables and metadata. This directory is inside the server root directory provided at startup.                                                                                                    |\n| cairo.commit.mode                              | nosync            | How changes to table are flushed to disk upon commit. Choices: `nosync`, `async` (flush call schedules update, returns immediately), `sync` (waits for flush on the appended column files to complete).                  |\n| cairo.create.as.select.retry.count             | 5                 | Number of types table creation or insertion will be attempted.                                                                                                                                                           |\n| cairo.default.map.type                         | fast              | Type of map used. Options: `fast` (speed at the expense of storage), `compact`.                                                                                                                                          |\n| cairo.default.symbol.cache.flag                | true              | When `true`, symbol values will be cached on Java heap instead of being looked up in the database files.                                                                                                                 |\n| cairo.default.symbol.capacity                  | 256               | Specifies approximate capacity for `SYMBOL` columns. It should be equal to number of unique symbol values stored in the table and getting this value badly wrong will cause performance degradation. Must be power of 2. |\n| cairo.file.operation.retry.count               | 30                | Number of attempts to open files.                                                                                                                                                                                        |\n| cairo.idle.check.interval                      | 300000            | Frequency of writer maintenance job in milliseconds.                                                                                                                                                                     |\n| cairo.inactive.reader.ttl                      | -120000           | Frequency of reader pool checks for inactive readers in milliseconds.                                                                                                                                                    |\n| cairo.inactive.writer.ttl                      | -600000           | Frequency of writer pool checks for inactive writers in milliseconds.                                                                                                                                                    |\n| cairo.index.value.block.size                   | 256               | Approximation of number of rows for a single index key, must be power of 2.                                                                                                                                              |\n| cairo.max.swap.file.count                      | 30                | Number of attempts to open swap files.                                                                                                                                                                                   |\n| cairo.mkdir.mode                               | 509               | File permission mode for new directories.                                                                                                                                                                                |\n| cairo.parallel.index.threshold                 | 100000            | Minimum number of rows before allowing use of parallel indexation.                                                                                                                                                       |\n| cairo.reader.pool.max.segments                 | 5                 | Number of attempts to get TableReader.                                                                                                                                                                                   |\n| cairo.spin.lock.timeout                        | 1000              | Timeout when attempting to get BitmapIndexReaders in millisecond.                                                                                                                                                        |\n| cairo.character.store.capacity                 | 1024              | Size of the CharacterStore.                                                                                                                                                                                              |\n| cairo.character.store.sequence.pool.capacity   | 64                | Size of the CharacterSequence pool.                                                                                                                                                                                      |\n| cairo.column.pool.capacity                     | 4096              | Size of the Column pool in the SqlCompiler.                                                                                                                                                                              |\n| cairo.compact.map.load.factor                  | 0.7               | Load factor for CompactMaps.                                                                                                                                                                                             |\n| cairo.expression.pool.capacity                 | 8192              | Size of the ExpressionNode pool in SqlCompiler.                                                                                                                                                                          |\n| cairo.fast.map.load.factor                     | 0.5               | Load factor for all FastMaps.                                                                                                                                                                                            |\n| cairo.sql.join.context.pool.capacity           | 64                | Size of the JoinContext pool in SqlCompiler.                                                                                                                                                                             |\n| cairo.lexer.pool.capacity                      | 2048              | Size of FloatingSequence pool in GenericLexer.                                                                                                                                                                           |\n| cairo.sql.map.key.capacity                     | 2M                | Key capacity in FastMap and CompactMap.                                                                                                                                                                                  |\n| cairo.sql.map.max.resizes                      | 2^31              | Number of map resizes in FastMap and CompactMap before a resource limit exception is thrown, each resize doubles the previous size.                                                                                      |\n| cairo.sql.map.page.size                        | 4m                | Memory page size for FastMap and CompactMap.                                                                                                                                                                             |\n| cairo.sql.map.max.pages                        | 2^31              | Memory max pages for CompactMap.                                                                                                                                                                                         |\n| cairo.model.pool.capacity                      | 1024              | Size of the QueryModel pool in the SqlCompiler.                                                                                                                                                                          |\n| cairo.sql.sort.key.page.size                   | 4M                | Memory page size for storing keys in LongTreeChain.                                                                                                                                                                      |\n| cairo.sql.sort.key.max.pages                   | 2^31              | Max number of pages for storing keys in LongTreeChain before a resource limit exception is thrown.                                                                                                                       |\n| cairo.sql.sort.light.value.page.size           | 1048576           | Memory page size for storing values in LongTreeChain.                                                                                                                                                                    |\n| cairo.sql.sort.light.value.max.pages           | 2^31              | Max pages for storing values in LongTreeChain.                                                                                                                                                                           |\n| cairo.sql.hash.join.value.page.size            | 16777216          | Memory page size of the slave chain in full hash joins.                                                                                                                                                                  |\n| cairo.sql.hash.join.value.max.pages            | 2^31              | Max pages of the slave chain in full hash joins.                                                                                                                                                                         |\n| cairo.sql.latest.by.row.count                  | 1000              | Number of rows for LATEST BY.                                                                                                                                                                                            |\n| cairo.sql.hash.join.light.value.page.size      | 1048576           | Memory page size of the slave chain in light hash joins.                                                                                                                                                                 |\n| cairo.sql.hash.join.light.value.max.pages      | 2^31              | Max pages of the slave chain in light hash joins.                                                                                                                                                                        |\n| cairo.sql.sort.value.page.size                 | 16777216          | Memory page size of file storing values in SortedRecordCursorFactory.                                                                                                                                                    |\n| cairo.sql.sort.value.max.pages                 | 2^31              | Max pages of file storing values in SortedRecordCursorFactory.                                                                                                                                                           |\n| cairo.work.steal.timeout.nanos                 | 10000             | Latch await timeout in nanos for stealing indexing work from other threads.                                                                                                                                              |\n| cairo.parallel.indexing.enabled                | true              | Allows parallel indexation. Works in conjunction with cairo.parallel.index.threshold.                                                                                                                                    |\n| cairo.sql.join.metadata.page.size              | 16384             | Memory page size for JoinMetadata file.                                                                                                                                                                                  |\n| cairo.sql.join.metadata.max.resizes            | 2^31              | Number of map resizes in JoinMetadata before a resource limit exception is thrown, each resize doubles the previous size.                                                                                                |\n| cairo.sql.analytic.column.pool.capacity        | 64                | Size of AnalyticColumn pool in SqlParser.                                                                                                                                                                                |\n| cairo.sql.create.table.model.pool.capacity     | 16                | Size of CreateTableModel pool in SqlParser.                                                                                                                                                                              |\n| cairo.sql.column.cast.model.pool.capacity      | 16                | Size of CreateTableModel pool in SqlParser.                                                                                                                                                                              |\n| cairo.sql.rename.table.model.pool.capacity     | 16                | Size of RenameTableModel pool in SqlParser.                                                                                                                                                                              |\n| cairo.sql.with.clause.model.pool.capacity      | 128               | Size of WithClauseModel pool in SqlParser.                                                                                                                                                                               |\n| cairo.sql.insert.model.pool.capacity           | 64                | Size of InsertModel pool in SqlParser.                                                                                                                                                                                   |\n| cairo.sql.copy.model.pool.capacity             | 32                | Size of CopyModel pool in SqlParser.                                                                                                                                                                                     |\n| cairo.sql.copy.buffer.size                     | 2M                | Size of buffer used when copying tables.                                                                                                                                                                                 |\n| cairo.sql.double.cast.scale                    | 12                | Maximum number of decimal places that types cast as doubles have.                                                                                                                                                        |\n| cairo.sql.float.cast.scale                     | 4                 | Maximum number of decimal places that types cast as floats have.                                                                                                                                                         |\n| cairo.sql.copy.formats.file                    | /text_loader.json | Name of file with user's set of date and timestamp formats.                                                                                                                                                              |\n| cairo.sql.jit.mode                             | on                | JIT compilation for SQL queries. May be disabled by setting this value to `off`.                                                                                                                                         |\n| cairo.date.locale                              | en                | The locale to handle date types.                                                                                                                                                                                         |\n| cairo.timestamp.locale                         | en                | The locale to handle timestamp types.                                                                                                                                                                                    |\n| cairo.o3.column.memory.size                    | 8M                | Memory page size per column for O3 operations. Please be aware O3 will use 2x of the set value per column (therefore a default of 2x8M).                                                                                 |\n| cairo.writer.data.append.page.size             | 16M               | mmap sliding page size that table writer uses to append data for each column.                                                                                                                                            |\n| cairo.writer.data.index.key.append.page.size   | 512K              | mmap page size for appending index key data.                                                                                                                                                                             |\n| cairo.writer.data.index.value.append.page.size | 16M               | mmap page size for appending value data.                                                                                                                                                                                 |\n| cairo.writer.misc.append.page.size             | 4K                | mmap page size for mapping small files, default value is OS page size (4k Linux, 64K windows, 16k OSX M1). Overriding this rounds to the nearest (greater) multiple of the OS page size.                                 |\n| cairo.writer.data.index.key.append.page.size   | 512K              | mmap page size for appending index key data; key data is number of distinct symbol values times 4 bytes.                                                                                                                 |\n| cairo.sql.column.purge.queue.capacity          | 128               | Purge column version job queue. Increase the size if column version not automatically cleanup after execution of UPDATE SQL statement. Reduce to decrease initial memory footprint.                                      |\n| cairo.sql.column.purge.task.pool.capacity      | 256               | Column version task object pool capacity. Increase to reduce GC, reduce to decrease memory footprint.                                                                                                                    |\n| cairo.sql.column.purge.retry.delay             | 10000             | Initial delay (\u03bcs) before re-trying purge of stale column files.                                                                                                                                                         |\n| cairo.sql.column.purge.retry.delay.multiplier  | 10.0              | Multiplier used to increases retry delay with each iteration.                                                                                                                                                            |\n| cairo.sql.column.purge.retry.delay.limit       | 60000000          | Delay limit (\u03bcs), upon reaching which, the re-try delay remains constant.                                                                                                                                                |\n| cairo.sql.column.purge.retry.limit.days        | 31                | Number of days purge system will continue to re-try deleting stale column files before giving up.                                                                                                                        |\n| cairo.system.table.prefix                      | sys.              | Prefix of the tables used for QuestDB internal data storage. These tables are hidden from QuestDB webconsole.                                                                                                            |\n| cairo.volumes                                  | -                 | A comma separated list of alias -> root-path pairs defining allowed volumes to be used in CREATE TABLE IN VOLUME statements.                                  |\n| cairo.system.table.prefix                      | sys.              | Prefix of the tables used for QuestDB internal data storage. These tables are hidden from QuestDB web console.                                                                                                           |\n| cairo.wal.enabled.default                      | false             | Setting defining whether WAL table is the default when using `CREATE TABLE`.                                                                                                                                             |\nWAL table configurations\nThe following WAL tables settings on parallel threads are configurable for applying WAL data to the table storage:\n| Property                 | Default                        | Description                                                                                                                                                                                          |\n| ------------------------ | ------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| wal.apply.worker.count   | equal to the CPU core count    | Number of dedicated worker threads assigned to handle WAL table data. |\n| wal.apply.worker.affinity | equal to the CPU core count    | Comma separated list of CPU core indexes.                                            |\n| wal.apply.worker.haltOnError     | false                | Flag that indicates if worker thread must shutdown on unhandled error   |\n| cairo.wal.purge.interval | 30000                       | Period in ms of how often WAL-applied files are cleaned up from the disk |\n| cairo.wal.segment.rollover.row.count | 200000           | The number of rows written to the same WAL segment before starting a new segment |\n| cairo.wal.commit.squash.row.limit | 500000                  | Maximum row count that can be squashed together from multiple transactions before applying to the table. A very low value can delay data visibility. |\nCSV import\nThis section describes configuration settings for using `COPY` to import large\nCSV files.\nMandatory settings to enable `COPY`:\n| Property                 | Default | Description                                                                                                                                                                                          |\n| ------------------------ | ------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| cairo.sql.copy.root      | null    | Input root directory for CSV imports via `COPY` SQL. This path should not overlap with other directory (e.g. db, conf) of running instance, otherwise import may delete or overwrite existing files. |\n| cairo.sql.copy.work.root | null    | Temporary import file directory. Defaults to `root_directory/tmp` if not set explicitly.                                                                                                             |\nOptional settings for `COPY`:\n| Property                            | Default | Description                                                                                                                                                                           |\n| ----------------------------------- | ------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| cairo.iouring.enabled               | true    | Enable or disable io_uring implementation. Applicable to newer Linux kernels only. Can be used to switch io_uring interface usage off if there's a kernel bug affecting it.           |\n| cairo.sql.copy.buffer.size          | 2 MiB   | Size of read buffers used in import.                                                                                                                                                  |\n| cairo.sql.copy.log.retention.days   | 3       | Number of days to keep import messages in `sys.text_import_log`.                                                                                                                      |\n| cairo.sql.copy.max.index.chunk.size | 100M    | Maximum size of index chunk file used to limit total memory requirements of import. Indexing phase should use roughly `thread_count * cairo.sql.copy.max.index.chunk.size` of memory. |\n| cairo.sql.copy.queue.capacity       | 32      | Size of copy task queue. Should be increased if there's more than 32 import workers.                                                                                                  |\nCSV import configuration for Docker\nFor QuestDB instances using Docker:\n\n`cairo.sql.copy.root` must be defined using one of the following settings:\nThe environment variable `QDB_CAIRO_SQL_COPY_ROOT`.\nThe `cairo.sql.copy.root` in `server.conf`.\nThe path for the source CSV file is mounted.\nThe source CSV file path and the path defined by `QDB_CAIRO_SQL_COPY_ROOT` are\n  identical.\nIt is optional to define `QDB_CAIRO_SQL_COPY_WORK_ROOT`.\n\nThe following is an example command to start a QuestDB instance on Docker, in\norder to import a CSV file:\n`shell\ndocker run -p 9000:9000 \\\n-v \"/tmp/questdb:/var/lib/questdb\" \\\n-v \"/tmp/questdb/my_input_root:/var/lib/questdb/questdb_import\" \\\n-e QDB_CAIRO_SQL_COPY_ROOT=/var/lib/questdb/questdb_import \\\nquestdb/questdb`\nWhere:\n\n`-v \"/tmp/questdb/my_input_root:/var/lib/questdb/questdb_import\"`: Defining a\n  source CSV file location to be `/tmp/questdb/my_input_root` on local machine\n  and mounting it to `/var/lib/questdb/questdb_import` in the container.\n`-e QDB_CAIRO_SQL_COPY_ROOT=/var/lib/questdb/questdb_import`: Defining the\n  copy root directory to be `/var/lib/questdb/questdb_import`.\n\nIt is important that the two path are identical\n(`/var/lib/questdb/questdb_import` in the example).\nParallel SQL execution\nThis section describes settings that can affect parallelism level of SQL\nexecution and therefore performance.\n| Property                                   | Default | Description                                                                                                                                                                |\n| ------------------------------------------ | ------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| cairo.sql.parallel.filter.enabled          | true    | Enable or disable parallel SQL filter execution. JIT compilation takes place only when this setting is enabled.                                                            |\n| cairo.sql.parallel.filter.pretouch.enabled | true    | Enable column pre-touch as part of the parallel SQL filter execution, to improve query performance for large tables.                                                       |\n| cairo.page.frame.shard.count               | 4       | Number of shards for both dispatch and reduce queues. Shards reduce queue contention between SQL statements that are executed concurrently.                                |\n| cairo.page.frame.reduce.queue.capacity     | 64      | Reduce queue is used for data processing and should be large enough to supply tasks for worker threads (shared worked pool).                                               |\n| cairo.page.frame.rowid.list.capacity       | 256     | Row ID list initial capacity for each slot of the reduce queue. Larger values reduce memory allocation rate, but increase minimal RSS size.                                |\n| cairo.page.frame.column.list.capacity      | 16      | Column list capacity for each slot of the reduce queue. Used by JIT-compiled filter functions. Larger values reduce memory allocation rate, but increase minimal RSS size. |\n| cairo.page.frame.task.pool.capacity        | 4       | Initial object pool capacity for local reduce tasks. These tasks are used to avoid blocking query execution when the reduce queue is full.                                 |\nPostgres wire protocol\nThis section describes configuration settings for client connections using\nPostgresSQL wire protocol.\n| Property                         | Default      | Description                                                                                                                                                                                       |\n| -------------------------------- | ------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| pg.enabled                       | true         | Configuration for enabling or disabling the Postres interface.                                                                                                                                    |\n| pg.net.bind.to                   | 0.0.0.0:8812 | IP address and port of Postgres wire protocol server. 0 means that the server will bind to all network interfaces. You can specify IP address of any individual network interface on your system. |\n| pg.net.connection.limit          | 64           | The maximum number permitted for simultaneous Postgres connections to the server. This value is intended to control server memory consumption.                                                    |\n| pg.net.connection.timeout        | 300000       | Connection idle timeout in milliseconds. Connections are closed by the server when this timeout lapses.                                                                                           |\n| pg.net.connection.rcvbuf         | -1           | Maximum send buffer size on each TCP socket. If value is -1 socket send buffer remains unchanged from OS default.                                                                                 |\n| pg.net.connection.sndbuf         | -1           | Maximum receive buffer size on each TCP socket. If value is -1, the socket receive buffer remains unchanged from OS default.                                                                      |\n| pg.net.connection.hint           | false        | Windows specific flag to overcome OS limitations on TCP backlog size                                                                                                                              |\n| pg.security.readonly             | false        | Forces PGWire read only mode when `true`, disabling commands which modify the data or data structure, e.g. INSERT, UPDATE, or CREATE TABLE.                                                       |\n| pg.character.store.capacity      | 4096         | Size of the CharacterStore.                                                                                                                                                                       |\n| pg.character.store.pool.capacity | 64           | Size of the CharacterStore pool capacity .                                                                                                                                                        |\n| pg.connection.pool.capacity      | 64           | The maximum amount of pooled connections this interface may have.                                                                                                                                 |\n| pg.password                      | quest        | Postgres database password.                                                                                                                                                                       |\n| pg.user                          | admin        | Postgres database username.                                                                                                                                                                       |\n| pg.readonly.user.enabled         | false        | Enable or disable Postgres database read-only user account. When enabled, this additional user can be used to open read-only connections to the database.                                         |\n| pg.readonly.password             | quest        | Postgres database read-only user password.                                                                                                                                                        |\n| pg.readonly.user                 | user         | Postgres database read-only user username.                                                                                                                                                        |\n| pg.select.cache.enabled          | true         | Enable or disable the SELECT query cache. Cache capacity is `number_of_blocks * number_of_rows`.                                                                                                  |\n| pg.select.cache.block.count      | 16           | Number of blocks to cache SELECT query execution plan against text to speed up execution.                                                                                                         |\n| pg.select.cache.row.count        | 16           | Number of rows to cache for SELECT query execution plan against text to speed up execution.                                                                                                       |\n| pg.insert.cache.enabled          | true         | Enable or disable the INSERT query cache. Cache capacity is `number_of_blocks * number_of_rows`.                                                                                                  |\n| pg.insert.cache.block.count      | 8            | Number of blocks to cache INSERT query execution plan against text to speed up execution.                                                                                                         |\n| pg.insert.cache.row.count        | 8            | Number of rows to cache for INSERT query execution plan against text to speed up execution.                                                                                                       |\n| pg.update.cache.enabled          | true         | Enable or disable the UPDATE query cache. Cache capacity is `number_of_blocks * number_of_rows`.                                                                                                  |\n| pg.update.cache.block.count      | 8            | Number of blocks to cache UPDATE query execution plan against text to speed up execution.                                                                                                         |\n| pg.update.cache.row.count        | 8            | Number of rows to cache for UPDATE query execution plan against text to speed up execution.                                                                                                       |\n| pg.max.blob.size.on.query        | 512k         | For binary values, clients will receive an error when requesting blob sizes above this value.                                                                                                     |\n| pg.recv.buffer.size              | 1M           | Size of the buffer for receiving data.                                                                                                                                                            |\n| pg.send.buffer.size              | 1M           | Size of the buffer for sending data.                                                                                                                                                              |\n| pg.date.locale                   | en           | The locale to handle date types.                                                                                                                                                                  |\n| pg.timestamp.locale              | en           | The locale to handle timestamp types.                                                                                                                                                             |\n| pg.worker.count                  | 0            | Number of dedicated worker threads assigned to handle PGWire queries. When `0`, the jobs will use the shared pool.                                                                                |\n| pg.worker.affinity               |              | Comma-separated list of thread numbers which should be pinned for Postgres ingestion. Example `pg.worker.affinity=1,2,3`.                                                                         |\n| pg.halt.on.error                 | false        | Whether ingestion should stop upon internal error.                                                                                                                                                |\nInfluxDB line protocol\nThis section describes ingestion settings for incoming messages using InfluxDB\nline protocol.\n| Property                  | Default | Description                                                                                             |\n| ------------------------- | ------- | ------------------------------------------------------------------------------------------------------- |\n| line.default.partition.by | DAY     | The default partitioning strategy applied to new tables dynamically created by sending records via ILP. |\nTCP specific settings\n| Property                                   | Default      | Description                                                                                                                                                                                                                                           |\n| ------------------------------------------ | ------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| line.tcp.enabled                           | true         | Enable or disable line protocol over TCP.                                                                                                                                                                                                             |\n| line.tcp.net.bind.to                       | 0.0.0.0:9009 | IP address of the network interface to bind listener to and port. By default, TCP receiver listens on all network interfaces.                                                                                                                         |\n| line.tcp.net.connection.limit              | 256          | The maximum number permitted for simultaneous connections to the server. This value is intended to control server memory consumption.                                                                                                                 |\n| line.tcp.net.connection.timeout            | 300000       | Connection idle timeout in milliseconds. Connections are closed by the server when this timeout lapses.                                                                                                                                               |\n| line.tcp.net.connection.hint               | false        | Windows specific flag to overcome OS limitations on TCP backlog size                                                                                                                                                                                  |\n| line.tcp.net.connection.rcvbuf             | -1           | Maximum buffer receive size on each TCP socket. If value is -1, the socket receive buffer remains unchanged from OS default.                                                                                                                          |\n| line.tcp.auth.db.path                      |              | Path which points to the authentication db file.                                                                                                                                                                                                      |\n| line.tcp.connection.pool.capacity          | 64           | The maximum amount of pooled connections this interface may have.                                                                                                                                                                                     |\n| line.tcp.timestamp                         | n            | Input timestamp resolution. Possible values are `n`, `u`, `ms`, `s` and `h`.                                                                                                                                                                          |\n| line.tcp.msg.buffer.size                   | 32768        | Size of the buffer read from queue. Maximum size of write request, regardless of the number of measurements.                                                                                                                                          |\n| line.tcp.maintenance.job.interval          | 1000         | Maximum amount of time (in milliseconds) between maintenance jobs committing any uncommitted data on inactive tables.                                                                                                                                 |\n| line.tcp.min.idle.ms.before.writer.release | 500          | Minimum amount of idle time (in milliseconds) before a table writer is released.                                                                                                                                                                      |\n| line.tcp.commit.interval.fraction          | 0.5          | Commit lag fraction. Used to calculate commit interval for the table according to the following formula: `commit_interval = commit_lag \u2217 fraction`. The calculated commit interval defines how long uncommitted data will need to remain uncommitted. |\n| line.tcp.commit.interval.default           | 1000         | Default commit interval in milliseconds.                                                                                                                                                                                                              |\n| line.tcp.max.measurement.size              | 32768        | Maximum size of any measurement.                                                                                                                                                                                                                      |\n| line.tcp.writer.queue.size                 | 128          | Size of the queue between network I/O and writer jobs. Each queue entry represents a measurement.                                                                                                                                                     |\n| line.tcp.writer.worker.count               |              | Number of dedicated I/O worker threads assigned to write data to tables. When `0`, the writer jobs will use the shared pool.                                                                                                                          |\n| line.tcp.writer.worker.affinity            |              | Comma-separated list of thread numbers which should be pinned for line protocol ingestion over TCP. CPU core indexes are 0-based.                                                                                                                     |\n| line.tcp.io.worker.count                   |              | Number of dedicated I/O worker threads assigned to parse TCP input. When `0`, the writer jobs will use the shared pool.                                                                                                                               |\n| line.tcp.io.worker.affinity                |              | Comma-separated list of thread numbers which should be pinned for line protocol ingestion over TCP. CPU core indexes are 0-based.                                                                                                                     |\n| line.tcp.default.partition.by              | DAY          | Table partition strategy to be used with tables that are created automatically by ILP. Possible values are: `HOUR`, `DAY`, `MONTH` and `YEAR`                                                                                                         |\n| line.tcp.disconnect.on.error               | true         | Disconnect TCP socket that sends malformed messages.                                                                                                                                                                                                  |\nUDP specific settings\n:::note\nThe UDP receiver is deprecated since QuestDB version 6.5.2. We recommend the\nTCP receiver instead.\n:::\n| Property                     | Default      | Description                                                                                                                                                                                                                      |\n| ---------------------------- | ------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| line.udp.join                | 232.1.2.3    | Multicast address receiver joins. This values is ignored when receiver is in \"unicast\" mode.                                                                                                                                     |\n| line.udp.bind.to             | 0.0.0.0:9009 | IP address of the network interface to bind listener to and port. By default UDP receiver listens on all network interfaces.                                                                                                     |\n| line.udp.commit.rate         | 1000000      | For packet bursts the number of continuously received messages after which receiver will force commit. Receiver will commit irrespective of this parameter when there are no messages.                                           |\n| line.udp.msg.buffer.size     | 2048         | Buffer used to receive single message. This value should be roughly equal to your MTU size.                                                                                                                                      |\n| line.udp.msg.count           | 10000        | Only for Linux. On Linux, QuestDB will use the `recvmmsg()` system call. This is the max number of messages to receive at once.                                                                                                  |\n| line.udp.receive.buffer.size | 8388608      | UDP socket buffer size. Larger size of the buffer will help reduce message loss during bursts.                                                                                                                                   |\n| line.udp.enabled             | true         | Enable or disable UDP receiver.                                                                                                                                                                                                  |\n| line.udp.own.thread          | false        | When `true`, UDP receiver will use its own thread and busy spin that for performance reasons. \"false\" makes receiver use worker threads that do everything else in QuestDB.                                                      |\n| line.udp.own.thread.affinity | -1           | -1 does not set thread affinity. OS will schedule thread and it will be liable to run on random cores and jump between the. 0 or higher pins thread to give core. This property is only valid when UDP receiver uses own thread. |\n| line.udp.unicast             | false        | When `true`, UDP will use unicast. Otherwise multicast.                                                                                                                                                                          |\n| line.udp.timestamp           | n            | Input timestamp resolution. Possible values are `n`, `u`, `ms`, `s` and `h`.                                                                                                                                                     |\n| line.udp.commit.mode         | nosync       | Commit durability. Available values are `nosync`, `sync` and `async`.                                                                                                                                                            |\nConfig Validation\nThe database startup phase checks for configuration issues, such as invalid or\ndeprecated settings. Issues may be classified as advisories or errors. Advisory\nissues are logged\nwithout causing the database to stop its startup sequence: These are usually\nsetting deprecation warnings. Configuration errors can optionally cause the\ndatabase to fail its startup.\n| Property                 | Default | Description                                                    |\n| ------------------------ | ------- | -------------------------------------------------------------- |\n| config.validation.strict | false   | When enabled, startup fails if there are configuration errors. |\nWe recommended enabling strict validation.\nTelemetry\nQuestDB sends anonymous telemetry data with information about usage which helps\nus improve the product over time. We do not collect any personally-identifying\ninformation, and we do not share any of this data with third parties.\n| Property          | Default | Description                                           |\n| ----------------- | ------- | ----------------------------------------------------- |\n| telemetry.enabled | true    | Enable or disable anonymous usage metrics collection. |\nLogging\nThe logging behavior of QuestDB may be set in dedicated configuration files or\nby environment variables. This section describes how to configure logging using\nthese methods.\nConfiguration file\nLogs may be configured via a dedicated configuration file `log.conf`.\n```shell title=\"log.conf\"\nlist of configured writers\nwriters=file,stdout\nfile writer\nw.file.class=io.questdb.log.LogFileWriter\nw.file.location=questdb-debug.log\nw.file.level=INFO,ERROR\nrolling file writer\nw.file.class=io.questdb.log.LogRollingFileWriter\nw.file.location=${log.dir}/questdb-rolling.log.${date:yyyyMMdd}\nw.file.level=INFO,ERROR\nrollEvery accepts: day, hour, minute, month\nw.file.rollEvery=day\nrollSize specifies size at which to roll a new log file: a number followed by k, m, g (KB, MB, GB respectively)\nw.file.rollSize=128m\nlifeDuration accepts: a number followed by s, m, h, d, w, M, y for seconds, minutes, hours, etc.\nw.file.lifeDuration=1d\nsizeLimit is the max fileSize of the log directory. Follows same format as rollSize\nw.file.sizeLimit=1g\nstdout\nw.stdout.class=io.questdb.log.LogConsoleWriter\nw.stdout.level=INFO,ERROR\n```\nQuestDB will look for `/log.conf` first in `conf/` directory and then on the\nclasspath unless this name is overridden via a command line property:\n`-Dout=/something_else.conf`. QuestDB will create `conf/log.conf` using default\nvalues If `-Dout` is not set and file doesn't exist .\nOn Windows log messages go to depending on run mode :\n\ninteractive session - console and `$dataDir\\log\\stdout-%Y-%m-%dT%H-%M-%S.txt`\n  (default is `.\\log\\stdout-%Y-%m-%dT%H-%M-%S.txt` )\nservice - `$dataDir\\log\\service-%Y-%m-%dT%H-%M-%S.txt` (default is\n  `C:\\Windows\\System32\\qdbroot\\log\\service-%Y-%m-%dT%H-%M-%S.txt` )\n\nEnvironment variables\nValues in the log configuration file can be overridden with environment\nvariables. All configuration keys must be formatted as described in the\nenvironment variables section above.\nFor example, to set logging on `ERROR` level only:\n`shell title=\"Setting log level to ERROR in log-stdout.conf\"\nw.stdout.level=ERROR`\nThis can be passed as an environment variable as follows:\n`shell title=\"Setting log level to ERROR via environment variable\"\nexport QDB_LOG_W_STDOUT_LEVEL=ERROR`\nConfiguring Docker logging\nWhen mounting a volume to a Docker container, a logging configuration file may\nbe provided in the container located at `./conf/log.conf`. For example, a file\nwith the following contents can be created:\n```shell title=\"./conf/log.conf\"\nlist of configured writers\nwriters=file,stdout,http.min\nfile writer\nw.file.class=io.questdb.log.LogFileWriter\nw.file.location=questdb-docker.log\nw.file.level=INFO,ERROR,DEBUG\nstdout\nw.stdout.class=io.questdb.log.LogConsoleWriter\nw.stdout.level=INFO\nmin http server, used monitoring\nw.http.min.class=io.questdb.log.LogConsoleWriter\nw.http.min.level=ERROR\nw.http.min.scope=http-min-server\n```\nThe current directory can be mounted:\n`shell title=\"Mount the current directory to a QuestDB container\"\ndocker run -p 9000:9000 -v \"$(pwd):/var/lib/questdb/\" questdb/questdb`\nThe container logs will be written to disk using the logging level and file name\nprovided in the `./conf/log.conf` file, in this case in `./questdb-docker.log`.\nPrometheus Alertmanager\nQuestDB includes a log writer that sends any message logged at critical level\n(logger.critical(\"may-day\")) to Prometheus Alertmanager over a TCP/IP socket.\nDetails for configuring this can be found in the\nPrometheus documentation. To configure\nthis writer, add it to the `writers` config alongside other log writers.\n```ini title=\"log.conf\"\nWhich writers to enable\nwriters=stdout,alert\nstdout\nw.stdout.class=io.questdb.log.LogConsoleWriter\nw.stdout.level=INFO\nPrometheus Alerting\nw.alert.class=io.questdb.log.LogAlertSocketWriter\nw.alert.level=CRITICAL\nw.alert.location=/alert-manager-tpt.json\nw.alert.alertTargets=localhost:9093,localhost:9096,otherhost:9093\nw.alert.defaultAlertHost=localhost\nw.alert.defaultAlertPort=9093\nThe `inBufferSize` and `outBufferSize` properties are the size in bytes for the\nsocket write buffers.\nw.alert.inBufferSize=2m\nw.alert.outBufferSize=4m\nDelay in milliseconds between two consecutive attempts to alert when\nthere is only one target configured\nw.alert.reconnectDelay=250\n```\nOf all properties, only `w.alert.class` and `w.alert.level` are required, the\nrest assume default values as stated above (except for `w.alert.alertTargets`\nwhich is empty by default).\nAlert targets are specified using `w.alert.alertTargets` as a comma-separated\nlist of up to 12 `host:port` TCP/IP addresses. Specifying a port is optional and\ndefaults to the value of `defaultAlertHost`. One of these alert managers is\npicked at random when QuestDB starts, and a connection is created.\nAll alerts will be sent to the chosen server unless it becomes unavailable. If\nit is unavailable, the next server is chosen. If there is only one server\nconfigured and a fail-over cannot occur, a delay of 250 milliseconds is added\nbetween send attempts.\nThe `w.alert.location` property refers to the path (absolute, otherwise relative\nto `-d database-root`) of a template file. By default, it is a resource file\nwhich contains:\n`json title=\"/alert-manager-tpt.json\"\n[\n  {\n    \"Status\": \"firing\",\n    \"Labels\": {\n      \"alertname\": \"QuestDbInstanceLogs\",\n      \"service\": \"QuestDB\",\n      \"category\": \"application-logs\",\n      \"severity\": \"critical\",\n      \"version\": \"${QDB_VERSION}\",\n      \"cluster\": \"${CLUSTER_NAME}\",\n      \"orgid\": \"${ORGID}\",\n      \"namespace\": \"${NAMESPACE}\",\n      \"instance\": \"${INSTANCE_NAME}\",\n      \"alertTimestamp\": \"${date: yyyy/MM/ddTHH:mm:ss.SSS}\"\n    },\n    \"Annotations\": {\n      \"description\": \"ERROR/cl:${CLUSTER_NAME}/org:${ORGID}/ns:${NAMESPACE}/db:${INSTANCE_NAME}\",\n      \"message\": \"${ALERT_MESSAGE}\"\n    }\n  }\n]`\nFour environment variables can be defined, and referred to with the\n`${VAR_NAME}` syntax:\n\nORGID\nNAMESPACE\nCLUSTER_NAME\nINSTANCE_NAME\n\nTheir default value is `GLOBAL`, they mean nothing outside a cloud environment.\nIn addition, `ALERT_MESSAGE` is a placeholder for the actual `critical` message\nbeing sent, and `QDB_VERSION` is the runtime version of the QuestDB instance\nsending the alert. The `${date: <format>}` syntax can be used to produce a\ntimestamp at the time of sending the alert.\nDebug\nQuestDB logging can be quickly forced globally to `DEBUG` via either providing",
    "tag": "questdb"
  },
  {
    "title": "overview.md",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/clients/overview.md",
    "content": "\ntitle: Clients overview\ndescription: Introducing questdb ilp client libraries for different languages\n\nimport { ILPClientsTable } from '@theme/ILPClientsTable'\nILP clients allow fast data ingestion while abstracting you away from the details of the wire protocol. \nIt's the recommended way to ingest data into QuestDB. Clients also support authentication and TLS encryption.\n\nFor other languages, we have examples and a\n  protocol reference. Please let us know if you cannot find a client for your\nfavourite language! \n:::note\nILP clients are for data ingestion only. You cannot use an ILP client to query database. If you are looking for ways to query\nQuestDB then see this page. ",
    "tag": "questdb"
  },
  {
    "title": "Quickstart",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/clients/java_ilp.md",
    "content": "\ntitle: Java ILP client\ndescription: Introducing QuestDB Java ILP Client\n\n\nimport Tabs from \"@theme/Tabs\"\nimport TabItem from \"@theme/TabItem\"\nimport CodeBlock from \"@theme/CodeBlock\"\nimport InterpolateReleaseData from \"../../../src/components/InterpolateReleaseData\"\nimport { RemoteRepoExample } from '@theme/RemoteRepoExample'\n\nQuickstart\nAdd a QuestDB as a dependency to your build project:\n\n<Tabs\n  defaultValue=\"maven\"\n  values={[\n    { label: \"Maven\", value: \"maven\" },\n    { label: \"Gradle\", value: \"gradle\" },\n  ]}\n\n\n (\n        \n          {`<dependency>\n  <groupId>org.questdb</groupId>\n  <artifactId>questdb</artifactId>\n  <version>${release.name}</version>\n</dependency>`}\n        \n      )}\n    />\n  \n\n (\n        \n          {`compile group: 'org.questdb', name: 'questdb', version: '${release.name}'`}\n        \n      )}\n    />\n  \n\n\n\nThe code bellow creates an instance of a client, configures it to connect to a\nQuestDB server running on localhost on a TCP port 9009. Then it sends two rows,\neach with one symbol, long column and one string\ncolumn. It instructs the QuestDB server to assign a timestamp by using a local\nwall-clock.\n\nExample with TLS and Authentication enabled\nThis sample configures a client to use TLS encryption for a connection to a\nQuestDB server. It also instructs the client to authenticate.\n\nThis configuration also works with QuestDB Cloud.\nIf you are using a self-managed QuestDB with\nauthentication enabled then `authToken`\nis the `d` portion of a JWK.\nGeneral usage pattern\n\nCreate a client instance via `Sender.builder()`.\nUse `table(CharSequence)` to select a table for inserting a new row.\nUse `symbol(CharSequence, CharSequence)` to add all symbols. You must add\n   symbols before adding other column type.\n\nUse the following options to add all the remaining columns:\n\n\n`stringColumn(CharSequence, CharSequence)`\n\n`longColumn(CharSequence, long)`\n`doubleColumn(CharSequence, double)`\n`boolColumn(CharSequence, boolean)`\n\n`timestampColumn(CharSequence, long)`\n\n\nUse `at(long)` or `atNow()` to set a designated timestamp.\n\nOptionally: You can use `flush()` to send locally buffered data into a\n   server.\nGo to the step no. 2 to start a new row.\nUse `close()` to dispose the Sender after you no longer need it.\n\nDesignated timestamp considerations\nThe following options determine how a\ndesignated timestamp is assigned:\n\n`atNow()` automatically assigns a timestamp based on a server wall-clock upon\n  receiving a row.\n`at(long)` assigns a specific timestamp.\n\nThe code samples above use QuestDB to assign timestamp automatically: `atNow()`\ninstructs the server to assign a timestamp by using a local wall-clock. To\nassign a specific timestamp, use `at(long)` instead of `atNow()`.\n:::note\nQuestDB works best when rows are ingested in chronological order. This means\nrows with older timestamps are ingested before rows with newer timestamps.\nReceiving out-of-order data can have a performance impact:\n\n\nFor QuestDB 6.6 and later versions, the out-of-order data ingestion has been\n  optimized and automated.\n\n\nFor QuestDB 6.5.5 and earlier versions, users need to configure\n  Out-of-order commit lag setting.\n\n\n:::\nPlease note that the client does not interpret the timestamp in any way. It\nmerely passes the timestamp to a server, and it is up to the server to interpret\nthe timestamp. The default behavior of a QuestDB server is to treat the\ndesignated timestamp as a number of nanoseconds since 1st Jan 1970 UTC. See\nTimestamps for more details.\n:::caution\nBy default, QuestDB's engine treats a designated timestamp as nanoseconds, but\nthis does not mean that `System.nanoTime()` can be used to get the current time\nand pass it to `at(long)`. `System.nanoTime()` is only useful for measuring\nelapsed time, and it is not related to any notion of system or wall-clock time.\n:::\nOther considerations\n\nThe Sender is not thread-safe. For multiple threads to send data to QuestDB,\n  each thread should have its own Sender instance. An object pool can also be\n  used to re-use Sender instances.\nThe Sender instance has to be closed after it is no longer in use. The Sender\n  implements the `java.lang.AutoCloseable` interface, and therefore the\n  try-with-resource\n  pattern can be used to ensure that the Sender is closed.\nA client buffers row data internally and sends them to a server in batches\n  when the buffer is full. This improves performance significantly, but it also\n  means that during a period of quiescence, some written rows may be retained in\n  the buffer and are not sent to a server until the buffer are full.\nThe method `flush()` can be called to force sending the internal buffer to a\n  server, even when the buffer is not full yet.\n\nConfiguration\nThe client offers a builder API to configure all supported options. See\n`Sender.builder()`.\nLimitations",
    "tag": "questdb"
  },
  {
    "title": "~ (match) and !~ (does not match)",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/operators/pattern-matching.md",
    "content": "\ntitle: Pattern matching operators\nsidebar_label: Pattern matching\ndescription: Pattern matching operators reference documentation.\n\nThis page describes the available operators to assist with performing pattern\nmatching. For operators using regular expressions (`regex` in the syntax), QuestDB uses\nJava regular expression implementation.\n~ (match) and !~ (does not match)\n\n`(string) ~ (regex)` - returns true if the `string` value matches a regular expression, `regex`, otherwise returns false (case sensitive match).\n`(string) !~ (regex)` - returns true if the `string` value fails to match a regular expression, `regex`, otherwise returns false (case sensitive match).\n\nArguments\n\n`string` is an expression that evaluates to the `string` data type.\n`regex` is any regular expression pattern.\n\nReturn value\nReturn value type is `boolean`.\nLIKE/ILIKE\n\n`(string) LIKE (pattern)` - returns true if the `string` value matches `pattern`, otherwise returns false (case sensitive match).\n`(string) ILIKE (pattern)` - returns true if the `string` value matches `pattern`, otherwise returns false (case insensitive match).\n\nArguments\n\n`string` is an expression that evaluates to the `string` data type.\n`pattern` is a pattern which can contain wildcards like `_` and `%`.\n\nReturn value\nReturn value type is `boolean`.\nDescription\nIf the pattern doesn't contain wildcards, then the pattern represents the string itself.\nThe wildcards which can be used in pattern are interpreted as follows:\n- `_` - matches any single character.\n- `%` - matches any sequence of zero or more characters.\nWildcards can be used as follows:\n`questdb-sql\nSELECT 'quest' LIKE 'quest' ;\n-- Returns true\nSELECT 'quest' LIKE 'ques_';   \n-- Returns true   \nSELECT 'quest' LIKE 'que%';\n-- Returns true   \nSELECT 'quest' LIKE '_ues_';\n-- Returns true\nSELECT 'quest' LIKE 'q_'\n-- Returns false`\n`ILIKE` performs a case insensitive match as follows:\n`questdb-sql\nSELECT 'quest' ILIKE 'QUEST';\n-- Returns true\nSELECT 'qUeSt' ILIKE 'QUEST';\n-- Returns true\nSELECT 'quest' ILIKE 'QUE%';\n-- Returns true\nSELECT 'QUEST' ILIKE '_ues_';\n-- Returns true`\nExamples\nLIKE\n`questdb-sql \nSELECT * FROM trades\nWHERE symbol LIKE '%-USD'\nLATEST ON timestamp PARTITION BY symbol;`\n| symbol | side | price | amount | timestamp |\n| --- | --- | --- | --- | --- |\n| ETH-USD | sell | 1348.13 | 3.22455108 | 2022-10-04T15:25:58.834362Z |\n| BTC-USD | sell | 20082.08 | 0.16591219 | 2022-10-04T15:25:59.742552Z |\nILIKE\n`questdb-sql \nSELECT * FROM trades\nWHERE symbol ILIKE '%-usd'\nLATEST ON timestamp PARTITION BY symbol;`\n| symbol | side | price | amount | timestamp |\n| --- | --- | --- | --- | --- |\n| ETH-USD | sell | 1348.13 | 3.22455108 | 2022-10-04T15:25:58.834362Z |\n| BTC-USD | sell | 20082.08 | 0.16591219 | 2022-10-04T15:25:59.742552Z |\nregexp_replace\n`regexp_replace (string1, regex , string2 )` - provides substitution of new text\nfor substrings that match regular expression patterns.\nArguments:\n\n`string1` is a source `string` value to be manipulated.\n`regex` is a regular expression pattern.\n`string2` is any `string` value to replace part or the whole of the source value.\n\nReturn value\nReturn value type is `string`. The source string is returned unchanged if there is no match to the pattern. If\nthere is a match, the source string is returned with the replacement string\nsubstituted for the matching substring.\nExamples:\n`questdb-sql title=\"Example description -  regexp_replace\"\nSELECT regexp_replace('MYSQL is a great database', '^(\\S*)', 'QuestDB');`\n```\nQuestDB is a great database",
    "tag": "questdb"
  },
  {
    "title": "~ NOT",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/operators/bitwise.md",
    "content": "\ntitle: Bitwise operators\nsidebar_label: Bitwise\ndescription: Bitwise operators reference documentation.\n\nThis page describes the available operators to assist with performing bitwise\noperations on numeric values.\nPrecedence of these operators is as follows:\n\n`~` NOT\n`&` AND\n`^` XOR\n`|` OR\n\n~ NOT\n`~` is a unary operation that performs logical negation on each bit. Bits that\nare 0 become 1, and those that are 1 become 0. Expects a value of `long` or `int` type.\nExamples:\n`questdb-sql\nSELECT ~1024`\n| column |\n| ------ |\n| -1025  |\n& AND\n`&` is a binary operation that takes two equal-length binary representations and\nperforms the logical AND operation on each pair of the corresponding bits.\nExpects values of `long` or `int` type.\nExamples:\n`questdb-sql\nSELECT 5 & 3`\n| column |\n| ------ |\n| 1      |\n^ XOR\n`^` is a binary operation that takes two bit patterns of equal length and\nperforms the logical exclusive OR (XOR) operation on each pair of corresponding bits.\nExpects a value of `long` or `int` type.\nExamples:\n`questdb-sql\nSELECT 5 ^ 3`\n| column |\n| ------ |\n| 6      |\n| OR\n`|` is a binary operation that takes two bit patterns of equal length and\nperforms the logical inclusive OR operation on each pair of corresponding bits.\nExpects a value of `long` or `int` type.\nExamples:\n`questdb-sql\nSELECT 5 | 3`\n| column |\n| ------ |",
    "tag": "questdb"
  },
  {
    "title": "within",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/operators/spatial.md",
    "content": "\ntitle: Spatial operators\nsidebar_label: Spatial\ndescription: Spatial operators reference documentation.\n\nThis page describes the available operators to assist with performing spatial\ncalculations. For more information on this type of data, see the\ngeohashes documentation and the\nspatial functions which have been added to\nhelp with filtering and generating data.\nwithin\n`within(geohash, ...)` - evaluates if a comma-separated list of geohashes are\nequal to are within another geohash.\n:::info\n\n\nThe `within` operator can only be used in `LATEST BY` queries and all symbol\n  columns within the query must be indexed.\n\n\nOnly geohash literals (`#ezzn5kxb`) are supported as opposed to geohashes\n  passed as strings (`'ezzn5kxb'`).\n\n\n:::\nArguments:\n\n`geohash` is a geohash type in text or binary form\n\nReturn value:\n\nevaluates to `true` if geohash values are a prefix or complete match based on\n  the geohashes passed as arguments\n\nExamples:\nGiven a table with the following contents:\n| ts                          | device_id | g1c | g8c      |\n| --------------------------- | --------- | --- | -------- |\n| 2021-09-02T14:20:07.721444Z | device_2  | e   | ezzn5kxb |\n| 2021-09-02T14:20:08.241489Z | device_1  | u   | u33w4r2w |\n| 2021-09-02T14:20:08.241489Z | device_3  | u   | u33d8b1b |\nThe `within` operator can be used to filter results by geohash:\n`questdb-sql\nSELECT * FROM pos\nWHERE g8c within(#ezz, #u33d8)\nLATEST ON ts PARTITON BY uuid;`\nThis yields the following results:\n| ts                          | device_id | g1c | g8c      |\n| --------------------------- | --------- | --- | -------- |\n| 2021-09-02T14:20:07.721444Z | device_2  | e   | ezzn5kxb |\n| 2021-09-02T14:20:08.241489Z | device_3  | u   | u33d8b1b |\nAdditionally, prefix-like matching can be performed to evaluate if geohashes\nexist within a larger grid:\n`questdb-sql\nSELECT * FROM pos\nWHERE g8c within(#u33)\nLATEST ON ts PARTITON BY uuid;`\n| ts                          | device_id | g1c | g8c      |\n| --------------------------- | --------- | --- | -------- |\n| 2021-09-02T14:20:08.241489Z | device_1  | u   | u33w4r2w |",
    "tag": "questdb"
  },
  {
    "title": "abs",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/function/numeric.md",
    "content": "\ntitle: Numeric functions\nsidebar_label: Numeric\ndescription: Numeric function reference documentation.\n\nThis page describes the available functions to assist with performing numeric\ncalculations.\nabs\n`abs(value)` return the absolute value. The behavior of `abs` is as follows:\n\nWhen the input `value` is positive, `abs` returns `value`\nWhen the input `value` is negative, `abs` returns `- value`\nWhen the input `value` is `0`, `abs` returns `0`\n\nArguments:\n\n`value` is any numeric value.\n\nReturn value:\nReturn value type is the same as the type of the argument.\nExamples:\n`questdb-sql\nSELECT\n    x - 2 a,\n    abs(x -2)\nFROM long_sequence(3);`\n| a   | abs |\n| --- | --- |\n| -1  | 1   |\n| 0   | 0   |\n| 1   | 1   |\nlog\n`log(value)` return the natural logarithm (loge) of a given number.\nArguments:\n\n`value` is any numeric value.\n\nReturn value:\nReturn value type is `double`.\nExamples:\n`questdb-sql\nSELECT log(4.123)`\n| log          |\n| ------------ |\n| 1.4165810537 |\npower\n`power(base, exponent)` returns the value of a number `base` raised to the\npower defined by `exponent`.\nArguments:\n\n`base` is any numeric value.\n`exponent` is any numeric value.\n\nReturn value:\nReturn value type is `double`.\nExamples:\n`questdb-sql\nSELECT power(2, 3);`\n| power |\n| ----- |\n| 8     |\nround\n`round(value, scale)` returns the closest value in the specified scale. It\nuses the \"half up\" tie-breaking method when the value is exactly halfway between\nthe `round_up` and `round_down` values.\nArguments:\n\n`value` is any numeric value.\n`scale` is the number of decimal points returned. A negative scale means the\n  rounding will occur to a digit to the left of the decimal point. For example,\n  -1 means the number will be rounded to the nearest tens and +1 to the nearest\n  tenths.\n\nReturn value:\nReturn value type is `double`.\nExamples:\n`questdb-sql\nSELECT\n    d,\n    round(d, -2),\n    round(d, -1),\n    round(d,0),\n    round(d,1),\n    round(d,2)\nFROM dbl;`\n| d            | round-2 | round-1 | round0 | round1 | round2  |\n| ------------ | ------- | ------- | ------ | ------ | ------- |\n| -0.811905406 | 0       | 0       | -1     | -0.8   | -0.81   |\n| -5.002768547 | 0       | -10     | -5     | -5     | -5      |\n| -64.75487334 | -100    | -60     | -65    | -64.8  | -64.75  |\n| -926.531695  | -900    | -930    | -927   | -926.5 | -926.53 |\n| 0.069361448  | 0       | 0       | 0      | 0.1    | 0.07    |\n| 4.003627053  | 0       | 0       | 4      | 4      | 4       |\n| 86.91359825  | 100     | 90      | 87     | 86.9   | 86.91   |\n| 376.3807766  | 400     | 380     | 376    | 376.4  | 376.38  |\nround_down\n`round_down(value, scale)` - rounds a value down to the specified scale\nArguments:\n\n`value` is any numeric value.\n`scale` is the number of decimal points returned. A negative scale means the\n  rounding will occur to a digit to the left of the decimal point. For example,\n  -1 means the number will be rounded to the nearest tens and +1 to the nearest\n  tenths.\n\nReturn value:\nReturn value type is `double`.\nExamples:\n`questdb-sql\nSELECT\n    d,\n    round_down(d, -2),\n    round_down(d, -1),\n    round_down(d,0),\n    round_down(d,1),\n    round_down(d,2)\nFROM dbl;`\n| d            | r_down-2 | r_down-1 | r_down0 | r_down1 | r_down2 |\n| ------------ | -------- | -------- | ------- | ------- | ------- |\n| -0.811905406 | 0        | 0        | 0       | -0.8    | -0.81   |\n| -5.002768547 | 0        | 0        | -5      | -5      | -5      |\n| -64.75487334 | 0        | -60      | -64     | -64.7   | -64.75  |\n| -926.531695  | -900     | -920     | -926    | -926.5  | -926.53 |\n| 0.069361448  | 0        | 0        | 0       | 0       | 0.06    |\n| 4.003627053  | 0        | 0        | 4       | 4       | 4       |\n| 86.91359825  | 0        | 80       | 86      | 86.9    | 86.91   |\n| 376.3807766  | 400      | 370      | 376     | 376.3   | 376.38  |\nround_half_even\n`round_half_even(value, scale)` - returns the closest value in the specified\nscale. It uses the \"half up\" tie-breaking method when the value is exactly\nhalfway between the `round_up` and `round_down` values.\nArguments:\n\n`value` is any numeric value.\n`scale` is the number of decimal points returned. A negative scale means the\n  rounding will occur to a digit to the left of the decimal point. For example,\n  -1 means the number will be rounded to the nearest tens and +1 to the nearest\n  tenths.\n\nReturn value:\nReturn value type is `double`.\nExamples:\n`questdb-sql title=\"Tie-breaker behavior\"\nSELECT\n    round_half_even(5.55, 1),\n    round_half_even(5.65, 1)\nFROM long_sequence(1);`\n| round_half_even | round_half_even |\n| --------------- | --------------- |\n| 5.6             | 5.6             |\n`questdb-sql title=\"More examples\"\nSELECT\n    d,\n    round_half_even(d, -2),\n    round_half_even(d, -1),\n    round_half_even(d,0),\n    round_half_even(d,1),\n    round_half_even(d,2)\nFROM dbl;`\n| d            | r_h_e-2 | r_h_e-1 | r_h_e0 | r_h_e1 | r_h_e2  |\n| ------------ | ------- | ------- | ------ | ------ | ------- |\n| -0.811905406 | 0       | 0       | -1     | -0.8   | -0.81   |\n| -5.002768547 | 0       | 0       | -5     | -5     | -5      |\n| -64.75487334 | -100    | -60     | -65    | -64.8  | -64.75  |\n| -926.531695  | -900    | -930    | -927   | -926.5 | -926.53 |\n| 0.069361448  | 0       | 0       | 0      | 0.1    | 0.07    |\n| 4.003627053  | 0       | 0       | 4      | 4      | 4       |\n| 86.91359825  | 100     | 90      | 87     | 86.9   | 86.91   |\n| 376.3807766  | 400     | 380     | 376    | 376.4  | 376.38  |\nround_up\n`round_up(value, scale)` - rounds a value up to the specified scale\nArguments:\n\n`value` is any numeric value.\n`scale` is the number of decimal points returned. A negative scale means the\n  rounding will occur to a digit to the left of the decimal point. For example,\n  -1 means the number will be rounded to the nearest tens and +1 to the nearest\n  tenths.\n\nReturn value:\nReturn value type is `double`.\nExamples:\n`questdb-sql\nSELECT\n    d,\n    round_up(d, -2),\n    round_up(d, -1),\n    round_up(d,0),\n    round_up(d,1),\n    round_up(d,2)\nFROM dbl;`\n| d            | r_up-2 | r_up-1 | r_up0 | r_up1  | r_up2   |\n| ------------ | ------ | ------ | ----- | ------ | ------- |\n| -0.811905406 | -100   | -10    | -1    | -0.9   | -0.82   |\n| -5.002768547 | -100   | -10    | -6    | -5.1   | -5.01   |\n| -64.75487334 | -100   | -70    | -65   | -64.8  | -64.76  |\n| -926.531695  | -1000  | -930   | -927  | -926.6 | -926.54 |\n| 0.069361448  | 100    | 10     | 1     | 0.1    | 0.07    |\n| 4.003627053  | 100    | 10     | 5     | 4.1    | 4.01    |\n| 86.91359825  | 100    | 90     | 87    | 87     | 86.92   |\n| 376.3807766  | 400    | 380    | 377   | 376.4  | 376.39  |\nsqrt\n`sqrt(value)` return the square root of a given number.\nArguments:\n\n`value` is any numeric value.\n\nReturn value:\nReturn value type is `double`.\nExamples:\n`questdb-sql\nSELECT sqrt(4000.32)`\n| log              |\n| ---------------- |",
    "tag": "questdb"
  },
  {
    "title": "concat",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/function/text.md",
    "content": "\ntitle: Text functions\nsidebar_label: Text\ndescription: Text function reference documentation.\n\nThis page describes the available functions to assist with performing text\nmanipulation such as concatenation, case conversion, and string length\ncalculation.\nconcat\n`concat(str, ...)` - concatenates a string from one or more input values.\n`questdb-sql title=\"Example\"\nSELECT firstName, lastName, concat(firstName, ' ', lastName) FROM names;`\n| firstName | lastName | concat        |\n| --------- | -------- | ------------- |\n| Tim       | Thompson | Tim Thompson  |\n| Anna      | Thompson | Anna Thompson |\n| Anna      | Mason    | Anna Mason    |\n| Tom       | Johnson  | Tom Johnson   |\n| Tim       | Smith    | Tim Smith     |\n:::tip\n`concat()` can be used to generate `line protocol`. See an example below.\n:::\n`questdb-sql title=\"Generating line protocol\"\nSELECT\nconcat(\n    'trades,instrument=', rnd_str(2,2,0),\n    ',side=', rnd_str('B', 'S'),\n    ' price=', abs(cast(rnd_double(0)*100000 AS INT)),\n    ',quantity=', abs(cast(rnd_double(0)*10000 AS INT)),\n    ' ',\n    1571270400000 + (x-1) * 100\n)\nFROM long_sequence(5) x;`\n`title=\"Result\"\ntrades,instrument=CR,side=B price=70867,quantity=9192 1571270400000\ntrades,instrument=LN,side=S price=37950,quantity=1439 1571270400100\ntrades,instrument=ZJ,side=S price=82829,quantity=8871 1571270400200\ntrades,instrument=EW,side=S price=10427,quantity=1945 1571270400300\ntrades,instrument=MI,side=B price=99348,quantity=8450 1571270400400`\nlength\n`length(string)` - reads length of `string` value type (result is `int`)\n`length(symbol)` - reads length of `symbol` value type (result is `int`)\n`length(blob)` - reads length of `binary` value type (result is `long`)\n\na `string`\na `symbol`\na `binary` blob\n\n`questdb-sql title=\"Example\"\nSELECT name a, length(name) b FROM names limit 4`\n| a      | b   |\n| ------ | --- |\n| AARON  | 5   |\n| AMELIE | 6   |\n| TOM    | 3   |\n| null   | -1  |\nleft\n`left(string, count)` - extracts a substring of the given length from a string\n(starting from left).\nArguments:\n\n`string` is a string to extract from.\n`count` is an integer specifying the count of characters to be extracted into\n  a substring.\n\nReturn value:\nReturns a string with the extracted characters.\nExamples:\n`questdb-sql title=\"Example\"\nSELECT name, left('Thompson', 3) l FROM names LIMIT 3`\n| name   | l   |\n| ------ | --- |\n| AARON  | AAR |\n| AMELIE | AME |\n| TOM    | TOM |\nright\n`right(string, count)` - extracts a substring of the given length from a string\n(starting from right).\nArguments:\n\n`string` is a string to extract from.\n`count` is an integer specifying the count of characters to be extracted into\n  a substring.\n\nReturn value:\nReturns a string with the extracted characters.\nExamples:\n`questdb-sql title=\"Example\"\nSELECT name, right('Thompson', 2) r FROM names LIMIT 3`\n| name   | l   |\n| ------ | --- |\n| AARON  | ON  |\n| AMELIE | IE  |\n| TOM    | OM  |\nstrpos / position\n`strpos(string, substring)` or `position(string, substring)` - searches for the first substring occurrence in a string, and returns\nthe index position of the starting character. If the substring is not found, this function returns `0`. The\nperformed search is case-sensitive.\nArguments:\n\n`string` is a string to search in.\n`substring` is a string to search for.\n\nReturn value:\nReturns an integer for the substring position. Positions start from `1`.\nExamples:\n```questdb-sql title=\"Example\"\nSELECT name, strpos(name, 'Thompson') idx \nFROM full_names \nLIMIT 4;\n-- This is equal to:\nSELECT name, position(name, 'Thompson') idx \nFROM full_names \nLIMIT 4;\n```\n| name          | idx |\n| ------------- | --- |\n| Tim Thompson  | 5   |\n| Anna Thompson | 6   |\n| Anna Mason    | 0   |\n| Tom Johnson   | 0   |\nAssuming we have a table `example_table` with a single string type column `col`:\n| col        |\n| ---------- |\n| apple,pear |\n| cat,dog    |\n| ...        |\nAs a more advanced example, we can use `strpos()` or `position()` to split the string values of\n`col`, in this case splitting at the comma character, `,` . By using\n`left()`/`right()` functions, we can choose the string values at the left and\nright of the comma:\n```questdb-sql title=\"Splitting string into two separate columns\"\nSELECT col,\n       left(col, strpos(col, ',') - 1) as col1,\n       right(col, length(col) - strpos(col, ',')) as col2\nFROM example_table;\n-- This is equal to:\nSELECT col,\n       left(col, position(col, ',') - 1) as col1,\n       right(col, length(col) - position(col, ',')) as col2\nFROM example_table;\n```\n| col        | col1  | col2 |\n| ---------- | ----- | ---- |\n| apple,pear | apple | pear |\n| cat,dog    | cat   | dog  |\nsubstring\n`substring(string, start, length)` - extracts a substring from the given string.\nArguments:\n\n`string` is a string to extract from.\n`start` is an integer specifying the position of the first character to be\n  extracted. Positions start from `1`.\n`length` is an integer specifying the count of characters to be extracted.\n  Should be non-negative.\n\nReturn value:\nReturns a string with the extracted characters. If any part the arguments is\n`null`, the function returns `null`.\nExamples:\n`questdb-sql title=\"Example\"\nSELECT id, substring(id, 1, 2) country FROM orders LIMIT 3`\n| id              | country |\n| --------------- | ------- |\n| UK2022072619373 | UK      |\n| UK2022072703162 | UK      |\n| US2022072676246 | US      |\nIf the `start` argument is negative, the output depends on the value of\n`start+length`:\n\nIf `start+length` is greater than 1, the substring stops at position\n  `start+length - 1`.\nIf `start+length` is zero, the output is empty string.\nIf `start+length` is less than zero, the output is `null`.\n\n`questdb-sql title=\"Example\"\nSELECT substring('Lorem ipsum dolor sit amet', -5, 9)`\n| substring |\n| --------- |\n| Lor       |\nto_lowercase / lower\n\n`to_lowercase(string)` or `lower(string)` - converts all upper case string\n  characters to lowercase\n\nArguments\n`string` is the input strong to be converted.\nReturn value\nReturn value type is `string`.\nExamples\n`questdb-sql\nSELECT lower('questDB');\n-- This is equal to:\nSELECT to_lowercase('questDB');`\n| to_lowercase |\n| ------------ |\n| questdb      |\nto_uppercase / upper\n\n`to_uppercase(string)` or `upper(string)` - converts all lower case string\n  characters to uppercase\n\nArguments\n`string` is the input strong to be converted.\nReturn value\nReturn value type is `string`.\nExamples\n`questdb-sql\nSELECT upper('questDB');\n-- This is equal to:\nSELECT to_uppercase('questDB');`\n| to_uppercase |\n| ------------ |",
    "tag": "questdb"
  },
  {
    "title": "systimestamp",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/function/date-time.md",
    "content": "\ntitle: Date and time functions\nsidebar_label: Date and time\ndescription: Date and time functions reference documentation.\n\nThis page describes the available functions to assist with performing time-based\ncalculations.\n:::info\nChecking if tables contain a designated timestamp column can be done via the\n`tables()` and `table_columns()` functions which are described in the\nmeta functions documentation page.\n:::\nsystimestamp\n`systimestamp()` - offset from UTC Epoch in microseconds. Calculates\n`UTC timestamp` using system's real time clock. The value is affected by\ndiscontinuous jumps in the system time (e.g., if the system administrator\nmanually changes the system time).\n:::warning\n`systimestamp()` value can change within the query execution timeframe and \nshould not be used in WHERE clause to filter designated timestamp column.\nPlease use `now()` instead .  \n:::\nArguments:\n\n`systimestamp()` does not accept arguments.\n\nReturn value:\nReturn value type is `timestamp`.\nExamples:\n`questdb-sql title=\"Insert current system timestamp\"\nINSERT INTO readings\nVALUES(systimestamp(), 123.5);`\n| ts                          | reading |\n| :-------------------------- | :------ |\n| 2020-01-02T19:28:48.727516Z | 123.5   |\nsysdate\n`sysdate()` - returns the timestamp of the host system as a `date` with\n`millisecond` precision.\nCalculates `UTC date` with millisecond precision using system's real time clock.\nThe value is affected by discontinuous jumps in the system time (e.g., if the\nsystem administrator manually changes the system time).\n:::warning\n`sysdate()` value can change within the query execution timeframe and\nshould not be used in WHERE clause to filter designated timestamp column.\nPlease use `now()` instead .\n:::\nArguments:\n\n`sysdate()` does not accept arguments.\n\nReturn value:\nReturn value type is `date`.\nExamples:\n`questdb-sql title=\"Insert current system date along with a value\"\nINSERT INTO readings\nVALUES(sysdate(), 123.5);`\n| sysdate                     | reading |\n| :-------------------------- | :------ |\n| 2020-01-02T19:28:48.727516Z | 123.5   |\n`questdb-sql title=\"Query based on last minute\"\nSELECT * FROM readings\nWHERE date_time > sysdate() - 60000000L;`\nnow\n`now()` - offset from UTC Epoch in microseconds.\nCalculates `UTC timestamp` using system's real time clock. Unlike\n`systimestamp()`, it does not change within the query execution timeframe and\nshould be used in WHERE clause to filter designated timestamp column relative to\ncurrent time, i.e.:\n\n`SELECT now() FROM long_sequence(200)` will return the same timestamp for all\n  rows\n`SELECT systimestamp() FROM long_sequence(200)` will have new timestamp values\n  for each row\n\nArguments:\n\n`now()` does not accept arguments.\n\nReturn value:\nReturn value type is `timestamp`.\nExamples:\n`questdb-sql title=\"Filter records to created within last day\"\nSELECT created, origin FROM telemetry\nWHERE created > dateadd('d', -1, now());`\n| created                     | origin |\n| :-------------------------- | :----- |\n| 2021-02-01T21:51:34.443726Z | 1      |\n`questdb-sql title=\"Query returns same timestamp in every row\"\nSELECT now() FROM long_sequence(3)`\n| now                         |\n| :-------------------------- |\n| 2021-02-01T21:51:34.443726Z |\n| 2021-02-01T21:51:34.443726Z |\n| 2021-02-01T21:51:34.443726Z |\n`questdb-sql title=\"Query based on last minute\"\nSELECT * FROM readings\nWHERE date_time > now() - 60000000L;`\ntimestamp_ceil\n`timestamp_ceil(unit, timestamp)` - performs a ceiling calculation on a\ntimestamp by given unit.\nA unit must be provided to specify which granularity to perform rounding.\nArguments:\n`timestamp_ceil(unit, timestamp)` has the following arguments:\n`unit` - may be one of the following:\n\n`T` milliseconds\n`s` seconds\n`m` minutes\n`h` hours\n`d` days\n`M` months\n`y` year\n\n`timestamp` - any timestamp value\nReturn value:\nReturn value type is `timestamp`.\nExamples:\n`questdb-sql\nWITH t AS (SELECT cast('2016-02-10T16:18:22.862145Z' AS timestamp) ts)\nSELECT\n  ts,\n  timestamp_ceil('T', ts) c_milli,\n  timestamp_ceil('s', ts) c_second,\n  timestamp_ceil('m', ts) c_minute,\n  timestamp_ceil('h', ts) c_hour,\n  timestamp_ceil('d', ts) c_day,\n  timestamp_ceil('M', ts) c_month,\n  timestamp_ceil('y', ts) c_year\n  FROM t`\n| ts                          | c_milli                     | c_second                    | c_minute                    | c_hour                      | c_day                       | c_month                     | c_year                       |\n| :-------------------------- | :-------------------------- | :-------------------------- | :-------------------------- | :-------------------------- | :-------------------------- | :-------------------------- | :--------------------------- |\n| 2016-02-10T16:18:22.862145Z | 2016-02-10T16:18:22.863000Z | 2016-02-10T16:18:23.000000Z | 2016-02-10T16:19:00.000000Z | 2016-02-10T17:00:00.000000Z | 2016-02-11T00:00:00.000000Z | 2016-03-01T00:00:00.000000Z | 2017-01-01T00:00:00.000000Z\" |\ntimestamp_floor\n`timestamp_floor(unit, timestamp)` - performs a floor calculation on a timestamp\nby given unit.\nA unit must be provided to specify which granularity to perform rounding.\nArguments:\n`timestamp_floor(unit, timestamp)` has the following arguments:\n`unit` - may be one of the following:\n\n`T` milliseconds\n`s` seconds\n`m` minutes\n`h` hours\n`d` days\n`M` months\n`y` year\n\n`timestamp` - any timestamp value\nReturn value:\nReturn value type is `timestamp`.\nExamples:\n`questdb-sql\nWITH t AS (SELECT cast('2016-02-10T16:18:22.862145Z' AS timestamp) ts)\nSELECT\n  ts,\n  timestamp_floor('T', ts) f_milli,\n  timestamp_floor('s', ts) f_second,\n  timestamp_floor('m', ts) f_minute,\n  timestamp_floor('h', ts) f_hour,\n  timestamp_floor('d', ts) f_day,\n  timestamp_floor('M', ts) f_month,\n  timestamp_floor('y', ts) f_year\n  FROM t`\n| ts                          | f_milli                     | f_second                    | f_minute                    | f_hour                      | f_day                       | f_month                     | f_year                      |\n| :-------------------------- | :-------------------------- | :-------------------------- | :-------------------------- | :-------------------------- | :-------------------------- | :-------------------------- | :-------------------------- |\n| 2016-02-10T16:18:22.862145Z | 2016-02-10T16:18:22.862000Z | 2016-02-10T16:18:22.000000Z | 2016-02-10T16:18:00.000000Z | 2016-02-10T16:00:00.000000Z | 2016-02-10T00:00:00.000000Z | 2016-02-01T00:00:00.000000Z | 2016-01-01T00:00:00.000000Z |\nto_timestamp\n`to_timestamp(string, format)` - converts `string` to `timestamp` by using the\nsupplied `format` to extract the value with microsecond precision.\nWhen the `format` definition does not match the `string` input, the result will\nbe `null`.\nFor more information about recognized timestamp formats, see the\ndate and timestamp format section.\nArguments:\n\n`string` is any string that represents a date and/or time.\n`format` is a string that describes the timestamp format in which `string` is\n  expressed.\n\nReturn value:\nReturn value type is `timestamp`. QuestDB provides `timestamp` with microsecond\nresolution. Input strings with nanosecond precision will be parsed but lose the\nprecision.\nExamples:\n`questdb-sql title=\"Pattern matching with microsecond precision\"\nSELECT to_timestamp('2020-03-01:15:43:21.127329', 'yyyy-MM-dd:HH:mm:ss.SSSUUU')\nFROM long_sequence(1);`\n| to_timestamp                |\n| :-------------------------- |\n| 2020-03-01T15:43:21.127329Z |\n`questdb-sql title=\"Precision loss when pattern matching with nanosecond precision\"\nSELECT to_timestamp('2020-03-01:15:43:00.000000001Z', 'yyyy-MM-dd:HH:mm:ss.SSSUUUNNNZ')\nFROM long_sequence(1);`\n| to_timestamp                |\n| :-------------------------- |\n| 2020-03-01T15:43:00.000000Z |\n`questdb-sql title=\"String does not match format\"\nSELECT to_timestamp('2020-03-01:15:43:21', 'yyyy')\nFROM long_sequence(1);`\n| to_timestamp |\n| :----------- |\n| null         |\n`questdb-sql title=\"Using with INSERT\"\nINSERT INTO measurements\nvalues(to_timestamp('2019-12-12T12:15', 'yyyy-MM-ddTHH:mm'), 123.5);`\n| timestamp                   | value |\n| :-------------------------- | :---- |\n| 2019-12-12T12:15:00.000000Z | 123.5 |\nNote that conversion of ISO timestamp format is optional. QuestDB automatically\nconverts `string` to `timestamp` if it is a partial or full form of\n`yyyy-MM-ddTHH:mm:ss.SSSUUU` or `yyyy-MM-dd HH:mm:ss.SSSUUU` with a valid time\noffset, `+01:00` or `Z`. See more examples in\nNative timestamp\nformat](/docs/reference/sql/where#native-timestamp-format).\nto_date\n`to_date(string, format)` - converts string to `date` by using the supplied\n`format` to extract the value.\nWill convert a `string` to `date` using the format definition passed as an\nargument. When the `format` definition does not match the `string` input, the\nresult will be `null`.\nFor more information about recognized timestamp formats, see the\ndate and timestamp format section.\nArguments:\n\n`string` is any string that represents a date and/or time.\n`format` is a string that describes the `date format` in which `string` is\n  expressed.\n\nReturn value:\nReturn value type is `date`\nExamples:\n`questdb-sql title=\"string matches format\"\nSELECT to_date('2020-03-01:15:43:21', 'yyyy-MM-dd:HH:mm:ss')\nFROM long_sequence(1);`\n| to_date                  |\n| :----------------------- |\n| 2020-03-01T15:43:21.000Z |\n`questdb-sql title=\"string does not match format\"\nSELECT to_date('2020-03-01:15:43:21', 'yyyy')\nFROM long_sequence(1);`\n| to_date |\n| :------ |\n| null    |\n`questdb-sql title=\"Using with INSERT\"\nINSERT INTO measurements\nvalues(to_date('2019-12-12T12:15', 'yyyy-MM-ddTHH:mm'), 123.5);`\n| date                     | value |\n| :----------------------- | :---- |\n| 2019-12-12T12:15:00.000Z | 123.5 |\nto_str\n`to_str(value, format)` - converts date or timestamp value to a string in the\nspecified format\nWill convert a date or timestamp value to a string using the format definition\npassed as an argument. When elements in the `format` definition are\nunrecognized, they will be passed-through as string.\nFor more information about recognized timestamp formats, see the\ndate and timestamp format section.\nArguments:\n\n`value` is any `date` or `timestamp`\n`format` is a timestamp format.\n\nReturn value:\nReturn value type is `string`\nExamples:\n\nBasic example\n\n`questdb-sql\nSELECT to_str(systimestamp(), 'yyyy-MM-dd') FROM long_sequence(1);`\n| to_str     |\n| :--------- |\n| 2020-03-04 |\n\nWith unrecognized timestamp definition\n\n`questdb-sql\nSELECT to_str(systimestamp(), 'yyyy-MM-dd gooD DAY 123') FROM long_sequence(1);`\n| to_str                  |\n| :---------------------- |\n| 2020-03-04 gooD DAY 123 |\nto_timezone\n`to_timezone(timestamp, timezone)` - converts a timestamp value to a specified\ntimezone. For more information on the time zone database used for this function,\nsee the\nQuestDB time zone database documentation.\nArguments:\n\n`timestamp` is any `timestamp` as Unix timestamp or string equivalent\n`timezone` may be `Country/City` tz database name, time zone abbreviation such\n  as `PST` or in UTC offset in string format.\n\nReturn value:\nReturn value type is `timestamp`\nExamples:\n\nUnix UTC timestamp in microseconds to `Europe/Berlin`\n\n`questdb-sql\nSELECT to_timezone(1623167145000000, 'Europe/Berlin')`\n| to_timezone                 |\n| :-------------------------- |\n| 2021-06-08T17:45:45.000000Z |\n\nUnix UTC timestamp in microseconds to PST by UTC offset\n\n`questdb-sql\nSELECT to_timezone(1623167145000000, '-08:00')`\n| to_timezone                 |\n| :-------------------------- |\n| 2021-06-08T07:45:45.000000Z |\n\nTimestamp as string to `PST`\n\n`questdb-sql\nSELECT to_timezone('2021-06-08T13:45:45.000000Z', 'PST')`\n| to_timezone                 |\n| :-------------------------- |\n| 2021-06-08T06:45:45.000000Z |\nto_utc\n`to_utc(timestamp, timezone)` - converts a timestamp by specified timezone to\nUTC. May be provided a timezone in string format or a UTC offset in hours and\nminutes. For more information on the time zone database used for this function,\nsee the\nQuestDB time zone database documentation.\nArguments:\n\n`timestamp` is any `timestamp` as Unix timestamp or string equivalent\n`timezone` may be `Country/City` tz database name, time zone abbreviation such\n  as `PST` or in UTC offset in string format.\n\nReturn value:\nReturn value type is `timestamp`\nExamples:\n\nConvert a Unix timestamp in microseconds from the `Europe/Berlin` timezone to\n  UTC\n\n`questdb-sql\nSELECT to_utc(1623167145000000, 'Europe/Berlin')`\n| to_utc                      |\n| :-------------------------- |\n| 2021-06-08T13:45:45.000000Z |\n\nUnix timestamp in microseconds from PST to UTC by UTC offset\n\n`questdb-sql\nSELECT to_utc(1623167145000000, '-08:00')`\n| to_utc                      |\n| :-------------------------- |\n| 2021-06-08T23:45:45.000000Z |\n\nTimestamp as string in `PST` to UTC\n\n`questdb-sql\nSELECT to_utc('2021-06-08T13:45:45.000000Z', 'PST')`\n| to_utc                      |\n| :-------------------------- |\n| 2021-06-08T20:45:45.000000Z |\ndateadd\n`dateadd(period, n, startDate)` - adds `n` `period` to `startDate`.\nArguments:\n\n`period` is a char. Period to be added. Available periods are `s`, `m`, `h`,\n  `d`, `M`, `y`.\n`n` is an int. Number of periods to add.\n`startDate` is a timestamp or date. Timestamp to add the periods to.\n\nReturn value:\nReturn value type is `timestamp`\nExamples:\n`questdb-sql title=\"Adding hours\"\nSELECT systimestamp(), dateadd('h', 2, systimestamp())\nFROM long_sequence(1);`\n| systimestamp                | dateadd                     |\n| :-------------------------- | :-------------------------- |\n| 2020-04-17T00:30:51.380499Z | 2020-04-17T02:30:51.380499Z |\n`questdb-sql title=\"Adding days\"\nSELECT systimestamp(), dateadd('d', 2, systimestamp())\nFROM long_sequence(1);`\n| systimestamp                | dateadd                     |\n| :-------------------------- | :-------------------------- |\n| 2020-04-17T00:30:51.380499Z | 2020-04-19T00:30:51.380499Z |\n`questdb-sql title=\"Adding months\"\nSELECT systimestamp(), dateadd('M', 2, systimestamp())\nFROM long_sequence(1);`\n| systimestamp                | dateadd                     |\n| :-------------------------- | :-------------------------- |\n| 2020-04-17T00:30:51.380499Z | 2020-06-17T00:30:51.380499Z |\ndatediff\n`datediff(period, date1, date2)` - returns the absolute number of `period`\nbetween `date1` and `date2`.\nArguments:\n\n`period` is a char. Period to be added. Available periods are `s`, `m`, `h`,\n  `d`, `M`, `y`.\n`date1` and `date2` are date or timestamp. Dates to compare\n\nReturn value:\nReturn value type is `int`\nExamples:\n`questdb-sql title=\"Difference in days\"\nSELECT datediff(\n    'd',\n    to_timestamp('2020-01-23','yyyy-MM-dd'),\n    to_timestamp('2020-01-27','yyyy-MM-dd'))\nFROM long_sequence(1);`\n| datediff |\n| :------- |\n| 4        |\n`questdb-sql title=\"Difference in months\"\nSELECT datediff(\n    'M',\n    to_timestamp('2020-01-23','yyyy-MM-dd'),\n    to_timestamp('2020-02-24','yyyy-MM-dd'))\nFROM long_sequence(1);`\n| datediff |\n| :------- |\n| 1        |\nmillis\n`millis(value)` - returns the `millis` of the second for a given date or\ntimestamp from `0` to `999`\nArguments:\n\n`value` is any `timestamp` or `date`\n\nReturn value:\nReturn value type is `int`\nExamples:\n`questdb-sql title=\"Millis of the second\"\nSELECT millis(\n    to_timestamp('2020-03-01:15:43:21.123456', 'yyyy-MM-dd:HH:mm:ss.SSSUUU'))\nFROM long_sequence(1);`\n| millis |\n| :----- |\n| 123    |\n`questdb-sql title=\"Parsing 3 digits when no unit is added after S\"\nSELECT millis(to_timestamp('2020-03-01:15:43:21.123', 'yyyy-MM-dd:HH:mm:ss.S'))\nFROM long_sequence(1);`\n| millis |\n| :----- |\n| 123    |\n`questdb-sql title=\"Using in an aggregation\"\nSELECT millis(ts), count() FROM transactions;`\n| second | count |\n| :----- | :---- |\n| 0      | 2323  |\n| 1      | 6548  |\n| ...    | ...   |\n| 998    | 9876  |\n| 999    | 2567  |\nmicros\n`micros(value)` - returns the `micros` of the millisecond for a given date or\ntimestamp from `0` to `999`\nArguments:\n\n`value` is any `timestamp` or `date`\n\nReturn value:\nReturn value type is `int`\nExamples:\n`questdb-sql title=\"Micros of the second\"\nSELECT micros(to_timestamp('2020-03-01:15:43:21.123456', 'yyyy-MM-dd:HH:mm:ss.SSSUUU'))\nFROM long_sequence(1);`\n| millis |\n| :----- |\n| 456    |\n`questdb-sql title=\"Parsing 3 digits when no unit is added after U\"\nSELECT micros(to_timestamp('2020-03-01:15:43:21.123456', 'yyyy-MM-dd:HH:mm:ss.SSSU'))\nFROM long_sequence(1);`\n| millis |\n| :----- |\n| 456    |\n`questdb-sql title=\"Using in an aggregation\"\nSELECT micros(ts), count() FROM transactions;`\n| second | count |\n| :----- | :---- |\n| 0      | 2323  |\n| 1      | 6548  |\n| ...    | ...   |\n| 998    | 9876  |\n| 999    | 2567  |\nsecond\n`second(value)` - returns the `second` of the minute for a given date or\ntimestamp from `0` to `59`\nArguments:\n\n`value` is any `timestamp` or `date`\n\nReturn value:\nReturn value type is `int`\nExamples:\n`questdb-sql title=\"Second of the minute\"\nSELECT second(to_timestamp('2020-03-01:15:43:21', 'yyyy-MM-dd:HH:mm:ss'))\nFROM long_sequence(1);`\n| second |\n| :----- |\n| 43     |\n`questdb-sql title=\"Using in an aggregation\"\nSELECT second(ts), count() FROM transactions;`\n| second | count |\n| :----- | :---- |\n| 0      | 2323  |\n| 1      | 6548  |\n| ...    | ...   |\n| 58     | 9876  |\n| 59     | 2567  |\nminute\n`minute(value)` - returns the `minute` of the hour for a given date or timestamp\nfrom `0` to `59`\nArguments:\n\n`value` is any `timestamp` or `date`\n\nReturn value:\nReturn value type is `int`\nExamples:\n`questdb-sql title=\"Minute of the hour\"\nSELECT minute(to_timestamp('2020-03-01:15:43:21', 'yyyy-MM-dd:HH:mm:ss'))\nFROM long_sequence(1);`\n| minute |\n| :----- |\n| 43     |\n`questdb-sql title=\"Using in an aggregation\"\nSELECT minute(ts), count() FROM transactions;`\n| minute | count |\n| :----- | :---- |\n| 0      | 2323  |\n| 1      | 6548  |\n| ...    | ...   |\n| 58     | 9876  |\n| 59     | 2567  |\nhour\n`hour(value)` - returns the `hour` of day for a given date or timestamp from `0`\nto `23`\nArguments:\n\n`value` is any `timestamp` or `date`\n\nReturn value:\nReturn value type is `int`\nExamples:\n`questdb-sql title=\"Hour of the day\"\nSELECT hour(to_timestamp('2020-03-01:15:43:21', 'yyyy-MM-dd:HH:mm:ss'))\nFROM long_sequence(1);`\n| hour |\n| :--- |\n| 12   |\n`questdb-sql title=\"Using in an aggregation\"\nSELECT hour(ts), count() FROM transactions;`\n| hour | count |\n| :--- | :---- |\n| 0    | 2323  |\n| 1    | 6548  |\n| ...  | ...   |\n| 22   | 9876  |\n| 23   | 2567  |\nday\n`day(value)` - returns the `day` of month for a given date or timestamp from `1`\nto `31`.\nArguments:\n\n`value` is any `timestamp` or `date`\n\nReturn value:\nReturn value type is `int`\nExamples:\n`questdb-sql title=\"Day of the month\"\nSELECT day(to_timestamp('2020-03-01:15:43:21', 'yyyy-MM-dd:HH:mm:ss'))\nFROM long_sequence(1);`\n| day |\n| :-- |\n| 01  |\n`questdb-sql title=\"Using in an aggregation\"\nSELECT day(ts), count() FROM transactions;`\n| day | count |\n| :-- | :---- |\n| 1   | 2323  |\n| 2   | 6548  |\n| ... | ...   |\n| 30  | 9876  |\n| 31  | 2567  |\nmonth\n`month(value)` - returns the `month` of year for a given date or timestamp from\n`1` to `12`\nArguments:\n\n`value` is any `timestamp` or `date`\n\nReturn value:\nReturn value type is `int`\nExamples:\n`questdb-sql title=\"Month of the year\"\nSELECT month(to_timestamp('2020-03-01:15:43:21', 'yyyy-MM-dd:HH:mm:ss'))\nFROM long_sequence(1);`\n| month |\n| :---- |\n| 03    |\n`questdb-sql title=\"Using in an aggregation\"\nSELECT month(ts), count() FROM transactions;`\n| month | count |\n| :---- | :---- |\n| 1     | 2323  |\n| 2     | 6548  |\n| ...   | ...   |\n| 11    | 9876  |\n| 12    | 2567  |\nyear\n`year(value)` - returns the `year` for a given date or timestamp\nArguments:\n\n`value` is any `timestamp` or `date`\n\nReturn value:\nReturn value type is `int`\nExamples:\n`questdb-sql title=\"Year\"\nSELECT year(to_timestamp('2020-03-01:15:43:21', 'yyyy-MM-dd:HH:mm:ss'))\nFROM long_sequence(1);`\n| year |\n| :--- |\n| 2020 |\n`questdb-sql title=\"Using in an aggregation\"\nSELECT month(ts), count() FROM transactions;`\n| year | count |\n| :--- | :---- |\n| 2015 | 2323  |\n| 2016 | 9876  |\n| 2017 | 2567  |\nis_leap_year\n`is_leap_year(value)` - returns `true` if the `year` of `value` is a leap year,\n`false` otherwise.\nArguments:\n\n`value` is any `timestamp` or `date`\n\nReturn value:\nReturn value type is `boolean`\nExamples:\n`questdb-sql\nSELECT year(ts), is_leap_year(ts) FROM myTable;`\n| year | is_leap_year |\n| :--- | :----------- |\n| 2020 | true         |\n| 2021 | false        |\n| 2022 | false        |\n| 2023 | false        |\n| 2024 | true         |\n| 2025 | false        |\ndays_in_month\n`days_in_month(value)` - returns the number of days in a month from a provided\ntimestamp or date.\nArguments:\n\n`value` is any `timestamp` or `date`\n\nReturn value:\nReturn value type is `int`\nExamples:\n`questdb-sql\nSELECT month(ts), days_in_month(ts) FROM myTable;`\n| month | days_in_month |\n| :---- | :------------ |\n| 4     | 30            |\n| 5     | 31            |\n| 6     | 30            |\n| 7     | 31            |\n| 8     | 31            |\nday_of_week\n`day_of_week(value)` - returns the day number in a week from `1` (Monday) to `7`\n(Sunday)\nArguments:\n\n`value` is any `timestamp` or `date`\n\nReturn value:\nReturn value type is `int`\nExamples:\n`questdb-sql\nSELECT to_str(ts,'EE'),day_of_week(ts) FROM myTable;`\n| day       | day_of_week |\n| :-------- | :---------- |\n| Monday    | 1           |\n| Tuesday   | 2           |\n| Wednesday | 3           |\n| Thursday  | 4           |\n| Friday    | 5           |\n| Saturday  | 6           |\n| Sunday    | 7           |\nday_of_week_sunday_first\n`day_of_week_sunday_first(value)` - returns the day number in a week from `1`\n(Sunday) to `7` (Saturday)\nArguments:\n\n`value` is any `timestamp` or `date`\n\nReturn value:\nReturn value type is `int`\nExamples:\n`questdb-sql\nSELECT to_str(ts,'EE'),day_of_week_sunday_first(ts) FROM myTable;`\n| day       | day_of_week_sunday_first |\n| :-------- | :----------------------- |\n| Monday    | 2                        |\n| Tuesday   | 3                        |\n| Wednesday | 4                        |\n| Thursday  | 5                        |\n| Friday    | 6                        |\n| Saturday  | 7                        |\n| Sunday    | 1                        |\nDate and timestamp format\nThe date and timestamp format is formed by units and arbitrary text. A unit is a\ncombination of letters representing a date or time component, as defined by the\ntable below. The letters used to form a unit are case-sensitive.\nSee\nTimestamps in QuestDB\nfor more examples of how the units are used to parse inputs.\n| Unit   | Date or Time Component                                                                                         | Presentation       | Examples                              |\n| ------ | -------------------------------------------------------------------------------------------------------------- | ------------------ | ------------------------------------- |\n| `G`    | Era designator                                                                                                 | Text               | AD                                    |\n| `y`    | `y` single digit or greedy year, depending on the input digit number                                           | Year               | 1996; 96; 999; 3                      |\n| `yy`   | Two digit year of the current century                                                                          | Year               | 96 (interpreted as 2096)              |\n| `yyy`  | Three-digit year                                                                                               | Year               | 999                                   |\n| `yyyy` | Four-digit year                                                                                                | Year               | 1996                                  |\n| `M`    | Month in year                                                                                                  | Month              | July; Jul; 07                         |\n| `w`    | Week in year                                                                                                   | Number             | 27                                    |\n| `ww`   | ISO week of year                                                                                               | Number             | 2                                     |\n| `D`    | Day in year                                                                                                    | Number             | 189                                   |\n| `d`    | Day in month                                                                                                   | Number             | 10                                    |\n| `F`    | Day of week in month                                                                                           | Number             | 2                                     |\n| `E`    | Day name in week                                                                                               | Text               | Tuesday; Tue                          |\n| `u`    | Day number of week (1 = Monday, ..., 7 = Sunday)                                                               | Number             | 1                                     |\n| `a`    | Am/pm marker                                                                                                   | Text               | PM                                    |\n| `H`    | Hour in day (0-23)                                                                                             | Number             | 0                                     |\n| `k`    | Hour in day (1-24)                                                                                             | Number             | 24                                    |\n| `K`    | Hour in am/pm (0-11)                                                                                           | Number             | 0                                     |\n| `h`    | Hour in am/pm (1-12)                                                                                           | Number             | 12                                    |\n| `m`    | Minute in hour                                                                                                 | Number             | 30                                    |\n| `s`    | Second in minute                                                                                               | Number             | 55                                    |\n| `SSS`  | 3-digit millisecond                                                                                            | Number             | 978                                   |\n| `S`    | Millisecond up to 3 digits: `S` parses 1 digit when followed by another `unit`. Otherwise, it parses 3 digits. | Number             | 900                                   |\n| `z`    | Time zone                                                                                                      | General time zone  | Pacific Standard Time; PST; GMT-08:00 |\n| `Z`    | Time zone                                                                                                      | RFC 822 time zone  | -0800                                 |\n| `X`    | Time zone                                                                                                      | ISO 8601 time zone | -08; -0800; -08:00                    |\n| `UUU`  | 3-digit microsecond                                                                                            | Number             | 698                                   |\n| `U`    | Microsecond up to 3 digits: `U` parses 1 digit when followed by another `unit`. Otherwise, it parses 3 digits. | Number             | 600                                   |\n| `U+`   | 6-digit microsecond                                                                                            | Number             | 600                                   |\n| `N`    | Nanosecond. QuestDB provides microsecond resolution so the parsed nanosecond will be truncated.                | Number             | N/A (truncated)                       |\n| `N+`   | 9-digit nanosecond. QuestDB provides microsecond resolution so the parsed nanosecond will be truncated.        | Number             | N/A (truncated)                       |\nExamples for greedy year format `y`\nThe interpretation of `y` depends on the input digit number:\n\nIf the input year is a two-digit number, the output timestamp assumes the\n  current century.\nOtherwise, the number is interpreted as it is.\n\n| Input year | Timestamp value interpreted by `y-M` | Notes                                                |\n| ---------- | ------------------------------------ | ---------------------------------------------------- |\n| `5-03`     | `0005-03-01T00:00:00.000000Z`        | Greedily parsing the number as it is                 |\n| `05-03`    | `2005-03-01T00:00:00.000000Z`        | Greedily parsing the number assuming current century |\n| `005-03`   | `0005-03-01T00:00:00.000000Z`        | Greedily parsing the number as it is                 |",
    "tag": "questdb"
  },
  {
    "title": "Usage",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/function/random-value-generator.md",
    "content": "\ntitle: Random value generator\nsidebar_label: Random value generator\ndescription: Random value generator function reference documentation.\n\nThe following functions have been created to help with our test suite. They are\nalso useful for users testing QuestDB on specific workloads in order to quickly\ngenerate large test datasets that mimic the structure of their actual data.\nValues can be generated either:\n\nPseudo randomly\nDeterministically\n  when specifying a `seed`\n\nQuestDB supports the following random generation functions:\n\nrnd_boolean\nrnd_byte\nrnd_short\nrnd_int\nrnd_long\nrnd_long256\nrnd_float\nrnd_double\nrnd_date\nrnd_timestamp\nrnd_char\nrnd_symbol\nrnd_str\nrnd_bin\nrnd_uuid4\n\nUsage\nRandom functions should be used for populating test tables only. They do not\nhold values in memory and calculations should not be performed at the same time\nas the random numbers are generated.\nFor example, running\n`SELECT round(a,2), a FROM (SELECT rnd_double() a FROM long_sequence(10));` is\nbad practice and will return inconsistent results.\nA better approach would be to populate a table and then run the query. So for\nexample\n\ncreate - `CREATE TABLE test(val double);`\npopulate -\n   `INSERT INTO test SELECT * FROM (SELECT rnd_double() FROM long_sequence(10));`\nquery - `SELECT round(val,2) FROM test;`\n\nGenerating sequences\nThis page describes the functions to generate values. To generate sequences of\nvalues, please refer the page about\nrow generators.\nrnd_boolean\n`rnd_boolean()` - generates a random `boolean` value, either `true` or `false`,\nboth having equal probability.\nReturn value:\nReturn value type is `boolean`.\nExamples:\n`questdb-sql title=\"Random boolean\"\nSELECT\n    value a,\n    count() b\nFROM (SELECT rnd_boolean() value FROM long_sequence(100));`\n| a     | b   |\n| ----- | --- |\n| true  | 47  |\n| false | 53  |\nrnd_byte\n\n`rnd_byte()` - returns a random integer which can take any value between `0`\n  and `127`.\n`rnd_byte(min, max)` - generates byte values in a specific range (for example\n  only positive, or between 1 and 10).\n\nArguments:\n\n`min`: is a `byte` representing the lowest possible generated value\n  (inclusive).\n`max`: is a `byte` representing the highest possible generated value\n  (inclusive).\n\nReturn value:\nReturn value type is `byte`.\nExamples:\n`questdb-sql title=\"Random byte\"\nSELECT rnd_byte() FROM long_sequence(5);\nSELECT rnd_byte(-1,1) FROM long_sequence(5);`\n`122,34,17,83,24\n0,1,-1,-1,0`\nrnd_short\n\n`rnd_short()` - returns a random integer which can take any value between\n  `-32768` and `32767`.\n`rnd_short(min, max)` - returns short values in a specific range (for example\n  only positive, or between 1 and 10). Supplying `min` above `max` will result\n  in an `invalid range` error.\n\nArguments:\n\n`min`: is a `short` representing the lowest possible generated value\n  (inclusive).\n`max`: is a `short` representing the highest possible generated value\n  (inclusive).\n\nReturn value:\nReturn value type is `short`.\nExamples:\n`questdb-sql title=\"Random short\"\nSELECT rnd_short() FROM long_sequence(5);\nSELECT rnd_short(-1,1) FROM long_sequence(5);`\n`-27434,234,-12977,8843,24\n0,1,-1,-1,0`\nrnd_int\n\n`rnd_int()` is used to return a random integer which can take any value\n  between `-2147483648` and `2147483647`.\n`rnd_int(min, max, nanRate)` is used to generate int values in a specific\n  range (for example only positive, or between 1 and 10), or to get occasional\n  `NaN` values along with int values.\n\nArguments:\n\n`min`: is an `int` representing the lowest possible generated value\n  (inclusive).\n`max`: is an `int` representing the highest possible generated value\n  (inclusive).\n`nanRate` is an `int` defining the frequency of occurrence of `NaN` values:\n`0`: No `NaN` will be returned.\n`1`: Will only return `NaN`.\n`N > 1`: On average, one in N generated values will be NaN.\n\nReturn value:\nReturn value type is `int`.\nExamples:\n`questdb-sql title=\"Random int\"\nSELECT rnd_int() FROM long_sequence(5)\nSELECT rnd_int(1,4,0) FROM long_sequence(5);\nSELECT rnd_int(1,4,1) FROM long_sequence(5);\nSELECT rnd_int(1,4,2) FROM long_sequence(5);`\n`1822685476, 1173192835, -2808202361, 78121757821, 44934191\n1,4,3,1,2\nnull,null,null,null,null\n1,null,4,null,2`\nrnd_long\n\n`rnd_long()` is used to return a random signed integer between\n  `0x8000000000000000L` and `0x7fffffffffffffffL`.\n`rnd_long(min, max, nanRate)` is used to generate long values in a specific\n  range (for example only positive, or between 1 and 10), or to get occasional\n  `NaN` values along with int values.\n\nArguments:\n\n`min`: is a `long` representing the lowest possible generated value\n  (inclusive).\n`max`: is a `long` representing the highest possible generated value\n  (inclusive).\n`nanRate` is an `int` defining the frequency of occurrence of `NaN` values:\n`0`: No `NaN` will be returned.\n`1`: Will only return `NaN`.\n`N > 1`: On average, one in N generated values will be `NaN`.\n\nReturn value:\nReturn value type is `long`.\nExamples:\n`questdb-sql title=\"Random long\"\nSELECT rnd_long() FROM long_sequence(5);\nSELECT rnd_long(1,4,0) FROM long_sequence(5);\nSELECT rnd_long(1,4,1) FROM long_sequence(5);\nSELECT rnd_long(-10000000,10000000,2) FROM long_sequence(5);`\n`questdb-sql\n1,4,3,1,2\nnull,null,null,null,null\n-164567594, -323331140, 26846334, -892982893, -351053301\n300291810703592700, 2787990010234796000, 4305203476273459700, -8518907563589124000, 8443756723558216000`\nrnd_long256\n\n`rnd_long256()` - generates a random `long256` value between 0 and 2^256.\n\nReturn value:\nReturn value type is `long256`.\nExamples:\n`questdb-sql title=\"Random long256\"\nSELECT rnd_long256() FROM long_sequence(5);`\n`0x5dd94b8492b4be20632d0236ddb8f47c91efc2568b4d452847b4a645dbe4871a,\n0x55f256188b3474aca83ccc82c597668bb84f36d3f5b25afd9e194c1867625918,\n0x630c6f02c1c2e0c2aa4ac80ab684aa36d91dd5233cc185bb7097400fa12e7de0,\n0xa9eeaa5268f911f4bcac2e89b621bd28bba90582077fc9fb9f14a53fcf6368b7,\n0x7c80546eea2ec093a5244e39efad3f39c5489d2337007fd0b61d8b141058724d`\nrnd_float\n\n`rnd_float()` - generates a random positive `float` between 0 and 1.\n`rnd_float(nanRate)` - generates a random positive `float` between 0 and 1\n  which will be `NaN` at a frequency defined by `nanRate`.\n\nArguments:\n\n`nanRate` is an `int` defining the frequency of occurrence of `NaN` values:\n`0`: No `NaN` will be returned.\n`1`: Will only return `NaN`.\n`N > 1`: On average, one in N generated values will be `NaN`.\n\nReturn value:\nReturn value type is `float`.\nExamples:\n`questdb-sql title=\"Random float\"\nSELECT rnd_float() FROM long_sequence(5);\nSELECT rnd_float(2) FROM long_sequence(6);`\n`0.3821478, 0.5162148, 0.22929084, 0.03736937, 0.39675003\n0.08108246, 0.7082644, null, 0.6784522, null, 0.5711276`\nrnd_double\n\n`rnd_double()` - generates a random positive `double` between 0 and 1.\n`rnd_double(nanRate)` - generates a random positive `double` between 0 and\n  1 which will be `NaN` at a frequency defined by `nanRate`.\n\nArguments:\n\n`nanRate` is an `int` defining the frequency of occurrence of `NaN` values:\n`0`: No `NaN` will be returned.\n`1`: Will only return `NaN`.\n`N > 1`: On average, one in N generated values will be `NaN`.\n\nReturn value:\nReturn value type is `double`.\nExamples:\n`questdb-sql title=\"Random double\"\nSELECT rnd_double() FROM long_sequence(5);\nSELECT rnd_double(2) FROM long_sequence(5);`\n`0.99115364871, 0.31011470271, 0.10776479191, 0.53938281731, 0.89820403511\n0.99115364871, null, null, 0.53938281731, 0.89820403511`\nrnd_date()\n\n`rnd_date()` generates a random date between `start` and `end` dates (both\n  inclusive). IT will also generate `NaN` values at a frequency defined by\n  `nanRate`. When `start` or `end` are invalid dates, or when `start` is\n  superior to `end`, it will return `invalid range` error. When `nanRate` is\n  inferior to 0, it will return `invalid NAN rate` error.\n\nArguments:\n\n`start` is a `date` defining the minimum possible generated date (inclusive)\n`end` is a `date` defining the maximum possible generated date (inclusive)\n`nanRate` defines the frequency of occurrence of `NaN` values:\n`0`: No `NaN` will be returned.\n`1`: Will only return `NaN`.\n`N > 1`: On average, one in N generated values will be NaN.\n\nReturn value:\nReturn value type is `date`.\nExamples:\n`questdb-sql title=\"Random date\"\nSELECT rnd_date(\n    to_date('2015', 'yyyy'),\n    to_date('2016', 'yyyy'),\n    0)\nFROM long_sequence(5);`\n`questdb-sql\n2015-01-29T18:00:17.402Z, 2015-11-15T20:22:14.112Z,\n2015-12-08T09:26:04.483Z, 2015-05-28T02:22:47.022Z,\n2015-10-13T19:16:37.034Z`\nrnd_timestamp()\n\n`rnd_timestamp(start, end, nanRate)` generates a random timestamp between\n  `start` and `end` timestamps (both inclusive). It will also generate `NaN`\n  values at a frequency defined by `nanRate`. When `start` or `end` are invalid\n  timestamps, or when `start` is superior to `end`, it will return\n  `invalid range` error. When `nanRate` is inferior to 0, it will return\n  `invalid NAN rate` error.\n\nArguments:\n\n`start` is a `timestamp` defining the minimum possible generated timestamp\n  (inclusive)\n`end` is a `timestamp` defining the maximum possible generated timestamp\n  (inclusive)\n`nanRate` defines the frequency of occurrence of `NaN` values:\n`0`: No `NaN` will be returned.\n`1`: Will only return `NaN`.\n`N > 1`: On average, one in N generated values will be NaN.\n\nReturn value:\nReturn value type is `timestamp`.\nExamples:\n`questdb-sql title=\"Random timestamp\"\nSELECT rnd_timestamp(\n    to_timestamp('2015', 'yyyy'),\n    to_timestamp('2016', 'yyyy'),\n    0)\nFROM long_sequence(5);`\n`questdb-sql\n2015-01-29T18:00:17.402762Z, 2015-11-15T20:22:14.112744Z,\n2015-12-08T09:26:04.483039Z, 2015-05-28T02:22:47.022680Z,\n2015-10-13T19:16:37.034203Z`\nSequences\nTo generate increasing timestamps, please refer the page about\nrow generators.\nrnd_char\n\n`rnd_char()` is used to generate a random `char` which will be an uppercase\n  character from the 26-letter A to Z alphabet. Letters from A to Z will be\n  generated with equal probability.\n\nReturn value:\nReturn value type is `char`.\nExamples:\n`questdb-sql title=\"Random char\"\nSELECT rnd_char() FROM long_sequence(5);`\n`G, P, E, W, K`\nrnd_symbol\n\n`rnd_symbol(symbolList)` is used to choose a random `symbol` from a list\n  defined by the user. It is useful when looking to generate specific symbols\n  from a finite list (e.g `BUY, SELL` or `AUTUMN, WINTER, SPRING, SUMMER`.\n  Symbols are randomly chosen from the list with equal probability. When only\n  one symbol is provided in the list, this symbol will be chosen with 100%\n  probability, in which case it is more efficient to use\n  `cast('your_symbol' as symbol`\n`rnd_symbol(list_size, minLength, maxLength, nullRate)` generated a finite\n  list of distinct random symbols and chooses one symbol from the list at\n  random. The finite list is of size `list_size`. The generated symbols length\n  is between `minLength` and `maxLength` (both inclusive). The function will\n  also generate `null` values at a rate defined by `nullRate`.\n\nArguments:\n\n`symbolList` is a variable-length list of possible `symbol` values expressed\n  as a comma-separated list of strings. For example,\n  `'a', 'bcd', 'efg123', '\u884c'`\n`list_size` is the number of distinct `symbol` values to generated\n`minLength` is an `int` defining the minimum length for of a generated symbol\n  (inclusive)\n`maxLength` is an `int` defining the maximum length for of a generated symbol\n  (inclusive)\n`nullRate` is an `int` defining the frequency of occurrence of `null` values:\n`0`: No `null` will be returned.\n`1`: Will only return `null`.\n`N > 1`: On average, one in N generated values will be `null`.\n\nReturn value:\nReturn value type is `symbol`.\nExamples:\n`questdb-sql title=\"Random symbol from a list\"\nSELECT rnd_symbol('ABC','def', '123')\nFROM long_sequence(5);`\n`'ABC', '123', 'def', '123', 'ABC'`\n`questdb-sql title=\"Random symbol, randomly generated\"\nSELECT rnd_symbol(2, 3, 4, 0)\nFROM long_sequence(5);`\n`'ABC', 'DEFG', 'ABC', 'DEFG', 'DEFG'`\nrnd_str\n\n`rnd_str(stringList)` is used to choose a random `string` from a list defined\n  by the user. It is useful when looking to generate specific strings from a\n  finite list (e.g `BUY, SELL` or `AUTUMN, WINTER, SPRING, SUMMER`. Strings are\n  randomly chosen from the list with equal probability. When only one string is\n  provided in the list, this string will be chosen with 100% probability.\n`rnd_str(list_size, minLength, maxLength, nullRate)` generated a finite list\n  of distinct random string and chooses one string from the list at random. The\n  finite list is of size `list_size`. The generated strings length is between\n  `minLength` and `maxLength` (both inclusive). The function will also generate\n  `null` values at a rate defined by `nullRate`.\n\nArguments:\n\n`strList` is a variable-length list of possible `string` values expressed as a\n  comma-separated list of strings. For example, `'a', 'bcd', 'efg123', '\u884c'`\n`list_size` is the number of distinct `string` values to generated\n`minLength` is an `int` defining the minimum length for of a generated string\n  (inclusive)\n`maxLength` is an `int` defining the maximum length for of a generated string\n  (inclusive)\n`nullRate` is an `int` defining the frequency of occurrence of `null` values:\n`0`: No `null` will be returned.\n`1`: Will only return `null`.\n`N > 1`: On average, one in N generated values will be `null`.\n\nReturn value:\nReturn value type is `string`.\nExamples:\n`questdb-sql title=\"Random string from a list\"\nSELECT rnd_str('ABC','def', '123')\nFROM long_sequence(5);`\n`'ABC', '123', 'def', '123', 'ABC'`\n`questdb-sql title=\"Random string, randomly generated\"\nSELECT rnd_str(3, 2, 2, 4)\nFROM long_sequence(8);`\n`'AB', 'CD', null, 'EF', 'CD', 'EF', null, 'AB'`\nrnd_bin\n\n`rnd_bin()` generates random binary data of a size up to `32` bytes.\n`rnd_bin(minBytes, maxBytes, nullRate)` generates random binary data of a size\n  between `minBytes` and `maxBytes` and returns `null` at a rate defined by\n  `nullRate`.\n\nArguments:\n\n`minBytes` is a `long` defining the minimum size in bytes for of a generated\n  binary (inclusive)\n`maxBytes` is a `long` defining the maximum size in bytes for of a generated\n  binary (inclusive)\n`nullRate` is an `int` defining the frequency of occurrence of `null` values:\n`0`: No `null` will be returned.\n`1`: Will only return `null`.\n`N > 1`: On average, one in N generated values will be `null`.\n\nReturn value:\nReturn value type is `binary`.\nExamples:\n`questdb-sql title=\"Random binary\"\nSELECT rnd_bin() FROM long_sequence(5);\nSELECT rnd_bin(2, 5, 2) FROM long_sequence(5);`\nrnd_uuid4\n\n`rnd_uuid4()` is used to generate a random UUID.\nThe generated UUIDs are version 4 as per the\n  RFC 4122 specification.\nGenerated UUIDs do not use a cryptographically strong random generator and should not be used for\n  security purposes.\n\nReturn value:\nReturn value type is `uuid`.\nExamples:\n`questdb-sql title=\"Random char\"\nSELECT rnd_uuid4() FROM long_sequence(3);`\n```\ndeca0b0b-b14b-4d39-b891-9e1e786a48e7\n2f113ebb-d36e-4e58-b804-6ece2263abe4\n6eddd24a-8889-4345-8001-822cc2d41951",
    "tag": "questdb"
  },
  {
    "title": "table_columns",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/function/meta.md",
    "content": "\ntitle: Meta functions\nsidebar_label: Meta\ndescription: Table and database metadata function reference documentation.\n\nThese functions provide table information including column details and metadata.\nThese functions are particularly useful for checking if tables contain a\ndesignated timestamp column.\ntable_columns\n`table_columns('tableName')` returns the schema of a table\nArguments:\n\n`tableName` is the name of an existing table as a string\n\nReturn value:\nReturns a `table` with the following columns:\n\n`column` - name of the available columns in the table\n`type` - type of the column\n`indexed` - if indexing is applied to this column\n`indexBlockCapacity` - how many row IDs to store in a single storage block on\n  disk\n`symbolCached` - whether this `symbol` column is cached\n`symbolCapacity` - how many distinct values this column of `symbol` type is\n  expected to have\n`designated` - if this is set as the designated timestamp column for this\n  table\n\nFor more details on the meaning and use of these values, see the\nCREATE TABLE documentation.\nExamples:\n`questdb-sql title=\"Get all columns in a table\"\ntable_columns('my_table')`\n| column | type      | indexed | indexBlockCapacity | symbolCached | symbolCapacity | designated |\n|--------|-----------|---------|--------------------|--------------|----------------|------------|\n| symb   | SYMBOL    | true    | 1048576            | false        | 256            | false      |\n| price  | DOUBLE    | false   | 0                  | false        | 0              | false      |\n| ts     | TIMESTAMP | false   | 0                  | false        | 0              | true       |\n| s      | STRING    | false   | 0                  | false        | 0              | false      |\n`questdb-sql title=\"Get designated timestamp column\"\nSELECT column, type, designated FROM table_columns('my_table') WHERE designated`\n| column | type      | designated |\n|--------|-----------|------------|\n| ts     | TIMESTAMP | true       |\n`questdb-sql title=\"Get the count of column types\"\nSELECT type, count() FROM table_columns('my_table');`\n| type      | count |\n|-----------|-------|\n| SYMBOL    | 1     |\n| DOUBLE    | 1     |\n| TIMESTAMP | 1     |\n| STRING    | 1     |\ntables\n`tables()` returns all tables in the database including table metadata.\nArguments:\n\n`tables()` does not require arguments.\n\nReturn value:\nReturns a `table`.\nExamples:\n`questdb-sql title=\"List all tables\"\ntables();`\n| id  | name        | designatedTimestamp | partitionBy | maxUncommittedRows | o3MaxLag   | walEnabled | directoryName    |\n|-----|-------------|---------------------|-------------|--------------------|------------|------------|------------------|\n| 1   | my_table    | ts                  | DAY         | 500000             | 30000000 0 | false      | my_table         |\n| 2   | device_data | null                | NONE        | 10000              | 30000000   | false      | device_data      |\n| 3   | short_lived | null                | HOUR        | 10000              | 30000000   | false      | short_lived (->) |\n`questdb-sql title=\"All tables in reverse alphabetical order\"\ntables() ORDER BY name DESC;`\n| id  | name        | designatedTimestamp | partitionBy | maxUncommittedRows | o3MaxLag  | walEnabled | directoryName    |\n|-----|-------------|---------------------|-------------|--------------------|-----------|------------|------------------|\n| 2   | device_data | null                | NONE        | 10000              | 30000000  | false      | device_data      |\n| 1   | my_table    | ts                  | DAY         | 500000             | 300000000 | false      | my_table         |\n| 3   | short_lived | ts                  | HOUR        | 10000              | 30000000  | false      | short_lived (->) |\n:::note\n`(->)` means the table was created using the IN VOLUME clause.\n:::\n`questdb-sql title=\"All tables with a daily partitioning strategy\"\ntables() WHERE partitionBy = 'DAY'`\n| id  | name     | designatedTimestamp | partitionBy | maxUncommittedRows | walEnabled | directoryName |\n|-----|----------|---------------------|-------------|--------------------|------------|---------------|\n| 1   | my_table | ts                  | DAY         | 500000             | true       | my_table      |\nwal_tables\n`wal_tables()` returns the WAL status for all WAL tables in the database.\nArguments:\n\n`wal_tables()` does not require arguments.\n\nReturn value:\nReturns a `table` including the following information:\n\n`name` - table name\n`suspended` - suspended status flag\n`writerTxn` - the last committed transaction in TableWriter\n`sequencerTxn` - the last committed transaction in the sequencer\n\nExamples:\n`questdb-sql title=\"List all tables\"\nwal_tables();`\n| name        | suspended | writerTxn | sequencerTxn | \n| ----------- |-----------|-----------|--------------| \n| sensor_wal  | false     | 2         | 4            | \n| weather_wal | false     | 3         | 3            | ",
    "tag": "questdb"
  },
  {
    "title": "long_sequence",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/function/row-generator.md",
    "content": "\ntitle: Row generator\nsidebar_label: Row generator\ndescription: Row generator function reference documentation.\n\nThe `long_sequence()` function may be used as a row generator to create table\ndata for testing. Basic usage of this function involves providing the number of\niterations required. Deterministic pseudo-random behavior can be achieved by\nproviding seed values when calling the function.\nThis function is commonly used in combination with\nrandom generator functions\nto produce mock data.\nlong_sequence\n\n`long_sequence(iterations)` - generates rows\n`long_sequence(iterations, seed1, seed2)` - generates rows deterministically\n\nArguments:\n-`iterations`: is a `long` representing the number of rows to generate. -`seed1`\nand `seed2` are `long64` representing both parts of a `long128` seed.\nRow generation\nThe `long_sequence()` function can be used to generate very large datasets for\ntesting e.g. billions of rows.\n`long_sequence(iterations)` is used to:\n\nGenerate a number of rows defined by `iterations`.\nGenerate a column `x:long` of monotonically increasing long integers starting\n  from 1, which can be accessed for queries.\n\nRandom number seed\nWhen `long_sequence` is used conjointly with\nrandom generators, these\nvalues are usually generated at random. The function supports a seed to be\npassed in order to produce deterministic results.\n:::info\nDeterministic procedural generation makes it easy to test on vasts amounts of\ndata without actually moving large files around across machines. Using the same\nseed on any machine at any time will consistently produce the same results for\nall random functions.\n:::\nExamples:\n`questdb-sql title=\"Generating multiple rows\"\nSELECT x, rnd_double()\nFROM long_sequence(5);`\n| x   | rnd_double   |\n| --- | ------------ |\n| 1   | 0.3279246687 |\n| 2   | 0.8341038236 |\n| 3   | 0.1023834675 |\n| 4   | 0.9130602021 |\n| 5   | 0.718276777  |\n`questdb-sql title=\"Accessing row_number using the x column\"\nSELECT x, x*x\nFROM long_sequence(5);`\n| x   | x*x |\n| --- | ---- |\n| 1   | 1    |\n| 2   | 4    |\n| 3   | 9    |\n| 4   | 16   |\n| 5   | 25   |\n`questdb-sql title=\"Using with a seed\"\nSELECT rnd_double()\nFROM long_sequence(2,128349234,4327897);`\n:::note\nThe results below will be the same on any machine at any time as long as they\nuse the same seed in long_sequence.\n:::\n| rnd_double         |\n| ------------------ |\n| 0.8251337821991485 |",
    "tag": "questdb"
  },
  {
    "title": "coalesce",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/function/conditional.md",
    "content": "\ntitle: Conditional functions\nsidebar_label: Conditional\ndescription: Conditional functions reference documentation.\n\nConditional functions allow for conditionally selecting input values. For\ninstance, the `coalesce()` function is useful for handling null data values and\nproviding replacement values.\ncoalesce\n`coalesce(value [, ...])` - returns the first non-null argument in a provided\nlist of arguments in cases where null values should not appear in query results.\nThis function is an implementation of the `COALESCE` expression in PostgreSQL\nand as such, should follow the expected behavior described in the\ncoalesce PostgreSQL documentation\nArguments:\n\n`coalesce(value [, ...])` `value` and subsequent comma-separated list of\n  arguments which may be of any type except binary. If the provided arguments\n  are of different types, one should be `CAST`able to another.\n\nReturn value:\nThe returned value is the first non-null argument passed.\nExamples:\nGiven a table with the following records:\n|timestamp                  |amount|\n|:--------------------------|:-----|\n|2021-02-11T09:39:16.332822Z|1     |\n|2021-02-11T09:39:16.333481Z|null  |\n|2021-02-11T09:39:16.333511Z|3     |\nThe following example demonstrates how to use `coalesce()` to return a default\nvalue of `0` for an expression if the `amount` column contains `null` values.\n`questdb-sql\nSELECT timestamp,\n       coalesce(amount, 0) as amount_not_null\nFROM transactions`\n|timestamp                  |amount_not_null|\n|:--------------------------|:--------------|\n|2021-02-11T09:39:16.332822Z|1              |\n|2021-02-11T09:39:16.333481Z|0              |\n|2021-02-11T09:39:16.333511Z|3              |\nnullif\n`nullif(value1, value2)` - returns a null value if `value1` is equal to `value2`\nor otherwise returns `value1`.\nThis function is an implementation of the `NULLIF` expression in PostgreSQL and\nas such, should follow the expected behavior described in the\nnullif PostgreSQL documentation.\nArguments:\n\n`value1` is any numeric, char, or string value.\n`value2` is any numeric, char, or string value.\n\nReturn value:\nThe returned value is either `NULL`, or the first argument passed.\nExamples:\nGiven a table with the following records:\n|timestamp                  |amount|\n|:--------------------------|:-----|\n|2021-02-11T09:39:16.332822Z|0     |\n|2021-02-11T09:39:16.333481Z|11    |\n|2021-02-11T09:39:16.333511Z|3     |\nThe following example demonstrates how to use `nullif()` to return a `null` if\nthe `amount` column contains `0` values.\n`questdb-sql\nSELECT timestamp,\n       nullif(amount, 0) as amount_null_if_zero\nFROM transactions`\n|timestamp                  |amount_null_if_zero|\n|:--------------------------|:------------------|\n|2021-02-11T09:39:16.332822Z|null               |\n|2021-02-11T09:39:16.333481Z|11                 |",
    "tag": "questdb"
  },
  {
    "title": "Syntax",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/function/analytic.md",
    "content": "\ntitle: Analytic functions\nsidebar_label: Analytic\ndescription: Analytic functions reference documentation.\n\nThis page describes the available analytic functions. QuestDB is working on\nadding more analytic functions.\nAn analytic function performs a calculation across a set of table rows that are\nsomehow related to the current row.\nSyntax\n\nAnalytic functions are used with an `OVER` clause to define the way data is\ngrouped and processed. The `OVER` clause is used with `PARTITION BY` and\n`ORDER BY` to set unique parameters and organize the rows.\nrow_number\n`row_number()` - assigns a row number to each row in a result set. For each\npartition, the row number starts with one and increments by one.\nArguments:\n\n`row_number` does not require arguments.\n\nReturn value:\nReturn value type is `long`.\nDescription\n`row_number()` returns values dynamically and there is no guarantee that the\nrows returned will be ordered exactly the same with each execution of the query.\nHence, an `ORDER BY` outside of the `OVER()` clause can be used to ensure the\noutput order.\nExamples:\nGiven a table `trades`, the queries below use `row_number()` with a `WHERE`\nclause to filter trading records added within one day.\nThe following query assigns row numbers and orders output based on them:\n`questdb-sql\nSELECT\nsymbol,\nside,\nprice,\namount,\nrow_number() OVER () AS row_num\nFROM trades\nWHERE timestamp > dateadd('d', -1, now())\nORDER BY row_num ASC;\n-- The ORDER BY clause arranges the output based on the assigned row_num.`\n| symbol  | side | price    | amount     | row_num |\n| :------ | :--- | :------- | :--------- | :------ |\n| BTC-USD | sell | 20633.47 | 0.17569298 | 1       |\n| ETH-USD | sell | 1560.04  | 1.3289     | 2       |\n| ETH-USD | sell | 1560.04  | 0.3        | 3       |\n| ETH-USD | sell | 1560     | 1.40426786 | 4       |\n| BTC-USD | buy  | 20633.48 | 0.00179092 | 5       |\nThe following query groups the table based on `symbol` and assigns row numbers\nto each group based on `price`:\n`questdb-sql\nSELECT\nsymbol,\nside,\nprice,\namount,\nrow_number() OVER (PARTITION BY symbol ORDER BY price) AS row_num\nFROM trades\nWHERE timestamp > dateadd('d', -1, now())\nORDER BY row_num ASC;\n-- The ORDER BY clause arranges the output based on the assigned row_num.`\n| symbol  | side | price   | amount     | row_num |\n| :------ | :--- | :------ | :--------- | :------ |\n| BTC-USD | Sell | 1479.41 | 0.10904633 | 1       |\n| ETH-USD | Sell | 20000   | 0.1        | 1       |\n| BTC-USD | Sell | 1479.45 | 0.02       | 2       |",
    "tag": "questdb"
  },
  {
    "title": "Syntax",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/function/timestamp.md",
    "content": "\ntitle: Timestamp function\nsidebar_label: Timestamp\ndescription: Timestamp function reference documentation.\n\n`timestamp(columnName)` elects a\ndesignated timestamp:\n\nduring a CREATE TABLE operation\nduring a SELECT operation\n  (`dynamic timestamp`)\nwhen ingesting data via ILP, for tables that do not already exist in QuestDB,\n  partitions are applied automatically by day by default with a `timestamp`\n  column\n\n:::info\n\n\nChecking if tables contain a designated timestamp column can be done via the\n  `tables()` and `table_columns()` functions which are described in the\n  meta functions documentation page.\n\n\nThe native timestamp format used by QuestDB is a Unix timestamp in microsecond\n  resolution. See\n  Timestamps in QuestDB\n  for more details.\n\n\n:::\nSyntax\nDuring a CREATE operation\nCreate a designated timestamp column\nduring table creation. For more information, refer to the\nCREATE TABLE section.\n\nDuring a SELECT operation\nCreates a designated timestamp column in\nthe result of a query. Assigning a timestamp in a `SELECT` statement\n(`dynamic timestamp`) allows for time series operations such as `LATEST BY`,\n`SAMPLE BY` or `LATEST BY` on tables which do not have a `designated timestamp`\nassigned.\n\nExamples\nDuring a CREATE operation\nThe following creates a table with\ndesignated timestamp.\n`questdb-sql title=\"Create table\"\nCREATE TABLE\ntemperatures(ts timestamp, sensorID symbol, sensorLocation symbol, reading double)\ntimestamp(ts);`\nDuring a SELECT operation\nThe following will query a table and assign a\ndesignated timestamp to the output. Note\nthe use of brackets to ensure the timestamp clause is applied to the result of\nthe query instead of the whole `readings` table.\n`questdb-sql title=\"Dynamic timestamp\"\n(SELECT cast(dateTime AS TIMESTAMP) ts, device, value FROM readings) timestamp(ts);`\nAlthough the `readings` table does not have a designated timestamp, we are able\nto create one on the fly. Now, we can use this into a subquery to perform\ntimestamp operations.\n`questdb-sql title=\"Dynamic timestamp subquery\"\nSELECT ts, avg(value) FROM\n(SELECT cast(dateTime AS TIMESTAMP) ts, value FROM readings) timestamp(ts)\nSAMPLE BY 1d;`\nIf the data is unordered, it is important to order it first.\n```questdb-sql title=\"Dynamic timestamp - unordered data\"\nSELECT ts, avg(value) FROM\n(SELECT ts, value FROM unordered_readings ORDER BY ts) timestamp(ts)\nSAMPLE BY 1d;",
    "tag": "questdb"
  },
  {
    "title": "avg",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/function/aggregation.md",
    "content": "\ntitle: Aggregate functions\nsidebar_label: Aggregate\ndescription: Aggregate functions reference documentation.\n\nThis page describes the available functions to assist with performing aggregate\ncalculations.\navg\n`avg(value)` calculates simple average of values ignoring missing data (e.g\n`null` values).\nArguments\n\n`value` is any numeric value.\n\nReturn value\nReturn value type is `double`.\nExamples\n`questdb-sql title=\"Average transaction amount\"\nSELECT avg(amount) FROM transactions;`\n| avg  |\n| :--- |\n| 22.4 |\n`questdb-sql title=\"Average transaction amount by payment_type\"\nSELECT payment_type, avg(amount) FROM transactions;`\n| payment_type | avg   |\n| :----------- | :---- |\n| cash         | 22.1  |\n| card         | 27.4  |\n| null         | 18.02 |\ncount\n\n`count()` or `count(*)` - counts the number of rows irrespective of underlying data.\n`count(column_name)` - counts the number of non-null values in a given column. \n\nArguments\n\n`count()` does not require arguments.\n`count(column_name)` - supports the following data types: \n`double`\n`float`\n`integer`\n`character`\n`short`\n`byte`\n`timestamp`\n`date`\n`long`\n`long256`\n`geohash`\n`string`\n`symbol`\n\nReturn value\nReturn value type is `long`.\nExamples\nCount of rows in the `transactions` table:\n`questdb-sql\nSELECT count() FROM transactions;`\n| count |\n| :---- |\n| 100   |\nCount of rows in the `transactions` table aggregated by the `payment_type` value:\n`questdb-sql\nSELECT payment_type, count() FROM transactions;`\n| payment_type | count |\n| :----------- | :---- |\n| cash         | 25    |\n| card         | 70    |\n| null         | 5     |\nCount non-null transaction amounts:\n`questdb-sql \nSELECT count(amount) FROM transactions;`\n| count |\n|:------|\n| 95    |\nCount non-null transaction amounts by `payment_type`:\n`questdb-sql \nSELECT payment_type, count(amount) FROM transactions;`\n| payment_type | count |\n|:-------------|:------|\n| cash         | 24    |\n| card         | 67    |\n| null         | 4     |\n:::note\n`null` values are aggregated with `count()`, but not with `count(column_name)`\n:::\ncount_distinct\n`count_distinct(column_name)` - counts distinct values in `string`, `symbol`,\n`long256`, `long`, or `int` columns.\nReturn value\nReturn value type is `long`.\nExamples\n\nCount of distinct sides in the transactions table. Side column can either be\n  `BUY` or `SELL` or `null`\n\n`questdb-sql\nSELECT count_distinct(side) FROM transactions;`\n| count_distinct |\n| :------------- |\n| 2              |\n\nCount of distinct counterparties in the transactions table aggregated by\n  `payment_type` value.\n\n`questdb-sql\nSELECT payment_type, count_distinct(counterparty) FROM transactions;`\n| payment_type | count_distinct |\n| :----------- | :------------- |\n| cash         | 3              |\n| card         | 23             |\n| null         | 5              |\n:::note\n`null` values are not counted in the `count_distinct` function.\n:::\nfirst/last\n\n`first(column_name)` - returns the first value of a column.\n`last(column_name)` - returns the last value of a column.\n\nSupported column datatype: `double`, `float`, `integer`, `character`, `short`,\n`byte`, `timestamp`, `date`, `long`, `geohash`.\nIf a table has a designated timestamp,\nthen the first row is always the row with the lowest timestamp (oldest) and the\nlast row is always the one with the highest (latest) timestamp. For a table\nwithout a designated timestamp column, `first` returns the first row and `last`\nreturns the last inserted row, regardless of any timestamp column.\nReturn value\nReturn value type is `string`.\nExamples\nGiven a table `sensors`, which has a designated timestamp column:\n| device_id  | temperature | ts                          |\n| :--------- | :---------- | :-------------------------- |\n| arduino-01 | 12          | 2021-06-02T14:33:19.970258Z |\n| arduino-02 | 10          | 2021-06-02T14:33:21.703934Z |\n| arduino-03 | 18          | 2021-06-02T14:33:23.707013Z |\nThe following query returns oldest value for the `device_id` column:\n`questdb-sql\nSELECT first(device_id) FROM sensors;`\n| first      |\n| :--------- |\n| arduino-01 |\nThe following query returns the latest symbol value for the `device_id` column:\n`questdb-sql\nSELECT last(device_id) FROM sensors;`\n| last       |\n| :--------- |\n| arduino-03 |\nWithout selecting a designated timestamp column, the table may be unordered and\nthe query may return different result. Given an unordered table\n`sensors_unordered`:\n| device_id  | temperature | ts                          |\n| :--------- | :---------- | :-------------------------- |\n| arduino-01 | 12          | 2021-06-02T14:33:19.970258Z |\n| arduino-03 | 18          | 2021-06-02T14:33:23.707013Z |\n| arduino-02 | 10          | 2021-06-02T14:33:21.703934Z |\nThe following query returns the first record for the `device_id` column:\n`questdb-sql\nSELECT first(device_id) FROM sensors_unordered;`\n| first      |\n| :--------- |\n| arduino-01 |\nThe following query returns the last record for the `device_id` column:\n`questdb-sql\nSELECT last(device_id) FROM sensors_unordered;`\n| last       |\n| :--------- |\n| arduino-02 |\nhaversine_dist_deg\n`haversine_dist_deg(lat, lon, ts)` - calculates the traveled distance for a\nseries of latitude and longitude points.\nArguments\n\n`lat` is the latitude expressed as degrees in decimal format (`double`)\n`lon` is the longitude expressed as degrees in decimal format (`double`)\n`ts` is the `timestamp` for the data point\n\nReturn value\nReturn value type is `double`.\nExamples\n`questdb-sql title=\"Calculate the aggregate traveled distance for each car_id\"\nSELECT car_id, haversine_dist_deg(lat, lon, k)\n  FROM table rides`\nksum\n`ksum(value)` - adds values ignoring missing data (e.g `null` values). Values\nare added using the\nKahan compensated sum algorithm.\nThis is only beneficial for floating-point values such as `float` or `double`.\nArguments\n\n`value` is any numeric value.\n\nReturn value\nReturn value type is the same as the type of the argument.\nExamples\n`questdb-sql\nSELECT ksum(a)\nFROM (SELECT rnd_double() a FROM long_sequence(100));`\n| ksum              |\n| :---------------- |\n| 52.79143968514029 |\nmax\n`max(value)` - returns the highest value ignoring missing data (e.g `null`\nvalues).\nArguments\n\n`value` is any numeric value\n\nReturn value\nReturn value type is the same as the type of the argument.\nExamples\n`questdb-sql title=\"Highest transaction amount\"\nSELECT max(amount) FROM transactions;`\n| max  |\n| :--- |\n| 55.3 |\n`questdb-sql title=\"Highest transaction amount by payment_type\"\nSELECT payment_type, max(amount) FROM transactions;`\n| payment_type | amount |\n| :----------- | :----- |\n| cash         | 31.5   |\n| card         | 55.3   |\n| null         | 29.2   |\nmin\n`min(value)` - returns the lowest value ignoring missing data (e.g `null`\nvalues).\nArguments\n\n`value` is any numeric value\n\nReturn value\nReturn value type is the same as the type of the argument.\nExamples\n`questdb-sql title=\"Lowest transaction amount\"\nSELECT min(amount) FROM transactions;`\n| min  |\n| :--- |\n| 12.5 |\n`questdb-sql title=\"Lowest transaction amount, by payment_type\"\nSELECT payment_type, min(amount) FROM transactions;`\n| payment_type | min  |\n| :----------- | :--- |\n| cash         | 12.5 |\n| card         | 15.3 |\n| null         | 22.2 |\nnsum\n`nsum(value)` - adds values ignoring missing data (e.g `null` values). Values\nare added using the\nNeumaier sum algorithm.\nThis is only beneficial for floating-point values such as `float` or `double`.\nArguments\n\n`value` is any numeric value.\n\nReturn value\nReturn value type is `double`.\nExamples\n`questdb-sql\nSELECT nsum(a)\nFROM (SELECT rnd_double() a FROM long_sequence(100));`\n| nsum             |\n| :--------------- |\n| 49.5442334742831 |\nstddev_samp\n`stddev_samp(value)` - calculates the sample standard deviation of values\nignoring missing data (e.g `null` values).\nArguments\n\n`value` is any numeric value.\n\nReturn value\nReturn value type is `double`.\nExamples\n`questdb-sql\nSELECT stddev_samp(x)\nFROM (SELECT x FROM long_sequence(100));`\n| stddev_samp     |\n| :-------------- |\n| 29.011491975882 |\nsum\n`sum(value)` - adds values ignoring missing data (e.g `null` values).\nArguments\n\n`value` is any numeric value.\n\nReturn value\nReturn value type is the same as the type of the argument.\nExamples\n`questdb-sql title=\"Sum all quantities in the transactions table\"\nSELECT sum(quantity) FROM transactions;`\n| sum |\n| :-- |\n| 100 |\n`questdb-sql title=\"Sum all quantities in the transactions table, aggregated by item\"\nSELECT item, sum(quantity) FROM transactions;`\n| item   | count |\n| :----- | :---- |\n| apple  | 53    |\n| orange | 47    |\nOverflow\n`sum` does not perform overflow check. To avoid overflow, you can cast the\nargument to wider type.\n```questdb-sql title=\"Cast as long to avoid overflow\"\nSELECT sum(cast(a AS LONG)) FROM table;",
    "tag": "questdb"
  },
  {
    "title": "base64",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/function/binary.md",
    "content": "\ntitle: Binary functions\nsidebar_label: Binary\ndescription: Binary function reference documentation.\n\nThis page describes the available functions to assist with working with binary data.\nbase64\n`base64(data, maxLength)` encodes raw binary data using the base64 encoding into\na string with a maximum length defined by `maxLength`.\nArguments:\n\n`data` is the binary data to be encoded.\n`maxLength` is the intended maximum length of the encoded string.\n\nReturn value:\nReturn value type is `string`.\n:::tip\nrnd_bin can be used to generate random binary data.\n:::\nExample:\n`questdb-sql\nSELECT base64(rnd_bin(), 20);`\n| base64                       |\n| ---------------------------- |",
    "tag": "questdb"
  },
  {
    "title": "rnd_geohash",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/function/spatial.md",
    "content": "\ntitle: Geospatial functions\nsidebar_label: Spatial\ndescription: Geospatial functions reference documentation.\n\nSpatial functions allow for operations relating to the geohash types which\nprovide geospatial data support. For more information on this type of data, see\nthe geohashes documentation and the\noperators which help with filtering data.\nrnd_geohash\n`rnd_geohash(bits)` returns a random geohash of variable precision.\nArguments:\n`bits` - an integer between `1` and `60` which determines the precision of the\ngenerated geohash.\nReturn value:\nReturns a `geohash`\nExamples:\n`questdb-sql\nSELECT rnd_geohash(7) g7,\n      rnd_geohash(10) g10,\n      rnd_geohash(30) g30,\n      rnd_geohash(29) g29,\n      rnd_geohash(60) g60\nFROM long_sequence(5);`\n| g7      | g10 | g30    | g29                           | g60          |\n| ------- | --- | ------ | ----------------------------- | ------------ |\n| 1101100 | 4h  | hsmmq8 | 01110101011001101111110111011 | rjtwedd0z72p |\n| 0010011 | vf  | f9jc1q | 10101111100101111111101101101 | fzj09w97tj1h |\n| 0101011 | kx  | fkhked | 01110110010001001000110001100 | v4cs8qsnjkeh |\n| 0000001 | 07  | qm99sm | 11001010011011000010101100101 | hrz9gq171nc5 |\n| 0101011 | 6t  | 3r8jb5 | 11011101010111001010010001010 | fm521tq86j2c |\nmake_geohash\n`make_geohash(lon, lat, bits)` returns a geohash equivalent of latitude and\nlongitude, with precision specified in bits.\n:::info\n`make_geohash()` is intended to be used via SQL over HTTP / PostgreSQL wire\nprotocol, for use within Java (embedded) scenario, see the\nJava embedded documentation for geohashes.\n:::\nArguments:\n\n`lon` - longitude coordinate as a floating point value with up to eight\n  decimal places\n`lat` - latitude coordinate as a floating point value with up to eight decimal\n  places\n`bits` - an integer between `1` and `60` which determines the precision of the\n  generated geohash.\n\nThe latitude and longitude arguments may be constants, column values or the\nresults of a function which produces them.\nReturn value:\nReturns a `geohash`.\n\nIf latitude and longitude comes from constants and is incorrect, an error is\n  thrown\nIf column values have invalid lat / long coordinates, this produces `null`.\n\nExamples:\n```questdb-sql\nSELECT make_geohash(142.89124148, -12.90604153, 40)",
    "tag": "questdb"
  },
  {
    "title": "isOrdered",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/function/boolean.md",
    "content": "\ntitle: Boolean functions\nsidebar_label: Boolean\ndescription: Boolean function reference documentation.\n\nThis page describes the available functions to assist with performing boolean\ncalculations on numeric and timestamp types.\nisOrdered\n`isOrdered(column)` return a `boolean` indicating whether the column values are\nordered in a table.\nArguments:\n\n`column` is a column name of numeric or timestamp type.\n\nReturn value:\nReturn value type is `boolean`.\nExamples:\nGiven a table with the following contents:\n|numeric_sequence|ts                         |\n|:---------------|:--------------------------|\n|1               |2021-05-01T11:00:00.000000Z|\n|2               |2021-05-01T12:00:00.000000Z|\n|3               |2021-05-01T13:00:00.000000Z|\n`questdb-sql\nSELECT isOrdered(numeric_sequence) is_num_ordered,\n       isOrdered(ts) is_ts_ordered\nFROM my_table`\n|is_num_ordered|is_ts_ordered|\n|:-------------|:------------|\n|true          |true         |\nAdding an integer and timestamp rows out-of-order\n|numeric_sequence|ts                         |\n|:---------------|:--------------------------|\n|1               |2021-05-01T11:00:00.000000Z|\n|2               |2021-05-01T12:00:00.000000Z|\n|3               |2021-05-01T13:00:00.000000Z|\n|2               |2021-05-01T12:00:00.000000Z|\n`questdb-sql\nSELECT isOrdered(numeric_sequence) FROM my_table`\n|is_num_ordered|is_ts_ordered|\n|:-------------|:------------|",
    "tag": "questdb"
  },
  {
    "title": "timestamp_sequence",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/function/timestamp-generator.md",
    "content": "\ntitle: Timestamp generator\nsidebar_label: Timestamp generator\ndescription: Timestamp generator function reference documentation.\n\nThe `timestamp_sequence()` function may be used as a timestamp generator to\ncreate data for testing. Pseudo-random steps can be achieved by providing a\nrandom function to the\n`step` argument. A `seed` value may be provided to a random function if the\nrandomly-generated `step` should be deterministic.\ntimestamp_sequence\n\n\n`timestamp_sequence(startTimestamp, step)` generates a sequence of `timestamp`\n  starting at `startTimestamp`, and incrementing by a `step` set as a `long`\n  value in microseconds. This `step` can be either;\n\n\na static value, in which case the growth will be monotonic, or\n\n\na randomized value, in which case the growth will be randomized. This is\n    done using\n    random value generator functions.\n\n\nArguments:\n\n`startTimestamp`: is a `timestamp` representing the starting (i.e lowest)\n  generated timestamp in the sequence.\n`step`: is a `long` representing the interval between 2 consecutive generated\n  timestamps in `microseconds`.\n\nReturn value:\nReturn value type is `timestamp`.\nExamples:\n`questdb-sql title=\"Monotonic timestamp increase\"\nSELECT x, timestamp_sequence(\n            to_timestamp('2019-10-17T00:00:00', 'yyyy-MM-ddTHH:mm:ss'),\n            100000L)\nFROM long_sequence(5);`\n| x   | timestamp_sequence          |\n| --- | --------------------------- |\n| 1   | 2019-10-17T00:00:00.000000Z |\n| 2   | 2019-10-17T00:00:00.100000Z |\n| 3   | 2019-10-17T00:00:00.200000Z |\n| 4   | 2019-10-17T00:00:00.300000Z |\n| 5   | 2019-10-17T00:00:00.400000Z |\n`questdb-sql title=\"Randomized timestamp increase\"\nSELECT x, timestamp_sequence(\n            to_timestamp('2019-10-17T00:00:00', 'yyyy-MM-ddTHH:mm:ss'),\n            rnd_short(1,5) * 100000L)\nFROM long_sequence(5);`\n| x   | timestamp_sequence          |\n| --- | --------------------------- |\n| 1   | 2019-10-17T00:00:00.000000Z |\n| 2   | 2019-10-17T00:00:00.100000Z |\n| 3   | 2019-10-17T00:00:00.600000Z |\n| 4   | 2019-10-17T00:00:00.900000Z |",
    "tag": "questdb"
  },
  {
    "title": "sin",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/function/trigonometric.md",
    "content": "\ntitle: Trigonometric functions\nsidebar_label: Trigonometric\ndescription: Trigonometric function reference documentation.\n\nThis page describes the available functions to assist with performing\ntrigonometric calculations.\n:::tip\nPositive and negative infinity values are expressed as `'Infinity'` or\n`'-Infinity'` in QuestDB.\n:::\nsin\n`sin(angleRadians)` returns the trigonometric sine of an angle.\nArguments\n\n`angleRadians` is a numeric value indicating the angle in radians.\n\nReturn value\nReturn value type is `double`.\nDescription\nSpecial case: if the argument is `NaN` or an infinity, then the result is `Null`.\nExamples\n`questdb-sql\nSELECT pi()/2 angle, sin(pi()/2) sin;`\n| angle          | sin |\n| -------------- | --- |\n| 1.570796326794 | 1   |\ncos\n`cos(angleRadians)` returns the trigonometric cosine of an angle.\nArguments\n\n`angleRadians` numeric value for the angle, in radians.\n\nReturn value\nReturn value type is `double`.\nDescription\nSpecial case: if the argument is `NaN` or an infinity, then the result is `Null`.\nExamples\n`questdb-sql\nSELECT pi()/2 angle, cos(pi()/2) cos;`\n| angle          | cos                   |\n| -------------- | --------------------- |\n| 1.570796326794 | 6.123233995736766e-17 |\ntan\n`tan(angleRadians)` returns the trigonometric tangent of an angle.\nArguments\n\n`angleRadians` numeric value for the angle, in radians.\n\nReturn value\nReturn value type is `double`.\nDescription\nSpecial case: if the argument is `NaN` or an infinity, then the result is `Null`.\nExamples\n`questdb-sql\nSELECT pi()/2 angle, tan(pi()/2) tan;`\n| angle          | tan               |\n| -------------- | ----------------- |\n| 1.570796326794 | 16331239353195370 |\ncot\n`cot(angleRadians)` returns the trigonometric cotangent of an angle.\nArguments\n\n`angleRadians` numeric value for the angle, in radians.\n\nReturn value\nReturn value type is `double`.\nDescription\nSpecial case: if the argument is `NaN`, 0, or an infinity, then the result is `Null`.\n\nExamples\n`questdb-sql\nSELECT pi()/2 angle, cot(pi()/2) cot;`\n| angle          | cot                   |\n| -------------- | --------------------- |\n| 1.570796326794 | 6.123233995736766e-17 |\nasin\n`asin(value)` the arcsine of a value.\nArguments\n\n`value` is a numeric value whose arcsine is to be returned.\n\nReturn value\nReturn value type is `double`. The returned angle is between -pi/2 and pi/2\ninclusively.\nDescription\nSpecial case: if the argument is `NaN` or an infinity, then the result is `Null`.\nExamples\n`questdb-sql\nSELECT asin(1.0) asin;`\n| asin           |\n| -------------- |\n| 1.570796326794 |\nacos\n`acos(value)` returns the arccosine of a value.\nArguments\n\n`value` is a numeric value whose arccosine is to be returned. The returned\n  angle is between 0.0 and pi inclusively.\n\nReturn value\nReturn value type is `double`.\nDescription\nSpecial cases: if the argument is `NaN` or its absolute value is greater than 1, then the\nresult is `Null`.\nExamples\n`questdb-sql\nSELECT acos(0.0) acos;`\n| acos           |\n| -------------- |\n| 1.570796326794 |\natan\n`atan(value)` returns the arctangent of a value.\nArguments\n\n`value` is a numeric value whose arctangent is to be returned.\n\nReturn value\nReturn value type is `double`. The returned angle is between -pi/2 and pi/2\ninclusively.\nDescription\nSpecial cases:\n\nIf the argument is `NaN`, then the result is `Null`.\nIf the argument is infinity, then the result is the closest value to pi/2 with\n  the same sign as the input.\n\nExamples\nSpecial case where input is `'-Infinity'`:\n`questdb-sql\nSELECT atan('-Infinity');`\nReturns the closest value to pi/2 with the same sign as the input:\n| atan            |\n| --------------- |\n| -1.570796326794 |\n`questdb-sql\nSELECT atan(1.0) atan;`\n| atan           |\n| -------------- |\n| 0.785398163397 |\natan2\n`atan2(valueY, valueX)` returns the angle theta from the conversion of\nrectangular coordinates (x, y) to polar (r, theta). This function computes\ntheta (the phase) by computing an arctangent of y/x in the range of -pi to pi\ninclusively.\nArguments\n\n`valueY` numeric ordinate coordinate.\n`valueX` numeric abscissa coordinate.\n\n:::note\nThe arguments to this function pass the y-coordinate first and the x-coordinate\nsecond.\n:::\nReturn value\nReturn value type is `double` between -pi and pi inclusively.\nDescription:\n`atan2(valueY, valueX)` measures the counterclockwise angle theta, in radians,\nbetween the positive x-axis and the point (x, y):\n\nSpecial cases:\n| input `valueY`        | input `valueX` | `atan2` return value               |\n| --------------------- | -------------- | ---------------------------------- |\n| 0                     | Positive value | 0                                  |\n| Positive finite value | 'Infinity'     | 0                                  |\n| -0                    | Positive value | 0                                  |\n| Negative finite value | 'Infinity'     | 0                                  |\n| 0                     | Negative value | Double value closest to pi         |\n| Positive finite value | '-Infinity'    | Double value closest to pi         |\n| -0                    | Negative value | Double value closest to -pi        |\n| Negative finite value | '-Infinity'    | Double value closest to -pi        |\n| Positive value        | 0 or -0        | Double value closest to pi/2       |\n| 'Infinity'            | Finite value   | Double value closest to pi/2       |\n| Negative value        | 0 or -0        | Double value closest to -pi/2      |\n| '-Infinity'           | Finite value   | Double value closest to -pi/2      |\n| 'Infinity'            | 'Infinity'     | Double value closest to pi/4       |\n| 'Infinity'            | '-Infinity'    | Double value closest to 3/4 * pi  |\n| '-Infinity'           | 'Infinity'     | Double value closest to -pi/4      |\n| '-Infinity'           | '-Infinity'    | Double value closest to -3/4 * pi |\nExamples\n`questdb-sql\nSELECT atan2(1.0, 1.0) atan2;`\n| atan2          |\n| -------------- |\n| 0.785398163397 |\nradians\n`radians(angleDegrees)` converts an angle measured in degrees to the equivalent\nangle measured in radians.\nArguments\n\n`angleDegrees` numeric value for the angle in degrees.\n\nReturn value\nReturn value type is `double`.\nExamples\n`questdb-sql\nSELECT radians(180);`\n| radians        |\n| -------------- |\n| 3.141592653589 |\ndegrees\n`degrees(angleRadians)` converts an angle measured in radians to the equivalent\nangle measured in degrees.\nArguments\n\n`angleRadians` numeric value for the angle in radians.\n\nReturn value\nReturn value type is `double`.\nExamples\n`questdb-sql\nSELECT degrees(pi());`\n| degrees |\n| ------- |\n| 180     |\npi\n`pi()` returns the constant pi as a double.\nArguments\nNone.\nReturn value\nReturn value type is `double`.\nExamples\n`questdb-sql\nSELECT pi();`\n| pi             |\n| -------------- |",
    "tag": "questdb"
  },
  {
    "title": "Examples",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/api/postgres.md",
    "content": "\ntitle: Postgres\ndescription: Postgres compatibility reference documentation.\n\nQuestDB supports the Postgres wire protocol. As a result, QuestDB is capable of\nrunning most of Postgres queries. This means that you can use your favorite\nPostgres client or driver with QuestDB, at no extra cost.\nThe storage model used by Postgres is fundamentally different to the one used by\nQuestDB. Some features that exists for Postgres do not apply to QuestDB.\nExamples\nWe provide examples in a number of programming languages. See our \"develop\" docs\nfor:\n\nInserting\nQuerying\nUpdating\n\nCompatibility\nList of supported features\n\nQuerying (all types expect `BLOB`)\nPrepared statements with bind parameters (check for specific libraries\n  below)\n`INSERT` statements with bind parameters\n`UPDATE` statements with bind parameters\nDDL execution\nBatch inserts with `JDBC`\nPlain authentication\n\nExamples which demonstrate how to use Postgres clients in a number of different\nlanguages can be found on the following pages:\n\nInsert data demonstrates\n  how to use the parameterized queries and prepared statements to insert data.\nQuery data shows how to\n  run queries against tables.\nUpdate data shows how to\n  update tables.\n\nList of supported connection properties\n| Name       | Example                    | Description                                                                                                                          |\n| ---------- | -------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ |\n| `database` | qdb                        | Should be set to any value for example `qdb`, database name is ignored, QuestDB does not have database instance name                 |\n| `user`     | admin                      | User name configured in `pg.user` or `pg.readonly.user` property in `server.conf`. Default value is `admin`                          |\n| `password` | quest                      | Password from `pg.password` or `pg.readonly.password` property in `server.conf`. Default value is `quest`                            |\n| `options`  | -c statement_timeout=60000 | The only supported option is `statement_timeout`. It specifies maximum execution time in milliseconds for SELECT or UPDATE statement |\nList of unsupported features\n\nSSL\nRemote file upload (`COPY` from `stdin`)\n`DELETE` statements\n`BLOB` transfer\n\nRecommended third party tools\nThe following list of third party tools includes drivers, clients or utility\nCLIs that our team has tested extensively. Picking an item from it will\nguarantee that your code will work with QuestDB.\nWe recognize that our community might value some features more than others. This\nis why we encourage you to open an issue on GitHub if\nyou think we are missing something important for your workflow.\nCLIs\nPSQL `12`\nSupport for `SELECT`, `INSERT`, `UPDATE`, `CREATE`, `DROP`, `TRUNCATE`.\nLibraries / Programmatic clients\nnode-postgres (NodeJS) `8.4`\npq (Go) `1.8`\npq (C) `12`\nPsycopg (Python) `2.9.3` and `3.1`\nruby-pg (Ruby) `1.4.3`\npg_connect (PHP) `8.1.0`\nDrivers",
    "tag": "questdb"
  },
  {
    "title": "Writing data",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/api/java-embedded.md",
    "content": "\ntitle: Java (embedded)\ndescription: Java embedded API reference documentation.\n\nimport CodeBlock from \"@theme/CodeBlock\"\nimport InterpolateReleaseData from \"../../../src/components/InterpolateReleaseData\"\nQuestDB is written in Java and can be used as any other Java library. Moreover,\nit is a single JAR with no additional dependencies.\nTo include QuestDB in your project, use the following:\nimport Tabs from \"@theme/Tabs\"\nimport TabItem from \"@theme/TabItem\"\n\n\n {\n    return (\n      \n        {`<dependency>\n  <groupId>org.questdb</groupId>\n  <artifactId>questdb</artifactId>\n  <version>${release.name}</version>\n</dependency>`}\n      \n    )\n  }}\n/>\n {\n    return (\n      \n        {`<dependency>\n  <groupId>org.questdb</groupId>\n  <artifactId>questdb</artifactId>\n  <version>${release.name}-jdk8</version>\n</dependency>`}\n      \n    )\n  }}\n/>\n\n\n {\n    return (\n      \n        implementation 'org.questdb:questdb:{release.name}'\n      \n    )\n  }}\n/>\n {\n    return (\n      \n        implementation 'org.questdb:questdb:{release.name}-jdk8'\n      \n    )\n  }}\n/>\n\n\nWriting data\nThis section provides example codes to write data to WAL and non-WAL tables. See\nWrite Ahead Log for details about the\ndifferences between WAL and non-WAL tables.\nThe following writers are available for data ingestion:\n\n`WalWriter` for WAL tables\n`TableWriter` for non-WAL tables\n`TableWriterAPI` for both WAL and non-WAL tables as it is an interface for\n  `WalWriter` and `Table Writer`\n\nWriting data using `WalWriter`\nThe `WalWriter` facilitates table writes to WAL tables. To successfully create\nan instance of `WalWriter`, the table must already exist.\n```java title=\"Example WalWriter\"\ntry (CairoEngine engine = new CairoEngine(configuration)) {\n    final SqlExecutionContext ctx = new SqlExecutionContextImpl(engine, 1);\n    try (SqlCompiler compiler = new SqlCompiler(engine)) {\n        compiler.compile(\"create table testTable (\" +\n                \"a int, b byte, c short, d long, e float, g double, h date, \" +\n                \"i symbol, j string, k boolean, l geohash(8c), ts timestamp\" +\n                \") timestamp(ts) partition by day WAL\", ctx);\n\n\n```    // write data into WAL\n    final TableToken tableToken = engine.getTableToken(\"testTable\");\n    try (WalWriter writer = engine.getWalWriter(ctx.getCairoSecurityContext(), tableToken)) {\n        for (int i = 0; i < 3; i++) {\n            TableWriter.Row row = writer.newRow(Os.currentTimeMicros());\n            row.putInt(0, 123);\n            row.putByte(1, (byte) 1111);\n            row.putShort(2, (short) 222);\n            row.putLong(3, 333);\n            row.putFloat(4, 4.44f);\n            row.putDouble(5, 5.55);\n            row.putDate(6, System.currentTimeMillis());\n            row.putSym(7, \"xyz\");\n            row.putStr(8, \"abc\");\n            row.putBool(9, true);\n            row.putGeoHash(10, GeoHashes.fromString(\"u33dr01d\", 0, 8));\n            row.append();\n        }\n        writer.commit();\n    }\n\n    // apply WAL to the table\n    try (ApplyWal2TableJob walApplyJob = new ApplyWal2TableJob(engine, 1, 1)) {\n        while (walApplyJob.run(0));\n    }\n}\n```\n\n\n}\n```\nWriting data using `TableWriter`\nNon-WAL tables do not allow concurrent writes via multiple interfaces. To\nsuccessfully create an instance, the table must:\n\nAlready exist\nHave no other open writers against it as the `TableWriter` constructor will\n  attempt to obtain an exclusive cross-process lock on the table.\n\n```java title=\"Example TableWriter\"\nfinal CairoConfiguration configuration = new DefaultCairoConfiguration(\"dbRoot\");\ntry (CairoEngine engine = new CairoEngine(configuration)) {\n    final SqlExecutionContext ctx = new SqlExecutionContextImpl(engine, 1);\n    try (SqlCompiler compiler = new SqlCompiler(engine)) {\n        compiler.compile(\"create table testTable (\" +\n                \"a int, b byte, c short, d long, e float, g double, h date, \" +\n                \"i symbol, j string, k boolean, l geohash(8c), ts timestamp\" +\n                \") timestamp(ts) partition by day\", ctx);\n\n\n```    // write data directly into the table\n    final TableToken tableToken = engine.getTableToken(\"testTable\");\n    try (TableWriter writer = engine.getWriter(ctx.getCairoSecurityContext(), tableToken, \"test\")) {\n        for (int i = 0; i < 11; i++) {\n            TableWriter.Row row = writer.newRow(Os.currentTimeMicros());\n            row.putInt(0, 123);\n            row.putByte(1, (byte) 1111);\n            row.putShort(2, (short) 222);\n            row.putLong(3, 333);\n            row.putFloat(4, 4.44f);\n            row.putDouble(5, 5.55);\n            row.putDate(6, System.currentTimeMillis());\n            row.putSym(7, \"xyz\");\n            row.putStr(8, \"abc\");\n            row.putBool(9, true);\n            row.putGeoHash(10, GeoHashes.fromString(\"u33dr01d\", 0, 8));\n            row.append();\n        }\n        writer.commit();\n    }\n}\n```\n\n\n}\n```\nWriting data using `TableWriterAPI`\n`TableWriterAPI` allows writing to both WAL and non-WAL tables by returning the\nsuitable `Writer` based on the table configurations. The table must already\nexist:\n```java title=\"Example TableWriterAPI\"\ntry (CairoEngine engine = new CairoEngine(configuration)) {\n    final SqlExecutionContext ctx = new SqlExecutionContextImpl(engine, 1);\n    try (SqlCompiler compiler = new SqlCompiler(engine)) {\n        compiler.compile(\"create table testTable (\" +\n                \"a int, b byte, c short, d long, e float, g double, h date, \" +\n                \"i symbol, j string, k boolean, l geohash(8c), ts timestamp\" +\n                \") timestamp(ts) partition by day WAL\", ctx);\n\n\n```    // write data into the table\n    final TableToken tableToken = engine.getTableToken(\"testTable\");\n    try (TableWriterAPI writer = engine.getTableWriterAPI(ctx.getCairoSecurityContext(), tableToken, \"test\")) {\n        for (int i = 0; i < 3; i++) {\n            TableWriter.Row row = writer.newRow(Os.currentTimeMicros());\n            row.putInt(0, 123);\n            row.putByte(1, (byte) 1111);\n            row.putShort(2, (short) 222);\n            row.putLong(3, 333);\n            row.putFloat(4, 4.44f);\n            row.putDouble(5, 5.55);\n            row.putDate(6, System.currentTimeMillis());\n            row.putSym(7, \"xyz\");\n            row.putStr(8, \"abc\");\n            row.putBool(9, true);\n            row.putGeoHash(10, GeoHashes.fromString(\"u33dr01d\", 0, 8));\n            row.append();\n        }\n        writer.commit();\n    }\n\n    // apply WAL to the table\n    try (ApplyWal2TableJob walApplyJob = new ApplyWal2TableJob(engine, 1, 1)) {\n        while (walApplyJob.run(0));\n    }\n}\n```\n\n\n}\n```\nDetailed steps\nConfigure Cairo engine\nCairoEngine is a resource manager for the embedded QuestDB. Its main function is\nto facilitate concurrent access to pools of `TableReader` and suitable writer\ninstances.\n`java title=\"New CairoEngine instance\"\nfinal CairoConfiguration configuration = new DefaultCairoConfiguration(\"data_dir\");\ntry (CairoEngine engine = new CairoEngine(configuration)) {`\nA typical application will need only one instance of `CairoEngine`. This\ninstance will start when the application starts and shuts down when the\napplication closes. You will need to close `CairoEngine` gracefully when the\napplication stops.\nQuestDB provides a default configuration which only requires the\n`data directory` to be specified. For a more advanced usage, the whole\n`CairoConfiguration` interface can be overridden.\nCreate an instance of SqlExecutionContext\nExecution context is a conduit for passing SQL execution artifacts to the\nexecution site. This instance is not thread-safe and it must not be shared\nbetween threads.\n`java title=\"Example of execution context\"\nfinal SqlExecutionContextImpl ctx = new SqlExecutionContextImpl(engine, 1);`\nThe second argument is the number of threads that will be helping to execute SQL\nstatements. Unless you are building another QuestDB server, this value should\nalways be 1.\nNew SqlCompiler instance and blank table\nBefore we start writing data using a writer, the target table has to exist.\nThere are several ways to create a new table and we recommend using\n`SqlCompiler`:\n```java title=\"Creating new table\"\n// Create a non-WAL table:\ntry (SqlCompiler compiler = new SqlCompiler(engine)) {\n    compiler.compile(\"create table abc (a int, b byte, c short, d long, e float, g double, h date, i symbol, j string, k boolean, l geohash(8c), ts timestamp) timestamp(ts) bypass wal\", ctx);\n// Create a WAL table:\ntry (SqlCompiler compiler = new SqlCompiler(engine)) {\n    compiler.compile(\"create table abc (a int, b byte, c short, d long, e float, g double, h date, i symbol, j string, k boolean, l geohash(8c), ts timestamp) timestamp(ts) wal\", ctx);\n```\nAs you will be able to see below, the table field types and indexes must match\nthe code that is populating the table.\nA new writer instance\nWe use `engine` to create an instance of the writer. This will enable reusing\nthis writer instance later, when we use the same method of creating table writer\nagain.\n`java title=\"New table writer instance for a non-WAL table\"\nfinal TableToken tableToken = engine.getTableToken(\"abc\");\ntry (TableWriter writer = engine.getWriter(ctx.getCairoSecurityContext(), tableToken, \"testing\")) {`\n`java title=\"New table writer instance for a WAL table\"\nfinal TableToken tableToken = engine.getTableToken(\"abc\");\ntry (WalWriter writer = engine.getWalWriter(ctx.getCairoSecurityContext(), tableToken)) {`\n`java title=\"New table writer instance for either a WAL or non-WAL table\"\nfinal TableToken tableToken = engine.getTableToken(\"abc\");\ntry (TableWriterAPI writer = engine.getTableWriterAPI(ctx.getCairoSecurityContext(), tableToken, \"testing\")) {`\n`TableWriter` - A non-WAL table uses `TableWriter`, which will hold an exclusive\nlock on table `abc` until it is closed and `testing` will be used as the lock\nreason. This lock is both intra- and inter-process. If you have two Java\napplications accessing the same table only one will succeed at one time.\n`WalWriter` - A WAL table uses `WalWriter` to enable concurrent data ingestion,\ndata modification, and schema changes, as the table is not locked.\n`TableWriterAPI` - Both WAL and Non-WAL tables can use `TableWriterAPI`. It is\nan interface implemented by both writers.\nCreate a new row\n```java title=\"Creating new table row with timestamp\"\nTableWriter.Row row = writer.newRow(Os.currentTimeMicros());\n```\nAlthough this operation semantically looks like a new object creation, the row\ninstance is actually being re-used under the hood. A Timestamp is necessary to\ndetermine a partition for the new row. Its value has to be either increment or\nstay the same as the last row. When the table is not partitioned and does not\nhave a designated timestamp column, the timestamp value can be omitted.\n```java title=\"Creating new table row without timestamp\"\nTableWriter.Row row = writer.newRow();\n```\nPopulate columns\nThere are put* methods for every supported data type. Columns are updated by an\nindex as opposed to by name.\n`java title=\"Populating table column\"\nrow.putLong(3, 333);`\nColumn update order is not important and updates can be sparse. All unset\ncolumns will default to NULL values.\nAppend a row\nFollowing method call:\n`java title=\"Appending a new row\"\nrow.append();`\nAppended rows are not visible to readers until they are committed. An unneeded\nrow can also be canceled if required.\n`java title=\"Cancelling half-populated row\"\nrow.cancel();`\nA pending row is automatically cancelled when `writer.newRow()` is called.\nConsider the following scenario:\n`java\nTableWriter.Row row = writer.newRow(Os.currentTimeMicros());\nrow.putInt(0, 123);\nrow.putByte(1, (byte) 1111);\nrow.putShort(2, (short) 222);\nrow.putLong(3, 333);\nrow = writer.newRow(Os.currentTimeMicros());\n...`\nSecond `newRow()` call would cancel all the updates to the row since the last\n`append()`.\nCommit changes\nTo make changes visible to readers, writer has to commit. `writer.commit` does\nthis job. Unlike traditional SQL databases, the size of the transaction does not\nmatter. You can commit anything between 1 and 1 trillion rows. We also spent\nconsiderable effort to ensure `commit()` is lightweight. You can drip one row at\na time in applications that require such behaviour.\nWriting columns in blocks\nQuestDB supports writing blocks of columnar data at once via the use of the\n`TableBlockWriter`. The `TableBlockWriter` instance is obtained from a\n`TableWriter` and can then be used to write in memory frames of columnar data. A\nframe of columnar data is just a piece of contiguous memory with each column\nvalue stored in it one after another. The `TableBlockWriter` will allow any\nnumber of such frames of columnar data to be written with an invocation of the\n`appendPageFrameColumn` method, before the block is either committed or\ncancelled (rolled back). Use of the `TableBlockWriter` requires that all columns\nhave the same number of rows written to them and within each column the frames\nneed to be added in append order.\nA `PageFrame` instance can optionally be used as a convenient interface to hold\nthe columnar frames and a `PageFrameCursor` instance can be used as an interface\nto provide a sequence of frames to be committed. Many of QuestDB's\n`RecordCursorFactory` implementations provide a `PageFrameCursor`.\n```java title=\"Example table block writer\"\nfinal CairoConfiguration configuration = new DefaultCairoConfiguration(\"data_dir\");\ntry (CairoEngine engine = new CairoEngine(configuration)) {\n    final SqlExecutionContextImpl ctx = new SqlExecutionContextImpl(engine, 1);\n    try (SqlCompiler compiler = new SqlCompiler(engine)) {\n\n\n```    PageFrameCursor cursor = ...; // Setup PageFrameCursor instance\n    compiler.compile(\"create table abc (a int, b byte, c short, d long, e float, g double, h date, i symbol, j string, k boolean, l geohash(8c), ts timestamp) timestamp(ts)\", ctx);\n\n    final TableToken tableToken = engine.getTableToken(\"abc\");\n    try (TableWriter writer = engine.getWriter(ctx.getCairoSecurityContext(), tableToken, \"testing\")) {\n        int columnCount = writer.getMetadata().getColumnCount();\n        TableBlockWriter blockWriter = writer.newBlock();\n\n        PageFrame frame;\n        while ((frame = cursor.next()) != null) {\n            for (int columnIndex = 0; columnIndex < columnCount; columnIndex++) {\n                blockWriter.appendPageFrameColumn(\n                        columnIndex,\n                        frame.getPageSize(columnIndex),\n                        frame.getPageAddress(columnIndex));\n            }\n        }\n        blockWriter.commit();\n    }\n}\n```\n\n\n}\n```\nExecuting queries\nWe provide a single API for executing all kinds of SQL queries. The example\nbelow focuses on `SELECT` and how to fetch data from a cursor.\n`java title=\"Compiling SQL\"\nfinal CairoConfiguration configuration = new DefaultCairoConfiguration(temp.getRoot().getAbsolutePath());\ntry (CairoEngine engine = new CairoEngine(configuration)) {\n    final SqlExecutionContextImpl ctx = new SqlExecutionContextImpl(engine, 1);\n    try (SqlCompiler compiler = new SqlCompiler(engine)) {\n        try (RecordCursorFactory factory = compiler.compile(\"abc\", ctx).getRecordCursorFactory()) {\n            try (RecordCursor cursor = factory.getCursor(ctx)) {\n                final Record record = cursor.getRecord();\n                while (cursor.hasNext()) {\n                    // access 'record' instance for field values\n                }\n            }\n        }\n    }\n}`\nDetailed steps\nThe steps to setup CairoEngine, execution context and SqlCompiler are the same\nas those we explained in writing data section. We will skip\nthem here and focus on fetching data.\nRecordCursorFactory\nYou can think of `RecordCursorFactory` as PreparedStatement. This is the entity\nthat holds SQL execution plan with all of the execution artefacts. Factories are\ndesigned to be reused and we strongly encourage caching them. You also need to\nmake sure that you close factories explicitly when you no longer need them.\nFailing to do so can cause memory and/or other resources leak.\nRecordCursor\nThis instance allows iterating over the dataset produced by SQL. Cursors are\nrelatively short-lived and do not imply fetching all the data. Note that you\nhave to close a cursor as soon as enough data is fetched ; the closing process\ncan happen at any time.\nCursors are not thread safe and cannot be shared between threads.\nRecord\nThis is cursor's data access API. Record instance is obtained from the cursor\noutside of the fetch loop.\n`java title=\"Example of fetching data from cursor\"\nfinal Record record = cursor.getRecord();\nwhile (cursor.hasNext()) {\n    // access 'record' instance for field values\n}`\nRecord does not hold the data. Instead, it is an API to pull data when data is\nneeded. Record instance remains the same while cursor goes over the data, making\ncaching of records pointless.\nInfluxDB sender library\nWe have a Java ILP client library to allow",
    "tag": "questdb"
  },
  {
    "title": "Examples",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/api/rest.md",
    "content": "\ntitle: REST API\nsidebar_label: REST\ndescription: REST API reference documentation.\n\nThe QuestDB REST API is based on standard HTTP features and is understood by\noff-the-shelf HTTP clients. It provides a simple way to interact with QuestDB\nand is compatible with most programming languages. API functions are fully keyed\non the URL and they use query parameters as their arguments.\nThe Web Console is the official Web client relying on the REST API. Find out\nmore in the section using the Web Console.\nAvailable methods\n\n/imp for importing data from `.CSV` files\n/exec to execute a SQL statement\n/exp to export data\n\nExamples\nWe provide examples in a number of programming languages. See our \"develop\" docs\nfor:\n\nInserting\nQuerying\n\n/imp - Import data\n`/imp` streams tabular text data directly into a table. It supports CSV, TAB and\npipe (`|`) delimited inputs with optional headers. There are no restrictions on\ndata size. Data types and structures are detected automatically, without\nadditional configuration. In some cases, additional configuration can be\nprovided to improve the automatic detection as described in\nuser-defined schema.\n:::note\nThe structure detection algorithm analyses the chunk in the beginning of the\nfile and relies on relative uniformity of data. When the first chunk is\nnon-representative of the rest of the data, automatic imports can yield errors.\nIf the data follows a uniform pattern, the number of lines which are analyzed\nfor schema detection can be reduced to improve performance during uploads using\nthe `http.text.analysis.max.lines` key. Usage of this setting is described in\nthe\nHTTP server configuration\ndocumentation.\n:::\nURL parameters\n`/imp` is expecting an HTTP POST request using the `multipart/form-data`\nContent-Type with following optional URL parameters which must be URL encoded:\n| Parameter     | Required | Default          | Description                                                                                                                                                                                                                                                      |\n| ------------- | -------- | ---------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `atomicity`   | No       | `2`              | `0`, `1` or `2`. Behaviour when an error is detected in the data. `0`: the entire file will be skipped. `1`: the row is skipped. `2`: the column is skipped.                                                                                                     |\n| `delimiter`   | No       |                  | URL encoded delimiter character. When set, import will try to detect the delimiter automatically. Since automatic delimiter detection requires at least two lines (rows) to be present in the file, this parameter may be used to allow single line file import. |\n| `durable`     | No       | `false`          | `true` or `false`. When set to `true`, import will be resilient against OS errors or power losses by forcing the data to be fully persisted before sending a response back to the user.                                                                          |\n| `fmt`         | No       | `tabular`        | Can be set to `json` to get the response formatted as such.                                                                                                                                                                                                      |\n| `forceHeader` | No       | `false`          | `true` or `false`. When `false`, QuestDB will try to infer if the first line of the file is the header line. When set to `true`, QuestDB will expect that line to be the header line.                                                                            |\n| `name`        | No       | Name of the file | Name of the table to create, see below.                                                                                                                                                                                        |\n| `overwrite`   | No       | `false`          | `true` or `false`. When set to true, any existing data or structure will be overwritten.                                                                                                                                                                         |\n| `partitionBy` | No       | `NONE`           | See partitions.                                                                                                                                                                                                           |\n| `skipLev`     | No       | `false`          | `true` or `false`. Skip \u201cLine Extra Values\u201d, when set to true, the parser will ignore those extra values rather than ignoring entire line. An extra value is something in addition to what is defined by the header.                                             |\n| `timestamp`   | No       |                  | Name of the column that will be used as a designated timestamp.                                                                                                                                                            |\n|  |\n`shell title=\"Example usage\"\ncurl -F data=@weather.csv \\\n'http://localhost:9000/imp?overwrite=true&name=new_table&timestamp=ts&partitionBy=MONTH'`\nFurther example queries with context on the source CSV file contents relative\nand the generated tables are provided in the examples section\nbelow.\nNames\nTable and column names are subject to restrictions, the following list of\ncharacters are automatically removed:\n`plain\n[whitespace]\n.\n?\n,\n:\n\\\n/\n\\\\\n\\0\n)\n(\n_\n+\n-\n*\n~\n%`\nWhen the header row is missing, column names are generated automatically.\nConsistency guarantees\n`/imp` benefits from the properties of the QuestDB\nstorage model,\nalthough Atomicity and Durability can be relaxed to meet convenience and\nperformance demands.\nAtomicity\nQuestDB is fully insured against any connection problems. If the server detects\nclosed socket(s), the entire request is rolled back instantly and transparently\nfor any existing readers. The only time data can be partially imported is when\natomicity is in `relaxed` mode and data cannot be converted to column type. In\nthis scenario, any \"defective\" row of data is discarded and `/imp` continues to\nstream request data into table.\nConsistency\nThis property is guaranteed by consistency of append transactions against\nQuestDB storage engine.\nIsolation\nData is committed to QuestDB storage engine at end of request. Uncommitted\ntransactions are not visible to readers.\nDurability\n`/imp` streams data from network socket buffer directly into memory mapped\nfiles. At this point data is handed over to the OS and is resilient against\nQuestDB internal errors and unlikely but hypothetically possible crashes. This\nis default method of appending data and it is chosen for its performance\ncharacteristics.\nExamples\nAutomatic schema detection\nThe following example uploads a file `ratings.csv` which has the following\ncontents:\n| ts                          | visMiles       | tempF | dewpF |\n| --------------------------- | -------------- | ----- | ----- |\n| 2010-01-01T00:00:00.000000Z | 8.8            | 34    | 30    |\n| 2010-01-01T00:51:00.000000Z | 9.100000000000 | 34    | 30    |\n| 2010-01-01T01:36:00.000000Z | 8.0            | 34    | 30    |\n| ...                         | ...            | ...   | ...   |\nAn import can be performed with automatic schema detection with the following\nrequest:\n`shell\ncurl -F data=@weather.csv 'http://localhost:9000/imp'`\nA HTTP status code of `200` will be returned and the response will be:\n`shell\n+-------------------------------------------------------------------------------+\n|      Location:  |     weather.csv  |        Pattern  | Locale  |      Errors  |\n|   Partition by  |            NONE  |                 |         |              |\n|      Timestamp  |            NONE  |                 |         |              |\n+-------------------------------------------------------------------------------+\n|   Rows handled  |           49976  |                 |         |              |\n|  Rows imported  |           49976  |                 |         |              |\n+-------------------------------------------------------------------------------+\n|              0  |              ts  |                TIMESTAMP  |           0  |\n|              1  |        visMiles  |                   DOUBLE  |           0  |\n|              2  |           tempF  |                      INT  |           0  |\n|              3  |           dewpF  |                      INT  |           0  |\n+-------------------------------------------------------------------------------+`\nUser-defined schema\nTo specify the schema of a table, a schema object can be provided:\n`shell\ncurl \\\n-F schema='[{\"name\":\"dewpF\", \"type\": \"STRING\"}]' \\\n-F data=@weather.csv 'http://localhost:9000/imp'`\n`shell title=\"Response\"\n+------------------------------------------------------------------------------+\n|      Location:  |    weather.csv  |        Pattern  | Locale  |      Errors  |\n|   Partition by  |           NONE  |                 |         |              |\n|      Timestamp  |           NONE  |                 |         |              |\n+------------------------------------------------------------------------------+\n|   Rows handled  |          49976  |                 |         |              |\n|  Rows imported  |          49976  |                 |         |              |\n+------------------------------------------------------------------------------+\n|              0  |             ts  |                TIMESTAMP  |           0  |\n|              1  |       visMiles  |                   DOUBLE  |           0  |\n|              2  |          tempF  |                      INT  |           0  |\n|              3  |          dewpF  |                   STRING  |           0  |\n+------------------------------------------------------------------------------+`\nNon-standard timestamp formats\nGiven a file `weather.csv` with the following contents which contains a\ntimestamp with a non-standard format:\n| ts                    | visMiles       | tempF | dewpF |\n| --------------------- | -------------- | ----- | ----- |\n| 2010-01-01 - 00:00:00 | 8.8            | 34    | 30    |\n| 2010-01-01 - 00:51:00 | 9.100000000000 | 34    | 30    |\n| 2010-01-01 - 01:36:00 | 8.0            | 34    | 30    |\n| ...                   | ...            | ...   | ...   |\nThe file can be imported as usual with the following request:\n`shell title=\"Importing CSV with non-standard timestamp\"\ncurl -F data=@weather.csv 'http://localhost:9000/imp'`\nA HTTP status code of `200` will be returned and the import will be successful,\nbut the timestamp column is detected as a `STRING` type:\n`shell title=\"Response with timestamp as STRING type\"\n+-------------------------------------------------------------------------------+\n|      Location:  |     weather.csv  |        Pattern  | Locale  |      Errors  |\n|   Partition by  |            NONE  |                 |         |              |\n|      Timestamp  |            NONE  |                 |         |              |\n+-------------------------------------------------------------------------------+\n|   Rows handled  |           49976  |                 |         |              |\n|  Rows imported  |           49976  |                 |         |              |\n+-------------------------------------------------------------------------------+\n|              0  |              ts  |                   STRING  |           0  |\n|              1  |        visMiles  |                   DOUBLE  |           0  |\n|              2  |           tempF  |                      INT  |           0  |\n|              3  |           dewpF  |                      INT  |           0  |\n+-------------------------------------------------------------------------------+`\nTo amend the timestamp column type, this example curl can be used which has a\n`schema` JSON object to specify that the `ts` column is of `TIMESTAMP` type with\nthe pattern `yyyy-MM-dd - HH:mm:ss`\nAdditionally, URL parameters are provided:\n\n`overwrite=true` to overwrite the existing table\n`timestamp=ts` to specify that the `ts` column is the designated timestamp\n  column for this table\n`partitionBy=MONTH` to set a\n  partitioning strategy on the table by\n  `MONTH`\n\n`shell title=\"Providing a user-defined schema\"\ncurl \\\n-F schema='[{\"name\":\"ts\", \"type\": \"TIMESTAMP\", \"pattern\": \"yyyy-MM-dd - HH:mm:ss\"}]' \\\n-F data=@weather.csv \\\n'http://localhost:9000/imp?overwrite=true&timestamp=ts&partitionBy=MONTH'`\nThe HTTP status code will be set to `200` and the response will show `0` errors\nparsing the timestamp column:\n`shell\n+------------------------------------------------------------------------------+\n|      Location:  |    weather.csv  |        Pattern  | Locale  |      Errors  |\n|   Partition by  |          MONTH  |                 |         |              |\n|      Timestamp  |             ts  |                 |         |              |\n+------------------------------------------------------------------------------+\n|   Rows handled  |          49976  |                 |         |              |\n|  Rows imported  |          49976  |                 |         |              |\n+------------------------------------------------------------------------------+\n|              0  |             ts  |                TIMESTAMP  |           0  |\n|              1  |       visMiles  |                   DOUBLE  |           0  |\n|              2  |          tempF  |                      INT  |           0  |\n|              3  |          dewpF  |                      INT  |           0  |\n+------------------------------------------------------------------------------+`\nJSON response\nIf you intend to upload CSV programmatically, it's easier to parse the response\nas JSON. Set `fmt=json` query argument on the request.\nHere's an example of a successful response:\n`json\n{\n  \"status\": \"OK\",\n  \"location\": \"example_table\",\n  \"rowsRejected\": 0,\n  \"rowsImported\": 3,\n  \"header\": false,\n  \"columns\": [\n    { \"name\": \"col1\", \"type\": \"SYMBOL\", \"size\": 4, \"errors\": 0 },\n    { \"name\": \"col2\", \"type\": \"DOUBLE\", \"size\": 8, \"errors\": 0 },\n    { \"name\": \"col3\", \"type\": \"BOOLEAN\", \"size\": 1, \"errors\": 0 }\n  ]\n}`\nHere is an example with request-level errors:\n`json\n{\n  \"status\": \"not enough lines [table=example_table]\"\n}`\nHere is an example with column-level errors due to unsuccessful casts:\n`json\n{\n  \"status\": \"OK\",\n  \"location\": \"example_table2\",\n  \"rowsRejected\": 0,\n  \"rowsImported\": 3,\n  \"header\": false,\n  \"columns\": [\n    { \"name\": \"col1\", \"type\": \"DOUBLE\", \"size\": 8, \"errors\": 3 },\n    { \"name\": \"col2\", \"type\": \"SYMBOL\", \"size\": 4, \"errors\": 0 },\n    { \"name\": \"col3\", \"type\": \"BOOLEAN\", \"size\": 1, \"errors\": 0 }\n  ]\n}`\nOut-of-order import\nThe following example imports a file which contains out-of-order records. The\n`timestamp` and `partitionBy` parameters must be provided for commit lag and\nmax uncommitted rows to have any effect. For more information on these\nparameters, see the commit lag guide.\n`shell\ncurl -F data=@weather.csv \\\n'http://localhost:9000/imp?&timestamp=ts&partitionBy=DAY&commitLag=120000000&maxUncommittedRows=10000'`\n/exec - Execute queries\n`/exec` compiles and executes the SQL query supplied as a parameter and returns\na JSON response.\n:::note\nThe query execution terminates automatically when the socket connection is\nclosed.\n:::\nOverview\nParameters\n`/exec` is expecting an HTTP GET request with following query parameters:\n| Parameter       | Required | Default | Description                                                                                                                                                                            |\n| --------------- | -------- | ------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `count`         | No       | `false` | `true` or `false`. Counts the number of rows and returns this value.                                                                                                                   |\n| `limit`         | No       |         | Allows limiting the number of rows to return. `limit=10` will return the first 10 rows (equivalent to `limit=1,10`), `limit=10,20` will return row numbers 10 through to 20 inclusive. |\n| `nm`            | No       | `false` | `true` or `false`. Skips the metadata section of the response when set to `true`.                                                                                                      |\n| `query`         | Yes      |         | URL encoded query text. It can be multi-line.                                                                                                                                          |\n| `timings`       | No       | `false` | `true` or `false`. When set to `true`, QuestDB will also include a `timings` property in the response which gives details about the execution times.                                   |\n| `explain`       | No       | `false` | `true` or `false`. When set to `true`, QuestDB will also include an `explain` property in the response which gives details about the execution plan.                                   |\n| `quoteLargeNum` | No       | `false` | `true` or `false`. When set to `true`, QuestDB will surround `LONG` type numbers with double quotation marks that will make them parsed as strings.                                    |\nThe parameters must be URL encoded.\nHeaders\nSupported HTTP headers:\n| Header              | Required | Description                                                               |\n| ------------------- | -------- | ------------------------------------------------------------------------- |\n| `Statement-Timeout` | No       | Query timeout in milliseconds, overrides default timeout from server.conf |\nExamples\nSELECT query example:\n`shell\ncurl -G \\\n  --data-urlencode \"query=SELECT timestamp, tempF FROM weather LIMIT 2;\" \\\n  --data-urlencode \"count=true\" \\\n  http://localhost:9000/exec`\nA HTTP status code of `200` is returned with the following response body:\n`json\n{\n  \"query\": \"SELECT timestamp, tempF FROM weather LIMIT 2;\",\n  \"columns\": [\n    {\n      \"name\": \"timestamp\",\n      \"type\": \"TIMESTAMP\"\n    },\n    {\n      \"name\": \"tempF\",\n      \"type\": \"INT\"\n    }\n  ],\n  \"dataset\": [\n    [\"2010-01-01T00:00:00.000000Z\", 34],\n    [\"2010-01-01T00:51:00.000000Z\", 34]\n  ],\n  \"count\": 2\n}`\nSELECT query returns response in the following format:\n`json\n{\n  \"query\": string,\n  \"columns\": Array<{ \"name\": string, \"type\": string }>\n  \"dataset\": Array<Array<Value for Column1, Value for Column2>>,\n  \"count\": Optional<number>,\n  \"timings\": Optional<{ compiler: number, count: number, execute: number }>,\n  \"explain\": Optional<{ jitCompiled: boolean }>\n}`\nYou can find the exact list of types in the\ndedicated page.\nUPDATE query example:\nThis request executes an update of table `weather` setting 2 minutes query\ntimeout\n`shell\ncurl -G \\\n  -H \"Statement-Timeout: 120000\" \\\n  --data-urlencode \"query=UPDATE weather SET tempF = tempF + 0.12 WHERE tempF > 60\" \\\n  http://localhost:9000/exec`\nA HTTP status code of `200` is returned with the following response body:\n`json\n{\n  \"ddl\": \"OK\",\n  \"updated\": 34\n}`\n/exp - Export data\nThis endpoint allows you to pass url-encoded queries but the request body is\nreturned in a tabular form to be saved and reused as opposed to JSON.\nOverview\n`/exp` is expecting an HTTP GET request with following parameters:\n| Parameter | Required | Description                                                                                                                                                                                                                  |\n| :-------- | :------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `limit`   | No       | Paging opp parameter. For example, `limit=10,20` will return row numbers 10 through to 20 inclusive and `limit=20` will return first 20 rows, which is equivalent to `limit=0,20`. `limit=-20` will return the last 20 rows. |\n| `query`   | Yes      | URL encoded query text. It can be multi-line.                                                                                                                                                                                |\nThe parameters must be URL encoded.\nExamples\nConsidering the query:\n`shell\ncurl -G \\\n  --data-urlencode \"query=SELECT AccidentIndex2, Date, Time FROM 'Accidents0514.csv'\" \\\n  --data-urlencode \"limit=5\" \\\n  http://localhost:9000/exp`\nA HTTP status code of `200` is returned with the following response body:\n`shell\n\"AccidentIndex\",\"Date\",\"Time\"\n200501BS00001,\"2005-01-04T00:00:00.000Z\",17:42\n200501BS00002,\"2005-01-05T00:00:00.000Z\",17:36\n200501BS00003,\"2005-01-06T00:00:00.000Z\",00:15\n200501BS00004,\"2005-01-07T00:00:00.000Z\",10:35\n200501BS00005,\"2005-01-10T00:00:00.000Z\",21:13`\nError responses\nMalformed queries\nA successful call to `/exec` or `/exp` which also contains a malformed query\nwill return response bodies with the following format:\n`json\n{\n  \"query\": string,\n  \"error\": string,\n  \"position\": number\n}`\nThe `position` field is the character number from the beginning of the string\nwhere the error was found.\nConsidering the query:\n`shell\ncurl -G \\\n  --data-urlencode \"query=SELECT * FROM table;\" \\\n  http://localhost:9000/exp`\nA HTTP status code of `400` is returned with the following response body:\n```json\n{\n  \"query\": \"SELECT * FROM table;\",\n  \"error\": \"function, literal or constant is expected\",\n  \"position\": 8\n}",
    "tag": "questdb"
  },
  {
    "title": "Examples",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/api/ilp/overview.md",
    "content": "\ntitle: ILP Overview\nsidebar_label: Overview\ndescription: InfluxDB line protocol reference documentation.\n\nQuestDB implements the\nInfluxDB line protocol\nto ingest data. QuestDB can listen for line protocol packets over\nTCP.\nThis page aims to provide examples for QuestDB experts setting up TCP without\nany client libraries, or those looking to implement a new client library\nyourself.\n:::tip\nFor general QuestDB users, client libraries are available for a number of\nlanguages: ILP client libraries.\n:::\nExamples\nWe provide examples in a number of programming languages. See our\nILP section of the \"develop\"\ndocs.\nUsage\nSyntax\n`shell\ntable_name,symbolset columnset timestamp\\n`\n| Element      | Definition                                                                                                                                                                 |\n| :----------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `table_name` | Name of the table where QuestDB will write data.                                                                                                                           |\n| `symbolset`  | A set of comma-separated `name=value` pairs that will be parsed as symbol columns.                                                                                         |\n| `columnset`  | A set of comma-separated `name=value` pairs that will be parsed as non-symbol columns.                                                                                     |\n| `timestamp`  | UNIX timestamp. The default unit is nanosecond and is configurable via `line.tcp.timestamp`. The value will be truncated to microsecond resolution when parsed by QuestDB. |\n`name` in the `name=value` pair always corresponds to `column name` in the\ntable.\n:::note\nEach ILP message has to end with a new line `\\n` character.\n:::\nBehavior\n\nWhen the `table_name` does not correspond to an existing table, QuestDB will\n  create the table on the fly using the name provided. Column types will be\n  automatically recognized and assigned based on the data.\nThe `timestamp` column is automatically created as\n  designated timestamp with the\n  partition strategy set to `DAY`. Alternatively,\n  use CREATE TABLE to create the table with\n  a different partition strategy before ingestion.\nWhen the timestamp is empty, QuestDB will use the server timestamp.\n\nDifference from InfluxDB\nQuestDB TCP Receiver uses ILP as both serialization and the transport format.\nInfluxDB on other hand uses HTTP as the transport and ILP as serialization\nformat. For this reason the existing InfluxDB client libraries will not work\nwith QuestDB.\nGeneric example\nLet's assume the following data:\n| timestamp           | city    | temperature | humidity | make      |\n| :------------------ | :------ | :---------- | :------- | :-------- |\n| 1465839830100400000 | London  | 23.5        | 0.343    | Omron     |\n| 1465839830100600000 | Bristol | 23.2        | 0.443    | Honeywell |\n| 1465839830100700000 | London  | 23.6        | 0.358    | Omron     |\nThe line protocol syntax for that table is:\n`shell\nreadings,city=London,make=Omron temperature=23.5,humidity=0.343 1465839830100400000\\n\nreadings,city=Bristol,make=Honeywell temperature=23.2,humidity=0.443 1465839830100600000\\n\nreadings,city=London,make=Omron temperature=23.6,humidity=0.348 1465839830100700000\\n`\nThis would create table similar to this SQL statement and populate it.\n`questdb-sql\nCREATE TABLE readings (\n  timestamp TIMESTAMP,\n  city SYMBOL,\n  temperature DOUBLE,\n  humidity DOUBLE,\n  make SYMBOL\n) TIMESTAMP(timestamp) PARTITION BY DAY;`\nDesignated timestamp\nDesignated timestamp is the trailing value of an ILP message. It is optional,\nand when present, is a timestamp in Epoch nanoseconds. When the timestamp is\nomitted, the server will insert each message using the system clock as the row\ntimestamp.\n:::warning\n\n\nWhile\n  columnset timestamp type units\n  are microseconds, the designated timestamp units are nanoseconds by default,\n  and can be overridden via the `line.tcp.timestamp` configuration property.\n\n\nThe native timestamp format used by QuestDB is a Unix timestamp in microsecond\n  resolution; timestamps in nanoseconds will be parsed and truncated to\n  microseconds.\n\n\n:::\n`shell title=\"Example of ILP message with desginated timestamp value\"\ntracking,loc=north val=200i 1000000000\\n`\n`shell title=\"Example of ILP message sans timestamp\"\ntracking,loc=north val=200i\\n`\n:::note\nWe recommend populating designated timestamp via trailing value syntax above.\n:::\nIt is also possible to populate designated timestamp via `columnset`. Please see\nmixed timestamp reference.\nIrregularly-structured data\nInfluxDB line protocol makes it possible to send data under different shapes.\nEach new entry may contain certain tags or fields, and others not. QuestDB\nsupports on-the-fly data structure changes with minimal overhead. Whilst the\nexample just above highlights structured data, it is possible for InfluxDB line\nprotocol users to send data as follows:\n`shell\nreadings,city=London temperature=23.2 1465839830100400000\\n\nreadings,city=London temperature=23.6 1465839830100700000\\n\nreadings,make=Honeywell temperature=23.2,humidity=0.443 1465839830100800000\\n`\nThis would result in the following table:\n| timestamp           | city   | temperature | humidity | make      |\n| :------------------ | :----- | :---------- | :------- | :-------- |\n| 1465839830100400000 | London | 23.5        | NULL     | NULL      |\n| 1465839830100700000 | London | 23.6        | NULL     | NULL      |\n| 1465839830100800000 | NULL   | 23.2        | 0.358    | Honeywell |\n:::tip\nWhilst we offer this function for flexibility, we recommend that users try to\nminimise structural changes to maintain operational simplicity.\n:::\nDuplicate column names\nIf line contains duplicate column names, the value stored in the table will be\nthat from the first `name=value` pair on each line. For example:\n`shell\ntrade,ticker=USD price=30,price=60 1638202821000000000\\n`\nPrice `30` is stored, `60` is ignored.\nName restrictions\nBoth table name and column names are allowed to have spaces ``. These spaces\nhave to be escaped with `\\`. For example both of these are valid lines.\n`shell\ntrade\\ table,ticker=USD price=30,details=\"Latest price\" 1638202821000000000\\n`\n`shell\ntrade,symbol\\ ticker=USD price=30,details=\"Latest price\" 1638202821000000000\\n`\nTable and column names must not contain any of the forbidden characters:\n`\\n`,`\\r`,`?`,`,`,`:`,`\"`,`'`,`\\`,`/`,`\\0`,`)`,`(`,`+`,`*`,`~` and `%`.\nAdditionally, table name must not start or end with the `.` character. Column\nname must not contain `.` and `-`.\nSymbolset\nArea of the message that contains comma-separated set of `name=value` pairs for\nsymbol columns. For example in a message like this:\n`shell\ntrade,ticker=BTCUSD,venue=coinbase price=30,price=60 1638202821000000000\\n`\n`symbolset` is `ticker=BTCUSD,venue=coinbase`. Please note the mandatory space\nbetween `symbolset` and `columnset`. Naming rules for columns are subject to\nduplicate rules and\nname restrictions.\nSymbolset values\n`symbolset` values are always interpreted as SYMBOL.\nParser takes values literally so please beware of accidentally using high\ncardinality types such as `9092i` or `1.245667`. This will result in a\nsignificant performance loss due to large mapping tables.\n`symbolset` values are not quoted. They are allowed to have special characters,\nsuch as `` (space), `=`, `,`, `\\n`, `\\r` and `\\`, which must be escaped with a\n`\\`. Example:\n`shell\ntrade,ticker=BTC\\\\USD\\,All,venue=coin\\ base price=30 1638202821000000000\\n`\nWhenever `symbolset` column does not exist, it will be added on-the-fly with\ntype `SYMBOL`. On other hand when the column does exist, it is expected to be of\n`SYMBOL` type, otherwise the line is rejected.\nColumnset\nArea of the message that contains comma-separated set of `name=value` pairs for\nnon-symbol columns. For example in a message like this:\n`shell\ntrade,ticker=BTCUSD priceLow=30,priceHigh=60 1638202821000000000\\n`\n`columnset` is `priceLow=30,priceHigh=60`. Naming rules for columns are subject\nto duplicate rules and\nname restrictions.\nColumnset values\n`columnset` supports several values types, which are used to either derive type\nof new column or mapping strategy when column already exists. These types are\nlimited by existing Influx Line Protocol specification. Wider QuestDB type\nsystem is available by creating table via SQL upfront. The following are\nsupported value types:\nInteger,\nLong256,\nFloat,\nString and",
    "tag": "questdb"
  },
  {
    "title": "Overview",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/api/ilp/udp-receiver.md",
    "content": "\ntitle: ILP UDP Receiver\nsidebar_label: UDP Receiver\ndescription: InfluxDB line protocol UDP receiver reference documentation.\n\n:::note\nThe UDP receiver is deprecated since QuestDB version 6.5.2.\nWe recommend the TCP receiver instead.\n:::\nThe UDP receiver can handle both single and multi row write requests. It is\ncurrently single-threaded, and performs both network I/O and write jobs out of\none thread. The UDP worker thread can work either on its own thread or use the\ncommon thread pool. It supports both multicast and unicast.\nOverview\nBy default, QuestDB listens for `multicast` line protocol packets over UDP on\n`232.1.2.3:9009`. If you are running QuestDB with Docker, you will need to\npublish the port `9009` using `-p 9009:9009` and publish multicast packets with\nTTL of at least 2. This port can be customized, and you can also configure\nQuestDB to listen for `unicast`.\nCommit strategy\nUncommitted rows are committed either:\n\nafter receiving a number of continuous messages equal to\n  `line.udp.commit.rate` or\nwhen UDP receiver has idle time, i.e. ingestion slows down or completely\n  stops.\n\nConfiguration\nThe UDP receiver configuration can be completely customized using\nconfiguration keys. You\ncan use this to configure the IP address and port the receiver binds to, commit\nrates, buffer size, whether it should run on a separate thread etc.\nExamples\nFind an example of how to use this in the",
    "tag": "questdb"
  },
  {
    "title": "Integer",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/api/ilp/columnset-types.md",
    "content": "\ntitle: ILP Columnset Value Types\nsidebar_label: Columnset Value Types\ndescription: Describes all support value types in ILP columnset.\n\nInteger\n64-bit signed integer values, which correspond to QuestDB type `long`. The\nvalues are required to have `i` suffix. For example:\n`shell\ntemps,device=cpu,location=south value=96i 1638202821000000000\\n`\nSometimes integer values are small and do not warrant 64 bits to store them. To\nreduce storage for such values it is possible to create a table upfront with\nsmaller type, for example:\n`questdb-sql\nCREATE TABLE temps (device SYMBOL, location SYMBOL, value SHORT);`\nThe line above will be accepted and `96i` will be cast to `short`.\n:::info\nType casts that cause data loss will cause entire line to be rejected.\n:::\nCast table\nThe following `cast` operations are supported when existing table column type is\nnot `long`:\n|           | `byte` | `short` | `int` | `long`   | `float` | `double` | `date` | `timestamp` |\n| :-------- | :----- | :------ | :---- | :------- | :------ | :------- | :----- | :---------- |\n| `integer` | cast   | cast    | cast  | `native` | cast    | cast     | cast   | cast        |\nLong256\nCustom type, which correspond to QuestDB type `long256`. The values are hex\nencoded 256-bit unsigned integer values with `i` suffix. For example:\n`shell\ntemps,device=cpu,location=south value=0x123a4i 1638202821000000000\\n`\nWhen column does not exist, it will be created with type `long256`. Values\noverflowing 256-bit integer will cause the entire line to be rejected.\n`long256` cannot be cast to anything else.\nFloat\nThese values correspond to QuestDB type `double`. They actually do not have any\nsuffix, which might lead to a confusion. For example:\n`shell\ntrade,ticker=BTCUSD price=30 1638202821000000000\\n`\n`price` value will be stored as `double` even though it does not look like a\nconventional double value would.\nCast table\nThe following `cast` operations are supported when existing table column type is\nnot `double`:\n|         | `float` | `double` |\n| :------ | :------ | :------- |\n| `float` | cast    | `native` |\nBoolean\nThese value correspond to QuestDB type `boolean`. In InfluxDB Line Protocol\n`boolean` values can be represented in any of the following ways:\n| Actual value | Single char lowercase | Single char uppercase | Full lowercase | Full camelcase | Full uppercase |\n| :----------- | :-------------------- | :-------------------- | :------------- | :------------- | :------------- |\n| `true`       | `t`                   | `T`                   | `true`         | `True`         | `TRUE`         |\n| `false`      | `f`                   | `F`                   | `false`        | `False`        | `FALSE`        |\nExample:\n`shell\nsensors,location=south warning=false\\n`\nCast table\nThe following `cast` operations are supported when existing table column type is\nnot `boolean`:\n|           | `boolean` | `byte` | `short` | `int` | `float` | `long` | `double` |\n| :-------- | :-------- | :----- | :------ | :---- | :------ | :----- | :------- |\n| `boolean` | `native`  | cast   | cast    | cast  | cast    | cast   | cast     |\nWhen cast to numeric type, boolean `true` is `1` and `false` is `0`\nString\nThese value correspond to QuestDB type `string`. They must be enclosed in\nquotes. The following characters in values must be escaped with a `\\`: `\"`,\n`\\n`, `\\r` and `\\`. For example:\n`shell\ntrade,ticker=BTCUSD description=\"this is a \\\"rare\\\" value\",user=\"John\" 1638202821000000000\\n`\nThe result:\n| timestamp           | ticker | description            | user |\n| :------------------ | :----- | :--------------------- | :--- |\n| 1638202821000000000 | BTCUSD | this is a \"rare\" value | John |\n:::note\nString values must be UTF-8 encoded before sending.\n:::\nCast table\nThe following `cast` operations are supported when existing table column type is\nnot `string`:\n|          | `char` | `string` | `geohash` | `symbol` |\n| :------- | :----- | :------- | :-------- | :------- |\n| `string` | cast   | `native` | cast      | no       |\nCast to CHAR\nString value can be cast to `char` type if its length is less than 2 characters.\nThe following example are valid lines:\n`shell\ntrade,ticker=BTCUSD status=\"A\" 1638202821000000000\\n\ntrade,ticker=BTCUSD status=\"\" 1638202821000000001\\n`\nThe result:\n| timestamp           | ticker | status |\n| :------------------ | :----- | :----- |\n| 1638202821000000000 | BTCUSD | A      |\n| 1638202821000000001 | BTCUSD | `null` |\nCasting strings with 2 or more characters to `char` will cause entire line to be\nrejected.\nCast to GEOHASH\nString value can be cast to `geohash` type when the destination column exists\nand is of a `GEOHASH` type already. Do make sure that column is created upfront.\nOtherwise, ILP will create `STRING` column regardless of the value.\nExample:\nUpcasting is an attempt to store higher resolution `geohash` in a lower\nresolution column. Let's create table before sending ILP message. Our `geohash`\ncolumn has resolution of 4 bits.\n`questdb-sql\nCREATE TABLE tracking (\n    geohash GEOHASH(4b),\n    ts TIMESTAMP\n) TIMESTAMP(ts) PARTITION BY HOUR;`\nSend message including `16c` `geohash` value:\n`shell\ntracking,obj=VLCC\\ STEPHANIE gh=\"9v1s8hm7wpkssv1h\" 1000000000\\n`\nThe result. `geohash` value has been truncated to size of the column.\n| ts                          | gh   |\n| :-------------------------- | :--- |\n| 1970-01-01T00:00:01.000000Z | 0100 |\nSending empty string value will insert `null` into `geohash` column of any size:\n`shell\ntracking,obj=VLCC\\ STEPHANIE gh=\"\" 2000000000\\n`\n| ts                          | gh     |\n| :-------------------------- | :----- |\n| 1970-01-01T00:00:01.000000Z | `null` |\n:::info Downcast of `geohash` value, which is inserting of lower resolution\nvalues into higher resolution column, will cause the entire line to be rejected.\n:::\nTimestamp\nThese value correspond to QuestDB type `timestamp`. Timestamp values are epoch\n`microseconds` suffixed with `t`. In this example we're populating\nnon-designated timestamp field `ts1`:\n`shell\ntracking,obj=VLCC\\ STEPHANIE gh=\"9v1s8hm7wpkssv1h\",ts1=10000t 1000000000\\n`\nIt is possible to populate designated timestamp using `columnset`, although\nthis is not recommended. Let's see how this works in practice. Assuming table:\n`questdb-sql\nCREATE TABLE (loc SYMBOL, ts TIMESTAMP) TIMESTAMP(ts) PARTITION BY DAY;`\nWhen we send:\n`shell title=\"Sending mixed desginated timestamp values\"\ntracking,loc=north ts=2000000000t 1000000000\\n\ntracking,loc=south ts=3000000000t\\n`\nThe result in `columnset` value always wins:\n| loc   | ts         |\n| :---- | :--------- |\n| north | 2000000000 |",
    "tag": "questdb"
  },
  {
    "title": "[key/user id] [key type] {keyX keyY}",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/api/ilp/authenticate.md",
    "content": "\ntitle: Authenticate\ndescription:\n  This page shows how to setup authentication for InfluxDB line protocol over\n  TCP using JSON web keys.\n\nAlthough the original protocol does not support it, we have added authentication\nover TCP for InfluxDB line protocol. This works by using an\nelliptic curve P-256\nJSON Web Token (JWT) to sign a server challenge. This page shows how to\nauthenticate clients with QuestDB when using\nInfluxDB line protocol for the TCP\nendpoint.\nPrerequisites\nQuestDB should be running and accessible and can be started via\nDocker, the binaries\nor Homebrew for macOS users.\nThe jose package is a C-language\nimplementation of the Javascript Object Signing and Encryption standard and may\nbe used for convenience to generate cryptographic keys. It's also recommended to\ninstall jq for parsing the JSON\noutput from the keys generated by `jose`\n\n\n`bash\nbrew install jose\nbrew install jq`\n\n\n`bash\nyum install jose\nyum install jq`\n\n\n`bash\napt install jose\napt install jq`\n\n\nServer configuration\nIn order to use this feature, you need to create an authentication file using\nthe following template:\n```bash\ntestUser1 ec-p-256-sha256 fLKYEaoEb9lrn3nkwLDA-M_xnuFOdSt9y0Z7_vWSHLU Dt5tbS1dEDMSYfym3fgMv0B99szno-dFc1rYF9t0aac\n[key/user id] [key type] {keyX keyY}\n```\nOnly elliptic curve (P-256) are supported (key type `ec-p-256-sha256`). An\nauthentication file can be generated using the `jose` utility with the following\ncommand.\n```bash\njose jwk gen -i '{\"alg\":\"ES256\", \"kid\": \"testUser1\"}' -o /var/lib/questdb/conf/full_auth.json\nKID=$(cat /var/lib/questdb/conf/full_auth.json | jq -r '.kid')\nX=$(cat /var/lib/questdb/conf/full_auth.json | jq -r '.x')\nY=$(cat /var/lib/questdb/conf/full_auth.json | jq -r '.y')\necho \"$KID ec-p-256-sha256 $X $Y\" | tee /var/lib/questdb/conf/auth.txt\n```\nOnce you created the file, you will need to reference it in the server\nconfiguration:\n`ini title='/path/to/server.conf'\nline.tcp.auth.db.path=conf/auth.txt`\nClient keys\nFor the server configuration above, the corresponding JSON Web Key must be\nstored on the client side. When sending a fully-composed JWK, it will have the\nfollowing keys:\n`json\n{\n  \"kty\": \"EC\",\n  \"d\": \"5UjEMuA0Pj5pjK8a-fa24dyIf-Es5mYny3oE_Wmus48\",\n  \"crv\": \"P-256\",\n  \"kid\": \"testUser1\",\n  \"x\": \"fLKYEaoEb9lrn3nkwLDA-M_xnuFOdSt9y0Z7_vWSHLU\",\n  \"y\": \"Dt5tbS1dEDMSYfym3fgMv0B99szno-dFc1rYF9t0aac\"\n}`\nFor this kind of key, the `d` property is used to generate the the secret key.\nThe `x` and `y` parameters are used to generate the public key (values that we\nretrieve in the server authentication file).\nAuthentication\nThe server will now expect the client to send its key id (terminated with `\\n`)\nstraight after `connect()`. The server will respond with a challenge (printable\ncharacters terminated with `\\n`). The client needs to sign the challenge and\nrespond to the server with the `base64` encoded signature (terminated with\n`\\n`). If all is good the client can then continue, if not the server will\ndisconnect and log the failure.\nimport Tabs from \"@theme/Tabs\"\nimport TabItem from \"@theme/TabItem\"\n\n\n```javascript\nconst { Socket } = require(\"net\")\nconst { Crypto } = require(\"node-webcrypto-ossl\")\nconst crypto = new Crypto()\nconst PORT = 9009\nconst HOST = \"localhost\"\nconst PRIVATE_KEY = \"5UjEMuA0Pj5pjK8a-fa24dyIf-Es5mYny3oE_Wmus48\"\nconst PUBLIC_KEY = {\n  x: \"fLKYEaoEb9lrn3nkwLDA-M_xnuFOdSt9y0Z7_vWSHLU\",\n  y: \"Dt5tbS1dEDMSYfym3fgMv0B99szno-dFc1rYF9t0aac\",\n}\nconst JWK = {\n  ...PUBLIC_KEY,\n  kid: \"testUser1\",\n  kty: \"EC\",\n  d: PRIVATE_KEY,\n  crv: \"P-256\",\n}\nconst client = new Socket()\nasync function write(data) {\n  return new Promise((resolve) => {\n    client.write(data, () => {\n      resolve()\n    })\n  })\n}\nasync function authenticate(challenge) {\n  // Check for trailing \\n which ends the challenge\n  if (challenge.slice(-1).readInt8() === 10) {\n    const apiKey = await crypto.subtle.importKey(\n      \"jwk\",\n      JWK,\n      { name: \"ECDSA\", namedCurve: \"P-256\" },\n      true,\n      [\"sign\"],\n    )\n\n\n```const signature = await crypto.subtle.sign(\n  { name: \"ECDSA\", hash: \"SHA-256\" },\n  apiKey,\n  challenge.slice(0, challenge.length - 1),\n)\n\nawait write(`${Buffer.from(signature).toString(\"base64\")}\\n`)\n\nreturn true\n```\n\n\n}\nreturn false\n}\nasync function sendData() {\n  const rows = [\n    `test,location=us temperature=22.4 ${Date.now() * 1e6}`,\n    `test,location=us temperature=21.4 ${Date.now() * 1e6}`,\n  ]\nfor (row of rows) {\n    await write(`${row}\\n`)\n  }\n}\nasync function run() {\n  let authenticated = false\n  let data\nclient.on(\"data\", async function (raw) {\n    data = !data ? raw : Buffer.concat([data, raw])\n\n\n```if (!authenticated) {\n  authenticated = await authenticate(data)\n  await sendData()\n  setTimeout(() => {\n    client.destroy()\n  }, 0)\n}\n```\n\n\n})\nclient.on(\"ready\", async function () {\n    await write(`${JWK.kid}\\n`)\n  })\nclient.connect(PORT, HOST)\n}\nrun()\n```\n\n\n```go\npackage main\nimport (\n    \"context\"\n    \"fmt\"\n    \"log\"\n    \"time\"\n\n\n```qdb \"github.com/questdb/go-questdb-client\"\n```\n\n\n)\nfunc main() {\n    ctx := context.TODO()\n    // Connect to QuestDB running on 127.0.0.1:9009\n    sender, err := qdb.NewLineSender(\n        ctx,\n        // Specify keyId and key for authentication.\n        qdb.WithAuth(\"testUser1\", \"5UjEMuA0Pj5pjK8a-fa24dyIf-Es5mYny3oE_Wmus48\"),\n    )\n    if err != nil {\n        log.Fatal(err)\n    }\n    // Make sure to close the sender on exit to release resources.\n    defer sender.Close()\n    // Send a few ILP messages.\n    err = sender.\n        Table(\"trades\").\n        Symbol(\"name\", \"test_ilp1\").\n        Float64Column(\"value\", 12.4).\n        AtNow(ctx)\n    if err != nil {\n        log.Fatal(err)\n    }\n    err = sender.\n        Table(\"trades\").\n        Symbol(\"name\", \"test_ilp2\").\n        Float64Column(\"value\", 11.4).\n        AtNow(ctx)\n    if err != nil {\n        log.Fatal(err)\n    }\n    // Make sure that the messages are sent over the network.\n    err = sender.Flush(ctx)\n    if err != nil {\n        log.Fatal(err)\n    }\n}\n```\n\n\n```python\nhttps://github.com/questdb/py-questdb-client\nfrom questdb.ingress import Sender, IngressError, TimestampNanos, TimestampMicros\nimport datetime\nimport sys\nHOST = 'localhost'\nPORT = 9009\ndef send_with_auth():\n    try:\n        auth = (\"YOUR_KID\", \"YOUR_D_KEY\", \"YOUR_X_KEY\", \"YOUR_Y_KEY\")\n        with Sender(HOST, PORT, auth=auth, tls=True) as sender:\n            buffer = sender.new_buffer()\n            buffer.row(\n                'trades',\n                symbols={'name': 'tls_client_timestamp'},\n                columns={'value': 12.4, 'valid_from': TimestampMicros.from_datetime(datetime.datetime.utcnow())},\n                at=TimestampNanos.from_datetime(datetime.datetime.utcnow()))\n            sender.flush(buffer)\n    except IngressError as e:\n        sys.stderr.write(f'Got error: {e}')\nif name == 'main':\n    send_with_auth()\n```  \n\n",
    "tag": "questdb"
  },
  {
    "title": "Overview",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/api/ilp/tcp-receiver.md",
    "content": "\ntitle: ILP TCP Receiver\nsidebar_label: TCP Receiver\ndescription: InfluxDB line protocol TCP receiver reference documentation.\n\nThe TCP receiver is a high-throughput ingestion-only API for QuestDB. Here are\nsome key facts about the service:\n\ningestion only, there is no query capability\naccepts plain text input in a form on InfluxDB Line Protocol\nimplicit transactions, batching\nsupports automatic table and column creation\nmulti-threaded, non-blocking\nsupports authentication\nencryption requires an optional external reverse-proxy\n\nOverview\nBy default, QuestDB listens over TCP on `0.0.0.0:9009`. The receiver consists of\ntwo thread pools, which is an important design feature to be aware of to\nconfigure the receiver for maximum performance. The `io worker` threads are\nresponsible for parsing text input. The `writer` threads are responsible for\npersisting data in tables. We will talk more about these in\ncapacity planning section.\nAuthentication\nAlthough the original protocol does not support it, we have added authentication\nover TCP. This works by using an\nelliptic curve P-256\nJSON Web Token (JWT) to sign a server challenge. Details of authentication over\nILP can be found in the\nauthentication documentation.\nInsert data\nFollow this link for\nexamples of sending data using ILP over TCP.\nError handling\nIt is recommended that sending applications reuse TCP connections. If QuestDB\nreceives an invalid message, it will discard invalid lines, produce an error\nmessage in the logs and forcibly disconnect the sender to prevent further data\nloss.\nData may be discarded because of:\n\nmissing new line characters at the end of messages\nan invalid data format such as unescaped special characters\ninvalid column / table name characters\nschema mismatch with existing tables\nmessage size overflows on the input buffer\nsystem errors such as no space left on the disk\n\nDetecting malformed input can be achieved through QuestDB logs by searching for\n`LineTcpMeasurementScheduler` and `LineTcpConnectionContext`, for example:\n`bash\n2022-02-03T11:01:51.007235Z I i.q.c.l.t.LineTcpMeasurementScheduler could not create table [tableName=trades, ex=`column name contains invalid characters [colName=trade_%]`, errno=0]`\nThe following input is tolerated by QuestDB:\n\na column is specified twice or more on the same line, QuestDB will pick the\n  first occurrence and ignore the rest\nmissing columns, their value will be defaulted to `null`/`0.0`/`false`\n  depending on the type of the column\nmissing designated timestamp, the current server time will be used to generate\n  the timestamp\nthe timestamp is specified as a column instead of appending it to the end of\n  the line\ntimestamp appears as a column and is also present at the end of the line, the\n  value sent as a field will be used\n\nCommit strategy\n:::note\nDeprecated content\nThis section applies to QuestDB 6.5.5 and earlier versions. From\nQuestDB 6.6 onwards, the\ndatabase adjusts relevant settings automatically and provides maximum ingestion\nspeed.\n:::\nILP transactions are implicit; the protocol is built to stream data at a high\nrate of speed and to support batching. There are three ways data is committed\nand becomes visible or partially visible. The commit method is chosen based on\nwhichever occurs first.\nRow-based commit\nEach table has a max uncommitted rows metadata property. The ILP server will\nissue a commit when the number of uncommitted rows reaches this value. The table\ncommit implementation retains data under max uncommitted rows or newer than the\ncommit lag (whichever is smallest) as uncommitted data. Committed data is\nvisible to table readers.\nThis parameter is set using in the following server configuration property:\n`shell title=\"Commit when this number of uncommitted records is reached\"\ncairo.max.uncommitted.rows=1000`\nIdle table timeout\nWhen there is no data ingested in the table after a set period, the ingested\nuncommitted data is fully committed, and table data becomes fully visible. The\ntimeout value is server-global and can be set via the following server\nconfiguration property:\n`shell title=\"Minimum amount of idle time (millis) before table writer is released\"\nline.tcp.min.idle.ms.before.writer.release=500`\nThe following server configuration property controls the interval to run idle\ntable checks:\n`shell title=\"Setting maintenance interval (millis)\"\nline.tcp.maintenance.job.interval=1000`\nInterval-based commit\nA table's commit lag metadata property\ndetermines how much uncommitted data will need to remain uncommitted for\nperformance reasons. This lag value is measured in time units from the table's\ndata. Data older than the lag value will be committed and become visible. ILP\nderives the commit interval as a function of the commit lag value for each\ntable. The difference is that the commit interval is a wall clock.\nThe commit interval is calculated for each table as a fraction of the commit lag\n`commitInterval = commitLag * fraction`\nThis fraction is `0.5` by default so if the table has a commit lag of `1` minute\nthe commit interval will be `30` seconds. The fraction used to derive the commit\ninterval can be set by the below configuration parameter.\n`shell title=\"Setting commit interval fraction\"\nline.tcp.commit.interval.fraction=0.2`\nIf the result of commit interval is `0`, the default commit interval of `2`\nseconds will be used. This can be changed in the configuration:\n`shell title=\"Setting the default commit interval in milliseconds\"\nline.tcp.commit.interval.default=5000`\nTo ease understanding of how time interval interacts with commit lag, let's look\nat how real-time data stream will become visible. The wall clock is roughly\naligned with time in the data stream in real-time data. Let's assume that table\nhas a commit lag value of 60 seconds and a commit interval of 20 seconds. After\nthe first 60 seconds of ingestion, ILP will attempt to commit 3 times. After\neach attempt, there will be no data visible to the application. This is because\nall the data will fall within the lag interval.\nOn the 4th commit, which would occur, 80 seconds after the data stream begins,\nthe application will see the first 20 seconds of the data, with the remaining 60\nseconds uncommitted. Each subsequent commit will reveal more data in 20-second\nincrements. It should be noted that both commit lag and commit interval should\nbe carefully chosen with both data visibility and ingestion performance in mind.\nCapacity planning\nTCP receiver makes use of 3 logical thread pools:\n\nI/O worker pool - `line.tcp.io.worker.count`, threads responsible for handling\n  incoming TCP connections and parsing received ILP messages\nwriter pool - `line.tcp.writer.worker.count`, threads responsible for table\n  writes\nshared pool - `shared.worker.count`, threads responsible for handling O3 data\n\nDepending on the number of concurrent TCP connections `io worker pool` size\nmight need to be adjusted. The ideal ratio is `1:1` - a thread per connection.\nIn less busy environments it is possible for single `io worker` thread to handle\nmultiple connections simultaneously. We recommend starting with conservative\nration, measure and increase ration up to `1:1`. More threads than connections\nwill be wasting server CPU.\nAnother consideration is the number of tables updates concurrently.\n`writer pool` should be tuned to increase concurrency. `writer` threads can also\nhandle multiple tables concurrently. `1:1` ratio is the maximum required ratio\nbetween `writer` threads and tables. If `1:1` ratio is not an option, avoid\nwriting to all tables from each connection. Instead, group connections and\ntables. For example, if there are 10 tables, 8 TCP connections and `writer pool`\nsize is set to 2, 4 TCP connections may be used to write into tables 1-5, while\n4 connections may write into tables 6-10.\n:::note\nSending updates for multiple tables from a single TCP connection might be\ninefficient. Consider using multiple connections to improve performance. If a\nsingle connection is unavoidable, keep `writer pool` size set to 1 for optimal\nCPU resource utilization.\n:::\nWhen ingesting data out of order (O3) `shared pool` accelerates O3 tasks. It is\nalso responsible for SQL execution. `shared pool` size should be set to use the\nremaining available CPU cores.\nConfiguration reference\nThe TCP receiver configuration can be completely customized using\nconfiguration keys. You\ncan use this to configure the thread pools, buffer and queue sizes, receiver IP",
    "tag": "questdb"
  },
  {
    "title": "Syntax",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/sql/alter-table-drop-column.md",
    "content": "\ntitle: ALTER TABLE DROP COLUMN\nsidebar_label: DROP COLUMN\ndescription: DROP COLUMN SQL keyword reference documentation.\n\nDrops a column from an existing table.\nBefore the operation can proceed, any current transactions are committed,\nregardless of the success or failure of the `DROP` operation. Dropping a column\ndoes not lock the table for reading and does not wait on any reads to finish.\nDropping columns will also attempt to remove files belonging to the column from\nall partitions, thus freeing up disk space immediately. If this is not\nimmediately possible on Windows, the file remove operation is postponed until\nfiles are released by all threads. The logical drop column will succeed on\nWindows in presence of active readers.\n:::caution\nUse `DROP COLUMN` with care, as QuestDB cannot recover data from dropped\ncolumns!\n:::\nSyntax\n\n\nExample\nThe following example deletes the column called `comment` from the table\n`ratings`\n```questdb-sql title=\"Dropping a column\"\nALTER TABLE ratings DROP COLUMN movieId;",
    "tag": "questdb"
  },
  {
    "title": "Syntax",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/sql/insert.md",
    "content": "\ntitle: INSERT keyword\nsidebar_label: INSERT\ndescription: INSERT SQL keyword reference documentation.\n\n`INSERT` ingests selected data into a database table.\nSyntax\nInserting values directly or using sub-queries:\n\nInserting using sub-query alias:\n\nDescription\n:::note\nIf the target partition is\nattached by a symbolic link,\nthe partition is read-only. `INSERT` operation on a read-only partition triggers\na critical-level log in the server, and the insert is a no-op.\n:::\nInserting values directly or using sub-queries:\n\n`VALUE`: Directly defines the values to be inserted.\n`SELECT`: Inserts values based on the result of a\n  SELECT query\n\nSetting sub-query alias:\n\n`WITH AS`: Inserts values based on a sub-query, to which an alias is given by\n  using WITH.\n\nParameter:\n\n`batch` expects a `batchCount` (integer) value defining how many records to\n  process at any one time.\n\nExamples\n`questdb-sql title=\"Inserting all columns\"\nINSERT INTO trades\nVALUES(\n    '2021-10-05T11:31:35.878Z',\n    'AAPL',\n    255,\n    123.33,\n    'B');`\n`questdb-sql title=\"Bulk inserts\"\nINSERT INTO trades\nVALUES\n    ('2021-10-05T11:31:35.878Z', 'AAPL', 245, 123.4, 'C'),\n    ('2021-10-05T12:31:35.878Z', 'AAPL', 245, 123.3, 'C'),\n    ('2021-10-05T13:31:35.878Z', 'AAPL', 250, 123.1, 'C'),\n    ('2021-10-05T14:31:35.878Z', 'AAPL', 250, 123.0, 'C');`\n`questdb-sql title=\"Specifying schema\"\nINSERT INTO trades (timestamp, symbol, quantity, price, side)\nVALUES(\n    to_timestamp('2019-10-17T00:00:00', 'yyyy-MM-ddTHH:mm:ss'),\n    'AAPL',\n    255,\n    123.33,\n    'B');`\n:::note\nColumns can be omitted during `INSERT` in which case the value will be `NULL`\n:::\n`questdb-sql title=\"Inserting only specific columns\"\nINSERT INTO trades (timestamp, symbol, price)\nVALUES(to_timestamp('2019-10-17T00:00:00', 'yyyy-MM-ddTHH:mm:ss'),'AAPL','B');`\nInserting query results\nThis method allows you to insert as many rows as your query returns at once.\n`questdb-sql title=\"Insert as select\"\nINSERT INTO confirmed_trades\n    SELECT timestamp, instrument, quantity, price, side\n    FROM unconfirmed_trades\n    WHERE trade_id = '47219345234';`\nUsing the WITH keyword to set up an alias for a\n`SELECT` sub-query:\n`questdb-sql title=\"Insert with sub-query\"\nWITH confirmed_id AS (\n    SELECT * FROM unconfirmed_trades\n    WHERE trade_id = '47219345234'\n    )\nINSERT INTO confirmed_trades\nSELECT * FROM confirmed_id;`\nParameters for QuestDB 6.5.5 and earlier versions\n:::note\nDeprecated content\nThis section applies to QuestDB 6.5.5 and earlier versions. From\nQuestDB 6.6 onwards, the\ndatabase adjusts relevant settings automatically and provides maximum ingestion\nspeed.\n:::\nInserting values directly or using sub-queries:\n\nInserting using sub-query alias:\n\nThe `commitLag` parameter may be provided to optimize `INSERT AS SELECT` or\n`WITH AS` queries when inserting\nout-of-order records into an ordered\ndataset:\n\n`commitLag` expects a `lagAmount` with a modifier to specify the time unit for\n  the value (i.e. `20s` for 20 seconds). The following table describes the units\n  that may be used:\n\n| unit | description  |\n  | ---- | ------------ |\n  | us   | microseconds |\n  | s    | seconds      |\n  | m    | minutes      |\n  | h    | hours        |\n  | d    | days         |\n```questdb-sql title=\"Insert as select with lag and batch size\"\nINSERT batch 100000 commitLag 180s INTO trades\nSELECT ts, instrument, quantity, price\nFROM unordered_trades",
    "tag": "questdb"
  },
  {
    "title": "Syntax",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/sql/vacuum-table.md",
    "content": "\ntitle: VACUUM TABLE\nsidebar_label: VACUUM TABLE\ndescription: VACUUM TABLE SQL keyword reference documentation\n\nThe `VACUUM TABLE` command triggers partition and column version cleanup.\nWhen a table is appended in an out-of-order manner, the `VACUUM TABLE` command\nwrites a new partition version to the disk. The old partition version directory\nis deleted once it is not read by `SELECT` queries. In the event of file system\nerrors, physical deletion of old files may be interrupted and an outdated\npartition version may be left behind consuming the disk space.\nWhen an `UPDATE` SQL statement is run, it copies column files of the selected\ntable. The old column files are automatically deleted but in certain\ncircumstances, they can be left behind. In this case, `VACUUM TABLE` can be used\nto re-trigger the deletion process of the old column files.\nThe `VACUUM TABLE` command starts a new scan over table partition directories\nand column files. It detects redundant, unused files consuming the disk\nspace and deletes them. `VACUUM TABLE` executes asynchronously, i.e. it may keep\nscanning and deleting files after their response is returned to the SQL client.\nThis command provides a manual mechanism to reclaim the disk space. The\nimplementation scans file system to detect duplicate directories and files and\nfrequent usage of the command can be relatively expensive. Thus, `VACUUM TABLE`\nhas to be executed sparingly.\nSyntax\n\n\nExample\n```questdb-sql\nVACUUM TABLE trades",
    "tag": "questdb"
  },
  {
    "title": "Syntax",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/sql/latest-on.md",
    "content": "\ntitle: LATEST ON keyword\nsidebar_label: LATEST ON\ndescription:\n  Reference documentation for using LATEST ON keywords with examples for\n  illustration.\n\nRetrieves the latest entry by timestamp for a given key or combination of keys,\nfor scenarios where multiple time series are stored in the same table.\nSyntax\n\nwhere:\n\n`columnName` used in the `LATEST ON` part of the clause is a `TIMESTAMP`\n  column.\n`columnName` list used in the `PARTITION BY` part of the clause is a list of\n  columns of one of the following types: `SYMBOL`, `STRING`, `BOOLEAN`, `SHORT`,\n  `INT`, `LONG`, `LONG256`, `CHAR`.\n\nDescription\n`LATEST ON` is used as part of a SELECT statement\nfor returning the most recent records per unique time series identified by the\n`PARTITION BY` column values. This function requires a\ndesignated timestamp.\n:::note\nTo use `LATEST ON`, a timestamp column used in the `LATEST ON` part needs to be\nspecified as a designated timestamp. More information can be found in the\ndesignated timestamp page for specifying\nthis at table creation or at query time.\n:::\nTo illustrate how `LATEST ON` is intended to be used, consider the `trips` table\nin the QuestDB demo instance. This table has a\n`payment_type` column as `SYMBOL` type which specifies the method of payment per\ntrip. We can find the most recent trip for each unique method of payment with\nthe following query:\n`questdb-sql\nSELECT payment_type, pickup_datetime, trip_distance\nFROM trips\nLATEST ON pickup_datetime PARTITION BY payment_type;`\n| payment_type | pickup_datetime             | trip_distance |\n| ------------ | --------------------------- | ------------- |\n| Dispute      | 2014-12-31T23:55:27.000000Z | 1.2           |\n| Voided       | 2019-06-27T17:56:45.000000Z | 1.9           |\n| Unknown      | 2019-06-30T23:57:42.000000Z | 3.9           |\n| No Charge    | 2019-06-30T23:59:30.000000Z | 5.2           |\n| Cash         | 2019-06-30T23:59:54.000000Z | 2             |\n| Card         | 2019-06-30T23:59:56.000000Z | 1             |\nThe above query returns the latest value within each time series stored in the\ntable. Those time series are determined based on the values in the column(s)\nspecified in the `PARTITION BY` part of the `LATEST ON` clause. In our example\nthose time series are represented by different payment types. Then the column\nused in the `LATEST ON` part of the clause stands for the designated timestamp\ncolumn for the table. This allows the database to find the latest value within\neach time series.\nThe below sections will demonstrate other ways to use the `LATEST ON` clause.\nYou can also write this query with the old syntax:\n`questdb-sql\nSELECT payment_type, pickup_datetime, trip_distance\nFROM trips\nLATEST BY payment_type;`\n:::note\nThe old `LATEST BY` syntax is deprecated and will be removed soon. While it's still supported\nby the database, you should use the new `LATEST ON PARTITION BY` syntax in your\napplications. There are two key requirements when using the new syntax:\n\nA timestamp column must always be specified\n`LATEST ON` has to follow the `WHERE` clause. In the old syntax, it was vice\n   versa.\n\n:::\nExamples\nFor the next examples, we can create a table called `balances` with the\nfollowing SQL:\n```questdb-sql\nCREATE TABLE balances (\n    cust_id SYMBOL,\n    balance_ccy SYMBOL,\n    balance DOUBLE,\n    ts TIMESTAMP\n) TIMESTAMP(ts) PARTITION BY DAY;\ninsert into balances values ('1', 'USD', 600.5, '2020-04-21T16:03:43.504432Z');\ninsert into balances values ('2', 'USD', 950, '2020-04-21T16:08:34.404665Z');\ninsert into balances values ('2', 'EUR', 780.2, '2020-04-21T16:11:22.704665Z');\ninsert into balances values ('1', 'USD', 1500, '2020-04-21T16:11:32.904234Z');\ninsert into balances values ('1', 'EUR', 650.5, '2020-04-22T16:11:32.904234Z');\ninsert into balances values ('2', 'USD', 900.75, '2020-04-22T16:12:43.504432Z');\ninsert into balances values ('2', 'EUR', 880.2, '2020-04-22T16:18:34.404665Z');\ninsert into balances values ('1', 'USD', 330.5, '2020-04-22T16:20:14.404997Z');\n```\nThis provides us with a table with the following content:\n| cust_id | balance_ccy | balance | ts                          |\n| ------- | ----------- | ------- | --------------------------- |\n| 1       | USD         | 600.5   | 2020-04-21T16:01:22.104234Z |\n| 2       | USD         | 950     | 2020-04-21T16:03:43.504432Z |\n| 2       | EUR         | 780.2   | 2020-04-21T16:08:34.404665Z |\n| 1       | USD         | 1500    | 2020-04-21T16:11:22.704665Z |\n| 1       | EUR         | 650.5   | 2020-04-22T16:11:32.904234Z |\n| 2       | USD         | 900.75  | 2020-04-22T16:12:43.504432Z |\n| 2       | EUR         | 880.2   | 2020-04-22T16:18:34.404665Z |\n| 1       | USD         | 330.5   | 2020-04-22T16:20:14.404997Z |\nSingle column\nWhen `LATEST ON` is provided a single column of the type `SYMBOL`, the query\nwill end after all distinct symbol values are found.\n`questdb-sql title=\"Latest records by customer ID\"\nSELECT * FROM balances\nLATEST ON ts PARTITION BY cust_id;`\nThe query returns two rows with the most recent records per unique `cust_id`\nvalue:\n| cust_id | balance_ccy | balance | ts                          |\n| ------- | ----------- | ------- | --------------------------- |\n| 2       | EUR         | 880.2   | 2020-04-22T16:18:34.404665Z |\n| 1       | USD         | 330.5   | 2020-04-22T16:20:14.404997Z |\nMultiple columns\nWhen multiple columns are specified in `LATEST ON` queries, the returned results\nare the most recent unique combinations of the column values. This example\nquery returns `LATEST ON` customer ID and balance currency:\n`questdb-sql title=\"Latest balance by customer and currency\"\nSELECT cust_id, balance_ccy, balance\nFROM balances\nLATEST ON ts PARTITION BY cust_id, balance_ccy;`\nThe results return the most recent records for each unique combination of\n`cust_id` and `balance_ccy`.\n| cust_id | balance_ccy | balance | inactive | ts                          |\n| ------- | ----------- | ------- | -------- | --------------------------- |\n| 1       | EUR         | 650.5   | FALSE    | 2020-04-22T16:11:32.904234Z |\n| 2       | USD         | 900.75  | FALSE    | 2020-04-22T16:12:43.504432Z |\n| 2       | EUR         | 880.2   | FALSE    | 2020-04-22T16:18:34.404665Z |\n| 1       | USD         | 330.5   | FALSE    | 2020-04-22T16:20:14.404997Z |\n:::info\nFor a single `SYMBOL` column, QuestDB will know all distinct values upfront and\nstop scanning table contents once the latest entry has been found for each\ndistinct symbol value. When `LATEST ON` is provided multiple columns, QuestDB\nhas to scan the entire table to find distinct combinations of column values.\nAlthough scanning is fast, performance will degrade on hundreds of millions of\nrecords. If there are multiple columns in the `LATEST ON` clause, this will\nresult in a full table scan.\n:::\nLATEST ON over sub-query\nFor this example, we can create another table called `unordered_balances` with\nthe following SQL:\n```questdb-sql\nCREATE TABLE unordered_balances (\n    cust_id SYMBOL,\n    balance_ccy SYMBOL,\n    balance DOUBLE,\n    ts TIMESTAMP\n);\ninsert into unordered_balances values ('2', 'USD', 950, '2020-04-21T16:08:34.404665Z');\ninsert into unordered_balances values ('1', 'USD', 330.5, '2020-04-22T16:20:14.404997Z');\ninsert into unordered_balances values ('2', 'USD', 900.75, '2020-04-22T16:12:43.504432Z');\ninsert into unordered_balances values ('1', 'USD', 1500, '2020-04-21T16:11:32.904234Z');\ninsert into unordered_balances values ('1', 'USD', 600.5, '2020-04-21T16:03:43.504432Z');\ninsert into unordered_balances values ('1', 'EUR', 650.5, '2020-04-22T16:11:32.904234Z');\ninsert into unordered_balances values ('2', 'EUR', 880.2, '2020-04-22T16:18:34.404665Z');\ninsert into unordered_balances values ('2', 'EUR', 780.2, '2020-04-21T16:11:22.704665Z');\n```\nNote that this table doesn't have a designated timestamp column and also\ncontains time series that are unordered by `ts` column.\nDue to the absent designated timestamp column, we can't use `LATEST ON` directly\non this table, but it's possible to use `LATEST ON` over a sub-query:\n`questdb-sql title=\"Latest balance by customer over unordered data\"\n(SELECT * FROM unordered_balances)\nLATEST ON ts PARTITION BY cust_id;`\nJust like with the `balances` table, the query returns two rows with the most\nrecent records per unique `cust_id` value:\n| cust_id | balance_ccy | balance | ts                          |\n| ------- | ----------- | ------- | --------------------------- |\n| 2       | EUR         | 880.2   | 2020-04-22T16:18:34.404665Z |\n| 1       | USD         | 330.5   | 2020-04-22T16:20:14.404997Z |\nExecution order\nThe following queries illustrate how to change the execution order in a query by\nusing brackets.\nWHERE first\n`questdb-sql\nSELECT * FROM balances\nWHERE balance > 800\nLATEST ON ts PARTITION BY cust_id;`\nThis query executes `WHERE` before `LATEST ON` and returns the most recent\nbalance which is above 800. The execution order is as follows:\n\nfilter out all balances below 800\nfind the latest balance by `cust_id`\n\n| cust_id | balance_ccy | balance | ts                          |\n| ------- | ----------- | ------- | --------------------------- |\n| 1       | USD         | 1500    | 2020-04-22T16:11:22.704665Z |\n| 2       | EUR         | 880.2   | 2020-04-22T16:18:34.404665Z |\nLATEST ON first\n`questdb-sql\n(SELECT * FROM balances LATEST ON ts PARTITION BY cust_id) --note the brackets\nWHERE balance > 800;`\nThis query executes `LATEST ON` before `WHERE` and returns the most recent\nrecords, then filters out those below 800. The steps are:\n\nFind the latest balances by customer ID.\nFilter out balances below 800. Since the latest balance for customer 1 is\n   equal to 330.5, it is filtered out in this step.\n\n| cust_id | balance_ccy | balance | inactive | ts                          |\n| ------- | ----------- | ------- | -------- | --------------------------- |\n| 2       | EUR         | 880.2   | FALSE    | 2020-04-22T16:18:34.404665Z |\nCombination\nIt's possible to combine a time-based filter with the balance filter from the\nprevious example to query the latest values for the `2020-04-21` date and filter\nout those below 800.\n`questdb-sql\n(balances WHERE ts in '2020-04-21' LATEST ON ts PARTITION BY cust_id)\nWHERE balance > 800;`\nSince QuestDB allows you to omit the `SELECT * FROM` part of the query, we\nomitted it to keep the query compact.\nSuch a combination is very powerful since it allows you to find the latest\nvalues for a time-slice of the data and then apply a filter to them in a single",
    "tag": "questdb"
  },
  {
    "title": "Syntax",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/sql/case.md",
    "content": "\ntitle: CASE keyword\nsidebar_label: CASE\ndescription: CASE SQL keyword reference documentation.\n\nSyntax\n\nDescription\n`CASE` goes through a set of conditions and returns a value corresponding to the\nfirst condition met. Each new condition follows the `WHEN condition THEN value`\nsyntax. The user can define a return value when no condition is met using\n`ELSE`. If `ELSE` is not defined and no conditions are met, then case returns\n`null`.\nExamples\nAssume the following data\n| name  | age |\n| ----- | --- |\n| Tom   | 4   |\n| Jerry | 19  |\n| Anna  | 25  |\n| Jack  | 8   |\n`questdb-sql title=\"CASE with ELSE\"\nSELECT\nname,\nCASE\n    WHEN age > 18 THEN 'major'\n    ELSE 'minor'\nEND\nFROM my_table`\nResult\n| name  | case  |\n| ----- | ----- |\n| Tom   | minor |\n| Jerry | major |\n| Anna  | major |\n| Jack  | minor |\n`questdb-sql title=\"CASE without ELSE\"\nSELECT\nname,\nCASE\n    WHEN age > 18 THEN 'major'\nEND\nFROM my_table`\nResult\n| name  | case  |\n| ----- | ----- |\n| Tom   | null  |\n| Jerry | major |\n| Anna  | major |",
    "tag": "questdb"
  },
  {
    "title": "Syntax",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/sql/alter-table-detach-partition.md",
    "content": "\ntitle: ALTER TABLE DETACH PARTITION\nsidebar_label: DETACH PARTITION\ndescription: DETACH PARTITION SQL keyword reference documentation.\n\nMakes partition data unavailable for reads and prepares partition directory for transportation. A partition detached by this SQL keyword can be \"re-attached\" using the complementary SQL keyword ALTER TABLE ATTACH PARTITION.\nSyntax\n\nExample\nTo detach one or more partitions, let's assume table `x` with 3 partitions, `2019-02-01`, `2019-02-02`, and `2019-02-03`:\n\n\nDetach two partitions using the SQL `ALTER TABLE DETACH PARTITION` command:\n```questdb-sql\nALTER TABLE x DETACH PARTITION LIST '2019-02-01', '2019-02-02';\n-- It is also possible to use WHERE clause to define the partition list:\nALTER TABLE sensors DETACH PARTITION WHERE < '2019-02-03T00';\n```\n\n\nUsers can move the partition, for example, to an S3 bucket:\n`bash\ncd /var/lib/questdb/db/x/\ntar cfz - '2019-02-01.detached' | aws s3 cp -  s3://questdb-internal/blobs/20190201.tar.gz\ntar cfz - '2019-02-02.detached' | aws s3 cp -  s3://questdb-internal/blobs/20190202.tar.gz`\nThe table directory is nested in the root directory. The root directory is set by `cairo.root` and is set to `db` by default. The detached partition files have the suffix `.detached`.\n\n\nLimitation\n\nQuestDB does not compress partitions after detaching nor does it change partition format significantly. In most cases, compression will have to be done manually before partitions are transported to cold storage.\nThe operation does not support detaching:\nAn active (the last) partition.\n\n\n",
    "tag": "questdb"
  },
  {
    "title": "Variable-sized type limitations",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/sql/datatypes.md",
    "content": "\ntitle: Data types\nsidebar_label: Data types\ndescription: Data types reference documentation.\n\nThe type system is derived from Java types.\n| Type Name         | Storage bits | Nullable | Description                                                                                                                                                                                                                                                         |\n| ----------------- | ------------ | -------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `boolean`         | `1`          | No       | Boolean `true` or `false`.                                                                                                                                                                                                                                          |\n| `byte`            | `8`          | No       | Signed integer `-128` to `127`.                                                                                                                                                                                                                                     |\n| `short`           | `16`         | No       | Signed integer `-32768` to `32767`.                                                                                                                                                                                                                                 |\n| `char`            | `16`         | Yes      | `unicode` character.                                                                                                                                                                                                                                                |\n| `int`             | `32`         | Yes      | Signed integer `0x80000000` to `0x7fffffff`.                                                                                                                                                                                                                        |\n| `float`           | `32`         | Yes      | Single precision IEEE 754 floating point value.                                                                                                                                                                                                                     |\n| `symbol`          | `32`         | Yes      | Symbols are stored as 32-bit signed indexes from symbol table. Each index will have a corresponding `string` value. Translation from index to string value is done automatically when data is being written or read. Symbol table is stored separately from column. |\n| `string`          | `32+n*16`    | Yes      | Length-prefixed sequence of UTF-16 encoded characters whose length is stored as signed 32-bit integer with maximum value of `0x7fffffff`.                                                                                                                           |\n| `long`            | `64`         | Yes      | Signed integer `0x8000000000000000L` to `0x7fffffffffffffffL`.                                                                                                                                                                                                      |\n| `date`            | `64`         | Yes      | Signed offset in milliseconds from Unix Epoch.                                                                                                                                                                       |\n| `timestamp`       | `64`         | Yes      | Signed offset in microseconds from Unix Epoch.                                                                                                                                                                       |\n| `double`          | `64`         | Yes      | Double precision IEEE 754 floating point value.                                                                                                                                                                                                                     |\n| `uuid`            | `128`        | Yes      | UUID values. See also the UUID type.                                                                                                                                               |\n| `binary`          | `64+n*8`     | Yes      | Length-prefixed sequence of bytes whose length is stored as signed 64-bit integer with maximum value of `0x7fffffffffffffffL`.                                                                                                                                      |\n| `long256`         | `256`        | Yes      | Unsigned 256-bit integer. Does not support arbitrary arithmetic operations, but only equality checks. Suitable for storing hash code, such as crypto public addresses.                                                                                              |\n| `geohash(<size>)` | `8`-`64`     | Yes      | Geohash with precision specified as a number followed by `b` for bits, `c` for chars. See the geohashes documentation for details on use and storage.                                                                                    |\nVariable-sized type limitations\n`BINARY` field size is limited either by 64-Bit signed int (8388608 peta bytes)\nor disk size, whichever is smaller.\n`STRING` field size is limited by either 32-bit signed int (1073741824\ncharacters) or disk size, whichever is smaller.\nType nullability\nNullable types use a specific value to mark `NULL` values:\n| Type Name        | Null value                                                           | Description                                                                                                            |\n|------------------| -------------------------------------------------------------------- |------------------------------------------------------------------------------------------------------------------------|\n| `float`          | `NaN`                                                                | As defined by IEEE 754 (`java.lang.Float.NaN`).                                                                        |\n| `double`         | `NaN`                                                                | As defined by IEEE 754 (`java.lang.Double.NaN`).                                                                       |\n| `long256`        | `0x8000000000000000800000000000000080000000000000008000000000000000` | The value equals four consecutive `long` null literals.                                                                |\n| `long`           | `0x8000000000000000L`                                                | Minimum possible value a `long` can take -2^63.                                                                        |\n| `date`           | `0x8000000000000000L`                                                | Minimum possible value a `long` can take -2^63.                                                                        |\n| `timestamp`      | `0x8000000000000000L`                                                | Minimum possible value a `long` can take -2^63.                                                                        |\n| `int`            | `0x80000000`                                                         | Minimum possible value an `int` can take, -2^31.                                                                       |\n| `uuid`           | `80000000-0000-0000-8000-000000000000`                               | Both 64 highest bits and 64 lowest bits set to -2^63.                                                                  |\n| `char`           | `0x0000`                                                             | 0.                                                                                                                     |\n| `geohash(byte)`  | `0xff`                                                               | Geohashes `from 1 up to included 7 bits`.                                                                              |\n| `geohash(short)` | `0xffff`                                                             | Geohashes `from 8 up to included 15 bits`.                                                                             |\n| `geohash(int)`   | `0xffffffff`                                                         | Geohashes `from 16 up to included 31 bits`.                                                                            |\n| `geohash(long)`  | `0xffffffffffffffff`                                                 | Geohashes `from 32 up to included 60 bits`.                                                                            |\n| `symbol`         | `0x80000000`                                                         | Symbols are stored as `int` offsets in a lookup file.                                                                  |\n| `string`         | `0xffffffff`                                                         | Strings are length prefixed, the length is an `int` and `-1` marks it `NULL` (no further storage is used).             |\n| `binary`         | `0xffffffffffffffff`                                                 | Binary columns are also length prefixed, the length is a `long` and `-1` marks it `NULL` (no further storage is used). |\nTo filter columns that contain, or don't contain, `NULL` values use a filter\nlike:\n`questdb-sql\nSELECT * FROM <table> WHERE <column> = NULL;\nSELECT * FROM <table> WHERE <column> != NULL;`\nAlternatively, from version 6.3 use the NULL equality operator aliases:\n`questdb-sql\nSELECT * FROM <table> WHERE <column> IS NULL;\nSELECT * FROM <table> WHERE <column> IS NOT NULL;`\n:::note\n`NULL` values still occupy disk space. \n:::\nThe UUID type\nQuestDB natively supports the `UUID` type, which should be used for `UUID`\ncolumns instead of storing `UUIDs` as `strings`. `UUID` columns are internally\nstored as 128-bit integers, allowing more efficient performance particularly in\nfiltering and sorting. Strings inserted into a `UUID` column is permitted but\nthe data will be converted to the `UUID` type.\n`questdb-sql title=\"Inserting strings into a UUID column\"\nCREATE TABLE my_table (\n    id UUID\n);\n[...]\nINSERT INTO my_table VALUES ('a0eebc99-9c0b-4ef8-bb6d-6bb9bd380a11');\n[...]\nSELECT * FROM my_table WHERE id = 'a0eebc99-9c0b-4ef8-bb6d-6bb9bd380a11';`\nIf you use the PGWire protocol then you can use the `uuid` type in your queries.\nThe JDBC API does not distinguish the UUID type, but the Postgres JDBC driver\nsupports it in prepared statements:\n`java\nUUID uuid = UUID.randomUUID();\nPreparedStatement ps = connection.prepareStatement(\"INSERT INTO my_table VALUES (?)\");\nps.setObject(1, uuid);`\nQuestDB ILP clients can",
    "tag": "questdb"
  },
  {
    "title": "Syntax",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/sql/alter-table-alter-column-drop-index.md",
    "content": "\ntitle: ALTER TABLE COLUMN DROP INDEX\nsidebar_label: DROP INDEX\ndescription: DROP INDEX SQL keyword reference documentation.\n\nRemoves an existing index from a column of type symbol.\nSyntax\n\nRemoving an index is an atomic, non-blocking, and non-waiting operation. Once\nthe operation is completed, the SQL engine stops using the index for SQL\nexecutions, and all its associated files are deleted.\nThis operation is similar to:\n`sql\nUPDATE tab SET column=column;`\nWhere `column` is a symbol column that has an index before the operation, and no\nindex afterwards. Readers of the table might be using the index in transaction\nA, in the meantime, a writer creates transaction B containing the new version of\nthe column, minus the index (metadata is set to not have index, and index files\nare not copied across to the newer version). When the readers are finished,\nQuestDB automatically deletes all the files pertaining to the version of the\ncolumn in transaction A (QuestDB uses hardlinks internally to avoid an actual\ncopy operation of the data files, as they do not change at all).\n:::info\nFor more information about indexes please refer to the\nINDEX documentation\n:::\nExample\n```questdb-sql title=\"Removing an index\"\nALTER TABLE trades ALTER COLUMN instrument DROP INDEX;",
    "tag": "questdb"
  },
  {
    "title": "Syntax",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/sql/alter-table-attach-partition.md",
    "content": "\ntitle: ALTER TABLE ATTACH PARTITION\nsidebar_label: ATTACH PARTITION\ndescription: ATTACH PARTITION SQL keyword reference documentation.\n\nRestores one or more partitions to the table where they have been detached from\nby using the SQL\nALTER TABLE DETACH PARTITION\nstatement.\nThis feature is part of the manual S3/cold storage solution, allowing restoring\ndata manually.\nSyntax\n\nDescription\nBefore executing `ATTACH PARTITION`, the partition folders to be attached must\nbe made available to QuestDB using one of the following methods:\n\nCopying the partition folders manually\nUsing a symbolic link\n\nThis section describes the details of each method.\nManual copy\nPartition folders can be manually moved from where they are stored into the\ntable folder in `db`. To make the partitions available for the attach operation,\nthe files need to be renamed `<partition_name>.attachable`.\nFor example, in a table partitioned by year, given a partition folder named\n`2020.detached`, rename it as `2020.attachable`, and move it to the table\nfolder.\nSymbolic links\nSymbolic links can be used to\nattach partition folders that exist potentially in a different volume as cold\nstorage. The partitions attached in this way will be read-only. To make\ndetached partition folders in cold storage available for attaching, for each\npartition folder, create a symbolic link with the name\nformat`<partition_name>.attachable` from the table's folder, and set the target\npath to the detached partition folder.\n:::info\n\n\nSQL statements that hit partitions attached via symbolic links may have slower\n  runtimes if their volumes have a slower disk.\n\n\nIn Windows, symbolic links require admin privileges, and thus this method is\n  not recommended.\n\n\n:::\nProperties using symbolic links\nPartitions attached via the symbolic link approach are read-only for the\nfollowing operations:\n\nDETACH PARTITION and\n  DROP PARTITION: Once the\n  partition folders are unlinked, the symbolic links are removed, but the\n  content remains. Detaching a partition that was attached via symbolic link\n  does not create a copy `<partition_name>.detached`.\nUPDATE: Attempts to update the read-only\n  partitions result in an error.\nINSERT: Attemps to insert data into a\n  read-only partition result in a critical-level log message being logged by the server, and the insertion is a no-op.\n  If Prometheus monitoring is configured, an\n  alert will be triggered.\n\nFor read-only partitions, the following operations are supported:\n\nADD COLUMN\nDROP COLUMN\nRENAME COLUMN\nADD INDEX\nDROP INDEX\n\nExample\nManual copy\nAssuming the QuestDB data directory is `/var/lib/questdb/db`, for a table `x`\nwith AWS S3 for cold storage:\n\nCopy files from S3:\n\n```bash\n   cd /var/lib/questdb/db/x\n   # Table x is the original table where the partition were detached from.\nmkdir 2019-02-01.attachable && aws s3 cp s3://questdb-internal/blobs/20190201.tar.gz - | tar xvfz - -C 2019-02-01.attachable --strip-components 1\n   mkdir 2019-02-02.attachable && aws s3 cp s3://questdb-internal/blobs/20190202.tar.gz - | tar xvfz - -C 2019-02-01.attachable --strip-components 1\n   ```\n\nExecute the SQL `ALTER TABLE ATTACH PARTITION` command:\n\n`questdb-sql\n   ALTER TABLE x ATTACH PARTITION LIST '2019-02-01', '2019-02-02';`\n\nAfter the SQL is executed, the partitions will be available to read.\n\nSymbolic link\nThe following example creates a table `tab` with some data, detaches all but the\nlast partition, and demonstrates how to attach the partitions using symbolic\nlinks.\nThese SQL statements create table `tab` partitioned by year, and insert seven\nrows that result in a total of seven partitions:\n```sql\nCREATE TABLE tab (name STRING, age INT, dob TIMESTAMP) TIMESTAMP(dob) PARTITION BY YEAR;\nINSERT INTO tab VALUES('B', 1, '2022-11-08T12:00:00.000000Z');\nINSERT INTO tab VALUES('C', 2, '2023-11-08T12:00:00.000000Z');\nINSERT INTO tab VALUES('D', 3, '2024-11-08T12:00:00.000000Z');\nINSERT INTO tab VALUES('E', 4, '2025-11-08T12:00:00.000000Z');\nINSERT INTO tab VALUES('F', 5, '2026-11-08T12:00:00.000000Z');\nINSERT INTO tab VALUES('A', 0, '2027-11-08T12:00:00.000000Z');\nINSERT INTO tab VALUES('0', 0, '2028-11-08T12:00:00.000000Z');\n```\nThis SQL statement detaches partitions 2022, 2023, 2024, 2025, 2026, and 2027:\n`sql\nALTER TABLE tab DETACH PARTITION WHERE dob < '2028';`\nAssuming QuestDB's root directory to be `/opt/homebrew/var/questdb/db`, the\ncontent of the table folder is:\n`shell\n2022.detached\n2023.detached\n2024.detached\n2025.detached\n2026.detached\n2027.detached\n2028.5\n_cv\n_meta\n_todo_\n_txn\n_txn_scoreboard\nseq`\nYou can now move those `<partition_name.detached>` folders to a different path,\npotentially a different volume:\n`shell\nmv /opt/homebrew/var/questdb/db/tab/*.detached /cold_storage/tab`\nWhen you want to attach these partitions back, create a symlink for every\npartition to be attached from the table folder\n`/opt/homebrew/var/questdb/db/tab`:\n`shell\nln -s /cold_storage/tab/2022.detached 2022.attachable\nln -s /cold_storage/tab/2023.detached 2023.attachable\nln -s /cold_storage/tab/2024.detached 2024.attachable\nln -s /cold_storage/tab/2025.detached 2025.attachable\nln -s /cold_storage/tab/2026.detached 2026.attachable\nln -s /cold_storage/tab/2027.detached 2027.attachable`\nThe content of the table folder should look like this now:\n`shell\n2022.attachable -> /cold_storage/tab/2022.detached\n2023.attachable -> /cold_storage/tab/2023.detached\n2024.attachable -> /cold_storage/tab/2024.detached\n2025.attachable -> /cold_storage/tab/2025.detached\n2026.attachable -> /cold_storage/tab/2026.detached\n2027.attachable -> /cold_storage/tab/2027.detached\n2028.5\n_cv\n_meta\n_todo_\n_txn\n_txn_scoreboard\nseq`\nAfter the symbolic links have been created, the partitions can be attached with\nthe following SQL statement:\n`sql\nALTER TABLE tab ATTACH PARTITION LIST '2022', '2023', '2024', '2025', '2026', '2027';`\n:::info\n\nThe SQL reference to the partitions does not include the suffix `.attachable`.\nThe `WHERE` clause is not supported when attaching partitions.\nThe latest partition cannot be detached. However, it can be irreversibly\n  deleted using DROP TABLE.\n\n:::\nLimitation\n\nS3/Cold storage interaction is manual. Partitions can only be attached to the\n  same table they were detached from. The table name must be the same. Moving\n  partitions between tables or database instances is not supported.\nThe operation will fail if a partition already exists. We are working on\n",
    "tag": "questdb"
  },
  {
    "title": "Syntax",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/sql/explain.md",
    "content": "\ntitle: EXPLAIN keyword\nsidebar_label: EXPLAIN\ndescription: EXPLAIN SQL keyword reference documentation.\n\n`EXPLAIN` displays the execution plan of an `INSERT`, `SELECT`, or `UPDATE`\nstatement.\nSyntax\n\nDescription\nA query execution plan shows how a statement will be implemented: which table is\ngoing to be accessed and how, what join method are employed, and which\npredicates are JIT-compiled etc.\n`EXPLAIN` output is a tree of nodes containing properties and subnodes (aka\nchild nodes).\nIn a plan such as:\n| QUERY PLAN                                                                 |\n| -------------------------------------------------------------------------- |\n| Async JIT Filter                                                           |\n| \u00a0\u00a0filter: 100<l                                                  |\n| \u00a0\u00a0workers: 1                                                     |\n| \u00a0\u00a0\u00a0\u00a0DataFrame                                          |\n| \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Row forward scan           |\n| \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Frame forward scan on: tab |\nthere are:\n\n4 nodes:\nAsync JIT Filter\nDataFrame\nRow forward scan\nFrame forward scan\n2 properties (both belong to Async JIT Filter node):\nfilter\nworkers\n\nFor simplicity, some nodes have special properties shown on the same line as\ntype; for example, `Filter filter: b.age=10` or `Limit lo: 10`.\nThe following list contains some plan node types:\n\n`Async Filter` - a parallelized filter that evaluates expressions with Java\n  code. In certain scenarios, it also implements the `LIMIT` keyword.\n`Async JIT Filter` - a parallelized filter that evaluates expressions with\n  Just-In-Time-compiled filter. In certain scenarios, it also implements the\n  `LIMIT` keyword.\n`Interval forward` - scans one or more table data ranges based on the\n  designated timestamp predicates. Scan endpoints are found via a binary search\n  on timestamp column.\n`CachedAnalytic` - container for window functions, e.g.\n  row_number()\n`Count` - returns the count of records in subnode.\n`Cursor-order scan` - scans table records using row ids taken from an index,\n  in index order - first all row ids linked to index value A, then B, etc.\n`DataFrame` - full or partial table scan. It contains two children:\nrow cursor - which iterates over rows inside a frame (e.g.\n    `Row forward scan`).\nframe cursor - which iterates over table partitions or partition chunks\n    (e.g. `Frame forward scan`).\n`Filter` - standalone (non-JIT-compiled, non-parallelized) filter.\n`Frame forward/backward scan` - scans table partitions in a specified\n  direction.\n`GroupBy` - group by with or without key(s). If `vectorized` field shows\n  `true`, then the node is parallelized and uses vectorized calculations.\n`Hash` - subnode of this node is used to build a hash table that is later\n  looked up (usually in a `JION` clause but also applies to `EXCEPT` or\n  `INTERSECT`).\n`Index forward/backward scan` - scans all row ids associated with a given\n  `symbol` value from start to finish or vice versa.\n`Limit` - standalone node implementing the `LIMIT` keyword. Other nodes can\n  implement `LIMIT` internally, e.g. the `Sort` node.\n`Row forward/backward scan` - scans data frame (usually partitioned) records\n  in a specified direction.\n`Sort` - sorts data. If low or hi property is specified, then the sort buffer\n  size is limited and a number of rows are skipped after sorting.\n`SampleBy` - `SAMPLE BY` keyword implementation. If the `fill` is not shown,\n  it means `fill(none)`.\n`Selected Record` - used to reorder or rename columns. It does not do any\n  significant processing on its own.\n`Table-order scan` - scans table records using row ids taken from an index in\n  table (physical) order - from the lowest to highest row id.\n`VirtualRecord` - adds expressions to a subnode's columns.\n\nOther node types should be easy to link to SQL and database concepts, e.g.\n`Except`, `Hash Join` or `Lt Join`.\nMany nodes, especially join and sort, have 'light' and 'heavy' variants, e.g.\n`Hash Join Light` and `Hash Join`. The former is used when child node(s) support\nefficient random access lookups (e.g. `DataFrame`) so storing row id in the\nbuffer is enough; otherwise, the whole record needs to be copied and the 'heavy'\nfactory is used.\nExamples\nTo illustrate how `EXPLAIN` works, consider the `trades` table\nin the QuestDB demo instance:\n`questdb-sql\nCREATE TABLE trades (\n  symbol SYMBOL CAPACITY 256 CACHE,\n  side SYMBOL CAPACITY 256 CACHE,\n  price DOUBLE,\n  amount DOUBLE,\n  timestamp TIMESTAMP\n) TIMESTAMP (timestamp) PARTITION BY DAY`\nUsing `EXPLAIN` for the plan for `SELECT`\nThe following query highlight the plan for `ORDER BY` for the table:\n`questdb-sql\nEXPLAIN SELECT * FROM trades ORDER BY ts DESC;`\n| QUERY PLAN                                             |\n| ------------------------------------------------------ |\n| DataFrame                                              |\n| \u00a0\u00a0\u00a0\u00a0Row backward scan              |\n| \u00a0\u00a0\u00a0\u00a0Frame backward scan on: trades |\nThe plan shows that no sort is required and the result is produced by scanning\nthe table backward.\nThe scanning direction is possible because the data in the `trades` table is\nstored in timestamp order.\nNow, let's check the plan for `trades` with a simple filter:\n`questdb-sql\nEXPLAIN SELECT * FROM trades WHERE amount > 100.0;`\n| QUERY PLAN                                                                    |\n| ----------------------------------------------------------------------------- |\n| Async JIT Filter                                                              |\n| \u00a0\u00a0filter: 100.0<amount                                              |\n| \u00a0\u00a0workers: 1                                                        |\n| \u00a0\u00a0\u00a0\u00a0DataFrame                                             |\n| \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Row forward scan              |\n| \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Frame forward scan on: trades |\nIn this example, the plan shows that the `trades` table undergoes a full scan\n(`DataFrame` and subnodes) and the data is processed by the parallelized\nJIT-compiled filter.\nUsing `EXPLAIN` for the plan for `CREATE` and `INSERT`\nApart from `SELECT`, `EXPLAIN` also works on `CREATE` and `INSERT` statements.\nSingle-row inserts are straightforward. The examples in this section show the\nplan for more complicated `CREATE` and `INSERT` queries.\n`questdb-sql\nEXPLAIN CREATE TABLE trades AS\n(\n  SELECT\n    rnd_symbol('a', 'b') symbol,\n    rnd_symbol('Buy', 'Sell') side,\n    rnd_double() price,\n    rnd_double() amount,\n    x::timestamp timestamp\n  FROM long_sequence(10)\n) TIMESTAMP(timestamp) PARTITION BY DAY;`\n| QUERY PLAN                                                                                                                       |\n| -------------------------------------------------------------------------------------------------------------------------------- |\n| Create table: trades                                                                                                             |\n| \u00a0\u00a0\u00a0\u00a0VirtualRecord                                                                                            |\n| \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0functions: [rnd_symbol([a,b]),rnd_symbol([Buy,Sell]),rnd_double(),rnd_double(),x::timestamp] |\n| \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0long_sequence count: 10                                                          |\nThe plan above shows that the data is fetched from a `long_sequence` cursor,\nwith random data generating functions called in `VirtualRecord`.\nThe same applies to the following query:\n`questdb-sql\nEXPLAIN INSERT INTO trades\n  SELECT\n    rnd_symbol('a', 'b') symbol,\n    rnd_symbol('Buy', 'Sell') side,\n    rnd_double() price,\n    rnd_double() amount,\n    x::timestamp timestamp\n  FROM long_sequence(10);`\n| QUERY PLAN                                                                                                                       |\n| -------------------------------------------------------------------------------------------------------------------------------- |\n| Insert into table: trades                                                                                                        |\n| \u00a0\u00a0\u00a0\u00a0VirtualRecord                                                                                            |\n| \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0functions: [rnd_symbol([a,b]),rnd_symbol([Buy,Sell]),rnd_double(),rnd_double(),x::timestamp] |\n| \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0long_sequence count: 10                                                          |\nOf course, statements could be much more complex than that. Consider the\nfollowing `UPDATE` query:\n`questdb-sql\nEXPLAIN UPDATE trades SET amount = 0 WHERE timestamp IN '2022-11-11';`\n| QUERY PLAN                                                                                                                                 |\n| ------------------------------------------------------------------------------------------------------------------------------------------ |\n| Update table: trades                                                                                                                       |\n| \u00a0\u00a0\u00a0\u00a0VirtualRecord                                                                                                      |\n| \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0functions: [0]                                                                                         |\n| \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0DataFrame                                                                                  |\n| \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Row forward scan                                                   |\n| \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Interval forward scan on: trades                                   |\n| \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0intervals: [static=[1668124800000000,1668211199999999] |\nThe important bit here is `Interval forward scan`. It means that the table is\nforward scanned only between points designated by the\n`timestamp IN '2022-11-11'` predicate, that is between\n`2022-11-11 00:00:00,000000` and `2022-11-11 23:59:59,999999` (shown as raw\nepoch micro values in the plan above). `VirtualRecord` is only used to pass 0\nconstant for each row coming from `DataFrame`.\nLimitations:\nTo minimize resource usage, the `EXPLAIN` command does not execute the\nstatement, to avoid paying a potentially large upfront cost for certain queries\n(especially those involving hash join or sort).\n`EXPLAIN` provides a useful indication of the query execution, but it does not\nguarantee to show the actual execution plan. This is because elements determined\nduring query runtime are missing.\nWhile `EXPLAIN` shows the number of workers that could be used by a parallelized\nnode it is only the upper limit. Depending on the data volume and system load, a\nquery can use fewer workers.\n:::note\nUnder the hood, the plan nodes are called `Factories`. Most plan nodes can be\nmapped to implementation by adding the `RecordCursorFactory` or\n`FrameCursorFactory` suffix, e.g.\n\n`DataFrame` -> `DataFrameRecordCursorFactory`\n`Async JIT Filter` -> `AsyncJitFilteredRecordCursorFactory`\n`SampleByFillNoneNotKeyed` -> `SampleByFillNoneNotKeyedRecordCursorFactory`\n  while some are a bit harder to identify, e.g.\n`GroupByRecord vectorized: false` ->\n  `io.questdb.griffin.engine.groupby.GroupByRecordCursorFactory`\n`GroupByRecord vectorized: true` ->\n  `io.questdb.griffin.engine.groupby.vect.GroupByRecordCursorFactory`\n\nOther classes can be identified by searching for the node name in the `toPlan()`\nmethods.\n:::\nSee also\nThis section includes links to additional information such as tutorials:",
    "tag": "questdb"
  },
  {
    "title": "Syntax",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/sql/drop.md",
    "content": "\ntitle: DROP TABLE keyword\nsidebar_label: DROP TABLE\ndescription: DROP TABLE SQL keyword reference documentation.\n\n`DROP TABLE` is used to permanently delete a table and its contents.\n:::caution\nThis command irremediably deletes the data in the target table. Unless the \ntable was created in a different volume than the standard, see \nCREATE TABLE IN VOLUME, in\nwhich case the table is only logically removed and data remains intact in \nits volume. In doubt, make sure you have created \nbackups of your data.\n:::\nSyntax\n\nIF EXISTS\nAn optional `IF EXISTS` clause may be added directly after the `DROP TABLE`\nkeywords to indicate that the selected table should be dropped if it exists.\nExample\n`questdb-sql\nDROP TABLE ratings;`\n:::tip\nTo delete the data inside a table but keep the table and its structure, use\nTRUNCATE.",
    "tag": "questdb"
  },
  {
    "title": "Syntax",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/sql/snapshot.md",
    "content": "\ntitle: SNAPSHOT keyword\nsidebar_label: SNAPSHOT\ndescription: SNAPSHOT SQL keyword reference documentation.\n\nPrepares the database for a filesystem (disk) snapshot.\n:::warning\nSnapshot statements are not supported on Windows OS.\n:::\nSyntax\n\nSnapshot process\nSnapshot recovery mechanism requires a snapshot instance ID to be specified\nusing the `cairo.snapshot.instance.id`\nconfiguration key:\n`shell title=\"server.conf\"\ncairo.snapshot.instance.id=your_id`\nA snapshot instance ID may be an arbitrary string value, such as string\nrepresentation of a UUID.\nCollecting a snapshot of the database involves the following steps:\n\nRun `SNAPSHOT PREPARE` statement to acquire reader locks for all database\n   tables, create table metadata file copies in the `snapshot` directory, and\n   flush the committed data to disk.\nStart a filesystem snapshot. Refer to the\n   next section to learn how to create a filesystem\n   snapshot on the most common cloud providers.\nRun `SNAPSHOT COMPLETE` statement to release the reader locks and delete the\n   metadata file copies.\n\nFor some cloud vendors, snapshot creation operation is asynchronous, i.e. the\npoint-in-time snapshot is created immediately, as soon as the operation starts,\nbut the end snapshot artifact may become available later. In such case, the\n`SNAPSHOT COMPLETE` statement (step 3) may be run without waiting for the end\nartifact, but once the snapshot creation has started.\n:::info\nNo DDL statements, such as `ALTER TABLE my_table DROP COLUMN my_col`, should be\nrun in parallel with the above steps. Otherwise, the snapshot may contain\ncorrupted metadata making it unusable.\n:::\nFilesystem snapshot\nThe most common ways to perform cloud-native filesystem snapshots are described\nin the following resources, which rely on similar steps but have minor\ndifferences in terminology and services:\n\nAWS -\n  creating EBS snapshots\nAzure -\n  creating snapshots of a virtual hard disk\nGCP - working\n  with persistent disk snapshots\n\nSnapshot recovery\nTo start the database on a filesystem snapshot, you should make sure to\nconfigure a different snapshot instance ID.\nWhen the database starts, it checks the current instance ID and the ID stored in\nthe `snapshot` directory, if present. On IDs mismatch, the database runs a\nsnapshot recovery procedure restoring the metadata files from the snapshot. When\nthis happens, you should see something like the following in the server logs:\n`2022-03-07T08:24:12.348004Z I i.q.g.DatabaseSnapshotAgent starting snapshot recovery [currentId=`id2`, previousId=`id1`]\n...\n2022-03-07T08:24:12.349922Z I i.q.g.DatabaseSnapshotAgent snapshot recovery finished [metaFilesCount=1, txnFilesCount=1, cvFilesCount=1]`\nSnapshot recovery can be disabled using the `cairo.snapshot.recovery.enabled`\nconfiguration key:\n`shell title=\"server.conf\"\ncairo.snapshot.recovery.enabled=false`\nExamples\n```questdb-sql\nSNAPSHOT PREPARE;\n-- Start a filesystem snapshot.\nSNAPSHOT COMPLETE;",
    "tag": "questdb"
  },
  {
    "title": "Syntax",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/sql/select.md",
    "content": "\ntitle: SELECT keyword\nsidebar_label: SELECT\ndescription: SELECT SQL keyword reference documentation.\n\n`SELECT` allows you to specify a list of columns and expressions to be selected\nand evaluated from a table.\nSyntax\n\n:::tip\nThe `table` can either be in your database (in which case you would pass the\ntable's name), or the result of a sub-query.\n:::\nSimple select\nAll columns\nQuestDB supports `SELECT * FROM tablename`. When selecting all, you can also\nomit most of the statement and pass the table name.\nThe two examples below are equivalent\n`questdb-sql title=\"QuestDB dialect\"\nratings;`\n`questdb-sql title=\"Traditional SQL equivalent\"\nSELECT * FROM ratings;`\nSpecific columns\nTo select specific columns, replace * by the names of the columns you are\ninterested in.\nExample:\n`questdb-sql\nSELECT movieId, rating FROM ratings;`\nArithmetic expressions\n`SELECT` is capable of evaluating multiple expressions and functions. You can\nmix comma separated lists of expressions with the column names you are\nselecting.\n`questdb-sql\nSELECT movieId, (100 - rating)*2, rating > 3.5 good\nFROM ratings;`\nThe result of `rating > 3.5` is a boolean. The column will be named good and\ntake values true or false.\nAliases\nUsing aliases allow you to give expressions or column names of your choice. You\ncan assign an alias to a column or an expression by writing the alias name you\nwant after that expression\n:::note\nAlias names and column names must be unique.\n:::\n`questdb-sql\nSELECT movieId alias1, rating alias2\nFROM ratings`\nAggregation\n:::info\nSupported aggregation functions are listed on the\naggregation reference.\n:::\nAggregation by group\nQuestDB evaluates aggregation functions without need for traditional `GROUP BY`.\nUse a mix of column names and aggregation functions in a `SELECT` clause. You\ncan have any number of discrete value columns and any number of aggregation\nfunctions.\n`questdb-sql title=\"QuestDB dialect\"\nSELECT movieId, avg(rating), count()\nFROM ratings;`\n`questdb-sql title=\"Traditional SQL equivalent\"\nSELECT movieId, avg(rating), count()\nFROM ratings\nGROUP BY movieId;`\nAggregation arithmetic\nAggregation functions can be used in arithmetic expressions. The following\ncomputes `mid` of rating values for every movie.\n`questdb-sql\nSELECT movieId, (min(rating) + max(rating))/2 mid, count() count\nFROM ratings;`\n:::tip\nWhenever possible, it is recommended to perform arithmetic `outside` of\naggregation functions as this can have a dramatic impact on performance. For\nexample, `min(value/2)` is going to execute considerably more slowly than\n`min(value)/2`, although both return the same result.\n:::\nSupported clauses\nQuestDB supports the following standard SQL clauses within SELECT statements.\nCASE\nConditional results based on expressions.\nSyntax\n\nFor more information, please refer to the\nCASE reference\nCAST\nConvert values and expression between types.\nSyntax\n\nFor more information, please refer to the\nCAST reference\nDISTINCT\nReturns distinct values of the specified column(s).\nSyntax\n\nFor more information, please refer to the\nDISTINCT reference.\nFILL\nDefines filling strategy for missing data in aggregation queries. This function\ncomplements SAMPLE BY queries.\nSyntax\n\nFor more information, please refer to the\nFILL reference.\nJOIN\nJoin tables based on a key or timestamp.\nSyntax\n\nFor more information, please refer to the\nJOIN reference\nLIMIT\nSpecify the number and position of records returned by a query.\nSyntax\n\nFor more information, please refer to the\nLIMIT reference.\nORDER BY\nOrders the results of a query by one or several columns.\nSyntax\n\nFor more information, please refer to the\nORDER BY reference\nUNION, EXCEPT & INTERSECT\nCombine the results of two or more select statements. Can include or ignore\nduplicates.\nSyntax\n\nFor more information, please refer to the\nUNION, EXCEPT & INTERSECT reference\nWHERE\nFilters query results\nSyntax\n\nQuestDB supports complex WHERE clauses along with type-specific searches. For\nmore information, please refer to the\nWHERE reference. There are different syntaxes for\ntext,\nnumeric, or\ntimestamp filters.\nAdditional time series clauses\nQuestDB augments SQL with the following clauses.\nLATEST ON\nRetrieves the latest entry by timestamp for a given key or combination of keys\nThis function requires a\ndesignated timestamp.\nSyntax\n\nFor more information, please refer to the\nLATEST ON reference.\nSAMPLE BY\nAggregates time series data into homogeneous time chunks. For example daily\naverage, monthly maximum etc. This function requires a\ndesignated timestamp.\nSyntax\n\nFor more information, please refer to the\nSAMPLE BY reference.\nTIMESTAMP\nDynamically creates a designated timestamp\non the output of a query. This allows to perform timestamp operations like\nSAMPLE BY or LATEST ON on tables which originally do\nnot have a designated timestamp.\n:::caution\nThe output query must be ordered by time. `TIMESTAMP()` does not check for order\nand using timestamp functions on unordered data may produce unexpected results.\n:::\nSyntax\n\nFor more information, refer to the",
    "tag": "questdb"
  },
  {
    "title": "Syntax",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/sql/sample-by.md",
    "content": "\ntitle: SAMPLE BY keyword\nsidebar_label: SAMPLE BY\ndescription: SAMPLE BY SQL keyword reference documentation.\n\n`SAMPLE BY` is used on time series data to summarize large datasets into\naggregates of homogeneous time chunks as part of a\nSELECT statement. Users performing `SAMPLE BY`\nqueries on datasets with missing data may make use of the\nFILL keyword to specify a fill behavior.\n`questdb-sql title=\"Sample trades table in 30 minute intervals\"\nSELECT time, avg(price) FROM trades SAMPLE BY 30m`\n:::info\nTo use `SAMPLE BY`, a table column needs to be specified as a designated\ntimestamp. Details about this concept can be found in the\ndesignated timestamp documentation.\n:::\nSyntax\n\n\n\nSample units\nThe size of sampled groups are specified with the following syntax:\n`questdb-sql\nSAMPLE BY n{units}`\nWhere the unit for sampled groups may be one of the following:\n| unit | description |\n| ---- | ----------- |\n| `U`  | microsecond |\n| `T`  | millisecond |\n| `s`  | second      |\n| `m`  | minute      |\n| `h`  | hour        |\n| `d`  | day         |\n| `M`  | month       |\n| `y`  | year       |\nFor example, given a table `trades`, the following query returns the number of\ntrades per hour:\n`questdb-sql\nSELECT ts, count() FROM trades SAMPLE BY 1h`\nFill options\nThe `FILL` keyword is optional and expects one or more `fillOption` strategies\nwhich will be applied to one or more aggregate columns. The following\nrestrictions apply:\n\nKeywords denoting fill strategies may not be combined. Only one option from\n  `NONE`, `NULL`, `PREV`, `LINEAR` and constants may be used.\n`LINEAR` strategy is not supported for keyed queries, i.e. queries that\n  contain non-aggregated columns other than the timestamp in the SELECT clause.\n\n| fillOption | Description                                                                                                               |\n| ---------- | ------------------------------------------------------------------------------------------------------------------------- |\n| `NONE`     | No fill applied. If there is no data, the time sample will be skipped in the results. A table could be missing intervals. |\n| `NULL`     | Fills with `NULL` values.                                                                                                 |\n| `PREV`     | Fills using the previous value.                                                                                           |\n| `LINEAR`   | Fills by linear interpolation of the 2 surrounding points.                                                                |\n| `x`        | Fills with a constant value - where `x` is the desired value, for example `FILL(100.05)`.                                 |\nConsider an example table named `prices` which has no records during the entire\nthird hour (`2021-01-01T03`):\n| ts                          | price |\n| --------------------------- | ----- |\n| 2021-01-01T01:00:00.000000Z | p1    |\n| 2021-01-01T02:00:00.000000Z | p2    |\n| 2021-01-01T04:00:00.000000Z | p4    |\n| 2021-01-01T05:00:00.000000Z | p5    |\nThe following query returns the maximum price per hour. As there are missing\nvalues, an aggregate cannot be calculated:\n`questdb-sql\nSELECT ts, max(price) max FROM prices SAMPLE BY 1h;`\nA row is missing for the `2021-01-01T03:00:00.000000Z` sample:\n| ts                          | max  |\n| --------------------------- | ---- |\n| 2021-01-01T01:00:00.000000Z | max1 |\n| 2021-01-01T02:00:00.000000Z | max2 |\n| 2021-01-01T04:00:00.000000Z | max4 |\n| 2021-01-01T05:00:00.000000Z | max5 |\nA `FILL` strategy can be employed which fills with the previous value using\n`PREV`:\n`questdb-sql\nSELECT ts, max(price) max FROM prices SAMPLE BY 1h FILL(PREV);`\n| ts                              | max      |\n| ------------------------------- | -------- |\n| 2021-01-01T01:00:00.000000Z     | max1     |\n| 2021-01-01T02:00:00.000000Z     | max2     |\n| 2021-01-01T03:00:00.000000Z | max2 |\n| 2021-01-01T04:00:00.000000Z     | max4     |\n| 2021-01-01T05:00:00.000000Z     | max5     |\nLinear interpolation is done using the `LINEAR` fill option:\n`questdb-sql\nSELECT ts, max(price) max FROM prices SAMPLE BY 1h FILL(LINEAR);`\n| ts                              | max               |\n| ------------------------------- | ----------------- |\n| 2021-01-01T01:00:00.000000Z     | max1              |\n| 2021-01-01T02:00:00.000000Z     | max2              |\n| 2021-01-01T03:00:00.000000Z | (max2+max4)/2 |\n| 2021-01-01T04:00:00.000000Z     | max4              |\n| 2021-01-01T05:00:00.000000Z     | max5              |\nA constant value can be used as a `fillOption`:\n`questdb-sql\nSELECT ts, max(price) max FROM prices SAMPLE BY 1h FILL(100.5);`\n| ts                              | max       |\n| ------------------------------- | --------- |\n| 2021-01-01T01:00:00.000000Z     | max1      |\n| 2021-01-01T02:00:00.000000Z     | max2      |\n| 2021-01-01T03:00:00.000000Z | 100.5 |\n| 2021-01-01T04:00:00.000000Z     | max4      |\n| 2021-01-01T05:00:00.000000Z     | max5      |\nFinally, `NULL` may be used as a `fillOption`:\n`questdb-sql\nSELECT ts, max(price) max FROM prices SAMPLE BY 1h FILL(NULL);`\n| ts                              | max      |\n| ------------------------------- | -------- |\n| 2021-01-01T01:00:00.000000Z     | max1     |\n| 2021-01-01T02:00:00.000000Z     | max2     |\n| 2021-01-01T03:00:00.000000Z | null |\n| 2021-01-01T04:00:00.000000Z     | max4     |\n| 2021-01-01T05:00:00.000000Z     | max5     |\n:::info\nThe `FILL` keyword must precede alignment described in the\nsample calculation section, i.e.:\n`questdb-sql\nSELECT ts, max(price) max FROM prices\nSAMPLE BY 1h FILL(LINEAR)\nALIGN TO ...`\n:::\nMultiple fill values\n`FILL()` accepts a list of values where each value corresponds to a single\naggregate column in the SELECT clause order:\n`questdb-sql\nSELECT min(price), max(price), avg(price), ts\nFROM prices\nSAMPLE BY 1h\nFILL(NULL, 10, PREV);`\nIn the above query `min(price)` aggregate will get `FILL(NULL)` strategy\napplied, `max(price)` will get `FILL(10)`, and `avg(price)` will get\n`FILL(PREV)`.\nSample calculation\nThe default time calculation of sampled groups is an absolute value, in other\nwords, sampling by one day is a 24 hour range which is not bound to calendar\ndates. To align sampled groups to calendar dates, the `ALIGN TO` keywords can be\nused and are described in the ALIGN TO CALENDAR section\nbelow.\nConsider a table `sensors` with the following data spanning three calendar days:\n| ts                          | val |\n| --------------------------- | --- |\n| 2021-05-31T23:10:00.000000Z | 10  |\n| 2021-06-01T01:10:00.000000Z | 80  |\n| 2021-06-01T07:20:00.000000Z | 15  |\n| 2021-06-01T13:20:00.000000Z | 10  |\n| 2021-06-01T19:20:00.000000Z | 40  |\n| 2021-06-02T01:10:00.000000Z | 90  |\n| 2021-06-02T07:20:00.000000Z | 30  |\nThe following query can be used to sample the table by day. Note that the\ndefault sample calculation can be made explicit in a query using\n`ALIGN TO FIRST OBSERVATION`:\n```questdb-sql\nSELECT ts, count() FROM sensors\nSAMPLE BY 1d\n-- Equivalent to\nSELECT ts, count() FROM sensors\nSAMPLE BY 1d\nALIGN TO FIRST OBSERVATION\n```\nThis query will return two rows:\n| ts                          | count |\n| --------------------------- | ----- |\n| 2021-05-31T23:10:00.000000Z | 5     |\n| 2021-06-01T23:10:00.000000Z | 2     |\nThe timestamp value for the 24 hour groups start at the first-observed\ntimestamp.\nALIGN TO CALENDAR\nThe option aligns data to calendar dates, with two optional parameters:\n\nTIME ZONE\nWITH OFFSET\n\nTIME ZONE\nA time zone may be provided for sampling with calendar alignment. Details on the\noptions for specifying time zones with available formats are provided in the\nguide for\nworking with timestamps and time zones.\n`questdb-sql\nSELECT ts, count() FROM sensors\nSAMPLE BY 1d\nALIGN TO CALENDAR TIME ZONE 'Europe/Berlin'`\nIn this case, the 24 hour samples begin at `2021-05-31T01:00:00.000000Z`:\n| ts                          | count |\n| --------------------------- | ----- |\n| 2021-05-31T01:00:00.000000Z | 1     |\n| 2021-06-01T01:00:00.000000Z | 4     |\n| 2021-06-02T01:00:00.000000Z | 2     |\nAdditionally, an offset may be applied when aligning sample calculation to\ncalendar\n`questdb-sql\nSELECT ts, count() FROM sensors\nSAMPLE BY 1d\nALIGN TO CALENDAR TIME ZONE 'Europe/Berlin' WITH OFFSET '00:45'`\nIn this case, the 24 hour samples begin at `2021-05-31T01:45:00.000000Z`:\n| ts                          | count |\n| --------------------------- | ----- |\n| 2021-05-31T01:45:00.000000Z | 2     |\n| 2021-06-01T01:45:00.000000Z | 4     |\n| 2021-06-02T01:45:00.000000Z | 1     |\nLocal timezone output\nThe timestamp values output from `SAMPLE BY` queries is in UTC. To have UTC\nvalues converted to specific timezones the\nto_timezone() function should\nbe used.\n`questdb-sql\nSELECT to_timezone(ts, 'PST') ts, count\nFROM (SELECT ts, count()\n      FROM sensors SAMPLE BY 2h\n      ALIGN TO CALENDAR TIME ZONE 'PST')`\nTime zone transitions\nCalendar dates may contain historical time zone transitions or may vary in the\ntotal number of hours due to daylight savings time. Considering the 31st October\n2021, in the `Europe/London` calendar day which consists of 25 hours:\n\n\nSunday, 31 October 2021, 02:00:00 clocks are turned backward 1 hour to\nSunday, 31 October 2021, 01:00:00 local standard time\n\n\nWhen a `SAMPLE BY` operation crosses time zone transitions in cases such as\nthis, the first sampled group which spans a transition will include aggregates\nby full calendar range. Consider a table `sensors` with one data point per hour\nspanning three calendar hours:\n| ts                          | val |\n| --------------------------- | --- |\n| 2021-10-31T00:10:00.000000Z | 10  |\n| 2021-10-31T01:10:00.000000Z | 20  |\n| 2021-10-31T02:10:00.000000Z | 30  |\n| 2021-10-31T03:10:00.000000Z | 40  |\n| 2021-10-31T04:10:00.000000Z | 50  |\nThe following query will sample by hour with the `Europe/London` time zone and\nalign to calendar ranges:\n`questdb-sql\nSELECT ts, count() FROM sensors\nSAMPLE BY 1h\nALIGN TO CALENDAR TIME ZONE 'Europe/London'`\nThe record count for the hour which encounters a time zone transition will\ncontain two records for both hours at the time zone transition:\n| ts                          | count |\n| --------------------------- | ----- |\n| 2021-10-31T00:00:00.000000Z | 2     |\n| 2021-10-31T01:00:00.000000Z | 1     |\n| 2021-10-31T02:00:00.000000Z | 1     |\n| 2021-10-31T03:00:00.000000Z | 1     |\nSimilarly, given one data point per hour on this table, running `SAMPLE BY 1d`\nwill have a count of `25` for this day when aligned to calendar time zone\n'Europe/London'.\nWITH OFFSET\nAligning sampling calculation can be provided an arbitrary offset in the format\n`'+/-HH:mm'`, for example:\n\n`'00:30'` plus thirty minutes\n`'+00:30'` plus thirty minutes\n`'-00:15'` minus 15 minutes\n\nThe query uses the default offset '00:00' if the parameter is not set.\n`questdb-sql\nSELECT ts, count() FROM sensors\nSAMPLE BY 1d\nALIGN TO CALENDAR WITH OFFSET '02:00'`\nIn this case, the 24 hour samples begin at `2021-05-31T02:00:00.000000Z`:\n| ts                          | count |\n| --------------------------- | ----- |\n| 2021-05-31T02:00:00.000000Z | 2     |\n| 2021-06-01T02:00:00.000000Z | 4     |\n| 2021-06-02T02:00:00.000000Z | 1     |\nExamples\nAssume the following table `trades`:\n| ts                          | quantity | price  |\n| --------------------------- | -------- | ------ |\n| 2021-05-31T23:45:10.000000Z | 10       | 100.05 |\n| 2021-06-01T00:01:33.000000Z | 5        | 100.05 |\n| 2021-06-01T00:15:14.000000Z | 200      | 100.15 |\n| 2021-06-01T00:30:40.000000Z | 300      | 100.15 |\n| 2021-06-01T00:45:20.000000Z | 10       | 100    |\n| 2021-06-01T01:00:50.000000Z | 50       | 100.15 |\nThis query will return the number of trades per hour:\n`questdb-sql title=\"Hourly interval\"\nSELECT ts, count() FROM trades SAMPLE BY 1h;`\n| ts                          | count |\n| --------------------------- | ----- |\n| 2021-05-31T23:45:10.000000Z | 3     |\n| 2021-06-01T00:45:10.000000Z | 1     |\n| 2021-05-31T23:45:10.000000Z | 1     |\n| 2021-06-01T00:45:10.000000Z | 1     |\nThe following will return the trade volume in 30 minute intervals\n`questdb-sql title=\"30 minute interval\"\nSELECT ts, sum(quantity*price) FROM trades SAMPLE BY 30m;`\n| ts                          | sum    |\n| --------------------------- | ------ |\n| 2021-05-31T23:45:10.000000Z | 1000.5 |\n| 2021-06-01T00:15:10.000000Z | 16024  |\n| 2021-06-01T00:45:10.000000Z | 8000   |\n| 2021-06-01T00:15:10.000000Z | 8012   |\n| 2021-06-01T00:45:10.000000Z | 8000   |\nThe following will return the average trade notional (where notional is = q *\np) by day:\n`questdb-sql title=\"Daily interval\"\nSELECT ts, avg(quantity*price) FROM trades SAMPLE BY 1d;`\n| ts                          | avg               |\n| --------------------------- | ----------------- |\n| 2021-05-31T23:45:10.000000Z | 6839.416666666667 |\nTo make this sample align to calendar dates:\n`questdb-sql title=\"Calendar alignment\"\nSELECT ts, avg(quantity*price) FROM trades SAMPLE BY 1d ALIGN TO CALENDAR;`\n| ts                          | avg    |\n| --------------------------- | ------ |\n| 2021-05-31T00:00:00.000000Z | 1000.5 |\n| 2021-06-01T00:00:00.000000Z | 8007.2 |\nSee also\nThis section includes links to additional information such as tutorials:\n\nSQL Extensions for Time Series Data in QuestDB\n",
    "tag": "questdb"
  },
  {
    "title": "Syntax",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/sql/order-by.md",
    "content": "\ntitle: ORDER BY keyword\nsidebar_label: ORDER BY\ndescription: ORDER BY SQL keyword reference documentation.\n\nSort the results of a query in ascending or descending order.\nSyntax\n\nDefault order is `ASC`. You can omit to order in ascending order.\nExamples\n`questdb-sql title=\"Omitting ASC will default to ascending order\"\nratings ORDER BY userId;`\n`questdb-sql title=\"Ordering in descending order\"\nratings ORDER BY userId DESC;`\n`questdb-sql title=\"Multi-level ordering\"\nratings ORDER BY userId, rating DESC;`\nResource management\n:::caution\nOrdering data requires holding it in RAM. For large operations, we suggest you\ncheck you have sufficient memory to perform the operation.",
    "tag": "questdb"
  },
  {
    "title": "Syntax",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/sql/alter-table-set-type.md",
    "content": "\ntitle: ALTER TABLE SET TYPE\nsidebar_label: SET TYPE\ndescription: ALTER TABLE SET TYPE SQL keyword reference documentation.\n\nConverts a non-WAL table to WAL, or a WAL table to non-WAL.\nSyntax\n\n\nDescription\nThe command schedules the conversion of the specified table to WAL or non-WAL type.\nThe actual conversion takes place on the next restart of the server.\nIf the command issued more than once before the restart, the last command overrides all previous ones.\nIf the target type of the conversion is the same as the current type of the table, the conversion request is ignored.\nExamples\nTo convert a non-WAL table to WAL:\n`ALTER TABLE weather SET TYPE WAL;\nrestart instance`\nTo convert a WAL table to non-WAL:\n```\nALTER TABLE weather SET TYPE BYPASS WAL;\nrestart instance",
    "tag": "questdb"
  },
  {
    "title": "Syntax",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/sql/reindex.md",
    "content": "\ntitle: REINDEX\nsidebar_label: REINDEX\ndescription: REINDEX SQL keyword reference documentation.\n\nRebuilds one or more index columns of the given table.\nThis operation is intended to be used after a hardware or software crash, when the index data are corrupted and the table cannot be opened for writes.\nThe operation can only be performed when there is no other reader and writer working on the table. During the operation, the table is locked and no read and write should be performed on the selected table.\n:::info\nFor more information about indexes please refer to the\nINDEX documentation\n:::\nSyntax\n\nOptions\nBy default, `REINDEX` rebuilds all indexes in the selected table. The following options can be used to narrow down the scope of the operation:\n\n`COLUMN`: When defined, `REINDEX` rebuilds the index for the selected column. \n`PARTITION`: When defined, `REINDEX` rebuilds index files in the selected partition only. The partition name must match the name of the directory for the given partition. The naming convention is detailed in Partitions.\n\nExample\nRebuilding all the indexes in the table `trades`:\n`questdb-sql title=\"Rebuilding an index\"\nREINDEX TABLE trades LOCK EXCLUSIVE;`\nRebuilding the index in the column `instruments`:\n`questdb-sql title=\"Rebuilding an index\"\nREINDEX TABLE trades COLUMN instruments LOCK EXCLUSIVE;`\nRebuilding one partition (`2021-12-17`) of the index in the column `instruments`:\n```questdb-sql title=\"Rebuilding an index\"\nREINDEX TABLE trades COLUMN instruments PARTITION '2021-12-17' LOCK EXCLUSIVE;",
    "tag": "questdb"
  },
  {
    "title": "Syntax",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/sql/alter-table-drop-partition.md",
    "content": "\ntitle: ALTER TABLE DROP PARTITION\nsidebar_label: DROP PARTITION\ndescription: DROP PARTITION SQL keyword reference documentation.\n\nDrops one or more partitions from an existing table.\nSimilar to dropping columns, dropping of partitions is a non-blocking and\nnon-waiting operation. While atomic for single partitions, dropping multiple\npartitions is in itself non-atomic. The operation will exit on the first failure\nand will not continue through a list of partitions if one fails to be dropped.\n:::caution\nUse `DROP PARTITION` with care, as QuestDB cannot recover data from dropped\npartitions!\n:::\nSyntax\n\n\nDrop partition by name\nThe partition name must match the name of the directory for the given partition.\nThe naming convention is detailed in Partitions.\nExamples\n`questdb-sql title=\"Drop a single partition\"\n--DAY\nALTER TABLE measurements DROP PARTITION LIST '2019-05-18';\n--MONTH\nALTER TABLE measurements DROP PARTITION LIST '2019-05';\n--YEAR\nALTER TABLE measurements DROP PARTITION LIST '2019';`\n`questdb-sql title=\"Drop multiple partitions\"\nALTER TABLE measurements DROP PARTITION LIST '2018','2019';`\nDrop partitions using boolean expression\nDrops partitions based on a boolean expression on the designated timestamp\ncolumn.\nExamples\n`questdb-sql title=\"Drop one partition\"\nALTER TABLE measurements\nDROP PARTITION\nWHERE timestamp = to_timestamp('2019-01-01:00:00:00', 'yyyy-MM-dd:HH:mm:ss');`\n```questdb-sql title=\"Drop all partitions older than 2018\"\nALTER TABLE measurements\nDROP PARTITION\nWHERE timestamp < to_timestamp('2018-01-01:00:00:00', 'yyyy-MM-dd:HH:mm:ss');",
    "tag": "questdb"
  },
  {
    "title": "Syntax",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/sql/alter-table-set-param.md",
    "content": "\ntitle: ALTER TABLE SET PARAM\nsidebar_label: SET PARAM\ndescription: SET PARAM SQL keyword reference documentation.\n\n`ALTER TABLE SET PARAM` sets table parameters via SQL.\n:::note\n\nChecking table metadata can be done via the `tables()` and `table_columns()`\n  functions, as described in the meta functions\n  documentation page.\n\n:::\nSyntax\n\n\n`maxUncommittedRows` - defines the maximum number of uncommitted rows per-table\nto keep in memory before triggering a commit for a specific table.\nThe purpose of specifying maximum uncommitted rows per table is to reduce the\noccurrences of resource-intensive commits when ingesting out-of-order data.\nThe global setting for the same parameter is `cairo.max.uncommitted.rows`.\nExample\nThe values for `maximum uncommitted rows` can be changed per each table with the\nfollowing SQL:\n`questdb-sql title=\"Altering out-of-order parameters via SQL\"\nALTER TABLE my_table SET PARAM maxUncommittedRows = 10000`\nChecking the values per-table may be done using the `tables()` function:\n`questdb-sql title=\"List table metadata\"\nSELECT id, name, maxUncommittedRows FROM tables();`\n| id  | name     | maxUncommittedRows |\n| --- | -------- | ------------------ |\n| 1   | my_table | 10000              |\nFor more details on retrieving table and column information, see the\nmeta functions documentation.\nParameters for QuestDB 6.5.5 and earlier versions\n:::note\nDeprecated content\n\n\nFor QuestDB 6.5.5 and earlier versions, the following keywords are useful for\n  configuring out-of-order ILP data ingestion on a per-table basis. For more\n  information on more details and when to apply them, see the documentation for\n  out-of-order data commits and\n  ILP commit strategy.\n\n\nFrom QuestDB 6.6 onwards, the database adjusts relevant settings automatically\n  and provides maximum ingestion speed.\n\n\n:::\nSyntax\n\n\nFor context on commit lag, see the guide for\nconfiguring commit lag of out-of-order data\nand ILP commit strategy.\n`commitLag` allows for specifying the expected maximum lag of late-arriving\nrecords when ingesting out-of-order data. The purpose of specifying a commit lag\nper table is to reduce the occurrences of resource-intensive commits when\ningesting out-of-order data. Incoming records will be kept in memory until for\nthe duration specified in lag, then all records up to the boundary will be\nordered and committed.\n`commitLag` expects a value with a modifier to specify the unit of time for the\nvalue:\n| unit | description  |\n| ---- | ------------ |\n| us   | microseconds |\n| s    | seconds      |\n| m    | minutes      |\n| h    | hours        |\n| d    | days         |\nTo specify `commitLag` value to 20 seconds:\n```questdb-sql\nALTER TABLE my_table SET PARAM commitLag = 20s",
    "tag": "questdb"
  },
  {
    "title": "Overview",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/sql/union-except-intersect.md",
    "content": "\ntitle: UNION EXCEPT INTERSECT keywords\nsidebar_label: UNION EXCEPT INTERSECT\ndescription: UNION, EXCEPT, and INTERSECT  SQL keyword reference documentation.\n\nOverview\n`UNION`, `EXCEPT`, and `INTERSECT` perform set operations.\n`UNION` is used to combine the results of two or more queries.\n`EXCEPT` and `INTERSECT` return distinct rows by comparing the results of two queries.\nTo work properly, all of the following must be true:\n\nEach query statement should return the same number of column.\nEach column to be combined should have data types that are either the same, or supported by `implicit cast`. See CAST for more information.\nColumns in each query statement should be in the same order.\n\nSyntax\nUNION\n\n\n`UNION` returns distinct results.\n`UNION ALL` returns all results including duplicates.\n`EXCEPT` returns distinct rows from the left input query that are not returned by\n  the right input query.\n`INTERSECT` returns rows that are returned by both input queries.\n\nExamples\nThe examples for the set operations use the following tables:\nsensor_1:\n| ID  | make              | city          |\n| --- | ----------------- | ------------- |\n| 1   | Honeywell         | New York      |\n| 2   | United Automation | Miami         |\n| 3   | Omron             | Miami         |\n| 4   | Honeywell         | San Francisco |\n| 5   | Omron             | Boston        |\n| 6   | RS Pro            | Boston        |\n| 1   | Honeywell         | New York      |\nsensor_2:\n| ID  | make              | city          |\n| --- | ----------------- | ------------- |\n| 1   | Honeywell         | San Francisco |\n| 2   | United Automation | Boston        |\n| 3   | Eberle            | New York      |\n| 4   | Honeywell         | Boston        |\n| 5   | Omron             | Boston        |\n| 6   | RS Pro            | Boston        |\nUNION\n`questdb-sql\nsensor_1 UNION sensor_2`\nreturns\n| ID  | make              | city          |\n| --- | ----------------- | ------------- |\n| 1   | Honeywell         | New York      |\n| 2   | United Automation | Miami         |\n| 3   | Omron             | Miami         |\n| 4   | Honeywell         | San Francisco |\n| 5   | Omron             | Boston        |\n| 6   | RS Pro            | Boston        |\n| 1   | Honeywell         | San Francisco |\n| 2   | United Automation | Boston        |\n| 3   | Eberle            | New York      |\n| 4   | Honeywell         | Boston        |\n`UNION` eliminates duplication even when one of the queries returns nothing.\nFor instance:\n`questdb-sql\nsensor_1\nUNION\nsensor_2 WHERE ID>10;`\nreturns:\n| ID  | make              | city          |\n| --- | ----------------- | ------------- |\n| 1   | Honeywell         | New York      |\n| 2   | United Automation | Miami         |\n| 3   | Omron             | Miami         |\n| 4   | Honeywell         | San Francisco |\n| 5   | Omron             | Boston        |\n| 6   | RS Pro            | Boston        |\nThe duplicate row in `sensor_1` is not returned as a result.\n`questdb-sql\nsensor_1 UNION ALL sensor_2`\nreturns\n| ID  | make              | city          |\n| --- | ----------------- | ------------- |\n| 1   | Honeywell         | New York      |\n| 2   | United Automation | Miami         |\n| 3   | Omron             | Miami         |\n| 4   | Honeywell         | San Francisco |\n| 5   | Omron             | Boston        |\n| 6   | RS Pro            | Boston        |\n| 1   | Honeywell         | San Francisco |\n| 2   | United Automation | Boston        |\n| 3   | Eberle            | New York      |\n| 4   | Honeywell         | Boston        |\n| 5   | Omron             | Boston        |\n| 6   | RS Pro            | Boston        |\nEXCEPT\n`questdb-sql\nsensor_1 EXCEPT sensor_2`\nreturns\n| ID  | make              | city          |\n| --- | ----------------- | ------------- |\n| 1   | Honeywell         | New York      |\n| 2   | United Automation | Miami         |\n| 3   | Omron             | Miami         |\n| 4   | Honeywell         | San Francisco |\nINTERSECT\n`questdb-sql\nsensor_1 INTERSECT sensor_2`\nreturns\n| ID  | make   | city   |\n| --- | ------ | ------ |\n| 5   | Omron  | Boston |\n| 6   | RS Pro | Boston |\nKeyword execution priority\nThe QuestDB's engine processes the keywords from left to right, unless the priority is defined by parenthesis.\nFor example:\n```questdb-sql\nquery_1 UNION query_2 EXCEPT query_3\n```\nis executed as:\n```questdb-sql\n(query_1 UNION query_2) EXCEPT query_3\n```\nSimilarly, the following syntax:\n```questdb-sql\nquery_1 UNION query_2 INTERSECT query_3\n```\nis executed as:\n```questdb-sql\n(query_1 UNION query_2) INTERSECT query_3\n```\nClauses\nThe set operations can be used with clauses such as `LIMIT`, `ORDER BY`, and `WHERE`. However, when the clause keywords are added after the set operations, the execution order for different clauses varies.\nFor `LIMIT` and `ORDER BY`, the clauses are applied after the set operations.\nFor example:\n```questdb-sql\nquery_1 UNION query_2\nLIMIT 3;\n```\nis executed as:\n```questdb-sql\n(query_1 UNION query_2)\nLIMIT 3;\n```\nFor `WHERE`, the clause is applied first to the query immediate prior to it.\n```questdb-sql\nquery_1 UNION query_2\nWHERE value = 1;\n```\nis executed as:\n```questdb-sql\nquery_1 UNION (query_2\nWHERE value = 1);\n```\n:::note\n\nQuestDB applies `GROUP BY` implicitly. See GROUP BY reference for more information.\nQuest does not support the clause `HAVING` yet.\n\n:::\nAlias\nWhen different aliases are used with set operations, the execution follows a left-right order and the output uses the first alias.\nFor example:\n```questdb-sql\nSELECT alias_1 FROM table_1\nUNION\nSELECT alias_2 FROM table_2;\n```\nThe output shows`alias_1`.",
    "tag": "questdb"
  },
  {
    "title": "Syntax",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/sql/group-by.md",
    "content": "\ntitle: GROUP BY keyword\nsidebar_label: GROUP BY\ndescription: GROUP BY SQL keyword reference documentation.\n\nGroups aggregation calculations by one or several keys. In QuestDB, this clause\nis optional.\nSyntax\n\n:::note\nQuestDB groups aggregation results implicitly and does not require the GROUP BY\nkeyword. It is only supported for convenience. Using the GROUP BY clause\nexplicitly will return the same results as if the clause was omitted.\n:::\nExamples\nThe below queries perform aggregations on a single key. Using `GROUP BY`\nexplicitly or implicitly yields the same results:\n`questdb-sql title=\"Single key aggregation, explicit GROUP BY\"\nSELECT sensorId, avg(temp)\nFROM readings\nGROUP BY sensorId;`\n`questdb-sql title=\"Single key aggregation, implicit GROUP BY\"\nSELECT sensorId, avg(temp)\nFROM readings;`\nThe below queries perform aggregations on multiple keys. Using `GROUP BY`\nexplicitly or implicitly yields the same results:\n`questdb-sql title=\"Multiple key aggregation, explicit GROUP BY\"\nSELECT sensorId, sensorType, avg(temp)\nFROM readings\nGROUP BY sensorId,sensorType;`\n`questdb-sql title=\"Multiple key aggregation, implicit GROUP BY\"\nSELECT sensorId, sensorType, avg(temp)\nFROM readings;`\nWhen used explicitly, the list of keys in the `GROUP BY` clause must match the\nlist of keys in the `SELECT` clause, otherwise an error will be returned:\n`questdb-sql title=\"Error - Column b is missing in the GROUP BY clause\"\nSELECT a, b, avg(temp)\nFROM tab\nGROUP BY a;`\n`questdb-sql title=\"Error - Column b is missing in the SELECT clause\"\nSELECT a, avg(temp)\nFROM tab\nGROUP BY a, b;`\n```questdb-sql title=\"Success - Columns match\"\nSELECT a, b, avg(temp)\nFROM tab\nGROUP BY a, b;",
    "tag": "questdb"
  },
  {
    "title": "fill.md",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/sql/fill.md",
    "content": "\ntitle: FILL keyword\nsidebar_label: FILL\ndescription: FILL SQL keyword reference documentation.\n\n:::info\nDocumentation for the `FILL` keyword can be found on the\nSAMPLE BY page.\n:::\nQueries using a SAMPLE BY aggregate on data\nwhich has missing records may return a discontinuous series of results. The\n`FILL` keyword allows for specifying a fill behavior for results which have\nmissing aggregates due to missing rows.\nTo specify a default handling for `null` values within queries, see the\ncoalesce() function",
    "tag": "questdb"
  },
  {
    "title": "Syntax",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/sql/create-table.md",
    "content": "\ntitle: CREATE TABLE reference\nsidebar_label: CREATE TABLE\ndescription: CREATE TABLE SQL keywords reference documentation.\n\nTo create a new table in the database, the `CREATE TABLE` keywords followed by\ncolumn definitions are used.\nSyntax\nTo create a table by manually entering parameters and settings:\n\n:::info\nChecking table metadata can be done via the `tables()` and `table_columns()`\nfunctions which are described in the\nmeta functions documentation page.\n:::\nTo create a table by cloning the metadata of an existing table:\n\nIF NOT EXISTS\nAn optional `IF NOT EXISTS` clause may be added directly after the\n`CREATE TABLE` keywords to indicate that a new table should be created if one\nwith the desired table name does not already exist.\n`questdb-sql\nCREATE TABLE IF NOT EXISTS test_table(price DOUBLE, ts TIMESTAMP) timestamp(ts);`\nTable name\nInternally the table name is used as a directory name on the file system. It can\ncontain both ASCII and Unicode characters. The table name must be unique and\nan error is returned if a table already exists with the requested name. Table\nnames containing spaces or period `.` character must be enclosed in double\nquotes, for example:\n`questdb-sql\nCREATE TABLE \"example out of.space\" (a INT);\nINSERT INTO \"example out of.space\" values (1);`\nColumn name\nAs with table names, the column name is used for file names internally. Although\nit does support both ASCII and Unicode characters, character restrictions\nspecific to the file system still apply. Tables may have up to 2,147,483,647\ncolumns.\n:::note\nColumn names must be unique within each table and must not contain a period\n`.` character.\n:::\nType definition\nWhen specifying a column, a name and\ntype definition must be provided. The `symbol`\ntype may have additional optional parameters applied.\n\nSymbols\nOptional keywords and parameters may follow the `symbol` type which allow for\nfurther optimization on the handling of this type. For more information on the\nbenefits of using this type, see the symbol overview.\nSymbol capacity\n`CAPACITY` is an optional keyword used when defining a symbol type on table\ncreation to indicate how many distinct values this column is expected to have.\nWhen `distinctValueEstimate` is not explicitly specified, a default value of\n`cairo.default.symbol.capacity` is used.\n`distinctValueEstimate` - the value used to size data structures for\nsymbols.\n`questdb-sql\nCREATE TABLE my_table(symb SYMBOL CAPACITY 128, price DOUBLE, ts TIMESTAMP),\n  INDEX (symb) timestamp(ts);`\nThe symbol capacity is not to be confused with index capacity described in\ncolumn indexes below.\n`questdb-sql\nCREATE TABLE my_table\n  (symb SYMBOL capacity 128 NOCACHE INDEX capacity 256, price DOUBLE, ts TIMESTAMP)\ntimestamp(ts);`\nSymbol caching\n`CACHE | NOCACHE` is used to specify whether a symbol should be cached. The\ndefault value is `CACHE` unless otherwise specified.\n`questdb-sql\nCREATE TABLE my_table\n  (symb SYMBOL CAPACITY 128 NOCACHE, price DOUBLE, ts TIMESTAMP)\ntimestamp(ts);`\nCasting types\n`castDef` - casts the type of a specific column. `columnRef` must reference\nexisting column in the `selectSql`\n\n`questdb-sql\nCREATE TABLE test AS (SELECT CAST(x as DOUBLE) x FROM long_sequence(10));`\nColumn indexes\nIndex definitions (`indexDef`) are used to create an\nindex for a table column. The referenced table column\nmust be of type symbol.\n\n`questdb-sql\nCREATE TABLE my_table(symb SYMBOL, price DOUBLE, ts TIMESTAMP),\n  INDEX (symb) TIMESTAMP(ts);`\nAn index capacity may be provided for the index by defining the index storage\nparameter, `valueBlockSize`:\n`questdb-sql\nCREATE TABLE my_table(symb SYMBOL, price DOUBLE, ts TIMESTAMP),\n  INDEX (symb CAPACITY 128) TIMESTAMP(ts);\n-- equivalent to\nCREATE TABLE my_table(symb SYMBOL INDEX CAPACITY 128, price DOUBLE, ts TIMESTAMP),\n  TIMESTAMP(ts);`\nSee Index for more information about\nindex capacity.\nCREATE TABLE AS\nWhen SQL (`selectSQL`) is `SELECT * FROM tab` or any arbitrary SQL result, the\nselected column names and their data type will be cloned to the new table.\n`questdb-sql title=\"Create table as select\"\nCREATE TABLE new_table AS(\n  SELECT\n    rnd_int() a,\n    rnd_double() b,\n    rnd_symbol('ABB', 'CDD') c\n  FROM\n    long_sequence(100)\n  WHERE false\n);`\nThe data type of a column can be changed:\n`questdb-sql title=\"Clone an existing wide table and change type of cherry-picked columns\"\nCREATE TABLE new_table AS (SELECT * FROM source_table WHERE false),\n  CAST(price AS LONG),\n  CAST(instrument as SYMBOL);`\nHere we changed type of `price` (assuming it was `INT`) to `LONG` and changed\ntype of `sym` to symbol and created an\nindex.\nDesignated timestamp\nThe timestamp function allows for specifying which column (which must be of\n`timestamp` type) should be a designated timestamp for the table. For more\ninformation, see the designated timestamp\nreference.\n:::caution\nThe designated timestamp column cannot be changed after the table has been\ncreated.\n:::\nPartitioning\n`PARTITION BY` allows for specifying the\npartitioning strategy for the table. Tables created\nvia SQL are not partitioned by default (`NONE`) and tables can be partitioned by one of\nthe following:\n\n`NONE`: the default when partition is not defined.\n`YEAR`\n`MONTH`\n`DAY`\n`HOUR`\n\n:::caution\nThe partitioning strategy cannot be changed after the table has been\ncreated.\n:::\nWAL table parameter\nIt is possible to create a WAL table, allowing concurrent data ingestion\nand modification through multiple interfaces:\n\n`WAL` creates a WAL table. When a WAL table is created, the table must has a partition that is not `NONE`.\n`BYPASS WAL` creates a non-WAL table.\nWhen neither option is specified,\n  the server configuration,\n  `cairo.wal.enabled.default`, is used:\n`true`: creates a WAL table.\n`false`: creates a non-WAL table.\n\nWITH table parameter\n\nThe parameter influences how often commits of out-of-order data occur. It may be\nset during table creation using the `WITH` keyword.\n`maxUncommittedRows` - defines the maximum number of uncommitted rows per-table\nto keep in memory before triggering a commit for a specific table.\nThe purpose of specifying maximum uncommitted rows per table is to reduce the\noccurrences of resource-intensive commits when ingesting out-of-order data.\nThe global setting for the same parameter is `cairo.max.uncommitted.rows`.\n`questdb-sql title=\"Setting out-of-order table parameters via SQL\"\nCREATE TABLE my_table (timestamp TIMESTAMP) TIMESTAMP(timestamp)\nPARTITION BY DAY WITH maxUncommittedRows=250000;`\nChecking the values per-table may be done using the `tables()` function:\n`questdb-sql title=\"List all tables\"\nSELECT id, name, maxUncommittedRows FROM tables();`\n| id  | name        | maxUncommittedRows |\n| :-- | :---------- | :----------------- |\n| 1   | my_table    | 250000             |\n| 2   | device_data | 10000              |\nTable target volume\nThe `IN VOLUME` clause is used to create a table in a different volume than the standard. The table\nis created in the specified target volume, and a symbolic link is created in the table's standard \nvolume to point to it.\n\nThe use of the comma (`,`) depends on the existence of the `WITH` clause:\n\nIf the `WITH` clause is present, a comma is mandatory before\n  `IN VOLUME`:\n\n`questdb-sql\n  CREATE TABLE my_table (i symbol, ts timestamp), index(i capacity 32) WITH maxUncommittedRows=7, IN VOLUME SECONDARY_VOLUME;`\n\nIf no `WITH` clause is used, the comma must not be added for the `IN VOLUME`\n  segment:\n\n`questdb-sql\n  CREATE TABLE my_table (i symbol, ts timestamp) IN VOLUME SECONDARY_VOLUME;`\nThe use of quotation marks (`'`) depends on the alias:\n\n\nIf the alias contains spaces, the quotation marks are required:\n`questdb-sql\nCREATE TABLE my_table (i symbol, ts timestamp), index(i capacity 32) IN VOLUME 'SECONDARY VOLUME';`\n- If the alias does not contain spaces, no quotation mark is necessary:\n`questdb-sql\nCREATE TABLE my_table (i symbol, ts timestamp), index(i capacity 32) IN VOLUME SECONDARY_VOLUME;`\n\n\nDescription\nThe table behaves the same way as if it had been created in the standard (default)\nvolume, with the exception that DROP TABLE \nremoves the symbolic link from the standard volume but the content pointed to is \nleft intact in its volume. A table using the same name in the same \nvolume cannot be created again as a result, it requires manual intervention \nto either remove or rename the table's directory in its volume.\nConfiguration\nThe secondary table target volume is defined by\n`cairo.volumes` in\nserver.conf. The default setting\ncontains an empty list, which means the feature is not enabled.\nTo enable the feature, define as many volume pairs as you need, with syntax \nalias -> volume-root-path, and separate different pairs with a comma. For example:\n`cairo.volumes=SECONDARY_VOLUME -> /Users/quest/mounts/secondary, BIN -> /var/bin`\nAdditional notes about defining the alias and volume root paths:\n\nAliases are case-insensitive.\nVolume root paths must be valid and exist at bootstrap time and at the time when the table is created.\nAliases and/or volume root paths can be single quoted, it is not required.\n\nQuestDB 6.5.5 and earlier versions\n\nFrom QuestDB 6.6 onwards,\nthe database adjusts relevant settings automatically and provides optimal\ningestion speed.\n\n`commitLag` - equivalent to `cairo.commit.lag` expects a value with a modifier\n  to specify the unit of time for the value:\n\n| unit | description  |\n  | ---- | ------------ |\n  | us   | microseconds |\n  | s    | seconds      |\n  | m    | minutes      |\n  | h    | hours        |\n  | d    | days         |\nFor more information on commit lag and the maximum uncommitted rows, see the\nguide for out-of-order commits and\nILP commit strategy.\nCREATE TABLE LIKE\nThe `LIKE` keyword clones the table schema of an existing table without copying\nthe data. Table settings and parameters such as designated timestamp, symbol\ncolumn indexes, and index capacity will be cloned, too.\n`questdb-sql title=\"Create table like\"\nCREATE TABLE new_table (LIKE my_table);`\nExamples\nThe following examples demonstrate creating tables from basic statements, and\nintroduce features such as partitioning and designated timestamps. For more\ninformation on the concepts introduced to below, see\n\ndesignated timestamp reference on\n  electing a timestamp column\npartition documentation which describes how\n  partitions work in QuestDB\nsymbol reference for using the `symbol` data type\n\nThis example will create a table without a designated timestamp and does not\nhave a partitioning strategy applied.\n`questdb-sql title=\"Basic example\"\nCREATE TABLE my_table(symb SYMBOL, price DOUBLE, ts TIMESTAMP, s STRING);`\nThe same table can be created and a designated timestamp may be specified.\n`questdb-sql title=\"Adding a designated timestamp\"\nCREATE TABLE my_table(symb SYMBOL, price DOUBLE, ts TIMESTAMP, s STRING)\n  TIMESTAMP(ts);`\n`questdb-sql title=\"Adding a partitioning strategy by DAY\"\nCREATE TABLE my_table(symb SYMBOL, price DOUBLE, ts TIMESTAMP, s STRING)\n  TIMESTAMP(ts)\nPARTITION BY DAY;`\n`questdb-sql title=\"Adding parameters for symbol type\"\nCREATE TABLE\n  my_table(symb SYMBOL CAPACITY 256 NOCACHE INDEX CAPACITY 1048576,\n  price DOUBLE, ts TIMESTAMP, s STRING)\n  TIMESTAMP(ts)\nPARTITION BY DAY;`\nLet's assume we imported a text file into the table `taxi_trips_unordered` and\nnow we want to turn this data into time series through ordering trips by\n`pickup_time`, assign dedicated timestamp and partition by month:\n```questdb-sql title=\"Create table as select with data manipulation\"\nCREATE TABLE taxi_trips AS(\n  SELECT * FROM taxi_trips_unordered ORDER BY pickup_time\n) TIMESTAMP(pickup_time)\nPARTITION BY MONTH;",
    "tag": "questdb"
  },
  {
    "title": "Syntax",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/sql/update.md",
    "content": "\ntitle: UPDATE keyword\nsidebar_label: UPDATE\ndescription: UPDATE SQL keyword reference documentation.\n\nUpdates data in a database table.\nSyntax\n\n:::note\n\nthe same `columnName` cannot be specified multiple times after the SET keyword\n  as it would be ambiguous\nthe designated timestamp column cannot be updated as it would lead to altering\n  history of the time series data\nIf the target partition is\n  attached by a symbolic link,\n  the partition is read-only. `UPDATE` operation on a read-only partition will\n  fail and generate an error. \n\n:::\nExamples\n`questdb-sql title=\"Update with constant\"\nUPDATE trades SET price = 125.34 WHERE symbol = 'AAPL';`\n`questdb-sql title=\"Update with function\"\nUPDATE book SET mid = (bid + ask)/2 WHERE symbol = 'AAPL';`\n`questdb-sql title=\"Update with join\"\nUPDATE spreads s SET s.spread = p.ask - p.bid FROM prices p WHERE s.symbol = p.symbol;`\n`questdb-sql title=\"Update with multiple joins\"\nUPDATE spreads s\nSET s.spread = p.ask - p.bid\nFROM prices p\nJOIN instruments i ON p.symbol = i.symbol\nWHERE s.timestamp = p.timestamp AND i.type = 'BOND';`\n```questdb-sql title=\"Update with a sub-query\"\nWITH up AS (\n    SELECT symbol, spread, ts\n    FROM temp_spreads\n    WHERE timestamp between '2022-01-02' and '2022-01-03'\n)\nUPDATE spreads s\nSET spread = up.spread\nFROM up\nWHERE up.ts = s.ts AND s.symbol = up.symbol;",
    "tag": "questdb"
  },
  {
    "title": "Syntax",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/sql/with.md",
    "content": "\ntitle: WITH keyword\nsidebar_label: WITH\ndescription: WITH SQL keyword reference documentation.\n\nSupports Common Table Expressions (CTEs), e.i., naming one or several\nsub-queries to be used with a SELECT,\nINSERT, or\nUPDATE query.\nUsing a CTE makes it easy to simplify large or complex statements which involve\nsub-queries, particularly when such sub-queries are used several times.\nSyntax\n\nWhere:\n\n`alias` is the name given to the sub-query for ease of reusing\n`subQuery` is a SQL query (e.g `SELECT * FROM table`)\n\nExamples\n`questdb-sql title=\"Single alias\"\nWITH first_10_users AS (SELECT * FROM users limit 10)\nSELECT user_name FROM first_10_users;`\n`questdb-sql title=\"Using recursively\"\nWITH first_10_users AS (SELECT * FROM users limit 10),\nfirst_5_users AS (SELECT * FROM first_10_users limit 5)\nSELECT user_name FROM first_5_users;`\n`questdb-sql title=\"Flag whether individual trips are longer or shorter than average\"\nWITH avg_distance AS (SELECT avg(trip_distance) average FROM trips)\nSELECT pickup_datetime, trips.trip_distance > avg_distance.average longer_than_average\nFROM trips CROSS JOIN avg_distance;`\n`questdb-sql title=\"Update with a sub-query\"\nWITH up AS (\n    SELECT symbol, spread, ts\n    FROM temp_spreads\n    WHERE timestamp between '2022-01-02' and '2022-01-03'\n)\nUPDATE spreads s\nSET spread = up.spread\nFROM up\nWHERE up.ts = s.ts AND s.symbol = up.symbol;`\n```questdb-sql title=\"Insert with a sub-query\"\nWITH up AS (\n    SELECT symbol, spread, ts\n    FROM temp_spreads\n    WHERE timestamp between '2022-01-02' and '2022-01-03'\n)\nINSERT INTO spreads\nSELECT * FROM up;",
    "tag": "questdb"
  },
  {
    "title": "Syntax",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/sql/backup.md",
    "content": "\ntitle: BACKUP keyword\nsidebar_label: BACKUP\ndescription: BACKUP SQL keyword reference documentation.\n\nCreates a backup for one, several, or all database tables.\nSyntax\n\nBackup directory\nBacking up a database or tables requires a backup directory which is set\nusing the `cairo.sql.backup.root`\nconfiguration key in a\nserver.conf file:\n`shell title=\"server.conf\"\ncairo.sql.backup.root=/Users/UserName/Desktop`\nThe backup directory can be on a disk local to the server, a remote disk or\na remote filesystem. QuestDB will enforce that the backup is only written in a\nlocation relative to the `backup directory`. This is a security feature to\ndisallow random file access by QuestDB.\nThe tables will be written in a directory with today's date with the default\nformat `yyyy-MM-dd` (e.g., `2020-04-20`). A custom date format can be specified\nusing the `cairo.sql.backup.dir.datetime.format`\nconfiguration key:\n`shell title=\"server.conf\"\ncairo.sql.backup.dir.datetime.format=yyyy-dd-MM`\nGiven a `BACKUP` query run on `2021-02-25`, the data and metadata files will be\nwritten following the\ndb directory structure\n`filestructure title=\"/path/to/backup_directory\"\n\u251c\u2500\u2500 2021-02-25\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 table1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ...\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 table2\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ...\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 table3\n\u2502\u00a0\u00a0 ...`\nIf a user performs several backups on the same date, each backup will be written\na new directory. Subsequent backups on the same date will look as follows:\n`filestructure title=\"/path/to/backup_directory\"\n\u251c\u2500\u2500 2021-02-22    'first'\n\u251c\u2500\u2500 2021-02-22.1  'second'\n\u251c\u2500\u2500 2021-02-22.2  'third'\n\u251c\u2500\u2500 2021-02-24    'first new date'\n\u251c\u2500\u2500 2021-02-24.1  'first new date'\n\u2502\u00a0\u00a0 ...`\nExamples\n`questdb-sql title=\"Single table\"\nBACKUP TABLE table1;`\n`questdb-sql title=\"Multiple tables\"\nBACKUP TABLE table1, table2, table3;`\n```questdb-sql title=\"All tables\"\nBACKUP DATABASE;",
    "tag": "questdb"
  },
  {
    "title": "Examples",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/sql/show.md",
    "content": "\ntitle: SHOW keyword\nsidebar_label: SHOW\ndescription: SHOW SQL keyword reference documentation.\n\nThis keyword provides column and table information including metadata such as\ncommit lag and max uncommitted row count.\nThe `SHOW` keyword is useful for checking if tables contain a\ndesignated timestamp column.\n:::info\nThese commands return the tables and columns as a table. If you would like to\nquery your tables and columns with filters or to use the results as part of a\nfunction, see table_columns()\nand tables() functions.\n:::\nExamples\nShow all tables:\n`questdb-sql\nSHOW TABLES;`\n| table    |\n| -------- |\n| weather  |\n| my_table |\n| ...      |\nShow all columns for table `my_table`\n`questdb-sql\nSHOW COLUMNS FROM my_table;`\n| column | type      | indexed | indexBlockCapacity | symbolCached | symbolCapacity | designated |\n| ------ | --------- | ------- | ------------------ | ------------ | -------------- | ---------- |\n| symb   | SYMBOL    | true    | 1048576            | false        | 256            | false      |\n| price  | DOUBLE    | false   | 0                  | false        | 0              | false      |\n| ts     | TIMESTAMP | false   | 0                  | false        | 0              | true       |",
    "tag": "questdb"
  },
  {
    "title": "Syntax",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/sql/cast.md",
    "content": "\ntitle: CAST keyword\nsidebar_label: CAST\ndescription: CAST SQL keyword reference documentation.\n\nType conversion. Can be either:\n\nExplicit via `cast()`\nImplicit, in which case it will be automatically\n  performed when required by the context.\n\nSyntax\n\nwhere:\n\n`expression` can be a constant, a column, or an expression that evaluates to a\n  value.\n`type` refers to the desired data type.\n\n`cast` can be used a part of arithmetic expression as normal\nExplicit conversion\nTypes can be converted from one to another using the `cast()` function.\nExamples\n`questdb-sql title=\"Queries\"\nSELECT\ncast(3L + 2L AS INT),\ncast(1578506142000000 AS TIMESTAMP),\ncast('10.2' AS DOUBLE),\ncast('\u884c' AS INT);`\n| cast | cast1                       | cast2 | cast3 |\n| ---- | --------------------------- | ----- | ----- |\n| 5    | 2020-01-08T17:55:42.000000Z | 10.2  | 34892 |\nExplicit casting of an expression to a smaller\ndata type may result in loss of data when the\noutput data type is smaller than the expression.\n\nCasting a decimal number type (`float` or `double`) to an integer number type\n  (`long`, `int`, `short`) will result in decimals drop.\nIf the integer part being cast is larger than the resulting data type, it will\n  be resized by truncating bits.\nConversions from `char` to a number type will return the corresponding\n  `unicode` number and vice versa.\n\nPrecision loss examples\n`questdb-sql title=\"Queries\"\nSELECT\ncast(3.5 + 2 AS INT),\ncast(7234623 AS SHORT),\ncast(2334444.323 AS SHORT);`\n| cast | cast1 | cast2  |\n| ---- | ----- | ------ |\n| 5    | 25663 | -24852 |\nWhen casting numbers into a smaller data type, QuestDB will truncate the higher\nbits of this number.\nImplicit conversion\nType casting may be necessary in certain context such as\n\nOperations involving various different types\nInserting values where the originating type is different from the destination\n  column type.\n\nQuestDB will attempt to convert to the data type required by the context. This\nis called `implicit cast` and does not require using the `cast()` function.\n:::note\nQuestDB will only perform implicit cast when they would not result in data being\ntruncated or precision being lost.\n:::\nThe below chart illustrates the explicit and implicit cast available in QuestDB.\n\n:::note\nImplicit casting prevents data loss. When an operation involves multiple types,\nthe resulting type will be the smallest possible type so that no data is lost.\n:::\n`questdb-sql title=\"Queries\"\nSELECT\n1234L + 567,\n1234L + 0.567,\nto_timestamp('2019-10-17T00:00:00', 'yyyy-MM-ddTHH:mm:ss') + 323,\nto_timestamp('2019-10-17T00:00:00', 'yyyy-MM-ddTHH:mm:ss') + 0.323;`\n| column | column1  | column2                     | column3          |\n| ------ | -------- | --------------------------- | ---------------- |",
    "tag": "questdb"
  },
  {
    "title": "Syntax",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/sql/distinct.md",
    "content": "\ntitle: DISTINCT keyword\nsidebar_label: DISTINCT\ndescription: DISTINCT SQL keyword reference documentation.\n\n`SELECT DISTINCT` is used to return only distinct (i.e different) values from a\ncolumn as part of a SELECT statement.\nSyntax\n\nExamples\nThe following query will return a list of all unique ratings in the table.\n`questdb-sql title=\"Simple query\"\nSELECT DISTINCT movieId\nFROM ratings;`\nSELECT DISTINCT can be used in conjunction with more advanced queries and\nfilters.\n`questdb-sql title=\"With aggregate\"\nSELECT DISTINCT movieId, count()\nFROM ratings;`\n```questdb-sql title=\"With filter\"\nSELECT DISTINCT movieId, count()\nFROM ratings\nWHERE score > 3;",
    "tag": "questdb"
  },
  {
    "title": "Syntax",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/sql/copy.md",
    "content": "\ntitle: COPY keyword\nsidebar_label: COPY\ndescription: COPY SQL keyword reference documentation.\n\n:::caution\nFor partitioned tables, the best `COPY` performance can be achieved only on a\nmachine with a local, physically attached SSD. It is possible to use a network\nblock storage, such as an AWS EBS volume to perform the operation, with the\nfollowing impact:\n\nUsers need to configure the maximum IOPS and throughput setting values for the\n  volume.\nThe required import time is likely to be 5-10x longer.\n\n:::\nSyntax\n\nDescription\nCopies tables from a delimited text file saved in the defined root directory\ninto QuestDB. `COPY` has the following import modes:\n\n\nParallel import, used for copying partitioned tables:\n\n\nThe parallel level is based on partition granularity. It is important to\n    choose the timestamp column and partition type correctly for the data to be\n    imported. The higher the granularity of the partitions, the faster an import\n    operation can be completed.\n\nIf the target table exists and is partitioned, the target table must be\n    empty.\nIf the target table does not exist, both `TIMESTAMP` and `PARTITION BY`\n    options must be defined to create a partitioned table. The `PARTITION BY`\n    value should not be `NONE`.\n\nWhen table does exist and is not empty, import is not supported.\n\n\nSerial import, used for copying non-partitioned tables:\n\n\nIf the target table exists and is not partitioned, the data is appended\n    provided the file structure matches the table.\n\nIf the target table does not exist, then it is created using metadata\n    derived from the file data.\n\n:::note\n`COPY` takes up all the available resources. While one import is running, new\nrequest(s) will be rejected.\n:::\n`COPY '<id>' CANCEL` cancels the copying operation defined by the import `id`,\nwhile an import is taking place.\nRoot directory\n`COPY` requires a defined root directory where CSV files are saved and copied\nfrom. A CSV file must be saved to the root directory before starting the `COPY`\noperation. There are two root directories to be defined:\n\n`cairo.sql.copy.root` is used for storing regular files to be imported.\n`cairo.sql.copy.work.root` is used for storing temporary files like indexes or\n  temporary partitions. Unless otherwise specified, it points to the\n  `root_directory/tmp` directory.\n\nUse the configuration keys to edit these\nproperties in\nCOPY configuration settings:\n`shell title=\"Example\"\ncairo.sql.copy.root=/Users/UserName/Desktop`\n`cairo.sql.copy.root` and `cairo.sql.copy.work.root` can be on a local disk to\nthe server, on a remote disk, or a remote filesystem. QuestDB enforces that the\ntables are only written from files located in a directory relative to the\ndirectories. This is a security feature preventing random file access by\nQuestDB.\n:::note\nFor Mac OS users, using a directory under `/Users` may prevent import due to\npermission problem. It is preferable to save the CSV file in a folder outside of\nthe `/Users` tree and set the root directory accordingly.\n:::\nLog table\n`COPY` generates a log table,`sys.text_import_log`, tracking `COPY` operation\nfor the last three days with the following information:\n| Column name   | Data type | Notes                                                                         |\n| ------------- | --------- | ----------------------------------------------------------------------------- |\n| ts            | timestamp | The log event timestamp                                                       |\n| id            | string    | Import id                                                                     |\n| table         | symbol    | Destination table name                                                        |\n| file          | symbol    | The source csv file                                                           |\n| phase         | symbol    | Import phase.* Available only in intermediate log records of parallel import |\n| status        | symbol    | The event status: started, finished, failed, cancelled                        |\n| message       | string    | The error message for when status is failed                                   |\n| rows_handled  | long      | The counters for the total number of scanned lines in the file                |\n|               |           | The counters are shown in the final log row for the given import              |\n| rows_imported | long      | The counters for the total number of imported rows                            |\n|               |           | The counters are shown in the final log row for the given import              |\n| errors        | long      | The number of errors for the given phase                                      |\n* Available phases for parallel import are:\n\nsetup\nboundary_check\nindexing\npartition_import\nsymbol_table_merge\nupdate_symbol_keys\nbuild_symbol_index\nmove_partitions\nattach_partitions\nanalyze_file_structure\ncleanup\n\nLog table row retention is configurable through\n`cairo.sql.copy.log.retention.days` setting, and is three days by default.\n`COPY` returns `id` value from `sys.text_import_log` to track the import\nprogress.\nOptions\n\n`HEADER true/false`: When `true`, QuestDB automatically assumes the first row\n  is a header. Otherwise, schema recognition is used to determine whether the\n  first row is used as header. The default setting is `false`.\n`TIMESTAMP`: Define the name of the timestamp column in the file to be\n  imported.\n`FORMAT`: Timestamp column format when the format is not the default\n  (`yyyy-MM-ddTHH:mm:ss.SSSUUUZ`) or cannot be detected. See\n  Date and Timestamp format\n  for more information.\n`DELIMITER`: Default setting is `,`.\n`PARTITION BY`: Partition unit.\n`ON ERROR`: Define responses to data parsing errors. The valid values are:\n`SKIP_ROW`: Skip the entire row\n`SKIP_COLUMN`: Skip column and use the default value (`null` for nullable\n    types, `false` for boolean, `0` for other non-nullable types)\n`ABORT`: Abort whole import on first error, and restore the pre-import table\n    status\n\nExamples\nFor more details on parallel import, please also see\nImporting data in bulk via CSV.\n`questdb-sql title=\"COPY\"\nCOPY weather FROM 'weather.csv' WITH HEADER true FORMAT 'yyyy-MM-ddTHH:mm:ss.SSSUUUZ' ON ERROR SKIP_ROW;`\nStarts an import asynchronously and returns an import id string:\n| id               |\n| ---------------- |\n| 55ca24e5ba328050 |\nThe log can be accessed by querying:\n`questdb-sql\nSELECT * FROM 'sys.text_import_log' WHERE id = '55ca24e5ba328050';`\nA sample log table:\n| ts                          | id               | table   | file        | phase | status  | message | rows_handled | rows_imported | errors |\n| --------------------------- | ---------------- | ------- | ----------- | ----- | ------- | ------- | ------------ | ------------- | ------ |\n| 2022-08-03T10:40:25.586455Z | 55ca24e5ba328050 | weather | weather.csv |       | started |         |              |               | 0      |\n|                             |                  |         |             |       |         |         |              |               |        |\nWhile it is running, import can be cancelled with:\n`questdb-sql\nCOPY '55ca24e5ba328050' CANCEL;`\nWithin a few seconds import should stop and message with 'cancelled' status\nshould appear in text_import_log, e.g.:\n`questdb-sql\nSELECT * FROM 'sys.text_import_log' WHERE id = '55ca24e5ba328050' LIMIT -1;`\n| ts                          | id               | table   | file        | phase | status    | message                                                    | rows_handled | rows_imported | errors |\n| :-------------------------- | ---------------- | ------- | ----------- | ----- | --------- | ---------------------------------------------------------- | ------------ | ------------- | ------ |",
    "tag": "questdb"
  },
  {
    "title": "Syntax",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/sql/alter-table-resume-wal.md",
    "content": "\ntitle: ALTER TABLE RESUME WAL\nsidebar_label: RESUME WAL\ndescription: ALTER TABLE RESUME WAL SQL keyword reference documentation.\n\nRestarts transactions of a WAL table after\nrecovery from errors.\nSyntax\n\n\nDescription\n`sequencerTxn` is the unique `txn` identification that the Sequencer issues to\ntransactions.\nWhen `sequencerTxn` is not specified, the operation resumes the WAL apply job\nfrom the next uncommitted transaction, including the failed one.\nWhen `sequencerTxn` is not specified, the operation resumes the WAL apply job\nfrom the provided `sequencerTxn` number explicitly.\n`ALTER TABLE RESUME WAL` is used to restart WAL table transactions after\nresolving errors. When transactions are stopped, the `suspended` status from the\nwal_tables() function is marked as\n`true`, and the `sequencerTxn` value indicates the last successful commit in the\nSequencer. Once the error is resolved, `ALTER TABLE RESUME WAL` restarts the\nsuspended WAL transactions from the failed transaction. Alternatively, an\noptional `sequencerTxn` value can be provided to skip the failed transaction.\nExamples\nUsing the wal_tables() function to\ninvestigate the table status:\n`questdb-sql title=\"List all tables\"\nwal_tables();`\n| name        | suspended | writerTxn | sequencerTxn |\n| ----------- |-----------|-----------|--------------|\n| sensor_wal  | false     | 6         | 6            |\n| weather_wal | true      | 3         | 5            |\nThe table `weather_wal` is suspended. The last successful commit in the\ntable is `3`.\nThe following query restarts transactions from the failed transaction, `4`:\n```questdb-sql\nALTER TABLE  weather_wal RESUME WAL;\n```\nAlternatively, specifying the `sequencerTxn` to skip the failed commit (`4` in\nthis case):\n```questdb-sql\nALTER TABLE  weather_wal RESUME WAL TRANSACTION 5;\n-- This is equivalent to\nALTER TABLE  weather_wal RESUME WAL TXN 5;",
    "tag": "questdb"
  },
  {
    "title": "Syntax",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/sql/join.md",
    "content": "\ntitle: JOIN keyword\nsidebar_label: JOIN\ndescription: JOIN SQL keyword reference documentation.\n\nQuestDB supports the type of joins you can frequently find in relational\ndatabases: `INNER`, `LEFT (OUTER)`, `CROSS`. Additionally, it implements joins\nwhich are particularly useful for time-series analytics: `ASOF`, `LT`, and\n`SPLICE`. `FULL` joins are not yet implemented and are on our roadmap.\nAll supported join types can be combined in a single SQL statement; QuestDB\nSQL's optimizer determines the best execution order and algorithms.\nThere are no known limitations on the size of tables or sub-queries used in\njoins and there are no limitations on the number of joins, either.\nSyntax\nHigh-level overview:\n\n\n`selectClause` - see SELECT for more\n  information.\n`whereClause` - see WHERE for more information.\n\nThe specific syntax for `joinClause` depends on the type of `JOIN`:\n\n\n`INNER` and `LEFT` `JOIN` allow arbitrary `JOIN` predicates, `operator`, in\n    the mandatory `ON` clause:\n\n\n\n\n`ASOF`, `LT`, and `SPLICE` `JOIN` only allow `=` as the `JOIN` predicate in\n    the optional `ON` clause:\n\n\n\n`CROSS JOIN` does not allow the `ON` clause:\n\n\nColumns from joined tables are combined in a single row. Columns with the same\nname originating from different tables will be automatically aliased to create a\nunique column namespace of the resulting set.\nThough it is usually preferable to explicitly specify join conditions, QuestDB\nwill analyze `WHERE` clauses for implicit join conditions and will derive\ntransient join conditions where necessary.\n:::tip\nWhen tables are joined on a column that has the same name in both tables you can\nuse the `ON (column)` shorthand.\n:::\nExecution order\nJoin operations are performed in order of their appearance in a SQL query. The\nfollowing query performs a join on a table with one million rows based on a\ncolumn from a smaller table with one hundred rows:\n`questdb-sql\nSELECT * FROM 1_million_rows\nINNER JOIN 1_hundred_rows\nON 1_million_rows.customer_id = 1_hundred_rows.referral_id;`\nThe performance of this query can be improved by rewriting the query as follows:\n`questdb-sql\nSELECT * FROM 1_hundred_rows\nINNER JOIN 1_million_rows\nON 1_million_rows.referral_id = 1_hundred_rows.customer_id;`\nImplicit joins\nIt is possible to join two tables using the following syntax:\n`questdb-sql\nSELECT *\nFROM a, b\nWHERE a.id = b.id;`\nThe type of join as well as the column are inferred from the `WHERE` clause, and\nmay be either an `INNER` or `CROSS` join. For the example above, the equivalent\nexplicit statement would be:\n`questdb-sql\nSELECT *\nFROM a\nJOIN b ON (id);`\n(INNER) JOIN\n`(INNER) JOIN` returns rows from two tables where the records on the compared\ncolumn have matching values in both tables. `JOIN` is interpreted as\n`INNER JOIN` by default, making the `INNER` keyword implicit.\nThe following query returns the `movieId` and the average rating from table\n`ratings`. It also adds a column for the `title` from the table `movies`. The\ncorresponding title will be identified based on the `movieId` in the `ratings`\ntable matching an `id` in the `movies` table.\n```questdb-sql title=\"INNER JOIN ON\"\nSELECT movieId a, title, avg(rating)\nFROM ratings\nINNER JOIN (SELECT movieId id, title FROM movies)\nON ratings.movieId = id;\n-- Omitting 'INNER' makes the query equivalent:\nSELECT movieId a, title, avg(rating)\nFROM ratings\nJOIN (SELECT movieId id, title FROM movies)\nON ratings.movieId = id;\n```\nLEFT (OUTER) JOIN\n`LEFT OUTER JOIN` or simply `LEFT JOIN` returns all records from the left\ntable, and if matched, the records of the right table. When there is no match\nfor the right table, it returns `NULL` values in right table fields.\nThe general syntax is as follows:\n```questdb-sql title=\"LEFT JOIN ON\"\nSELECT tab1.colA, tab2.colB\nFROM table1 tab1\nLEFT OUTER JOIN table2 tab2\nON tab1.colA = tab2.colB;\n-- Omitting 'OUTER' makes the query equivalent:\nSELECT tab1.colA, tab2.colB\nFROM table1 tab1\nLEFT JOIN table2 tab2\nON tab1.colA = tab2.colB;\n```\nA `LEFT OUTER JOIN` query can also be used to select all rows in the left table\nthat do not exist in the right table.\n`questdb-sql\nSELECT tab1.colA, tab2.colB\nFROM table1 tab1\nLEFT OUTER JOIN table2 tab2\nON tab1.colA = tab2.colB\nWHERE tab2.colB = NULL;`\nCROSS JOIN\n`CROSS JOIN` returns the Cartesian product of the two tables being joined and\ncan be used to create a table with all possible combinations of columns. The\nfollowing query returns all possible combinations of `starters` and `deserts`:\n`questdb-sql\nSELECT *\nFROM starters\nCROSS JOIN deserts;`\n:::note\n`CROSS JOIN` does not have an `ON` clause.\n:::\nASOF JOIN\n`ASOF JOIN` joins two different time-series measured. For each row in the first\ntime-series, the `ASOF JOIN` takes from the second time-series a timestamp that\nmeets both of the following criteria:\n\nThe timestamp is the closest to the first timestamp.\nThe timestamp is strictly prior or equal to the first timestamp.\n\nExample\nGiven the following tables:\nTable `bids` (the left table):\n\n\n\n| ts                          | bid |\n| --------------------------- | --- |\n| 2019-10-17T00:00:00.000000Z | 100 |\n| 2019-10-17T00:00:00.100000Z | 101 |\n| 2019-10-17T00:00:00.300000Z | 102 |\n| 2019-10-17T00:00:00.500000Z | 103 |\n| 2019-10-17T00:00:00.600000Z | 104 |\n\n\nThe `asks` table (the right table):\n\n\n\n| ts                          | ask |\n| --------------------------- | --- |\n| 2019-10-17T00:00:00.100000Z | 100 |\n| 2019-10-17T00:00:00.300000Z | 101 |\n| 2019-10-17T00:00:00.400000Z | 102 |\n\n\nAn `ASOF JOIN` query can look like the following:\n`questdb-sql\nSELECT bids.ts timebid, asks.ts timeask, bid, ask\nFROM bids\nASOF JOIN asks;`\nThis is the JOIN result:\n\n\n| timebid                     | timeask                     | bid | ask  |\n| --------------------------- | --------------------------- | --- | ---- |\n| 2019-10-17T00:00:00.000000Z | NULL                        | 101 | NULL |\n| 2019-10-17T00:00:00.100000Z | 2019-10-17T00:00:00.100000Z | 101 | 100  |\n| 2019-10-17T00:00:00.300000Z | 2019-10-17T00:00:00.300000Z | 102 | 101  |\n| 2019-10-17T00:00:00.500000Z | 2019-10-17T00:00:00.400000Z | 103 | 102  |\n| 2019-10-17T00:00:00.600000Z | 2019-10-17T00:00:00.400000Z | 104 | 102  |\n\n\nThe result has all rows from the `bids` table joined with rows from the `asks`\ntable. For each timestamp from the `bids` table, the query looks for a timestamp that\nis equal or prior to it from the `asks` table. If no matching timestamp is\nfound, NULL is inserted.\nUsing `ON` for matching column value\nAn additional `ON` clause can be used to join the tables based on the value of a\nselected column.\nThe query above does not use the optional `ON` clause. If both tables store data\nfor multiple stocks, `ON` clause provides a way to find asks for bids with\nmatching stock value.\nTable `bids` (the left table):\n| ts                          | bid | stock |\n| --------------------------- | --- | :---- |\n| 2019-10-17T00:00:00.000000Z | 500 | AAPL  |\n| 2019-10-17T00:00:00.100000Z | 101 | GOOG  |\n| 2019-10-17T00:00:00.200000Z | 102 | GOOG  |\n| 2019-10-17T00:00:00.300000Z | 501 | AAPL  |\n| 2019-10-17T00:00:00.500000Z | 103 | GOOG  |\n| 2019-10-17T00:00:00.600000Z | 502 | AAPL  |\n| 2019-10-17T00:00:00.600000Z | 200 | IBM   |\nTable `asks` (the right table):\n| ts                          | ask | stock |\n| --------------------------- | --- | :---- |\n| 2019-10-17T00:00:00.000000Z | 500 | AAPL  |\n| 2019-10-17T00:00:00.100000Z | 501 | AAPL  |\n| 2019-10-17T00:00:00.100000Z | 100 | GOOG  |\n| 2019-10-17T00:00:00.400000Z | 502 | AAPL  |\n| 2019-10-17T00:00:00.700000Z | 200 | IBM   |\nNotice how both tables have a new column `stock` that stores the stock name. The\n`ON` clause allows you to match the value of the `stock` column in the `bids`\ntable with that in the `asks` table:\n`questdb-sql\nSELECT bids.stock stock, bids.ts timebid, asks.ts timeask, bid, ask\nFROM bids\nASOF JOIN asks ON (stock);`\nThe above query returns these results:\n| stock | timebid                     | timeask                     | bid | ask  |\n| :---- | --------------------------- | --------------------------- | --- | ---- |\n| AAPL  | 2019-10-17T00:00:00.000000Z | 2019-10-17T00:00:00.000000Z | 500 | 500  |\n| GOOG  | 2019-10-17T00:00:00.100000Z | 2019-10-17T00:00:00.100000Z | 101 | 100  |\n| GOOG  | 2019-10-17T00:00:00.200000Z | 2019-10-17T00:00:00.100000Z | 102 | 100  |\n| AAPL  | 2019-10-17T00:00:00.300000Z | 2019-10-17T00:00:00.100000Z | 501 | 501  |\n| GOOG  | 2019-10-17T00:00:00.500000Z | 2019-10-17T00:00:00.100000Z | 103 | 100  |\n| AAPL  | 2019-10-17T00:00:00.600000Z | 2019-10-17T00:00:00.400000Z | 502 | 502  |\n| IBM   | 2019-10-17T00:00:00.600000Z | NULL                        | 200 | NULL |\nThis query returns all rows from the `bids` table joined with records from the\n`asks` table that meet both the following criterion:\n\nThe `stock` column of the two tables has the same value\nThe timestamp of the `asks` record is prior to or equal to the timestamp of\n  the `bids` record.\n\nThe IBM record in the `bids` table is not joined with any record in the `asks`\ntable because there is no record in the `asks` table with the same stock name\nand a timestamp prior to or equal to the timestamp of the IBM record. The asks\ntable has a record with the IBM stock name but its timestamp is\n`2019-10-17T00:00:00.700000Z` which is after the timestamp of the IBM record in\nthe `bids` table and therefore not joined.\nTimestamp considerations\n`ASOF` join can be performed only on tables or result sets that are ordered by\ntime. When a table is created with a\ndesignated timestamp the order of records\nis enforced and the timestamp column name is in the table metadata. `ASOF` join\nuses this timestamp column from metadata.\nIn case tables do not have a designated timestamp column, but data is in\nchronological order, timestamp columns can be specified at runtime:\n`questdb-sql\nSELECT bids.ts timebid, bid, ask\nFROM (bids timestamp(ts))\nASOF JOIN (asks timestamp (ts));`\n:::caution\n`ASOF` join does not check timestamp order, if data is not in chronological\norder, the join result is non-deterministic.\n:::\nLT JOIN\nSimilar to `ASOF JOIN`, `LT JOIN` joins two different time-series measured. For\neach row in the first time-series, the `LT JOIN` takes from the second\ntime-series a timestamp that meets both of the following criteria:\n\nThe timestamp is the closest to the first timestamp.\nThe timestamp is strictly prior to the first timestamp.\n\nIn other words: `LT JOIN` won't join records with equal timestamps.\nExample\nConsider the following tables:\nTable `bids`:\n| ts                          | bid |\n| --------------------------- | --- |\n| 2019-10-17T00:00:00.000000Z | 101 |\n| 2019-10-17T00:00:00.300000Z | 102 |\n| 2019-10-17T00:00:00.500000Z | 103 |\nTable `asks`:\n| ts                          | ask |\n| --------------------------- | --- |\n| 2019-10-17T00:00:00.000000Z | 100 |\n| 2019-10-17T00:00:00.300000Z | 101 |\n| 2019-10-17T00:00:00.400000Z | 102 |\nAn `LT JOIN` can be built using the following query:\n`questdb-sql\nSELECT bids.ts timebid, asks.ts timeask, bid, ask\nFROM bids\nLT JOIN asks;`\nThe query above returns the following results:\n| timebid                     | timeask                     | bid | ask  |\n| --------------------------- | --------------------------- | --- | ---- |\n| 2019-10-17T00:00:00.000000Z | NULL                        | 101 | NULL |\n| 2019-10-17T00:00:00.300000Z | 2019-10-17T00:00:00.000000Z | 102 | 100  |\n| 2019-10-17T00:00:00.500000Z | 2019-10-17T00:00:00.400000Z | 103 | 102  |\nNotice how the first record in the `bids` table is not joined with any record in\nthe `asks` table. This is because there is no record in the `asks` table with a\ntimestamp prior to the timestamp of the first record in the `bids` table.\nSimilarly, the second record in the `bids` table is joined with the first record\nin the `asks` table because the timestamp of the first record in the `asks`\ntable is prior to the timestamp of the second record in the `bids` table.\n:::note\n`LT` join is often useful to join a table to itself in order to get preceding\nvalues for every row.\n:::\nSPLICE JOIN\n`SPLICE JOIN` is a full `ASOF JOIN`. It will return all the records from both\ntables. For each record from left table splice join will find prevailing record\nfrom right table and for each record from right table - prevailing record from\nleft table.\nConsidering the following tables:\nTable `asks`:\n| ts                          | ask |\n| --------------------------- | --- |\n| 2019-10-17T00:00:00.000000Z | 100 |\n| 2019-10-17T00:00:00.200000Z | 101 |\n| 2019-10-17T00:00:00.400000Z | 102 |\nTable `bids`:\n| ts                          | bid |\n| --------------------------- | --- |\n| 2019-10-17T00:00:00.100000Z | 101 |\n| 2019-10-17T00:00:00.300000Z | 102 |\n| 2019-10-17T00:00:00.500000Z | 103 |\nA `SPLICE JOIN` can be built as follows:\n`questdb-sql\nSELECT bids.ts timebid, bid, ask\nFROM bids\nSPLICE JOIN asks;`\nThis query returns the following results:\n| timebid                     | bid  | ask |\n| --------------------------- | ---- | --- |\n| null                        | null | 100 |\n| 2019-10-17T00:00:00.100000Z | 101  | 100 |\n| 2019-10-17T00:00:00.100000Z | 101  | 101 |\n| 2019-10-17T00:00:00.300000Z | 102  | 101 |\n| 2019-10-17T00:00:00.300000Z | 102  | 102 |\n| 2019-10-17T00:00:00.500000Z | 103  | 102 |\nNote that the above query does not use the optional `ON` clause. In case you\nneed additional filtering on the two tables, the `ON` clause can be used as\nfollows:\n```questdb-sql\nSELECT ts timebid, stock bidStock, bid, ask\nFROM bids\nSPLICE JOIN\n    (\n    SELECT ts timesask, stock askStock, ask ask\n    FROM asks\n    )\n    ON bidStock=askStock;",
    "tag": "questdb"
  },
  {
    "title": "Syntax",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/sql/limit.md",
    "content": "\ntitle: LIMIT keyword\nsidebar_label: LIMIT\ndescription: LIMIT SQL keyword reference documentation.\n\nSpecify the number and position of records returned by a\nSELECT statement.\nIn other implementations of SQL, this is sometimes replaced by statements such\nas `OFFSET` or `ROWNUM` Our implementation of `LIMIT` encompasses both in one\nstatement.\nSyntax\n\n\n`numberOfRecords` is the number of records to return.\n`upperBound` and `lowerBound` is the return range. `lowerBound` is\n  exclusive and `upperBound` is inclusive.\n\nA `positive` number will return the `first` n records. A `negative` number will\nreturn the `last` n records.\nExamples\n`questdb-sql title=\"First 5 results\"\nSELECT * FROM ratings LIMIT 5;`\n`questdb-sql title=\"Last 5 results\"\nSELECT * FROM ratings LIMIT -5;`\n`questdb-sql title=\"Range results - this will return records 3, 4 and 5\"\nSELECT * FROM ratings LIMIT 2,5;`\n`negative` range parameters will return results from the bottom of the table.\nAssuming a table with `n` records, the following will return records between n-7\n(exclusive) and n-3 (inclusive), i.e {n-6, n-5, n-4, n-3}. Both `upperBound` and\n`lowerBound` must be negative numbers, in this case:\n```questdb-sql title=\"Range results (negative)\"\nSELECT * FROM ratings LIMIT -7, -3;",
    "tag": "questdb"
  },
  {
    "title": "Syntax",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/sql/alter-table-add-column.md",
    "content": "\ntitle: ALTER TABLE ADD COLUMN\nsidebar_label: ADD COLUMN\ndescription: ALTER TABLE ADD COLUMN SQL keyword reference documentation.\n\nAdds a new column of a specified type to an existing table.\nThe new column is not back-populated even if the table contains data. While a\nsingle column is added atomically, adding multiple columns is not an atomic\noperation. QuestDB will stop adding the remaining columns on the list on the\nfirst failure. It is therefore possible to add some columns and not others.\n:::caution\n\n\nNew column names may only consist of letters, numbers and underscores `_`\n\n\nAdding a new column does not lock the table for reading and does not wait on\n  any reads to finish.\n\n\n:::\nSyntax\n\n\nExamples\nAdd a new column called `comment` of type `STRING` type to the table `ratings`\n`questdb-sql title=\"New column\"\nALTER TABLE ratings ADD COLUMN comment STRING;`\nWhen adding a column of `Symbol` type, optional keywords may be passed which are\nunique to this type. These keywords are described in the\nSymbol type section of the\n`CREATE TABLE` documentation.\nThe following example shows how to add a new `SYMBOL` column with `NOCACHE` and\n`INDEX` keywords:\n```questdb-sql title=\"New symbol column\"\nALTER TABLE ratings ADD COLUMN comment SYMBOL NOCACHE INDEX;",
    "tag": "questdb"
  },
  {
    "title": "Syntax",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/sql/where.md",
    "content": "\ntitle: WHERE keyword\nsidebar_label: WHERE\ndescription: WHERE SQL keyword reference documentation.\n\n`WHERE` clause filters data. Filter expressions are required to return boolean\nresult.\n:::info\nQuestDB includes a JIT compiler for SQL queries which contain `WHERE` clauses.\nTo find out more about this functionality with details on enabling its use, see\nthe JIT compiler documentation.\n:::\nSyntax\nThe general syntax is as follows. Specific filters have distinct syntaxes\ndetailed thereafter.\n\nLogical operators\nQuestDB supports `AND`, `OR`, `NOT` as logical operators and can assemble\nconditions using brackets `()`.\n\n`questdb-sql title=\"Example\"\nSELECT * FROM table\nWHERE\na = 1 AND (b = 2 OR c = 3 AND NOT d);`\nSymbol and string\nQuestDB can filter strings and symbols based on equality, inequality, and\nregular expression patterns.\nExact match\nEvaluates match of a string or symbol.\n\n`questdb-sql title=\"Example\"\nSELECT * FROM users\nWHERE name = 'John';`\n| name | age |\n| ---- | --- |\n| John | 31  |\n| John | 45  |\n| ...  | ... |\nDoes NOT match\nEvaluates mismatch of a string or symbol.\n\n`questdb-sql title=\"Example\"\nSELECT * FROM users\nWHERE name != 'John';`\n| name | age |\n| ---- | --- |\n| Tim  | 31  |\n| Tom  | 45  |\n| ...  | ... |\nRegular expression match\nEvaluates match against a regular expression defined using\njava.util.regex\npatterns.\n\n`questdb-sql title=\"Regex example\"\nSELECT * FROM users WHERE name ~ 'Jo';`\n| name     | age |\n| -------- | --- |\n| Joe      | 31  |\n| Jonathan | 45  |\n| ...      | ... |\nRegular expression does NOT match\nEvaluates mismatch against a regular expression defined using\njava.util.regex\npatterns.\n\n`questdb-sql title=\"Example\"\nSELECT * FROM users WHERE name !~ 'Jo';`\n| name | age |\n| ---- | --- |\n| Tim  | 31  |\n| Tom  | 45  |\n| ...  | ... |\nList search\nEvaluates match or mismatch against a list of elements.\n\n`questdb-sql title=\"List match\"\nSELECT * FROM users WHERE name in('Tim', 'Tom');`\n| name | age |\n| ---- | --- |\n| Tim  | 31  |\n| Tom  | 45  |\n| ...  | ... |\n`questdb-sql title=\"List mismatch\"\nSELECT * FROM users WHERE NOT name in('Tim', 'Tom');`\n| name   | age |\n| ------ | --- |\n| Aaron  | 31  |\n| Amelie | 45  |\n| ...    | ... |\nNumeric\nQuestDB can filter numeric values based on equality, inequality, comparison, and\nproximity.\n:::note\nFor timestamp filters, we recommend the\ntimestamp search notation which is faster and less\nverbose.\n:::\nEquality, inequality and comparison\n\n`questdb-sql title=\"Superior or equal to 23\"\nSELECT * FROM users WHERE age >= 23;`\n`questdb-sql title=\"Equal to 23\"\nSELECT * FROM users WHERE age = 23;`\n`questdb-sql title=\"NOT Equal to 23\"\nSELECT * FROM users WHERE age != 23;`\n\nBoolean\n\nUsing the columnName will return `true` values. To return `false` values,\nprecede the column name with the `NOT` operator.\n`questdb-sql title=\"Example - true\"\nSELECT * FROM users WHERE isActive;`\n| userId | isActive |\n| ------ | -------- |\n| 12532  | true     |\n| 38572  | true     |\n| ...    | ...      |\n`questdb-sql title=\"Example - false\"\nSELECT * FROM users WHERE NOT isActive;`\n| userId | isActive |\n| ------ | -------- |\n| 876534 | false    |\n| 43234  | false    |\n| ...    | ...      |\nTimestamp and date\nQuestDB supports both its own timestamp search notation and standard search\nbased on inequality. This section describes the use of the timestamp search\nnotation which is efficient and fast but requires a\ndesignated timestamp.\nIf a table does not have a designated timestamp applied during table creation,\none may be applied dynamically\nduring a select operation.\nNative timestamp format\nQuestDB automatically recognizes strings formatted as ISO timestamp as a\n`timestamp` type. The following are valid examples of strings parsed as\n`timestamp` types:\n| Valid STRING Format              | Resulting Timestamp         |\n| -------------------------------- | --------------------------- |\n| 2010-01-12T12:35:26.123456+01:30 | 2010-01-12T11:05:26.123456Z |\n| 2010-01-12T12:35:26.123456+01    | 2010-01-12T11:35:26.123456Z |\n| 2010-01-12T12:35:26.123456Z      | 2010-01-12T12:35:26.123456Z |\n| 2010-01-12T12:35:26.12345        | 2010-01-12T12:35:26.123450Z |\n| 2010-01-12T12:35:26.1234         | 2010-01-12T12:35:26.123400Z |\n| 2010-01-12T12:35:26.123          | 2010-01-12T12:35:26.123000Z |\n| 2010-01-12T12:35:26.12           | 2010-01-12T12:35:26.120000Z |\n| 2010-01-12T12:35:26.1            | 2010-01-12T12:35:26.100000Z |\n| 2010-01-12T12:35:26              | 2010-01-12T12:35:26.000000Z |\n| 2010-01-12T12:35                 | 2010-01-12T12:35:00.000000Z |\n| 2010-01-12T12                    | 2010-01-12T12:00:00.000000Z |\n| 2010-01-12                       | 2010-01-12T00:00:00.000000Z |\n| 2010-01                          | 2010-01-01T00:00:00.000000Z |\n| 2010                             | 2010-01-01T00:00:00.000000Z |\n| 2010-01-12 12:35:26.123456-02:00 | 2010-01-12T14:35:26.123456Z |\n| 2010-01-12 12:35:26.123456Z      | 2010-01-12T12:35:26.123456Z |\n| 2010-01-12 12:35:26.123          | 2010-01-12T12:35:26.123000Z |\n| 2010-01-12 12:35:26.12           | 2010-01-12T12:35:26.120000Z |\n| 2010-01-12 12:35:26.1            | 2010-01-12T12:35:26.100000Z |\n| 2010-01-12 12:35:26              | 2010-01-12T12:35:26.000000Z |\n| 2010-01-12 12:35                 | 2010-01-12T12:35:00.000000Z |\nExact timestamp\nSyntax\n\n`questdb-sql title=\"Timestamp equals date\"\nSELECT scores WHERE ts = '2010-01-12T00:02:26.000Z';`\n| ts                       | score |\n| ------------------------ | ----- |\n| 2010-01-12T00:02:26.000Z | 2.4   |\n| 2010-01-12T00:02:26.000Z | 3.1   |\n| ...                      | ...   |\n`questdb-sql title=\"Timestamp equals timestamp\"\nSELECT scores WHERE ts = '2010-01-12T00:02:26.000000Z';`\n| ts                          | score |\n| --------------------------- | ----- |\n| 2010-01-12T00:02:26.000000Z | 2.4   |\n| 2010-01-12T00:02:26.000000Z | 3.1   |\n| ...                         | ...   |\nTime range\nReturns results within a defined range.\nSyntax\n\n`questdb-sql title=\"Results in a given year\"\nSELECT * FROM scores WHERE ts IN '2018';`\n| ts                          | score |\n| --------------------------- | ----- |\n| 2018-01-01T00:0000.000000Z  | 123.4 |\n| ...                         | ...   |\n| 2018-12-31T23:59:59.999999Z | 115.8 |\n`questdb-sql title=\"Results in a given minute\"\nSELECT * FROM scores WHERE ts IN '2018-05-23T12:15';`\n| ts                          | score |\n| --------------------------- | ----- |\n| 2018-05-23T12:15:00.000000Z | 123.4 |\n| ...                         | ...   |\n| 2018-05-23T12:15:59.999999Z | 115.8 |\nTime range with modifier\nYou can apply a modifier to further customize the range. The modifier extends\nthe upper bound of the original timestamp based on the modifier parameter. An\noptional interval with occurrence can be set, to apply the search in the given\ntime range repeatedly, for a set number of times.\nSyntax\n\n\n`timestamp` is the original time range for the query.\n\n`modifier` is a signed integer modifying the upper bound applying to the\n`timestamp`:\n\n\nA `positive` value extends the selected period.\n\n\nA `negative` value reduces the selected period.\n\n\n`interval` is an unsigned integer indicating the desired interval period for\n  the time range.\n\n`repetition` is an unsigned integer indicating the number of times the\n  interval should be applied.\n\nExamples\nModifying the range:\n`questdb-sql title=\"Results in a given year and the first month of the next year\"\nSELECT * FROM scores WHERE ts IN '2018;1M';`\nThe range is 2018. The modifier extends the upper bound (originally 31 Dec 2018)\nby one month.\n| ts                          | score |\n| --------------------------- | ----- |\n| 2018-01-01T00:00:00.000000Z | 123.4 |\n| ...                         | ...   |\n| 2019-01-31T23:59:59.999999Z | 115.8 |\n`questdb-sql title=\"Results in a given month excluding the last 3 days\"\nSELECT * FROM scores WHERE ts IN '2018-01;-3d';`\nThe range is Jan 2018. The modifier reduces the upper bound (originally 31\nJan 2018) by 3 days.\n| ts                          | score |\n| --------------------------- | ----- |\n| 2018-01-01T00:00:00.000000Z | 123.4 |\n| ...                         | ...   |\n| 2018-01-28T23:59:59.999999Z | 113.8 |\nModifying the interval:\n```questdb-sql title=\"Results on a given date with an interval\"\nSELECT * FROM scores WHERE ts IN '2018-01-01;1d;1y;2';\n```\nThe range is extended by one day from Jan 1 2018, with a one-year interval,\nrepeated twice. This means that the query searches for results on Jan 1-2 in\n2018 and in 2019:\n| ts                          | score |\n| --------------------------- | ----- |\n| 2018-01-01T00:00:00.000000Z | 123.4 |\n| ...                         | ...   |\n| 2018-01-02T23:59:59.999999Z | 110.3 |\n| 2019-01-01T00:00:00.000000Z | 128.7 |\n| ...                         | ...   |\n| 2019-01-02T23:59:59.999999Z | 103.8 |\nIN with multiple arguments\nSyntax\n`IN` with more than 1 argument is treated as standard SQL `IN`. It is a\nshorthand of multiple `OR` conditions, i.e. the following query:\n`questdb-sql title=\"IN list\"\nSELECT * FROM scores\nWHERE ts IN ('2018-01-01', '2018-01-01T12:00', '2018-01-02');`\nis equivalent to:\n`questdb-sql title=\"IN list equivalent OR\"\nSELECT * FROM scores\nWHERE ts = '2018-01-01' or ts = '2018-01-01T12:00' or ts = '2018-01-02');`\n| ts                          | value |\n| --------------------------- | ----- |\n| 2018-01-01T00:00:00.000000Z | 123.4 |\n| 2018-01-01T12:00:00.000000Z | 589.1 |\n| 2018-01-02T00:00:00.000000Z | 131.5 |\nBETWEEN\nSyntax\nFor non-standard ranges, users can explicitly specify the target range using the\n`BETWEEN` operator. As with standard SQL, both upper and lower bounds of\n`BETWEEN` are inclusive, and the order of lower and upper bounds is not\nimportant so that `BETWEEN X AND Y` is equivalent to `BETWEEN Y AND X`.\n`questdb-sql title=\"Explicit range\"\nSELECT * FROM scores\nWHERE ts BETWEEN '2018-01-01T00:00:23.000000Z' AND '2018-01-01T00:00:23.500000Z';`\n| ts                          | value |\n| --------------------------- | ----- |\n| 2018-01-01T00:00:23.000000Z | 123.4 |\n| ...                         | ...   |\n| 2018-01-01T00:00:23.500000Z | 131.5 |\n`BETWEEN` can accept non-constant bounds, for example, the following query will\nreturn all records older than one year before the current date:\n```questdb-sql title=\"One year before current date\"\nSELECT * FROM scores\nWHERE ts BETWEEN to_str(now(), 'yyyy-MM-dd')\nAND dateadd('y', -1, to_str(now(), 'yyyy-MM-dd'));",
    "tag": "questdb"
  },
  {
    "title": "Syntax",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/reference/sql/alter-table-alter-column-add-index.md",
    "content": "\ntitle: ALTER TABLE COLUMN ADD INDEX\nsidebar_label: ADD INDEX\ndescription: ADD INDEX SQL keyword reference documentation.\n\nIndexes an existing symbol column.\nSyntax\n\n\nAdding an index is an atomic, non-blocking and non-waiting operation. Once\ncomplete, the SQL optimizer will start using the new index for SQL executions.\n:::info\nFor more information about indexes please refer to the\nINDEX documentation\n:::\nExample\n```questdb-sql title=\"Adding an index\"\nALTER TABLE trades ALTER COLUMN instrument ADD INDEX;",
    "tag": "questdb"
  },
  {
    "title": "Creating a table",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/get-started/first-database.md",
    "content": "\ntitle: Create my first dataset\ndescription:\n  This document shows how to work with QuestDB as a time series database by\n  generating dummy time series data, insert the data into a table, then querying\n  and cleaning up the example data set.\n\nThe goal of this guide is to explore QuestDB's features and to interact with\ntime series data and assumes you have an instance running. You can find guides\nto setup QuestDB on the introduction page. In this tutorial, you will\nlearn how to:\n\nCreate tables\nPopulate tables with sample data\nRun simple and advanced queries\nDelete tables\n\nAs an example, we will look at hypothetical temperature readings from a variety\nof sensors.\n:::info\nAll commands are run through the Web Console\naccessible at http://localhost:9000.\nYou can also run the same SQL via the\nPostgres endpoint or the\nREST API.\n:::\nCreating a table\nThe first step is to create tables. One table will contain the metadata of our\nsensors, and the other will contain the actual readings (payload data) from\nthese sensors.\nLet's start by creating the `sensors` table:\n`questdb-sql\nCREATE TABLE sensors (ID LONG, make STRING, city STRING);`\nFor more information about this statement, please refer to the\nCREATE TABLE reference documentation.\nInserting data\nLet's populate our `sensors` table with procedurally-generated data:\n`questdb-sql title=\"Insert as SELECT\"\nINSERT INTO sensors\n    SELECT\n        x ID, --increasing integer\n        rnd_str('Eberle', 'Honeywell', 'Omron', 'United Automation', 'RS Pro') make,\n        rnd_str('New York', 'Miami', 'Boston', 'Chicago', 'San Francisco') city\n    FROM long_sequence(10000) x\n;`\nFor more information about insert statements, refer to the\nINSERT reference documentation. To learn more\nabout the functions used here, see the\nrandom generator and\nrow generator pages.\nOur `sensors` table now contains 10,000 randomly-generated sensor values of\ndifferent makes and in various cities. Use this command to view the table:\n`questdb-sql\n'sensors';`\nIt should look like the table below:\n| ID  | make              | city    |\n| :-- | :---------------- | :------ |\n| 1   | Honeywell         | Chicago |\n| 2   | United Automation | Miami   |\n| 3   | Honeywell         | Chicago |\n| 4   | Omron             | Miami   |\n| ... | ...               | ...     |\nLet's now create some sensor readings. In this case, we will create the table\nand generate the data at the same time:\n`questdb-sql title=\"Create table as\"\nCREATE TABLE readings\nAS(\n    SELECT\n        x ID,\n        timestamp_sequence(to_timestamp('2019-10-17T00:00:00', 'yyyy-MM-ddTHH:mm:ss'), rnd_long(1,10,0) * 100000L) ts,\n        rnd_double(0)*8 + 15 temp,\n        rnd_long(0, 10000, 0) sensorId\n    FROM long_sequence(10000000) x)\nTIMESTAMP(ts)\nPARTITION BY MONTH;`\nThe query above demonstrates how to use the following features:\n\n`TIMESTAMP(ts)` elects the `ts` column as a\n  designated timestamp. This enables\n  partitioning tables by time.\n`PARTITION BY MONTH` creates a monthly partitioning strategy where the stored\n  data is effectively sharded by month.\n\nThe generated data will look like the following:\n| ID  | ts                          | temp        | sensorId |\n| :-- | :-------------------------- | :---------- | :------- |\n| 1   | 2019-10-17T00:00:00.000000Z | 19.37373911 | 9160     |\n| 2   | 2019-10-17T00:00:00.600000Z | 21.91184617 | 9671     |\n| 3   | 2019-10-17T00:00:01.400000Z | 16.58367834 | 8731     |\n| 4   | 2019-10-17T00:00:01.500000Z | 16.69308815 | 3447     |\n| 5   | 2019-10-17T00:00:01.600000Z | 19.67991569 | 7985     |\n| ... | ...                         | ...         | ...      |\nRunning queries\nLet's select all records from the `readings` table (note that `SELECT * FROM` is\noptional in QuestDB):\n`questdb-sql\nreadings;`\nLet's also select the `count` of records from `readings`:\n`questdb-sql\nSELECT count() FROM readings;`\n| count      |\n| :--------- |\n| 10,000,000 |\nand the average reading:\n`questdb-sql\nSELECT avg(temp) FROM readings;`\n| average         |\n| :-------------- |\n| 18.999217780895 |\nWe can now use the `sensors` table alongside the `readings` table to get more\ninteresting results using a `JOIN`:\n`questdb-sql\nSELECT *\nFROM readings\nJOIN(\n    SELECT ID sensId, make, city\n    FROM sensors)\nON readings.sensorId = sensId;`\nThe results should look like the table below:\n| ID  | ts                          | temp            | sensorId | sensId | make      | city          |\n| :-- | :-------------------------- | :-------------- | :------- | :----- | :-------- | :------------ |\n| 1   | 2019-10-17T00:00:00.000000Z | 16.472200460982 | 3211     | 3211   | Omron     | New York      |\n| 2   | 2019-10-17T00:00:00.100000Z | 16.598432033599 | 2319     | 2319   | Honeywell | San Francisco |\n| 3   | 2019-10-17T00:00:00.100000Z | 20.293681747009 | 8723     | 8723   | Honeywell | New York      |\n| 4   | 2019-10-17T00:00:00.100000Z | 20.939263119843 | 885      | 885    | RS Pro    | San Francisco |\n| 5   | 2019-10-17T00:00:00.200000Z | 19.336660059029 | 3200     | 3200   | Honeywell | San Francisco |\n| 6   | 2019-10-17T00:00:01.100000Z | 20.946643576954 | 4053     | 4053   | Honeywell | Miami         |\n`questdb-sql title=\"Aggregation keyed by city\"\nSELECT city, max(temp)\nFROM readings\nJOIN(\n    SELECT ID sensId, city\n    FROM sensors) a\nON readings.sensorId = a.sensId;`\nThe results should look like the table below:\n| city          | max             |\n| :------------ | :-------------- |\n| New York      | 22.999998786398 |\n| San Francisco | 22.999998138348 |\n| Miami         | 22.99999994818  |\n| Chicago       | 22.999991705861 |\n| Boston        | 22.999999233377 |\n`questdb-sql title=\"Aggregation by hourly time buckets\"\nSELECT ts, city, make, avg(temp)\nFROM readings timestamp(ts)\nJOIN\n    (SELECT ID sensId, city, make\n    FROM sensors\n    WHERE city='Miami' AND make='Omron') a\nON readings.sensorId = a.sensId\nWHERE ts IN '2019-10-21;1d' -- this is an interval between 21-10 and 1 day later`\nThe results should look like the table below:\n| ts                          | city  | make  | average         |\n| :-------------------------- | :---- | :---- | :-------------- |\n| 2019-10-21T00:00:44.600000Z | Miami | Omron | 20.004285872098 |\n| 2019-10-21T00:00:52.400000Z | Miami | Omron | 16.68436714013  |\n| 2019-10-21T00:01:05.400000Z | Miami | Omron | 15.243684089291 |\n| 2019-10-21T00:01:06.100000Z | Miami | Omron | 17.193984104315 |\n| 2019-10-21T00:01:07.100000Z | Miami | Omron | 20.778686822666 |\n| ...                         | ...   | ...   | ...             |\nFor more information about these statements, please refer to the\nSELECT and JOIN pages.\nDeleting tables\nWe can now clean up the demo data by using `DROP TABLE` SQL. Be careful using\nthis statement as QuestDB cannot recover data that is deleted in this way:\n```questdb-sql\nDROP TABLE readings;\nDROP TABLE sensors;",
    "tag": "questdb"
  },
  {
    "title": "Prerequisites",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/get-started/binaries.md",
    "content": "\ntitle: Get started with QuestDB from the binaries\nsidebar_label: Binaries\ndescription:\n  How to install and launch QuestDB from the binaries which are available on the\n  Get QuestDB page.\n\nimport CodeBlock from \"@theme/CodeBlock\"\nimport InterpolateReleaseData from \"../../src/components/InterpolateReleaseData\"\nimport { getAssets } from '../../src/utils/get-assets'\nimport Tabs from \"@theme/Tabs\";\nimport TabItem from \"@theme/TabItem\";\nexport const platforms = [\n  { label: \"Any (no JVM)\", value: \"noJre\" },\n  { label: \"Linux\", value: \"linux\" },\n  { label: \"FreeBSD\", value: \"bsd\" },\n  { label: \"Windows\", value: \"windows\" },\n];\nThis page describes how to download and run QuestDB via binaries. QuestDB comes with a `questdb.sh` script on Linux or FreeBSD, and a `questdb.exe` executable on Windows. For macOS, check out Homebrew.\nPrerequisites\nJava 11\nYou need to have Java 11 installed locally. To check your installed version:\n`shell\njava -version`\nIf you do not have Java installed, install one of the following supported packages for your operating system:\n\nAdoptOpenJDK\nAmazon Corretto\nOpenJDK\nOracle Java\n\nOther Java distributions might work but are not tested.\n`JAVA_HOME`\nThe environment variable `JAVA_HOME` needs to point to your Java 11 installation\nfolder.\nDownload the binaries\n\n<Tabs\n  defaultValue=\"noJre\"\n  values={platforms}\n\n{platforms.map((platform) => (\n    \n {\n          const assets = getAssets(release)\n          const href = assets[platform.value].href\n          return (\n            \n              {href.split(\"/\").reverse()[0]}\n            \n          )\n        }}\n      />\n    \n  ))}\n\n\n\nThe Java runtime is packaged directly with QuestDB and you do not need anything else.\nExtract the tarballs\n\n\n  {platforms.map((platform) => (\n    \n {\n          const assets = getAssets(release)\n          const href = assets[platform.value].href\n          return (\n            \n              {`tar -xvf ${href.split(\"/\").reverse()[0]}`}\n            \n          )\n        }}\n      />\n    \n  ))}\n\n\nRun QuestDB\n\n\n\n\n`shell\n./questdb.sh start`\n\n\n`shell\nquestdb.exe start`\n\n\nUpgrade QuestDB version\n:::note\nCheck the release notes and ensure\nthat necessary backup is completed.\n:::\nTo upgrade the QuestDB version: stop the instance, overwrite the binaries folder with new binaries, and then restart the instance:\n\n\n\n\n```shell\n./questdb.sh stop\n(Overwrite the binaries folder with new binaries)\n./questdb.sh start\n```\n\n\n```shell\nquestdb.exe stop\n(Overwrite the binaries folder with new binaries)\nquestdb.exe start\n```\n\n\nNext steps\nOnce you extracted the tarball, you are ready to use QuestDB. Navigate to our\ncommand-line options page to learn more",
    "tag": "questdb"
  },
  {
    "title": "The existing QuestDB version is 6.5.2:",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/get-started/docker.md",
    "content": "\ntitle: Get started with QuestDB via Docker\nsidebar_label: Docker\ndescription:\n  Guide showing how to use QuestDB with Docker. This also covers how to import\n  and persist QuestDB data in a docker container.\n\nimport InterpolateReleaseData from \"../../src/components/InterpolateReleaseData\"\nimport CodeBlock from \"@theme/CodeBlock\"\nimport Tabs from \"@theme/Tabs\"\nimport TabItem from \"@theme/TabItem\"\nQuestDB has images for both Linux/macOS and Windows on\nDocker Hub.\nInstall Docker\nTo begin, install Docker. You can find guides for your platform on the\nofficial documentation.\nRun QuestDB image\nOnce Docker is installed, you will need to pull QuestDB's image from\nDocker Hub and create a container.\nThis can be done with a single command using:\n (\n    \n      {`docker run \\\\\n  -p 9000:9000 -p 9009:9009 -p 8812:8812 -p 9003:9003 \\\\\n  -v \"$(pwd):/var/lib/questdb\" \\\\\n  questdb/questdb:${release.name}`}\n    \n  )}\n/>\nThis command starts a docker container from `questdb/questdb` image. In\naddition, it exposes some ports and also mounts a volume, to allow your data to persist.\nBelow each parameter is described in detail.\n`-p` parameter to expose ports\nThis parameter will expose a port to the host. You can specify:\n\n`-p 9000:9000` - REST API and\n  Web Console\n`-p 9009:9009` - InfluxDB line protocol\n`-p 8812:8812` - Postgres wire protocol\n`-p 9003:9003` -\n  Min health server\n\nAll ports are optional, you can pick only the ones you need. For example, it is\nenough to expose `8812` if you only plan to use\nInfluxDB line protocol.\n`-v` parameter to mount storage\nThis parameter will make a local folder available to QuestDB docker container.\nIt will have all data ingested to QuestDB, server logs and configuration.\nThe QuestDB root_directory is in the\nfollowing location:\n\nexport const volumeTabs = [\n  { label: \"Linux\", value: \"linux\", code: '/var/lib/questdb' },\n  { label: \"macOS\", value: \"macos\", code: '/var/lib/questdb' },\n  { label: \"Windows\", value: \"windows\", code: 'C:\\questdb' },\n]\n\n{ volumeTabs.map(tab => (\n  \n{tab.code}\n\n))}\n\n\nDocker image version\nBy default, `questdb/questdb` points to the latest QuestDB version available on\nDocker. However, it is recommended to define the version used.\n (\n    \n      {`questdb/questdb:${release.name}`}\n    \n  )}\n/>\nContainer status\nYou can check the status of your container with `docker ps`. It also lists the\nexposed ports:\n`shell\ndocker ps`\n`shell title=\"Result\"\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                NAMES\ndd363939f261        questdb/questdb     \"/app/bin/java -m io\u2026\"   3 seconds ago       Up 2 seconds        8812/tcp, 9000/tcp   frosty_gauss`\nImporting data and sending queries\nWhen QuestDB is running, you can start interacting with it:\n\nPort `9000` is for REST. More info is available on the\n  REST documentation page.\nPort `8812` is used for Postgres. Check our\n  Postgres reference page.\nPort `9009` is dedicated to ILP. Consult our\n  InfluxDB page.\n\nData persistence\nMounting a volume\nVolumes can be mounted to the QuestDB Docker container so that data may be\npersisted or server configuration settings may be passed to an instance. The\nfollowing example demonstrated how to mount the current directory to a QuestDB\ncontainer using the `-v` flag in a Docker `run` command:\n (\n    \n      {`docker run -p 9000:9000 \\\\\n  -p 9009:9009 \\\\\n  -p 8812:8812 \\\\\n  -p 9003:9003 \\\\\n  -v \"$(pwd):/var/lib/questdb\" \\\\\n  questdb/questdb:${release.name}`}\n    \n  )}\n/>\nThe current directory will then have data persisted to disk for convenient\nmigration or backups:\n`bash title=\"Current directory contents\"\n\u251c\u2500\u2500 conf\n\u2502   \u2514\u2500\u2500 server.conf\n\u251c\u2500\u2500 db\n\u2514\u2500\u2500 public`\nFor details on passing QuestDB server settings to a Docker container, see the\nDocker section of the server\nconfiguration documentation.\nUpgrade QuestDB version\nIt is possible to upgrade your QuestDB instance on Docker when a volume is\nmounted to maintain data persistence.\n:::note\n\nCheck the release notes and\n  ensure that necessary backup is completed.\nUpgrading an instance is possible only when the original instance has a volume\n  mounted. Without mounting a volume for the original instance, the following\n  steps create a new instance and data in the old instance cannot be retrieved.\n\n:::\n\nRun `docker ps` to copy the container name or ID:\n\n```shell title=\"Container status\"\nThe existing QuestDB version is 6.5.2:\nCONTAINER ID        IMAGE                    COMMAND                  CREATED             STATUS              PORTS                NAMES\ndd363939f261        questdb/questdb:6.5.2     \"/app/bin/java -m io\u2026\"   3 seconds ago       Up 2 seconds        8812/tcp, 9000/tcp   frosty_gauss\n```\n\nStop the instance and then remove the container:\n\n`shell\ndocker stop dd363939f261\ndocker rm dd363939f261`\n\nDownload the latest QuestDB image:\n\n (\n    \n      {`docker pull questdb/questdb:${release.name}`}\n    \n  )}\n/>\n\nStart a new container with the new version and the same volume mounted:\n\n (\n    \n      {`docker run -p 8812:8812 -p 9000:9000 -v \"$(pwd):/var/lib/questdb\" questdb/questdb:${release.name}`}\n    \n  )}\n/>\nWriting logs to disk\nWhen mounting a volume to a Docker container, a logging configuration file may\nbe provided in the container located at `/conf/log.conf`:\n`bash title=\"Current directory contents\"\n\u2514\u2500\u2500 conf\n    \u251c\u2500\u2500 log.conf\n    \u2514\u2500\u2500 server.conf`\nFor example, a file with the following contents can be created:\n```shell title=\"./conf/log.conf\"\nlist of configured writers\nwriters=file,stdout,http.min\nfile writer\nw.file.class=io.questdb.log.LogFileWriter\nw.file.location=questdb-docker.log\nw.file.level=INFO,ERROR,DEBUG\nstdout\nw.stdout.class=io.questdb.log.LogConsoleWriter\nw.stdout.level=INFO\nmin http server, used monitoring\nw.http.min.class=io.questdb.log.LogConsoleWriter\nw.http.min.level=ERROR\nw.http.min.scope=http-min-server\n```\nThe current directory can be mounted:\n`shell title=\"Mounting the current directory to a QuestDB container\"\ndocker run -p 9000:9000 \\\n -p 9009:9009 \\\n -p 8812:8812 \\\n -p 9003:9003 \\\n -v \"$(pwd):/root/.questdb/\" questdb/questdb`\nThe container logs will be written to disk using the logging level and file name\nprovided in the `conf/log.conf` file, in this case in `./questdb-docker.log`:\n`shell title=\"Current directory tree\"\n\u251c\u2500\u2500 conf\n\u2502  \u251c\u2500\u2500 log.conf\n\u2502  \u2514\u2500\u2500 server.conf\n\u251c\u2500\u2500 db\n\u2502  \u251c\u2500\u2500 table1\n\u2502  \u2514\u2500\u2500 table2\n\u251c\u2500\u2500 public\n\u2502  \u251c\u2500\u2500 ui / assets\n\u2502  \u251c\u2500\u2500 ...\n\u2502  \u2514\u2500\u2500 version.txt\n\u2514\u2500\u2500 questdb-docker.log`\nFor more information on logging, see the\nconfiguration reference documentation.\nRestart an existing container\nRunning the following command will create a new container for the QuestDB image:\n`shell\ndocker run -p 9000:9000 \\\n  -p 9009:9009 \\\n  -p 8812:8812 \\\n  -p 9003:9003 \\\n  questdb/questdb`\nBy giving the container a name with `--name container_name`, we have an easy way\nto refer to the container created by run later on:\n`shell\ndocker run -p 9000:9000 \\\n  -p 9009:9009 \\\n  -p 8812:8812 \\\n  -p 9003:9003 \\\n  --name docker_questdb \\\n  questdb/questdb`\nIf we want to re-use this container and its data after it has been stopped, we\ncan use the following commands:\n```shell\nbring the container up\ndocker start docker_questdb\nshut the container down\ndocker stop docker_questdb\n```\nAlternatively, users can obtain a running container's ID with 'docker ps' and\nrestart it using the\nUUID short identifier:\n```shell title=\"Starting a container by ID\"\ndocker start dd363939f261",
    "tag": "questdb"
  },
  {
    "title": "learn-more.md",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/get-started/learn-more.md",
    "content": "\ntitle: Learn more\nsidebar_label: Learn more\ndescription:\n  This document collects key concepts and guides for users starting testing data\n  with QuestDB.\n\nTo learn more about QuestDB and to create a proof of concept, please refer to\nDesign for performance. In addition,\nour Capacity planning provides a more\nadvanced optimization guide for using QuestDB in production environment.\nThe below are a few concepts and SQL keywords that QuestDB leverages for optimal\ntime series performance:\n\nSymbol: A data type designed for frequently used\n  string values\nCommit lag: A guide on out-of-order\n  data ingestion and associated configuration parameters\nSQL extensions: A guide to special QuestDB\n  SQL keywords and syntax designed for time series data\nDesignated timestamp and\n  Partitions: Concepts and settings relevant to\n",
    "tag": "questdb"
  },
  {
    "title": "Install Homebrew",
    "source": "https://github.com/questdb/questdb.io/tree/master/docs/get-started/homebrew.md",
    "content": "\ntitle: Get started with QuestDB via Homebrew (macOS)\nsidebar_label: Homebrew\ndescription:\n  A short guide for getting started with installing and running QuestDB via\n  Homebrew on macOS.\n\nEach software release of QuestDB is distributed via the\nHomebrew package manager.\nInstall Homebrew\nUsers who already have Homebrew installed may skip this section and proceed to\nInstall QuestDB. Otherwise, Homebrew can be installed by\nrunning the official\ninstallation script\nvia bash:\n`shell\n/bin/bash -c \\\n\"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)\"`\nInstall QuestDB\nTo install QuestDB via Homebrew, run the following command:\n`shell\nbrew install questdb`\nOn macOS, the location of the root directory of QuestDB and\nserver configuration files depending on the\nchip:\n\nPath on Macs with Apple Silicon (M1 or M2) chip:\n\n`shell\n  /opt/homebrew/var/questdb`\n\nPath on Macs with Intel chip:\n\n`shell\n  /usr/local/var/questdb`\nThe file structure is as the following:\n```bash\n/questdb\n\u251c\u2500\u2500 conf\n\u251c\u2500\u2500 db\n\u251c\u2500\u2500 log\n\u2514\u2500\u2500 public\n```\nUninstall QuestDB\nTo remove QuestDB, use Homebrew's `uninstall` command:\n`shell\nquestdb uninstall`\nTroubleshooting Homebrew issues\nIt's recommended to first run `update` before trying to install packages or\ndiagnose errors:\n`shell\nbrew update`\nHomebrew comes with a basic diagnostic command which can help find\ninconsistencies with system settings and permissions. This command will exit\nwith a non-zero status if any potential problems are found:\n`shell\nbrew doctor`\nUpgrade QuestDB version\n:::note\nCheck the release note and ensure\nthat necessary backup is completed.\n:::\nOnce the latest QuestDB version is published on\nHomebrew,\nthe command to upgrade QuestDB version is:\n`shell\nbrew upgrade questdb`\nNext steps\nOnce you installed the QuestDB with Homebrew, you can navigate to our\ncommand-line options page to learn more",
    "tag": "questdb"
  }
]