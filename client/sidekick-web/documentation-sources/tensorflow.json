[
  {
    "title": "TensorFlow Documentation",
    "source": "https://github.com/tensorflow/docs/tree/master/",
    "content": "TensorFlow Documentation\n\n\n\nThese are the source files for the guide and tutorials on\ntensorflow.org.\nTo contribute to the TensorFlow documentation, please read\nCONTRIBUTING.md, the\nTensorFlow docs contributor guide,\nand the style guide.\nTo file a docs issue, use the issue tracker in the\ntensorflow/tensorflow repo.\nAnd join the TensorFlow documentation contributors on the\ndocs@tensorflow.org mailing list.\nCommunity translations\nCommunity translations\nare located in the\ntensorflow/docs-l10n repo. These docs\nare contributed, reviewed, and maintained by the community as best-effort. To\nparticipate as a translator or reviewer, see the `site/<lang>/README.md`, join\nthe language mailing list, and submit a pull request.\nLicense",
    "tag": "tensorflow"
  },
  {
    "title": "Contributing",
    "source": "https://github.com/tensorflow/docs/tree/master/CONTRIBUTING.md",
    "content": "Contributing\nYou don't need to be a developer or a technical writer to make a significant\nimpact on the TensorFlow documentation\u2014just a GitHub account.\nThis guide shows how to make contributions to tensorflow.org.\nSee the\nTensorFlow docs contributor guide\nfor guidance. For questions, the\ndocs@tensorflow.org\nmailing list is available.\nQuestions about TensorFlow usage are better addressed on\nStack Overflow or the\ndiscuss@tensorflow.org\nmailing list.\nTo contribute to the TensorFlow code repositories, see the\nTensorFlow code contributor guide\nand the\nTensorFlow contribution guidelines.\nContributor License Agreements\nWe love patches! To publish your changes, you must sign either the individual or\ncorporate Contributor License Agreement (CLA):\n\nIf you are an individual writing original documentation or source code and\n  you're sure you own the intellectual property, sign an\n  individual CLA.\nIf you work for a company that wants to allow you to contribute your work, sign\n  a corporate CLA.\n\nWe can accept your pull requests after you sign the CLA. We can only receive\noriginal documentation and source code from you and other people that have\nsigned the CLA.\nAbout our docs\nThe TensorFlow documentation is written in Markdown\nand Jupyter/Colab notebooks.\nThe root of tensorflow.org/ is found in the\n`site/en` directory.\nNot all technical content on tensorflow.org is located in `site/en`. Some\nprojects have their repositories under\ngithub.com/tensorflow and they contain\nproject-specific documentation. These projects are navigable from the\ntensorflow/docs `site/en` directory and include a redirect link to where the\ndocs can be updated.\nThe API reference is generated from the source code located in the core\ntensorflow/tensorflow repository\nand other projects.\nAdditionally, some non-technical content, images, and design elements are not",
    "tag": "tensorflow"
  },
  {
    "title": "TensorFlow Docs notebook tools",
    "source": "https://github.com/tensorflow/docs/tree/master/tools/tensorflow_docs/tools",
    "content": "TensorFlow Docs notebook tools\nThe `tensorflow-docs` package contains a collection of notebook tools designed\nfor open source documentation workflows.\nJupyter notebooks are the\npreferred documentation format for TensorFlow\nguides and\ntutorials. These docs integrate with\nGoogle Colab\nfor reproducible environments, regularly tested code examples, and more\nmaintainable documentation.\nInstall\nUse `pip` to install the latest `tensorflow-docs` package directly from the\ntensorflow/docs GitHub repository:\n`shell\n$ python3 -m pip install -U --user git+https://github.com/tensorflow/docs`\nnbfmt\nA notebook formatting tool that makes Jupyter notebook source diffs consistent\nand easier to review. Since notebook authoring environments differ with regards\nto file output, indentation, metadata and other non-specified fields; `nbfmt`\nuses opinionated defaults with a preference for the TensorFlow docs Colab\nworkflow. To format a notebook, install the `tensorflow-docs` package and run\nthe `nbfmt` tool:\n```shell\n$ python3 -m tensorflow_docs.tools.nbfmt [options] notebook.ipynb [...]\n$ python3 -m tensorflow_docs.tools.nbfmt --help\n```\n`nbfmt` accepts directory arguments that format all child notebooks (skipping\nnon-notebook files).\nFor TensorFlow docs projects, notebooks without output cells are executed and\ntested; notebooks with saved output cells are published as-is. `nbfmt`\nrespects the notebook state and uses the `--remove_outputs` option to explicitly\nremove output cells.\nThe `--test` flag is for continuous integration tests. This does not format the\nnotebook, rather it exits with an error code if the notebook is not in an\nup-to-date formatted state. See the tensorflow/docs\nGitHub Actions workflow\nfor an example.\nPre-commit\nYou can set up the `nbfmt` tool as a pre-commit check in other repos. To do\nthis, use a standard Git hook or use the\nhttps://pre-commit.com/ framework to create the hook\nfor you.\nIf you want to use pre-commit to handle the hook installation for you, include\nthe .pre-commit-hooks.yaml file in your repo with\nthe following contents:\n`repos:\n- repo: https://github.com/tensorflow/docs\n  rev: pre-commit`\nSomeone who clones that repo for development would then install the hook with:\n```shell\nInstall pre-commit framework\n$ pip3 install pre-commit\nInstall hooks\n$ pre-commit install\n```\nnblint\nA notebook linting tool that checks documentation style rules. Used to catch\ncommon errors and useful for CI tests. To lint a notebook, install the\n`tensorflow-docs` package and run the `nblint` tool:\n```shell\n$ python3 -m tensorflow_docs.tools.nblint [options] notebook.ipynb [...]\n$ python3 -m tensorflow_docs.tools.nblint --fix [options] notebook.ipynb [...]\n$ python3 -m tensorflow_docs.tools.nblint --help\n```\nSome styles require a user-defined argument passed at the command-line. For\nexample, the `tensorflow` style (default) uses the `repo` argument to check links:\n`shell\n$ python3 -m tensorflow_docs.tools.nblint --arg=repo:tensorflow/docs notebook.ipynb`\nLints are assertions that test specific sections of the notebook. These lints\nare collected into\nstyle modules.\n`nblint` tests the `google` and `tensorflow` styles by default, and different\nstyles can be set with the `--styles` option:\n`shell\n$ python3 -m tensorflow_docs.tools.nblint \\\n    --styles=tensorflow,tensorflow_docs_l10n --arg=repo:tensorflow/docs-1l0n \\\n    notebook.ipynb`\nA style module may contain some lint checks that do not fit your project. You\ncan exclude specific lint checks with the `--exclude_lint` option:\n`shell\n$ python3 -m tensorflow_docs.tools.nblint \\\n    --styles=tensorflow --arg=repo:community/repo-name \\\n    --exclude_lint=tensorflow::copyright_check \\\n    --exclude_lint=tensorflow::button_website \\\n    ./community/notebook.ipynb`\nSome lint errors can be automatically fixed in the notebook file:\n`shell\n$ python3 -m tensorflow_docs.tools.nblint --fix \\\n    --arg=repo:tensorflow/docs notebook.ipynb`\nThis applies the lint fixes to the notebook and overwrites the file. Not all\nlint errors have an associated fix. Fixes are applied from the loaded style",
    "tag": "tensorflow"
  },
  {
    "title": "Title (ML Task)",
    "source": "https://github.com/tensorflow/docs/tree/master/tools/templates/tflite_model_page_template.md",
    "content": "Title (ML Task)\nShort description of ML Task. Link off to relevant content on the TF docs for\nbackground info.\nGet started\nImage of model output (preferably GIFs)\n\nIf you are new to TensorFlow Lite and are working with Android or iOS, we\nrecommend exploring the following example applications that can help you get\nstarted.\nDownload\nAndroid model\nDownload\niOS model\nIf you are using a platform other than Android/iOS, or if you are already\nfamiliar with the\nTensorFlow Lite APIs,\ndownload the starter model and supporting files (if applicable).\nDownload\nstarter model\nModel description\nIn this section, include content about:\nHow it works\n\nHow does the model work? Provide usage instructions with examples as appropriate.\n\nInputs\n\nWhich format does the model expect inputs in?\nHow can the user convert inputs of a certain type into a compatible format?\n\nOutputs\n\nHow does the user interpret the model results?\n\nLimitations\n\nWhat can the user not do with the model?\n\nModel customization\n\nHow can the user customize this model to work with their datasets?\n\nPerformance benchmarks\nFurther reading and resources\n\nAcademic paper (if applicable)\nUse cases\n",
    "tag": "tensorflow"
  },
  {
    "title": "Subsite projects",
    "source": "https://github.com/tensorflow/docs/tree/master/tools/templates/subsite",
    "content": "Subsite projects\nSubsite projects are sections of the\ntensorflow.org website that do not live in the\ntensorflow/docs repo. Instead, the project\ndocs live with the code in the project repo. Some example subsites:\n\nTensorFlow Probability\n  [tensorflow/probability]\nTensorFlow Serving\n  [tensorflow/serving]\n\nDocumentation changes are submitted to the project repo and not the\ntensorflow/docs repo. Guides can be Markdown files or Colab/Jupyter notebooks.\nSet up the base template for the subsite project\n\nCopy the `tools/templates/subsite/g3doc` directory from the docs repo to the\n   project repo:\n\n`$ cp -r tensorflow/docs/tools/templates/subsite/g3doc tensorflow/myproject/`\n\n\n```In GitHub, you may rename the project's `/g3doc` directory to `/docs`.\n```\n\n\n\nIn the project's `g3doc/` directory, replace `PROJECT_NAME` in each template\n   file with the short name of the project. This is used for the project URL,\n   for example, `https://www.tensorflow.org/myproject`:\n\n`$ find tensorflow/myproject/g3doc/ -type f | xargs sed -i 's/PROJECT_NAME/myproject/g'`\nUpdate the configuration files\n\nThe `_book.yaml` file configures the lower tabs and left navigation for\n   files. Each page must have an entry in `_book.yaml` to be navigable on\n   tensorflow.org.\nThe TensorFlow docs team must set up a project file.\n\nChanges to `.yaml` files must be approved by the TensorFlow docs team.\nSet up the API generator for reference docs\nTo build reference docs for the project, write a `build_docs.py` script using the\napi_generator\nAPI. The best way to do this is to look at examples from other subsite projects.\nIf the project does not have an API reference, remove this navigation section\nfrom the `_book.yaml` file.\nCreate a link to your project docs\nTo make it easier for contributors to find your doc set, add a project entry to\ntensorflow/docs/site/en\nand include a `README.md` file with a link. For example,",
    "tag": "tensorflow"
  },
  {
    "title": "TensorFlow docs",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en",
    "content": "TensorFlow docs\nThese are the source files for the core TensorFlow\nguide,\ntutorials, and other technical docs.\nPlease read the\ncontributor guide\nto submit patches to the TensorFlow documentation and code.\nTensorFlow ecosystem projects\nIn addition to the core TensorFlow docs,\ntensorflow.org hosts documentation for many\nlibraries and extensions.\nThese docs are sourced from separate project repos and where pull requests can\nbe sent. The following is a list of TensorFlow documentation projects published\non the website and a link to their source files:\ntensorflow.org project | GitHub docs location\n-----------------------|---------------------\n/addons | https://github.com/tensorflow/addons/tree/master/docs\n/agents | https://github.com/tensorflow/agents/tree/master/docs\n/cloud | https://github.com/tensorflow/cloud/tree/master/g3doc\n/datasets | https://github.com/tensorflow/datasets/tree/master/docs\n/decision_forests | https://github.com/tensorflow/decision-forests/tree/main/documentation\n/federated | https://github.com/tensorflow/federated/tree/main/docs\n/graphics | https://github.com/tensorflow/graphics/tree/master/tensorflow_graphics/g3doc\n/hub | https://github.com/tensorflow/hub/tree/master/docs\n/io | https://github.com/tensorflow/io/tree/master/docs/\n/js | https://github.com/tensorflow/tfjs-website/tree/master/docs\n/jvm | https://github.com/tensorflow/java/tree/master/docs\n/lattice | https://github.com/tensorflow/lattice/tree/master/docs\n/lite | https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/g3doc\n/mlir | https://github.com/tensorflow/tensorflow/tree/master/tensorflow/compiler/mlir/g3doc\n/model_optimization | https://github.com/tensorflow/model-optimization/tree/master/tensorflow_model_optimization/g3doc\n/neural_structured_learning | https://github.com/tensorflow/neural-structured-learning/tree/master/g3doc\n/probability | https://github.com/tensorflow/probability/tree/main/tensorflow_probability/g3doc\n/quantum | https://github.com/tensorflow/quantum/tree/master/docs\n/ranking | https://github.com/tensorflow/ranking/tree/master/docs\n/recommenders | https://github.com/tensorflow/recommenders/tree/main/docs\n/responsible_ai/fairness_indicators | https://github.com/tensorflow/fairness-indicators/tree/master/g3doc\n/responsible_ai/model_card_toolkit | https://github.com/tensorflow/model-card-toolkit/tree/master/model_card_toolkit/documentation\n/responsible_ai/model_remediation | https://github.com/tensorflow/model-remediation/tree/master/docs\n/responsible_ai/privacy | https://github.com/tensorflow/privacy/tree/master/g3doc\n/tensorboard | https://github.com/tensorflow/tensorboard/tree/master/docs\n/guide/keras | https://github.com/keras-team/keras-io/tree/master/guides\n/text | https://github.com/tensorflow/text/tree/master/docs\n/tfx | https://github.com/tensorflow/tfx/tree/master/docs\n/tfx/guide/serving | https://github.com/tensorflow/serving/tree/master/tensorflow_serving/g3doc",
    "tag": "tensorflow"
  },
  {
    "title": "Optimize TensorFlow GPU performance with the TensorFlow Profiler",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/guide/gpu_performance_analysis.md",
    "content": "Optimize TensorFlow GPU performance with the TensorFlow Profiler\nOverview\nThis guide will show you how to use the TensorFlow Profiler with TensorBoard to\ngain insight into and get the maximum performance out of your GPUs, and debug\nwhen one or more of your GPUs are underutilized.\nIf you are new to the Profiler:\n\nGet started with the\n    TensorFlow Profiler: Profile model performance\n    notebook with a Keras example and\n    TensorBoard.\nLearn about various profiling tools and methods available for optimizing\n    TensorFlow performance on the host (CPU) with the\n    Optimize TensorFlow performance using the Profiler\n    guide.\n\nKeep in mind that offloading computations to GPU may not always be beneficial,\nparticularly for small models. There can be overhead due to:\n\nData transfer between the host (CPU) and the device (GPU); and\nDue to the latency involved when the host launches GPU kernels.\n\nPerformance optimization workflow\nThis guide outlines how to debug performance issues starting with a single GPU,\nthen moving to a single host with multiple GPUs.\nIt is recommended to debug performance issues in the following order:\n\nOptimize and debug the performance on one GPU:\nCheck if the input pipeline is a bottleneck.\nDebug the performance of one GPU.\nEnable mixed precision (with `fp16` (float16)) and optionally enable\n    XLA.\n\n\nOptimize and debug the performance on the multi-GPU single host.\n\nFor example, if you are using a TensorFlow\ndistribution strategy\nto train a model on a single host with multiple GPUs and notice suboptimal GPU\nutilization, you should first optimize and debug the performance for one GPU\nbefore debugging the multi-GPU system.\nAs a baseline for getting performant code on GPUs, this guide assumes you are\nalready using `tf.function`. The Keras `Model.compile` and `Model.fit` APIs will\nutilize `tf.function` automatically under the hood. When writing a custom\ntraining loop with `tf.GradientTape`, refer to the\nBetter performance with tf.function\non how to enable `tf.function`s.\nThe next sections discuss suggested approaches for each of the scenarios above\nto help identify and fix performance bottlenecks.\n1. Optimize the performance on one GPU\nIn an ideal case, your program should have high GPU utilization, minimal CPU\n(the host) to GPU (the device) communication, and no overhead from the input\npipeline.\nThe first step in analyzing the performance is to get a profile for a model\nrunning with one GPU.\nTensorBoard's Profiler\noverview page\u2014which\nshows a top level view of how your model performed during a profile run\u2014can\nprovide an idea of how far away your program is from the ideal scenario.\n\nThe key numbers to pay attention to the overview page are:\n\nHow much of the step time is from actual device execution\nThe percentage of ops placed on device vs host\nHow many kernels use `fp16`\n\nAchieving optimal performance means maximizing these numbers in all three cases.\nTo get an in-depth understanding of your program, you will need to be familiar\nwith TensorBoard's Profiler\ntrace viewer. The\nsections below show some common trace viewer patterns that you should look for\nwhen diagnosing performance bottlenecks.\nBelow is an image of a model trace view running on one GPU. From the TensorFlow\nName Scope and TensorFlow Ops sections, you can identify different parts of\nthe model, like the forward pass, the loss function, backward pass/gradient\ncalculation, and the optimizer weight update. You can also have the ops running\non the GPU next to each Stream, which refer to CUDA streams. Each stream is\nused for specific tasks. In this trace, Stream#118 is used to launch compute\nkernels and device-to-device copies. Stream#119 is used for host-to-device\ncopy and Stream#120 for device to host copy.\nThe trace below shows common characteristics of a performant model.\n\nFor example, the GPU compute timeline (Stream#118) looks \"busy\" with very few\ngaps. There are minimal copies from host to device (Stream #119) and from\ndevice to host (Stream #120), as well as minimal gaps between steps. When you\nrun the Profiler for your program, you may not be able to identify these ideal\ncharacteristics in your trace view. The rest of this guide covers common\nscenarios and how to fix them.\n1. Debug the input pipeline\nThe first step in GPU performance debugging is to determine if your program is\ninput-bound. The easiest way to figure this out is to use the Profiler\u2019s\nInput-pipeline analyzer,\non TensorBoard, which provides an overview of time spent in the input pipeline.\n\nYou can take the following potential actions if your input-pipeline contributes\nsignificantly to step time:\n\nYou can use the `tf.data`-specific\n    guide to learn\n    how to debug your input pipeline.\nAnother quick way to check if the input pipeline is the bottleneck is to use\n    randomly generated input data that does not need any pre-processing.\n    Here is an example\n    of using this technique for a ResNet model. If the input pipeline is\n    optimal, you should experience similar performance with real data and with\n    generated random/synthetic data. The only overhead in the synthetic data\n    case will be due to input data copy which again can be prefetched and\n    optimized.\n\nIn addition, refer to the\nbest practices for optimizing the input data pipeline.\n2. Debug the performance of one GPU\nThere are several factors that can contribute to low GPU utilization. Below are\nsome scenarios commonly observed when looking at the\ntrace viewer and\npotential solutions.\n1. Analyze gaps between steps\nA common observation when your program is not running optimally is gaps between\ntraining steps. In the image of the trace view below, there is a large gap\nbetween steps 8 and 9, meaning that the GPU is idle during that time.\n\nIf your trace viewer shows large gaps between steps, this could be an indication\nthat your program is input bound. In that case you should refer to the previous\nsection on debugging your input pipeline if you have not already done so.\nHowever, even with an optimized input pipeline, you can still have gaps between\nthe end of one step and the start of another due to CPU thread contention.\n`tf.data` makes use of background threads to parallelize pipeline processing.\nThese threads may interfere with GPU host-side activity that happens at the\nbeginning of each step, such as copying data or scheduling GPU operations.\nIf you notice large gaps on the host side, which schedules these ops on the GPU,\nyou can set the environment variable `TF_GPU_THREAD_MODE=gpu_private`. This\nensures that GPU kernels are launched from their own dedicated threads, and\ndon't get queued behind `tf.data` work.\nGaps between steps can also be caused by metric calculations, Keras callbacks,\nor ops outside of `tf.function` that run on the host. These ops don\u2019t have as\ngood performance as the ops inside a TensorFlow graph. Additionally, some of\nthese ops run on the CPU and copy tensors back and forth from the GPU.\nIf after optimizing your input pipeline you still notice gaps between steps in\nthe trace viewer, you should look at the model code between steps and check if\ndisabling callbacks/metrics improves performance. Some details of these ops are\nalso on the trace viewer (both device and host side).The recommendation in this\nscenario is to amortize the overhead of these ops by executing them after a\nfixed number of steps instead of every step. When using the `Model.compile` method in\nthe `tf.keras` API, setting the `steps_per_execution` flag does\nthis automatically. For custom training loops, use `tf.while_loop`.\n2. Achieve higher device utilization\n1. Small GPU kernels and host kernel launch delays\nThe host enqueues kernels to be run on the GPU, but there is a latency (around\n20-40 \u03bcs) involved before kernels are actually executed on the GPU. In an ideal\ncase, the host enqueues enough kernels on the GPU such that the GPU spends most\nof its time executing, rather than waiting on the host to enqueue more kernels.\nThe Profiler's\noverview page on\nTensorBoard shows how much time the GPU was idle due to waiting on the host to\nlaunch kernels. In the image below, the GPU is idle for about 10% of the step\ntime waiting on kernels to be launched.\n\nThe trace viewer for\nthis same program shows small gaps between kernels where the host is busy\nlaunching kernels on the GPU.\n\nBy launching a lot of small ops on the GPU (like a scalar add, for example), the\nhost might not keep up with the GPU. The\nTensorFlow Stats\ntool in TensorBoard for the same Profile shows 126,224 Mul operations taking\n2.77 seconds. Thus, each kernel is about 21.9 \u03bcs, which is very small (around\nthe same time as launch latency) and can potentially result in host kernel\nlaunch delays.\n\nIf your trace viewer\nshows many small gaps between ops on the GPU like in the image above, you can:\n\nConcatenate small tensors and use vectorized ops or use a larger batch size\n    to make each launched kernel do more work, which will keep the GPU busy for\n    longer.\nMake sure you are using `tf.function` to create TensorFlow graphs, so that\n    you are not running ops in a pure eager mode. If you are using `Model.fit`\n    (as oppose to a custom training loop with `tf.GradientTape`), then\n    `tf.keras.Model.compile` will automatically do this for you.\nFuse kernels using XLA with `tf.function(jit_compile=True)` or\n    auto-clustering. For more details, go to the\n    Enable mixed precision and XLA section\n    below to learn how to enable XLA to get higher performance. This feature can\n    lead to high device utilization.\n\n2. TensorFlow op placement\nThe Profiler\noverview page shows\nyou the percentage of ops placed on the host vs. the device (you can also verify\nthe placement of specific ops by looking at the\ntrace viewer. Like in\nthe image below, you want the percentage of ops on the host to be very small\ncompared to the device.\n\nIdeally, most of the compute intensive ops should be placed on the GPU.\nTo find out which devices the operations and tensors in your model are assigned\nto, set `tf.debugging.set_log_device_placement(True)` as the first statement of\nyour program.\nNote that in some cases, even if you specify an op to be placed on a particular\ndevice, its implementation might override this condition (example:`tf.unique`).\nEven for single GPU training, specifying a distribution strategy, such as\n`tf.distribute.OneDeviceStrategy`, can result in more deterministic placement of\nops on your device.\nOne reason for having the majority of ops placed on the GPU is to prevent\nexcessive memory copies between the host and the device (memory copies for model\ninput/output data between host and device are expected). An example of excessive\ncopying is demonstrated in the trace view below on GPU streams #167, #168,\nand #169.\n\nThese copies can sometimes hurt the performance if they block GPU kernels from\nexecuting. Memory copy operations in the\ntrace viewer have more\ninformation about the ops that are the source of these copied tensors, but it\nmight not always be easy to associate a memCopy with an op. In these cases, it\nis helpful to look at the ops nearby to check if the memory copy happens at the\nsame location in every step.\n3. More efficient kernels on GPUs\nOnce your program's GPU utilization is acceptable, the next step is to look into\nincreasing the efficiency of the GPU kernels by utilizing Tensor Cores or fusing\nops.\n1. Utilize Tensor Cores\nModern NVIDIA\u00ae GPUs have specialized\nTensor Cores that can\nsignificantly improve the performance of eligible kernels.\nYou can use TensorBoard's\nGPU kernel stats\nto visualize which GPU kernels are Tensor Core-eligible, and which kernels are\nusing Tensor Cores. Enabling `fp16` (see Enabling Mixed Precision section below)\nis one way to make your program\u2019s General Matrix Multiply (GEMM) kernels (matmul\nops) utilize the Tensor Core. GPU kernels use the Tensor Cores efficiently when\nthe precision is fp16 and input/output tensor dimensions are divisible by 8 or\n16 (for `int8`).\nNote: With cuDNN v7.6.3 and later, convolution dimensions will automatically be\npadded where necessary to leverage Tensor Cores.\nFor other detailed recommendations on how to make kernels efficient for GPUs,\nrefer to the\nNVIDIA\u00ae deep learning performance\nguide.\n2. Fuse ops\nUse `tf.function(jit_compile=True)` to fuse smaller ops to form bigger kernels\nleading to significant performance gains. To learn more, refer to the\nXLA guide.\n3. Enable mixed precision and XLA\nAfter following the above steps, enabling mixed precision and XLA are two\noptional steps you can take to improve performance further. The suggested\napproach is to enable them one by one and verify that the performance benefits\nare as expected.\n1. Enable mixed precision\nThe TensorFlow\nMixed precision guide\nshows how to enable `fp16` precision on GPUs. Enable\nAMP on NVIDIA\u00ae GPUs to\nuse Tensor Cores and realize up to 3x overall speedups when compared to using\njust `fp32` (float32) precision on Volta and newer GPU architectures.\nMake sure that matrix/tensor dimensions satisfy requirements for calling kernels\nthat use Tensor Cores. GPU kernels use the Tensor Cores efficiently when the\nprecision is fp16 and input/output dimensions are divisible by 8 or 16 (for\nint8).\nNote that with cuDNN v7.6.3 and later, convolution dimensions will automatically\nbe padded where necessary to leverage Tensor Cores.\nFollow the best practices below to maximize the performance benefits of `fp16`\nprecision.\n1. Use optimal fp16 kernels\nWith `fp16` enabled, your program\u2019s matrix multiplications (GEMM) kernels,\nshould use the corresponding `fp16` version that utilizes the Tensor Cores.\nHowever, in some cases, this does not happen and you do not experience the\nexpected speedup from enabling `fp16`, as your program falls back to the\ninefficient implementation instead.\n\nThe GPU kernel\nstats page shows which ops are Tensor Core eligible and which kernels are\nactually using the efficient Tensor Core. The\nNVIDIA\u00ae guide on deep learning performance\ncontains additional suggestions on how to leverage Tensor Cores. Additionally,\nthe benefits of using `fp16` will also show in kernels that were previously\nmemory bound, as now the ops will take half the time.\n2. Dynamic vs. static loss scaling\nLoss scaling is necessary when using `fp16` to prevent underflow due to low\nprecision. There are two types of loss scaling, dynamic and static, both of\nwhich are explained in greater detail in the\nMixed Precision guide.\nYou can use the `mixed_float16` policy to automatically enable loss scaling\nwithin the Keras optimizer.\nNote: The Keras mixed precision API defaults to evaluating standalone softmax\nops (ops not part of a Keras loss function) as `fp16` which can lead to\nnumerical issues and poor convergence. Cast such ops to `fp32` for optimal\nperformance.\nWhen trying to optimize performance, it is important to remember that dynamic\nloss scaling can introduce additional conditional ops that run on the host, and\nlead to gaps that will be visible between steps in the trace viewer. On the\nother hand, static loss scaling does not have such overheads and can be a better\noption in terms of performance with the catch that you need to specify the\ncorrect static-loss scale value.\n2. Enable XLA with tf.function(jit_compile=True) or auto-clustering\nAs a final step in getting the best performance with a single GPU, you can\nexperiment with enabling XLA, which will fuse ops and lead to better device\nutilization and a lower memory footprint. For details on how to enable XLA in\nyour program with `tf.function(jit_compile=True)` or auto-clustering, refer to\nthe XLA guide.\nYou can set the global JIT level to `-1` (off), `1`, or `2`. A higher level is\nmore aggressive and may reduce parallelism and use more memory. Set the value to\n`1` if you have memory restrictions. Note that XLA does not perform well for\nmodels with variable input tensor shapes as the XLA compiler would have to keep\ncompiling kernels whenever it encounters new shapes.\n2. Optimize the performance on the multi-GPU single host\nThe `tf.distribute.MirroredStrategy` API can be used to scale model training\nfrom one GPU to multiple GPUs on a single host. (To learn more about how to do\ndistributed training with TensorFlow, refer to the\nDistributed training with TensorFlow,\nUse a GPU, and\nUse TPUs guides and the\nDistributed training with Keras\ntutorial.)\nAlthough the transition from one GPU to multiple GPUs should ideally be scalable\nout of the box, you can sometimes encounter performance issues.\nWhen going from training with a single GPU to multiple GPUs on the same host,\nideally you should experience the performance scaling with only the additional\noverhead of gradient communication and increased host thread utilization.\nBecause of this overhead, you will not have an exact 2x speedup if you move from\n1 to 2 GPUs, for example.\nThe trace view below shows an example of the extra communication overhead when\ntraining on multiple GPUs. There is some overhead to concatenate the gradients,\ncommunicate them across replicas, and split them before doing the weight update.\n\nThe following checklist will help you achieve better performance when optimizing\nthe performance in the multi-GPU scenario:\n\nTry to maximize the batch size, which will lead to higher device utilization\n    and amortize the costs of communication across multiple GPUs. Using the\n    memory profiler\n    helps get a sense of how close your program is to peak memory utilization.\n    Note that while a higher batch size can affect convergence, this is usually\n    outweighed by the performance benefits.\nWhen moving from a single GPU to multiple GPUs, the same host now has to\n    process much more input data. So, after (1), it is recommended to re-check\n    the input pipeline performance and make sure it is not a bottleneck.\nCheck the GPU timeline in your program's trace view for any unnecessary\n    AllReduce calls, as this results in a synchronization across all devices. In\n    the trace view shown above, the AllReduce is done via the\n    NCCL kernel, and there is only one NCCL\n    call on each GPU for the gradients on each step.\nCheck for unnecessary D2H, H2D and D2D copy operations that can be\n    minimized.\nCheck the step time to make sure each replica is doing the same work. For\n    example, it can happen that one GPU (typically, `GPU0`) is oversubscribed\n    because the host mistakenly ends up putting more work on it.\nLastly, check the training step across all GPUs in your trace view for any\n    ops that are executing sequentially. This usually happens when your program\n    includes control dependencies from one GPU to another. In the past,\n    debugging the performance in this situation has been solved on a\n    case-by-case basis. If you observe this behavior in your program,\n    file a GitHub issue\n    with images of your trace view.\n\n1. Optimize gradient AllReduce\nWhen training with a synchronous strategy, each device receives a portion of the\ninput data.\nAfter computing the forward and backwards passes through the model, the\ngradients calculated on each device need to be aggregated and reduced. This\ngradient AllReduce happens after the gradient calculation on each device, and\nbefore the optimizer updates the model weights.\nEach GPU first concatenates the gradients across the model layers, communicates\nthem across GPUs using `tf.distribute.CrossDeviceOps`\n(`tf.distribute.NcclAllReduce` is the default), and then returns the gradients\nafter reduction per layer.\nThe optimizer will use these reduced gradients to update the weights of your\nmodel. Ideally, this process should happen at the same time on all GPUs to\nprevent any overheads.\nThe time to AllReduce should be approximately the same as:\n`(number of parameters * 4bytes)/ (communication bandwidth)`\nThis calculation is useful as a quick check to understand whether the\nperformance you have when running a distributed training job is as expected, or\nif you need to do further performance debugging. You can get the number of\nparameters in your model from `Model.summary`.\nNote that each model parameter is 4 bytes in size since TensorFlow uses `fp32`\n(float32) to communicate gradients. Even when you have `fp16` enabled, NCCL\nAllReduce utilizes `fp32` parameters.\nTo get the benefits of scaling, the step-time needs to be much higher compared\nto these overheads. One way to achieve this is to use a higher batch size as\nbatch size affects step time, but does not impact the communication overhead.\n2. GPU host thread contention\nWhen running multiple GPUs, the CPU\u2019s job is to keep all of the devices busy by\nefficiently launching GPU kernels across the devices.\nHowever, when there are a lot of independent operations that the CPU can\nschedule on one GPU, the CPU can decide to use a lot of its host threads to keep\none GPU busy, and then launch kernels on another GPU in a non-deterministic\norder. This can cause a skew or negative scaling, which can negatively affect\nthe performance.\nThe trace viewer below\nshows the overhead when the CPU staggers GPU kernel launches inefficiently, as\n`GPU1` is idle and then starts running ops after `GPU2` has started.\n\nThe trace view for the host shows that the host is launching kernels on `GPU2`\nbefore launching them on `GPU1` (note that the below `tf_Compute*` ops are not\nindicative of CPU threads).\n\nIf you experience this kind of staggering of GPU kernels in your program\u2019s trace\nview, the recommended action is to:\n\nSet the TensorFlow environment variable `TF_GPU_THREAD_MODE` to\n    `gpu_private`. This environment variable will tell the host to keep threads\n    for a GPU private.\nBy default,`TF_GPU_THREAD_MODE=gpu_private` sets the number of threads to 2,\n    which is sufficient in most cases. However, that number can be changed by\n    setting the TensorFlow environment variable `TF_GPU_THREAD_COUNT` to the\n",
    "tag": "tensorflow"
  },
  {
    "title": "TensorFlow version compatibility",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/guide/versions.md",
    "content": "TensorFlow version compatibility\nThis document is for users who need backwards compatibility across different\nversions of TensorFlow (either for code or data), and for developers who want\nto modify TensorFlow while preserving compatibility.\nSemantic versioning 2.0\nTensorFlow follows Semantic Versioning 2.0 (semver) for its\npublic API. Each release version of TensorFlow has the form `MAJOR.MINOR.PATCH`.\nFor example, TensorFlow version 1.2.3 has `MAJOR` version 1, `MINOR` version 2,\nand `PATCH` version 3. Changes to each number have the following meaning:\n\n\nMAJOR:  Potentially backwards incompatible changes.  Code and data that\n  worked with a previous major release will not necessarily work with the new\n  release. However, in some cases existing TensorFlow graphs and checkpoints\n  may be migratable to the newer release; see\n  Compatibility of graphs and checkpoints\n  for details on data compatibility.\n\n\nMINOR: Backwards compatible features, speed improvements, etc. Code and\n  data that worked with a previous minor release and which depends only on the\n  non-experimental public API will continue to work unchanged.  For details on\n  what is and is not the public API, see What is covered.\n\n\nPATCH: Backwards compatible bug fixes.\n\n\nFor example, release 1.0.0 introduced backwards incompatible changes from\nrelease 0.12.1.  However, release 1.1.1 was backwards compatible with release\n1.0.0.\n\nWhat is covered\nOnly the public APIs of TensorFlow are backwards compatible across minor and\npatch versions.  The public APIs consist of\n\n\nAll the documented Python functions and classes in the\n  `tensorflow` module and its submodules, except for\n\nPrivate symbols: any function, class, etc., whose name start with `_`\nExperimental and `tf.contrib` symbols, see below for\n  details.\n\n\n\nNote that the code in the `examples/` and `tools/` directories is not\n  reachable through the `tensorflow` Python module and is thus not covered by\n  the compatibility guarantee.\nIf a symbol is available through the `tensorflow` Python module or its\n  submodules, but is not documented, then it is not considered part of the\n  public API.\n\n\nThe compatibility API (in Python, the `tf.compat` module). At major versions,\n  we may release utilities and additional endpoints to help users with the\n  transition to a new major version. These API symbols are deprecated and not\n  supported (i.e., we will not add any features, and we will not fix bugs\n  other than to fix vulnerabilities), but they do fall under our compatibility\n  guarantees.\n\n\nThe C API.\n\n\nThe following protocol buffer files:\n\nattr_value\nconfig\nevent\ngraph\nop_def\nreader_base\nsummary\ntensor\ntensor_shape\ntypes\n\n\n\n\nWhat is not covered\nSome parts of TensorFlow can change in backward incompatible ways at any point.\nThese include:\n\n\nExperimental APIs: To facilitate development, we exempt some API symbols\n    clearly marked as experimental from the compatibility guarantees. In\n    particular, the following are not covered by any compatibility guarantees:\n\nany symbol in the `tf.contrib` module or its submodules;\nany symbol (module, function, argument, property, class, or constant)\n    whose name contains `experimental` or `Experimental`; or\nany symbol whose fully qualified name includes a module or class which\n    is itself experimental. This includes fields and submessages of any\n    protocol buffer called `experimental`.\n\n\n\nOther languages: TensorFlow APIs in languages other than Python and C,\n    such as:\n\nC++ (exposed through header files in\n    tensorflow/cc).\nJava,\nGo\nJavaScript\n\n\n\nDetails of composite ops: Many public functions in Python expand to\n    several primitive ops in the graph, and these details will be part of any\n    graphs saved to disk as `GraphDef`s. These details may change for minor\n    releases. In particular, regression tests that check for exact matching\n    between graphs are likely to break across minor releases, even though the\n    behavior of the graph should be unchanged and existing checkpoints will\n    still work.\n\n\nFloating point numerical details: The specific floating point values\n    computed by ops may change at any time. Users should rely only on\n    approximate accuracy and numerical stability, not on the specific bits\n    computed. Changes to numerical formulas in minor and patch releases should\n    result in comparable or improved accuracy, with the caveat that in machine\n    learning improved accuracy of specific formulas may result in decreased\n    accuracy for the overall system.\n\n\nRandom numbers: The specific random numbers computed may change at any\n    time. Users should rely only on approximately correct distributions and\n    statistical strength, not the specific bits computed. See the\n    random number generation guide for details.\n\n\nVersion skew in distributed Tensorflow: Running two different versions\n    of TensorFlow in a single cluster is unsupported. There are no guarantees\n    about backwards compatibility of the wire protocol.\n\n\nBugs: We reserve the right to make backwards incompatible behavior\n    (though not API) changes if the current implementation is clearly broken,\n    that is, if it contradicts the documentation or if a well-known and\n    well-defined intended behavior is not properly implemented due to a bug. For\n    example, if an optimizer claims to implement a well-known optimization\n    algorithm but does not match that algorithm due to a bug, then we will fix\n    the optimizer. Our fix may break code relying on the wrong behavior for\n    convergence. We will note such changes in the release notes.\n\n\nUnused API: We reserve the right to make backwards incompatible changes\n    to APIs for which we find no documented uses (by performing audit of\n    TensorFlow usage through GitHub search). Before making any such changes, we\n    will announce our intention to make the change on the\n    announce@ mailing list,\n    providing instructions for how to address any breakages (if applicable), and\n    wait for two weeks to give our community a chance to share their feedback.\n\n\nError behavior: We may replace errors with non-error behavior. For\n    instance, we may change a function to compute a result instead of raising an\n    error, even if that error is documented. We also reserve the right to change\n    the text of error messages. In addition, the type of an error may change\n    unless the exception type for a specific error condition is specified in the\n    documentation.\n\n\n\nCompatibility of SavedModels, graphs and checkpoints\nSavedModel is the preferred serialization format to use in TensorFlow programs.\nSavedModels contain two parts: One or more graphs encoded as `GraphDefs` and a\nCheckpoint. The graphs describe the data flow of ops to be run, and checkpoints\ncontain the saved tensor values of variables in a graph.\nMany TensorFlow users create SavedModels, and load and execute them with a\nlater release of TensorFlow. In compliance with semver,\nSavedModels written with one version of TensorFlow can be loaded and evaluated\nwith a later version of TensorFlow with the same major release.\nWe make additional guarantees for supported SavedModels. We call a SavedModel\nwhich was created using only non-deprecated, non-experimental,\nnon-compatibility APIs in TensorFlow major version `N` a SavedModel supported\nin version `N`. Any SavedModel supported in TensorFlow major version `N` can be\nloaded and executed with TensorFlow major version `N+1`. However, the\nfunctionality required to build or modify such a model may not be available any\nmore, so this guarantee only applies to the unmodified SavedModel.\nWe will endeavor to preserve backwards compatibility as long as possible, so\nthat the serialized files are usable over long periods of time.\nGraphDef compatibility\nGraphs are serialized via the `GraphDef` protocol buffer.  To facilitate\nbackwards incompatible changes to graphs, each `GraphDef` has a version number\nseparate from the TensorFlow version.  For example, `GraphDef` version 17\ndeprecated the `inv` op in favor of `reciprocal`.  The semantics are:\n\n\nEach version of TensorFlow supports an interval of `GraphDef` versions. This\n  interval will be constant across patch releases, and will only grow across\n  minor releases.  Dropping support for a `GraphDef` version will only occur\n  for a major release of TensorFlow (and only aligned with the version support\n  guaranteed for SavedModels).\n\n\nNewly created graphs are assigned the latest `GraphDef` version number.\n\n\nIf a given version of TensorFlow supports the `GraphDef` version of a graph,\n  it will load and evaluate with the same behavior as the TensorFlow version\n  used to generate it (except for floating point numerical details and random\n  numbers as outlined above), regardless of the major version of TensorFlow.\n  In particular, a GraphDef which is compatible with a checkpoint file in one\n  version of TensorFlow (such as is the case in a SavedModel) will remain\n  compatible with that checkpoint in subsequent versions, as long as the\n  GraphDef is supported.\n\n\nNote that this applies only to serialized Graphs in GraphDefs (and\n  SavedModels): Code which reads a checkpoint may not be able to read\n  checkpoints generated by the same code running a different version of\n  TensorFlow.\n\n\nIf the `GraphDef` upper bound is increased to X in a (minor) release, there\n  will be at least six months before the lower bound is increased to X.  For\n  example (we're using hypothetical version numbers here):\n\nTensorFlow 1.2 might support `GraphDef` versions 4 to 7.\nTensorFlow 1.3 could add `GraphDef` version 8 and support versions 4 to 8.\nAt least six months later, TensorFlow 2.0.0 could drop support for\n  versions 4 to 7, leaving version 8 only.\n\n\n\nNote that because major versions of TensorFlow are usually published more than\n  6 months apart, the guarantees for supported SavedModels detailed above are\n  much stronger than the 6 months guarantee for GraphDefs.\nFinally, when support for a `GraphDef` version is dropped, we will attempt to\nprovide tools for automatically converting graphs to a newer supported\n`GraphDef` version.\nGraph and checkpoint compatibility when extending TensorFlow\nThis section is relevant only when making incompatible changes to the `GraphDef`\nformat, such as when adding ops, removing ops, or changing the functionality\nof existing ops.  The previous section should suffice for most users.\n\nBackward and partial forward compatibility\nOur versioning scheme has three requirements:\n\nBackward compatibility to support loading graphs and checkpoints\n    created with older versions of TensorFlow.\nForward compatibility to support scenarios where the producer of a\n    graph or checkpoint is upgraded to a newer version of TensorFlow before\n    the consumer.\nEnable evolving TensorFlow in incompatible ways. For example, removing ops,\n    adding attributes, and removing attributes.\n\nNote that while the `GraphDef` version mechanism is separate from the TensorFlow\nversion, backwards incompatible changes to the `GraphDef` format are still\nrestricted by Semantic Versioning.  This means functionality can only be removed\nor changed between `MAJOR` versions of TensorFlow (such as `1.7` to `2.0`).\nAdditionally, forward compatibility is enforced within Patch releases (`1.x.1`\nto `1.x.2` for example).\nTo achieve backward and forward compatibility and to know when to enforce changes\nin formats, graphs and checkpoints have metadata that describes when they\nwere produced. The sections below detail the TensorFlow implementation and\nguidelines for evolving `GraphDef` versions.\nIndependent data version schemes\nThere are different data versions for graphs and checkpoints. The two data\nformats evolve at different rates from each other and also at different rates\nfrom TensorFlow. Both versioning systems are defined in\ncore/public/version.h.\nWhenever a new version is added, a note is added to the header detailing what\nchanged and the date.\nData, producers, and consumers\nWe distinguish between the following kinds of data version information:\n* producers: binaries that produce data.  Producers have a version\n  (`producer`) and a minimum consumer version that they are compatible with\n  (`min_consumer`).\n* consumers: binaries that consume data.  Consumers have a version\n  (`consumer`) and a minimum producer version that they are compatible with\n  (`min_producer`).\nEach piece of versioned data has a VersionDef\nversions\nfield which records the `producer` that made the data, the `min_consumer`\nthat it is compatible with, and a list of `bad_consumers` versions that are\ndisallowed.\nBy default, when a producer makes some data, the data inherits the producer's\n`producer` and `min_consumer` versions. `bad_consumers` can be set if specific\nconsumer versions are known to contain bugs and must be avoided. A consumer can\naccept a piece of data if the following are all true:\n\n`consumer` >= data's `min_consumer`\ndata's `producer` >= consumer's `min_producer`\n`consumer` not in data's `bad_consumers`\n\nSince both producers and consumers come from the same TensorFlow code base,\ncore/public/version.h\ncontains a main data version which is treated as either `producer` or\n`consumer` depending on context and both `min_consumer` and `min_producer`\n(needed by producers and consumers, respectively). Specifically,\n\nFor `GraphDef` versions, we have `TF_GRAPH_DEF_VERSION`,\n    `TF_GRAPH_DEF_VERSION_MIN_CONSUMER`, and\n    `TF_GRAPH_DEF_VERSION_MIN_PRODUCER`.\nFor checkpoint versions, we have `TF_CHECKPOINT_VERSION`,\n    `TF_CHECKPOINT_VERSION_MIN_CONSUMER`, and\n    `TF_CHECKPOINT_VERSION_MIN_PRODUCER`.\n\nAdd a new attribute with default to an existing op\nFollowing the guidance below gives you forward compatibility only if the set of\nops has not changed:\n\nIf forward compatibility is desired, set `strip_default_attrs` to `True`\n   while exporting the model using either the\n   `tf.saved_model.SavedModelBuilder.add_meta_graph_and_variables`\n   and `tf.saved_model.SavedModelBuilder.add_meta_graph`\n   methods of the `SavedModelBuilder` class, or\n   `tf.estimator.Estimator.export_saved_model`\nThis strips off the default valued attributes at the time of\n   producing/exporting the models. This makes sure that the exported\n   `tf.MetaGraphDef` does not contain the new op-attribute when the default\n   value is used.\nHaving this control could allow out-of-date consumers (for example, serving\n   binaries that lag behind training binaries) to continue loading the models\n   and prevent interruptions in model serving.\n\nEvolving GraphDef versions\nThis section explains how to use this versioning mechanism to make different\ntypes of changes to the `GraphDef` format.\nAdd an op\nAdd the new op to both consumers and producers at the same time, and do not\nchange any `GraphDef` versions. This type of change is automatically\nbackward compatible, and does not impact forward compatibility plan since\nexisting producer scripts will not suddenly use the new functionality.\nAdd an op and switch existing Python wrappers to use it\n\nImplement new consumer functionality and increment the `GraphDef` version.\nIf it is possible to make the wrappers use the new functionality only in\n    cases that did not work before, the wrappers can be updated now.\nChange Python wrappers to use the new functionality. Do not increment\n    `min_consumer`, since models that do not use this op should not break.\n\nRemove or restrict an op's functionality\n\nFix all producer scripts (not TensorFlow itself) to not use the banned op or\n    functionality.\nIncrement the `GraphDef` version and implement new consumer functionality\n    that bans the removed op or functionality for GraphDefs at the new version\n    and above. If possible, make TensorFlow stop producing `GraphDefs` with the\n    banned functionality. To do so, add the\n    REGISTER_OP(...).Deprecated(deprecated_at_version,\n    message).\nWait for a major release for backward compatibility purposes.\nIncrease `min_producer` to the GraphDef version from (2) and remove the\n    functionality entirely.\n\nChange an op's functionality\n\nAdd a new similar op named `SomethingV2` or similar and go through the\n    process of adding it and switching existing Python wrappers to use it.\n    To ensure forward compatibility use the checks suggested in\n    compat.py\n    when changing the Python wrappers.\nRemove the old op (Can only take place with a major version change due to\n    backward compatibility).\nIncrease `min_consumer` to rule out consumers with the old op, add back the\n    old op as an alias for `SomethingV2`, and go through the process to switch\n    existing Python wrappers to use it.\nGo through the process to remove `SomethingV2`.\n\nBan a single unsafe consumer version\n\nBump the `GraphDef` version and add the bad version to `bad_consumers` for\n    all new GraphDefs. If possible, add to `bad_consumers` only for GraphDefs\n    which contain a certain op or similar.\nIf existing consumers have the bad version, push them out as soon as\n",
    "tag": "tensorflow"
  },
  {
    "title": "Analyze `tf.data` performance with the TF Profiler",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/guide/data_performance_analysis.md",
    "content": "Analyze `tf.data` performance with the TF Profiler\nOverview\nThis guide assumes familiarity with the TensorFlow\nProfiler and\ntf.data. It aims to provide step by\nstep instructions with examples to help users diagnose and fix input pipeline\nperformance issues.\nTo begin, collect a profile of your TensorFlow job. Instructions on how to do so\nare available for\nCPUs/GPUs\nand\nCloud TPUs.\n\nThe analysis workflow detailed below focuses on the trace viewer tool in the\nProfiler. This tool displays a timeline that shows the duration of ops executed\nby your TensorFlow program and allows you to identify which ops take the longest\nto execute. For more information on the trace viewer, check out\nthis section of the TF\nProfiler guide. In general, `tf.data` events will appear on the host CPU\ntimeline.\nAnalysis Workflow\nPlease follow the workflow below. If you have feedback to help us improve it,\nplease\ncreate a github issue\nwith the label \u201ccomp:data\u201d.\n1. Is your `tf.data` pipeline producing data fast enough?\nBegin by ascertaining whether the input pipeline is the bottleneck for your\nTensorFlow program.\nTo do so, look for `IteratorGetNext::DoCompute` ops in the trace viewer. In\ngeneral, you expect to see these at the start of a step. These slices represent\nthe time it takes for your input pipeline to yield a batch of elements when it\nis requested. If you\u2019re using keras or iterating over your dataset in a\n`tf.function`, these should be found in `tf_data_iterator_get_next` threads.\nNote that if you\u2019re using a\ndistribution strategy,\nyou may see `IteratorGetNextAsOptional::DoCompute` events instead of\n`IteratorGetNext::DoCompute`(as of TF 2.3).\n\nIf the calls return quickly (<= 50 us), this means that your data is\navailable when it is requested. The input pipeline is not your bottleneck; see\nthe Profiler guide for more generic\nperformance analysis tips.\n\nIf the calls return slowly, `tf.data` is unable to keep up with the\nconsumer\u2019s requests. Continue to the next section.\n2. Are you prefetching data?\nThe best practice for input pipeline performance is to insert a\n`tf.data.Dataset.prefetch` transformation at the end of your `tf.data` pipeline.\nThis transformation overlaps the input pipeline\u2019s preprocessing computation with\nthe next step of model computation and is required for optimal input pipeline\nperformance when training your model. If you\u2019re prefetching data, you should see\na `Iterator::Prefetch` slice on the same thread as the\n`IteratorGetNext::DoCompute` op.\n\nIf you don\u2019t have a `prefetch` at the end of your pipeline, you should add\none. For more information about `tf.data` performance recommendations, see the\ntf.data performance guide.\nIf you\u2019re already prefetching data, and the input pipeline is still your\nbottleneck, continue to the next section to further analyze performance.\n3. Are you reaching high CPU utilization?\n`tf.data` achieves high throughput by trying to make the best possible use of\navailable resources. In general, even when running your model on an accelerator\nlike a GPU or TPU, the `tf.data` pipelines are run on the CPU. You can check\nyour utilization with tools like sar and\nhtop, or in the\ncloud monitoring console if you\u2019re running on GCP.\nIf your utilization is low, this suggests that your input pipeline may not\nbe taking full advantage of the host CPU. You should consult the\ntf.data performance guide\nfor best practices. If you have applied the best practices and utilization and\nthroughput remain low, continue to Bottleneck analysis\nbelow.\nIf your utilization is approaching the resource limit, in order to improve\nperformance further, you need to either improve the efficiency of your input\npipeline (for example, avoiding unnecessary computation) or offload computation.\nYou can improve the efficiency of your input pipeline by avoiding unnecessary\ncomputation in `tf.data`. One way of doing this is inserting a\ntf.data.Dataset.cache\ntransformation after computation-intensive work if your data fits into memory;\nthis reduces computation at the cost of increased memory usage. Additionally,\ndisabling intra-op parallelism in `tf.data` has the potential to increase\nefficiency by > 10%, and can be done by setting the following option on your\ninput pipeline:\n`python\ndataset = ...\noptions = tf.data.Options()\noptions.experimental_threading.max_intra_op_parallelism = 1\ndataset = dataset.with_options(options)`\n4. Bottleneck Analysis\nThe following section walks through how to read `tf.data` events in the trace\nviewer to understand where the bottleneck is and possible mitigation strategies.\nUnderstanding `tf.data` events in the Profiler\nEach `tf.data` event in the Profiler has the name `Iterator::<Dataset>`, where\n`<Dataset>` is the name of the dataset source or transformation. Each event also\nhas the long name `Iterator::<Dataset_1>::...::<Dataset_n>`, which you can see\nby clicking on the `tf.data` event. In the long name, `<Dataset_n>` matches\n`<Dataset>` from the (short) name, and the other datasets in the long name\nrepresent downstream transformations.\n\nFor example, the above screenshot was generated from the following code:\n`python\ndataset = tf.data.Dataset.range(10)\ndataset = dataset.map(lambda x: x)\ndataset = dataset.repeat(2)\ndataset = dataset.batch(5)`\nHere, the `Iterator::Map` event has the long name\n`Iterator::BatchV2::FiniteRepeat::Map`. Note that the datasets name may differ\nslightly from the python API (for example, FiniteRepeat instead of Repeat), but\nshould be intuitive enough to parse.\nSynchronous and asynchronous transformations\nFor synchronous `tf.data` transformations (such as `Batch` and `Map`), you will\nsee events from upstream transformations on the same thread. In the above\nexample, since all the transformations used are synchronous, all the events\nappear on the same thread.\nFor asynchronous transformations (such as `Prefetch`, `ParallelMap`,\n`ParallelInterleave` and `MapAndBatch`) events from upstream transformations\nwill be on a different thread. In such cases, the \u201clong name\u201d can help you\nidentify which transformation in a pipeline an event corresponds to.\n\nFor example, the above screenshot was generated from the following code:\n`python\ndataset = tf.data.Dataset.range(10)\ndataset = dataset.map(lambda x: x)\ndataset = dataset.repeat(2)\ndataset = dataset.batch(5)\ndataset = dataset.prefetch(1)`\nHere, the `Iterator::Prefetch` events are on the `tf_data_iterator_get_next`\nthreads. Since `Prefetch` is asynchronous, its input events (`BatchV2`) will be\non a different thread, and can be located by searching for the long name\n`Iterator::Prefetch::BatchV2`. In this case, they are on the\n`tf_data_iterator_resource` thread. From its long name, you can deduce that\n`BatchV2` is upstream of `Prefetch`. Furthermore, the `parent_id` of the\n`BatchV2` event will match the ID of the `Prefetch` event.\nIdentifying the bottleneck\nIn general, to identify the bottleneck in your input pipeline, walk the input\npipeline from the outermost transformation all the way to the source. Starting\nfrom the final transformation in your pipeline, recurse into upstream\ntransformations until you find a slow transformation or reach a source dataset,\nsuch as `TFRecord`. In the example above, you would start from `Prefetch`, then\nwalk upstream to `BatchV2`, `FiniteRepeat`, `Map`, and finally `Range`.\nIn general, a slow transformation corresponds to one whose events are long, but\nwhose input events are short. Some examples follow below.\nNote that the final (outermost) transformation in most host input pipelines is\nthe `Iterator::Model` event. The Model transformation is introduced\nautomatically by the `tf.data` runtime and is used for instrumenting and\nautotuning the input pipeline performance.\nIf your job is using a\ndistribution strategy,\nthe trace viewer will contain additional events that correspond to the device\ninput pipeline. The outermost transformation of the device pipeline (nested\nunder `IteratorGetNextOp::DoCompute` or\n`IteratorGetNextAsOptionalOp::DoCompute`) will be an `Iterator::Prefetch` event\nwith an upstream `Iterator::Generator` event. You can find the corresponding\nhost pipeline by searching for `Iterator::Model` events.\nExample 1\n\nThe above screenshot is generated from the following input pipeline:\n`python\ndataset = tf.data.TFRecordDataset(filename)\ndataset = dataset.map(parse_record)\ndataset = dataset.batch(32)\ndataset = dataset.repeat()`\nIn the screenshot, observe that (1) `Iterator::Map` events are long, but (2) its\ninput events (`Iterator::FlatMap`) return quickly. This suggests that the\nsequential Map transformation is the bottleneck.\nNote that in the screenshot, the `InstantiatedCapturedFunction::Run` event\ncorresponds to the time it takes to execute the map function.\nExample 2\n\nThe above screenshot is generated from the following input pipeline:\n`python\ndataset = tf.data.TFRecordDataset(filename)\ndataset = dataset.map(parse_record, num_parallel_calls=2)\ndataset = dataset.batch(32)\ndataset = dataset.repeat()`\nThis example is similar to the above, but uses ParallelMap instead of Map. We\nnotice here that (1) `Iterator::ParallelMap` events are long, but (2) its input\nevents `Iterator::FlatMap` (which are on a different thread, since ParallelMap\nis asynchronous) are short. This suggests that the ParallelMap transformation is\nthe bottleneck.\nAddressing the bottleneck\nSource datasets\nIf you\u2019ve identified a dataset source as the bottleneck, such as reading from\nTFRecord files, you can improve performance by parallelizing data extraction. To\ndo so, ensure that your data is sharded across multiple files and use\n`tf.data.Dataset.interleave` with the `num_parallel_calls` parameter set to\n`tf.data.AUTOTUNE`. If determinism is not important to your\nprogram, you can further improve performance by setting the\n`deterministic=False` flag on `tf.data.Dataset.interleave` as of TF 2.2. For\nexample, if you\u2019re reading from TFRecords, you can do the following:\n`python\ndataset = tf.data.Dataset.from_tensor_slices(filenames)\ndataset = dataset.interleave(tf.data.TFRecordDataset,\n  num_parallel_calls=tf.data.AUTOTUNE,\n  deterministic=False)`\nNote that sharded files should be reasonably large to amortize the overhead of\nopening a file. For more details on parallel data extraction, see\nthis section\nof the `tf.data` performance guide.\nTransformation datasets\nIf you\u2019ve identified an intermediate `tf.data` transformation as the bottleneck,\nyou can address it by parallelizing the transformation or\ncaching the computation\nif your data fits into memory and it is appropriate. Some transformations such\nas `Map` have parallel counterparts; the\ntf.data performance guide demonstrates\nhow to parallelize these. Other transformations, such as `Filter`, `Unbatch`,\nand `Batch` are inherently sequential; you can parallelize them by introducing\n\u201couter parallelism\u201d. For example, supposing your input pipeline initially looks\nlike the following, with `Batch` as the bottleneck:\n`python\nfilenames = tf.data.Dataset.list_files(file_path, shuffle=is_training)\ndataset = filenames_to_dataset(filenames)\ndataset = dataset.batch(batch_size)`\nYou can introduce \u201couter parallelism\u201d by running multiple copies of the input\npipeline over sharded inputs and combining the results:\n```python\nfilenames = tf.data.Dataset.list_files(file_path, shuffle=is_training)\ndef make_dataset(shard_index):\n  filenames = filenames.shard(NUM_SHARDS, shard_index)\n  dataset = filenames_to_dataset(filenames)\n  Return dataset.batch(batch_size)\nindices = tf.data.Dataset.range(NUM_SHARDS)\ndataset = indices.interleave(make_dataset,\n                             num_parallel_calls=tf.data.AUTOTUNE)\ndataset = dataset.prefetch(tf.data.AUTOTUNE)\n```\nAdditional resources\n\ntf.data performance guide\n    on how to write performance `tf.data` input pipelines\nInside TensorFlow video: tf.data best practices \nProfiler guide\n",
    "tag": "tensorflow"
  },
  {
    "title": "Optimize TensorFlow performance using the Profiler",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/guide/profiler.md",
    "content": "Optimize TensorFlow performance using the Profiler\n[TOC]\nThis guide demonstrates how to use the tools available with the TensorFlow\nProfiler to track the performance of your TensorFlow models. You will learn how\nto understand how your model performs on the host (CPU), the device (GPU), or on\na combination of both the host and device(s).\nProfiling helps understand the hardware resource consumption (time and memory)\nof the various TensorFlow operations (ops) in your model and resolve performance\nbottlenecks and, ultimately, make the model execute faster.\nThis guide will walk you through how to install the Profiler, the various tools\navailable, the different modes of how the Profiler collects performance data,\nand some recommended best practices to optimize model performance.\nIf you want to profile your model performance on Cloud TPUs, refer to the\nCloud TPU guide.\nInstall the Profiler and GPU prerequisites\nInstall the Profiler plugin for TensorBoard with pip. Note that the Profiler\nrequires the latest versions of TensorFlow and TensorBoard (>=2.2).\n`shell\npip install -U tensorboard_plugin_profile`\nTo profile on the GPU, you must:\n\nMeet the NVIDIA\u00ae GPU drivers and CUDA\u00ae Toolkit requirements listed on\n    TensorFlow GPU support software requirements.\n\nMake sure the\n    NVIDIA\u00ae CUDA\u00ae Profiling Tools Interface\n    (CUPTI) exists on the path:\n`shell\n/sbin/ldconfig -N -v $(sed 's/:/ /g' <<< $LD_LIBRARY_PATH) | \\\ngrep libcupti`\n\n\nIf you don't have CUPTI on the path, prepend its installation directory to the\n`$LD_LIBRARY_PATH` environment variable by running:\n`shell\nexport LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH`\nThen, run the `ldconfig` command above again to verify that the CUPTI library is\nfound.\nResolve privilege issues\nWhen you run profiling with CUDA\u00ae Toolkit in a Docker environment or on Linux,\nyou may encounter issues related to insufficient CUPTI privileges\n(`CUPTI_ERROR_INSUFFICIENT_PRIVILEGES`). Go to the\nNVIDIA Developer Docs{:.external}\nto learn more about how you can resolve these issues on Linux.\nTo resolve CUPTI privilege issues in a Docker environment, run\n`shell\ndocker run option '--privileged=true'`\n\nProfiler tools\nAccess the Profiler from the Profile tab in TensorBoard, which appears only\nafter you have captured some model data.\nNote: The Profiler requires internet access to load the\nGoogle Chart libraries.\nSome charts and tables may be missing if you run TensorBoard entirely offline on\nyour local machine, behind a corporate firewall, or in a data center.\nThe Profiler has a selection of tools to help with performance analysis:\n\nOverview Page\nInput Pipeline Analyzer\nTensorFlow Stats\nTrace Viewer\nGPU Kernel Stats\nMemory Profile Tool\nPod Viewer\n\n\nOverview page\nThe overview page provides a top level view of how your model performed during a\nprofile run. The page shows you an aggregated overview page for your host and\nall devices, and some recommendations to improve your model training\nperformance. You can also select individual hosts in the Host dropdown.\nThe overview page displays data as follows:\n\n\n\nPerformance Summary: Displays a high-level summary of your model\n    performance. The performance summary has two parts:\n\n\nStep-time breakdown: Breaks down the average step time into multiple\n    categories of where time is spent:\n\nCompilation: Time spent compiling kernels.\nInput: Time spent reading input data.\nOutput: Time spent reading output data.\nKernel launch: Time spent by the host to launch kernels\nHost compute time..\nDevice-to-device communication time.\nOn-device compute time.\nAll others, including Python overhead.\n\n\n\nDevice compute precisions - Reports the percentage of device compute\n    time that uses 16 and 32-bit computations.\n\n\n\n\nStep-time Graph: Displays a graph of device step time (in milliseconds)\n    over all the steps sampled. Each step is broken into the multiple categories\n    (with different colors) of where time is spent. The red area corresponds to\n    the portion of the step time the devices were sitting idle waiting for input\n    data from the host. The green area shows how much of time the device was\n    actually working.\n\n\nTop 10 TensorFlow operations on device (e.g. GPU): Displays the\n    on-device ops that ran the longest.\nEach row displays an op's self time (as the percentage of time taken by all\nops), cumulative time, category, and name.\n\n\nRun Environment: Displays a high-level summary of the model run\n    environment including:\n\nNumber of hosts used.\nDevice type (GPU/TPU).\nNumber of device cores.\n\n\n\nRecommendation for Next Step: Reports when a model is input bound and\n    recommends tools you can use to locate and resolve model performance\n    bottlenecks.\n\n\n\nInput pipeline analyzer\nWhen a TensorFlow program reads data from a file it begins at the top of the\nTensorFlow graph in a pipelined manner. The read process is divided into\nmultiple data processing stages connected in series, where the output of one\nstage is the input to the next one. This system of reading data is called the\ninput pipeline.\nA typical pipeline for reading records from files has the following stages:\n\nFile reading.\nFile preprocessing (optional).\nFile transfer from the host to the device.\n\nAn inefficient input pipeline can severely slow down your application. An\napplication is considered input bound when it spends a significant portion\nof time in the input pipeline. Use the insights obtained from the input pipeline\nanalyzer to understand where the input pipeline is inefficient.\nThe input pipeline analyzer tells you immediately whether your program is input\nbound and walks you through device- and host-side analysis to debug performance\nbottlenecks at any stage in the input pipeline.\nCheck the guidance on input pipeline performance for recommended best practices\nto optimize your data input pipelines.\nInput pipeline dashboard\nTo open the input pipeline analyzer, select Profile, then select\ninput_pipeline_analyzer from the Tools dropdown.\n\nThe dashboard contains three sections:\n\nSummary: Summarizes the overall input pipeline with information on\n    whether your application is input bound and, if so, by how much.\nDevice-side analysis: Displays detailed, device-side analysis results,\n    including the device step-time and the range of device time spent waiting\n    for input data across cores at each step.\nHost-side analysis: Shows a detailed analysis on the host side,\n    including a breakdown of input processing time on the host.\n\nInput pipeline summary\nThe Summary reports if your program is input bound by presenting the\npercentage of device time spent on waiting for input from the host. If you are\nusing a standard input pipeline that has been instrumented, the tool reports\nwhere most of the input processing time is spent.\nDevice-side analysis\nThe device-side analysis provides insights on time spent on the device versus on\nthe host and how much device time was spent waiting for input data from the\nhost.\n\nStep time plotted against step number: Displays a graph of device step\n    time (in milliseconds) over all the steps sampled. Each step is broken into\n    the multiple categories (with different colors) of where time is spent. The\n    red area corresponds to the portion of the step time the devices were\n    sitting idle waiting for input data from the host. The green area shows how\n    much of the time the device was actually working.\nStep time statistics: Reports the average, standard deviation, and range\n    ([minimum, maximum]) of the device step time.\n\nHost-side analysis\nThe host-side analysis reports a breakdown of the input processing time (the\ntime spent on `tf.data` API ops) on the host into several categories:\n\nReading data from files on demand: Time spent on reading data from files\n    without caching, prefetching, and interleaving.\nReading data from files in advance: Time spent reading files, including\n    caching, prefetching, and interleaving.\nData preprocessing: Time spent on preprocessing ops, such as image\n    decompression.\nEnqueuing data to be transferred to device: Time spent putting data into\n    an infeed queue before transferring the data to the device.\n\nExpand Input Op Statistics to inspect the statistics for individual input\nops and their categories broken down by execution time.\n\nA source data table will appear with each entry containing the following\ninformation:\n\nInput Op: Shows the TensorFlow op name of the input op.\nCount: Shows the total number of instances of op execution during the\n    profiling period.\nTotal Time (in ms): Shows the cumulative sum of time spent on each of\n    those instances.\nTotal Time %: Shows the total time spent on an op as a fraction of the\n    total time spent in input processing.\nTotal Self Time (in ms): Shows the cumulative sum of the self time spent\n    on each of those instances. The self time here measures the time spent\n    inside the function body, excluding the time spent in the function it calls.\nTotal Self Time %. Shows the total self time as a fraction of the total\n    time spent on input processing.\nCategory. Shows the processing category of the input op.\n\n\nTensorFlow stats\nThe TensorFlow Stats tool displays the performance of every TensorFlow op (op)\nthat is executed on the host or device during a profiling session.\n\nThe tool displays performance information in two panes:\n\n\nThe upper pane displays up to four pie charts:\n\nThe distribution of self-execution time of each op on the host.\nThe distribution of self-execution time of each op type on the host.\nThe distribution of self-execution time of each op on the device.\nThe distribution of self-execution time of each op type on the device.\n\n\n\nThe lower pane shows a table that reports data about TensorFlow ops with one\n    row for each op and one column for each type of data (sort columns by\n    clicking the heading of the column). Click the Export as CSV button on\n    the right side of the upper pane to export the data from this table as a CSV\n    file.\nNote that:\n\n\nIf any ops have child ops:\n\nThe total \"accumulated\" time of an op includes the time spent inside\n    the child ops.\nThe total \"self\" time of an op does not include the time spent\n    inside the child ops.\n\n\n\nIf an op executes on the host:\n\nThe percentage of the total self-time on device incurred by the op\n    on will be 0.\nThe cumulative percentage of the total self-time on device up to and\n    including this op will be 0.\n\n\n\nIf an op executes on the device:\n\nThe percentage of the total self-time on host incurred by this op\n    will be 0.\nThe cumulative percentage of the total self-time on host up to and\n    including this op will be 0.\n\n\n\n\n\nYou can choose to include or exclude Idle time in the pie charts and table.\n\nTrace viewer\nThe trace viewer displays a timeline that shows:\n\nDurations for the ops that were executed by your TensorFlow model\nWhich part of the system (host or device) executed an op. Typically, the\n    host executes input operations, preprocesses training data and transfers it\n    to the device, while the device executes the actual model training\n\nThe trace viewer allows you to identify performance problems in your model, then\ntake steps to resolve them. For example, at a high level, you can identify\nwhether input or model training is taking the majority of the time. Drilling\ndown, you can identify which ops take the longest to execute. Note that the\ntrace viewer is limited to 1 million events per device.\nTrace viewer interface\nWhen you open the trace viewer, it appears displaying your most recent run:\n\nThis screen contains the following main elements:\n\nTimeline pane: Shows ops that the device and the host executed over\n    time.\nDetails pane: Shows additional information for ops selected in the\n    Timeline pane.\n\nThe Timeline pane contains the following elements:\n\nTop bar: Contains various auxiliary controls.\nTime axis: Shows time relative to the beginning of the trace.\nSection and track labels: Each section contains multiple tracks and has\n    a triangle on the left that you can click to expand and collapse the\n    section. There is one section for every processing element in the system.\nTool selector: Contains various tools for interacting with the trace\n    viewer such as Zoom, Pan, Select, and Timing. Use the Timing tool to mark a\n    time interval.\nEvents: These show the time during which an op was executed or the\n    duration of meta-events, such as training steps.\n\nSections and tracks\nThe trace viewer contains the following sections:\n\nOne section for each device node, labeled with the number of the device\n    chip and the device node within the chip (for example, `/device:GPU:0 (pid\n    0)`). Each device node section contains the following tracks:\nStep: Shows the duration of the training steps that were running on\n    the device\nTensorFlow Ops: Shows the ops executed on the device\nXLA Ops: Shows XLA operations\n    (ops) that ran on the device if XLA is the compiler used (each\n    TensorFlow op is translated into one or several XLA ops. The XLA\n    compiler translates the XLA ops into code that runs on the device).\n\n\nOne section for threads running on the host machine's CPU, labeled\n    \"Host Threads\". The section contains one track for each CPU thread. Note\n    that you can ignore the information displayed alongside the section labels.\n\nEvents\nEvents within the timeline are displayed in different colors; the colors\nthemselves have no specific meaning.\nThe trace viewer can also display traces of Python function calls in your\nTensorFlow program. If you use the `tf.profiler.experimental.start` API, you can\nenable Python tracing by using the `ProfilerOptions` namedtuple when starting\nprofiling. Alternatively, if you use the sampling mode for profiling, you can\nselect the level of tracing by using the dropdown options in the Capture\nProfile dialog.\n\n\nGPU kernel stats\nThis tool shows performance statistics and the originating op for every GPU\naccelerated kernel.\n\nThe tool displays information in two panes:\n\n\nThe upper pane displays a pie chart which shows the CUDA kernels that have\n    the highest total time elapsed.\n\n\nThe lower pane displays a table with the following data for each unique\n    kernel-op pair:\n\nA rank in descending order of total elapsed GPU duration grouped by\n    kernel-op pair.\nThe name of the launched kernel.\nThe number of GPU registers used by the kernel.\nThe total size of shared (static + dynamic shared) memory used in bytes.\nThe block dimension expressed as `blockDim.x, blockDim.y, blockDim.z`.\nThe grid dimensions expressed as `gridDim.x, gridDim.y, gridDim.z`.\nWhether the op is eligible to use\n    Tensor Cores.\nWhether the kernel contains Tensor Core instructions.\nThe name of the op that launched this kernel.\nThe number of occurrences of this kernel-op pair.\nThe total elapsed GPU time in microseconds.\nThe average elapsed GPU time in microseconds.\nThe minimum elapsed GPU time in microseconds.\nThe maximum elapsed GPU time in microseconds.\n\n\n\n\nMemory profile tool {: id = 'memory_profile_tool'}\nThe Memory Profile tool monitors the memory usage of your device during the\nprofiling interval. You can use this tool to:\n\nDebug out of memory (OOM) issues by pinpointing peak memory usage and the\n    corresponding memory allocation to TensorFlow ops. You can also debug OOM\n    issues that may arise when you run\n    multi-tenancy inference.\nDebug memory fragmentation issues.\n\nThe memory profile tool displays data in three sections:\n\nMemory Profile Summary\nMemory Timeline Graph\nMemory Breakdown Table\n\nMemory profile summary\nThis section displays a high-level summary of the memory profile of your\nTensorFlow program as shown below:\n\nThe memory profile summary has six fields:\n\nMemory ID: Dropdown which lists all available device memory systems.\n    Select the memory system you want to view from the dropdown.\n#Allocation: The number of memory allocations made during the profiling\n    interval.\n#Deallocation: The number of memory deallocations in the profiling\n    interval\nMemory Capacity: The total capacity (in GiBs) of the memory system that\n    you select.\nPeak Heap Usage: The peak memory usage (in GiBs) since the model started\n    running.\nPeak Memory Usage: The peak memory usage (in GiBs) in the profiling\n    interval. This field contains the following sub-fields:\nTimestamp: The timestamp of when the peak memory usage occurred on\n    the Timeline Graph.\nStack Reservation: Amount of memory reserved on the stack (in GiBs).\nHeap Allocation: Amount of memory allocated on the heap (in GiBs).\nFree Memory: Amount of free memory (in GiBs). The Memory Capacity is\n    the sum total of the Stack Reservation, Heap Allocation, and Free\n    Memory.\nFragmentation: The percentage of fragmentation (lower is better). It\n    is calculated as a percentage of `(1 - Size of the largest chunk of free\n    memory / Total free memory)`.\n\n\n\nMemory timeline graph\nThis section displays a plot of the memory usage (in GiBs) and the percentage of\nfragmentation versus time (in ms).\n\nThe X-axis represents the timeline (in ms) of the profiling interval. The Y-axis\non the left represents the memory usage (in GiBs) and the Y-axis on the right\nrepresents the percentage of fragmentation. At each point in time on the X-axis,\nthe total memory is broken down into three categories: stack (in red), heap (in\norange), and free (in green). Hover over a specific timestamp to view the\ndetails about the memory allocation/deallocation events at that point like\nbelow:\n\nThe pop-up window displays the following information:\n\ntimestamp(ms): The location of the selected event on the timeline.\nevent: The type of event (allocation or deallocation).\nrequested_size(GiBs): The amount of memory requested. This will be a\n    negative number for deallocation events.\nallocation_size(GiBs): The actual amount of memory allocated. This will\n    be a negative number for deallocation events.\ntf_op: The TensorFlow op that requests the allocation/deallocation.\nstep_id: The training step in which this event occurred.\nregion_type: The data entity type that this allocated memory is for.\n    Possible values are `temp` for temporaries, `output` for activations and\n    gradients, and `persist`/`dynamic` for weights and constants.\ndata_type: The tensor element type (e.g., uint8 for 8-bit unsigned\n    integer).\ntensor_shape: The shape of the tensor being allocated/deallocated.\nmemory_in_use(GiBs): The total memory that is in use at this point of\n    time.\n\nMemory breakdown table\nThis table shows the active memory allocations at the point of peak memory usage\nin the profiling interval.\n\nThere is one row for each TensorFlow Op and each row has the following columns:\n\nOp Name: The name of the TensorFlow op.\nAllocation Size (GiBs): The total amount of memory allocated to this op.\nRequested Size (GiBs): The total amount of memory requested for this op.\nOccurrences: The number of allocations for this op.\nRegion type: The data entity type that this allocated memory is for.\n    Possible values are `temp` for temporaries, `output` for activations and\n    gradients, and `persist`/`dynamic` for weights and constants.\nData type: The tensor element type.\nShape: The shape of the allocated tensors.\n\nNote: You can sort any column in the table and also filter rows by op name.\n\nPod viewer\nThe Pod Viewer tool shows the breakdown of a training step across all workers.\n\n\nThe upper pane has a slider for selecting the step number.\nThe lower pane displays a stacked column chart. This is a high level view of\n    broken down step-time categories placed atop one another. Each stacked\n    column represents a unique worker.\nWhen you hover over a stacked column, the card on the left-hand side shows\n    more details about the step breakdown.\n\n\ntf.data bottleneck analysis\nWarning: This tool is experimental. Please open a\nGitHub Issue if the analysis\nresult seems incorrect.\nThe `tf.data` bottleneck analysis tool automatically detects bottlenecks in\n`tf.data` input pipelines in your program and provides recommendations on how to\nfix them. It works with any program using `tf.data` regardless of the platform\n(CPU/GPU/TPU). Its analysis and recommendations are based on this\nguide.\nIt detects a bottleneck by following these steps:\n\nFind the most input bound host.\nFind the slowest execution of a `tf.data` input pipeline.\nReconstruct the input pipeline graph from the profiler trace.\nFind the critical path in the input pipeline graph.\nIdentify the slowest transformation on the critical path as a bottleneck.\n\nThe UI is divided into three sections: Performance Analysis Summary,\nSummary of All Input Pipelines and Input Pipeline Graph.\nPerformance analysis summary\n\nThis section provides the summary of the analysis. It reports on slow `tf.data`\ninput pipelines detected in the profile. This section also shows the most input\nbound host and its slowest input pipeline with the max latency. Most\nimportantly, it identifies which part of the input pipeline is the bottleneck\nand how to fix it. The bottleneck information is provided with the iterator type\nand its long name.\nHow to read tf.data iterator's long name\nA long name is formatted as `Iterator::<Dataset_1>::...::<Dataset_n>`. In the\nlong name, `<Dataset_n>` matches the iterator type and the other datasets in the\nlong name represent downstream transformations.\nFor example, consider the following input pipeline dataset:\n`python\ndataset = tf.data.Dataset.range(10).map(lambda x: x).repeat(2).batch(5)`\nThe long names for the iterators from the above dataset will be:\nIterator Type | Long Name\n:------------ | :----------------------------------\nRange         | Iterator::Batch::Repeat::Map::Range\nMap           | Iterator::Batch::Repeat::Map\nRepeat        | Iterator::Batch::Repeat\nBatch         | Iterator::Batch\nSummary of all input pipelines\n\nThis section provides the summary of all input pipelines across all hosts.\nTypically there is one input pipeline. When using the distribution strategy,\nthere is one host input pipeline running the program's `tf.data` code and\nmultiple device input pipelines retrieving data from the host input pipeline and\ntransferring it to the devices.\nFor each input pipeline, it shows the statistics of its execution time. A call\nis counted as slow if it takes longer than 50 \u03bcs.\nInput pipeline graph\n\nThis section shows the input pipeline graph with the execution time information.\nYou can use \"Host\" and \"Input Pipeline\" to choose which host and input pipeline\nto see. Executions of the input pipeline are sorted by the execution time in\ndescending order which you can choose using the Rank dropdown.\n\nThe nodes on the critical path have bold outlines. The bottleneck node, which is\nthe node with the longest self time on the critical path, has a red outline. The\nother non-critical nodes have gray dashed outlines.\nIn each node,Start Time indicates the start time of the execution. The same\nnode may be executed multiple times, for example, if there is a `Batch` op in\nthe input pipeline. If it is executed multiple times, it is the start time of\nthe first execution.\nTotal Duration is the wall time of the execution. If it is executed multiple\ntimes, it is the sum of the wall times of all executions.\nSelf Time is Total Time without the overlapped time with its immediate\nchild nodes.\n\"# Calls\" is the number of times the input pipeline is executed.\n\nCollect performance data\nThe TensorFlow Profiler collects host activities and GPU traces of your\nTensorFlow model. You can configure the Profiler to collect performance data\nthrough either the programmatic mode or the sampling mode.\nProfiling APIs\nYou can use the following APIs to perform profiling.\n\n\nProgrammatic mode using the TensorBoard Keras Callback\n    (`tf.keras.callbacks.TensorBoard`)\n```python\nProfile from batches 10 to 15\ntb_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,\n                                             profile_batch='10, 15')\nTrain the model and use the TensorBoard Keras callback to collect\nperformance profiling data\nmodel.fit(train_data,\n          steps_per_epoch=20,\n          epochs=5,\n          callbacks=[tb_callback])\n```\n\n\nProgrammatic mode using the `tf.profiler` Function API\n```python\ntf.profiler.experimental.start('logdir')\nTrain the model here\ntf.profiler.experimental.stop()\n```\n\n\nProgrammatic mode using the context manager\n`python\nwith tf.profiler.experimental.Profile('logdir'):\n    # Train the model here\n    pass`\n\n\nNote: Running the Profiler for too long can cause it to run out of memory. It is\nrecommended to profile no more than 10 steps at a time. Avoid profiling the\nfirst few batches to avoid inaccuracies due to initialization overhead.\n\n\n\nSampling mode: Perform on-demand profiling by using\n    `tf.profiler.experimental.server.start` to start a gRPC server with your\n    TensorFlow model run. After starting the gRPC server and running your model,\n    you can capture a profile through the Capture Profile button in the\n    TensorBoard profile plugin. Use the script in the Install profiler section\n    above to launch a TensorBoard instance if it is not already running.\nAs an example,\n```python\nStart a profiler server before your model runs.\ntf.profiler.experimental.server.start(6009)\n(Model code goes here).\nSend a request to the profiler server to collect a trace of your model.\ntf.profiler.experimental.client.trace('grpc://localhost:6009',\n                                      'gs://your_tb_logdir', 2000)\n```\nAn example for profiling multiple workers:\n```python\nE.g. your worker IP addresses are 10.0.0.2, 10.0.0.3, 10.0.0.4, and you\nwould like to profile for a duration of 2 seconds.\ntf.profiler.experimental.client.trace(\n    'grpc://10.0.0.2:8466,grpc://10.0.0.3:8466,grpc://10.0.0.4:8466',\n    'gs://your_tb_logdir',\n    2000)\n```\n\n\n\n\nUse the Capture Profile dialog to specify:\n\nA comma-delimited list of profile service URLs or TPU names.\nA profiling duration.\nThe level of device, host, and Python function call tracing.\nHow many times you want the Profiler to retry capturing profiles if\n    unsuccessful at first.\n\nProfiling custom training loops\nTo profile custom training loops in your TensorFlow code, instrument the\ntraining loop with the `tf.profiler.experimental.Trace` API to mark the step\nboundaries for the Profiler.\nThe `name` argument is used as a prefix for the step names, the `step_num`\nkeyword argument is appended in the step names, and the `_r` keyword argument\nmakes this trace event get processed as a step event by the Profiler.\nAs an example,\n`python\nfor step in range(NUM_STEPS):\n    with tf.profiler.experimental.Trace('train', step_num=step, _r=1):\n        train_data = next(dataset)\n        train_step(train_data)`\nThis will enable the Profiler's step-based performance analysis and cause the\nstep events to show up in the trace viewer.\nMake sure that you include the dataset iterator within the\n`tf.profiler.experimental.Trace` context for accurate analysis of the input\npipeline.\nThe code snippet below is an anti-pattern:\nWarning: This will result in inaccurate analysis of the input pipeline.\n`python\nfor step, train_data in enumerate(dataset):\n    with tf.profiler.experimental.Trace('train', step_num=step, _r=1):\n        train_step(train_data)`\nProfiling use cases\nThe profiler covers a number of use cases along four different axes. Some of the\ncombinations are currently supported and others will be added in the future.\nSome of the use cases are:\n\nLocal vs. remote profiling: These are two common ways of setting up your\n    profiling environment. In local profiling, the profiling API is called on\n    the same machine your model is executing, for example, a local workstation\n    with GPUs. In remote profiling, the profiling API is called on a different\n    machine from where your model is executing, for example, on a Cloud TPU.\nProfiling multiple workers: You can profile multiple machines when using\n    the distributed training capabilities of TensorFlow.\nHardware platform: Profile CPUs, GPUs, and TPUs.\n\nThe table below provides a quick overview of the TensorFlow-supported use cases\nmentioned above:\n\n| Profiling API                | Local     | Remote    | Multiple  | Hardware  |\n:                              :           :           : workers   : Platforms :\n| :--------------------------- | :-------- | :-------- | :-------- | :-------- |\n| TensorBoard Keras          | Supported | Not       | Not       | CPU, GPU  |\n: Callback                   :           : Supported : Supported :           :\n| `tf.profiler.experimental` | Supported | Not       | Not       | CPU, GPU  |\n: start/stop API    :           : Supported : Supported :           :\n| `tf.profiler.experimental` | Supported | Supported | Supported | CPU, GPU, |\n: client.trace API  :           :           :           : TPU       :\n| Context manager API      | Supported | Not       | Not       | CPU, GPU  |\n:                              :           : supported : Supported :           :\n\nBest practices for optimal model performance\nUse the following recommendations as applicable for your TensorFlow models to\nachieve optimal performance.\nIn general, perform all transformations on the device and ensure that you use\nthe latest compatible version of libraries like cuDNN and Intel MKL for your\nplatform.\nOptimize the input data pipeline\nUse the data from the [#input_pipeline_analyzer] to optimize your data input\npipeline. An efficient data input pipeline can drastically improve the speed of\nyour model execution by reducing device idle time. Try to incorporate the best\npractices detailed in the\nBetter performance with the tf.data API\nguide and below to make your data input pipeline more efficient.\n\n\nIn general, parallelizing any ops that do not need to be executed\n    sequentially can significantly optimize the data input pipeline.\n\n\nIn many cases, it helps to change the order of some calls or to tune the\n    arguments such that it works best for your model. While optimizing the input\n    data pipeline, benchmark only the data loader without the training and\n    backpropagation steps to quantify the effect of the optimizations\n    independently.\n\n\nTry running your model with synthetic data to check if the input pipeline is\n    a performance bottleneck.\n\n\nUse `tf.data.Dataset.shard` for multi-GPU training. Ensure you shard very\n    early on in the input loop to prevent reductions in throughput. When working\n    with TFRecords, ensure you shard the list of TFRecords and not the contents\n    of the TFRecords.\n\n\nParallelize several ops by dynamically setting the value of\n    `num_parallel_calls` using `tf.data.AUTOTUNE`.\n\n\nConsider limiting the usage of `tf.data.Dataset.from_generator` as it is\n    slower compared to pure TensorFlow ops.\n\n\nConsider limiting the usage of `tf.py_function` as it cannot be serialized\n    and is not supported to run in distributed TensorFlow.\n\n\nUse `tf.data.Options` to control static optimizations to the input pipeline.\n\n\nAlso read the `tf.data` performance analysis\nguide for more\nguidance on optimizing your input pipeline.\nOptimize data augmentation\nWhen working with image data, make your\ndata augmentation\nmore efficient by casting to different data types after applying\nspatial transformations, such as flipping, cropping, rotating, etc.\nNote: Some ops like `tf.image.resize` transparently change the `dtype` to\n`fp32`. Make sure you normalize your data to lie between `0` and `1` if its not\ndone automatically. Skipping this step could lead to `NaN` errors if you have\nenabled AMP.\nUse NVIDIA\u00ae DALI\nIn some instances, such as when you have a system with a high GPU to CPU ratio,\nall of the above optimizations may not be enough to eliminate bottlenecks in the\ndata loader caused due to limitations of CPU cycles.\nIf you are using NVIDIA\u00ae GPUs for computer vision and audio deep learning\napplications, consider using the Data Loading Library\n(DALI)\nto accelerate the data pipeline.\nCheck the\nNVIDIA\u00ae DALI: Operations\ndocumentation for a list of supported DALI ops.\nUse threading and parallel execution\nRun ops on multiple CPU threads with the `tf.config.threading` API to execute\nthem faster.\nTensorFlow automatically sets the number of parallelism threads by default. The\nthread pool available for running TensorFlow ops depends on the number of CPU\nthreads available.\nControl the maximum parallel speedup for a single op by using\n`tf.config.threading.set_intra_op_parallelism_threads`. Note that if you run\nmultiple ops in parallel, they will all share the available thread pool.\nIf you have independent non-blocking ops (ops with no directed path between them\non the graph), use `tf.config.threading.set_inter_op_parallelism_threads` to run\nthem concurrently using the available thread pool.\nMiscellaneous\nWhen working with smaller models on NVIDIA\u00ae GPUs, you can set\n`tf.compat.v1.ConfigProto.force_gpu_compatible=True` to force all CPU tensors to\nbe allocated with CUDA pinned memory to give a significant boost to model\nperformance. However, exercise caution while using this option for unknown/very\nlarge models as this might negatively impact the host (CPU) performance.\nImprove device performance\nFollow the best practices detailed here and in the\nGPU performance optimization guide\nto optimize on-device TensorFlow model performance.\nIf you are using NVIDIA GPUs, log the GPU and memory utilization to a CSV file\nby running:\n`shell\nnvidia-smi\n--query-gpu=utilization.gpu,utilization.memory,memory.total,\nmemory.free,memory.used --format=csv`\nConfigure data layout\nWhen working with data that contains channel information (like images), optimize\nthe data layout format to prefer channels last (NHWC over NCHW).\nChannel-last data formats improve\nTensor Core\nutilization and provide significant performance improvements especially in\nconvolutional models when coupled with AMP. NCHW data layouts can still be\noperated on by Tensor Cores, but introduce additional overhead due to automatic\ntranspose ops.\nYou can optimize the data layout to prefer NHWC layouts by setting\n`data_format=\"channels_last\"` for layers such as `tf.keras.layers.Conv2D`,\n`tf.keras.layers.Conv3D`, and\n`tf.keras.layers.RandomRotation`.\nUse `tf.keras.backend.set_image_data_format` to set the default data layout\nformat for the Keras backend API.\nMax out the L2 cache\nWhen working with NVIDIA\u00ae GPUs, execute the code snippet below before the\ntraining loop to max out the L2 fetch granularity to 128 bytes.\n```python\nimport ctypes\n_libcudart = ctypes.CDLL('libcudart.so')\nSet device limit on the current device\ncudaLimitMaxL2FetchGranularity = 0x05\npValue = ctypes.cast((ctypes.c_int*1)(), ctypes.POINTER(ctypes.c_int))\n_libcudart.cudaDeviceSetLimit(ctypes.c_int(0x05), ctypes.c_int(128))\n_libcudart.cudaDeviceGetLimit(pValue, ctypes.c_int(0x05))\nassert pValue.contents.value == 128\n```\nConfigure GPU thread usage\nThe GPU thread mode decides how GPU threads are used.\nSet the thread mode to `gpu_private` to make sure that preprocessing does not\nsteal all the GPU threads. This will reduce the kernel launch delay during\ntraining. You can also set the number of threads per GPU. Set these values using\nenvironment variables.\n```python\nimport os\nos.environ['TF_GPU_THREAD_MODE']='gpu_private'\nos.environ['TF_GPU_THREAD_COUNT']='1'\n```\nConfigure GPU memory options\nIn general, increase the batch size and scale the model to better utilize GPUs\nand get higher throughput. Note that increasing the batch size will change the\nmodel\u2019s accuracy so the model needs to be scaled by tuning hyperparameters like\nthe learning rate to meet the target accuracy.\nAlso, use `tf.config.experimental.set_memory_growth` to allow GPU memory to grow\nto prevent all the available memory from being fully allocated to ops that\nrequire only a fraction of the memory. This allows other processes which consume\nGPU memory to run on the same device.\nTo learn more, check out the\nLimiting GPU memory growth\nguidance in the GPU guide to learn more.\nMiscellaneous\n\n\nIncrease the training mini-batch size (number of training samples used per\n    device in one iteration of the training loop) to the maximum amount that\n    fits without an out of memory (OOM) error on the GPU. Increasing the batch\n    size impacts the model's accuracy\u2014so make sure you scale the model by tuning\n    hyperparameters to meet the target accuracy.\n\n\nDisable reporting OOM errors during tensor allocation in production code.\n    Set `report_tensor_allocations_upon_oom=False` in `tf.compat.v1.RunOptions`.\n\n\nFor models with convolution layers, remove bias addition if using batch\n    normalization. Batch normalization shifts values by their mean and this\n    removes the need to have a constant bias term.\n\n\nUse TF Stats to find out how efficiently on-device ops run.\n\n\nUse `tf.function` to perform computations and optionally, enable the\n    `jit_compile=True` flag (`tf.function(jit_compile=True`). To learn more, go\n    to\n    Use XLA tf.function.\n\n\nMinimize host Python operations between steps and reduce callbacks.\n    Calculate metrics every few steps instead of at every step.\n\n\nKeep the device compute units busy.\n\n\nSend data to multiple devices in parallel.\n\n\nConsider\n    using 16-bit numerical representations,\n    such as `fp16`\u2014the half-precision floating point format specified by IEEE\u2014or\n    the Brain floating-point\n    bfloat16 format.\n\n\nAdditional resources\n\nThe\n    TensorFlow Profiler: Profile model performance\n    tutorial with Keras and TensorBoard where you can apply the advice in this\n    guide.\nThe\n    Performance profiling in TensorFlow 2\n    talk from the TensorFlow Dev Summit 2020.\nThe TensorFlow Profiler demo\n    from the TensorFlow Dev Summit 2020.\n\nKnown limitations\nProfiling multiple GPUs on TensorFlow 2.2 and TensorFlow 2.3\nTensorFlow 2.2 and 2.3 support multiple GPU profiling for single host systems\nonly; multiple GPU profiling for multi-host systems is not supported. To profile\nmulti-worker GPU configurations, each worker has to be profiled independently.\nFrom TensorFlow 2.4 multiple workers can be profiled using the\n`tf.profiler.experimental.client.trace` API.\nCUDA\u00ae Toolkit 10.2 or later is required to profile multiple GPUs. As TensorFlow\n2.2 and 2.3 support CUDA\u00ae Toolkit versions only up to 10.1, you need to create\nsymbolic links to `libcudart.so.10.1` and `libcupti.so.10.1`:\n```shell\nsudo ln -s /usr/local/cuda/lib64/libcudart.so.10.2 /usr/local/cuda/lib64/libcudart.so.10.1\nsudo ln -s /usr/local/cuda/extras/CUPTI/lib64/libcupti.so.10.2 /usr/local/cuda/extras/CUPTI/lib64/libcupti.so.10.1",
    "tag": "tensorflow"
  },
  {
    "title": "Create an op",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/guide/create_op.md",
    "content": "Create an op\nNote: To guarantee that your C++ custom ops are ABI compatible with TensorFlow's\nofficial pip packages, please follow the guide at\nCustom op repository. It has an\nend-to-end code example, as well as Docker images for building and distributing\nyour custom ops.\nIf you'd like to create an op that isn't covered by the existing TensorFlow\nlibrary, we recommend that you first try writing the op in Python as\na composition of existing Python ops or functions. If that isn't possible, you\ncan create a custom C++ op. There are several reasons why you might want to\ncreate a custom C++ op:\n\nIt's not easy or possible to express your operation as a composition of\n    existing ops.\nIt's not efficient to express your operation as a composition of existing\n    primitives.\nYou want to hand-fuse a composition of primitives that a future compiler\n    would find difficult fusing.\n\nFor example, imagine you want to implement something like \"median pooling\",\nsimilar to the \"MaxPool\" operator, but computing medians over sliding windows\ninstead of maximum values.  Doing this using a composition of operations may be\npossible (e.g., using ExtractImagePatches and TopK), but may not be as\nperformance- or memory-efficient as a native operation where you can do\nsomething more clever in a single, fused operation. As always, it is typically\nfirst worth trying to express what you want using operator composition, only\nchoosing to add a new operation if that proves to be difficult or inefficient.\nTo incorporate your custom op you'll need to:\n\nRegister the new op in a C++ file. Op registration defines an interface\n    (specification) for the op's functionality, which is independent of the\n    op's implementation. For example, op registration defines the op's name and\n    the op's inputs and outputs. It also defines the shape function\n    that is used for tensor shape inference.\nImplement the op in C++. The implementation of an op is known\n    as a kernel, and it is the concrete implementation of the specification you\n    registered in Step 1. There can be multiple kernels for different input /\n    output types or architectures (for example, CPUs, GPUs).\nCreate a Python wrapper (optional). This wrapper is the public API that's\n    used to create the op in Python. A default wrapper is generated from the\n    op registration, which can be used directly or added to.\nWrite a function to compute gradients for the op (optional).\nTest the op. We usually do this in Python for convenience, but you can also\n    test the op in C++. If you define gradients, you can verify them with the\n    Python `tf.test.compute_gradient_error`.\n    See\n    relu_op_test.py as\n    an example that tests the forward functions of Relu-like operators and\n    their gradients.\n\nPrerequisites\n\nSome familiarity with C++.\nMust have installed the\n    TensorFlow binary, or must have\n    downloaded TensorFlow source,\n    and be able to build it.\n\nDefine the op interface\nYou define the interface of an op by registering it with the TensorFlow system.\nIn the registration, you specify the name of your op, its inputs (types and\nnames) and outputs (types and names), as well as docstrings and\nany attrs the op might require.\nTo see how this works, suppose you'd like to create an op that takes a tensor of\n`int32`s and outputs a copy of the tensor, with all but the first element set to\nzero. To do this, create a file named `zero_out.cc`. Then add a call to the\n`REGISTER_OP` macro that defines the interface for your op:\n```c++\ninclude \"tensorflow/core/framework/op.h\"\ninclude \"tensorflow/core/framework/shape_inference.h\"\nusing namespace tensorflow;\nREGISTER_OP(\"ZeroOut\")\n    .Input(\"to_zero: int32\")\n    .Output(\"zeroed: int32\")\n    .SetShapeFn( {\n      c->set_output(0, c->input(0));\n      return Status::OK();\n    });\n```\nThis `ZeroOut` op takes one tensor `to_zero` of 32-bit integers as input, and\noutputs a tensor `zeroed` of 32-bit integers. The op also uses a shape function\nto ensure that the output tensor is the same shape as the input tensor. For\nexample, if the input is a tensor of shape [10, 20], then this shape function\nspecifies that the output shape is also [10, 20].\nNote: The op name must be in CamelCase and it must be unique among all other ops\nthat are registered in the binary.\nImplement the kernel for the op\nAfter you define the interface, provide one or more implementations of the op.\nTo create one of these kernels, create a class that extends `OpKernel` and\noverrides the `Compute` method. The `Compute` method provides one `context`\nargument of type `OpKernelContext*`, from which you can access useful things\nlike the input and output tensors.\nAdd your kernel to the file you created above. The kernel might look something\nlike this:\n```c++\ninclude \"tensorflow/core/framework/op_kernel.h\"\nusing namespace tensorflow;\nclass ZeroOutOp : public OpKernel {\n public:\n  explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}\nvoid Compute(OpKernelContext* context) override {\n    // Grab the input tensor\n    const Tensor& input_tensor = context->input(0);\n    auto input = input_tensor.flat();\n\n\n```// Create an output tensor\nTensor* output_tensor = NULL;\nOP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),\n                                                 &output_tensor));\nauto output_flat = output_tensor->flat<int32>();\n\n// Set all but the first element of the output tensor to 0.\nconst int N = input.size();\nfor (int i = 1; i < N; i++) {\n  output_flat(i) = 0;\n}\n\n// Preserve the first input value if possible.\nif (N > 0) output_flat(0) = input(0);\n```\n\n\n}\n};\n```\nAfter implementing your kernel, you register it with the TensorFlow system. In\nthe registration, you specify different constraints under which this kernel\nwill run. For example, you might have one kernel made for CPUs, and a separate\none for GPUs.\nTo do this for the `ZeroOut` op, add the following to `zero_out.cc`:\n`c++\nREGISTER_KERNEL_BUILDER(Name(\"ZeroOut\").Device(DEVICE_CPU), ZeroOutOp);`\n\nImportant: Instances of your OpKernel may be accessed concurrently.\n  Your `Compute` method must be thread-safe. Guard any access to class\n  members with a mutex. Or better yet, don't share state via class members!\n  Consider using a ResourceMgr\n  to keep track of op state.\n\nMulti-threaded CPU kernels\nTo write a multi-threaded CPU kernel, the Shard function in\nwork_sharder.h\ncan be used. This function shards a computation function across the\nthreads configured to be used for intra-op threading (see\nintra_op_parallelism_threads in\nconfig.proto).\nGPU kernels\nA GPU kernel is implemented in two parts: the OpKernel and the CUDA kernel and\nits launch code.\nSometimes the OpKernel implementation is common between a CPU and GPU kernel,\nsuch as around inspecting inputs and allocating outputs.  In that case, a\nsuggested implementation is to:\n\nDefine the OpKernel templated on the Device and the primitive type of the\n   tensor.\nTo do the actual computation of the output, the Compute function calls a\n    templated functor struct.\nThe specialization of that functor for the CPUDevice is defined in the same\n   file, but the specialization for the GPUDevice is defined in a .cu.cc file,\n   since it will be compiled with the CUDA compiler.\n\nHere is an example implementation.\n```c++\n// kernel_example.h\nifndef KERNEL_EXAMPLE_H_\ndefine KERNEL_EXAMPLE_H_\ninclude \ntemplate \nstruct ExampleFunctor {\n  void operator()(const Device& d, int size, const T in, T out);\n};\nif GOOGLE_CUDA\n// Partially specialize functor for GpuDevice.\ntemplate \nstruct ExampleFunctor {\n  void operator()(const Eigen::GpuDevice& d, int size, const T in, T out);\n};\nendif\nendif KERNEL_EXAMPLE_H_\n```\n```c++\n// kernel_example.cc\ninclude \"kernel_example.h\"\ninclude \"tensorflow/core/framework/op.h\"\ninclude \"tensorflow/core/framework/shape_inference.h\"\ninclude \"tensorflow/core/framework/op_kernel.h\"\nusing namespace tensorflow;\nusing CPUDevice = Eigen::ThreadPoolDevice;\nusing GPUDevice = Eigen::GpuDevice;\nREGISTER_OP(\"Example\")\n    .Attr(\"T: numbertype\")\n    .Input(\"input: T\")\n    .Output(\"input_times_two: T\")\n    .SetShapeFn( {\n      c->set_output(0, c->input(0));\n      return Status::OK();\n    });\n// CPU specialization of actual computation.\ntemplate \nstruct ExampleFunctor {\n  void operator()(const CPUDevice& d, int size, const T in, T out) {\n    for (int i = 0; i < size; ++i) {\n      out[i] = 2 * in[i];\n    }\n  }\n};\n// OpKernel definition.\n// template parameter  is the datatype of the tensors.\ntemplate \nclass ExampleOp : public OpKernel {\n public:\n  explicit ExampleOp(OpKernelConstruction* context) : OpKernel(context) {}\nvoid Compute(OpKernelContext* context) override {\n    // Grab the input tensor\n    const Tensor& input_tensor = context->input(0);\n\n\n```// Create an output tensor\nTensor* output_tensor = NULL;\nOP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),\n                                                 &output_tensor));\n\n// Do the computation.\nOP_REQUIRES(context, input_tensor.NumElements() <= tensorflow::kint32max,\n            errors::InvalidArgument(\"Too many elements in tensor\"));\nExampleFunctor<Device, T>()(\n    context->eigen_device<Device>(),\n    static_cast<int>(input_tensor.NumElements()),\n    input_tensor.flat<T>().data(),\n    output_tensor->flat<T>().data());\n```\n\n\n}\n};\n// Register the CPU kernels.\n#define REGISTER_CPU(T)                                          \\\n  REGISTER_KERNEL_BUILDER(                                       \\\n      Name(\"Example\").Device(DEVICE_CPU).TypeConstraint(\"T\"), \\\n      ExampleOp);\nREGISTER_CPU(float);\nREGISTER_CPU(int32);\n// Register the GPU kernels.\nifdef GOOGLE_CUDA\n#define REGISTER_GPU(T)                                          \\\n  / Declare explicit instantiations in kernel_example.cu.cc. / \\\n  extern template class ExampleFunctor;            \\\n  REGISTER_KERNEL_BUILDER(                                       \\\n      Name(\"Example\").Device(DEVICE_GPU).TypeConstraint(\"T\"), \\\n      ExampleOp);\nREGISTER_GPU(float);\nREGISTER_GPU(int32);\nendif  // GOOGLE_CUDA\n```\n```c++\n// kernel_example.cu.cc\nifdef GOOGLE_CUDA\ndefine EIGEN_USE_GPU\ninclude \"kernel_example.h\"\ninclude \"tensorflow/core/util/gpu_kernel_helper.h\"\nusing namespace tensorflow;\nusing GPUDevice = Eigen::GpuDevice;\n// Define the CUDA kernel.\ntemplate \nglobal void ExampleCudaKernel(const int size, const T in, T out) {\n  for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < size;\n       i += blockDim.x * gridDim.x) {\n    out[i] = 2 * __ldg(in + i);\n  }\n}\n// Define the GPU implementation that launches the CUDA kernel.\ntemplate \nvoid ExampleFunctor::operator()(\n    const GPUDevice& d, int size, const T in, T out) {\n  // Launch the cuda kernel.\n  //\n  // See core/util/gpu_kernel_helper.h for example of computing\n  // block count and thread_per_block count.\n  int block_count = 1024;\n  int thread_per_block = 20;\n  ExampleCudaKernel\n      <<>>(size, in, out);\n}\n// Explicitly instantiate functors for the types of OpKernels registered.\ntemplate struct ExampleFunctor;\ntemplate struct ExampleFunctor;\nendif  // GOOGLE_CUDA\n```\nBuild the op library\nCompile the op using your system compiler (TensorFlow binary installation)\nYou should be able to compile `zero_out.cc` with a `C++` compiler such as `g++`\nor `clang` available on your system. The binary PIP package installs the header\nfiles and the library that you need to compile your op in locations that are\nsystem specific. However, the TensorFlow python library provides the\n`get_include` function to get the header directory, and the `get_lib` directory\nhas a shared object to link against.\nHere are the outputs of these functions on an Ubuntu machine.\n```bash\n$ python\n\n\n\nimport tensorflow as tf\ntf.sysconfig.get_include()\n'/usr/local/lib/python3.6/site-packages/tensorflow/include'\ntf.sysconfig.get_lib()\n'/usr/local/lib/python3.6/site-packages/tensorflow'\n```\n\n\n\nAssuming you have `g++` installed, here is the sequence of commands you can use\nto compile your op into a dynamic library.\n`bash\nTF_CFLAGS=( $(python -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_compile_flags()))') )\nTF_LFLAGS=( $(python -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_link_flags()))') )\ng++ -std=c++14 -shared zero_out.cc -o zero_out.so -fPIC ${TF_CFLAGS[@]} ${TF_LFLAGS[@]} -O2`\nOn macOS, the additional flag \"-undefined dynamic_lookup\" is required when\nbuilding the `.so` file.\n\nNote on `gcc` version `>=5`: gcc uses the new C++\n  ABI since version `5`. The binary pip\n  packages available on the TensorFlow website are built with `gcc4` that uses\n  the older ABI. If you compile your op library with `gcc>=5`, add\n  `-D_GLIBCXX_USE_CXX11_ABI=0` to the command line to make the library\n  compatible with the older abi.\n\nCompile the op using bazel (TensorFlow source installation)\nIf you have TensorFlow sources installed, you can make use of TensorFlow's build\nsystem to compile your op. Place a BUILD file with following Bazel build rule in\nthe tensorflow/core/user_ops directory.\n```python\nload(\"//tensorflow:tensorflow.bzl\", \"tf_custom_op_library\")\ntf_custom_op_library(\n    name = \"zero_out.so\",\n    srcs = [\"zero_out.cc\"],\n)\n```\nRun the following command to build `zero_out.so`.\n`bash\n$ bazel build --config opt //tensorflow/core/user_ops:zero_out.so`\nFor compiling the `Example` operation, with the CUDA Kernel, you need to use the `gpu_srcs` parameter\nof `tf_custom_op_library`. Place a BUILD file with the following Bazel build rule in a new folder\ninside the tensorflow/core/user_ops directory (e.g. \"example_gpu\").\n```python\nload(\"//tensorflow:tensorflow.bzl\", \"tf_custom_op_library\")\ntf_custom_op_library(\n    # kernel_example.cc  kernel_example.cu.cc  kernel_example.h\n    name = \"kernel_example.so\",\n    srcs = [\"kernel_example.h\", \"kernel_example.cc\"],\n    gpu_srcs = [\"kernel_example.cu.cc\", \"kernel_example.h\"],\n)\n```\nRun the following command to build `kernel_example.so`.\n`bash\n$ bazel build --config opt //tensorflow/core/user_ops/example_gpu:kernel_example.so`\nNote: As explained above, if you are compiling with gcc>=5 add\n`--cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"` to the Bazel command line arguments.\n\nNote: Although you can create a shared library (a `.so` file) with the\n  standard `cc_library` rule, we strongly recommend that you use the\n  `tf_custom_op_library` macro. It adds some required dependencies, and\n  performs checks to ensure that the shared library is compatible with\n  TensorFlow's plugin loading mechanism.\n\nUse the op in Python\nTensorFlow Python API provides the\n`tf.load_op_library` function to\nload the dynamic library and register the op with the TensorFlow\nframework. `load_op_library` returns a Python module that contains the Python\nwrappers for the op and the kernel. Thus, once you have built the op, you can\ndo the following to run it from Python:\n```python\nimport tensorflow as tf\nzero_out_module = tf.load_op_library('./zero_out.so')\nprint(zero_out_module.zero_out([[1, 2], [3, 4]]).numpy())\nPrints\narray([[1, 0], [0, 0]], dtype=int32)\n```\nKeep in mind, the generated function will be given a snake_case name (to comply\nwith PEP8). So, if your op is\nnamed `ZeroOut` in the C++ files, the python function will be called `zero_out`.\nTo make the op available as a regular function `import`-able from a Python\nmodule, it maybe useful to have the `load_op_library` call in a Python source\nfile as follows:\n```python\nimport tensorflow as tf\nzero_out_module = tf.load_op_library('./zero_out.so')\nzero_out = zero_out_module.zero_out\n```\nVerify that the op works\nA good way to verify that you've successfully implemented your op is to write a\ntest for it. Create the file\n`zero_out_op_test.py` with the contents:\n```python\nimport tensorflow as tf\nclass ZeroOutTest(tf.test.TestCase):\n  def testZeroOut(self):\n    zero_out_module = tf.load_op_library('./zero_out.so')\n    with self.test_session():\n      result = zero_out_module.zero_out([5, 4, 3, 2, 1])\n      self.assertAllEqual(result.eval(), [5, 0, 0, 0, 0])\nif name == \"main\":\n  tf.test.main()\n```\nThen run your test (assuming you have tensorflow installed):\n`sh\n$ python zero_out_op_test.py`\nBuild advanced features into your op\nNow that you know how to build a basic (and somewhat restricted) op and\nimplementation, we'll look at some of the more complicated things you will\ntypically need to build into your op. This includes:\n\nConditional checks and validation\nOp registration\nAttrs\nAttr types\nPolymorphism\nInputs and outputs\nBackwards compatibility\n\n\nGPU support\nCompiling the kernel for the GPU device\n\n\nImplement the gradient in Python\nShape functions in C++\n\nConditional checks and validation\nThe example above assumed that the op applied to a tensor of any shape.  What\nif it only applied to vectors?  That means adding a check to the above OpKernel\nimplementation.\n```c++\n  void Compute(OpKernelContext* context) override {\n    // Grab the input tensor\n    const Tensor& input_tensor = context->input(0);\n\n\n```OP_REQUIRES(context, TensorShapeUtils::IsVector(input_tensor.shape()),\n            errors::InvalidArgument(\"ZeroOut expects a 1-D vector.\"));\n// ...\n```\n\n\n}\n```\nThis asserts that the input is a vector, and returns having set the\n`InvalidArgument` status if it isn't.  The\nOP_REQUIRES macro takes three arguments:\n\nThe `context`, which can either be an `OpKernelContext` or\n    `OpKernelConstruction` pointer (see\n    tensorflow/core/framework/op_kernel.h),\n    for its `SetStatus()` method.\nThe condition.  For example, there are functions for validating the shape\n    of a tensor in\n    tensorflow/core/framework/tensor_shape.h\nThe error itself, which is represented by a `Status` object, see\n    tensorflow/core/platform/status.h. A\n    `Status` has both a type (frequently `InvalidArgument`, but see the list of\n    types) and a message.  Functions for constructing an error may be found in\n    tensorflow/core/platform/errors.h.\n\nAlternatively, if you want to test whether a `Status` object returned from some\nfunction is an error, and if so return it, use\nOP_REQUIRES_OK.  Both of these macros return from the\nfunction on error.\nOp registration\nAttrs\nOps can have attrs, whose values are set when the op is added to a graph. These\nare used to configure the op, and their values can be accessed both within the\nkernel implementation and in the types of inputs and outputs in the op\nregistration. Prefer using an input instead of an attr when possible, since\ninputs are more flexible. This is because attrs are constants and must be\ndefined at graph construction time. In contrast, inputs are Tensors whose\nvalues can be dynamic; that is, inputs can change every step, be set using a\nfeed, etc. Attrs are used for things that can't be done with inputs: any\nconfiguration that affects the signature (number or type of inputs or outputs)\nor that can't change from step-to-step.\nYou define an attr when you register the op, by specifying its name and type\nusing the `Attr` method, which expects a spec of the form:\n`<name>: <attr-type-expr>`\nwhere `<name>` begins with a letter and can be composed of alphanumeric\ncharacters and underscores, and `<attr-type-expr>` is a type expression of the\nform described below.\nFor example, if you'd like the `ZeroOut` op to preserve a user-specified index,\ninstead of only the 0th element, you can register the op like so:\n`c++\nREGISTER_OP(\"ZeroOut\")\n    .Attr(\"preserve_index: int\")\n    .Input(\"to_zero: int32\")\n    .Output(\"zeroed: int32\");`\n(Note that the set of attribute types is different from the\n`tf.DType` used for inputs and outputs.)\nYour kernel can then access this attr in its constructor via the `context`\nparameter:\n`c++\nclass ZeroOutOp : public OpKernel {\n public:\n  explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {\n    // Get the index of the value to preserve\n    OP_REQUIRES_OK(context,\n                   context->GetAttr(\"preserve_index\", &preserve_index_));\n    // Check that preserve_index is positive\n    OP_REQUIRES(context, preserve_index_ >= 0,\n                errors::InvalidArgument(\"Need preserve_index >= 0, got \",\n                                        preserve_index_));\n  }\n  void Compute(OpKernelContext* context) override {\n    // ...\n  }\n private:\n  int preserve_index_;\n};`\nwhich can then be used in the `Compute` method:\n```c++\n  void Compute(OpKernelContext* context) override {\n    // ...\n\n\n```// We're using saved attr to validate potentially dynamic input\n// So we check that preserve_index is in range\nOP_REQUIRES(context, preserve_index_ < input.dimension(0),\n            errors::InvalidArgument(\"preserve_index out of range\"));\n\n// Set all the elements of the output tensor to 0\nconst int N = input.size();\nfor (int i = 0; i < N; i++) {\n  output_flat(i) = 0;\n}\n\n// Preserve the requested input value\noutput_flat(preserve_index_) = input(preserve_index_);\n```\n\n\n}\n```\nAttr types\nThe following types are supported in an attr:\n\n`string`: Any sequence of bytes (not required to be UTF8).\n`int`: A signed integer.\n`float`: A floating point number.\n`bool`: True or false.\n`type`: One of the (non-ref) values of DataType.\n`shape`: A TensorShapeProto.\n`list(<type>)`: A list of `<type>`, where `<type>` is one of the above types.\n  Note that `list(list(<type>))` is invalid.\n\nSee also: op_def_builder.cc:FinalizeAttr for a definitive list.\nDefault values and constraints\nAttrs may have default values, and some types of attrs can have constraints. To\ndefine an attr with constraints, you can use the following `<attr-type-expr>`s:\n`{'<string1>', '<string2>'}`: The value must be a string that has either the\nvalue `<string1>` or `<string2>`. The name of the type, `string`, is implied\nwhen you use this syntax. This emulates an enum:\n`c++\nREGISTER_OP(\"EnumExample\")\n    .Attr(\"e: {'apple', 'orange'}\");`\n`{<type1>, <type2>}`: The value is of type `type`, and must be one of `<type1>`\nor `<type2>`, where `<type1>` and `<type2>` are supported `tf.DType`. You don't\nspecify that the type of the attr is `type`. This is implied when you have a\nlist of types in `{...}`. For example, in this case the attr `t` is a type that\nmust be an `int32`, a `float`, or a `bool`:\n`c++\nREGISTER_OP(\"RestrictedTypeExample\")\n    .Attr(\"t: {int32, float, bool}\");`\nThere are shortcuts for common type constraints:\n\n`numbertype`: Type `type` restricted to the numeric (non-string and\n    non-bool) types.\n`realnumbertype`: Like `numbertype` without complex types.\n`quantizedtype`: Like `numbertype` but just the quantized number types.\n\nThe specific lists of types allowed by these are defined by the functions (like\n`NumberTypes()`) in\ntensorflow/core/framework/types.h.\nIn this example the attr `t` must be one of the numeric types:\n`c++\nREGISTER_OP(\"NumberType\")\n    .Attr(\"t: numbertype\");`\nFor this op:\n`python\ntf.number_type(t=tf.int32)  # Valid\ntf.number_type(t=tf.bool)   # Invalid`\nLists can be combined with other lists and single types. The following op allows\nattr `t` to be any of the numeric types, or the bool type:\n`c++\nREGISTER_OP(\"NumberOrBooleanType\")\n    .Attr(\"t: {numbertype, bool}\");`\nFor this op:\n`python\ntf.number_or_boolean_type(t=tf.int32)  # Valid\ntf.number_or_boolean_type(t=tf.bool)   # Valid\ntf.number_or_boolean_type(t=tf.string) # Invalid`\n`int >= <n>`: The value must be an int whose value is greater than or equal to\n`<n>`, where `<n>` is a natural number. For example, the following op\nregistration specifies that the attr `a` must have a value that is at least `2`:\n`c++\nREGISTER_OP(\"MinIntExample\")\n    .Attr(\"a: int >= 2\");`\n`list(<type>) >= <n>`: A list of type `<type>` whose length is greater than or\nequal to `<n>`. For example, the following op registration specifies that the\nattr `a` is a list of types (either `int32` or `float`), and that there must be\nat least 3 of them:\n`c++\nREGISTER_OP(\"TypeListExample\")\n    .Attr(\"a: list({int32, float}) >= 3\");`\nTo set a default value for an attr (making it optional in the generated code),\nadd `= <default>` to the end, as in:\n`c++\nREGISTER_OP(\"AttrDefaultExample\")\n    .Attr(\"i: int = 0\");`\nAdditionally, both a constraint and a default value can be specified:\n`c++\nREGISTER_OP(\"AttrConstraintAndDefaultExample\")\n    .Attr(\"i: int >= 1 = 1\");`\nThe supported syntax of the default value is what would be used in the proto\nrepresentation of the resulting GraphDef definition.\nHere are examples for how to specify a default for all types:\n`c++\nREGISTER_OP(\"AttrDefaultExampleForAllTypes\")\n   .Attr(\"s: string = 'foo'\")\n   .Attr(\"i: int = 0\")\n   .Attr(\"f: float = 1.0\")\n   .Attr(\"b: bool = true\")\n   .Attr(\"ty: type = DT_INT32\")\n   .Attr(\"sh: shape = { dim { size: 1 } dim { size: 2 } }\")\n   .Attr(\"te: tensor = { dtype: DT_INT32 int_val: 5 }\")\n   .Attr(\"l_empty: list(int) = []\")\n   .Attr(\"l_int: list(int) = [2, 3, 5, 7]\");`\nNote in particular that the values of type `type`\nuse `tf.DType`.\nPolymorphism\nType polymorphism\nFor ops that can take different types as input or produce different output\ntypes, you can specify an attr in\nan input or output type in the op registration.  Typically\nyou would then register an `OpKernel` for each supported type.\nFor instance, if you'd like the `ZeroOut` op to work on `float`s\nin addition to `int32`s, your op registration might look like:\n`c++\nREGISTER_OP(\"ZeroOut\")\n    .Attr(\"T: {float, int32}\")\n    .Input(\"to_zero: T\")\n    .Output(\"zeroed: T\");`\nYour op registration now specifies that the input's type must be `float`, or\n`int32`, and that its output will be the same type, since both have type `T`.\nNaming\nInputs, outputs, and attrs generally should be given snake_case names. The one\nexception is attrs that are used as the type of an input or in the type of an\noutput. Those attrs can be inferred when the op is added to the graph and so\ndon't appear in the op's function. For example, this last definition of ZeroOut\nwill generate a Python function that looks like:\n```python\ndef zero_out(to_zero, name=None):\n  \"\"\"...\n  Args:\n    to_zero: A`Tensor`. Must be one of the following types:`float32`,`int32`.\n    name: A name for the operation (optional).\nReturns:\n    A `Tensor`. Has the same type as `to_zero`.\n  \"\"\"\n```\nIf `to_zero` is passed an `int32` tensor, then `T` is automatically set to\n`int32` (well, actually `DT_INT32`). Those inferred attrs are given Capitalized\nor CamelCase names.\nCompare this with an op that has a type attr that determines the output type:\n`c++\nREGISTER_OP(\"StringToNumber\")\n    .Input(\"string_tensor: string\")\n    .Output(\"output: out_type\")\n    .Attr(\"out_type: {float, int32} = DT_FLOAT\");\n    .Doc(R\"doc(\nConverts each string in the input Tensor to the specified numeric type.\n)doc\");`\nIn this case, the user has to specify the output type, as in the generated\nPython:\n```python\ndef string_to_number(string_tensor, out_type=None, name=None):\n  \"\"\"Converts each string in the input Tensor to the specified numeric type.\nArgs:\n    string_tensor: A `Tensor` of type `string`.\n    out_type: An optional `tf.DType` from: `tf.float32, tf.int32`.\n      Defaults to `tf.float32`.\n    name: A name for the operation (optional).\nReturns:\n    A `Tensor` of type `out_type`.\n  \"\"\"\n```\nType polymorphism example\n```c++\ninclude \"tensorflow/core/framework/op_kernel.h\"\nclass ZeroOutInt32Op : public OpKernel {\n  // as before\n};\nclass ZeroOutFloatOp : public OpKernel {\n public:\n  explicit ZeroOutFloatOp(OpKernelConstruction* context)\n      : OpKernel(context) {}\nvoid Compute(OpKernelContext* context) override {\n    // Grab the input tensor\n    const Tensor& input_tensor = context->input(0);\n    auto input = input_tensor.flat();\n\n\n```// Create an output tensor\nTensor* output = NULL;\nOP_REQUIRES_OK(context,\n               context->allocate_output(0, input_tensor.shape(), &output));\nauto output_flat = output->template flat<float>();\n\n// Set all the elements of the output tensor to 0\nconst int N = input.size();\nfor (int i = 0; i < N; i++) {\n  output_flat(i) = 0;\n}\n\n// Preserve the first input value\nif (N > 0) output_flat(0) = input(0);\n```\n\n\n}\n};\n// Note that TypeConstraint(\"T\") means that attr \"T\" (defined\n// in the op registration above) must be \"int32\" to use this template\n// instantiation.\nREGISTER_KERNEL_BUILDER(\n    Name(\"ZeroOut\")\n    .Device(DEVICE_CPU)\n    .TypeConstraint(\"T\"),\n    ZeroOutInt32Op);\nREGISTER_KERNEL_BUILDER(\n    Name(\"ZeroOut\")\n    .Device(DEVICE_CPU)\n    .TypeConstraint(\"T\"),\n    ZeroOutFloatOp);\n```\nTo preserve backwards compatibility, you should\nspecify a default value when adding an attr\nto an existing op:\n`c++\nREGISTER_OP(\"ZeroOut\")\n  .Attr(\"T: {float, int32} = DT_INT32\")\n  .Input(\"to_zero: T\")\n  .Output(\"zeroed: T\")`\nLet's say you wanted to add more types, say `double`:\n`c++\nREGISTER_OP(\"ZeroOut\")\n    .Attr(\"T: {float, double, int32}\")\n    .Input(\"to_zero: T\")\n    .Output(\"zeroed: T\");`\nInstead of writing another `OpKernel` with redundant code as above, often you\nwill be able to use a C++ template instead.  You will still have one kernel\nregistration (`REGISTER_KERNEL_BUILDER` call) per overload.\n```c++\ntemplate \nclass ZeroOutOp : public OpKernel {\n public:\n  explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}\nvoid Compute(OpKernelContext* context) override {\n    // Grab the input tensor\n    const Tensor& input_tensor = context->input(0);\n    auto input = input_tensor.flat();\n\n\n```// Create an output tensor\nTensor* output = NULL;\nOP_REQUIRES_OK(context,\n               context->allocate_output(0, input_tensor.shape(), &output));\nauto output_flat = output->template flat<T>();\n\n// Set all the elements of the output tensor to 0\nconst int N = input.size();\nfor (int i = 0; i < N; i++) {\n  output_flat(i) = 0;\n}\n\n// Preserve the first input value\nif (N > 0) output_flat(0) = input(0);\n```\n\n\n}\n};\n// Note that TypeConstraint(\"T\") means that attr \"T\" (defined\n// in the op registration above) must be \"int32\" to use this template\n// instantiation.\nREGISTER_KERNEL_BUILDER(\n    Name(\"ZeroOut\")\n    .Device(DEVICE_CPU)\n    .TypeConstraint(\"T\"),\n    ZeroOutOp);\nREGISTER_KERNEL_BUILDER(\n    Name(\"ZeroOut\")\n    .Device(DEVICE_CPU)\n    .TypeConstraint(\"T\"),\n    ZeroOutOp);\nREGISTER_KERNEL_BUILDER(\n    Name(\"ZeroOut\")\n    .Device(DEVICE_CPU)\n    .TypeConstraint(\"T\"),\n    ZeroOutOp);\n```\nIf you have more than a couple overloads, you can put the registration in a\nmacro.\n```c++\ninclude \"tensorflow/core/framework/op_kernel.h\"\n#define REGISTER_KERNEL(type)                                       \\\n  REGISTER_KERNEL_BUILDER(                                          \\\n      Name(\"ZeroOut\").Device(DEVICE_CPU).TypeConstraint(\"T\"), \\\n      ZeroOutOp)\nREGISTER_KERNEL(int32);\nREGISTER_KERNEL(float);\nREGISTER_KERNEL(double);\nundef REGISTER_KERNEL\n```\nDepending on the list of types you are registering the kernel for, you may be\nable to use a macro provided by\ntensorflow/core/framework/register_types.h:\n```c++\ninclude \"tensorflow/core/framework/op_kernel.h\"\ninclude \"tensorflow/core/framework/register_types.h\"\nREGISTER_OP(\"ZeroOut\")\n    .Attr(\"T: realnumbertype\")\n    .Input(\"to_zero: T\")\n    .Output(\"zeroed: T\");\ntemplate \nclass ZeroOutOp : public OpKernel { ... };\n#define REGISTER_KERNEL(type)                                       \\\n  REGISTER_KERNEL_BUILDER(                                          \\\n      Name(\"ZeroOut\").Device(DEVICE_CPU).TypeConstraint(\"T\"), \\\n      ZeroOutOp)\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNEL);\nundef REGISTER_KERNEL\n```\nList inputs and outputs\nIn addition to being able to accept or produce different types, ops can consume\nor produce a variable number of tensors.\nIn the next example, the attr `T` holds a list of types, and is used as the\ntype of both the input `in` and the output `out`.  The input and output are\nlists of tensors of that type (and the number and types of tensors in the output\nare the same as the input, since both have type `T`).\n`c++\nREGISTER_OP(\"PolymorphicListExample\")\n    .Attr(\"T: list(type)\")\n    .Input(\"in: T\")\n    .Output(\"out: T\");`\nYou can also place restrictions on what types can be specified in the list. In\nthis next case, the input is a list of `float` and `double` tensors. The op\naccepts, for example, input types `(float, double, float)` and in that case the\noutput type would also be `(float, double, float)`.\n`c++\nREGISTER_OP(\"ListTypeRestrictionExample\")\n    .Attr(\"T: list({float, double})\")\n    .Input(\"in: T\")\n    .Output(\"out: T\");`\nIf you want all the tensors in a list to be of the same type, you might do\nsomething like:\n`c++\nREGISTER_OP(\"IntListInputExample\")\n    .Attr(\"N: int\")\n    .Input(\"in: N * int32\")\n    .Output(\"out: int32\");`\nThis accepts a list of `int32` tensors, and uses an `int` attr `N` to\nspecify the length of the list.\nThis can be made type polymorphic as well.  In the next\nexample, the input is a list of tensors (with length `\"N\"`) of the same (but\nunspecified) type (`\"T\"`), and the output is a single tensor of matching type:\n`c++\nREGISTER_OP(\"SameListInputExample\")\n    .Attr(\"N: int\")\n    .Attr(\"T: type\")\n    .Input(\"in: N * T\")\n    .Output(\"out: T\");`\nBy default, tensor lists have a minimum length of 1. You can change that default\nusing\na \">=\" constraint on the corresponding attr.\nIn this next example, the input is a list of at least 2 `int32` tensors:\n`c++\nREGISTER_OP(\"MinLengthIntListExample\")\n    .Attr(\"N: int >= 2\")\n    .Input(\"in: N * int32\")\n    .Output(\"out: int32\");`\nThe same syntax works with `\"list(type)\"` attrs:\n`c++\nREGISTER_OP(\"MinimumLengthPolymorphicListExample\")\n    .Attr(\"T: list(type) >= 3\")\n    .Input(\"in: T\")\n    .Output(\"out: T\");`\nInputs and outputs\nTo summarize the above, an op registration can have multiple inputs and outputs:\n`c++\nREGISTER_OP(\"MultipleInsAndOuts\")\n    .Input(\"y: int32\")\n    .Input(\"z: float\")\n    .Output(\"a: string\")\n    .Output(\"b: int32\");`\nEach input or output spec is of the form:\n`<name>: <io-type-expr>`\nwhere `<name>` begins with a letter and can be composed of alphanumeric\ncharacters and underscores. `<io-type-expr>` is one of the following type\nexpressions:\n\n`<type>`, where `<type>` is a supported input type (e.g. `float`, `int32`,\n  `string`). This specifies a single tensor of the given type.\n\nSee\n  `tf.DType`.\n`c++\n  REGISTER_OP(\"BuiltInTypesExample\")\n      .Input(\"integers: int32\")\n      .Input(\"complex_numbers: complex64\");`\n\n`<attr-type>`, where `<attr-type>` is the name of an Attr with type\n  `type` or `list(type)` (with a possible type restriction). This syntax allows\n  for polymorphic ops.\n\n```c++\n  REGISTER_OP(\"PolymorphicSingleInput\")\n      .Attr(\"T: type\")\n      .Input(\"in: T\");\nREGISTER_OP(\"RestrictedPolymorphicSingleInput\")\n      .Attr(\"T: {int32, int64}\")\n      .Input(\"in: T\");\n  ```\nReferencing an attr of type `list(type)` allows you to accept a sequence of\n  tensors.\n```c++\n  REGISTER_OP(\"ArbitraryTensorSequenceExample\")\n      .Attr(\"T: list(type)\")\n      .Input(\"in: T\")\n      .Output(\"out: T\");\nREGISTER_OP(\"RestrictedTensorSequenceExample\")\n      .Attr(\"T: list({int32, int64})\")\n      .Input(\"in: T\")\n      .Output(\"out: T\");\n  ```\nNote that the number and types of tensors in the output `out` is the same as\n  in the input `in`, since both are of type `T`.\n\nFor a sequence of tensors with the same type: `<number> * <type>`, where\n  `<number>` is the name of an Attr with type `int`.  The `<type>` can\n  either be a `tf.DType`,\n  or the name of an attr with type `type`.  As an example of the first, this\n  op accepts a list of `int32` tensors:\n\n`c++\n  REGISTER_OP(\"Int32SequenceExample\")\n      .Attr(\"NumTensors: int\")\n      .Input(\"in: NumTensors * int32\")`\nWhereas this op accepts a list of tensors of any type, as long as they are all\n  the same:\n`c++\n  REGISTER_OP(\"SameTypeSequenceExample\")\n      .Attr(\"NumTensors: int\")\n      .Attr(\"T: type\")\n      .Input(\"in: NumTensors * T\")`\n\nFor a reference to a tensor: `Ref(<type>)`, where `<type>` is one of the\n  previous types.\n\nAny attr used in the type of an input will be inferred. By convention those\ninferred attrs use capital names (like `T` or `N`). Otherwise inputs, outputs,\nand attrs have names like function parameters (e.g. `num_outputs`). For more\ndetails, see the earlier section on naming.\nFor more details, see\ntensorflow/core/framework/op_def_builder.h.\nBackwards compatibility\nLet's assume you have written a nice, custom op and shared it with others, so\nyou have happy customers using your operation.  However, you'd like to make\nchanges to the op in some way.\nIn general, changes to existing, checked-in specifications must be\nbackwards-compatible: changing the specification of an op must not break prior\nserialized `GraphDef` protocol buffers constructed from older specifications.\nThe details of `GraphDef` compatibility are\ndescribed here.\nThere are several ways to preserve backwards-compatibility.\n\n\nAny new attrs added to an operation must have default values defined, and\n    with that default value the op must have the original behavior. To change an\n    operation from not polymorphic to polymorphic, you must give a default\n    value to the new type attr to preserve the original signature by default.\n    For example, if your operation was:\n`c++\nREGISTER_OP(\"MyGeneralUnaryOp\")\n    .Input(\"in: float\")\n    .Output(\"out: float\");`\nyou can make it polymorphic in a backwards-compatible way using:\n`c++\nREGISTER_OP(\"MyGeneralUnaryOp\")\n    .Input(\"in: T\")\n    .Output(\"out: T\")\n    .Attr(\"T: numerictype = DT_FLOAT\");`\n\n\nYou can safely make a constraint on an attr less restrictive. For example,\n    you can change from `{int32, int64}` to `{int32, int64, float}` or `type`.\n    Or you may change from `{\"apple\", \"orange\"}` to `{\"apple\", \"banana\",\n    \"orange\"}` or `string`.\n\n\nYou can change single inputs / outputs into list inputs / outputs, as long\n    as the default for the list type matches the old signature.\n\n\nYou can add a new list input / output, if it defaults to empty.\n\n\nNamespace any new ops you create, by prefixing the op names with something\n    unique to your project. This avoids having your op colliding with any ops\n    that might be included in future versions of TensorFlow.\n\n\nPlan ahead! Try to anticipate future uses for the op. Some signature changes\n    can't be done in a compatible way (for example, making a list of the same\n    type into a list of varying types).\n\n\nThe full list of safe and unsafe changes can be found in\ntensorflow/core/framework/op_compatibility_test.cc.\nIf you cannot make your change to an operation backwards compatible, then create\na new operation with a new name with the new semantics.\nAlso note that while these changes can maintain `GraphDef` compatibility, the\ngenerated Python code may change in a way that isn't compatible with old\ncallers.  The Python API may be kept compatible by careful changes in a\nhand-written Python wrapper, by keeping the old signature except possibly adding\nnew optional arguments to the end.  Generally incompatible changes may only be\nmade when TensorFlow changes major versions, and must conform to the\nGraphDef version semantics.\nGPU support\nYou can implement different OpKernels and register one for CPU and another for\nGPU, just like you can register kernels for different types.\nThere are several examples of kernels with GPU support in\ntensorflow/core/kernels/.\nNotice some kernels have a CPU version in a `.cc` file, a GPU version in a file\nending in `_gpu.cu.cc`, and some code shared in common in a `.h` file.\nFor example, the `tf.pad` has\neverything but the GPU kernel in tensorflow/core/kernels/pad_op.cc.\nThe GPU kernel is in\ntensorflow/core/kernels/pad_op_gpu.cu.cc,\nand the shared code is a templated class defined in\ntensorflow/core/kernels/pad_op.h.\nWe organize the code this way for two reasons: it allows you to share common\ncode among the CPU and GPU implementations, and it puts the GPU implementation\ninto a separate file so that it can be compiled only by the GPU compiler.\nOne thing to note, even when the GPU kernel version of `pad` is used, it still\nneeds its `\"paddings\"` input in CPU memory.  To mark that inputs or outputs are\nkept on the CPU, add a `HostMemory()` call to the kernel registration, e.g.:\n`c++\n#define REGISTER_GPU_KERNEL(T)                         \\\n  REGISTER_KERNEL_BUILDER(Name(\"Pad\")                  \\\n                              .Device(DEVICE_GPU)      \\\n                              .TypeConstraint<T>(\"T\")  \\\n                              .HostMemory(\"paddings\"), \\\n                          PadOp<GPUDevice, T>)`\nCompiling the kernel for the GPU device\nLook at\ncuda_op_kernel.cu.cc\nfor an example that uses a CUDA kernel to implement an op. The\n`tf_custom_op_library` accepts a `gpu_srcs` argument in which the list of source\nfiles containing the CUDA kernels (`*.cu.cc` files) can be specified. For use\nwith a binary installation of TensorFlow, the CUDA kernels have to be compiled\nwith NVIDIA's `nvcc` compiler. Here is the sequence of commands you can use to\ncompile the\ncuda_op_kernel.cu.cc\nand\ncuda_op_kernel.cc\ninto a single dynamically loadable library:\n```bash\nnvcc -std=c++14 -c -o cuda_op_kernel.cu.o cuda_op_kernel.cu.cc \\\n  ${TF_CFLAGS[@]} -D GOOGLE_CUDA=1 -x cu -Xcompiler -fPIC\ng++ -std=c++14 -shared -o cuda_op_kernel.so cuda_op_kernel.cc \\\n  cuda_op_kernel.cu.o ${TF_CFLAGS[@]} -fPIC -lcudart ${TF_LFLAGS[@]}\n```\n`cuda_op_kernel.so` produced above can be loaded as usual in Python, using the\n`tf.load_op_library` function.\nNote that if your CUDA libraries are not installed in `/usr/local/lib64`,\nyou'll need to specify the path explicitly in the second (g++) command above.\nFor example, add `-L /usr/local/cuda-8.0/lib64/` if your CUDA is installed in\n`/usr/local/cuda-8.0`.\nNote: In some Linux settings, additional options to `nvcc` compiling step are\nneeded. Add `-D_MWAITXINTRIN_H_INCLUDED` to the `nvcc` command line to avoid\nerrors from `mwaitxintrin.h`.\nImplement the gradient in Python\nGiven a graph of ops, TensorFlow uses automatic differentiation\n(backpropagation) to add new ops representing gradients with respect to the\nexisting ops.\nTo make automatic differentiation work for new ops, you must register a gradient\nfunction which computes gradients with respect to the ops' inputs given\ngradients with respect to the ops' outputs.\nMathematically, if an op computes \\(y = f(x)\\) the registered gradient op\nconverts gradients \\(\\partial L/ \\partial y\\) of loss \\(L\\) with respect to\n\\(y\\) into gradients \\(\\partial L/ \\partial x\\) with respect to \\(x\\) via\nthe chain rule:\n$$\\frac{\\partial L}{\\partial x}\n    = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial x}\n    = \\frac{\\partial L}{\\partial y} \\frac{\\partial f}{\\partial x}.$$\nIn the case of `ZeroOut`, only one entry in the input affects the output, so the\ngradient with respect to the input is a sparse \"one hot\" tensor.  This is\nexpressed as follows:\n```python\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import sparse_ops\n@ops.RegisterGradient(\"ZeroOut\")\ndef _zero_out_grad(op, grad):\n  \"\"\"The gradients for `zero_out`.\nArgs:\n    op: The `zero_out` `Operation` that we are differentiating, which we can use\n      to find the inputs and outputs of the original op.\n    grad: Gradient with respect to the output of the `zero_out` op.\nReturns:\n    Gradients with respect to the input of `zero_out`.\n  \"\"\"\n  to_zero = op.inputs[0]\n  shape = array_ops.shape(to_zero)\n  index = array_ops.zeros_like(shape)\n  first_grad = array_ops.reshape(grad, [-1])[0]\n  to_zero_grad = sparse_ops.sparse_to_dense([index], shape, first_grad, 0)\n  return [to_zero_grad]  # List of one Tensor, since we have one input\n```\nDetails about registering gradient functions with\n`tf.RegisterGradient`:\n\n\nFor an op with one output, the gradient function will take an `tf.Operation`,\n  `op`, and a `tf.Tensor` `grad` and build new ops out of the tensors\n  `op.inputs[i]`, `op.outputs[i]`, and `grad`. Information about any attrs can\n  be found via `tf.Operation.get_attr`.\n\n\nIf the op has multiple outputs, the gradient function will take `op` and\n  `grads`, where `grads` is a list of gradients with respect to each output.\n  The result of the gradient function must be a list of `Tensor` objects\n  representing the gradients with respect to each input.\n\n\nIf there is no well-defined gradient for some input, such as for integer\n  inputs used as indices, the corresponding returned gradient should be\n  `None`.  For example, for an op taking a floating point tensor `x` and an\n  integer index `i`, the gradient function would `return [x_grad, None]`.\n\n\nIf there is no meaningful gradient for the op at all, you often will not have\n  to register any gradient, and as long as the op's gradient is never needed,\n  you will be fine. In some cases, an op has no well-defined gradient but can\n  be involved in the computation of the gradient. Here you can use\n  `ops.NotDifferentiable` to automatically propagate zeros backwards.\n\n\nNote that at the time the gradient function is called, only the data flow graph\nof ops is available, not the tensor data itself.  Thus, all computation must be\nperformed using other tensorflow ops, to be run at graph execution time.\nShape functions in C++\nThe TensorFlow API has a feature called \"shape inference\" that provides\ninformation about the shapes of tensors without having to execute the\ngraph. Shape inference is supported by \"shape functions\" that are registered for\neach op type in the C++ `REGISTER_OP` declaration, and perform two roles:\nasserting that the shapes of the inputs are compatible during graph\nconstruction, and specifying the shapes for the outputs.\nShape functions are defined as operations on the\n`shape_inference::InferenceContext` class. For example, in the shape function\nfor ZeroOut:\n`c++\n    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\n      c->set_output(0, c->input(0));\n      return Status::OK();\n    });`\n`c->set_output(0, c->input(0));` declares that the first output's shape should\nbe set to the first input's shape. If the output is selected by its index as in the above example, the second parameter of `set_output` should be a `ShapeHandle` object. You can create an empty `ShapeHandle` object by its default constructor. The `ShapeHandle` object for an input with index `idx` can be obtained by `c->input(idx)`.\nThere are a number of common shape functions\nthat apply to many ops, such as `shape_inference::UnchangedShape` which can be\nfound in common_shape_fns.h and used as follows:\n`c++\nREGISTER_OP(\"ZeroOut\")\n    .Input(\"to_zero: int32\")\n    .Output(\"zeroed: int32\")\n    .SetShapeFn(::tensorflow::shape_inference::UnchangedShape);`\nA shape function can also constrain the shape of an input. For the version of\nZeroOut with a vector shape constraint, the shape function\nwould be as follows:\n`c++\n    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\n      ::tensorflow::shape_inference::ShapeHandle input;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 1, &input));\n      c->set_output(0, input);\n      return Status::OK();\n    });`\nThe `WithRank` call validates that the input shape `c->input(0)` has\na shape with exactly one dimension (or if the input shape is unknown,\nthe output shape will be a vector with one unknown dimension).\nIf your op is polymorphic with multiple inputs, you can use\nmembers of `InferenceContext` to determine the number of shapes to check, and\n`Merge` to validate that the shapes are all compatible (alternatively, access\nattributes that indicate the lengths, with `InferenceContext::GetAttr`, which\nprovides access to the attributes of the op).\n`c++\n    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\n      ::tensorflow::shape_inference::ShapeHandle input;\n      ::tensorflow::shape_inference::ShapeHandle output;\n      for (size_t i = 0; i < c->num_inputs(); ++i) {\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(i), 2, &input));\n        TF_RETURN_IF_ERROR(c->Merge(output, input, &output));\n      }\n      c->set_output(0, output);\n      return Status::OK();\n    });`\nSince shape inference is an optional feature, and the shapes of tensors may vary\ndynamically, shape functions must be robust to incomplete shape information for\nany of the inputs. The `Merge` method in InferenceContext\nallows the caller to assert that two shapes are the same, even if either\nor both of them do not have complete information. Shape functions are defined\nfor all of the core TensorFlow ops and provide many different usage examples.\nThe `InferenceContext` class has a number of functions that can be used to\ndefine shape function manipulations.  For example, you can validate that a\nparticular dimension has a very specific value using `InferenceContext::Dim` and\n`InferenceContext::WithValue`; you can specify that an output dimension is the\nsum / product of two input dimensions using `InferenceContext::Add` and\n`InferenceContext::Multiply`. See the `InferenceContext` class for\nall of the various shape manipulations you can specify. The following example sets\nshape of the first output to (n, 3), where first input has shape (n, ...)\n`c++\n.SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\n    c->set_output(0, c->Matrix(c->Dim(c->input(0), 0), 3));\n    return Status::OK();\n});`\nIf you have a complicated shape function, you should consider adding a test for\nvalidating that various input shape combinations produce the expected output\nshape combinations.  You can see examples of how to write these tests in some\nour\ncore ops tests.\n(The syntax of `INFER_OK` and `INFER_ERROR` are a little cryptic, but try to be\ncompact in representing input and output shape specifications in tests.  For\nnow, see the surrounding comments in those tests to get a sense of the shape\nstring specification).\nBuild a pip package for your custom op\nTo build a `pip` package for your op, see the\ntensorflow/custom-op example. This\nguide shows how to build custom ops from the TensorFlow pip package instead\nof building TensorFlow from source.",
    "tag": "tensorflow"
  },
  {
    "title": "TensorFlow Core APIs overview",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/guide/core/index.md",
    "content": "TensorFlow Core APIs overview\nThe TensorFlow Core APIs provide a set of comprehensive, composable, and\nextensible low-level APIs for high-performance (distributed & accelerated)\ncomputation, primarily aimed at building machine learning (ML) models as well as\nauthoring ML workflow tools and frameworks within the TensorFlow platform. These\nAPIs provide a foundation for creating highly configurable models with\nfine-grain control and new frameworks from the ground up.\nThe Core APIs can be used as an alternative to high-level machine learning APIs\nlike Keras. These high-level APIs are best suited for general machine learning\nneeds. They offer a variety of modules that abstract away the complexities of ML\nwhile also offering functionalities for customization through subclassing. If\nyou are looking for an overview of TensorFlow using Keras, see the Quickstarts\nand Keras sections in the tutorials\nCore API developer audience\nThe TensorFlow Core low-level APIs are designed with the following ML Developers\nin mind:\n\nResearchers building complex models with high levels of configurability\nDevelopers interested in using TensorFlow as a high-performance scientific\n    computing platform\nFramework authors building tools on top of the TensorFlow platform\nHigh-level API users interested in:\nAdding additional functionalities to their machine learning workflows\n    such as custom layers, losses, models, and optimizers\nLearning more about the inner workings of their models\n\n\n\nCore API applications\nThe TensorFlow Core APIs provide access to low level functionality within the\nTensorFlow ecosystem. This API provides more flexibility and control for\nbuilding ML models, applications, and tools, compared to high-level APIs, such\nas Keras.\nBuild models and workflows\nThe Core APIs are most commonly used to build highly customizable and optimized\nmachine learning models and workflows. Here are some of the ways that the\nTensorFlow Core APIs can improve your machine learning models and workflow\ndevelopment:\n\n\nBuilding non-traditional models or layers that do not fully fit the\n    structures supported by high-level APIs\nBuilding custom layers, losses, models, and optimizers within Keras\nImplementing new optimization techniques to expedite convergence during\n    training\nCreating custom metrics for performance evaluation\nDesigning highly-configurable training loops with support for features like\n    batching, cross-validation, and distribution strategies\n\nBuild frameworks and tools\nThe TensorFlow Core APIs can also serve as the building blocks for new\nhigh-level frameworks. Here are some examples of tools and frameworks that are\ncreated with the low-level APIs:\n\n\nKeras: deep learning for humans\nTensorFlow Model Optimization Toolkit:\n    a suite of tools to optimize ML models for deployment and execution\nTensorFlow Graphics: a library for\n    making useful graphics functions widely accessible\n\nBuild for scientific computing\nThe TensorFlow Core APIs can also be applied outside the realm of machine\nlearning. Here are a few general-purpose use cases of TensorFlow for scientific\ncomputing:\n\n\nPhysics simulations for solid mechanics and\n    fluid dynamics problems\nGraphics rendering applications like\n    ray tracing\nSolving\n    constrained optimization problems\n\nCore API components\nHere are some of the fundamental components that comprise TensorFlow Core\u2019s low-\nlevel APIs. Note that this is not an all-encompassing list:\n\n\nData structures : `tf.Tensor`, `tf.Variable`, `tf.TensorArray`\nPrimitive APIs: `tf.shape`,\n    slicing, `tf.concat`,\n    `tf.bitwise`\nNumerical: `tf.math`, `tf.linalg`, `tf.random`\nFunctional components: `tf.function`, `tf.GradientTape`\nDistribution: DTensor\nExport: `tf.saved_model`\n\nNext steps\nThe Build with Core documentation provides tutorials of basic machine learning\nconcepts from scratch. These tutorials in this section help you get comfortable\nwith writing low-level code with Core APIs that you can then apply to more\ncomplex use cases of your own.\nNote: You should not use the Core APIs to simply re-implement high-level APIs,\nand it is possible to use high-level APIs, such as Keras, with the Core APIs.\nTo get started using and learning more about the Core APIs, check out the",
    "tag": "tensorflow"
  },
  {
    "title": "TF1.x -> TF2 migration overview",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/guide/migrate/migrate_tf2.md",
    "content": "TF1.x -> TF2 migration overview\nTensorFlow 2 is fundamentally different from TF1.x in several ways. You can\nstill run unmodified TF1.x code\n(except for contrib)\nagainst TF2 binary installations like so:\n`python\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()`\nHowever, this is not running TF2 behaviors and APIs, and may not work as\nexpected with code written for TF2. If you are not running with TF2 behaviors\nactive, you are effectively running TF1.x on top of a TF2 installation. Read the\nTF1 vs TF2 behaviors guide for more details on how TF2 is\ndifferent from TF1.x.\nThis guide provides an overview of the process to migrate your TF1.x code to\nTF2. This enables you to take advantage of new and future feature improvements\nand also make your code simpler, more performant, and easier to maintain.\nIf you are using `tf.keras`'s high level APIs and training exclusively with\n`model.fit`, your code should more or less be fully compatible with TF2 except\nfor the following caveats:\n\nTF2 has new\n    default learning rates\n    for Keras optimizers.\nTF2 may have changed\n    the \"name\" that metrics are logged to.\n\nTF2 migration process\nBefore migrating, learn about the behavior and API differences between TF1.x and\nTF2 by reading the guide.\n\nRun the automated script to convert some of your TF1.x API usage to\n    `tf.compat.v1`.\nRemove old `tf.contrib` symbols (check\n    TF Addons and\n    TF-Slim).\nMake your TF1.x model forward passes run in TF2 with eager execution\n    enabled.\nUpgrade your TF1.x code for training loops and saving/loading models to TF2\n    equivalents.\n(Optional) Migrate your TF2-compatible `tf.compat.v1` APIs to idiomatic TF2\n    APIs.\n\nThe following sections expand upon the steps outlined above.\nRun the symbol conversion script\nThis executes an initial pass at rewriting your code symbols to run against TF\n2.x binaries, but won't make your code idiomatic to TF 2.x nor will it\nautomatically make your code compatible with TF2 behaviors.\nYour code will most likely still make use of `tf.compat.v1` endpoints to access\nplaceholders, sessions, collections, and other TF1.x-style functionality.\nRead the guide to find out more about the best practices for\nusing the symbol conversion script.\nRemove usage of `tf.contrib`\nThe `tf.contrib` module has been sunsetted and several of its submodules have\nbeen integrated into the core TF2 API. The other submodules are now spun-off\ninto other projects like TF IO and\nTF Addons.\nA large amount of older TF1.x code uses the\nSlim\nlibrary, which was packaged with TF1.x as `tf.contrib.layers`. When migrating\nyour Slim code to TF2, switch your Slim API usages to point to the\ntf-slim pip package. Then, read the\nmodel mapping guide\nto learn how to convert Slim code.\nAlternatively, if you use Slim pre-trained models you may consider trying out\nKeras's pre-traimed models from `tf.keras.applications` or\nTF Hub's TF2 `SavedModel`s exported\nfrom the original Slim code.\nMake TF1.x model forward passes run with TF2 behaviors enabled\nTrack variables and losses\nTF2 does not support global collections.\nEager execution in TF2 does not support `tf.Graph` collection-based APIs. This\naffects how you construct and track variables.\nFor new TF2 code you would use `tf.Variable` instead of `v1.get_variable` and\nuse Python objects to collect and track variables instead of\n`tf.compat.v1.variable_scope`. Typically this would be one of:\n\n`tf.keras.layers.Layer`\n`tf.keras.Model`\n`tf.Module`\n\nAggregate lists of variables (like\n`tf.Graph.get_collection(tf.GraphKeys.VARIABLES)`) with the `.variables` and\n`.trainable_variables` attributes of the `Layer`, `Module`, or `Model` objects.\nThe `Layer` and `Model` classes implement several other properties that remove\nthe need for global collections. Their `.losses` property can be a replacement\nfor using the `tf.GraphKeys.LOSSES` collection.\nRead the model mapping guide to find out more about\nusing the TF2 code modeling shims to embed your existing `get_variable` and\n`variable_scope` based code inside of `Layers`, `Models`, and `Modules`. This\nwill let you the execute forward passes with eager execution enabled without\nmajor rewrites.\nAdapting to other behavior changes\nIf the model mapping guide on its own is insufficient\nto get your model forward pass running other behavior changes that may be more\ndetails, see the guide on TF1.x vs TF2 behaviors to learn\nabout the other behavior changes and how you can adapt to them. Also check out\nthe\nmaking new Layers and Models via subclassing guide\nfor details.\nValidating your results\nSee the model validation guide for easy tools\nand guidance around how you can (numerically) validate that your model is\nbehaving correctly when eager execution is enabled. You may find this especially\nuseful when paired with the model mapping guide.\nUpgrade training, evaluation, and import/export code\nTF1.x training loops built with `v1.Session`-style `tf.estimator.Estimator`s and\nother collections-based approaches are not compatible with the new behaviors of\nTF2. It is important you migrate all your TF1.x training code as combining it\nwith TF2 code can cause unexpected behaviors.\nYou can choose from among several strategies to do this.\nThe highest-level approach is to use `tf.keras`. The high level functions in\nKeras manage a lot of the low-level details that might be easy to miss if you\nwrite your own training loop. For example, they automatically collect the\nregularization losses, and set the `training=True` argument when calling the\nmodel.\nRefer to the Estimator migration guide to learn\nhow you can migrate `tf.estimator.Estimator`s code to use\nvanilla and\ncustom\n`tf.keras` training loops.\nCustom training loops give you finer control over your model such as tracking\nthe weights of individual layers. Read the guide on\nbuilding training loops from scratch\nto learn how to use `tf.GradientTape` to retrieve model weights and use them to\nupdate the model.\nConvert TF1.x optimizers to Keras optimizers\nThe optimizers in `tf.compat.v1.train`, such as the\nAdam optimizer\nand the\ngradient descent optimizer,\nhave equivalents in `tf.keras.optimizers`.\nThe table below summarizes how you can convert these legacy optimizers to their\nKeras equivalents. You can directly replace the TF1.x version with the TF2\nversion unless additional steps (such as\nupdating the default learning rate)\nare required.\nNote that converting your optimizers\nmay make old checkpoints incompatible.\n\n\nTF1.x\nTF2\nAdditional steps\n\n\n`tf.v1.train.GradientDescentOptimizer`\n`tf.keras.optimizers.SGD`\nNone\n\n\n`tf.v1.train.MomentumOptimizer`\n`tf.keras.optimizers.SGD`\nInclude the `momentum` argument\n\n\n`tf.v1.train.AdamOptimizer`\n`tf.keras.optimizers.Adam`\nRename `beta1` and `beta2` arguments to `beta_1` and `beta_2`\n\n\n`tf.v1.train.RMSPropOptimizer`\n`tf.keras.optimizers.RMSprop`\nRename the `decay` argument to `rho`\n\n\n`tf.v1.train.AdadeltaOptimizer`\n`tf.keras.optimizers.Adadelta`\nNone\n\n\n`tf.v1.train.AdagradOptimizer`\n`tf.keras.optimizers.Adagrad`\nNone\n\n\n`tf.v1.train.FtrlOptimizer`\n`tf.keras.optimizers.Ftrl`\nRemove the `accum_name` and `linear_name` arguments\n\n\n`tf.contrib.AdamaxOptimizer`\n`tf.keras.optimizers.Adamax`\nRename the `beta1`, and `beta2` arguments to `beta_1` and `beta_2`\n\n\n`tf.contrib.Nadam`\n`tf.keras.optimizers.Nadam`\nRename the `beta1`, and `beta2` arguments to `beta_1` and `beta_2`\n\n\nNote: In TF2, all epsilons (numerical stability constants) now default to `1e-7`\ninstead of `1e-8`. This difference is negligible in most use cases.\nUpgrade data input pipelines\nThere are many ways to feed data to a `tf.keras` model. They will accept Python\ngenerators and Numpy arrays as input.\nThe recommended way to feed data to a model is to use the `tf.data` package,\nwhich contains a collection of high performance classes for manipulating data.\nThe `dataset`s belonging to `tf.data` are efficient, expressive, and integrate\nwell with TF2.\nThey can be passed directly to the `tf.keras.Model.fit` method.\n`python\nmodel.fit(dataset, epochs=5)`\nThey can be iterated over directly standard Python:\n`python\nfor example_batch, label_batch in dataset:\n    break`\nIf you are still using `tf.queue`, these are now only supported as\ndata-structures, not as input pipelines.\nYou should also migrate all feature preprocessing code that\nuses`tf.feature_columns`. Read the\nmigration guide for more details.\nSaving and loading models\nTF2 uses object-based checkpoints. Read the\ncheckpoint migration guide to learn more about\nmigrating off name-based TF1.x checkpoints. Also read the\ncheckpoints guide in the core\nTensorFlow docs.\nThere are no significant compatibility concerns for saved models. Read the\nSavedModel guide for more information about migrating\n`SavedModel`s in TF1.x to TF2. In general,\n\nTF1.x saved_models work in TF2.\nTF2 saved_models work in TF1.x if all the ops are supported.\n\nAlso refer to the\nGraphDef section in the\n`SavedModel` migration guide for more information on working with `Graph.pb` and\n`Graph.pbtxt` objects.\n(Optional) Migrate off `tf.compat.v1` symbols\nThe `tf.compat.v1` module contains the complete TF1.x API, with its original\nsemantics.\nEven after following the steps above and ending up with code that is fully\ncompatible with all TF2 behaviors, it is likely there may be many mentions of\n`compat.v1` apis that happen to be compatible with TF2. You should avoid using\nthese legacy `compat.v1` apis for any new code that you write, though they will\ncontinue working for your already-written code.\nHowever, you may choose to migrate the existing usages to non-legacy TF2 APIs.\nThe docstrings of individual `compat.v1` symbols will often explain how to\nmigrate them to non-legacy TF2 APIs. Additionally, the\nmodel mapping guide's section on incremental migration to idiomatic TF2 APIs\nmay help with this as well.\nResources and further reading\nAs mentioned previously, it is a good practice to migrate all your TF1.x code to\nTF2. Read the guides in the\nMigrate to TF2 section of the TensorFlow",
    "tag": "tensorflow"
  },
  {
    "title": "Install TensorFlow for Java",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/install/lang_java_legacy.md",
    "content": "Install TensorFlow for Java\nWarning: TensorFlow for Java is deprecated and will be removed in a future\nversion of TensorFlow once the replacement is stable.\nTensorFlow provides a\nJava API\u2014\nuseful for loading models created with Python and running them within a Java\napplication.\nNightly Libtensorflow Java packages\nLibtensorflow JNI packages are built nightly and uploaded to GCS for all\nsupported platforms. They are uploaded to the\nlibtensorflow-nightly GCS bucket\nand are indexed by operating system and date built.\nSupported Platforms\nTensorFlow for Java is supported on the following systems:\n\nUbuntu 16.04 or higher; 64-bit, x86\nmacOS 10.12.6 (Sierra) or higher\nWindows 7 or higher; 64-bit, x86\n\nTo use TensorFlow on Android see TensorFlow Lite\nTensorFlow with Apache Maven\nTo use TensorFlow with Apache Maven{:.external},\nadd the dependency to the project's `pom.xml` file:\n`xml\n<dependency>\n  <groupId>org.tensorflow</groupId>\n  <artifactId>tensorflow</artifactId>\n  <version>2.4.0</version>\n</dependency>`\nGPU support\nIf your system has GPU support, add the following TensorFlow\ndependencies to the project's `pom.xml` file:\n`xml\n<dependency>\n  <groupId>org.tensorflow</groupId>\n  <artifactId>libtensorflow</artifactId>\n  <version>2.4.0</version>\n</dependency>\n<dependency>\n  <groupId>org.tensorflow</groupId>\n  <artifactId>libtensorflow_jni_gpu</artifactId>\n  <version>2.4.0</version>\n</dependency>`\nExample program\nThis example shows how to build an Apache Maven project with TensorFlow. First,\nadd the TensorFlow dependency to the project's `pom.xml` file:\n`xml\n<project>\n  <modelVersion>4.0.0</modelVersion>\n  <groupId>org.myorg</groupId>\n  <artifactId>hellotensorflow</artifactId>\n  <version>1.0-SNAPSHOT</version>\n  <properties>\n    <exec.mainClass>HelloTensorFlow</exec.mainClass>\n    <!-- The sample code requires at least JDK 1.7. -->\n    <!-- The maven compiler plugin defaults to a lower version -->\n    <maven.compiler.source>1.7</maven.compiler.source>\n    <maven.compiler.target>1.7</maven.compiler.target>\n  </properties>\n  <dependencies>\n    <dependency>\n      <groupId>org.tensorflow</groupId>\n      <artifactId>tensorflow</artifactId>\n      <version>1.14.0</version>\n    </dependency>\n  </dependencies>\n</project>`\nCreate the source file (`src/main/java/HelloTensorFlow.java`):\n```java\nimport org.tensorflow.Graph;\nimport org.tensorflow.Session;\nimport org.tensorflow.Tensor;\nimport org.tensorflow.TensorFlow;\npublic class HelloTensorFlow {\n  public static void main(String[] args) throws Exception {\n    try (Graph g = new Graph()) {\n      final String value = \"Hello from \" + TensorFlow.version();\n\n\n```  // Construct the computation graph with a single operation, a constant\n  // named \"MyConst\" with a value \"value\".\n  try (Tensor t = Tensor.create(value.getBytes(\"UTF-8\"))) {\n    // The Java API doesn't yet include convenience functions for adding operations.\n    g.opBuilder(\"Const\", \"MyConst\").setAttr(\"dtype\", t.dataType()).setAttr(\"value\", t).build();\n  }\n\n  // Execute the \"MyConst\" operation in a Session.\n  try (Session s = new Session(g);\n      // Generally, there may be multiple output tensors,\n      // all of them must be closed to prevent resource leaks.\n      Tensor output = s.runner().fetch(\"MyConst\").run().get(0)) {\n    System.out.println(new String(output.bytesValue(), \"UTF-8\"));\n  }\n}\n```\n\n\n}\n}\n```\nCompile and execute:\n\nmvn -q compile exec:java  # Use -q to hide logging\n\nThe command outputs: `Hello from version`\nSuccess: TensorFlow for Java is configured.\nTensorFlow with the JDK\nTensorFlow can be used with the JDK through the Java Native Interface (JNI).\nDownload\n\nDownload the TensorFlow Jar Archive (JAR): libtensorflow.jar\nDownload and extract the Java Native Interface (JNI) file for your operating\nsystem and processor support:\n\n\nJNI versionURL\nLinux\n\nLinux CPU only\nhttps://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow_jni-cpu-linux-x86_64-2.4.0.tar.gz\n\n\nLinux GPU support\nhttps://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow_jni-gpu-linux-x86_64-2.4.0.tar.gz\n\nmacOS\n\nmacOS CPU only\nhttps://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow_jni-cpu-darwin-x86_64-2.4.0.tar.gz\n\nWindows\n\nWindows CPU only\nhttps://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow_jni-cpu-windows-x86_64-2.4.0.zip\n\n\nWindows GPU support\nhttps://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow_jni-gpu-windows-x86_64-2.4.0.zip\n\n\nNote: On Windows, the native library (`tensorflow_jni.dll`) requires\n`msvcp140.dll` at runtime. See the\nWindows build from source guide to install the\nVisual C++ 2019 Redistributable{:.external}.\nCompile\nUsing the `HelloTensorFlow.java` file from the previous example,\ncompile a program that uses TensorFlow. Make sure the `libtensorflow.jar` is\naccessible to your `classpath`:\n\njavac -cp libtensorflow-2.4.0.jar HelloTensorFlow.java\n\nRun\nTo execute a TensorFlow Java program, the JVM must access `libtensorflow.jar` and\nthe extracted JNI library.\n\n\nLinux / macOS\njava -cp libtensorflow-2.4.0.jar:. -Djava.library.path=./jni HelloTensorFlow\n\n\nWindows\njava -cp libtensorflow-2.4.0.jar;. -Djava.library.path=jni HelloTensorFlow\n\n\nThe command outputs: `Hello from version`\nSuccess: TensorFlow for Java is configured.\nBuild from source\nTensorFlow is open source. Read\nthe instructions{:.external}\n",
    "tag": "tensorflow"
  },
  {
    "title": "Build from source",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/install/source.md",
    "content": "Build from source\nBuild a TensorFlow pip package from source and install it on Ubuntu Linux and\nmacOS. While the instructions might work for other systems, it is only tested\nand supported for Ubuntu and macOS.\nNote: Well-tested, pre-built TensorFlow packages for Linux and macOS\nsystems are already provided.\nSetup for Linux and macOS\nInstall the following build tools to configure your development environment.\nInstall Python and the TensorFlow package dependencies\n\n\nUbuntu\n\n\n\n```sudo apt install python3-dev python3-pip```\n\n\n\n\n\nmacOS\nRequires Xcode 9.2 or later.\nInstall using the Homebrew package manager:\n\n\n\n```/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"```\n\n\n\n\n```export PATH=\"/usr/local/opt/python/libexec/bin:$PATH\"```\n\n\n\n\n```# if you are on macOS 10.12 (Sierra) use `export PATH=\"/usr/local/bin:/usr/local/sbin:$PATH\"````\n\n\n\n\n```brew install python```\n\n\n\n\n\nInstall the TensorFlow pip package dependencies (if using a virtual\nenvironment, omit the `--user` argument):\n\n\n\n```pip install -U --user pip numpy wheel packaging requests opt_einsum```\n\n\n\n\n```pip install -U --user keras_preprocessing --no-deps```\n\n\n\nNote: A `pip` version >19.0 is required to install the TensorFlow 2 `.whl`\npackage. Additional required dependencies are listed in the\nsetup.py\nfile under `REQUIRED_PACKAGES`.\nInstall Bazel\nTo build TensorFlow, you will need to install Bazel.\nBazelisk is an easy way to install\nBazel and automatically downloads the correct Bazel version for TensorFlow. For\nease of use, add Bazelisk as the `bazel` executable in your `PATH`.\nIf Bazelisk is not available, you can manually\ninstall Bazel. Make\nsure to install the correct Bazel version from TensorFlow's\n.bazelversion\nfile.\nInstall GPU support (optional, Linux only)\nThere is no GPU support for macOS.\nRead the GPU support guide to install the drivers and additional\nsoftware required to run TensorFlow on a GPU.\nNote: It is easier to set up one of TensorFlow's GPU-enabled Docker images.\nDownload the TensorFlow source code\nUse Git{:.external} to clone the\nTensorFlow repository{:.external}:\n\n\n\n```git clone https://github.com/tensorflow/tensorflow.git```\n\n\n\n\n```cd tensorflow```\n\n\n\nThe repo defaults to the `master` development branch. You can also check out a\nrelease branch{:.external}\nto build:\n\ngit checkout branch_name  # r2.2, r2.3, etc.\n\nOptional: Configure the build\nTensorFlow builds are configured by the `.bazelrc` file in the repository's\nroot directory. The `./configure` or `./configure.py` scripts can be used to\nadjust common settings.\nIf you need to change the configuration, run the `./configure` script from\nthe repository's root directory. This script will prompt you for the location of\nTensorFlow dependencies and asks for additional build configuration options\n(compiler flags, for example). Refer to the Sample session section for\ndetails.\n\n./configure\n\nThere is also a python version of this script, `./configure.py`. If using a\nvirtual environment, `python configure.py` prioritizes paths\nwithin the environment, whereas `./configure` prioritizes paths outside\nthe environment. In both cases you can change the default.\nSample session\nThe following shows a sample run of `./configure` script (your\nsession may differ):\n\nView sample configuration session\n\n./configure\nYou have bazel 5.0.0 installed.\nPlease specify the location of python. [Default is /Library/Frameworks/Python.framework/Versions/3.9/bin/python3]: \n\n\nFound possible Python library paths:\n  /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages\nPlease input the desired Python library path to use.  Default is [/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages]\n\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\nNo ROCm support will be enabled for TensorFlow.\n\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\nNo CUDA support will be enabled for TensorFlow.\n\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: n\nClang will not be downloaded.\n\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -Wno-sign-compare]: n\n\n\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\nNot configuring the WORKSPACE for Android builds.\n\nDo you wish to build TensorFlow with iOS support? [y/N]: n\nNo iOS support will be enabled for TensorFlow.\n\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\n    --config=mkl            # Build with MKL support.\n    --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).\n    --config=monolithic     # Config for mostly static monolithic build.\n    --config=numa           # Build with NUMA support.\n    --config=dynamic_kernels    # (Experimental) Build kernels into separate shared objects.\n    --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.\nPreconfigured Bazel build configs to DISABLE default on features:\n    --config=nogcp          # Disable GCP support.\n    --config=nonccl         # Disable NVIDIA NCCL support.\n\n\n\nConfiguration options\nGPU support\nFor GPU support, set `cuda=Y` during configuration and specify the\nversions of CUDA and cuDNN. If your system has multiple versions of CUDA or\ncuDNN installed, explicitly set the version instead of relying on the default.\n`./configure` creates symbolic links to your system's CUDA libraries\u2014so if you\nupdate your CUDA library paths, this configuration step must be run again before\nbuilding.\nOptimizations\nFor compilation optimization flags, the default (`-march=native`) optimizes the\ngenerated code for your machine's CPU type. However, if building TensorFlow for\na different CPU type, consider a more specific optimization flag. Check the\nGCC manual{:.external}\nfor examples.\nPreconfigured configurations\nThere are some preconfigured build configs available that can be added to the\n`bazel build` command, for example:\n\n`--config=dbg` \u2014Build with debug info. See\n    CONTRIBUTING.md\n    for details.\n`--config=mkl` \u2014Support for the\n    Intel\u00ae MKL-DNN{:.external}.\n`--config=monolithic` \u2014Configuration for a mostly static, monolithic build.\n\nBuild and install the pip package\nThe pip package is build in two steps. A `bazel build` commands creates a\n\"package-builder\" program. You then run the package-builder to create the\npackage.\nBuild the package-builder\nUse `bazel build` to create the TensorFlow 2.x package-builder with CPU-only\nsupport:\n\nbazel build [--config=option] //tensorflow/tools/pip_package:build_pip_package\n\nGPU support\nNote: GPU support can be enabled with `cuda=Y` during the `./configure` stage.\nTo build a TensorFlow package-builder with GPU support:\n\nbazel build --config=cuda [--config=option] //tensorflow/tools/pip_package:build_pip_package\n\nBazel build options\nRefer to the Bazel\ncommand-line reference\nfor\nbuild options.\nBuilding TensorFlow from source can use a lot of RAM. If your system is\nmemory-constrained, limit Bazel's RAM usage with: `--local_ram_resources=2048`.\nThe official TensorFlow packages are built with a GCC\ntoolchain that complies with the manylinux2010 package standard.\nFor GCC 5 and later, compatibility with the older ABI can be built using:\n`--cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"`. ABI compatibility ensures that custom\nops built against the official TensorFlow package continue to work with the\nGCC 5 built package.\nBuild the package\nThe `bazel build` command creates an executable named `build_pip_package`\u2014this\nis the program that builds the `pip` package. Run the executable as shown\nbelow to build a `.whl` package in the `/tmp/tensorflow_pkg` directory.\nTo build from a release branch:\n\n./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\n\nTo build from master, use `--nightly_flag` to get the right dependencies:\n\n./bazel-bin/tensorflow/tools/pip_package/build_pip_package --nightly_flag /tmp/tensorflow_pkg\n\nAlthough it is possible to build both CUDA and non-CUDA configurations under the\nsame source tree, it's recommended to run `bazel clean` when switching between\nthese two configurations in the same source tree.\nInstall the package\nThe filename of the generated `.whl` file depends on the TensorFlow version and\nyour platform. Use `pip install` to install the package, for example:\n\npip install /tmp/tensorflow_pkg/tensorflow-version-tags.whl\n\nSuccess: TensorFlow is now installed.\nDocker Linux builds\nTensorFlow's Docker development images are an easy way to set up an environment\nto build Linux packages from source. These images already contain the source\ncode and dependencies required to build TensorFlow. Go to the TensorFlow\nDocker guide for installation instructions and the\nlist of available image tags{:.external}.\nCPU-only\nThe following example uses the `:devel` image to build a CPU-only package from\nthe latest TensorFlow source code. Check the Docker guide for\navailable TensorFlow `-devel` tags.\nDownload the latest development image and start a Docker container that you'll\nuse to build the pip package:\n\n\n\n```docker pull tensorflow/tensorflow:devel```\n\n\n\n\n```docker run -it -w /tensorflow_src -v $PWD:/mnt -e HOST_PERMS=\"$(id -u):$(id -g)\" \\\n    tensorflow/tensorflow:devel bash```\n\n\n\n\n\n```git pull  # within the container, download the latest source code```\n\n\n\nThe above `docker run` command starts a shell in the `/tensorflow_src`\ndirectory\u2014the root of the source tree. It mounts the host's current directory in\nthe container's `/mnt` directory, and passes the host user's information to the\ncontainer through an environmental variable (used to set permissions\u2014Docker can\nmake this tricky).\nAlternatively, to build a host copy of TensorFlow within a container, mount the\nhost source tree at the container's `/tensorflow` directory:\n\ndocker run -it -w /tensorflow -v /path/to/tensorflow:/tensorflow -v $PWD:/mnt \\\n    -e HOST_PERMS=\"$(id -u):$(id -g)\" tensorflow/tensorflow:devel bash\n\nWith the source tree set up, build the TensorFlow package within the container's\nvirtual environment:\n\nOptional: Configure the build\u2014this prompts the user to answer build configuration\n    questions.\nBuild the tool used to create the pip package.\nRun the tool to create the pip package.\nAdjust the ownership permissions of the file for outside the container.\n\n\n\n\n```./configure  # if necessary```\n\n\n\n\n\n```bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package```\n\n\n\n\n\n```./bazel-bin/tensorflow/tools/pip_package/build_pip_package /mnt  # create package```\n\n\n\n\n\n```chown $HOST_PERMS /mnt/tensorflow-version-tags.whl```\n\n\n\nInstall and verify the package within the container:\n\n\n\n```pip uninstall tensorflow  # remove current version```\n\n\n\n\n\n```pip install /mnt/tensorflow-version-tags.whl```\n\n\n\n\n```cd /tmp  # don't import from source directory```\n\n\n\n\n```python -c \"import tensorflow as tf; print(tf.__version__)\"```\n\n\n\nSuccess: TensorFlow is now installed.\nOn your host machine, the TensorFlow pip package is in the current directory\n(with host user permissions):\n`./tensorflow-version-tags.whl`\nGPU support\nDocker is the easiest way to build GPU support for TensorFlow since the host\nmachine only requires the\nNVIDIA\u00ae\u00a0driver{:.external}\n(the NVIDIA\u00ae CUDA\u00ae Toolkit doesn't have to be installed). Refer to the\nGPU support guide and the TensorFlow Docker guide to\nset up nvidia-docker{:.external}\n(Linux only).\nThe following example downloads the TensorFlow `:devel-gpu` image and uses\n`nvidia-docker` to run the GPU-enabled container. This development image is\nconfigured to build a pip package with GPU support:\n\n\n\n```docker pull tensorflow/tensorflow:devel-gpu```\n\n\n\n\n```docker run --gpus all -it -w /tensorflow -v $PWD:/mnt -e HOST_PERMS=\"$(id -u):$(id -g)\" \\\n    tensorflow/tensorflow:devel-gpu bash```\n\n\n\n\n```git pull  # within the container, download the latest source code```\n\n\n\nThen, within the container's virtual environment, build the TensorFlow package\nwith GPU support:\n\n\n\n```./configure  # if necessary```\n\n\n\n\n\n```bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package```\n\n\n\n\n\n```./bazel-bin/tensorflow/tools/pip_package/build_pip_package /mnt  # create package```\n\n\n\n\n\n```chown $HOST_PERMS /mnt/tensorflow-version-tags.whl```\n\n\n\nInstall and verify the package within the container and check for a GPU:\n\n\n\n```pip uninstall tensorflow  # remove current version```\n\n\n\n\n\n```pip install /mnt/tensorflow-version-tags.whl```\n\n\n\n\n```cd /tmp  # don't import from source directory```\n\n\n\n\n```python -c \"import tensorflow as tf; print(\\\"Num GPUs Available: \\\", len(tf.config.list_physical_devices('GPU')))\"```\n\n\n\nSuccess: TensorFlow is now installed.\n\nTested build configurations\nLinux\nCPU\n\nVersionPython versionCompilerBuild tools\ntensorflow-2.11.03.7-3.10GCC 9.3.1Bazel 5.3.0\ntensorflow-2.10.03.7-3.10GCC 9.3.1Bazel 5.0.0\ntensorflow-2.9.03.7-3.10GCC 9.3.1Bazel 5.0.0\ntensorflow-2.8.03.7-3.10GCC 7.3.1Bazel 4.2.1\ntensorflow-2.7.03.7-3.9GCC 7.3.1Bazel 3.7.2\ntensorflow-2.6.03.6-3.9GCC 7.3.1Bazel 3.7.2\ntensorflow-2.5.03.6-3.9GCC 7.3.1Bazel 3.7.2\ntensorflow-2.4.03.6-3.8GCC 7.3.1Bazel 3.1.0\ntensorflow-2.3.03.5-3.8GCC 7.3.1Bazel 3.1.0\ntensorflow-2.2.03.5-3.8GCC 7.3.1Bazel 2.0.0\ntensorflow-2.1.02.7, 3.5-3.7GCC 7.3.1Bazel 0.27.1\ntensorflow-2.0.02.7, 3.3-3.7GCC 7.3.1Bazel 0.26.1\ntensorflow-1.15.02.7, 3.3-3.7GCC 7.3.1Bazel 0.26.1\ntensorflow-1.14.02.7, 3.3-3.7GCC 4.8Bazel 0.24.1\ntensorflow-1.13.12.7, 3.3-3.7GCC 4.8Bazel 0.19.2\ntensorflow-1.12.02.7, 3.3-3.6GCC 4.8Bazel 0.15.0\ntensorflow-1.11.02.7, 3.3-3.6GCC 4.8Bazel 0.15.0\ntensorflow-1.10.02.7, 3.3-3.6GCC 4.8Bazel 0.15.0\ntensorflow-1.9.02.7, 3.3-3.6GCC 4.8Bazel 0.11.0\ntensorflow-1.8.02.7, 3.3-3.6GCC 4.8Bazel 0.10.0\ntensorflow-1.7.02.7, 3.3-3.6GCC 4.8Bazel 0.10.0\ntensorflow-1.6.02.7, 3.3-3.6GCC 4.8Bazel 0.9.0\ntensorflow-1.5.02.7, 3.3-3.6GCC 4.8Bazel 0.8.0\ntensorflow-1.4.02.7, 3.3-3.6GCC 4.8Bazel 0.5.4\ntensorflow-1.3.02.7, 3.3-3.6GCC 4.8Bazel 0.4.5\ntensorflow-1.2.02.7, 3.3-3.6GCC 4.8Bazel 0.4.5\ntensorflow-1.1.02.7, 3.3-3.6GCC 4.8Bazel 0.4.2\ntensorflow-1.0.02.7, 3.3-3.6GCC 4.8Bazel 0.4.2\n\nGPU\n\nVersionPython versionCompilerBuild toolscuDNNCUDA\ntensorflow-2.11.03.7-3.10GCC 9.3.1Bazel 5.3.08.111.2\ntensorflow-2.10.03.7-3.10GCC 9.3.1Bazel 5.1.18.111.2\ntensorflow-2.9.03.7-3.10GCC 9.3.1Bazel 5.0.08.111.2\ntensorflow-2.8.03.7-3.10GCC 7.3.1Bazel 4.2.18.111.2\ntensorflow-2.7.03.7-3.9GCC 7.3.1Bazel 3.7.28.111.2\ntensorflow-2.6.03.6-3.9GCC 7.3.1Bazel 3.7.28.111.2\ntensorflow-2.5.03.6-3.9GCC 7.3.1Bazel 3.7.28.111.2\ntensorflow-2.4.03.6-3.8GCC 7.3.1Bazel 3.1.08.011.0\ntensorflow-2.3.03.5-3.8GCC 7.3.1Bazel 3.1.07.610.1\ntensorflow-2.2.03.5-3.8GCC 7.3.1Bazel 2.0.07.610.1\ntensorflow-2.1.02.7, 3.5-3.7GCC 7.3.1Bazel 0.27.17.610.1\ntensorflow-2.0.02.7, 3.3-3.7GCC 7.3.1Bazel 0.26.17.410.0\ntensorflow_gpu-1.15.02.7, 3.3-3.7GCC 7.3.1Bazel 0.26.17.410.0\ntensorflow_gpu-1.14.02.7, 3.3-3.7GCC 4.8Bazel 0.24.17.410.0\ntensorflow_gpu-1.13.12.7, 3.3-3.7GCC 4.8Bazel 0.19.27.410.0\ntensorflow_gpu-1.12.02.7, 3.3-3.6GCC 4.8Bazel 0.15.079\ntensorflow_gpu-1.11.02.7, 3.3-3.6GCC 4.8Bazel 0.15.079\ntensorflow_gpu-1.10.02.7, 3.3-3.6GCC 4.8Bazel 0.15.079\ntensorflow_gpu-1.9.02.7, 3.3-3.6GCC 4.8Bazel 0.11.079\ntensorflow_gpu-1.8.02.7, 3.3-3.6GCC 4.8Bazel 0.10.079\ntensorflow_gpu-1.7.02.7, 3.3-3.6GCC 4.8Bazel 0.9.079\ntensorflow_gpu-1.6.02.7, 3.3-3.6GCC 4.8Bazel 0.9.079\ntensorflow_gpu-1.5.02.7, 3.3-3.6GCC 4.8Bazel 0.8.079\ntensorflow_gpu-1.4.02.7, 3.3-3.6GCC 4.8Bazel 0.5.468\ntensorflow_gpu-1.3.02.7, 3.3-3.6GCC 4.8Bazel 0.4.568\ntensorflow_gpu-1.2.02.7, 3.3-3.6GCC 4.8Bazel 0.4.55.18\ntensorflow_gpu-1.1.02.7, 3.3-3.6GCC 4.8Bazel 0.4.25.18\ntensorflow_gpu-1.0.02.7, 3.3-3.6GCC 4.8Bazel 0.4.25.18\n\nmacOS\nCPU\n\nVersionPython versionCompilerBuild tools\ntensorflow-2.11.03.7-3.10Clang from xcode 10.14Bazel 5.3.0\ntensorflow-2.10.03.7-3.10Clang from xcode 10.14Bazel 5.1.1\ntensorflow-2.9.03.7-3.10Clang from xcode 10.14Bazel 5.0.0\ntensorflow-2.8.03.7-3.10Clang from xcode 10.14Bazel 4.2.1\ntensorflow-2.7.03.7-3.9Clang from xcode 10.11Bazel 3.7.2\ntensorflow-2.6.03.6-3.9Clang from xcode 10.11Bazel 3.7.2\ntensorflow-2.5.03.6-3.9Clang from xcode 10.11Bazel 3.7.2\ntensorflow-2.4.03.6-3.8Clang from xcode 10.3Bazel 3.1.0\ntensorflow-2.3.03.5-3.8Clang from xcode 10.1Bazel 3.1.0\ntensorflow-2.2.03.5-3.8Clang from xcode 10.1Bazel 2.0.0\ntensorflow-2.1.02.7, 3.5-3.7Clang from xcode 10.1Bazel 0.27.1\ntensorflow-2.0.02.7, 3.5-3.7Clang from xcode 10.1Bazel 0.27.1\ntensorflow-2.0.02.7, 3.3-3.7Clang from xcode 10.1Bazel 0.26.1\ntensorflow-1.15.02.7, 3.3-3.7Clang from xcode 10.1Bazel 0.26.1\ntensorflow-1.14.02.7, 3.3-3.7Clang from xcodeBazel 0.24.1\ntensorflow-1.13.12.7, 3.3-3.7Clang from xcodeBazel 0.19.2\ntensorflow-1.12.02.7, 3.3-3.6Clang from xcodeBazel 0.15.0\ntensorflow-1.11.02.7, 3.3-3.6Clang from xcodeBazel 0.15.0\ntensorflow-1.10.02.7, 3.3-3.6Clang from xcodeBazel 0.15.0\ntensorflow-1.9.02.7, 3.3-3.6Clang from xcodeBazel 0.11.0\ntensorflow-1.8.02.7, 3.3-3.6Clang from xcodeBazel 0.10.1\ntensorflow-1.7.02.7, 3.3-3.6Clang from xcodeBazel 0.10.1\ntensorflow-1.6.02.7, 3.3-3.6Clang from xcodeBazel 0.8.1\ntensorflow-1.5.02.7, 3.3-3.6Clang from xcodeBazel 0.8.1\ntensorflow-1.4.02.7, 3.3-3.6Clang from xcodeBazel 0.5.4\ntensorflow-1.3.02.7, 3.3-3.6Clang from xcodeBazel 0.4.5\ntensorflow-1.2.02.7, 3.3-3.6Clang from xcodeBazel 0.4.5\ntensorflow-1.1.02.7, 3.3-3.6Clang from xcodeBazel 0.4.2\ntensorflow-1.0.02.7, 3.3-3.6Clang from xcodeBazel 0.4.2\n\nGPU\n\nVersionPython versionCompilerBuild toolscuDNNCUDA\ntensorflow_gpu-1.1.02.7, 3.3-3.6Clang from xcodeBazel 0.4.25.18\ntensorflow_gpu-1.0.02.7, 3.3-3.6Clang from xcodeBazel 0.4.25.18",
    "tag": "tensorflow"
  },
  {
    "title": "Build from source on Windows",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/install/source_windows.md",
    "content": "Build from source on Windows\nBuild a TensorFlow pip package from source and install it on Windows.\nNote: We already provide well-tested, pre-built\nTensorFlow packages for Windows systems.\nSetup for Windows\nInstall the following build tools to configure your Windows development\nenvironment.\nInstall Python and the TensorFlow package dependencies\nInstall a\nPython 3.7+ 64-bit release for Windows{:.external}.\nSelect pip as an optional feature and add it to your `%PATH%` environmental\nvariable.\nInstall the TensorFlow pip package dependencies:\n\n\n\n```pip3 install -U six numpy wheel packaging```\n\n\n\n\n```pip3 install -U keras_preprocessing --no-deps```\n\n\n\nThe dependencies are listed in the\nsetup.py\nfile under `REQUIRED_PACKAGES`.\nInstall Bazel\nInstall Bazel, the build tool used to compile\nTensorFlow. For Bazel version, see the\ntested build configurations for Windows.\nConfigure Bazel to\nbuild\nC++.\nAdd the location of the Bazel executable to your `%PATH%` environment variable.\nInstall MSYS2\nInstall MSYS2{:.external} for the bin tools needed to\nbuild TensorFlow. If MSYS2 is installed to `C:\\msys64`, add\n`C:\\msys64\\usr\\bin` to your `%PATH%` environment variable. Then, using `cmd.exe`,\nrun:\n\npacman -S git patch unzip\n\nInstall Visual C++ Build Tools 2019\nInstall the Visual C++ build tools 2019. This comes with Visual Studio 2019\nbut can be installed separately:\n\nGo to the\n    Visual Studio downloads{:.external},\nSelect Redistributables and Build Tools,\nDownload and install:\nMicrosoft Visual C++ 2019 Redistributable\nMicrosoft Build Tools 2019\n\n\n\nNote: TensorFlow is tested against the Visual Studio 2019.\nInstall GPU support (optional)\nSee the Windows GPU support guide to install the drivers and\nadditional software required to run TensorFlow on a GPU.\nDownload the TensorFlow source code\nUse Git{:.external} to clone the\nTensorFlow repository{:.external}\n(`git` is installed with MSYS2):\n\n\n\n```git clone https://github.com/tensorflow/tensorflow.git```\n\n\n\n\n```cd tensorflow```\n\n\n\nThe repo defaults to the `master` development branch. You can also check out a\nrelease branch{:.external}\nto build:\n\ngit checkout branch_name  # r1.9, r1.10, etc.\n\nKey Point: If you're having build problems on the latest development branch, try\na release branch that is known to work.\nOptional: Configure the build\nTensorFlow builds are configured by the `.bazelrc` file in the respoitory's\nroot directory. The `./configure` or `./configure.py` scripts can be used to\nadjust common settings.\nIf you need to change the configuration, run the `./configure` script from\nthe repository's root directory.\n\npython ./configure.py\n\nThis script prompts you for the location of TensorFlow dependencies and asks for\nadditional build configuration options (compiler flags, for example). The\nfollowing shows a sample run of `python ./configure.py` (your session may\ndiffer):\n\nView sample configuration session\n\npython ./configure.py\nStarting local Bazel server and connecting to it...\n................\nYou have bazel 0.15.0 installed.\nPlease specify the location of python. [Default is C:\\python36\\python.exe]:\n\nFound possible Python library paths:\n  C:\\python36\\lib\\site-packages\nPlease input the desired Python library path to use.  Default is [C:\\python36\\lib\\site-packages]\n\nDo you wish to build TensorFlow with CUDA support? [y/N]: Y\nCUDA support will be enabled for TensorFlow.\n\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]:\n\nPlease specify the location where CUDA 9.0 toolkit is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0]:\n\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 7.0\n\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0]: C:\\tools\\cuda\n\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,7.0]: 3.7\n\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\n\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:\nEigen strong inline overridden.\n\nConfiguration finished\n\n\nConfiguration options\nFor GPU support, specify the versions of CUDA and cuDNN. If your\nsystem has multiple versions of CUDA or cuDNN installed, explicitly set the\nversion instead of relying on the default. `./configure.py` creates symbolic\nlinks to your system's CUDA libraries\u2014so if you update your CUDA library paths,\nthis configuration step must be run again before building.\nWarning: TF-TRT Windows support is provided experimentally. No guarantee is made\nregarding functionality or engineering support. Use at your own risk.\nBuild and install the pip package\nThe pip package gets built in two steps. A `bazel build` commands creates a\n\"package-builder\" program. You then run the package-builder to create the\npackage.\nBuild the package-builder\ntensorflow:master repo has been updated to build 2.x by default.\nInstall Bazel and use\n`bazel build` to create the TensorFlow package-builder.\n\nbazel build //tensorflow/tools/pip_package:build_pip_package\n\nCPU-only\nUse `bazel` to make the TensorFlow package builder with CPU-only support:\n\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\n\nGPU support\nTo make the TensorFlow package builder with GPU support:\n\nbazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\n\nBazel build options\nUse this option when building to avoid issue with package creation:\ntensorflow:issue#22390\n\n--define=no_tensorflow_py_deps=true\n\nSee the Bazel command-line reference\nfor\nbuild options.\nBuilding TensorFlow from source can use a lot of RAM. If your system is\nmemory-constrained, limit Bazel's RAM usage with: `--local_ram_resources=2048`.\nIf building with GPU support, add `--copt=-nvcc_options=disable-warnings`\nto suppress nvcc warning messages.\nBuild the package\nThe `bazel build` command creates an executable named `build_pip_package`\u2014this\nis the program that builds the `pip` package. For example, the following builds\na `.whl` package in the `C:/tmp/tensorflow_pkg` directory:\n\nbazel-bin\\tensorflow\\tools\\pip_package\\build_pip_package C:/tmp/tensorflow_pkg\n\nAlthough it is possible to build both CUDA and non-CUDA configs under the\nsame source tree, we recommend running `bazel clean` when switching between\nthese two configurations in the same source tree.\nInstall the package\nThe filename of the generated `.whl` file depends on the TensorFlow version and\nyour platform. Use `pip3 install` to install the package, for example:\n\npip3 install C:/tmp/tensorflow_pkg/tensorflow-version-cp36-cp36m-win_amd64.whl\n\nSuccess: TensorFlow is now installed.\nBuild using the MSYS shell\nTensorFlow can also be built using the MSYS shell. Make the changes listed\nbelow, then follow the previous instructions for the Windows native command line\n(`cmd.exe`).\nDisable MSYS path conversion {:.hide-from-toc}\nMSYS automatically converts arguments that look like Unix paths to Windows\npaths, and this doesn't work with `bazel`. (The label `//path/to:bin` is\nconsidered a Unix absolute path since it starts with a slash.)\n\n\n\n```export MSYS_NO_PATHCONV=1```\n\n\n\n\n```export MSYS2_ARG_CONV_EXCL=\"*\"```\n\n\n\nSet your PATH {:.hide-from-toc}\nAdd the Bazel and Python installation directories to your `$PATH` environmental\nvariable. If Bazel is installed to `C:\\tools\\bazel.exe`, and Python to\n`C:\\Python36\\python.exe`, set your `PATH` with:\n\n# Use Unix-style with ':' as separator\n\n\n```export PATH=\"/c/tools:$PATH\"```\n\n\n\n\n```export PATH=\"/c/Python36:$PATH\"```\n\n\n\nFor GPU support, add the CUDA and cuDNN bin directories to your `$PATH`:\n\n\n\n```export PATH=\"/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0/bin:$PATH\"```\n\n\n\n\n```export PATH=\"/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0/extras/CUPTI/libx64:$PATH\"```\n\n\n\n\n```export PATH=\"/c/tools/cuda/bin:$PATH\"```\n\n\n\n\nTested build configurations\nCPU\n\nVersionPython versionCompilerBuild tools\ntensorflow-2.11.03.7-3.10MSVC 2019Bazel 5.3.0\ntensorflow-2.10.03.7-3.10MSVC 2019Bazel 5.1.1\ntensorflow-2.9.03.7-3.10MSVC 2019Bazel 5.0.0\ntensorflow-2.8.03.7-3.10MSVC 2019Bazel 4.2.1\ntensorflow-2.7.03.7-3.9MSVC 2019Bazel 3.7.2\ntensorflow-2.6.03.6-3.9MSVC 2019Bazel 3.7.2\ntensorflow-2.5.03.6-3.9MSVC 2019Bazel 3.7.2\ntensorflow-2.4.03.6-3.8MSVC 2019Bazel 3.1.0\ntensorflow-2.3.03.5-3.8MSVC 2019Bazel 3.1.0\ntensorflow-2.2.03.5-3.8MSVC 2019Bazel 2.0.0\ntensorflow-2.1.03.5-3.7MSVC 2019Bazel 0.27.1-0.29.1\ntensorflow-2.0.03.5-3.7MSVC 2017Bazel 0.26.1\ntensorflow-1.15.03.5-3.7MSVC 2017Bazel 0.26.1\ntensorflow-1.14.03.5-3.7MSVC 2017Bazel 0.24.1-0.25.2\ntensorflow-1.13.03.5-3.7MSVC 2015 update 3Bazel 0.19.0-0.21.0\ntensorflow-1.12.03.5-3.6MSVC 2015 update 3Bazel 0.15.0\ntensorflow-1.11.03.5-3.6MSVC 2015 update 3Bazel 0.15.0\ntensorflow-1.10.03.5-3.6MSVC 2015 update 3Cmake v3.6.3\ntensorflow-1.9.03.5-3.6MSVC 2015 update 3Cmake v3.6.3\ntensorflow-1.8.03.5-3.6MSVC 2015 update 3Cmake v3.6.3\ntensorflow-1.7.03.5-3.6MSVC 2015 update 3Cmake v3.6.3\ntensorflow-1.6.03.5-3.6MSVC 2015 update 3Cmake v3.6.3\ntensorflow-1.5.03.5-3.6MSVC 2015 update 3Cmake v3.6.3\ntensorflow-1.4.03.5-3.6MSVC 2015 update 3Cmake v3.6.3\ntensorflow-1.3.03.5-3.6MSVC 2015 update 3Cmake v3.6.3\ntensorflow-1.2.03.5-3.6MSVC 2015 update 3Cmake v3.6.3\ntensorflow-1.1.03.5MSVC 2015 update 3Cmake v3.6.3\ntensorflow-1.0.03.5MSVC 2015 update 3Cmake v3.6.3\n\nGPU\n\nVersionPython versionCompilerBuild toolscuDNNCUDA\ntensorflow_gpu-2.11.03.7-3.10MSVC 2019Bazel 5.3.08.111.2\ntensorflow_gpu-2.10.03.7-3.10MSVC 2019Bazel 5.1.18.111.2\ntensorflow_gpu-2.9.03.7-3.10MSVC 2019Bazel 5.0.08.111.2\ntensorflow_gpu-2.8.03.7-3.10MSVC 2019Bazel 4.2.18.111.2\ntensorflow_gpu-2.7.03.7-3.9MSVC 2019Bazel 3.7.28.111.2\ntensorflow_gpu-2.6.03.6-3.9MSVC 2019Bazel 3.7.28.111.2\ntensorflow_gpu-2.5.03.6-3.9MSVC 2019Bazel 3.7.28.111.2\ntensorflow_gpu-2.4.03.6-3.8MSVC 2019Bazel 3.1.08.011.0\ntensorflow_gpu-2.3.03.5-3.8MSVC 2019Bazel 3.1.07.610.1\ntensorflow_gpu-2.2.03.5-3.8MSVC 2019Bazel 2.0.07.610.1\ntensorflow_gpu-2.1.03.5-3.7MSVC 2019Bazel 0.27.1-0.29.17.610.1\ntensorflow_gpu-2.0.03.5-3.7MSVC 2017Bazel 0.26.17.410\ntensorflow_gpu-1.15.03.5-3.7MSVC 2017Bazel 0.26.17.410\ntensorflow_gpu-1.14.03.5-3.7MSVC 2017Bazel 0.24.1-0.25.27.410\ntensorflow_gpu-1.13.03.5-3.7MSVC 2015 update 3Bazel 0.19.0-0.21.07.410\ntensorflow_gpu-1.12.03.5-3.6MSVC 2015 update 3Bazel 0.15.07.29.0\ntensorflow_gpu-1.11.03.5-3.6MSVC 2015 update 3Bazel 0.15.079\ntensorflow_gpu-1.10.03.5-3.6MSVC 2015 update 3Cmake v3.6.379\ntensorflow_gpu-1.9.03.5-3.6MSVC 2015 update 3Cmake v3.6.379\ntensorflow_gpu-1.8.03.5-3.6MSVC 2015 update 3Cmake v3.6.379\ntensorflow_gpu-1.7.03.5-3.6MSVC 2015 update 3Cmake v3.6.379\ntensorflow_gpu-1.6.03.5-3.6MSVC 2015 update 3Cmake v3.6.379\ntensorflow_gpu-1.5.03.5-3.6MSVC 2015 update 3Cmake v3.6.379\ntensorflow_gpu-1.4.03.5-3.6MSVC 2015 update 3Cmake v3.6.368\ntensorflow_gpu-1.3.03.5-3.6MSVC 2015 update 3Cmake v3.6.368\ntensorflow_gpu-1.2.03.5-3.6MSVC 2015 update 3Cmake v3.6.35.18\ntensorflow_gpu-1.1.03.5MSVC 2015 update 3Cmake v3.6.35.18\ntensorflow_gpu-1.0.03.5MSVC 2015 update 3Cmake v3.6.35.18",
    "tag": "tensorflow"
  },
  {
    "title": "Install TensorFlow with pip",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/install/pip.md",
    "content": "Install TensorFlow with pip\n\nThis guide is for the latest stable version of TensorFlow. For the\npreview build (nightly), use the pip package named\n`tf-nightly`. Refer to these tables for\nolder TensorFlow version requirements. For the CPU-only build use the pip\npackage named `tensorflow-cpu`.\nHere are the quick versions of the install commands. Scroll down for the\nstep-by-step instructions.\n\n\n{Linux}\nNote: Starting with TensorFlow `2.10`, Linux CPU-builds for Aarch64/ARM64\nprocessors are built, maintained, tested and released by a third party:\nAWS.\nInstalling the tensorflow\npackage on an ARM machine installs AWS's\ntensorflow-cpu-aws package.\nThey are provided as-is. Tensorflow will use reasonable efforts to maintain\nthe availability and integrity of this pip package. There may be delays if\nthe third party fails to release the pip package. See\nthis blog post\nfor more information about this collaboration.\n```bash\nconda install -c conda-forge cudatoolkit=11.2.2 cudnn=8.1.0\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/\npython3 -m pip install tensorflow\nVerify install:\npython3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\n```\n\n\n{MacOS}\n```bash\nThere is currently no official GPU support for MacOS.\npython3 -m pip install tensorflow\nVerify install:\npython3 -c \"import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\n```\n\n\n{Windows Native}\nCaution: TensorFlow `2.10` was the last TensorFlow release that\nsupported GPU on native-Windows.\nStarting with TensorFlow `2.11`, you will need to install\nTensorFlow in WSL2,\nor install `tensorflow-cpu` and, optionally, try the\nTensorFlow-DirectML-Plugin\n```bash\nconda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0\nAnything above 2.10 is not supported on the GPU on Windows Native\npython -m pip install \"tensorflow<2.11\"\nVerify install:\npython -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\n```\n\n\n{Windows WSL2}\nNote: TensorFlow with GPU access is supported for WSL2 on Windows 10 19044 or\nhigher. This corresponds to Windows 10 version 21H2, the November 2021\nupdate. You can get the latest update from here:\nDownload Windows 10{:.external}.\nFor instructions, see\nInstall WSL2{:.external}\nand\nNVIDIA\u2019s setup docs{:.external}\nfor CUDA in WSL.\n```bash\nconda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/\npython3 -m pip install tensorflow\nVerify install:\npython3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\n```\n\n\n{CPU}\nNote: Starting with TensorFlow `2.10`, Windows CPU-builds for x86/x64\nprocessors are built, maintained, tested and released by a third party:\nIntel.\nInstalling the Windows-native tensorflow\nor tensorflow-cpu\npackage installs Intel's\ntensorflow-intel\npackage. These packages are provided as-is. Tensorflow will use reasonable\nefforts to maintain the availability and integrity of this pip package.\nThere may be delays if the third party fails to release the pip package. See\nthis blog post\nfor more information about this\ncollaboration.\n```bash\npython3 -m pip install tensorflow\nVerify install:\npython3 -c \"import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\n```\n\n\n{Nightly}\n```bash\npython3 -m pip install tf-nightly\nVerify install:\npython3 -c \"import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\n```\n\n\nHardware requirements\nNote: TensorFlow binaries use\nAVX instructions{:.external}\nwhich may not run on older CPUs.\nThe following GPU-enabled devices are supported:\n\nNVIDIA\u00ae GPU card with CUDA\u00ae architectures 3.5, 5.0, 6.0, 7.0, 7.5, 8.0 and\n    higher. See the list of\n    CUDA\u00ae-enabled GPU cards{:.external}.\nFor GPUs with unsupported CUDA\u00ae architectures, or to avoid JIT compilation\n    from PTX, or to use different versions of the NVIDIA\u00ae libraries, see the\n    Linux build from source guide.\nPackages do not contain PTX code except for the latest supported CUDA\u00ae\n    architecture; therefore, TensorFlow fails to load on older GPUs when\n    `CUDA_FORCE_PTX_JIT=1` is set. (See\n    Application Compatibility\n    for details.) {:.external}\n\nNote: The error message \"Status: device kernel image is invalid\" indicates that\nthe TensorFlow package does not contain PTX for your architecture. You can\nenable compute capabilities by building TensorFlow from source.\nSystem requirements\n\nUbuntu 16.04 or higher (64-bit)\nmacOS 10.12.6 (Sierra) or higher (64-bit) (no GPU support)\nWindows Native - Windows 7 or higher (64-bit) (no GPU support after TF 2.10)\nWindows WSL2 - Windows 10 19044 or higher (64-bit)\n\nNote: GPU support is available for Ubuntu and Windows with CUDA\u00ae-enabled cards.\nSoftware requirements\n\nPython 3.7\u20133.10\npip version 19.0 or higher for Linux (requires `manylinux2010` support) and\n    Windows. pip version 20.3 or higher for macOS.\nWindows Native Requires\n    Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019{:.external}\n\nThe following NVIDIA\u00ae software are only required for GPU support.\n\nNVIDIA\u00ae GPU drivers{:.external}\n    version 450.80.02 or higher.\nCUDA\u00ae Toolkit 11.2{:.external}.\ncuDNN SDK 8.1.0{:.external}.\n(Optional)\nTensorRT{:.external}\n    to improve latency and throughput for inference.\n\nStep-by-step instructions\n\n\n{Linux}\n1. System requirements\n\nUbuntu 16.04 or higher (64-bit)\n\nTensorFlow only officially support Ubuntu. However, the following\ninstructions may also work for other Linux distros.\nNote: Starting with TensorFlow `2.10`, Linux CPU-builds for Aarch64/ARM64\nprocessors are built, maintained, tested and released by a third party:\nAWS.\nInstalling the tensorflow\npackage on an ARM machine installs AWS's\ntensorflow-cpu-aws package.\nThey are provided as-is. Tensorflow will use reasonable efforts to maintain\nthe availability and integrity of this pip package. There may be delays if\nthe third party fails to release the pip package. See\nthis blog post\nfor more information about this collaboration.\n2. Install Miniconda\nMiniconda{:.external} is the\nrecommended approach for installing TensorFlow with GPU support.\nIt creates a separate environment to avoid changing any installed\nsoftware in your system. This is also the easiest way to install the required\nsoftware especially for the GPU setup.\nYou can use the following command to install Miniconda. During installation,\nyou may need to press enter and type \"yes\".\n`bash\ncurl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -o Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh`\nYou may need to restart your terminal or `source ~/.bashrc` to enable the\n`conda` command. Use `conda -V` to test if it is installed successfully.\n3. Create a conda environment\nCreate a new conda environment named `tf` with the following command.\n`bash\nconda create --name tf python=3.9`\nYou can deactivate and activate it with the following commands.\n`bash\nconda deactivate\nconda activate tf`\nMake sure it is activated for the rest of the installation.\n4. GPU setup\nYou can skip this section if you only run TensorFlow on the CPU.\nFirst install the\nNVIDIA GPU driver{:.external}\nif you have not. You can use the following command to verify it is\ninstalled.\n`bash\nnvidia-smi`\nThen install CUDA and cuDNN with conda.\n`bash\nconda install -c conda-forge cudatoolkit=11.2.2 cudnn=8.1.0`\nConfigure the system paths. You can do it with the following command every time\nyou start a new terminal after activating your conda environment.\n`bash\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/`\nFor your convenience it is recommended that you automate it with the following\ncommands. The system paths will be automatically configured when you\nactivate this conda environment.\n`bash\nmkdir -p $CONDA_PREFIX/etc/conda/activate.d\necho 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/' > $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh`\n5. Install TensorFlow\nTensorFlow requires a recent version of pip, so upgrade your pip\ninstallation to be sure you're running the latest version.\n`bash\npip install --upgrade pip`\nThen, install TensorFlow with pip.\nNote: Do not install TensorFlow with conda. It may not have the latest stable\nversion. pip is recommended since TensorFlow is only officially released to\nPyPI.\n`bash\npip install tensorflow==2.11.*`\n6. Verify install\nVerify the CPU setup:\n`bash\npython3 -c \"import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"`\nIf a tensor is returned, you've installed TensorFlow successfully.\nVerify the GPU setup:\n`bash\npython3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"`\nIf a list of GPU devices is returned, you've installed TensorFlow\nsuccessfully.\nUbuntu 22.04\nIn Ubuntu 22.04, you may encounter the following error:\n`Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice.\n...\nCouldn't invoke ptxas --version\n...\nInternalError: libdevice not found at ./libdevice.10.bc [Op:__some_op]`\nTo fix this error, you will need to run the following commands.\n```bash\nInstall NVCC\nconda install -c nvidia cuda-nvcc=11.3.58\nConfigure the XLA cuda directory\nmkdir -p $CONDA_PREFIX/etc/conda/activate.d\nprintf 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/\\nexport XLA_FLAGS=--xla_gpu_cuda_data_dir=$CONDA_PREFIX/lib/\\n' > $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\nsource $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\nCopy libdevice file to the required path\nmkdir -p $CONDA_PREFIX/lib/nvvm/libdevice\ncp $CONDA_PREFIX/lib/libdevice.10.bc $CONDA_PREFIX/lib/nvvm/libdevice/\n```\n\n\n{MacOS}\n\n\n### 1. System requirements\n\n\n```*   macOS 10.12.6 (Sierra) or higher (64-bit)\n\nNote: For users of Apple M1 computers, to get native performance, you'll\nwant to follow the instructions found\n[here](https://developer.apple.com/metal/tensorflow-plugin/){:.external}.\nConda has shown to have the smoothest install. Packages that\ninclude custom C++ extensions for TensorFlow also need to be compiled for\nApple M1. Some packages, like\n[tensorflow_decision_forests](https://www.tensorflow.org/decision_forests)\npublish M1-compatible versions, but many packages don't. To use those\nlibraries, you will have to use TensorFlow with x86 emulation and Rosetta.\n\nCurrently there is no official GPU support for running TensorFlow on\nMacOS. The following is instructions are for running on CPU.\n\n### 2. Check Python version\n\nCheck if your Python environment is already configured:\n\nNote: Requires Python 3.7\u20133.10, and pip >= 20.3 for MacOS.\n\n```bash\npython3 --version\npython3 -m pip --version\n```\n```\n\n\n### 2. Install Miniconda\nMiniconda{:.external}\n   is the recommended approach for installing TensorFlow with GPU support.\n   It creates a separate environment to avoid changing any installed\n   software in your system. This is also the easiest way to install the required\n   software especially for the GPU setup.\n\n\n``````bash\ncurl https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh -o Miniconda3-latest-MacOSX-x86_64.sh\nbash Miniconda3-latest-MacOSX-x86_64.sh\n```\n\nYou may need to restart your terminal or `source ~/.bashrc` to enable the\n`conda` command. Use `conda -V` to test if it is installed successfully.\n\n### 4. Create a conda environment\n\nCreate a new conda environment named `tf` with the following command.\n\n```bash\nconda create --name tf python=3.9\n```\n\nYou can deactivate and activate it with the following commands.\n\n```bash\nconda deactivate\nconda activate tf\n```\n\nMake sure it is activated for the rest of the installation.\n\n### 5. Install TensorFlow\n\nTensorFlow requires a recent version of pip, so upgrade your pip\ninstallation to be sure you're running the latest version.\n\n```bash\npip install --upgrade pip\n```\n\nThen, install TensorFlow with pip.\n\nNote: Do not install TensorFlow with conda. It may not have the latest stable\nversion. pip is recommended since TensorFlow is only officially released to\nPyPI.\n\n```bash\npip install tensorflow\n```\n\n### 6. Verify install\n\n```bash\npython3 -c \"import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\n```\n\nIf a tensor is returned, you've installed TensorFlow successfully.\n```\n\n\n\n{Windows Native}\n\nCaution: TensorFlow `2.10` was the last TensorFlow release that\n   supported GPU on native-Windows.\n   Starting with TensorFlow `2.11`, you will need to install\n   TensorFlow in WSL2,\n   or install `tensorflow-cpu` and, optionally, try the\n   TensorFlow-DirectML-Plugin\n## 1. System requirements\n\n\nWindows 7 or higher (64-bit)\nNote: Starting with TensorFlow `2.10`, Windows CPU-builds for x86/x64\nprocessors are built, maintained, tested and released by a third party:\nIntel.\nInstalling the windows-native tensorflow\nor tensorflow-cpu\npackage installs Intel's\ntensorflow-intel\npackage. These packages are provided as-is. Tensorflow will use reasonable\nefforts to maintain the availability and integrity of this pip package.\nThere may be delays if the third party fails to release the pip package. See\nthis blog post\nfor more information about this\ncollaboration.\n2. Install Microsoft Visual C++ Redistributable\nInstall the Microsoft Visual C++ Redistributable for Visual Studio 2015,\n2017, and 2019. Starting with the TensorFlow 2.1.0 version, the\n`msvcp140_1.dll` file is required from this package (which may not be\nprovided from older redistributable packages). The redistributable comes\nwith Visual Studio 2019 but can be installed separately:\n\nGo to the\n    Microsoft Visual C++ downloads{:.external}.\nScroll down the page to the Visual Studio 2015, 2017 and 2019 section.\nDownload and install the Microsoft Visual C++ Redistributable for\n    Visual Studio 2015, 2017 and 2019 for your platform.\n\nMake sure\nlong paths are enabled{:.external}\non Windows.\n3. Install Miniconda\nMiniconda{:.external}\nis the recommended approach for installing TensorFlow with GPU support.\nIt creates a separate environment to avoid changing any installed\nsoftware in your system. This is also the easiest way to install the\nrequired software especially for the GPU setup.\nDownload the\nMiniconda Windows Installer{:.external}.\nDouble-click the downloaded file and follow the instructions on the screen.\n4. Create a conda environment\nCreate a new conda environment named `tf` with the following command.\n`bash\nconda create --name tf python=3.9`\nYou can deactivate and activate it with the following commands.\n`bash\nconda deactivate\nconda activate tf`\nMake sure it is activated for the rest of the installation.\n5. GPU setup\nYou can skip this section if you only run TensorFlow on CPU.\nFirst install\nNVIDIA GPU driver{:.external}\nif you have not.\nThen install the CUDA, cuDNN with conda.\n`bash\nconda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0`\n6. Install TensorFlow\nTensorFlow requires a recent version of pip, so upgrade your pip\ninstallation to be sure you're running the latest version.\n`bash\npip install --upgrade pip`\nThen, install TensorFlow with pip.\nNote: Do not install TensorFlow with conda. It may not have the latest stable\nversion. pip is recommended since TensorFlow is only officially released to\nPyPI.\n```bash\nAnything above 2.10 is not supported on the GPU on Windows Native\npip install \"tensorflow<2.11\" \n```\n7. Verify install\nVerify the CPU setup:\n`bash\npython -c \"import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"`\nIf a tensor is returned, you've installed TensorFlow successfully.\nVerify the GPU setup:\n`bash\npython -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"`\nIf a list of GPU devices is returned, you've installed TensorFlow\nsuccessfully.\n\n\n{Windows WSL2}\n1. System requirements\n\nWindows 10 19044 or higher (64-bit). This corresponds to Windows 10\n    version 21H2, the November 2021 update.\n\nSee the following documents to:\n\nDownload the latest Windows 10 update{:.external}.\nInstall WSL2{:.external}\nSetup NVIDIA\u00ae GPU support in WSL2{:.external}\n\n2. Install Miniconda\nMiniconda{:.external} is the\nrecommended approach for installing TensorFlow with GPU support.\nIt creates a separate environment to avoid changing any installed\nsoftware in your system. This is also the easiest way to install the required\nsoftware especially for the GPU setup.\nYou can use the following command to install Miniconda. During installation,\nyou may need to press enter and type \"yes\".\n`bash\ncurl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -o Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh`\nYou may need to restart your terminal or `source ~/.bashrc` to enable the\n`conda` command. Use `conda -V` to test if it is installed successfully.\n3. Create a conda environment\nCreate a new conda environment named tf with the following command.\n`bash\nconda create --name tf python=3.9`\nYou can deactivate and activate it with the following commands.\n`bash\nconda deactivate\nconda activate tf`\nMake sure it is activated for the rest of the installation.\n4. GPU setup\nYou can skip this section if you only run TensorFlow on the CPU.\nFirst install the\nNVIDIA GPU driver{:.external}\nif you have not. You can use the following command to verify it is\ninstalled.\n`bash\nnvidia-smi`\nThen install CUDA and cuDNN with conda.\n`bash\nconda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0`\nConfigure the system paths. You can do it with following command everytime\nyour start a new terminal after activating your conda environment.\n`bash\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/`\nFor your convenience it is recommended that you automate it with the following\ncommands. The system paths will be automatically configured when you\nactivate this conda environment.\n`bash\nmkdir -p $CONDA_PREFIX/etc/conda/activate.d\necho 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/' > $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh`\n5. Install TensorFlow\nTensorFlow requires a recent version of pip, so upgrade your pip\ninstallation to be sure you're running the latest version.\n`bash\npip install --upgrade pip`\nThen, install TensorFlow with pip.\nNote: Do not install TensorFlow with conda. It may not have the latest stable\nversion. pip is recommended since TensorFlow is only officially released to\nPyPI.\n`bash\npip install tensorflow`\n6. Verify install\nVerify the CPU setup:\n`bash\npython3 -c \"import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"`\nIf a tensor is returned, you've installed TensorFlow successfully.\nVerify the GPU setup:\n`bash\npython3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"`\nIf a list of GPU devices is returned, you've installed TensorFlow\nsuccessfully.\n\n\nPackage location\nA few installation mechanisms require the URL of the TensorFlow Python package.\nThe value you specify depends on your Python version.\n\nVersionURL\nLinux\n\nPython 3.7 GPU\u00a0support\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-2.11.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n\n\nPython 3.7 CPU-only\nhttps://storage.googleapis.com/tensorflow/linux/cpu/tensorflow_cpu-2.11.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n\n\nPython 3.8 GPU\u00a0support\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n\n\nPython 3.8 CPU-only\nhttps://storage.googleapis.com/tensorflow/linux/cpu/tensorflow_cpu-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n\n\nPython 3.9 GPU\u00a0support\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-2.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n\n\nPython 3.9 CPU-only\nhttps://storage.googleapis.com/tensorflow/linux/cpu/tensorflow_cpu-2.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n\n\nPython 3.10 GPU\u00a0support\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-2.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n\n\nPython 3.10 CPU-only\nhttps://storage.googleapis.com/tensorflow/linux/cpu/tensorflow_cpu-2.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n\nmacOS (CPU-only)\n\nPython 3.7\nhttps://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-2.11.0-cp37-cp37m-macosx_10_14_x86_64.whl\n\n\nPython 3.8\nhttps://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-2.11.0-cp38-cp38-macosx_10_14_x86_64.whl\n\n\nPython 3.9\nhttps://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-2.11.0-cp39-cp39-macosx_10_14_x86_64.whl\n\n\nPython 3.10\nhttps://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-2.11.0-cp310-cp310-macosx_10_14_x86_64.whl\n\nWindows\n\nPython 3.7 CPU-only\nhttps://storage.googleapis.com/tensorflow/windows/cpu/tensorflow_cpu-2.11.0-cp37-cp37m-win_amd64.whl\n\n\nPython 3.8 CPU-only\nhttps://storage.googleapis.com/tensorflow/windows/cpu/tensorflow_cpu-2.11.0-cp38-cp38-win_amd64.whl\n\n\nPython 3.9 CPU-only\nhttps://storage.googleapis.com/tensorflow/windows/cpu/tensorflow_cpu-2.11.0-cp39-cp39-win_amd64.whl\n\n\nPython 3.10 CPU-only\nhttps://storage.googleapis.com/tensorflow/windows/cpu/tensorflow_cpu-2.11.0-cp310-cp310-win_amd64.whl\n",
    "tag": "tensorflow"
  },
  {
    "title": "Build and install error messages",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/install/errors.md",
    "content": "Build and install error messages\nTensorFlow uses GitHub issues{:.external},\nStack Overflow{:.external} and\nTensorFlow Forum{:.external}\nto track, document, and discuss build and installation problems.\nThe following list links error messages to a solution or discussion. If you find\nan installation or build problem that is not listed, please search the GitHub\nissues and Stack Overflow. If you still can't find the error message, ask a new\nquestion on Stack Overflow with the `tensorflow` tag.\n\nGitHub issue or Stack\u00a0Overflow Error Message\n38896424\n31058\n\"No matching distribution found for tensorflow\":\n      Pip can't find a TensorFlow package compatible with your system. Check the\n      system requirements and\n        Python version\n\n\n\n22390\nUnzipping simple_console_for_windows.zip to create runfiles tree...\n[./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip]\n  End-of-central-directory signature not found.  Either this file is not\n  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n  latter case the central directory and zipfile comment will be found on\n  the last disk(s) of this archive.\nunzip: cannot find zipfile directory in one of ./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip or\n        ./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip.zip, and cannot find ./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip.ZIP, period.\n\n\n\n36159194\nImportError: libcudart.so.Version: cannot open shared object file:\n  No such file or directory\n\n\n41991101\nImportError: libcudnn.Version: cannot open shared object file:\n  No such file or directory\n\n\n36371137\nlibprotobuf ERROR google/protobuf/src/google/protobuf/io/coded_stream.cc:207] A\n  protocol message was rejected because it was too big (more than 67108864 bytes).\n  To increase the limit (or to disable these warnings), see\n  \nCodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\n\n\n35252888\nError importing tensorflow. Unless you are using bazel, you should\n  not try to import tensorflow from its source directory; please exit the\n  tensorflow source tree, and relaunch your python interpreter from\n  there.\n\n\n33623453\nIOError: [Errno 2] No such file or directory:\n  '/tmp/pip-o6Tpui-build/setup.py'\n\n\n42006320\nImportError: Traceback (most recent call last):\n  File \".../tensorflow/core/framework/graph_pb2.py\", line 6, in \n  from google.protobuf import descriptor as _descriptor\n  ImportError: cannot import name 'descriptor'\n\n\n\n35190574 \nSSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify\n  failed\n\n\n42009190\n\n  Installing collected packages: setuptools, protobuf, wheel, numpy, tensorflow\n  Found existing installation: setuptools 1.1.6\n  Uninstalling setuptools-1.1.6:\n  Exception:\n  ...\n  [Errno 1] Operation not permitted:\n  '/tmp/pip-a1DXRT-uninstall/.../lib/python/_markerlib' \n\n\n36933958\n\n  ...\n  Installing collected packages: setuptools, protobuf, wheel, numpy, tensorflow\n  Found existing installation: setuptools 1.1.6\n  Uninstalling setuptools-1.1.6:\n  Exception:\n  ...\n  [Errno 1] Operation not permitted:\n  '/tmp/pip-a1DXRT-uninstall/System/Library/Frameworks/Python.framework/\n   Versions/2.7/Extras/lib/python/_markerlib'\n\n\n\n42006320\nImportError: Traceback (most recent call last):\nFile \".../tensorflow/core/framework/graph_pb2.py\", line 6, in \nfrom google.protobuf import descriptor as _descriptor\nImportError: cannot import name 'descriptor'\n\n\n\n33623453\nIOError: [Errno 2] No such file or directory:\n  '/tmp/pip-o6Tpui-build/setup.py'\n\n\n35190574 \nSSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify\n  failed\n\n\n42009190\n\n  Installing collected packages: setuptools, protobuf, wheel, numpy, tensorflow\n  Found existing installation: setuptools 1.1.6\n  Uninstalling setuptools-1.1.6:\n  Exception:\n  ...\n  [Errno 1] Operation not permitted:\n  '/tmp/pip-a1DXRT-uninstall/.../lib/python/_markerlib' \n\n\n33622019\nImportError: No module named copyreg\n\n\n37810228\nDuring a pip install operation, the system returns:\n  OSError: [Errno 1] Operation not permitted\n\n\n\n33622842\nAn import tensorflow statement triggers an error such as the\n  following:Traceback (most recent call last):\n  File \"\", line 1, in \n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/__init__.py\",\n    line 4, in \n    from tensorflow.python import *\n    ...\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/core/framework/tensor_shape_pb2.py\",\n    line 22, in \n    serialized_pb=_b('\\n,tensorflow/core/framework/tensor_shape.proto\\x12\\ntensorflow\\\"d\\n\\x10TensorShapeProto\\x12-\\n\\x03\\x64im\\x18\\x02\n      \\x03(\\x0b\\x32\n      .tensorflow.TensorShapeProto.Dim\\x1a!\\n\\x03\\x44im\\x12\\x0c\\n\\x04size\\x18\\x01\n      \\x01(\\x03\\x12\\x0c\\n\\x04name\\x18\\x02 \\x01(\\tb\\x06proto3')\n  TypeError: __init__() got an unexpected keyword argument 'syntax'\n\n\n\n42075397\nA pip install command triggers the following error:\n...\nYou have not agreed to the Xcode license agreements, please run\n'xcodebuild -license' (for user-level acceptance) or\n'sudo xcodebuild -license' (for system-wide acceptance) from within a\nTerminal window to review and agree to the Xcode license agreements.\n...\n  File \"numpy/core/setup.py\", line 653, in get_mathlib_info\n\n    raise RuntimeError(\"Broken toolchain: cannot link a simple C program\")\n\nRuntimeError: Broken toolchain: cannot link a simple C program\n\n\n\n41007279\n\n[...\\stream_executor\\dso_loader.cc] Couldn't open CUDA library nvcuda.dll\n\n\n\n41007279\n\n[...\\stream_executor\\cuda\\cuda_dnn.cc] Unable to load cuDNN DSO\n\n\n\n42006320\nImportError: Traceback (most recent call last):\nFile \"...\\tensorflow\\core\\framework\\graph_pb2.py\", line 6, in \nfrom google.protobuf import descriptor as _descriptor\nImportError: cannot import name 'descriptor'\n\n\n\n42011070\nNo module named \"pywrap_tensorflow\"\n\n\n42217532\n\nOpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op: BestSplits\n\n\n\n43134753\n\nThe TensorFlow library wasn't compiled to use SSE instructions\n\n\n\n38896424\n\nCould not find a version that satisfies the requirement tensorflow\n\n\n\n42006320\nImportError: Traceback (most recent call last):\nFile \".../tensorflow/core/framework/graph_pb2.py\", line 6, in \nfrom google.protobuf import descriptor as _descriptor\nImportError: cannot import name 'descriptor'\n\n\n\n33623453\nIOError: [Errno 2] No such file or directory:\n  '/tmp/pip-o6Tpui-build/setup.py'\n\n\n35190574 \nSSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify\n  failed\n\n\n42009190\n\n  Installing collected packages: setuptools, protobuf, wheel, numpy, tensorflow\n  Found existing installation: setuptools 1.1.6\n  Uninstalling setuptools-1.1.6:\n  Exception:\n  ...\n  [Errno 1] Operation not permitted:\n  '/tmp/pip-a1DXRT-uninstall/.../lib/python/_markerlib' \n\n\n33622019\nImportError: No module named copyreg\n\n\n37810228\nDuring a pip install operation, the system returns:\n  OSError: [Errno 1] Operation not permitted\n\n\n\n33622842\nAn import tensorflow statement triggers an error such as the\n  following:Traceback (most recent call last):\n  File \"\", line 1, in \n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/__init__.py\",\n    line 4, in \n    from tensorflow.python import *\n    ...\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/core/framework/tensor_shape_pb2.py\",\n    line 22, in \n    serialized_pb=_b('\\n,tensorflow/core/framework/tensor_shape.proto\\x12\\ntensorflow\\\"d\\n\\x10TensorShapeProto\\x12-\\n\\x03\\x64im\\x18\\x02\n      \\x03(\\x0b\\x32\n      .tensorflow.TensorShapeProto.Dim\\x1a!\\n\\x03\\x44im\\x12\\x0c\\n\\x04size\\x18\\x01\n      \\x01(\\x03\\x12\\x0c\\n\\x04name\\x18\\x02 \\x01(\\tb\\x06proto3')\n  TypeError: __init__() got an unexpected keyword argument 'syntax'\n\n\n\n41293077\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow\n  library wasn't compiled to use SSE4.1 instructions, but these are available on\n  your machine and could speed up CPU computations.\n\n\n42013316\nImportError: libcudart.so.8.0: cannot open shared object file:\n  No such file or directory\n\n\n42013316\nImportError: libcudnn.5: cannot open shared object file:\n  No such file or directory\n\n\n35953210\nInvoking `python` or `ipython` generates the following error:\n  ImportError: cannot import name pywrap_tensorflow\n\n\n45276830\nexternal/local_config_cc/BUILD:50:5: in apple_cc_toolchain rule\n  @local_config_cc//:cc-compiler-darwin_x86_64: Xcode version must be specified\n  to use an Apple CROSSTOOL.\n\n\n\n47080760\nundefined reference to `cublasGemmEx@libcublas.so.9.0'\n\n\n22512\nModuleNotFoundError: No module named 'tensorflow.python._pywrap_tensorflow_internal'\n\n\n22512, 22794\nImportError: DLL load failed: The specified module could not be found.\n\n\n24835\nCould not install packages due to an EnvironmentError: [Errno 2] No such file or directory: [long path name]\n",
    "tag": "tensorflow"
  },
  {
    "title": "GPU device plugins",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/install/gpu_plugins.md",
    "content": "GPU device plugins\nNote: This page is for non-NVIDIA\u00ae GPU devices. For NVIDIA\u00ae GPU support, go to\nthe Install TensorFlow with pip guide.\nTensorFlow's\npluggable device{:.external}\narchitecture adds new device support as separate plug-in packages that are\ninstalled alongside the official TensorFlow package.\nThe mechanism requires no device-specific changes in the TensorFlow code. It\nrelies on C APIs to communicate with the TensorFlow binary in a stable manner.\nPlug-in developers maintain separate code repositories and distribution packages\nfor their plugins and are responsible for testing their devices.\nUse device plugins\nTo use a particular device, like one would a native device in TensorFlow, users\nonly have to install the device plug-in package for that device. The following\ncode snippet shows how the plugin for a new demonstration device, Awesome\nProcessing Unit (APU), is installed and used. For simplicity, this sample APU\nplug-in only has one custom kernel for ReLU:\n```sh\nInstall the APU example plug-in package\n$ pip install tensorflow-apu-0.0.1-cp36-cp36m-linux_x86_64.whl\n...\nSuccessfully installed tensorflow-apu-0.0.1\n```\nWith the plug-in installed, test that the device is visible and run an operation\non the new APU device:\n```python\nimport tensorflow as tf   # TensorFlow registers PluggableDevices here.\ntf.config.list_physical_devices()  # APU device is visible to TensorFlow.\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:APU:0', device_type='APU')]\na = tf.random.normal(shape=[5], dtype=tf.float32)  # Runs on CPU.\nb =  tf.nn.relu(a)         # Runs on APU.\nwith tf.device(\"/APU:0\"):  # Users can also use 'with tf.device' syntax.\n  c = tf.nn.relu(a)        # Runs on APU.\nwith tf.device(\"/CPU:0\"):\n  c = tf.nn.relu(a)        # Runs on CPU.\n@tf.function  # Defining a tf.function\ndef run():\n  d = tf.random.uniform(shape=[100], dtype=tf.float32)  # Runs on CPU.\n  e = tf.nn.relu(d)        # Runs on APU.\nrun()  # PluggableDevices also work with tf.function and graph mode.\n```\nAvailable devices\nMetal `PluggableDevice` for macOS GPUs:\n\nWorks with TF 2.5 or later.\nGetting started guide{:.external}.\nFor questions and feedback, please visit the\n    Apple Developer Forum{:.external}.\n\nDirectML `PluggableDevice` for Windows and WSL (preview):\n\nWorks with `tensorflow-cpu` package, version 2.10 or later.\nPyPI wheel{:.external}.\nGitHub repo{:.external}.\nFor questions, feedback or to raise issues, please visit the\n    Issues page of tensorflow-directml-plugin on GitHub{:.external}.\n\nIntel\u00ae Extension for TensorFlow `PluggableDevice` for Linux and WSL:\n\nWorks with TF 2.10 or later.\nGetting started guide\nPyPI wheel{:.external}.\nGitHub repo{:.external}.\nFor questions, feedback, or to raise issues, please visit the\n",
    "tag": "tensorflow"
  },
  {
    "title": "Docker",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/install/docker.md",
    "content": "Docker\nDocker{:.external} uses containers to\ncreate virtual environments that isolate a TensorFlow installation from the rest\nof the system. TensorFlow programs are run within this virtual environment that\ncan share resources with its host machine (access directories, use the GPU,\nconnect to the Internet, etc.). The\nTensorFlow Docker images{:.external}\nare tested for each release.\nDocker is the easiest way to enable TensorFlow GPU support on Linux since only the\nNVIDIA\u00ae GPU driver{:.external}\nis required on the host machine (the NVIDIA\u00ae CUDA\u00ae Toolkit does not need to\nbe installed).\nTensorFlow Docker requirements\n\nInstall Docker{:.external} on\n   your local host machine.\nFor GPU support on Linux, install NVIDIA Docker support{:.external}.\nTake note of your Docker version with `docker -v`. Versions earlier than 19.03 require nvidia-docker2 and the `--runtime=nvidia` flag. On versions including and after 19.03, you will use the `nvidia-container-toolkit` package and the `--gpus all` flag. Both options are documented on the page linked above.\n\nNote: To run the `docker` command without `sudo`, create the `docker` group and\nadd your user. For details, see the\npost-installation steps for Linux{:.external}.\nDownload a TensorFlow Docker image\nThe official TensorFlow Docker images are located in the \ntensorflow/tensorflow{:.external}\nDocker Hub repository. Image releases are tagged{:.external}\nusing the following format:\n| Tag         | Description                                                                                                          |\n|-------------|----------------------------------------------------------------------------------------------------------------------|\n| `latest`    | The latest release of TensorFlow CPU binary image. Default.                                                          |\n| `nightly`   | Nightly builds of the TensorFlow image. (Unstable.)                                                                  |\n| `version` | Specify the version of the TensorFlow binary image, for example\\: 2.8.3                                          |\n| `devel`     | Nightly builds of a TensorFlow `master` development environment. Includes TensorFlow source code.                    |\n| `custom-op` | Special experimental image for developing TF custom ops.  More info here. |\nEach base tag has variants that add or change functionality:\n| Tag Variants      | Description                                                                       |\n| ---               | ---                                                                               |\n| `tag``-gpu`     | The specified tag release with GPU support. (See below)         |\n| `tag``-jupyter` | The specified tag release with Jupyter (includes TensorFlow tutorial notebooks) |\nYou can use multiple variants at once. For example, the following downloads\nTensorFlow release images to your machine:\n\n\n\n```docker pull tensorflow/tensorflow                     # latest stable release```\n\n\n\n\n```docker pull tensorflow/tensorflow:devel-gpu           # nightly dev release w/ GPU support```\n\n\n\n\n```docker pull tensorflow/tensorflow:latest-gpu-jupyter  # latest release w/ GPU support and Jupyter```\n\n\n\nStart a TensorFlow Docker container\nTo start a TensorFlow-configured container, use the following command form:\n\ndocker run [-it] [--rm] [-p hostPort:containerPort] tensorflow/tensorflow[:tag] [command]\n\nFor details, see the docker run reference{:.external}.\nExamples using CPU-only images\nLet's verify the TensorFlow installation using the `latest` tagged image. Docker\ndownloads a new TensorFlow image the first time it is run:\n\ndocker run -it --rm tensorflow/tensorflow \\\n   python -c \"import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\n\nSuccess: TensorFlow is now installed. Read the tutorials to get started.\nLet's demonstrate some more TensorFlow Docker recipes. Start a `bash` shell\nsession within a TensorFlow-configured container:\n\ndocker run -it tensorflow/tensorflow bash\n\nWithin the container, you can start a `python` session and import TensorFlow.\nTo run a TensorFlow program developed on the host machine within a container,\nmount the host directory and change the container's working directory\n(`-v hostDir:containerDir -w workDir`):\n\ndocker run -it --rm -v $PWD:/tmp -w /tmp tensorflow/tensorflow python ./script.py\n\nPermission issues can arise when files created within a container are exposed to\nthe host. It's usually best to edit files on the host system.\nStart a Jupyter Notebook{:.external} server using\nTensorFlow's nightly build:\n\ndocker run -it -p 8888:8888 tensorflow/tensorflow:nightly-jupyter\n\nFollow the instructions and open the URL in your host web browser:\n`http://127.0.0.1:8888/?token=...`\nGPU support\nDocker is the easiest way to run TensorFlow on a GPU since the host machine\nonly requires the NVIDIA\u00ae driver{:.external}\n(the NVIDIA\u00ae CUDA\u00ae Toolkit is not required).\nInstall the Nvidia Container Toolkit{:.external} \nto add NVIDIA\u00ae GPU support to Docker. `nvidia-container-runtime` is only\navailable for Linux. See the `nvidia-container-runtime`\nplatform support FAQ{:.external}\nfor details.\nCheck if a GPU is available:\n\nlspci | grep -i nvidia\n\nVerify your `nvidia-docker` installation:\n\ndocker run --gpus all --rm nvidia/cuda nvidia-smi\n\nNote: `nvidia-docker` v2 uses `--runtime=nvidia` instead of `--gpus all`. `nvidia-docker` v1 uses the `nvidia-docker` alias, \nrather than the `--runtime=nvidia` or `--gpus all` command line flags.\nExamples using GPU-enabled images\nDownload and run a GPU-enabled TensorFlow image (may take a few minutes):\n\ndocker run --gpus all -it --rm tensorflow/tensorflow:latest-gpu \\\n   python -c \"import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\n\nIt can take a while to set up the GPU-enabled image. If repeatedly running\nGPU-based scripts, you can use `docker exec` to reuse a container.\nUse the latest TensorFlow GPU image to start a `bash` shell session in the container:\n\ndocker run --gpus all -it tensorflow/tensorflow:latest-gpu bash\n\nSuccess: TensorFlow is now installed. Read the tutorials to get",
    "tag": "tensorflow"
  },
  {
    "title": "Citing TensorFlow",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/about/bib.md",
    "content": "Citing TensorFlow\nTensorFlow publishes a DOI for the open-source code base using Zenodo.org:\n10.5281/zenodo.4724125\nTensorFlow's white papers are listed for citation below.\nLarge-Scale Machine Learning on Heterogeneous Distributed Systems\nAccess this white paper.\nAbstract: TensorFlow is an interface for expressing machine learning\nalgorithms and an implementation for executing such algorithms.\nA computation expressed using TensorFlow can be\nexecuted with little or no change on a wide variety of heterogeneous\nsystems, ranging from mobile devices such as phones\nand tablets up to large-scale distributed systems of hundreds\nof machines and thousands of computational devices such as\nGPU cards. The system is flexible and can be used to express\na wide variety of algorithms, including training and inference\nalgorithms for deep neural network models, and it has been\nused for conducting research and for deploying machine learning\nsystems into production across more than a dozen areas of\ncomputer science and other fields, including speech recognition,\ncomputer vision, robotics, information retrieval, natural\nlanguage processing, geographic information extraction, and\ncomputational drug discovery. This paper describes the TensorFlow\ninterface and an implementation of that interface that\nwe have built at Google. The TensorFlow API and a reference\nimplementation were released as an open-source package under\nthe Apache 2.0 license in November, 2015 and are available at\nwww.tensorflow.org.\nIn BibTeX format\nIf you use TensorFlow in your research and would like to cite the TensorFlow\nsystem, we suggest you cite this whitepaper.\n\n@misc{tensorflow2015-whitepaper,\ntitle={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},\nurl={https://www.tensorflow.org/},\nnote={Software available from tensorflow.org},\nauthor={\n    Mart\\'{i}n~Abadi and\n    Ashish~Agarwal and\n    Paul~Barham and\n    Eugene~Brevdo and\n    Zhifeng~Chen and\n    Craig~Citro and\n    Greg~S.~Corrado and\n    Andy~Davis and\n    Jeffrey~Dean and\n    Matthieu~Devin and\n    Sanjay~Ghemawat and\n    Ian~Goodfellow and\n    Andrew~Harp and\n    Geoffrey~Irving and\n    Michael~Isard and\n    Yangqing Jia and\n    Rafal~Jozefowicz and\n    Lukasz~Kaiser and\n    Manjunath~Kudlur and\n    Josh~Levenberg and\n    Dandelion~Man\\'{e} and\n    Rajat~Monga and\n    Sherry~Moore and\n    Derek~Murray and\n    Chris~Olah and\n    Mike~Schuster and\n    Jonathon~Shlens and\n    Benoit~Steiner and\n    Ilya~Sutskever and\n    Kunal~Talwar and\n    Paul~Tucker and\n    Vincent~Vanhoucke and\n    Vijay~Vasudevan and\n    Fernanda~Vi\\'{e}gas and\n    Oriol~Vinyals and\n    Pete~Warden and\n    Martin~Wattenberg and\n    Martin~Wicke and\n    Yuan~Yu and\n    Xiaoqiang~Zheng},\n  year={2015},\n}\n\nOr in textual form:\n\nMart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo,\nZhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis,\nJeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow,\nAndrew Harp, Geoffrey Irving, Michael Isard, Rafal Jozefowicz, Yangqing Jia,\nLukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Man\u00e9, Mike Schuster,\nRajat Monga, Sherry Moore, Derek Murray, Chris Olah, Jonathon Shlens,\nBenoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker,\nVincent Vanhoucke, Vijay Vasudevan, Fernanda Vi\u00e9gas,\nOriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke,\nYuan Yu, and Xiaoqiang Zheng.\nTensorFlow: Large-scale machine learning on heterogeneous systems,\n2015. Software available from tensorflow.org.\n\nTensorFlow: A System for Large-Scale Machine Learning\nAccess this white paper.\nAbstract: TensorFlow is a machine learning system that operates at\nlarge scale and in heterogeneous environments. TensorFlow\nuses dataflow graphs to represent computation,\nshared state, and the operations that mutate that state. It\nmaps the nodes of a dataflow graph across many machines\nin a cluster, and within a machine across multiple computational\ndevices, including multicore CPUs, general purpose\nGPUs, and custom-designed ASICs known as\nTensor Processing Units (TPUs). This architecture gives\nflexibility to the application developer: whereas in previous\n\u201cparameter server\u201d designs the management of shared\nstate is built into the system, TensorFlow enables developers\nto experiment with novel optimizations and training algorithms.\nTensorFlow supports a variety of applications,\nwith a focus on training and inference on deep neural networks.\nSeveral Google services use TensorFlow in production,\nwe have released it as an open-source project, and\nit has become widely used for machine learning research.\nIn this paper, we describe the TensorFlow dataflow model\nand demonstrate the compelling performance that TensorFlow\nachieves for several real-world applications.",
    "tag": "tensorflow"
  },
  {
    "title": "Datasets for Estimators",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/guide/datasets_for_estimators.md",
    "content": "Datasets for Estimators\nThe `tf.data` module contains a collection of classes that allows you to\neasily load data, manipulate it, and pipe it into your model. This document\nintroduces the API by walking through two simple examples:\n\nReading in-memory data from numpy arrays.\nReading lines from a csv file.\n\n\nBasic input\nTaking slices from an array is the simplest way to get started with `tf.data`.\nThe Premade Estimators chapter describes\nthe following `train_input_fn`, from\niris_data.py,\nto pipe the data into the Estimator:\n``` python\ndef train_input_fn(features, labels, batch_size):\n    \"\"\"An input function for training\"\"\"\n    # Convert the inputs to a Dataset.\n    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n\n\n```# Shuffle, repeat, and batch the examples.\ndataset = dataset.shuffle(1000).repeat().batch(batch_size)\n\n# Return the dataset.\nreturn dataset\n```\n\n\n```\nLet's look at this more closely.\nArguments\nThis function expects three arguments. Arguments expecting an \"array\" can\naccept nearly anything that can be converted to an array with `numpy.array`.\nOne exception is\ntuple\nwhich, as we will see, has special meaning for `Datasets`.\n\n`features`: A `{'feature_name':array}` dictionary (or\n  DataFrame)\n  containing the raw input features.\n`labels` : An array containing the\n  label\n  for each example.\n`batch_size` : An integer indicating the desired batch size.\n\nIn premade_estimator.py\nwe retrieved the Iris data using the `iris_data.load_data()` function.\nYou can run it, and unpack the results as follows:\n``` python\nimport iris_data\nFetch the data\ntrain, test = iris_data.load_data()\nfeatures, labels = train\n```\nThen we passed this data to the input function, with a line similar to this:\n`python\nbatch_size=100\niris_data.train_input_fn(features, labels, batch_size)`\nLet's walk through the `train_input_fn()`.\nSlices\nThe function starts by using the `tf.data.Dataset.from_tensor_slices` function\nto create a `tf.data.Dataset` representing slices of the array. The array is\nsliced across the first dimension. For example, an array containing the\nMNIST training data has a shape of `(60000, 28, 28)`. Passing this to\n`from_tensor_slices` returns a `Dataset` object containing 60000 slices, each one\na 28x28 image.\nThe code that returns this `Dataset` is as follows:\n``` python\ntrain, test = tf.keras.datasets.mnist.load_data()\nmnist_x, mnist_y = train\nmnist_ds = tf.data.Dataset.from_tensor_slices(mnist_x)\nprint(mnist_ds)\n```\nThis will print the following line, showing the\nshapes and\ntypes of the items in\nthe dataset. Note that a `Dataset` does not know how many items it contains.\n`<TensorSliceDataset shapes: (28,28), types: tf.uint8>`\nThe `Dataset` above represents a simple collection of arrays, but datasets are\nmuch more powerful than this. A `Dataset` can transparently handle any nested\ncombination of dictionaries or tuples (or\nnamedtuple\n).\nFor example after converting the iris `features`\nto a standard python dictionary, you can then convert the dictionary of arrays\nto a `Dataset` of dictionaries as follows:\n`python\ndataset = tf.data.Dataset.from_tensor_slices(dict(features))\nprint(dataset)`\n```\n<TensorSliceDataset\nshapes: {\n    SepalLength: (), PetalWidth: (),\n    PetalLength: (), SepalWidth: ()},\ntypes: {\n      SepalLength: tf.float64, PetalWidth: tf.float64,\n      PetalLength: tf.float64, SepalWidth: tf.float64}\n\n```\n\nHere we see that when a `Dataset` contains structured elements, the `shapes`\nand `types` of the `Dataset` take on the same structure. This dataset contains\ndictionaries of scalars, all of type\n`tf.float64`.\nThe first line of the iris `train_input_fn` uses the same functionality, but\nadds another level of structure. It creates a dataset containing\n`(features_dict, label)` pairs.\nThe following code shows that the label is a scalar with type `int64`:\n``` python\nConvert the inputs to a Dataset.\ndataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\nprint(dataset)\n```\n```\n<TensorSliceDataset\n    shapes: (\n        {\n          SepalLength: (), PetalWidth: (),\n          PetalLength: (), SepalWidth: ()},\n        ()),\n\n\n```types: (\n    {\n      SepalLength: tf.float64, PetalWidth: tf.float64,\n      PetalLength: tf.float64, SepalWidth: tf.float64},\n    tf.int64)>\n```\n\n\n```\nManipulation\nCurrently the `Dataset` would iterate over the data once, in a fixed order, and\nonly produce a single element at a time. It needs further processing before it\ncan be used for training. Fortunately, the `tf.data.Dataset` class provides\nmethods to better prepare the data for training. The next line of the input\nfunction takes advantage of several of these methods:\n``` python\nShuffle, repeat, and batch the examples.\ndataset = dataset.shuffle(1000).repeat().batch(batch_size)\n```\nThe `tf.data.Dataset.shuffle` method uses a fixed-size buffer to\nshuffle the items as they pass through. In this case the `buffer_size` is\ngreater than the number of examples in the `Dataset`, ensuring that the data is\ncompletely shuffled (The Iris data set only contains 150 examples).\nThe `tf.data.Dataset.repeat` method restarts the `Dataset` when\nit reaches the end. To limit the number of epochs, set the `count` argument.\nThe `tf.data.Dataset.batch` method collects a number of examples and\nstacks them, to create batches. This adds a dimension to their shape. The new\ndimension is added as the first dimension. The following code uses\nthe `batch` method on the MNIST `Dataset`, from earlier. This results in a\n`Dataset` containing 3D arrays representing stacks of `(28,28)` images:\n`python\nprint(mnist_ds.batch(100))`\n`<BatchDataset\n  shapes: (?, 28, 28),\n  types: tf.uint8>`\nNote that the dataset has an unknown batch size because the last batch will\nhave fewer elements.\nIn `train_input_fn`, after batching the `Dataset` contains 1D vectors of\nelements where each scalar was previously:\n`python\nprint(dataset)`\n```\n<TensorSliceDataset\n    shapes: (\n        {\n          SepalLength: (?,), PetalWidth: (?,),\n          PetalLength: (?,), SepalWidth: (?,)},\n        (?,)),\n\n\n```types: (\n    {\n      SepalLength: tf.float64, PetalWidth: tf.float64,\n      PetalLength: tf.float64, SepalWidth: tf.float64},\n    tf.int64)>\n```\n\n\n```\nReturn\nAt this point the `Dataset` contains `(features_dict, labels)` pairs.\nThis is the format expected by the `train` and `evaluate` methods, so the\n`input_fn` returns the dataset.\nThe `labels` can/should be omitted when using the `predict` method.\n\nReading a CSV File\nThe most common real-world use case for the `Dataset` class is to stream data\nfrom files on disk. The `tf.data` module includes a variety of\nfile readers. Let's see how parsing the Iris dataset from the csv file looks\nusing a `Dataset`.\nThe following call to the `iris_data.maybe_download` function downloads the\ndata if necessary, and returns the pathnames of the resulting files:\n`python\nimport iris_data\ntrain_path, test_path = iris_data.maybe_download()`\nThe iris_data.csv_input_fn\nfunction contains an alternative implementation that parses the csv files using\na `Dataset`.\nLet's look at how to build an Estimator-compatible input function that reads\nfrom the local files.\nBuild the `Dataset`\nWe start by building a `tf.data.TextLineDataset` object to\nread the file one line at a time. Then, we call the\n`tf.data.Dataset.skip` method to skip over the first line of the file, which contains a header, not an example:\n`python\nds = tf.data.TextLineDataset(train_path).skip(1)`\nBuild a csv line parser\nWe will start by building a function to parse a single line.\nThe following `iris_data.parse_line` function accomplishes this task using the\n`tf.decode_csv` function, and some simple python code:\nWe must parse each of the lines in the dataset in order to generate the\nnecessary `(features, label)` pairs. The following `_parse_line` function\ncalls `tf.decode_csv` to parse a single line into its features\nand the label. Since Estimators require that features be represented as a\ndictionary, we rely on Python's built-in `dict` and `zip` functions to build\nthat dictionary.  The feature names are the keys of that dictionary.\nWe then call the dictionary's `pop` method to remove the label field from\nthe features dictionary:\n``` python\nMetadata describing the text columns\nCOLUMNS = ['SepalLength', 'SepalWidth',\n           'PetalLength', 'PetalWidth',\n           'label']\nFIELD_DEFAULTS = [[0.0], [0.0], [0.0], [0.0], [0]]\ndef _parse_line(line):\n    # Decode the line into its fields\n    fields = tf.decode_csv(line, FIELD_DEFAULTS)\n\n\n```# Pack the result into a dictionary\nfeatures = dict(zip(COLUMNS,fields))\n\n# Separate the label from the features\nlabel = features.pop('label')\n\nreturn features, label\n```\n\n\n```\nParse the lines\nDatasets have many methods for manipulating the data while it is being piped\nto a model. The most heavily-used method is `tf.data.Dataset.map`, which\napplies a transformation to each element of the `Dataset`.\nThe `map` method takes a `map_func` argument that describes how each item in the\n`Dataset` should be transformed.\n\n\n\n\nThe `tf.data.Dataset.map` method applies the `map_func` to\ntransform each item in the `Dataset`.\n\nSo to parse the lines as they are streamed out of the csv file, we pass our\n`_parse_line` function to the `map` method:\n`python\nds = ds.map(_parse_line)\nprint(ds)`\n`<MapDataset\nshapes: (\n    {SepalLength: (), PetalWidth: (), ...},\n    ()),\ntypes: (\n    {SepalLength: tf.float32, PetalWidth: tf.float32, ...},\n    tf.int32)>`\nNow instead of simple scalar strings, the dataset contains `(features, label)`\npairs.\nthe remainder of the `iris_data.csv_input_fn` function is identical\nto `iris_data.train_input_fn` which was covered in the in the\nBasic input section.\nTry it out\nThis function can be used as a replacement for\n`iris_data.train_input_fn`. It can be used to feed an estimator as follows:\n``` python\ntrain_path, test_path = iris_data.maybe_download()\nAll the inputs are numeric\nfeature_columns = [\n    tf.feature_column.numeric_column(name)\n    for name in iris_data.CSV_COLUMN_NAMES[:-1]]\nBuild the estimator\nest = tf.estimator.LinearClassifier(feature_columns,\n                                    n_classes=3)\nTrain the estimator\nbatch_size = 100\nest.train(\n    steps=1000,\n    input_fn=lambda : iris_data.csv_input_fn(train_path, batch_size))\n```\nEstimators expect an `input_fn` to take no arguments. To work around this\nrestriction, we use `lambda` to capture the arguments and provide the expected\ninterface.\nSummary\nThe `tf.data` module provides a collection of classes and functions for easily\nreading data from a variety of sources. Furthermore, `tf.data` has simple\npowerful methods for applying a wide variety of standard and custom\ntransformations.\nNow you have the basic idea of how to efficiently load data into an\nEstimator. Consider the following documents next:\n\nCreating Custom Estimators, which demonstrates how to build your own\n  custom `Estimator` model.\nThe Low Level Introduction, which demonstrates\n  how to experiment directly with `tf.data.Datasets` using TensorFlow's low\n  level APIs.\nImporting Data which goes into great detail about additional\n  functionality of `Datasets`.\n",
    "tag": "tensorflow"
  },
  {
    "title": "TensorFlow Debugger",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/guide/debugger.md",
    "content": "TensorFlow Debugger\n[TOC]\n`tfdbg` is a specialized debugger for TensorFlow. It lets you view the internal\nstructure and states of running TensorFlow graphs during training and inference,\nwhich is difficult to debug with general-purpose debuggers such as Python's `pdb`\ndue to TensorFlow's computation-graph paradigm.\nThis guide focuses on the command-line interface (CLI) of `tfdbg`. For guide on\nhow to use the graphical user interface (GUI) of tfdbg, i.e., the\nTensorBoard Debugger Plugin, please visit\nits README.\nNote: The TensorFlow debugger uses a\ncurses-based text\nuser interface. On macOS, the `ncurses` library is required and can be\ninstalled with `brew install ncurses`. On Windows, curses isn't as\nwell supported, so a readline-based\ninterface can be used with tfdbg by installing `pyreadline` with `pip`. If you\nuse Anaconda3, you can install it with a command such as\n`\"C:\\Program Files\\Anaconda3\\Scripts\\pip.exe\" install pyreadline`. Unofficial\nWindows curses packages can be downloaded\nhere, then subsequently\ninstalled using `pip install <your_version>.whl`, however curses on Windows may\nnot work as reliably as curses on Linux or Mac.\nThis tutorial demonstrates how to use the tfdbg CLI to debug the appearance\nof nans and\ninfs, a frequently-encountered type\nof bug in TensorFlow model development. The following example is for users who\nuse the low-level\nSession API of\nTensorFlow. Later sections of this document describe how to use tfdbg with\nhigher-level APIs of TensorFlow, including `tf.estimator`, `tf.keras` / `keras`\nand `tf.contrib.slim`. To observe such an issue, run the following command\nwithout the debugger (the source code can be found\nhere):\n\npython -m tensorflow.python.debug.examples.v1.debug_mnist\n\nThis code trains a simple neural network for MNIST digit image recognition.\nNotice that the accuracy increases slightly after the first training step, but\nthen gets stuck at a low (near-chance) level:\n\nAccuracy at step 0: 0.1113\nAccuracy at step 1: 0.3183\nAccuracy at step 2: 0.098\nAccuracy at step 3: 0.098\nAccuracy at step 4: 0.098\n\nWondering what might have gone wrong, you suspect that certain nodes in the\ntraining graph generated bad numeric values such as `inf`s and `nan`s, because\nthis is a common cause of this type of training failure.\nLet's use tfdbg to debug this issue and pinpoint the exact graph node where this\nnumeric problem first surfaced.\nWrapping TensorFlow Sessions with tfdbg\nTo add support for tfdbg in our example, all that is needed is to add the\nfollowing lines of code and wrap the Session object with a debugger wrapper.\nThis code is already added in\ndebug_mnist.py,\nso you can activate tfdbg CLI with the `--debug` flag at the command line.\n```python\nLet your BUILD target depend on \"//tensorflow/python/debug:debug_py\"\n(You don't need to worry about the BUILD dependency if you are using a pip\ninstall of open-source TensorFlow.)\nfrom tensorflow.python import debug as tf_debug\nsess = tf_debug.LocalCLIDebugWrapperSession(sess)\n```\nThis wrapper has the same interface as Session, so enabling debugging requires\nno other changes to the code. The wrapper provides additional features,\nincluding:\n\nBringing up a CLI before and after `Session.run()` calls, to let you\ncontrol the execution and inspect the graph's internal state.\nAllowing you to register special `filters` for tensor values, to facilitate\nthe diagnosis of issues.\n\nIn this example, we have already registered a tensor filter called\n`tfdbg.has_inf_or_nan`,\nwhich simply determines if there are any `nan` or `inf` values in any\nintermediate tensors (tensors that are neither inputs or outputs of the\n`Session.run()` call, but are in the path leading from the inputs to the\noutputs). This filter is for `nan`s and `inf`s is a common enough use case that\nwe ship it with the\ndebug_data\nmodule.\nNote: You can also write your own custom filters. See `tfdbg.DebugDumpDir.find`\nfor additional information.\nDebugging Model Training with tfdbg\nLet's try training the model again, but with the `--debug` flag added this time:\n\npython -m tensorflow.python.debug.examples.v1.debug_mnist --debug\n\nThe debug wrapper session will prompt you when it is about to execute the first\n`Session.run()` call, with information regarding the fetched tensor and feed\ndictionaries displayed on the screen.\n\nThis is what we refer to as the run-start CLI. It lists the feeds and fetches\nto the current `Session.run` call, before executing anything.\nIf the screen size is too small to display the content of the message in its\nentirety, you can resize it.\nUse the PageUp / PageDown / Home / End keys to navigate the\nscreen output. On most keyboards lacking those keys Fn + Up /\nFn + Down / Fn + Right / Fn + Left will work.\nEnter the `run` command (or just `r`) at the command prompt:\n`tfdbg> run`\nThe `run` command causes tfdbg to execute until the end of the next\n`Session.run()` call, which calculates the model's accuracy using a test data\nset. tfdbg augments the runtime Graph to dump all intermediate tensors.\nAfter the run ends, tfdbg displays all the dumped tensors values in the\nrun-end CLI. For example:\n\nThis list of tensors can also be obtained by running the command `lt` after you\nexecuted `run`.\ntfdbg CLI Frequently-Used Commands\nTry the following commands at the `tfdbg>` prompt (referencing the code at\n`tensorflow/python/debug/examples/v1/debug_mnist.py`):\n| Command            | Syntax or Option | Explanation  | Example                   |\n|:-------------------|:---------------- |:------------ |:------------------------- |\n| `lt` | | List dumped tensors. | `lt` |\n| | `-n <name_pattern>` | List dumped tensors with names matching given regular-expression pattern. | `lt -n Softmax.*` |\n| | `-t <op_pattern>` | List dumped tensors with op types matching given regular-expression pattern. | `lt -t MatMul` |\n| | `-f <filter_name>` | List only the tensors that pass a registered tensor filter. | `lt -f has_inf_or_nan` |\n| | `-f <filter_name> -fenn <regex>` | List only the tensors that pass a registered tensor filter, excluding nodes with names matching the regular expression. | `lt -f has_inf_or_nan` `-fenn .*Sqrt.*` |\n| | `-s <sort_key>` | Sort the output by given `sort_key`, whose possible values are `timestamp` (default), `dump_size`, `op_type` and `tensor_name`. | `lt -s dump_size` |\n| | `-r` | Sort in reverse order. | `lt -r -s dump_size` |\n| `pt` | | Print value of a dumped tensor. | |\n| | `pt <tensor>` | Print tensor value. | `pt hidden/Relu:0` |\n| | `pt <tensor>[slicing]` | Print a subarray of tensor, using numpy-style array slicing. | `pt hidden/Relu:0[0:50,:]` |\n| | `-a` | Print the entirety of a large tensor, without using ellipses. (May take a long time for large tensors.) | `pt -a hidden/Relu:0[0:50,:]` |\n| | `-r <range>` | Highlight elements falling into specified numerical range. Multiple ranges can be used in conjunction. | `pt hidden/Relu:0 -a -r [[-inf,-1],[1,inf]]` |\n| | `-n <number>` | Print dump corresponding to specified 0-based dump number. Required for tensors with multiple dumps. | `pt -n 0 hidden/Relu:0` |\n| | `-s` | Include a summary of the numeric values of the tensor (applicable only to non-empty tensors with Boolean and numeric types such as `int*` and `float*`.) | `pt -s hidden/Relu:0[0:50,:]` |\n| | `-w` | Write the value of the tensor (possibly sliced) to a Numpy file using numpy.save() | `pt -s hidden/Relu:0 -w /tmp/relu.npy` |\n| `@[coordinates]` | | Navigate to specified element in `pt` output. | `@[10,0]` or `@10,0` |\n| `/regex` | |  less-style search for given regular expression. | `/inf` |\n| `/` | | Scroll to the next line with matches to the searched regex (if any). | `/` |\n| `pf` | | Print a value in the feed_dict to `Session.run`. | |\n| | `pf <feed_tensor_name>` | Print the value of the feed. Also note that the `pf` command has the `-a`, `-r` and `-s` flags (not listed below), which have the same syntax and semantics as the identically-named flags of `pt`. | `pf input_xs:0` |\n| eval | | Evaluate arbitrary Python and numpy expression. | |\n| | `eval <expression>` | Evaluate a Python / numpy expression, with numpy available as `np` and debug tensor names enclosed in backticks. | `eval \"np.matmul((`output/Identity:0` / `Softmax:0`).T, `Softmax:0`)\"` |\n| | `-a` | Print a large-sized evaluation result in its entirety, i.e., without using ellipses. | `eval -a 'np.sum(`Softmax:0`, axis=1)'` |\n| | `-w` | Write the result of the evaluation to a Numpy file using numpy.save() | `eval -a 'np.sum(`Softmax:0`, axis=1)' -w /tmp/softmax_sum.npy` |\n| `ni` | | Display node information. | |\n| | `-a` | Include node attributes in the output. | `ni -a hidden/Relu` |\n| | `-d` | List the debug dumps available from the node. | `ni -d hidden/Relu` |\n| | `-t` | Display the Python stack trace of the node's creation. | `ni -t hidden/Relu` |\n| `li` | | List inputs to node | |\n| | `-r` | List the inputs to node, recursively (the input tree.) | `li -r hidden/Relu:0` |\n| | `-d <max_depth>` | Limit recursion depth under the `-r` mode. | `li -r -d 3 hidden/Relu:0` |\n| | `-c` | Include control inputs. | `li -c -r hidden/Relu:0` |\n| | `-t` | Show op types of input nodes. | `li -t -r hidden/Relu:0` |\n| `lo` | | List output recipients of node | |\n| | `-r` | List the output recipients of node, recursively (the output tree.) | `lo -r hidden/Relu:0` |\n| | `-d <max_depth>` | Limit recursion depth under the `-r` mode. | `lo -r -d 3 hidden/Relu:0` |\n| | `-c` | Include recipients via control edges. | `lo -c -r hidden/Relu:0` |\n| | `-t` | Show op types of recipient nodes. | `lo -t -r hidden/Relu:0` |\n| `ls` | | List Python source files involved in node creation. | |\n| | `-p <path_pattern>` | Limit output to source files matching given regular-expression path pattern. | `ls -p .*debug_mnist.*` |\n| | `-n` | Limit output to node names matching given regular-expression pattern. | `ls -n Softmax.*` |\n| `ps` | | Print Python source file. | |\n| | `ps <file_path>` | Print given Python source file source.py, with the lines annotated with the nodes created at each of them (if any). | `ps /path/to/source.py` |\n| | `-t` | Perform annotation with respect to Tensors, instead of the default, nodes. | `ps -t /path/to/source.py` |\n| | `-b <line_number>` | Annotate source.py beginning at given line. | `ps -b 30 /path/to/source.py` |\n| | `-m <max_elements>` | Limit the number of elements in the annotation for each line. | `ps -m 100 /path/to/source.py` |\n| `run` | | Proceed to the next Session.run() | `run` |\n| | `-n` | Execute through the next `Session.run` without debugging, and drop to CLI right before the run after that. | `run -n` |\n| | `-t <T>` | Execute `Session.run` `T - 1` times without debugging, followed by a run with debugging. Then drop to CLI right after the debugged run. | `run -t 10` |\n| | `-f <filter_name>` | Continue executing `Session.run` until any intermediate tensor triggers the specified Tensor filter (causes the filter to return `True`). | `run -f has_inf_or_nan` |\n| | `-f <filter_name> -fenn <regex>` | Continue executing `Session.run` until any intermediate tensor whose node names doesn't match the regular expression triggers the specified Tensor filter (causes the filter to return `True`). | `run -f has_inf_or_nan -fenn .*Sqrt.*` |\n| | `--node_name_filter <pattern>` | Execute the next `Session.run`, watching only nodes with names matching the given regular-expression pattern. | `run --node_name_filter Softmax.*` |\n| | `--op_type_filter <pattern>` | Execute the next `Session.run`, watching only nodes with op types matching the given regular-expression pattern. | `run --op_type_filter Variable.*` |\n| | `--tensor_dtype_filter <pattern>` | Execute the next `Session.run`, dumping only Tensors with data types (`dtype`s) matching the given regular-expression pattern. | `run --tensor_dtype_filter int.*` |\n| | `-p` | Execute the next `Session.run` call in profiling mode. | `run -p` |\n| `ri` | | Display information about the current run, including fetches and feeds. | `ri` |\n| `config` | | Set or show persistent TFDBG UI configuration. | |\n| | `set` | Set the value of a config item: {`graph_recursion_depth`, `mouse_mode`}. | `config set graph_recursion_depth 3` |\n| | `show` | Show current persistent UI configuration. | `config show` |\n| `version` | | Print the version of TensorFlow and its key dependencies. | `version` |\n| `help` | | Print general help information | `help` |\n| | `help <command>` | Print help for given command. | `help lt` |\nNote that each time you enter a command, a new screen output\nwill appear. This is somewhat analogous to web pages in a browser. You can\nnavigate between these screens by clicking the `<--` and\n`-->` text arrows near the top-left corner of the CLI.\nOther Features of the tfdbg CLI\nIn addition to the commands listed above, the tfdbg CLI provides the following\nadditional features:\n\nTo navigate through previous tfdbg commands, type in a few characters\n    followed by the Up or Down arrow keys. tfdbg will show you the history of\n    commands that started with those characters.\nTo navigate through the history of screen outputs, do either of the\n    following:\nUse the `prev` and `next` commands.\nClick underlined `<--` and `-->` links near the top left corner of the\n  screen.\n\n\nTab completion of commands and some command arguments.\nTo redirect the screen output to a file instead of the screen, end the\n    command with bash-style redirection. For example, the following command\n    redirects the output of the pt command to the `/tmp/xent_value_slices.txt`\n    file:\n\n\ntfdbg> pt cross_entropy/Log:0[:, 0:10] > /tmp/xent_value_slices.txt\n\nFinding `nan`s and `inf`s\nIn this first `Session.run()` call, there happen to be no problematic numerical\nvalues. You can move on to the next run by using the command `run` or its\nshorthand `r`.\n\nTIP: If you enter `run` or `r` repeatedly, you will be able to move through\nthe `Session.run()` calls in a sequential manner.\nYou can also use the `-t` flag to move ahead a number of `Session.run()` calls\nat a time, for example:\n`tfdbg> run -t 10`\n\nInstead of entering `run` repeatedly and manually searching for `nan`s and\n`inf`s in the run-end UI after every `Session.run()` call (for example, by using\nthe `pt` command shown in the table above) , you can use the following\ncommand to let the debugger repeatedly execute `Session.run()` calls without\nstopping at the run-start or run-end prompt, until the first `nan` or `inf`\nvalue shows up in the graph. This is analogous to conditional breakpoints in\nsome procedural-language debuggers:\n\ntfdbg> run -f has_inf_or_nan\n\n\nNOTE: The preceding command works properly because a tensor filter called\n`has_inf_or_nan` has been registered for you when the wrapped session is\ncreated. This filter detects `nan`s and `inf`s (as explained previously).\nIf you have registered any other filters, you can\nuse \"run -f\" to have tfdbg run until any tensor triggers that filter (cause\nthe filter to return True).\n``` python\ndef my_filter_callable(datum, tensor):\n  # A filter that detects zero-valued scalars.\n  return len(tensor.shape) == 0 and tensor == 0.0\nsess.add_tensor_filter('my_filter', my_filter_callable)\n```\nThen at the tfdbg run-start prompt run until your filter is triggered:\n`tfdbg> run -f my_filter`\n\nSee this API document\nfor more information on the expected signature and return value of the predicate\n`Callable` used with `add_tensor_filter()`.\n\nAs the screen display indicates on the first line, the `has_inf_or_nan` filter is first triggered\nduring the fourth `Session.run()` call: an\nAdam optimizer\nforward-backward training pass on the graph. In this run, 36 (out of the total\n95) intermediate tensors contain `nan` or `inf` values. These tensors are listed\nin chronological order, with their timestamps displayed on the left. At the top\nof the list, you can see the first tensor in which the bad numerical values\nfirst surfaced: `cross_entropy/Log:0`.\nTo view the value of the tensor, click the underlined tensor name\n`cross_entropy/Log:0` or enter the equivalent command:\n\ntfdbg> pt cross_entropy/Log:0\n\nScroll down a little and you will notice some scattered `inf` values. If the\ninstances of `inf` and `nan` are difficult to spot by eye, you can use the\nfollowing command to perform a regex search and highlight the output:\n\ntfdbg> /inf\n\nOr, alternatively:\n\ntfdbg> /(inf|nan)\n\nYou can also use the `-s` or `--numeric_summary` command to get a quick summary\nof the types of numeric values in the tensor:\n\ntfdbg> pt -s cross_entropy/Log:0\n\nFrom the summary, you can see that several of the 1000 elements of the\n`cross_entropy/Log:0` tensor are `-inf`s (negative infinities).\nWhy did these infinities appear? To further debug, display more information\nabout the node `cross_entropy/Log` by clicking the underlined `node_info` menu\nitem on the top or entering the equivalent node_info (`ni`) command:\n\ntfdbg> ni cross_entropy/Log\n\n\nYou can see that this node has the op type `Log`\nand that its input is the node `Softmax`. Run the following command to\ntake a closer look at the input tensor:\n\ntfdbg> pt Softmax:0\n\nExamine the values in the input tensor, searching for zeros:\n\ntfdbg> /0\\.000\n\nIndeed, there are zeros. Now it is clear that the origin of the bad numerical\nvalues is the node `cross_entropy/Log` taking logs of zeros. To find out the\nculprit line in the Python source code, use the `-t` flag of the `ni` command\nto show the traceback of the node's construction:\n\ntfdbg> ni -t cross_entropy/Log\n\nIf you click \"node_info\" at the top of the screen, tfdbg automatically shows the\ntraceback of the node's construction.\nFrom the traceback, you can see that the op is constructed at the following\nline:\ndebug_mnist.py:\n`python\ndiff = y_ * tf.log(y)`\ntfdbg has a feature that makes it easy to trace Tensors and ops back to\nlines in Python source files. It can annotate lines of a Python file with\nthe ops or Tensors created by them. To use this feature,\nsimply click the underlined line numbers in the stack trace output of the\n`ni -t <op_name>` commands, or use the `ps` (or `print_source`) command such as:\n`ps /path/to/source.py`. For example, the following screenshot shows the output\nof a `ps` command.\n\nFixing the problem\nTo fix the problem, edit `debug_mnist.py`, changing the original line:\n`python\ndiff = -(y_ * tf.log(y))`\nto the built-in, numerically-stable implementation of softmax cross-entropy:\n`python\ndiff = tf.losses.softmax_cross_entropy(labels=y_, logits=logits)`\nRerun with the `--debug` flag as follows:\n\npython -m tensorflow.python.debug.examples.v1.debug_mnist --debug\n\nAt the `tfdbg>` prompt, enter the following command:\n\nrun -f has_inf_or_nan\n\nConfirm that no tensors are flagged as containing `nan` or `inf` values, and\naccuracy now continues to rise rather than getting stuck. Success!\nDebugging TensorFlow Estimators\nThis section explains how to debug TensorFlow programs that use the `Estimator`\nAPIs. Part of the convenience provided by these APIs is that\nthey manage `Session`s internally. This makes the `LocalCLIDebugWrapperSession`\ndescribed in the preceding sections inapplicable. Fortunately, you can still\ndebug them by using special `hook`s provided by `tfdbg`.\n`tfdbg` can debug the\n`tf.estimator.Estimator.train`,\n`tf.estimator.Estimator.evaluate` and\n`tf.estimator.Estimator.predict`\nmethods of tf-learn `Estimator`s. To debug `Estimator.train()`,\ncreate a `LocalCLIDebugHook` and supply it in the `hooks` argument. For example:\n```python\nFirst, let your BUILD target depend on \"//tensorflow/python/debug:debug_py\"\n(You don't need to worry about the BUILD dependency if you are using a pip\ninstall of open-source TensorFlow.)\nfrom tensorflow.python import debug as tf_debug\nCreate a LocalCLIDebugHook and use it as a monitor when calling fit().\nhooks = [tf_debug.LocalCLIDebugHook()]\nTo debug `train`:\nclassifier.train(input_fn,\n                 steps=1000,\n                 hooks=hooks)\n```\nSimilarly, to debug `Estimator.evaluate()` and `Estimator.predict()`, assign\nhooks to the `hooks` parameter, as in the following example:\n```python\nTo debug `evaluate`:\naccuracy_score = classifier.evaluate(eval_input_fn,\n                                     hooks=hooks)[\"accuracy\"]\nTo debug `predict`:\npredict_results = classifier.predict(predict_input_fn, hooks=hooks)\n```\ndebug_tflearn_iris.py,\ncontains a full example of how to use the tfdbg with `Estimator`s. To run this\nexample, do:\n\npython -m tensorflow.python.debug.examples.v1.debug_tflearn_iris --debug\n\nThe `LocalCLIDebugHook` also allows you to configure a `watch_fn` that can be\nused to flexibly specify what `Tensor`s to watch on different `Session.run()`\ncalls, as a function of the `fetches` and `feed_dict` and other states. See\n`tfdbg.DumpingDebugWrapperSession.__init__`\nfor more details.\nDebugging Keras Models with TFDBG\nTo use TFDBG with\ntf.keras,\nlet the Keras backend use a TFDBG-wrapped Session object. For example, to use\nthe CLI wrapper:\n``` python\nimport tensorflow as tf\nfrom tensorflow.python import debug as tf_debug\ntf.keras.backend.set_session(tf_debug.LocalCLIDebugWrapperSession(tf.Session()))\nDefine your keras model, called \"model\".\nCalls to `fit()`, 'evaluate()`and`predict()` methods will break into the\nTFDBG CLI.\nmodel.fit(...)\nmodel.evaluate(...)\nmodel.predict(...)\n```\nWith minor modification, the preceding code example also works for the\nnon-TensorFlow version of Keras running against a\nTensorFlow backend. You just need to replace `tf.keras.backend` with\n`keras.backend`.\nDebugging tf-slim with TFDBG\nTFDBG supports debugging of training and evaluation with\ntf-slim.\nAs detailed below, training and evaluation require slightly different debugging\nworkflows.\nDebugging training in tf-slim\nTo debug the training process, provide `LocalCLIDebugWrapperSession` to the\n`session_wrapper` argument of `slim.learning.train()`. For example:\n``` python\nimport tensorflow as tf\nfrom tensorflow.python import debug as tf_debug\n... Code that creates the graph and the train_op ...\ntf.contrib.slim.learning.train(\n    train_op,\n    logdir,\n    number_of_steps=10,\n    session_wrapper=tf_debug.LocalCLIDebugWrapperSession)\n```\nDebugging evaluation in tf-slim\nTo debug the evaluation process, provide `LocalCLIDebugHook` to the\n`hooks` argument of `slim.evaluation.evaluate_once()`. For example:\n``` python\nimport tensorflow as tf\nfrom tensorflow.python import debug as tf_debug\n... Code that creates the graph and the eval and final ops ...\ntf.contrib.slim.evaluation.evaluate_once(\n    '',\n    checkpoint_path,\n    logdir,\n    eval_op=my_eval_op,\n    final_op=my_value_op,\n    hooks=[tf_debug.LocalCLIDebugHook()])\n```\nOffline Debugging of Remotely-Running Sessions\nOften, your model is running on a remote machine or a process that you don't\nhave terminal access to. To perform model debugging in such cases, you can use\nthe `offline_analyzer` binary of `tfdbg` (described below). It operates on\ndumped data directories. This can be done to both the lower-level `Session` API\nand the higher-level `Estimator` API.\nDebugging Remote tf.Sessions\nIf you interact directly with the `tf.Session` API in `python`, you can\nconfigure the `RunOptions` proto that you call your `Session.run()` method\nwith, by using the method `tfdbg.watch_graph`.\nThis will cause the intermediate tensors and runtime graphs to be dumped to a\nshared storage location of your choice when the `Session.run()` call occurs\n(at the cost of slower performance). For example:\n```python\nfrom tensorflow.python import debug as tf_debug\n... Code where your session and graph are set up...\nrun_options = tf.RunOptions()\ntf_debug.watch_graph(\n      run_options,\n      session.graph,\n      debug_urls=[\"file:///shared/storage/location/tfdbg_dumps_1\"])\nBe sure to specify different directories for different run() calls.\nsession.run(fetches, feed_dict=feeds, options=run_options)\n```\nLater, in an environment that you have terminal access to (for example, a local\ncomputer that can access the shared storage location specified in the code\nabove), you can load and inspect the data in the dump directory on the shared\nstorage by using the `offline_analyzer` binary of `tfdbg`. For example:\n\npython -m tensorflow.python.debug.cli.offline_analyzer \\\n    --dump_dir=/shared/storage/location/tfdbg_dumps_1\n\nThe `Session` wrapper `DumpingDebugWrapperSession` offers an easier and more\nflexible way to generate file-system dumps that can be analyzed offline.\nTo use it, simply wrap your session in a `tf_debug.DumpingDebugWrapperSession`.\nFor example:\n```python\nLet your BUILD target depend on \"//tensorflow/python/debug:debug_py\n(You don't need to worry about the BUILD dependency if you are using a pip\ninstall of open-source TensorFlow.)\nfrom tensorflow.python import debug as tf_debug\nsess = tf_debug.DumpingDebugWrapperSession(\n    sess, \"/shared/storage/location/tfdbg_dumps_1/\", watch_fn=my_watch_fn)\n```\nThe `watch_fn` argument accepts a `Callable` that allows you to configure what\n`tensor`s to watch on different `Session.run()` calls, as a function of the\n`fetches` and `feed_dict` to the `run()` call and other states.\nC++ and other languages\nIf your model code is written in C++ or other languages, you can also\nmodify the `debug_options` field of `RunOptions` to generate debug dumps that\ncan be inspected offline. See\nthe proto definition\nfor more details.\nDebugging Remotely-Running Estimators\nIf your remote TensorFlow server runs `Estimator`s,\nyou can use the non-interactive `DumpingDebugHook`. For example:\n```python\nLet your BUILD target depend on \"//tensorflow/python/debug:debug_py\n(You don't need to worry about the BUILD dependency if you are using a pip\ninstall of open-source TensorFlow.)\nfrom tensorflow.python import debug as tf_debug\nhooks = [tf_debug.DumpingDebugHook(\"/shared/storage/location/tfdbg_dumps_1\")]\n```\nThen this `hook` can be used in the same way as the `LocalCLIDebugHook` examples\ndescribed earlier in this document.\nAs the training, evaluation or prediction happens with `Estimator`,\ntfdbg creates directories having the following name pattern:\n`/shared/storage/location/tfdbg_dumps_1/run_<epoch_timestamp_microsec>_<uuid>`.\nEach directory corresponds to a `Session.run()` call that underlies\nthe `fit()` or `evaluate()` call. You can load these directories and inspect\nthem in a command-line interface in an offline manner using the\n`offline_analyzer` offered by tfdbg. For example:\n`bash\npython -m tensorflow.python.debug.cli.offline_analyzer \\\n    --dump_dir=\"/shared/storage/location/tfdbg_dumps_1/run_<epoch_timestamp_microsec>_<uuid>\"`\nFrequently Asked Questions\nQ: Do the timestamps on the left side of the `lt` output reflect actual\n       performance in a non-debugging session?\nA: No. The debugger inserts additional special-purpose debug nodes to the\n       graph to record the values of intermediate tensors. These nodes\n       slow down the graph execution. If you are interested in profiling your\n       model, check out\n\nThe profiling mode of tfdbg: `tfdbg> run -p`.\ntfprof\n      and other profiling tools for TensorFlow.\n\nQ: How do I link tfdbg against my `Session` in Bazel? Why do I see an\n       error such as \"ImportError: cannot import name debug\"?\nA: In your BUILD rule, declare dependencies:\n       `\"//tensorflow:tensorflow_py\"` and `\"//tensorflow/python/debug:debug_py\"`.\n       The first is the dependency that you include to use TensorFlow even\n       without debugger support; the second enables the debugger.\n       Then, In your Python file, add:\n```python\nfrom tensorflow.python import debug as tf_debug\nThen wrap your TensorFlow Session with the local-CLI wrapper.\nsess = tf_debug.LocalCLIDebugWrapperSession(sess)\n```\nQ: Does tfdbg help debug runtime errors such as shape mismatches?\nA: Yes. tfdbg intercepts errors generated by ops during runtime and presents\n       the errors with some debug instructions to the user in the CLI.\n       See examples:\n\n# Debugging shape mismatch during matrix multiplication.\npython -m tensorflow.python.debug.examples.v1.debug_errors \\\n    --error shape_mismatch --debug\n\n# Debugging uninitialized variable.\npython -m tensorflow.python.debug.examples.v1.debug_errors \\\n    --error uninitialized_variable --debug\n\nQ: How can I let my tfdbg-wrapped Sessions or Hooks run the debug mode\nonly from the main thread?\nA:\nThis is a common use case, in which the `Session` object is used from multiple\nthreads concurrently. Typically, the child threads take care of background tasks\nsuch as running enqueue operations. Often, you want to debug only the main\nthread (or less frequently, only one of the child threads). You can use the\n`thread_name_filter` keyword argument of `LocalCLIDebugWrapperSession` to\nachieve this type of thread-selective debugging. For example, to debug from the\nmain thread only, construct a wrapped `Session` as follows:\n`python\nsess = tf_debug.LocalCLIDebugWrapperSession(sess, thread_name_filter=\"MainThread$\")`\nThe above example relies on the fact that main threads in Python have the\ndefault name `MainThread`.\nQ: The model I am debugging is very large. The data dumped by tfdbg\nfills up the free space of my disk. What can I do?\nA:\nYou might encounter this problem in any of the following situations:\n\nmodels with many intermediate tensors\nvery large intermediate tensors\nmany `tf.while_loop` iterations\n\nThere are three possible workarounds or solutions:\n\nThe constructors of `LocalCLIDebugWrapperSession` and `LocalCLIDebugHook`\n   provide a keyword argument, `dump_root`, to specify the path\n   to which tfdbg dumps the debug data. You can use it to let tfdbg dump the\n   debug data on a disk with larger free space. For example:\n\n```python\nFor LocalCLIDebugWrapperSession\nsess = tf_debug.LocalCLIDebugWrapperSession(dump_root=\"/with/lots/of/space\")\nFor LocalCLIDebugHook\nhooks = [tf_debug.LocalCLIDebugHook(dump_root=\"/with/lots/of/space\")]\n```\n   Make sure that the directory pointed to by dump_root is empty or nonexistent.`tfdbg` cleans up the dump directories before exiting.\n\nReduce the batch size used during the runs.\nUse the filtering options of tfdbg's `run` command to watch only specific\n   nodes in the graph. For example:\n\n`tfdbg> run --node_name_filter .*hidden.*\n   tfdbg> run --op_type_filter Variable.*\n   tfdbg> run --tensor_dtype_filter int.*`\nThe first command above watches only nodes whose name match the\n   regular-expression pattern `.*hidden.*`. The second command watches only\n   operations whose name match the pattern `Variable.*`. The third one watches\n   only the tensors whose dtype match the pattern `int.*` (e.g., `int32`).\nQ: Why can't I select text in the tfdbg CLI?\nA: This is because the tfdbg CLI enables mouse events in the terminal by\n       default. This mouse-mask mode\n       overrides default terminal interactions, including text selection. You\n       can re-enable text selection by using the command `mouse off` or\n       `m off`.\nQ: Why does the tfdbg CLI show no dumped tensors when I debug code like the following?\n`python\na = tf.ones([10], name=\"a\")\nb = tf.add(a, a, name=\"b\")\nsess = tf.Session()\nsess = tf_debug.LocalCLIDebugWrapperSession(sess)\nsess.run(b)`\nA: The reason why you see no data dumped is because every node in the\n       executed TensorFlow graph is constant-folded by the TensorFlow runtime.\n       In this example, `a` is a constant tensor; therefore, the fetched\n       tensor `b` is effectively also a constant tensor. TensorFlow's graph\n       optimization folds the graph that contains `a` and `b` into a single\n       node to speed up future runs of the graph, which is why `tfdbg` does\n       not generate any intermediate tensor dumps. However, if `a` were a\n       `tf.Variable`, as in the following example:\n``` python\nimport numpy as np\na = tf.Variable(np.ones(10), name=\"a\")\nb = tf.add(a, a, name=\"b\")\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\nsess = tf_debug.LocalCLIDebugWrapperSession(sess)\nsess.run(b)\n```\nthe constant-folding would not occur and `tfdbg` should show the intermediate\ntensor dumps.\nQ: I am debugging a model that generates unwanted infinities or NaNs. But\n       there are some nodes in my model that are known to generate infinities\n       or NaNs in their output tensors even under completely normal conditions.\n       How can I skip those nodes during my `run -f has_inf_or_nan` actions?\nA: Use the `--filter_exclude_node_names` (`-fenn` for short) flag. For\n       example, if you known you have a node with name matching the regular\n       expression `.*Sqrt.*` that generates infinities or NaNs regardless\n       of whether the model is behaving correctly, you can exclude the nodes\n       from the infinity/NaN-finding runs with the command\n       `run -f has_inf_or_nan -fenn .*Sqrt.*`.\nQ: Is there a GUI for tfdbg?\nA: Yes, the TensorBoard Debugger Plugin is the GUI of tfdbg.\n       It offers features such as inspection of the computation graph,\n       real-time visualization of tensor values, continuation to tensor\n       and conditional breakpoints, and tying tensors to their\n       graph-construction source code, all in the browser environment.\n       To get started, please visit",
    "tag": "tensorflow"
  },
  {
    "title": "Creating Custom Estimators",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/guide/custom_estimators.md",
    "content": "Creating Custom Estimators\nThis document introduces custom Estimators. In particular, this document\ndemonstrates how to create a custom `tf.estimator.Estimator` that\nmimics the behavior of the pre-made Estimator\n`tf.estimator.DNNClassifier` in solving the Iris problem. See\nthe Pre-Made Estimators chapter for details\non the Iris problem.\nTo download and access the example code invoke the following two commands:\n`shell\ngit clone https://github.com/tensorflow/models/\ncd models/samples/core/get_started`\nIn this document we will be looking at\ncustom_estimator.py.\nYou can run it with the following command:\n`bsh\npython custom_estimator.py`\nIf you are feeling impatient, feel free to compare and contrast\ncustom_estimator.py\nwith\npremade_estimator.py.\n(which is in the same directory).\nPre-made vs. custom\nAs the following figure shows, pre-made Estimators are subclasses of the\n`tf.estimator.Estimator` base class, while custom Estimators are an instance\nof tf.estimator.Estimator:\n\n\n\n\nPre-made and custom Estimators are all Estimators.\n\nPre-made Estimators are fully baked. Sometimes though, you need more control\nover an Estimator's behavior.  That's where custom Estimators come in. You can\ncreate a custom Estimator to do just about anything. If you want hidden layers\nconnected in some unusual fashion, write a custom Estimator. If you want to\ncalculate a unique\nmetric\nfor your model, write a custom Estimator.  Basically, if you want an Estimator\noptimized for your specific problem, write a custom Estimator.\nA model function (or `model_fn`) implements the ML algorithm. The\nonly difference between working with pre-made Estimators and custom Estimators\nis:\n\nWith pre-made Estimators, someone already wrote the model function for you.\nWith custom Estimators, you must write the model function.\n\nYour model function could implement a wide range of algorithms, defining all\nsorts of hidden layers and metrics.  Like input functions, all model functions\nmust accept a standard group of input parameters and return a standard group of\noutput values. Just as input functions can leverage the Dataset API, model\nfunctions can leverage the Layers API and the Metrics API.\nLet's see how to solve the Iris problem with a custom Estimator. A quick\nreminder--here's the organization of the Iris model that we're trying to mimic:\n\n\n\n\nOur implementation of Iris contains four features, two hidden layers,\nand a logits output layer.\n\nWrite an Input function\nOur custom Estimator implementation uses the same input function as our\npre-made Estimator implementation, from\niris_data.py.\nNamely:\n```python\ndef train_input_fn(features, labels, batch_size):\n    \"\"\"An input function for training\"\"\"\n    # Convert the inputs to a Dataset.\n    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n\n\n```# Shuffle, repeat, and batch the examples.\ndataset = dataset.shuffle(1000).repeat().batch(batch_size)\n\n# Return the dataset.\nreturn dataset\n```\n\n\n```\nThis input function builds an input pipeline that yields batches of\n`(features, labels)` pairs, where `features` is a dictionary features.\nCreate feature columns\nAs detailed in the Premade Estimators and\nFeature Columns chapters, you must define\nyour model's feature columns to specify how the model should use each feature.\nWhether working with pre-made Estimators or custom Estimators, you define\nfeature columns in the same fashion.\nThe following code creates a simple `numeric_column` for each input feature,\nindicating that the value of the input feature should be used directly as an\ninput to the model:\n```python\nFeature columns describe how to use the input.\nmy_feature_columns = []\nfor key in train_x.keys():\n    my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n```\nWrite a model function\nThe model function we'll use has the following call signature:\n`python\ndef my_model_fn(\n   features, # This is batch_features from input_fn\n   labels,   # This is batch_labels from input_fn\n   mode,     # An instance of tf.estimator.ModeKeys\n   params):  # Additional configuration`\nThe first two arguments are the batches of features and labels returned from\nthe input function; that is, `features` and `labels` are the handles to the\ndata your model will use. The `mode` argument indicates whether the caller is\nrequesting training, predicting, or evaluation.\nThe caller may pass `params` to an Estimator's constructor. Any `params` passed\nto the constructor are in turn passed on to the `model_fn`. In\ncustom_estimator.py\nthe following lines create the estimator and set the params to configure the\nmodel. This configuration step is similar to how we configured the `tf.estimator.DNNClassifier` in\nPremade Estimators.\n`python\nclassifier = tf.estimator.Estimator(\n    model_fn=my_model_fn,\n    params={\n        'feature_columns': my_feature_columns,\n        # Two hidden layers of 10 nodes each.\n        'hidden_units': [10, 10],\n        # The model must choose between 3 classes.\n        'n_classes': 3,\n    })`\nTo implement a typical model function, you must do the following:\n\nDefine the model.\nSpecify additional calculations for each of\n  the three different modes:\nPredict\nEvaluate\nTrain\n\n\n\nDefine the model\nThe basic deep neural network model must define the following three sections:\n\nAn input layer\nOne or more hidden layers\nAn output layer\n\nDefine the input layer\nThe first line of the `model_fn` calls `tf.feature_column.input_layer` to\nconvert the feature dictionary and `feature_columns` into input for your model,\nas follows:\n`python\n    # Use `input_layer` to apply the feature columns.\n    net = tf.feature_column.input_layer(features, params['feature_columns'])`\nThe preceding line applies the transformations defined by your feature columns,\ncreating the model's input layer.\n\n\n\nHidden Layers\nIf you are creating a deep neural network, you must define one or more hidden\nlayers. The Layers API provides a rich set of functions to define all types of\nhidden layers, including convolutional, pooling, and dropout layers. For Iris,\nwe're simply going to call `tf.layers.dense` to create hidden layers, with\ndimensions defined by `params['hidden_layers']`. In a `dense` layer each node\nis connected to every node in the preceding layer.  Here's the relevant code:\n`python\n    # Build the hidden layers, sized according to the 'hidden_units' param.\n    for units in params['hidden_units']:\n        net = tf.layers.dense(net, units=units, activation=tf.nn.relu)`\n\nThe `units` parameter defines the number of output neurons in a given layer.\nThe `activation` parameter defines the activation function \u2014\n  Relu in this\n  case.\n\nThe variable `net` here signifies the current top layer of the network. During\nthe first iteration, `net` signifies the input layer. On each loop iteration\n`tf.layers.dense` creates a new layer, which takes the previous layer's output\nas its input, using the variable `net`.\nAfter creating two hidden layers, our network looks as follows. For\nsimplicity, the figure does not show all the units in each layer.\n\n\n\nNote that `tf.layers.dense` provides many additional capabilities, including\nthe ability to set a multitude of regularization parameters. For the sake of\nsimplicity, though, we're going to simply accept the default values of the\nother parameters.\nOutput Layer\nWe'll define the output layer by calling `tf.layers.dense` yet again, this\ntime without an activation function:\n`python\n    # Compute logits (1 per class).\n    logits = tf.layers.dense(net, params['n_classes'], activation=None)`\nHere, `net` signifies the final hidden layer. Therefore, the full set of layers\nis now connected as follows:\n\n\n\n\nThe final hidden layer feeds into the output layer.\n\nWhen defining an output layer, the `units` parameter specifies the number of\noutputs. So, by setting `units` to `params['n_classes']`, the model produces\none output value per class. Each element of the output vector will contain the\nscore, or \"logit\", calculated for the associated class of Iris: Setosa,\nVersicolor, or Virginica, respectively.\nLater on, these logits will be transformed into probabilities by the\n`tf.nn.softmax` function.\nImplement training, evaluation, and prediction {#modes}\nThe final step in creating a model function is to write branching code that\nimplements prediction, evaluation, and training.\nThe model function gets invoked whenever someone calls the Estimator's `train`,\n`evaluate`, or `predict` methods. Recall that the signature for the model\nfunction looks like this:\n`python\ndef my_model_fn(\n   features, # This is batch_features from input_fn\n   labels,   # This is batch_labels from input_fn\n   mode,     # An instance of tf.estimator.ModeKeys, see below\n   params):  # Additional configuration`\nFocus on that third argument, mode. As the following table shows, when someone\ncalls `train`, `evaluate`, or `predict`, the Estimator framework invokes your model\nfunction with the mode parameter set as follows:\n| Estimator method                 |    Estimator Mode |\n|:---------------------------------|:------------------|\n|`tf.estimator.Estimator.train` |`tf.estimator.ModeKeys.TRAIN` |\n|`tf.estimator.Estimator.evaluate`  |`tf.estimator.ModeKeys.EVAL`      |\n|`tf.estimator.Estimator.predict`|`tf.estimator.ModeKeys.PREDICT` |\nFor example, suppose you instantiate a custom Estimator to generate an object\nnamed `classifier`. Then, you make the following call:\n`python\nclassifier = tf.estimator.Estimator(...)\nclassifier.train(input_fn=lambda: my_input_fn(FILE_TRAIN, True, 500))`\nThe Estimator framework then calls your model function with mode set to\n`ModeKeys.TRAIN`.\nYour model function must provide code to handle all three of the mode values.\nFor each mode value, your code must return an instance of\n`tf.estimator.EstimatorSpec`, which contains the information the caller\nrequires. Let's examine each mode.\nPredict\nWhen the Estimator's `predict` method is called, the `model_fn` receives\n`mode = ModeKeys.PREDICT`. In this case, the model function must return a\n`tf.estimator.EstimatorSpec` containing the prediction.\nThe model must have been trained prior to making a prediction. The trained model\nis stored on disk in the `model_dir` directory established when you\ninstantiated the Estimator.\nThe code to generate the prediction for this model looks as follows:\n```python\nCompute predictions.\npredicted_classes = tf.argmax(logits, 1)\nif mode == tf.estimator.ModeKeys.PREDICT:\n    predictions = {\n        'class_ids': predicted_classes[:, tf.newaxis],\n        'probabilities': tf.nn.softmax(logits),\n        'logits': logits,\n    }\n    return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n```\nThe prediction dictionary contains everything that your model returns when run\nin prediction mode.\n\n\n\nThe `predictions` holds the following three key/value pairs:\n\n`class_ids` holds the class id (0, 1, or 2) representing the model's\n    prediction of the most likely species for this example.\n`probabilities` holds the three probabilities (in this example, 0.02, 0.95,\n    and 0.03)\n`logit` holds the raw logit values (in this example, -1.3, 2.6, and -0.9)\n\nWe return that dictionary to the caller via the `predictions` parameter of the\n`tf.estimator.EstimatorSpec`. The Estimator's\n`tf.estimator.Estimator.predict` method will yield these\ndictionaries.\nCalculate the loss\nFor both training and evaluation we need to calculate the\nmodel's loss. This is the\nobjective\nthat will be optimized.\nWe can calculate the loss by calling `tf.losses.sparse_softmax_cross_entropy`.\nThe value returned by this function will be approximately 0 at lowest,\nwhen the probability of the correct class (at index `label`) is near 1.0.\nThe loss value returned is progressively larger as the probability of the\ncorrect class decreases.\nThis function returns the average over the whole batch.\n```python\nCompute loss.\nloss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n```\nEvaluate\nWhen the Estimator's `evaluate` method is called, the `model_fn` receives\n`mode = ModeKeys.EVAL`. In this case, the model function must return a\n`tf.estimator.EstimatorSpec` containing the model's loss and optionally one\nor more metrics.\nAlthough returning metrics is optional, most custom Estimators do return at\nleast one metric. TensorFlow provides a Metrics module `tf.metrics` to\ncalculate common metrics.  For brevity's sake, we'll only return accuracy. The\n`tf.metrics.accuracy` function compares our predictions against the\ntrue values, that is, against the labels provided by the input function. The\n`tf.metrics.accuracy` function requires the labels and predictions to have the\nsame shape. Here's the call to `tf.metrics.accuracy`:\n``` python\nCompute evaluation metrics.\naccuracy = tf.metrics.accuracy(labels=labels,\n                               predictions=predicted_classes,\n                               name='acc_op')\n```\nThe `tf.estimator.EstimatorSpec` returned for evaluation\ntypically contains the following information:\n\n`loss`, which is the model's loss\n`eval_metric_ops`, which is an optional dictionary of metrics.\n\nSo, we'll create a dictionary containing our sole metric. If we had calculated\nother metrics, we would have added them as additional key/value pairs to that\nsame dictionary.  Then, we'll pass that dictionary in the `eval_metric_ops`\nargument of `tf.estimator.EstimatorSpec`. Here's the code:\n```python\nmetrics = {'accuracy': accuracy}\ntf.summary.scalar('accuracy', accuracy[1])\nif mode == tf.estimator.ModeKeys.EVAL:\n    return tf.estimator.EstimatorSpec(\n        mode, loss=loss, eval_metric_ops=metrics)\n```\nThe `tf.summary.scalar` will make accuracy available to TensorBoard\nin both `TRAIN` and `EVAL` modes. (More on this later).\nTrain\nWhen the Estimator's `train` method is called, the `model_fn` is called\nwith `mode = ModeKeys.TRAIN`. In this case, the model function must return an\n`EstimatorSpec` that contains the loss and a training operation.\nBuilding the training operation will require an optimizer. We will use\n`tf.train.AdagradOptimizer` because we're mimicking the `DNNClassifier`, which\nalso uses `Adagrad` by default. The `tf.train` package provides many other\noptimizers\u2014feel free to experiment with them.\nHere is the code that builds the optimizer:\n`python\noptimizer = tf.train.AdagradOptimizer(learning_rate=0.1)`\nNext, we build the training operation using the optimizer's\n`tf.train.Optimizer.minimize` method on the loss we calculated\nearlier.\nThe `minimize` method also takes a `global_step` parameter. TensorFlow uses this\nparameter to count the number of training steps that have been processed\n(to know when to end a training run). Furthermore, the `global_step` is\nessential for TensorBoard graphs to work correctly. Simply call\n`tf.train.get_global_step` and pass the result to the `global_step`\nargument of `minimize`.\nHere's the code to train the model:\n`python\ntrain_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())`\nThe `tf.estimator.EstimatorSpec` returned for training\nmust have the following fields set:\n\n`loss`, which contains the value of the loss function.\n`train_op`, which executes a training step.\n\nHere's our code to call `EstimatorSpec`:\n`python\nreturn tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)`\nThe model function is now complete.\nThe custom Estimator\nInstantiate the custom Estimator through the Estimator base class as follows:\n`python\n    # Build 2 hidden layer DNN with 10, 10 units respectively.\n    classifier = tf.estimator.Estimator(\n        model_fn=my_model_fn,\n        params={\n            'feature_columns': my_feature_columns,\n            # Two hidden layers of 10 nodes each.\n            'hidden_units': [10, 10],\n            # The model must choose between 3 classes.\n            'n_classes': 3,\n        })`\nHere the `params` dictionary serves the same purpose as the key-word\narguments of `DNNClassifier`; that is, the `params` dictionary lets you\nconfigure your Estimator without modifying the code in the `model_fn`.\nThe rest of the code to train, evaluate, and generate predictions using our\nEstimator is the same as in the\nPremade Estimators chapter. For\nexample, the following line will train the model:\n```python\nTrain the Model.\nclassifier.train(\n    input_fn=lambda:iris_data.train_input_fn(train_x, train_y, args.batch_size),\n    steps=args.train_steps)\n```\nTensorBoard\nYou can view training results for your custom Estimator in TensorBoard. To see\nthis reporting, start TensorBoard from your command line as follows:\n```bsh\nReplace PATH with the actual path passed as model_dir\ntensorboard --logdir=PATH\n```\nThen, open TensorBoard by browsing to: http://localhost:6006\nAll the pre-made Estimators automatically log a lot of information to\nTensorBoard. With custom Estimators, however, TensorBoard only provides one\ndefault log (a graph of the loss) plus the information you explicitly tell\nTensorBoard to log. For the custom Estimator you just created, TensorBoard\ngenerates the following:\n\n\n\n\n\n\nTensorBoard displays three graphs.\n\nIn brief, here's what the three graphs tell you:\n\n\nglobal_step/sec: A performance indicator showing how many batches (gradient\n  updates) we processed per second as the model trains.\n\n\nloss: The loss reported.\n\n\naccuracy: The accuracy is recorded by the following two lines:\n\n`eval_metric_ops={'my_accuracy': accuracy}`, during evaluation.\n`tf.summary.scalar('accuracy', accuracy[1])`, during training.\n\n\n\nThese tensorboard graphs are one of the main reasons it's important to pass a\n`global_step` to your optimizer's `minimize` method. The model can't record\nthe x-coordinate for these graphs without it.\nNote the following in the `my_accuracy` and `loss` graphs:\n\nThe orange line represents training.\nThe blue dot represents evaluation.\n\nDuring training, summaries (the orange line) are recorded periodically as\nbatches are processed, which is why it becomes a graph spanning x-axis range.\nBy contrast, evaluation produces only a single point on the graph for each call\nto `evaluate`. This point contains the average over the entire evaluation call.\nThis has no width on the graph as it is evaluated entirely from the model state\nat a particular training step (from a single checkpoint).\nAs suggested in the following figure, you may see and also selectively\ndisable/enable the reporting using the controls on the left side.\n\n\n\n\nEnable or disable reporting.\n\nSummary\nAlthough pre-made Estimators can be an effective way to quickly create new\nmodels, you will often need the additional flexibility that custom Estimators\nprovide. Fortunately, pre-made and custom Estimators follow the same\nprogramming model. The only practical difference is that you must write a model\nfunction for custom Estimators; everything else is the same.\nFor more details, be sure to check out:\n\nThe\n  official TensorFlow implementation of MNIST,\n  which uses a custom estimator.\nThe TensorFlow\n  official models repository,\n  which contains more curated examples using custom estimators.\nThis TensorBoard video, which introduces\n  TensorBoard.\nThe Low Level Introduction, which demonstrates\n  how to experiment directly with TensorFlow's low level APIs, making debugging\n",
    "tag": "tensorflow"
  },
  {
    "title": "TensorBoard: Graph Visualization",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/guide/graph_viz.md",
    "content": "TensorBoard: Graph Visualization\nTensorFlow computation graphs are powerful but complicated. The graph visualization can help you understand and debug them. Here's an example of the visualization at work.\n\nVisualization of a TensorFlow graph.\nTo see your own graph, run TensorBoard pointing it to the log directory of the job, click on the graph tab on the top pane and select the appropriate run using the menu at the upper left corner. For in depth information on how to run TensorBoard and make sure you are logging all the necessary information, see TensorBoard: Visualizing Learning.\nName scoping and nodes\nTypical TensorFlow graphs can have many thousands of nodes--far too many to see\neasily all at once, or even to lay out using standard graph tools. To simplify,\nvariable names can be scoped and the visualization uses this information to\ndefine a hierarchy on the nodes in the graph.  By default, only the top of this\nhierarchy is shown. Here is an example that defines three operations under the\n`hidden` name scope using\n`tf.name_scope`:\n```python\nimport tensorflow as tf\nwith tf.name_scope('hidden') as scope:\n  a = tf.constant(5, name='alpha')\n  W = tf.Variable(tf.random_uniform([1, 2], -1.0, 1.0), name='weights')\n  b = tf.Variable(tf.zeros([1]), name='biases')\n```\nThis results in the following three op names:\n\n`hidden/alpha`\n`hidden/weights`\n`hidden/biases`\n\nBy default, the visualization will collapse all three into a node labeled `hidden`.\nThe extra detail isn't lost. You can double-click, or click\non the orange `+` sign in the top right to expand the node, and then you'll see\nthree subnodes for `alpha`, `weights` and `biases`.\nHere's a real-life example of a more complicated node in its initial and\nexpanded states.\n\n\n\n\n\n\n\n\n\n\n\n      Initial view of top-level name scope `pool_1`. Clicking on the orange `+` button on the top right or double-clicking on the node itself will expand it.\n    \n\n      Expanded view of `pool_1` name scope. Clicking on the orange `-` button on the top right or double-clicking on the node itself will collapse the name scope.\n    \n\n\nGrouping nodes by name scopes is critical to making a legible graph. If you're\nbuilding a model, name scopes give you control over the resulting visualization.\nThe better your name scopes, the better your visualization.\nThe figure above illustrates a second aspect of the visualization. TensorFlow\ngraphs have two kinds of connections: data dependencies and control\ndependencies. Data dependencies show the flow of tensors between two ops and\nare shown as solid arrows, while control dependencies use dotted lines. In the\nexpanded view (right side of the figure above) all the connections are data\ndependencies with the exception of the dotted line connecting `CheckNumerics`\nand `control_dependency`.\nThere's a second trick to simplifying the layout. Most TensorFlow graphs have a\nfew nodes with many connections to other nodes. For example, many nodes might\nhave a control dependency on an initialization step. Drawing all edges between\nthe `init` node and its dependencies would create a very cluttered view.\nTo reduce clutter, the visualization separates out all high-degree nodes to an\nauxiliary area on the right and doesn't draw lines to represent their edges.\nInstead of lines, we draw small node icons to indicate the connections.\nSeparating out the auxiliary nodes typically doesn't remove critical\ninformation since these nodes are usually related to bookkeeping functions.\nSee Interaction for how to move nodes between the main graph\nand the auxiliary area.\n\n\n\n\n\n\n\n\n\n\n\n      Node `conv_1` is connected to `save`. Note the little `save` node icon on its right.\n    \n\n`save` has a high degree, and will appear as an auxiliary node. The connection with `conv_1` is shown as a node icon on its left. To further reduce clutter, since `save` has a lot of connections, we show the first 5 and abbreviate the others as `... 12 more`.\n    \n\n\nOne last structural simplification is series collapsing. Sequential\nmotifs--that is, nodes whose names differ by a number at the end and have\nisomorphic structures--are collapsed into a single stack of nodes, as shown\nbelow. For networks with long sequences, this greatly simplifies the view. As\nwith hierarchical nodes, double-clicking expands the series. See\nInteraction for how to disable/enable series collapsing for a\nspecific set of nodes.\n\n\n\n\n\n\n\n\n\n\n\n      A collapsed view of a node sequence.\n    \n\n      A small piece of the expanded view, after double-click.\n    \n\n\nFinally, as one last aid to legibility, the visualization uses special icons\nfor constants and summary nodes. To summarize, here's a table of node symbols:\nSymbol | Meaning\n--- | ---\n | High-level node representing a name scope. Double-click to expand a high-level node.\n | Sequence of numbered nodes that are not connected to each other.\n | Sequence of numbered nodes that are connected to each other.\n | An individual operation node.\n | A constant.\n | A summary node.\n | Edge showing the data flow between operations.\n | Edge showing the control dependency between operations.\n | A reference edge showing that the outgoing operation node can mutate the incoming tensor.\nInteraction {#interaction}\nNavigate the graph by panning and zooming. Click and drag to pan, and use a\nscroll gesture to zoom. Double-click on a node, or click on its `+` button, to\nexpand a name scope that represents a group of operations. To easily keep\ntrack of the current viewpoint when zooming and panning, there is a minimap in\nthe bottom right corner.\nTo close an open node, double-click it again or click its `-` button. You can\nalso click once to select a node. It will turn a darker color, and details\nabout it and the nodes it connects to will appear in the info card at upper\nright corner of the visualization.\n\n\n\n\n\n\n\n\n\n\n\n      Info card showing detailed information for the `conv2` name scope. The inputs and outputs are combined from the inputs and outputs of the operation nodes inside the name scope. For name scopes no attributes are shown.\n    \n\n      Info card showing detailed information for the `DecodeRaw` operation node. In addition to inputs and outputs, the card shows the device and the attributes associated with the current operation.\n    \n\n\nTensorBoard provides several ways to change the visual layout of the graph. This\ndoesn't change the graph's computational semantics, but it can bring some\nclarity to the network's structure. By right clicking on a node or pressing\nbuttons on the bottom of that node's info card, you can make the following\nchanges to its layout:\n\nNodes can be moved between the main graph and the auxiliary area.\nA series of nodes can be ungrouped so that the nodes in the series do not\nappear grouped together. Ungrouped series can likewise be regrouped.\n\nSelection can also be helpful in understanding high-degree nodes. Select any\nhigh-degree node, and the corresponding node icons for its other connections\nwill be selected as well. This makes it easy, for example, to see which nodes\nare being saved--and which aren't.\nClicking on a node name in the info card will select it. If necessary, the\nviewpoint will automatically pan so that the node is visible.\nFinally, you can choose two color schemes for your graph, using the color menu\nabove the legend. The default Structure View shows structure: when two\nhigh-level nodes have the same structure, they appear in the same color of the\nrainbow. Uniquely structured nodes are gray. There's a second view, which shows\nwhat device the different operations run on. Name scopes are colored\nproportionally to the fraction of devices for the operations inside them.\nThe images below give an illustration for a piece of a real-life graph.\n\n\n\n\n\n\n\n\n\n\n\n      Structure view: The gray nodes have unique structure. The orange `conv1` and `conv2` nodes have the same structure, and analogously for nodes with other colors.\n    \n\n      Device view: Name scopes are colored proportionally to the fraction of devices of the operation nodes inside them. Here, purple means GPU and the green is CPU.\n    \n\n\nTensor shape information\nWhen the serialized `GraphDef` includes tensor shapes, the graph visualizer\nlabels edges with tensor dimensions, and edge thickness reflects total tensor\nsize. To include tensor shapes in the `GraphDef` pass the actual graph object\n(as in `sess.graph`) to the `FileWriter` when serializing the graph.\nThe images below show the CIFAR-10 model with tensor shape information:\n\n\n\n\n\n\n\n\n      CIFAR-10 model with tensor shape information.\n    \n\n\nRuntime statistics\nOften it is useful to collect runtime metadata for a run, such as total memory\nusage, total compute time, and tensor shapes for nodes. The code example below\nis a snippet from the train and test section of a modification of the\nEstimators MNIST tutorial, in which we have\nrecorded summaries and\nruntime statistics. See the\nTensorboard\nfor details on how to record summaries.\nFull source is here.\n```python\n  # Train the model, and also write summaries.\n  # Every 10th step, measure test-set accuracy, and write test summaries\n  # All other steps, run train_step on training data, & add training summaries\ndef feed_dict(train):\n    \"\"\"Make a TensorFlow feed_dict: maps data onto Tensor placeholders.\"\"\"\n    if train or FLAGS.fake_data:\n      xs, ys = mnist.train.next_batch(100, fake_data=FLAGS.fake_data)\n      k = FLAGS.dropout\n    else:\n      xs, ys = mnist.test.images, mnist.test.labels\n      k = 1.0\n    return {x: xs, y_: ys, keep_prob: k}\nfor i in range(FLAGS.max_steps):\n    if i % 10 == 0:  # Record summaries and test-set accuracy\n      summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))\n      test_writer.add_summary(summary, i)\n      print('Accuracy at step %s: %s' % (i, acc))\n    else:  # Record train set summaries, and train\n      if i % 100 == 99:  # Record execution stats\n        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n        run_metadata = tf.RunMetadata()\n        summary, _ = sess.run([merged, train_step],\n                              feed_dict=feed_dict(True),\n                              options=run_options,\n                              run_metadata=run_metadata)\n        train_writer.add_run_metadata(run_metadata, 'step%d' % i)\n        train_writer.add_summary(summary, i)\n        print('Adding run metadata for', i)\n      else:  # Record a summary\n        summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))\n        train_writer.add_summary(summary, i)\n```\nThis code will emit runtime statistics for every 100th step starting at step99.\nWhen you launch tensorboard and go to the Graph tab, you will now see options\nunder \"Session runs\" which correspond to the steps where run metadata was added.\nSelecting one of these runs will show you the snapshot of the network at that\nstep, fading out unused nodes. In the controls on the left hand side, you will\nbe able to color the nodes by total memory or total compute time. Additionally,\nclicking on a node will display the exact total memory, compute time, and\ntensor output sizes.\n\n\n\n\n\n\n\n\n\n\n\n",
    "tag": "tensorflow"
  },
  {
    "title": "Estimators",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/guide/estimators.md",
    "content": "Estimators\nThis document introduces `tf.estimator`\u2014a high-level TensorFlow\nAPI that greatly simplifies machine learning programming. Estimators encapsulate\nthe following actions:\n\ntraining\nevaluation\nprediction\nexport for serving\n\nYou may either use the pre-made Estimators we provide or write your\nown custom Estimators.  All Estimators\u2014whether pre-made or custom\u2014are\nclasses based on the `tf.estimator.Estimator` class.\nFor a quick example try Estimator tutorials.\nTo see each sub-topic in depth, see the Estimator guides.\nFor an overview of the API design, see our white paper here.\nNote: TensorFlow also includes a deprecated `Estimator` class at\n`tf.contrib.learn.Estimator`, which you should not use.\nAdvantages of Estimators\nEstimators provide the following benefits:\n\nYou can run Estimator-based models on a local host or on a\n    distributed multi-server environment without changing your model.\n    Furthermore, you can run Estimator-based models on CPUs, GPUs,\n    or TPUs without recoding your model.\nEstimators simplify sharing implementations between model developers.\nYou can develop a state of the art model with high-level intuitive code.\n    In short, it is generally much easier to create models with Estimators\n    than with the low-level TensorFlow APIs.\nEstimators are themselves built on `tf.keras.layers`, which\n    simplifies customization.\nEstimators build the graph for you.\n\nEstimators provide a safe distributed training loop that controls how and\n    when to:\n\nbuild the graph\ninitialize variables\nload data\nhandle exceptions\ncreate checkpoint files and recover from failures\nsave summaries for TensorBoard\n\n\n\nWhen writing an application with Estimators, you must separate the data input\npipeline from the model.  This separation simplifies experiments with\ndifferent data sets.\nPre-made Estimators\nPre-made Estimators enable you to work at a much higher conceptual level\nthan the base TensorFlow APIs. You no longer have to worry about creating\nthe computational graph or sessions since Estimators handle all\nthe \"plumbing\" for you.  That is, pre-made Estimators create and manage\n`tf.Graph` and `tf.Session` objects for you.  Furthermore,\npre-made Estimators let you experiment with different model architectures by\nmaking only minimal code changes.  `tf.estimator.DNNClassifier`,\nfor example, is a pre-made Estimator class that trains classification models\nbased on dense, feed-forward neural networks.\nStructure of a pre-made Estimators program\nA TensorFlow program relying on a pre-made Estimator typically consists\nof the following four steps:\n\n\nWrite one or more dataset importing functions. For example, you might\n    create one function to import the training set and another function to\n    import the test set. Each dataset importing function must return two\n    objects:\n\na dictionary in which the keys are feature names and the\n    values are Tensors (or SparseTensors) containing the corresponding\n    feature data\na Tensor containing one or more labels\n\nFor example, the following code illustrates the basic skeleton for\nan input function:\n\n\n```def input_fn(dataset):\n   ...  # manipulate dataset, extracting the feature dict and the label\n   return feature_dict, label\n```\n\n\n(See Importing Data for full details.)\n\n\nDefine the feature columns. Each `tf.feature_column`\n    identifies a feature name, its type, and any input pre-processing.\n    For example, the following snippet creates three feature\n    columns that hold integer or floating-point data.  The first two\n    feature columns simply identify the feature's name and type. The\n    third feature column also specifies a lambda the program will invoke\n    to scale the raw data:\n\n\n```# Define three numeric feature columns.\npopulation = tf.feature_column.numeric_column('population')\ncrime_rate = tf.feature_column.numeric_column('crime_rate')\nmedian_education = tf.feature_column.numeric_column('median_education',\n                    normalizer_fn=lambda x: x - global_education_mean)\n```\n\n\n\n\nInstantiate the relevant pre-made Estimator.  For example, here's\n    a sample instantiation of a pre-made Estimator named `LinearClassifier`:\n\n\n```# Instantiate an estimator, passing the feature columns.\nestimator = tf.estimator.LinearClassifier(\n    feature_columns=[population, crime_rate, median_education])\n```\n\n\n\n\nCall a training, evaluation, or inference method.\n    For example, all Estimators provide a `train` method, which trains a model.\n\n\n```# `input_fn` is the function created in Step 1\nestimator.train(input_fn=my_training_set, steps=2000)\n```\n\n\n\n\nBenefits of pre-made Estimators\nPre-made Estimators encode best practices, providing the following benefits:\n\nBest practices for determining where different parts of the computational\n    graph should run, implementing strategies on a single machine or on a\n    cluster.\nBest practices for event (summary) writing and universally useful\n    summaries.\n\nIf you don't use pre-made Estimators, you must implement the preceding\nfeatures yourself.\nCustom Estimators\nThe heart of every Estimator\u2014whether pre-made or custom\u2014is its\nmodel function, which is a method that builds graphs for training,\nevaluation, and prediction. When you are using a pre-made Estimator,\nsomeone else has already implemented the model function. When relying\non a custom Estimator, you must write the model function yourself. A\ncompanion document\nexplains how to write the model function.\nRecommended workflow\nWe recommend the following workflow:\n\nAssuming a suitable pre-made Estimator exists, use it to build your\n    first model and use its results to establish a baseline.\nBuild and test your overall pipeline, including the integrity and\n    reliability of your data with this pre-made Estimator.\nIf suitable alternative pre-made Estimators are available, run\n    experiments to determine which pre-made Estimator produces the\n    best results.\nPossibly, further improve your model by building your own custom Estimator.\n\nCreating Estimators from Keras models\nYou can convert existing Keras models to Estimators. Doing so enables your Keras\nmodel to access Estimator's strengths, such as distributed training. Call\n`tf.keras.estimator.model_to_estimator` as in the\nfollowing sample:\n```python\nInstantiate a Keras inception v3 model.\nkeras_inception_v3 = tf.keras.applications.inception_v3.InceptionV3(weights=None)\nCompile model with the optimizer, loss, and metrics you'd like to train with.\nkeras_inception_v3.compile(optimizer=tf.keras.optimizers.SGD(lr=0.0001, momentum=0.9),\n                          loss='categorical_crossentropy',\n                          metric='accuracy')\nCreate an Estimator from the compiled Keras model. Note the initial model\nstate of the keras model is preserved in the created Estimator.\nest_inception_v3 = tf.keras.estimator.model_to_estimator(keras_model=keras_inception_v3)\nTreat the derived Estimator as you would with any other Estimator.\nFirst, recover the input name(s) of Keras model, so we can use them as the\nfeature column name(s) of the Estimator input function:\nkeras_inception_v3.input_names  # print out: ['input_1']\nOnce we have the input name(s), we can create the input function, for example,\nfor input(s) in the format of numpy ndarray:\ntrain_input_fn = tf.estimator.inputs.numpy_input_fn(\n    x={\"input_1\": train_data},\n    y=train_labels,\n    num_epochs=1,\n    shuffle=False)\nTo train, we call Estimator's train function:\nest_inception_v3.train(input_fn=train_input_fn, steps=2000)\n```\nNote that the names of feature columns and labels of a keras estimator come from\nthe corresponding compiled keras model. For example, the input key names for\n`train_input_fn` above can be obtained from `keras_inception_v3.input_names`,\nand similarly, the predicted output names can be obtained from\n`keras_inception_v3.output_names`.\nFor more details, please refer to the documentation for",
    "tag": "tensorflow"
  },
  {
    "title": "Save and Restore",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/guide/saved_model.md",
    "content": "Save and Restore\nThe `tf.train.Saver` class provides methods to save and restore models. The\n`tf.saved_model.simple_save` function is an easy way to build a\n`tf.saved_model` suitable for serving. Estimators\nautomatically save and restore variables in the `model_dir`.\nSave and restore variables\nTensorFlow Variables are the best way to represent shared, persistent state\nmanipulated by your program. The `tf.train.Saver` constructor adds `save` and\n`restore` ops to the graph for all, or a specified list, of the variables in the\ngraph.  The `Saver` object provides methods to run these ops, specifying paths\nfor the checkpoint files to write to or read from.\n`Saver` restores all variables already defined in your model. If you're\nloading a model without knowing how to build its graph (for example, if you're\nwriting a generic program to load models), then read the\nOverview of saving and restoring models section\nlater in this document.\nTensorFlow saves variables in binary checkpoint files that map variable\nnames to tensor values.\nCaution: TensorFlow model files are code. Be careful with untrusted code.\nSee Using TensorFlow Securely\nfor details.\nSave variables\nCreate a `Saver` with `tf.train.Saver()` to manage all variables in the\nmodel. For example, the following snippet demonstrates how to call the\n`tf.train.Saver.save` method to save variables to checkpoint files:\n```python\nCreate some variables.\nv1 = tf.get_variable(\"v1\", shape=[3], initializer = tf.zeros_initializer)\nv2 = tf.get_variable(\"v2\", shape=[5], initializer = tf.zeros_initializer)\ninc_v1 = v1.assign(v1+1)\ndec_v2 = v2.assign(v2-1)\nAdd an op to initialize the variables.\ninit_op = tf.global_variables_initializer()\nAdd ops to save and restore all the variables.\nsaver = tf.train.Saver()\nLater, launch the model, initialize the variables, do some work, and save the\nvariables to disk.\nwith tf.Session() as sess:\n  sess.run(init_op)\n  # Do some work with the model.\n  inc_v1.op.run()\n  dec_v2.op.run()\n  # Save the variables to disk.\n  save_path = saver.save(sess, \"/tmp/model.ckpt\")\n  print(\"Model saved in path: %s\" % save_path)\n```\nRestore variables\nThe `tf.train.Saver` object not only saves variables to checkpoint files, it\nalso restores variables. Note that when you restore variables you do not have\nto initialize them beforehand. For example, the following snippet demonstrates\nhow to call the `tf.train.Saver.restore` method to restore variables from the\ncheckpoint files:\n```python\ntf.reset_default_graph()\nCreate some variables.\nv1 = tf.get_variable(\"v1\", shape=[3])\nv2 = tf.get_variable(\"v2\", shape=[5])\nAdd ops to save and restore all the variables.\nsaver = tf.train.Saver()\nLater, launch the model, use the saver to restore variables from disk, and\ndo some work with the model.\nwith tf.Session() as sess:\n  # Restore variables from disk.\n  saver.restore(sess, \"/tmp/model.ckpt\")\n  print(\"Model restored.\")\n  # Check the values of the variables\n  print(\"v1 : %s\" % v1.eval())\n  print(\"v2 : %s\" % v2.eval())\n```\nNote: There is not a physical file called `/tmp/model.ckpt`. It is the prefix of\nfilenames created for the checkpoint. Users only interact with the prefix\ninstead of physical checkpoint files.\nChoose variables to save and restore\nIf you do not pass any arguments to `tf.train.Saver()`, the saver handles all\nvariables in the graph.  Each variable is saved under the name that was passed\nwhen the variable was created.\nIt is sometimes useful to explicitly specify names for variables in the\ncheckpoint files.  For example, you may have trained a model with a variable\nnamed `\"weights\"` whose value you want to restore into a variable named\n`\"params\"`.\nIt is also sometimes useful to only save or restore a subset of the variables\nused by a model.  For example, you may have trained a neural net with five\nlayers, and you now want to train a new model with six layers that reuses the\nexisting weights of the five trained layers. You can use the saver to restore\nthe weights of just the first five layers.\nYou can easily specify the names and variables to save or load by passing to the\n`tf.train.Saver()` constructor either of the following:\n\nA list of variables (which will be stored under their own names).\nA Python dictionary in which keys are the names to use and the values are the\nvariables to manage.\n\nContinuing from the save/restore examples shown earlier:\n```python\ntf.reset_default_graph()\nCreate some variables.\nv1 = tf.get_variable(\"v1\", [3], initializer = tf.zeros_initializer)\nv2 = tf.get_variable(\"v2\", [5], initializer = tf.zeros_initializer)\nAdd ops to save and restore only `v2` using the name \"v2\"\nsaver = tf.train.Saver({\"v2\": v2})\nUse the saver object normally after that.\nwith tf.Session() as sess:\n  # Initialize v1 since the saver will not.\n  v1.initializer.run()\n  saver.restore(sess, \"/tmp/model.ckpt\")\nprint(\"v1 : %s\" % v1.eval())\n  print(\"v2 : %s\" % v2.eval())\n```\nNotes:\n\n\nYou can create as many `Saver` objects as you want if you need to save and\n   restore different subsets of the model variables.  The same variable can be\n   listed in multiple saver objects; its value is only changed when the\n   `Saver.restore()` method is run.\n\n\nIf you only restore a subset of the model variables at the start of a\n   session, you have to run an initialize op for the other variables.  See\n   `tf.variables_initializer` for more information.\n\n\nTo inspect the variables in a checkpoint, you can use the\n   inspect_checkpoint\n   library, particularly the `print_tensors_in_checkpoint_file` function.\n\n\nBy default, `Saver` uses the value of the `tf.Variable.name` property\n   for each variable.  However, when you create a `Saver` object, you may\n   optionally choose names for the variables in the checkpoint files.\n\n\nInspect variables in a checkpoint\nWe can quickly inspect variables in a checkpoint with the\ninspect_checkpoint library.\nContinuing from the save/restore examples shown earlier:\n```python\nimport the inspect_checkpoint library\nfrom tensorflow.python.tools import inspect_checkpoint as chkp\nprint all tensors in checkpoint file\nchkp.print_tensors_in_checkpoint_file(\"/tmp/model.ckpt\", tensor_name='', all_tensors=True)\ntensor_name:  v1\n[ 1.  1.  1.]\ntensor_name:  v2\n[-1. -1. -1. -1. -1.]\nprint only tensor v1 in checkpoint file\nchkp.print_tensors_in_checkpoint_file(\"/tmp/model.ckpt\", tensor_name='v1', all_tensors=False)\ntensor_name:  v1\n[ 1.  1.  1.]\nprint only tensor v2 in checkpoint file\nchkp.print_tensors_in_checkpoint_file(\"/tmp/model.ckpt\", tensor_name='v2', all_tensors=False)\ntensor_name:  v2\n[-1. -1. -1. -1. -1.]\n```\n\nSave and restore models\nUse `SavedModel` to save and load your model\u2014variables, the graph, and the\ngraph's metadata. This is a language-neutral, recoverable, hermetic\nserialization format that enables higher-level systems and tools to produce,\nconsume, and transform TensorFlow models. TensorFlow provides several ways to\ninteract with `SavedModel`, including the `tf.saved_model` APIs,\n`tf.estimator.Estimator`, and a command-line interface.\nBuild and load a SavedModel\nSimple save\nThe easiest way to create a `SavedModel` is to use the `tf.saved_model.simple_save`\nfunction:\n`python\nsimple_save(session,\n            export_dir,\n            inputs={\"x\": x, \"y\": y},\n            outputs={\"z\": z})`\nThis configures the `SavedModel` so it can be loaded by\nTensorFlow serving and supports the\nPredict API.\nTo access the classify, regress, or multi-inference APIs, use the manual\n`SavedModel` builder APIs or an `tf.estimator.Estimator`.\nManually build a SavedModel\nIf your use case isn't covered by `tf.saved_model.simple_save`, use the manual\n`tf.saved_model.builder` to create a `SavedModel`.\nThe `tf.saved_model.builder.SavedModelBuilder` class provides functionality to\nsave multiple `MetaGraphDef`s.  A MetaGraph is a dataflow graph, plus\nits associated variables, assets, and signatures.  A `MetaGraphDef`\nis the protocol buffer representation of a MetaGraph.  A signature is\nthe set of inputs to and outputs from a graph.\nIf assets need to be saved and written or copied to disk, they can be provided\nwhen the first `MetaGraphDef` is added. If multiple `MetaGraphDef`s are\nassociated with an asset of the same name, only the first version is retained.\nEach `MetaGraphDef` added to the SavedModel must be annotated with\nuser-specified tags. The tags provide a means to identify the specific\n`MetaGraphDef` to load and restore, along with the shared set of variables\nand assets. These tags\ntypically annotate a `MetaGraphDef` with its functionality (for example,\nserving or training), and optionally with hardware-specific aspects (for\nexample, GPU).\nFor example, the following code suggests a typical way to use\n`SavedModelBuilder` to build a SavedModel:\n```python\nexport_dir = ...\n...\nbuilder = tf.saved_model.builder.SavedModelBuilder(export_dir)\nwith tf.Session(graph=tf.Graph()) as sess:\n  ...\n  builder.add_meta_graph_and_variables(sess,\n                                       [tag_constants.TRAINING],\n                                       signature_def_map=foo_signatures,\n                                       assets_collection=foo_assets,\n                                       strip_default_attrs=True)\n...\nAdd a second MetaGraphDef for inference.\nwith tf.Session(graph=tf.Graph()) as sess:\n  ...\n  builder.add_meta_graph([tag_constants.SERVING], strip_default_attrs=True)\n...\nbuilder.save()\n```\n\nForward compatibility via `strip_default_attrs=True`\nFollowing the guidance below gives you forward compatibility only if the set of\nOps has not changed.\nThe `tf.saved_model.builder.SavedModelBuilder` class allows\nusers to control whether default-valued attributes must be stripped from the\nNodeDefs\nwhile adding a meta graph to the SavedModel bundle. Both\n`tf.saved_model.builder.SavedModelBuilder.add_meta_graph_and_variables`\nand `tf.saved_model.builder.SavedModelBuilder.add_meta_graph`\nmethods accept a Boolean flag `strip_default_attrs` that controls this behavior.\nIf `strip_default_attrs` is `False`, the exported `tf.MetaGraphDef` will have\nthe default valued attributes in all its `tf.NodeDef` instances.\nThis can break forward compatibility with a sequence of events such as the\nfollowing:\n\nAn existing Op (`Foo`) is updated to include a new attribute (`T`) with a\n   default (`bool`) at version 101.\nA model producer such as a \"trainer binary\" picks up this change (version 101)\n   to the `OpDef` and re-exports an existing model that uses Op `Foo`.\nA model consumer (such as Tensorflow Serving) running an older\n   binary (version 100) doesn't have attribute `T` for Op `Foo`, but tries to\n   import this model. The model consumer doesn't recognize attribute `T` in a\n   `NodeDef` that uses Op `Foo` and therefore fails to load the model.\nBy setting `strip_default_attrs` to True, the model producers can strip away\n   any default valued attributes in the `NodeDefs`. This helps ensure that newly\n   added attributes with defaults don't cause older model consumers to fail\n   loading models regenerated with newer training binaries.\n\nSee compatibility guidance\nfor more information.\nLoading a SavedModel in Python\nThe Python version of the SavedModel\n`tf.saved_model.loader`\nprovides load and restore capability for a SavedModel. The `load` operation\nrequires the following information:\n\nThe session in which to restore the graph definition and variables.\nThe tags used to identify the MetaGraphDef to load.\nThe location (directory) of the SavedModel.\n\nUpon a load, the subset of variables, assets, and signatures supplied as part of\nthe specific MetaGraphDef will be restored into the supplied session.\n`python\nexport_dir = ...\n...\nwith tf.Session(graph=tf.Graph()) as sess:\n  tf.saved_model.loader.load(sess, [tag_constants.TRAINING], export_dir)\n  ...`\nLoad a SavedModel in C++\nThe C++ version of the SavedModel\nloader\nprovides an API to load a SavedModel from a path, while allowing\n`SessionOptions` and `RunOptions`.\nYou have to specify the tags associated with the graph to be loaded.\nThe loaded version of SavedModel is referred to as `SavedModelBundle`\nand contains the MetaGraphDef and the session within which it is loaded.\n`c++\nconst string export_dir = ...\nSavedModelBundle bundle;\n...\nLoadSavedModel(session_options, run_options, export_dir, {kSavedModelTagTrain},\n               &bundle);`\nLoad and serve a SavedModel in TensorFlow serving\nYou can easily load and serve a SavedModel with the TensorFlow Serving Model\nServer binary. See instructions\non how to install the server, or build it if you wish.\nOnce you have the Model Server, run it with:\n`tensorflow_model_server --port=port-numbers --model_name=your-model-name --model_base_path=your_model_base_path`\nSet the port and model_name flags to values of your choosing. The\nmodel_base_path flag expects to be to a base directory, with each version of\nyour model residing in a numerically named subdirectory. If you only have a\nsingle version of your model, simply place it in a subdirectory like so:\n\nPlace the model in /tmp/model/0001\nSet model_base_path to /tmp/model\n\nStore different versions of your model in numerically named subdirectories of a\ncommon base directory. For example, suppose the base directory is `/tmp/model`.\nIf you have only one version of your model, store it in `/tmp/model/0001`. If\nyou have two versions of your model, store the second version in\n`/tmp/model/0002`, and so on.  Set the `--model-base_path` flag to the base\ndirectory (`/tmp/model`, in this example).  TensorFlow Model Server will serve\nthe model in the highest numbered subdirectory of that base directory.\nStandard constants\nSavedModel offers the flexibility to build and load TensorFlow graphs for a\nvariety of use-cases. For the most common use-cases, SavedModel's APIs\nprovide a set of constants in Python and C++ that are easy to\nreuse and share across tools consistently.\nStandard MetaGraphDef tags\nYou may use sets of tags to uniquely identify a `MetaGraphDef` saved in a\nSavedModel. A subset of commonly used tags is specified in:\n\nPython\nC++\n\nStandard SignatureDef constants\nA SignatureDef\nis a protocol buffer that defines the signature of a computation\nsupported by a graph.\nCommonly used input keys, output keys, and method names are\ndefined in:\n\nPython\nC++\n\nUsing SavedModel with Estimators\nAfter training an `Estimator` model, you may want to create a service\nfrom that model that takes requests and returns a result.  You can run such a\nservice locally on your machine or deploy it in the cloud.\nTo prepare a trained Estimator for serving, you must export it in the standard\nSavedModel format. This section explains how to:\n\nSpecify the output nodes and the corresponding\n  APIs\n  that can be served (Classify, Regress, or Predict).\nExport your model to the SavedModel format.\nServe the model from a local server and request predictions.\n\nPrepare serving inputs\nDuring training, an input_fn() ingests data\nand prepares it for use by the model.  At serving time, similarly, a\n`serving_input_receiver_fn()` accepts inference requests and prepares them for\nthe model.  This function has the following purposes:\n\nTo add placeholders to the graph that the serving system will feed\n   with inference requests.\nTo add any additional ops needed to convert data from the input format\n   into the feature `Tensor`s expected by the model.\n\nThe function returns a `tf.estimator.export.ServingInputReceiver` object,\nwhich packages the placeholders and the resulting feature `Tensor`s together.\nA typical pattern is that inference requests arrive in the form of serialized\n`tf.Example`s, so the `serving_input_receiver_fn()` creates a single string\nplaceholder to receive them.  The `serving_input_receiver_fn()` is then also\nresponsible for parsing the `tf.Example`s by adding a `tf.parse_example` op to\nthe graph.\nWhen writing such a `serving_input_receiver_fn()`, you must pass a parsing\nspecification to `tf.parse_example` to tell the parser what feature names to\nexpect and how to map them to `Tensor`s. A parsing specification takes the\nform of a dict from feature names to `tf.FixedLenFeature`, `tf.VarLenFeature`,\nand `tf.SparseFeature`.  Note this parsing specification should not include\nany label or weight columns, since those will not be available at serving\ntime\u2014in contrast to a parsing specification used in the `input_fn()` at\ntraining time.\nIn combination, then:\n```py\nfeature_spec = {'foo': tf.FixedLenFeature(...),\n                'bar': tf.VarLenFeature(...)}\ndef serving_input_receiver_fn():\n  \"\"\"An input receiver that expects a serialized tf.Example.\"\"\"\n  serialized_tf_example = tf.placeholder(dtype=tf.string,\n                                         shape=[default_batch_size],\n                                         name='input_example_tensor')\n  receiver_tensors = {'examples': serialized_tf_example}\n  features = tf.parse_example(serialized_tf_example, feature_spec)\n  return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)\n```\nThe `tf.estimator.export.build_parsing_serving_input_receiver_fn` utility\nfunction provides that input receiver for the common case.\n\nNote: when training a model to be served using the Predict API with a local\nserver, the parsing step is not needed because the model will receive raw\nfeature data.\n\nEven if you require no parsing or other input processing\u2014that is, if the\nserving system will feed feature `Tensor`s directly\u2014you must still provide\na `serving_input_receiver_fn()` that creates placeholders for the feature\n`Tensor`s and passes them through.  The\n`tf.estimator.export.build_raw_serving_input_receiver_fn` utility provides for\nthis.\nIf these utilities do not meet your needs, you are free to write your own\n`serving_input_receiver_fn()`.  One case where this may be needed is if your\ntraining `input_fn()` incorporates some preprocessing logic that must be\nrecapitulated at serving time.  To reduce the risk of training-serving skew, we\nrecommend encapsulating such processing in a function which is then called\nfrom both `input_fn()` and `serving_input_receiver_fn()`.\nNote that the `serving_input_receiver_fn()` also determines the input\nportion of the signature.  That is, when writing a\n`serving_input_receiver_fn()`, you must tell the parser what signatures\nto expect and how to map them to your model's expected inputs.\nBy contrast, the output portion of the signature is determined by the model.\n\nSpecify the outputs of a custom model\nWhen writing a custom `model_fn`, you must populate the `export_outputs` element\nof the `tf.estimator.EstimatorSpec` return value. This is a dict of\n`{name: output}` describing the output signatures to be exported and used during\nserving.\nIn the usual case of making a single prediction, this dict contains\none element, and the `name` is immaterial.  In a multi-headed model, each head\nis represented by an entry in this dict.  In this case the `name` is a string\nof your choice that can be used to request a specific head at serving time.\nEach `output` value must be an `ExportOutput` object  such as\n`tf.estimator.export.ClassificationOutput`,\n`tf.estimator.export.RegressionOutput`, or\n`tf.estimator.export.PredictOutput`.\nThese output types map straightforwardly to the\nTensorFlow Serving APIs,\nand so determine which request types will be honored.\nNote: In the multi-headed case, a `SignatureDef` will be generated for each\nelement of the `export_outputs` dict returned from the model_fn, named using\nthe same keys.  These `SignatureDef`s differ only in their outputs, as\nprovided by the corresponding `ExportOutput` entry.  The inputs are always\nthose provided by the `serving_input_receiver_fn`.\nAn inference request may specify the head by name.  One head must be named\nusing signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY\nindicating which `SignatureDef` will be served when an inference request\ndoes not specify one.\n\nPerform the export\nTo export your trained Estimator, call\n`tf.estimator.Estimator.export_savedmodel` with the export base path and\nthe `serving_input_receiver_fn`.\n`py\nestimator.export_savedmodel(export_dir_base, serving_input_receiver_fn,\n                            strip_default_attrs=True)`\nThis method builds a new graph by first calling the\n`serving_input_receiver_fn()` to obtain feature `Tensor`s, and then calling\nthis `Estimator`'s `model_fn()` to generate the model graph based on those\nfeatures. It starts a fresh `Session`, and, by default, restores the most recent\ncheckpoint into it.  (A different checkpoint may be passed, if needed.)\nFinally it creates a time-stamped export directory below the given\n`export_dir_base` (i.e., `export_dir_base/<timestamp>`), and writes a\nSavedModel into it containing a single `MetaGraphDef` saved from this\nSession.\n\nNote: It is your responsibility to garbage-collect old exports.\nOtherwise, successive exports will accumulate under `export_dir_base`.\n\nServe the exported model locally\nFor local deployment, you can serve your model using\nTensorFlow Serving, an open-source project that loads a\nSavedModel and exposes it as a gRPC service.\nFirst, install TensorFlow Serving.\nThen build and run the local model server, substituting `$export_dir_base` with\nthe path to the SavedModel you exported above:\n`sh\nbazel build //tensorflow_serving/model_servers:tensorflow_model_server\nbazel-bin/tensorflow_serving/model_servers/tensorflow_model_server --port=9000 --model_base_path=$export_dir_base`\nNow you have a server listening for inference requests via gRPC on port 9000!\nRequest predictions from a local server\nThe server responds to gRPC requests according to the\nPredictionService\ngRPC API service definition.  (The nested protocol buffers are defined in\nvarious neighboring files).\nFrom the API service definition, the gRPC framework generates client libraries\nin various languages providing remote access to the API.  In a project using the\nBazel build tool, these libraries are built automatically and provided via\ndependencies like these (using Python for example):\n`build\n  deps = [\n    \"//tensorflow_serving/apis:classification_proto_py_pb2\",\n    \"//tensorflow_serving/apis:regression_proto_py_pb2\",\n    \"//tensorflow_serving/apis:predict_proto_py_pb2\",\n    \"//tensorflow_serving/apis:prediction_service_proto_py_pb2\"\n  ]`\nPython client code can then import the libraries thus:\n`py\nfrom tensorflow_serving.apis import classification_pb2\nfrom tensorflow_serving.apis import regression_pb2\nfrom tensorflow_serving.apis import predict_pb2\nfrom tensorflow_serving.apis import prediction_service_pb2`\n\nNote: `prediction_service_pb2` defines the service as a whole and so\nis always required.  However a typical client will need only one of\n`classification_pb2`, `regression_pb2`, and `predict_pb2`, depending on the\ntype of requests being made.\n\nSending a gRPC request is then accomplished by assembling a protocol buffer\ncontaining the request data and passing it to the service stub.  Note how the\nrequest protocol buffer is created empty and then populated via the\ngenerated protocol buffer API.\n```py\nfrom grpc.beta import implementations\nchannel = implementations.insecure_channel(host, int(port))\nstub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\nrequest = classification_pb2.ClassificationRequest()\nexample = request.input.example_list.examples.add()\nexample.features.feature['x'].float_list.value.extend(image[0].astype(float))\nresult = stub.Classify(request, 10.0)  # 10 secs timeout\n```\nThe returned result in this example is a `ClassificationResponse` protocol\nbuffer.\nThis is a skeletal example; please see the Tensorflow Serving\ndocumentation and examples\nfor more details.\n\nNote: `ClassificationRequest` and `RegressionRequest` contain a\n`tensorflow.serving.Input` protocol buffer, which in turn contains a list of\n`tensorflow.Example` protocol buffers.  `PredictRequest`, by contrast,\ncontains a mapping from feature names to values encoded via `TensorProto`.\nCorrespondingly: When using the `Classify` and `Regress` APIs, TensorFlow\nServing feeds serialized `tf.Example`s to the graph, so your\n`serving_input_receiver_fn()` should include a `tf.parse_example()` Op.\nWhen using the generic `Predict` API, however, TensorFlow Serving feeds raw\nfeature data to the graph, so a pass through `serving_input_receiver_fn()`\nshould be used.\n\n\n\nCLI to inspect and execute SavedModel\nYou can use the SavedModel Command Line Interface (CLI) to inspect and\nexecute a SavedModel.\nFor example, you can use the CLI to inspect the model's `SignatureDef`s.\nThe CLI enables you to quickly confirm that the input\nTensor dtype and shape match the model. Moreover, if you\nwant to test your model, you can use the CLI to do a sanity check by\npassing in sample inputs in various formats (for example, Python\nexpressions) and then fetching the output.\nInstall the SavedModel CLI\nBroadly speaking, you can install TensorFlow in either of the following\ntwo ways:\n\nBy installing a pre-built TensorFlow binary.\nBy building TensorFlow from source code.\n\nIf you installed TensorFlow through a pre-built TensorFlow binary,\nthen the SavedModel CLI is already installed on your system\nat pathname `bin\\saved_model_cli`.\nIf you built TensorFlow from source code, you must run the following\nadditional command to build `saved_model_cli`:\n`$ bazel build third_party/tensorflow/python/tools:saved_model_cli`\nOverview of commands\nThe SavedModel CLI supports the following two commands on a\n`MetaGraphDef` in a SavedModel:\n\n`show`, which shows a computation on a `MetaGraphDef` in a SavedModel.\n`run`, which runs a computation on a `MetaGraphDef`.\n\n`show` command\nA SavedModel contains one or more `MetaGraphDef`s, identified by their tag-sets.\nTo serve a model, you\nmight wonder what kind of `SignatureDef`s are in each model, and what are their\ninputs and outputs.  The `show` command let you examine the contents of the\nSavedModel in hierarchical order.  Here's the syntax:\n`usage: saved_model_cli show [-h] --dir DIR [--all]\n[--tag_set TAG_SET] [--signature_def SIGNATURE_DEF_KEY]`\nFor example, the following command shows all available\nMetaGraphDef tag-sets in the SavedModel:\n`$ saved_model_cli show --dir /tmp/saved_model_dir\nThe given SavedModel contains the following tag-sets:\nserve\nserve, gpu`\nThe following command shows all available `SignatureDef` keys in\na `MetaGraphDef`:\n`$ saved_model_cli show --dir /tmp/saved_model_dir --tag_set serve\nThe given SavedModel `MetaGraphDef` contains `SignatureDefs` with the\nfollowing keys:\nSignatureDef key: \"classify_x2_to_y3\"\nSignatureDef key: \"classify_x_to_y\"\nSignatureDef key: \"regress_x2_to_y3\"\nSignatureDef key: \"regress_x_to_y\"\nSignatureDef key: \"regress_x_to_y2\"\nSignatureDef key: \"serving_default\"`\nIf a `MetaGraphDef` has multiple tags in the tag-set, you must specify\nall tags, each tag separated by a comma. For example:\n\n$ saved_model_cli show --dir /tmp/saved_model_dir --tag_set serve,gpu\n\nTo show all inputs and outputs TensorInfo for a specific `SignatureDef`, pass in\nthe `SignatureDef` key to `signature_def` option. This is very useful when you\nwant to know the tensor key value, dtype and shape of the input tensors for\nexecuting the computation graph later. For example:\n`$ saved_model_cli show --dir \\\n/tmp/saved_model_dir --tag_set serve --signature_def serving_default\nThe given SavedModel SignatureDef contains the following input(s):\n  inputs['x'] tensor_info:\n      dtype: DT_FLOAT\n      shape: (-1, 1)\n      name: x:0\nThe given SavedModel SignatureDef contains the following output(s):\n  outputs['y'] tensor_info:\n      dtype: DT_FLOAT\n      shape: (-1, 1)\n      name: y:0\nMethod name is: tensorflow/serving/predict`\nTo show all available information in the SavedModel, use the `--all` option.\nFor example:\n\n$ saved_model_cli show --dir /tmp/saved_model_dir --all\nMetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n\nsignature_def['classify_x2_to_y3']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['inputs'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: x2:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['scores'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: y3:0\n  Method name is: tensorflow/serving/classify\n\n...\n\nsignature_def['serving_default']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['x'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: x:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['y'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: y:0\n  Method name is: tensorflow/serving/predict\n\n`run` command\nInvoke the `run` command to run a graph computation, passing\ninputs and then displaying (and optionally saving) the outputs.\nHere's the syntax:\n`usage: saved_model_cli run [-h] --dir DIR --tag_set TAG_SET --signature_def\n                           SIGNATURE_DEF_KEY [--inputs INPUTS]\n                           [--input_exprs INPUT_EXPRS]\n                           [--input_examples INPUT_EXAMPLES] [--outdir OUTDIR]\n                           [--overwrite] [--tf_debug]`\nThe `run` command provides the following three ways to pass inputs to the model:\n\n`--inputs` option enables you to pass numpy ndarray in files.\n`--input_exprs` option enables you to pass Python expressions.\n`--input_examples` option enables you to pass `tf.train.Example`.\n\n`--inputs`\nTo pass input data in files, specify the `--inputs` option, which takes the\nfollowing general format:\n`bsh\n--inputs <INPUTS>`\nwhere INPUTS is either of the following formats:\n\n`<input_key>=<filename>`\n`<input_key>=<filename>[<variable_name>]`\n\nYou may pass multiple INPUTS. If you do pass multiple inputs, use a semicolon\nto separate each of the INPUTS.\n`saved_model_cli` uses `numpy.load` to load the filename.\nThe filename may be in any of the following formats:\n\n`.npy`\n`.npz`\npickle format\n\nA `.npy` file always contains a numpy ndarray. Therefore, when loading from\na `.npy` file, the content will be directly assigned to the specified input\ntensor. If you specify a variable_name with that `.npy` file, the\nvariable_name will be ignored and a warning will be issued.\nWhen loading from a `.npz` (zip) file, you may optionally specify a\nvariable_name to identify the variable within the zip file to load for\nthe input tensor key.  If you don't specify a variable_name, the SavedModel\nCLI will check that only one file is included in the zip file and load it\nfor the specified input tensor key.\nWhen loading from a pickle file, if no `variable_name` is specified in the\nsquare brackets, whatever that is inside the pickle file will be passed to the\nspecified input tensor key. Otherwise, the SavedModel CLI will assume a\ndictionary is stored in the pickle file and the value corresponding to\nthe variable_name will be used.\n`--input_exprs`\nTo pass inputs through Python expressions, specify the `--input_exprs` option.\nThis can be useful for when you don't have data\nfiles lying around, but still want to sanity check the model with some simple\ninputs that match the dtype and shape of the model's `SignatureDef`s.\nFor example:\n`bsh\n`<input_key>=[[1],[2],[3]]``\nIn addition to Python expressions, you may also pass numpy functions. For\nexample:\n`bsh\n`<input_key>=np.ones((32,32,3))``\n(Note that the `numpy` module is already available to you as `np`.)\n`--input_examples`\nTo pass `tf.train.Example` as inputs, specify the `--input_examples` option.\nFor each input key, it takes a list of dictionary, where each dictionary is an\ninstance of `tf.train.Example`. The dictionary keys are the features and the\nvalues are the value lists for each feature.\nFor example:\n`bsh\n`<input_key>=[{\"age\":[22,24],\"education\":[\"BS\",\"MS\"]}]``\nSave output\nBy default, the SavedModel CLI writes output to stdout. If a directory is\npassed to `--outdir` option, the outputs will be saved as npy files named after\noutput tensor keys under the given directory.\nUse `--overwrite` to overwrite existing output files.\nTensorFlow debugger (tfdbg) integration\nIf `--tf_debug` option is set, the SavedModel CLI will use the\nTensorFlow Debugger (tfdbg) to watch the intermediate Tensors and runtime\ngraphs or subgraphs while running the SavedModel.\nFull examples of `run`\nGiven:\n\nYour model simply adds `x1` and `x2` to get output `y`.\nAll tensors in the model have shape `(-1, 1)`.\nYou have two `npy` files:\n`/tmp/my_data1.npy`, which contains a numpy ndarray `[[1], [2], [3]]`.\n`/tmp/my_data2.npy`, which contains another numpy\n      ndarray `[[0.5], [0.5], [0.5]]`.\n\nTo run these two `npy` files through the model to get output `y`, issue\nthe following command:\n`$ saved_model_cli run --dir /tmp/saved_model_dir --tag_set serve \\\n--signature_def x1_x2_to_y --inputs 'x1=/tmp/my_data1.npy;x2=/tmp/my_data2.npy' \\\n--outdir /tmp/out\nResult for output key y:\n[[ 1.5]\n [ 2.5]\n [ 3.5]]`\nLet's change the preceding example slightly. This time, instead of two\n`.npy` files, you now have an `.npz` file and a pickle file. Furthermore,\nyou want to overwrite any existing output file.  Here's the command:\n`$ saved_model_cli run --dir /tmp/saved_model_dir --tag_set serve \\\n--signature_def x1_x2_to_y \\\n--inputs 'x1=/tmp/my_data1.npz[x];x2=/tmp/my_data2.pkl' --outdir /tmp/out \\\n--overwrite\nResult for output key y:\n[[ 1.5]\n [ 2.5]\n [ 3.5]]`\nYou may specify python expression instead of an input file. For example,\nthe following command replaces input `x2` with a Python expression:\n`$ saved_model_cli run --dir /tmp/saved_model_dir --tag_set serve \\\n--signature_def x1_x2_to_y --inputs x1=/tmp/my_data1.npz[x] \\\n--input_exprs 'x2=np.ones((3,1))'\nResult for output key y:\n[[ 2]\n [ 3]\n [ 4]]`\nTo run the model with the TensorFlow Debugger on, issue the\nfollowing command:\n`$ saved_model_cli run --dir /tmp/saved_model_dir --tag_set serve \\\n--signature_def serving_default --inputs x=/tmp/data.npz[x] --tf_debug`\n\nStructure of a SavedModel directory\nWhen you save a model in SavedModel format, TensorFlow creates\na SavedModel directory consisting of the following subdirectories\nand files:\n`bsh\nassets/\nassets.extra/\nvariables/\n    variables.data-?????-of-?????\n    variables.index\nsaved_model.pb|saved_model.pbtxt`\nwhere:\n\n`assets` is a subfolder containing auxiliary (external) files,\n  such as vocabularies.  Assets are copied to the SavedModel location\n  and can be read when loading a specific `MetaGraphDef`.\n`assets.extra` is a subfolder where higher-level libraries and users can\n  add their own assets that co-exist with the model, but are not loaded by\n  the graph.  This subfolder is not managed by the SavedModel libraries.\n`variables` is a subfolder that includes output from\n  `tf.train.Saver`.\n`saved_model.pb` or `saved_model.pbtxt` is the SavedModel protocol buffer.\n  It includes the graph definitions as `MetaGraphDef` protocol buffers.\n\nA single SavedModel can represent multiple graphs.  In this case, all the\ngraphs in the SavedModel share a single set of checkpoints (variables)\nand assets. For example, the following diagram shows one SavedModel\ncontaining three `MetaGraphDef`s, all three of which share the same set\nof checkpoints and assets:\n\nEach graph is associated with a specific set of tags, which enables",
    "tag": "tensorflow"
  },
  {
    "title": "Graphs and Sessions",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/guide/graphs.md",
    "content": "Graphs and Sessions\nTensorFlow uses a dataflow graph to represent your computation in terms of\nthe dependencies between individual operations. This leads to a low-level\nprogramming model in which you first define the dataflow graph, then create a\nTensorFlow session to run parts of the graph across a set of local and\nremote devices.\nThis guide will be most useful if you intend to use the low-level programming\nmodel directly. Higher-level APIs such as `tf.estimator.Estimator` and Keras\nhide the details of graphs and sessions from the end user, but this guide may\nalso be useful if you want to understand how these APIs are implemented.\nWhy dataflow graphs?\n\nDataflow is a common\nprogramming model for parallel computing. In a dataflow graph, the nodes\nrepresent units of computation, and the edges represent the data consumed or\nproduced by a computation. For example, in a TensorFlow graph, the `tf.matmul`\noperation would correspond to a single node with two incoming edges (the\nmatrices to be multiplied) and one outgoing edge (the result of the\nmultiplication).\n\nDataflow has several advantages that TensorFlow leverages when executing your\nprograms:\n\n\nParallelism. By using explicit edges to represent dependencies between\n  operations, it is easy for the system to identify operations that can execute\n  in parallel.\n\n\nDistributed execution. By using explicit edges to represent the values\n  that flow between operations, it is possible for TensorFlow to partition your\n  program across multiple devices (CPUs, GPUs, and TPUs) attached to different\n  machines. TensorFlow inserts the necessary communication and coordination\n  between devices.\n\n\nCompilation. TensorFlow's XLA compiler can\n  use the information in your dataflow graph to generate faster code, for\n  example, by fusing together adjacent operations.\n\n\nPortability. The dataflow graph is a language-independent representation\n  of the code in your model. You can build a dataflow graph in Python, store it\n  in a SavedModel, and restore it in a C++ program for\n  low-latency inference.\n\n\nWhat is a `tf.Graph`?\nA `tf.Graph` contains two relevant kinds of information:\n\n\nGraph structure. The nodes and edges of the graph, indicating how\n  individual operations are composed together, but not prescribing how they\n  should be used. The graph structure is like assembly code: inspecting it can\n  convey some useful information, but it does not contain all of the useful\n  context that source code conveys.\n\n\nGraph collections. TensorFlow provides a general mechanism for storing\n  collections of metadata in a `tf.Graph`. The `tf.add_to_collection` function\n  enables you to associate a list of objects with a key (where `tf.GraphKeys`\n  defines some of the standard keys), and `tf.get_collection` enables you to\n  look up all objects associated with a key. Many parts of the TensorFlow\n  library use this facility: for example, when you create a `tf.Variable`, it\n  is added by default to collections representing \"global variables\" and\n  \"trainable variables\". When you later come to create a `tf.train.Saver` or\n  `tf.train.Optimizer`, the variables in these collections are used as the\n  default arguments.\n\n\nBuilding a `tf.Graph`\nMost TensorFlow programs start with a dataflow graph construction phase. In this\nphase, you invoke TensorFlow API functions that construct new `tf.Operation`\n(node) and `tf.Tensor` (edge) objects and add them to a `tf.Graph`\ninstance. TensorFlow provides a default graph that is an implicit argument\nto all API functions in the same context.  For example:\n\n\nCalling `tf.constant(42.0)` creates a single `tf.Operation` that produces the\n  value `42.0`, adds it to the default graph, and returns a `tf.Tensor` that\n  represents the value of the constant.\n\n\nCalling `tf.matmul(x, y)` creates a single `tf.Operation` that multiplies\n  the values of `tf.Tensor` objects `x` and `y`, adds it to the default graph,\n  and returns a `tf.Tensor` that represents the result of the multiplication.\n\n\nExecuting `v = tf.Variable(0)` adds to the graph a `tf.Operation` that will\n  store a writeable tensor value that persists between `tf.Session.run` calls.\n  The `tf.Variable` object wraps this operation, and can be used like a\n  tensor, which will read the current value of the\n  stored value. The `tf.Variable` object also has methods such as\n  `tf.Variable.assign` and `tf.Variable.assign_add` that\n  create `tf.Operation` objects that, when executed, update the stored value.\n  (See Variables for more information about variables.)\n\n\nCalling `tf.train.Optimizer.minimize` will add operations and tensors to the\n  default graph that calculates gradients, and return a `tf.Operation` that,\n  when run, will apply those gradients to a set of variables.\n\n\nMost programs rely solely on the default graph. However,\nsee Dealing with multiple graphs for more\nadvanced use cases. High-level APIs such as the `tf.estimator.Estimator` API\nmanage the default graph on your behalf, and--for example--may create different\ngraphs for training and evaluation.\nNote: Calling most functions in the TensorFlow API merely adds operations\nand tensors to the default graph, but does not perform the actual\ncomputation. Instead, you compose these functions until you have a `tf.Tensor`\nor `tf.Operation` that represents the overall computation--such as performing\none step of gradient descent--and then pass that object to a `tf.Session` to\nperform the computation. See the section \"Executing a graph in a `tf.Session`\"\nfor more details.\nNaming operations\nA `tf.Graph` object defines a namespace for the `tf.Operation` objects it\ncontains. TensorFlow automatically chooses a unique name for each operation in\nyour graph, but giving operations descriptive names can make your program easier\nto read and debug. The TensorFlow API provides two ways to override the name of\nan operation:\n\n\nEach API function that creates a new `tf.Operation` or returns a new\n  `tf.Tensor` accepts an optional `name` argument. For example,\n  `tf.constant(42.0, name=\"answer\")` creates a new `tf.Operation` named\n  `\"answer\"` and returns a `tf.Tensor` named `\"answer:0\"`. If the default graph\n  already contains an operation named `\"answer\"`, then TensorFlow would append\n  `\"_1\"`, `\"_2\"`, and so on to the name, in order to make it unique.\n\n\nThe `tf.name_scope` function makes it possible to add a name scope prefix\n  to all operations created in a particular context. The current name scope\n  prefix is a `\"/\"`-delimited list of the names of all active `tf.name_scope`\n  context managers. If a name scope has already been used in the current\n  context, TensorFlow appends `\"_1\"`, `\"_2\"`, and so on. For example:\n\n\n```python\n  c_0 = tf.constant(0, name=\"c\")  # => operation named \"c\"\n# Already-used names will be \"uniquified\".\n  c_1 = tf.constant(2, name=\"c\")  # => operation named \"c_1\"\n# Name scopes add a prefix to all operations created in the same context.\n  with tf.name_scope(\"outer\"):\n    c_2 = tf.constant(2, name=\"c\")  # => operation named \"outer/c\"\n\n\n```# Name scopes nest like paths in a hierarchical file system.\nwith tf.name_scope(\"inner\"):\n  c_3 = tf.constant(3, name=\"c\")  # => operation named \"outer/inner/c\"\n\n# Exiting a name scope context will return to the previous prefix.\nc_4 = tf.constant(4, name=\"c\")  # => operation named \"outer/c_1\"\n\n# Already-used name scopes will be \"uniquified\".\nwith tf.name_scope(\"inner\"):\n  c_5 = tf.constant(5, name=\"c\")  # => operation named \"outer/inner_1/c\"\n```\n\n\n```\nThe graph visualizer uses name scopes to group operations and reduce the visual\ncomplexity of a graph. See Visualizing your graph for\nmore information.\nNote that `tf.Tensor` objects are implicitly named after the `tf.Operation`\nthat produces the tensor as output. A tensor name has the form `\"<OP_NAME>:<i>\"`\nwhere:\n\n`\"<OP_NAME>\"` is the name of the operation that produces it.\n`\"<i>\"` is an integer representing the index of that tensor among the\n  operation's outputs.\n\nPlacing operations on different devices\nIf you want your TensorFlow program to use multiple different devices, the\n`tf.device` function provides a convenient way to request that all operations\ncreated in a particular context are placed on the same device (or type of\ndevice).\nA device specification has the following form:\n`/job:<JOB_NAME>/task:<TASK_INDEX>/device:<DEVICE_TYPE>:<DEVICE_INDEX>`\nwhere:\n\n`<JOB_NAME>` is an alpha-numeric string that does not start with a number.\n`<DEVICE_TYPE>` is a registered device type (such as `GPU` or `CPU`).\n`<TASK_INDEX>` is a non-negative integer representing the index of the task\n  in the job named `<JOB_NAME>`. See `tf.train.ClusterSpec` for an explanation\n  of jobs and tasks.\n`<DEVICE_INDEX>` is a non-negative integer representing the index of the\n  device, for example, to distinguish between different GPU devices used in the\n  same process.\n\nYou do not need to specify every part of a device specification. For example,\nif you are running in a single-machine configuration with a single GPU, you\nmight use `tf.device` to pin some operations to the CPU and GPU:\n```python\nOperations created outside either context will run on the \"best possible\"\ndevice. For example, if you have a GPU and a CPU available, and the operation\nhas a GPU implementation, TensorFlow will choose the GPU.\nweights = tf.random_normal(...)\nwith tf.device(\"/device:CPU:0\"):\n  # Operations created in this context will be pinned to the CPU.\n  img = tf.decode_jpeg(tf.read_file(\"img.jpg\"))\nwith tf.device(\"/device:GPU:0\"):\n  # Operations created in this context will be pinned to the GPU.\n  result = tf.matmul(weights, img)\n```\nIf you are deploying TensorFlow in a typical distributed configuration,\nyou might specify the job name and task ID to place variables on\na task in the parameter server job (`\"/job:ps\"`), and the other operations on\ntask in the worker job (`\"/job:worker\"`):\n```python\nwith tf.device(\"/job:ps/task:0\"):\n  weights_1 = tf.Variable(tf.truncated_normal([784, 100]))\n  biases_1 = tf.Variable(tf.zeros([100]))\nwith tf.device(\"/job:ps/task:1\"):\n  weights_2 = tf.Variable(tf.truncated_normal([100, 10]))\n  biases_2 = tf.Variable(tf.zeros([10]))\nwith tf.device(\"/job:worker\"):\n  layer_1 = tf.matmul(train_batch, weights_1) + biases_1\n  layer_2 = tf.matmul(train_batch, weights_2) + biases_2\n```\n`tf.device` gives you a lot of flexibility to choose placements for individual\noperations or broad regions of a TensorFlow graph. In many cases, there are\nsimple heuristics that work well. For example, the\n`tf.train.replica_device_setter` API can be used with `tf.device` to place\noperations for data-parallel distributed training. For example, the\nfollowing code fragment shows how `tf.train.replica_device_setter` applies\ndifferent placement policies to `tf.Variable` objects and other operations:\n```python\nwith tf.device(tf.train.replica_device_setter(ps_tasks=3)):\n  # tf.Variable objects are, by default, placed on tasks in \"/job:ps\" in a\n  # round-robin fashion.\n  w_0 = tf.Variable(...)  # placed on \"/job:ps/task:0\"\n  b_0 = tf.Variable(...)  # placed on \"/job:ps/task:1\"\n  w_1 = tf.Variable(...)  # placed on \"/job:ps/task:2\"\n  b_1 = tf.Variable(...)  # placed on \"/job:ps/task:0\"\ninput_data = tf.placeholder(tf.float32)     # placed on \"/job:worker\"\n  layer_0 = tf.matmul(input_data, w_0) + b_0  # placed on \"/job:worker\"\n  layer_1 = tf.matmul(layer_0, w_1) + b_1     # placed on \"/job:worker\"\n```\nTensor-like objects\nMany TensorFlow operations take one or more `tf.Tensor` objects as arguments.\nFor example, `tf.matmul` takes two `tf.Tensor` objects, and `tf.add_n` takes\na list of `n` `tf.Tensor` objects. For convenience, these functions will accept\na tensor-like object in place of a `tf.Tensor`, and implicitly convert it\nto a `tf.Tensor` using the `tf.convert_to_tensor` method. Tensor-like objects\ninclude elements of the following types:\n\n`tf.Tensor`\n`tf.Variable`\nnumpy.ndarray\n`list` (and lists of tensor-like objects)\nScalar Python types: `bool`, `float`, `int`, `str`\n\nYou can register additional tensor-like types using\n`tf.register_tensor_conversion_function`.\nNote: By default, TensorFlow will create a new `tf.Tensor` each time you use\nthe same tensor-like object. If the tensor-like object is large (e.g. a\n`numpy.ndarray` containing a set of training examples) and you use it multiple\ntimes, you may run out of memory. To avoid this, manually call\n`tf.convert_to_tensor` on the tensor-like object once and use the returned\n`tf.Tensor` instead.\nExecuting a graph in a `tf.Session`\nTensorFlow uses the `tf.Session` class to represent a connection between the\nclient program---typically a Python program, although a similar interface is\navailable in other languages---and the C++ runtime. A `tf.Session` object\nprovides access to devices in the local machine, and remote devices using the\ndistributed TensorFlow runtime. It also caches information about your\n`tf.Graph` so that you can efficiently run the same computation multiple times.\nCreating a `tf.Session`\nIf you are using the low-level TensorFlow API, you can create a `tf.Session`\nfor the current default graph as follows:\n```python\nCreate a default in-process session.\nwith tf.Session() as sess:\n  # ...\nCreate a remote session.\nwith tf.Session(\"grpc://example.org:2222\"):\n  # ...\n```\nSince a `tf.Session` owns physical resources (such as GPUs and\nnetwork connections), it is typically used as a context manager (in a `with`\nblock) that automatically closes the session when you exit the block. It is\nalso possible to create a session without using a `with` block, but you should\nexplicitly call `tf.Session.close` when you are finished with it to free the\nresources.\nNote: Higher-level APIs such as `tf.train.MonitoredTrainingSession` or\n`tf.estimator.Estimator` will create and manage a `tf.Session` for you. These\nAPIs accept optional `target` and `config` arguments (either directly, or as\npart of a `tf.estimator.RunConfig` object), with the same meaning as\ndescribed below.\n`tf.Session.__init__` accepts three optional arguments:\n\n\n`target`. If this argument is left empty (the default), the session will\n  only use devices in the local machine. However, you may also specify a\n  `grpc://` URL to specify the address of a TensorFlow server, which gives the\n  session access to all devices on machines that this server controls. See\n  `tf.train.Server` for details of how to create a TensorFlow\n  server. For example, in the common between-graph replication\n  configuration, the `tf.Session` connects to a `tf.train.Server` in the same\n  process as the client. The distributed TensorFlow\n  deployment guide describes other common scenarios.\n\n\n`graph`. By default, a new `tf.Session` will be bound to---and only able\n  to run operations in---the current default graph. If you are using multiple\n  graphs in your program (see Programming with multiple\n  graphs for more details), you can specify\n  an explicit `tf.Graph` when you construct the session.\n\n\n`config`. This argument allows you to specify a `tf.ConfigProto` that\n  controls the behavior of the session. For example, some of the configuration\n  options include:\n\n\n`allow_soft_placement`. Set this to `True` to enable a \"soft\" device\nplacement algorithm, which ignores `tf.device` annotations that attempt\nto place CPU-only operations on a GPU device, and places them on the CPU\ninstead.\n\n\n`cluster_def`. When using distributed TensorFlow, this option allows you\nto specify what machines to use in the computation, and provide a mapping\nbetween job names, task indices, and network addresses. See\n`tf.train.ClusterSpec.as_cluster_def` for details.\n\n\n`graph_options.optimizer_options`. Provides control over the optimizations\nthat TensorFlow performs on your graph before executing it.\n\n\n`gpu_options.allow_growth`. Set this to `True` to change the GPU memory\nallocator so that it gradually increases the amount of memory allocated,\nrather than allocating most of the memory at startup.\n\n\n\n\nUsing `tf.Session.run` to execute operations\nThe `tf.Session.run` method is the main mechanism for running a `tf.Operation`\nor evaluating a `tf.Tensor`. You can pass one or more `tf.Operation` or\n`tf.Tensor` objects to `tf.Session.run`, and TensorFlow will execute the\noperations that are needed to compute the result.\n`tf.Session.run` requires you to specify a list of fetches, which determine\nthe return values, and may be a `tf.Operation`, a `tf.Tensor`, or\na tensor-like type such as `tf.Variable`. These fetches\ndetermine what subgraph of the overall `tf.Graph` must be executed to\nproduce the result: this is the subgraph that contains all operations named in\nthe fetch list, plus all operations whose outputs are used to compute the value\nof the fetches. For example, the following code fragment shows how different\narguments to `tf.Session.run` cause different subgraphs to be executed:\n```python\nx = tf.constant([[37.0, -23.0], [1.0, 4.0]])\nw = tf.Variable(tf.random_uniform([2, 2]))\ny = tf.matmul(x, w)\noutput = tf.nn.softmax(y)\ninit_op = w.initializer\nwith tf.Session() as sess:\n  # Run the initializer on `w`.\n  sess.run(init_op)\n# Evaluate `output`. `sess.run(output)` will return a NumPy array containing\n  # the result of the computation.\n  print(sess.run(output))\n# Evaluate `y` and `output`. Note that `y` will only be computed once, and its\n  # result used both to return `y_val` and as an input to the `tf.nn.softmax()`\n  # op. Both `y_val` and `output_val` will be NumPy arrays.\n  y_val, output_val = sess.run([y, output])\n```\n`tf.Session.run` also optionally takes a dictionary of feeds, which is a\nmapping from `tf.Tensor` objects (typically `tf.placeholder` tensors) to\nvalues (typically Python scalars, lists, or NumPy arrays) that will be\nsubstituted for those tensors in the execution. For example:\n```python\nDefine a placeholder that expects a vector of three floating-point values,\nand a computation that depends on it.\nx = tf.placeholder(tf.float32, shape=[3])\ny = tf.square(x)\nwith tf.Session() as sess:\n  # Feeding a value changes the result that is returned when you evaluate `y`.\n  print(sess.run(y, {x: [1.0, 2.0, 3.0]}))  # => \"[1.0, 4.0, 9.0]\"\n  print(sess.run(y, {x: [0.0, 0.0, 5.0]}))  # => \"[0.0, 0.0, 25.0]\"\n# Raises `tf.errors.InvalidArgumentError`, because you must feed a value for\n  # a `tf.placeholder()` when evaluating a tensor that depends on it.\n  sess.run(y)\n# Raises `ValueError`, because the shape of `37.0` does not match the shape\n  # of placeholder `x`.\n  sess.run(y, {x: 37.0})\n```\n`tf.Session.run` also accepts an optional `options` argument that enables you\nto specify options about the call, and an optional `run_metadata` argument that\nenables you to collect metadata about the execution. For example, you can use\nthese options together to collect tracing information about the execution:\n```\ny = tf.matmul([[37.0, -23.0], [1.0, 4.0]], tf.random_uniform([2, 2]))\nwith tf.Session() as sess:\n  # Define options for the `sess.run()` call.\n  options = tf.RunOptions()\n  options.output_partition_graphs = True\n  options.trace_level = tf.RunOptions.FULL_TRACE\n# Define a container for the returned metadata.\n  metadata = tf.RunMetadata()\nsess.run(y, options=options, run_metadata=metadata)\n# Print the subgraphs that executed on each device.\n  print(metadata.partition_graphs)\n# Print the timings of each operation that executed.\n  print(metadata.step_stats)\n```\nVisualizing your graph\nTensorFlow includes tools that can help you to understand the code in a graph.\nThe graph visualizer is a component of TensorBoard that renders the\nstructure of your graph visually in a browser. The easiest way to create a\nvisualization is to pass a `tf.Graph` when creating the\n`tf.summary.FileWriter`:\n```python\nBuild your graph.\nx = tf.constant([[37.0, -23.0], [1.0, 4.0]])\nw = tf.Variable(tf.random_uniform([2, 2]))\ny = tf.matmul(x, w)\n...\nloss = ...\ntrain_op = tf.train.AdagradOptimizer(0.01).minimize(loss)\nwith tf.Session() as sess:\n  # `sess.graph` provides access to the graph used in a `tf.Session`.\n  writer = tf.summary.FileWriter(\"/tmp/log/...\", sess.graph)\n# Perform your computation...\n  for i in range(1000):\n    sess.run(train_op)\n    # ...\nwriter.close()\n```\nNote: If you are using a `tf.estimator.Estimator`, the graph (and any\nsummaries) will be logged automatically to the `model_dir` that you specified\nwhen creating the estimator.\nYou can then open the log in `tensorboard`, navigate to the \"Graph\" tab, and\nsee a high-level visualization of your graph's structure. Note that a typical\nTensorFlow graph---especially training graphs with automatically computed\ngradients---has too many nodes to visualize at once. The graph visualizer makes\nuse of name scopes to group related operations into \"super\" nodes. You can\nclick on the orange \"+\" button on any of these super nodes to expand the\nsubgraph inside.\n\nFor more information about visualizing your TensorFlow application with\nTensorBoard, see the TensorBoard site.\nProgramming with multiple graphs\nNote: When training a model, a common way of organizing your code is to use one\ngraph for training your model, and a separate graph for evaluating or performing\ninference with a trained model. In many cases, the inference graph will be\ndifferent from the training graph: for example, techniques like dropout and\nbatch normalization use different operations in each case. Furthermore, by\ndefault utilities like `tf.train.Saver` use the names of `tf.Variable` objects\n(which have names based on an underlying `tf.Operation`) to identify each\nvariable in a saved checkpoint. When programming this way, you can either use\ncompletely separate Python processes to build and execute the graphs, or you can\nuse multiple graphs in the same process. This section describes how to use\nmultiple graphs in the same process.\nAs noted above, TensorFlow provides a \"default graph\" that is implicitly passed\nto all API functions in the same context. For many applications, a single graph\nis sufficient. However, TensorFlow also provides methods for manipulating\nthe default graph, which can be useful in more advanced use cases. For example:\n\n\nA `tf.Graph` defines the namespace for `tf.Operation` objects: each\n  operation in a single graph must have a unique name. TensorFlow will\n  \"uniquify\" the names of operations by appending `\"_1\"`, `\"_2\"`, and so on to\n  their names if the requested name is already taken. Using multiple explicitly\n  created graphs gives you more control over what name is given to each\n  operation.\n\n\nThe default graph stores information about every `tf.Operation` and\n  `tf.Tensor` that was ever added to it. If your program creates a large number\n  of unconnected subgraphs, it may be more efficient to use a different\n  `tf.Graph` to build each subgraph, so that unrelated state can be garbage\n  collected.\n\n\nYou can install a different `tf.Graph` as the default graph, using the\n`tf.Graph.as_default` context manager:\n```python\ng_1 = tf.Graph()\nwith g_1.as_default():\n  # Operations created in this scope will be added to`g_1`.\n  c = tf.constant(\"Node in g_1\")\n# Sessions created in this scope will run operations from `g_1`.\n  sess_1 = tf.Session()\ng_2 = tf.Graph()\nwith g_2.as_default():\n  # Operations created in this scope will be added to `g_2`.\n  d = tf.constant(\"Node in g_2\")\nAlternatively, you can pass a graph when constructing a `tf.Session`:\n`sess_2` will run operations from `g_2`.\nsess_2 = tf.Session(graph=g_2)\nassert c.graph is g_1\nassert sess_1.graph is g_1\nassert d.graph is g_2\nassert sess_2.graph is g_2\n```\nTo inspect the current default graph, call `tf.get_default_graph`, which\nreturns a `tf.Graph` object:\n```python\nPrint all of the operations in the default graph.\ng = tf.get_default_graph()\nprint(g.get_operations())",
    "tag": "tensorflow"
  },
  {
    "title": "TensorFlow 1.x guide (archived)",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/guide",
    "content": "TensorFlow 1.x guide (archived)\nNote: Please use the latest guide at https://www.tensorflow.org/guide\nThe documents in this unit dive into the details of how TensorFlow 1.x\nworks. The units are as follows:\nHigh Level APIs\n\nKeras, TensorFlow's high-level API for building and\n  training deep learning models.\nEager Execution, an API for writing TensorFlow code\n  imperatively, like you would use Numpy.\nImporting Data, easy input pipelines to bring your data into\n  your TensorFlow program.\nEstimators, a high-level API that provides\n  fully-packaged models ready for large-scale training and production.\n\nEstimators\n\nPremade Estimators, the basics of premade\n  Estimators.\nCheckpoints, save training progress and resume where you\n  left off.\nFeature Columns, handle a variety of input data types\n  without changes to the model.\nDatasets for Estimators, use `tf.data` to\n  input data.\nCreating Custom Estimators, write your own\n  Estimator.\n\nAccelerators\n\nDistributed Strategy\nUsing GPUs explains how TensorFlow assigns operations to\n  devices and how you can change the arrangement manually.\nUsing TPUs explains how to modify `Estimator` programs to\n  run on a TPU.\n\nLow Level APIs\n\nIntroduction, which introduces the\n  basics of how you can use TensorFlow outside of the high Level APIs.\nTensors, which explains how to create,\n  manipulate, and access Tensors--the fundamental object in TensorFlow.\nVariables, which details how\n  to represent shared, persistent state in your program.\nGraphs and Sessions\nControl flow, using AutoGraph and `tf.function`.\nSave and Restore, which\n  explains how to save and restore variables and models.\nRagged Tensors, which explains how to use\n  Ragged Tensors to encode nested variable-length lists.\n\nML Concepts\n\nEmbeddings, which introduces the concept\n  of embeddings, provides a simple example of training an embedding in\n  TensorFlow, and explains how to view embeddings with the TensorBoard\n  Embedding Projector.\n\nDebugging\n\nTensorFlow Debugger, which\n  explains how to use the TensorFlow debugger (tfdbg).\n\nPerformance\nPerformance is an important consideration when training machine learning models.\nPerformance speeds up and scales research while also providing end users with\nnear instant predictions.\n\nPerformance overview contains a collection of best\n  practices for optimizing your TensorFlow code.\nData input pipeline describes the `tf.data` API\n  for building efficient data input pipelines for TensorFlow.\nBenchmarks contain a collection of benchmark\n  results for a variety of hardware configurations.\n\nAdditionally, TensorFlow Lite has\noptimization techniques\nfor mobile and embedded devices.\nExtend\nThis section explains how developers can add functionality to TensorFlow's\ncapabilities.\n\nTensorFlow architecture presents an architectural\n  overview.\nCreate an op, which explains how to create your own operations.\nCustom filesystem plugin, which explains how to add\n  support for your own shared or distributed filesystem.\nCustom file and record formats, which details how to add\n  support for your own file and record formats.\nModel files, for creating tools compatible with\n  TensorFlow's model format.\nLanguage bindings, Python is currently the only\n  language supported by TensorFlow's API stability promises. However, TensorFlow\n  also provides functionality to create or develop features in other languages.\nC++ guide\n\nXLA (Accelerated Linear Algebra) is an\nexperimental compiler for linear algebra that optimizes TensorFlow computations.\nMisc\n\nTensorFlow Version Compatibility,\n",
    "tag": "tensorflow"
  },
  {
    "title": "Using TPUs",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/guide/using_tpu.md",
    "content": "Using TPUs\nExperimental support for Cloud TPUs is\ncurrently available for Keras and Google Colab. Run Colab notebooks on a TPU by\nchanging the hardware accelerator in your notebook settings:\nRuntime > Change runtime type > Hardware accelerator > TPU. The following\nTPU-enabled Colab notebooks are available to test:\n\nA quick test, just to measure FLOPS.\nA CNN image classifier with tf.keras.\nAn LSTM markov chain text generator with tf.keras\n\nTPUEstimator\nThe remainder of this guide demonstrates the `tf.estimator.tpu.TPUEstimator`\nclass to drive a Cloud TPU. A\n`tf.estimator.Estimator` is a model-level abstraction that can drive models on\nCPU and GPUs. You must use `tf.estimator.tpu.TPUEstimator` to drive a model on\nCloud TPUs. See the premade Estimator guide\nand custom Estimator guide to learn more about\nEstimators.\nThe simplest way to maintain a model that can be run both on CPU/GPU or on a\nCloud TPU is to define the model's inference phase (from inputs to predictions)\noutside of the `model_fn`. Then maintain separate implementations of the\n`Estimator` setup and `model_fn`, both wrapping this inference step. For an\nexample of this pattern compare the `mnist.py` and `mnist_tpu.py` implementation in\ntensorflow/models.\nRun a TPUEstimator locally\nTo create a standard `Estimator` you call the constructor, and pass it a\n`model_fn`, for example:\n`my_estimator = tf.estimator.Estimator(model_fn=my_model_fn)`\nThe changes required to use a `tf.estimator.tpu.TPUEstimator` on your local\nmachine are relatively minor. The constructor requires two additional arguments.\nYou should set the `use_tpu` argument to `False`, and pass a\n`tf.estimator.tpu.RunConfig` as the `config` argument, as shown below:\n`python\nmy_tpu_estimator = tf.estimator.tpu.TPUEstimator(\n    model_fn=my_model_fn,\n    config=tf.estimator.tpu.RunConfig(),\n    use_tpu=False)`\nJust this simple change will allow you to run a `TPUEstimator` locally.\nThe majority of example TPU models can be run in this local mode,\nby setting the command line flags as follows:\n`$ python mnist_tpu.py --use_tpu=False --master=''`\nNote: This `use_tpu=False` argument is useful for trying out the `TPUEstimator`\nAPI. It is not meant to be a complete TPU compatibility test. Successfully\nrunning a model locally in a `TPUEstimator` does not guarantee that it will\nwork on a TPU.\nBuilding a `tpu.RunConfig`\nWhile the default `RunConfig` is sufficient for local training, these settings\ncannot be ignored in real usage. A more typical setup for a `RunConfig`, that\ncan be switched to use a Cloud TPU, might be as follows:\n``` python\nimport tempfile\nimport subprocess\nclass FLAGS(object):\n  use_tpu=False\n  tpu_name=None\n  # Use a local temporary path for the `model_dir`\n  model_dir = tempfile.mkdtemp()\n  # Number of training steps to run on the Cloud TPU before returning control.\n  iterations = 50\n  # A single Cloud TPU has 8 shards.\n  num_shards = 8\nif FLAGS.use_tpu:\n    my_project_name = subprocess.check_output([\n        'gcloud','config','get-value','project'])\n    my_zone = subprocess.check_output([\n        'gcloud','config','get-value','compute/zone'])\n    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n            tpu=[FLAGS.tpu_name],\n            zone=my_zone,\n            project=my_project_name)\n    master = tpu_cluster_resolver.get_master()\nelse:\n    master = ''\nmy_tpu_run_config = tf.estimator.tpu.RunConfig(\n    master=master,\n    evaluation_master=master,\n    model_dir=FLAGS.model_dir,\n    session_config=tf.ConfigProto(\n        allow_soft_placement=True, log_device_placement=True),\n    tpu_config=tf.estimator.tpu.TPUConfig(FLAGS.iterations,\n                                          FLAGS.num_shards),\n)\n```\nThen you must pass the `tf.estimator.tpu.RunConfig` to the constructor:\n`python\nmy_tpu_estimator = tf.estimator.tpu.TPUEstimator(\n    model_fn=my_model_fn,\n    config = my_tpu_run_config,\n    use_tpu=FLAGS.use_tpu)`\nTypically the `FLAGS` would be set by command line arguments. To switch from\ntraining locally to training on a Cloud TPU you would need to:\n\nSet `FLAGS.use_tpu` to `True`\nSet `FLAGS.tpu_name` so the `tf.contrib.cluster_resolver.TPUClusterResolver` can find it\nSet `FLAGS.model_dir` to a Google Cloud Storage bucket url (`gs://`).\n\nOptimizer\nWhen training on a Cloud TPU, you must wrap the optimizer in a\n`tf.contrib.tpu.CrossShardOptimizer`, which uses an `allreduce` to aggregate\ngradients and broadcast the result to each shard (each TPU core).\nThe `CrossShardOptimizer` is not compatible with local training. So, to have\nthe same code run both locally and on a Cloud TPU, add lines like the following:\n`python\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\nif FLAGS.use_tpu:\n  optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)`\nIf you prefer to avoid a global `FLAGS` variable in your model code, one\napproach is to set the optimizer as one of the `Estimator`'s params,\nas follows:\n`python\nmy_tpu_estimator = tf.estimator.tpu.TPUEstimator(\n    model_fn=my_model_fn,\n    config = my_tpu_run_config,\n    use_tpu=FLAGS.use_tpu,\n    params={'optimizer':optimizer})`\nModel Function\nThis section details the changes you must make to the model function\n(`model_fn`) for `TPUEstimator` compatibility.\nStatic shapes\nDuring regular usage TensorFlow attempts to determine the shapes of each\n`tf.Tensor` during graph construction. During execution any unknown shape\ndimensions are determined dynamically,\nsee Tensor Shapes for more details.\nTo run on Cloud TPUs, TensorFlow models are compiled using\nXLA. XLA uses a similar system for determining\nshapes at compile time. XLA requires that all tensor dimensions be statically\ndefined at compile time. All shapes must evaluate to a constant, and not depend\non external data, or stateful operations like variables or a random number\ngenerator.\nSummaries\nRemove any use of `tf.summary` from your model.\nTensorBoard summaries are\na great way see inside your model. A minimal set of basic summaries are\nautomatically recorded by the `TPUEstimator`, to `event` files in the\n`model_dir`. Custom summaries, however, are currently unsupported when training\non a Cloud TPU. So while the `TPUEstimator` will still run locally with\nsummaries, it will fail if used on a TPU.\nMetrics\nBuild your evaluation metrics dictionary in a stand-alone `metric_fn`.\nEvaluation metrics are an essential part of training a model. These are fully\nsupported on Cloud TPUs, but with a slightly different syntax.\nA standard `tf.metrics` returns two tensors. The first returns the running\naverage of the metric value, while the second updates the running average and\nreturns the value for this batch:\n`running_average, current_batch = tf.metrics.accuracy(labels, predictions)`\nIn a standard `Estimator` you create a dictionary of these pairs and return it\nas part of the `EstimatorSpec`:\n```python\nmy_metrics = {'accuracy': tf.metrics.accuracy(labels, predictions)}\nreturn tf.estimator.EstimatorSpec(\n  ...\n  eval_metric_ops=my_metrics\n)\n```\nIn a `TPUEstimator` you instead pass a function (which returns a metrics\ndictionary) and a list of argument tensors, as shown below:\n```python\ndef my_metric_fn(labels, predictions):\n   return {'accuracy': tf.metrics.accuracy(labels, predictions)}\nreturn tf.estimator.tpu.TPUEstimatorSpec(\n  ...\n  eval_metrics=(my_metric_fn, [labels, predictions])\n)\n```\nUse TPUEstimatorSpec\n`TPUEstimatorSpec` do not support hooks, and require function wrappers for\nsome fields.\nAn `Estimator`'s `model_fn` must return an `EstimatorSpec`. An `EstimatorSpec`\nis a simple structure of named fields containing all the `tf.Tensors` of the\nmodel that the `Estimator` may need to interact with.\n`TPUEstimators` use a `tf.estimator.tpu.TPUEstimatorSpec`. There are a few\ndifferences between it and a standard `tf.estimator.EstimatorSpec`:\n\nThe `eval_metric_ops` must be wrapped into a `metrics_fn`, this field is\n   renamed `eval_metrics`.\nThe `tf.train.SessionRunHook` are unsupported, so these fields are\n   omitted.\nThe `tf.train.Scaffold`, if used, must also be wrapped in a\n   function. This field is renamed to `scaffold_fn`.\n\n`Scaffold` and `Hooks` are for advanced usage, and can typically be omitted.\nInput functions\nInput functions work mainly unchanged as they run on the host computer, not the\nCloud TPU itself. This section explains the two necessary adjustments.\nParams argument\nThe `input_fn` for a standard `Estimator` can include a `params` argument; the\n`input_fn` for a `TPUEstimator` must include a `params` argument. This is\nnecessary to allow the estimator to set the batch size for each replica of the\ninput stream. So the minimum signature for an `input_fn` for a `TPUEstimator`\nis:\n`def my_input_fn(params):\n  pass`\nWhere `params['batch-size']` will contain the batch size.\nStatic shapes and batch size\nThe input pipeline generated by your `input_fn` is run on CPU. So it is mostly\nfree from the strict static shape requirements imposed by the XLA/TPU environment.\nThe one requirement is that the batches of data fed from your input pipeline to\nthe TPU have a static shape, as determined by the standard TensorFlow shape\ninference algorithm. Intermediate tensors are free to have a dynamic shapes.\nIf shape inference has failed, but the shape is known it is possible to\nimpose the correct shape using `tf.set_shape`. \nIn the example below the shape inference algorithm fails, but it is correctly\nusing `set_shape`:\n```\nx = tf.zeros(tf.constant([1,2,3])+1)\nx.shape\nTensorShape([Dimension(None), Dimension(None), Dimension(None)])\nx.set_shape([2,3,4])\n```\nIn many cases the batch size is the only unknown dimension. A typical input\npipeline, using `tf.data`, will usually produce batches of a fixed size. The\nlast batch of a finite `Dataset`, however, is typically smaller, containing just\nthe remaining elements. Since a `Dataset` does not know its own length or\nfiniteness, the standard `tf.data.Dataset.batch` method cannot determine if all\nbatches will have a fixed size batch on its own:\n```\nparams = {'batch_size':32}\nds = tf.data.Dataset.from_tensors([0, 1, 2])\nds = ds.repeat().batch(params['batch_size'])\nds\n\n```\nThe most straightforward fix is to use\n`tf.data.Dataset.apply` and `tf.contrib.data.batch_and_drop_remainder`\nas follows:\n```\nparams = {'batch_size':32}\nds = tf.data.Dataset.from_tensors([0, 1, 2])\nds = ds.repeat().apply(\n    tf.contrib.data.batch_and_drop_remainder(params['batch_size']))\nds\n\n```\nThe one downside to this approach is that, as the name implies, this batching\nmethod throws out any fractional batch at the end of the dataset. This is fine\nfor an infinitely repeating dataset being used for training, but could be a\nproblem if you want to train for an exact number of epochs.\nTo do an exact 1-epoch of evaluation you can work around this by manually\npadding the length of the batches, and setting the padding entries to have zero\nweight when creating your `tf.metrics`.\nDatasets\nEfficient use of the `tf.data.Dataset` API is critical when using a Cloud\nTPU, as it is impossible to use the Cloud TPU's unless you can feed it data\nquickly enough. See Input Pipeline Performance Guide for details on dataset performance.\nFor all but the simplest experimentation (using\n`tf.data.Dataset.from_tensor_slices` or other in-graph data) you will need to\nstore all data files read by the `TPUEstimator`'s `Dataset` in Google Cloud\nStorage Buckets.\nFor most use-cases, we recommend converting your data into `TFRecord`\nformat and using a `tf.data.TFRecordDataset` to read it. This, however, is not\na hard requirement and you can use other dataset readers\n(`FixedLengthRecordDataset` or `TextLineDataset`) if you prefer.\nSmall datasets can be loaded entirely into memory using\n`tf.data.Dataset.cache`.\nRegardless of the data format used, it is strongly recommended that you\nuse large files, on the order of\n100MB. This is especially important in this networked setting as the overhead\nof opening a file is significantly higher.\nIt is also important, regardless of the type of reader used, to enable buffering\nusing the `buffer_size` argument to the constructor. This argument is specified\nin bytes. A minimum of a few MB (`buffer_size=8*1024*1024`) is recommended so\nthat data is available when needed.\nThe TPU-demos repo includes\na script\nfor downloading the imagenet dataset and converting it to an appropriate format.\nThis together with the imagenet\nmodels\nincluded in the repo demonstrate all of these best-practices.\nNext steps\n\nGoogle Cloud TPU Documentation \u2014Set up\n  and run a Google\u00a0Cloud\u00a0TPU.\nMigrating to TPUEstimator API\n  \u2014This tutorial describes how to convert a model program using the Estimator\u00a0API\n    to one using the `TPUEstimator` API.\nTPU Demos Repository \u2014Examples of\n  Cloud\u00a0TPU compatible models.\nThe Google Cloud TPU Performance Guide\n  \u2014Enhance Cloud TPU performance further by adjusting Cloud TPU configuration\n",
    "tag": "tensorflow"
  },
  {
    "title": "Embeddings",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/guide/embedding.md",
    "content": "Embeddings\nThis document introduces the concept of embeddings, gives a simple example of\nhow to train an embedding in TensorFlow, and explains how to view embeddings\nwith the TensorBoard Embedding Projector\n(live example). The first two parts target\nnewcomers to machine learning or TensorFlow, and the Embedding Projector how-to\nis for users at all levels.\nAn alternative tutorial on these concepts is available in the\nEmbeddings section of Machine Learning Crash Course.\n[TOC]\nAn embedding is a mapping from discrete objects, such as words, to vectors\nof real numbers. For example, a 300-dimensional embedding for English words\ncould include:\n`blue:  (0.01359, 0.00075997, 0.24608, ..., -0.2524, 1.0048, 0.06259)\nblues:  (0.01396, 0.11887, -0.48963, ..., 0.033483, -0.10007, 0.1158)\norange:  (-0.24776, -0.12359, 0.20986, ..., 0.079717, 0.23865, -0.014213)\noranges:  (-0.35609, 0.21854, 0.080944, ..., -0.35413, 0.38511, -0.070976)`\nThe individual dimensions in these vectors typically have no inherent meaning.\nInstead, it's the overall patterns of location and distance between vectors\nthat machine learning takes advantage of.\nEmbeddings are important for input to machine learning. Classifiers, and neural\nnetworks more generally, work on vectors of real numbers. They train best on\ndense vectors, where all values contribute to define an object. However, many\nimportant inputs to machine learning, such as words of text, do not have a\nnatural vector representation. Embedding functions are the standard and\neffective way to transform such discrete input objects into useful\ncontinuous vectors.\nEmbeddings are also valuable as outputs of machine learning. Because embeddings\nmap objects to vectors, applications can use similarity in vector space (for\ninstance, Euclidean distance or the angle between vectors) as a robust and\nflexible measure of object similarity. One common use is to find nearest\nneighbors.  Using the same word embeddings as above, for instance, here are the\nthree nearest neighbors for each word and the corresponding angles:\n`blue:  (red, 47.6\u00b0), (yellow, 51.9\u00b0), (purple, 52.4\u00b0)\nblues:  (jazz, 53.3\u00b0), (folk, 59.1\u00b0), (bluegrass, 60.6\u00b0)\norange:  (yellow, 53.5\u00b0), (colored, 58.0\u00b0), (bright, 59.9\u00b0)\noranges:  (apples, 45.3\u00b0), (lemons, 48.3\u00b0), (mangoes, 50.4\u00b0)`\nThis would tell an application that apples and oranges are in some way more\nsimilar (45.3\u00b0 apart) than lemons and oranges (48.3\u00b0 apart).\nEmbeddings in TensorFlow\nTo create word embeddings in TensorFlow, we first split the text into words\nand then assign an integer to every word in the vocabulary. Let us assume that\nthis has already been done, and that `word_ids` is a vector of these integers.\nFor example, the sentence \u201cI have a cat.\u201d could be split into\n`[\u201cI\u201d, \u201chave\u201d, \u201ca\u201d, \u201ccat\u201d, \u201c.\u201d]` and then the corresponding `word_ids` tensor\nwould have shape `[5]` and consist of 5 integers. To map these word ids\nto vectors, we need to create the embedding variable and use the\n`tf.nn.embedding_lookup` function as follows:\n`word_embeddings = tf.get_variable(\u201cword_embeddings\u201d,\n    [vocabulary_size, embedding_size])\nembedded_word_ids = tf.nn.embedding_lookup(word_embeddings, word_ids)`\nAfter this, the tensor `embedded_word_ids` will have shape `[5, embedding_size]`\nin our example and contain the embeddings (dense vectors) for each of the 5\nwords. At the end of training, `word_embeddings` will contain the embeddings\nfor all words in the vocabulary.\nEmbeddings can be trained in many network types, and with various loss\nfunctions and data sets. For example, one could use a recurrent neural network\nto predict the next word from the previous one given a large corpus of\nsentences, or one could train two networks to do multi-lingual translation.\nThese methods are described in the Vector Representations of Words\ntutorial.\nVisualizing Embeddings\nTensorBoard includes the Embedding Projector, a tool that lets you\ninteractively visualize embeddings. This tool can read embeddings from your\nmodel and render them in two or three dimensions.\nThe Embedding Projector has three panels:\n\nData panel on the top left, where you can choose the run, the embedding\n  variable and data columns to color and label points by.\nProjections panel on the bottom left, where you can choose the type of\n  projection.\nInspector panel on the right side, where you can search for particular\n  points and see a list of nearest neighbors.\n\nProjections\nThe Embedding Projector provides three ways to reduce the dimensionality of a\ndata set.\n\n\nt-SNE:\n  a nonlinear nondeterministic algorithm (T-distributed stochastic neighbor\n  embedding) that tries to preserve local neighborhoods in the data, often at\n  the expense of distorting global structure. You can choose whether to compute\n  two- or three-dimensional projections.\n\n\nPCA:\n  a linear deterministic algorithm (principal component analysis) that tries to\n  capture as much of the data variability in as few dimensions as possible. PCA\n  tends to highlight large-scale structure in the data, but can distort local\n  neighborhoods. The Embedding Projector computes the top 10 principal\n  components, from which you can choose two or three to view.\n\n\nCustom: a linear projection onto horizontal and vertical axes that you\n  specify using labels in the data. You define the horizontal axis, for\n  instance, by giving text patterns for \"Left\" and \"Right\". The Embedding\n  Projector finds all points whose label matches the \"Left\" pattern and\n  computes the centroid of that set; similarly for \"Right\".  The line passing\n  through these two centroids defines the horizontal axis. The vertical axis is\n  likewise computed from the centroids for points matching the \"Up\" and \"Down\"\n  text patterns.\n\n\nFurther useful articles are\nHow to Use t-SNE Effectively and\nPrincipal Component Analysis Explained Visually.\nExploration\nYou can explore visually by zooming, rotating, and panning using natural\nclick-and-drag gestures. Hovering your mouse over a point will show any\nmetadata for that point.  You can also inspect nearest-neighbor\nsubsets.  Clicking on a point causes the right pane to list the nearest\nneighbors, along with distances to the current point. The nearest-neighbor\npoints are also highlighted in the projection.\nIt is sometimes useful to restrict the view to a subset of points and perform\nprojections only on those points. To do so, you can select points in multiple\nways:\n\nAfter clicking on a point, its nearest neighbors are also selected.\nAfter a search, the points matching the query are selected.\nEnabling selection, clicking on a point and dragging defines a selection\n  sphere.\n\nThen click the \"Isolate nnn points\" button at the top of the Inspector pane\non the right hand side. The following image shows 101 points selected and ready\nfor the user to click \"Isolate 101 points\":\n\nSelection of the nearest neighbors of \u201cimportant\u201d in a word embedding dataset.\nAdvanced tip: filtering with custom projection can be powerful. Below, we\nfiltered the 100 nearest neighbors of \u201cpolitics\u201d and projected them onto the\n\u201cworst\u201d - \u201cbest\u201d vector as an x axis. The y axis is random. As a result, one\nfinds on the right side \u201cideas\u201d, \u201cscience\u201d, \u201cperspective\u201d, \u201cjournalism\u201d but on\nthe left \u201ccrisis\u201d, \u201cviolence\u201d and \u201cconflict\u201d.\n\n\n\n\n\n\n\n\n\n\n\n      Custom projection controls.\n    \n\n      Custom projection of neighbors of \"politics\" onto \"best\" - \"worst\" vector.\n    \n\n\nTo share your findings, you can use the bookmark panel in the bottom right\ncorner and save the current state (including computed coordinates of any\nprojection) as a small file. The Projector can then be pointed to a set of one\nor more of these files, producing the panel below. Other users can then walk\nthrough a sequence of bookmarks.\n\nMetadata\nIf you are working with an embedding, you'll probably want to attach\nlabels/images to the data points. You can do this by generating a metadata file\ncontaining the labels for each point and clicking \"Load data\" in the data panel\nof the Embedding Projector. The metadata can be either labels or images that\nare stored in a separate file.\nFor labels metadata, the format should\nbe a TSV file\n(tab characters shown in red) where the first line contains column headers\n(shown in bold) and subsequent lines contain the metadata values. For example:\n`\nWord\\tFrequency\n  Airplane\\t345\n  Car\\t241\n  ...\n`\nThe order of lines in the metadata file is assumed to match the order of\nvectors in the embedding variable, except for the header.  Consequently, the\n(i+1)-th line in the metadata file corresponds to the i-th row of the embedding\nvariable.  If the TSV metadata file has only a single column, then we don\u2019t\nexpect a header row, and assume each row is the label of the embedding. We\ninclude this exception because it matches the commonly-used \"vocab file\"\nformat.\nTo use images as metadata, create a single sprite image consisting of small\nthumbnails\u2014one for each vector in the embedding. A sprite image stores\nthumbnails for all the sample images in a single large image. The sprite should\nstore thumbnails in row-first order: the first data point placed in the top left\nand the last data point in the bottom right, though the last row doesn't have to\nbe filled, as shown below.\n\n\n0\n1\n2\n\n\n3\n4\n5\n\n\n6\n7\n\n\n\nFollow this link\nto see a fun example of thumbnail images in the Embedding Projector.\nMini-FAQ\nIs \"embedding\" an action or a thing?\nBoth. People talk about embedding words in a vector space (action) and about\nproducing word embeddings (things).  Common to both is the notion of embedding\nas a mapping from discrete objects to vectors. Creating or applying that\nmapping is an action, but the mapping itself is a thing.\nAre embeddings high-dimensional or low-dimensional?\nIt depends. A 300-dimensional vector space of words and phrases, for instance,\nis often called low-dimensional (and dense) when compared to the millions of\nwords and phrases it can contain. But mathematically it is high-dimensional,\ndisplaying many properties that are dramatically different from what our human\nintuition has learned about 2- and 3-dimensional spaces.\nIs an embedding the same as an embedding layer?\nNo. An embedding layer is a part of neural network, but an embedding is a more",
    "tag": "tensorflow"
  },
  {
    "title": "TensorFlow Version Compatibility",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/guide/version_compat.md",
    "content": "TensorFlow Version Compatibility\nThis document is for users who need backwards compatibility across different\nversions of TensorFlow (either for code or data), and for developers who want\nto modify TensorFlow while preserving compatibility.\nSemantic Versioning 2.0\nTensorFlow follows Semantic Versioning 2.0 (semver) for its\npublic API. Each release version of TensorFlow has the form `MAJOR.MINOR.PATCH`.\nFor example, TensorFlow version 1.2.3 has `MAJOR` version 1, `MINOR` version 2,\nand `PATCH` version 3. Changes to each number have the following meaning:\n\n\nMAJOR:  Potentially backwards incompatible changes.  Code and data that\n  worked with a previous major release will not necessarily work with the new\n  release. However, in some cases existing TensorFlow graphs and checkpoints\n  may be migratable to the newer release; see\n  Compatibility of graphs and checkpoints\n  for details on data compatibility.\n\n\nMINOR: Backwards compatible features, speed improvements, etc.  Code and\n  data that worked with a previous minor release and which depends only on the\n  public API will continue to work unchanged.  For details on what is and is\n  not the public API, see What is covered.\n\n\nPATCH: Backwards compatible bug fixes.\n\n\nFor example, release 1.0.0 introduced backwards incompatible changes from\nrelease 0.12.1.  However, release 1.1.1 was backwards compatible with release\n1.0.0.\nWhat is covered\nOnly the public APIs of TensorFlow are backwards compatible across minor and\npatch versions.  The public APIs consist of\n\nAll the documented Python functions and classes in the\n  `tensorflow` module and its submodules, except for\nfunctions and classes in `tf.contrib`\nfunctions and classes whose names start with `_` (as these are private)\nfunctions, arguments, properties and classes whose name starts with\n  `experimental`, or whose fully qualified name includes a module called\n  `experimental`\n  Note that the code in the `examples/` and `tools/` directories is not\n  reachable through the `tensorflow` Python module and is thus not covered by\n  the compatibility guarantee.\n\n\n\nIf a symbol is available through the `tensorflow` Python module or its\n  submodules, but is not documented, then it is not considered part of the\n  public API.\n\n\nThe C API.\n\n\nThe following protocol buffer files:\n\nattr_value\nconfig\nevent\ngraph\nop_def\nreader_base\nsummary\ntensor\ntensor_shape\ntypes\n\n\n\n\nWhat is not covered\nSome API functions are explicitly marked as \"experimental\" and can change in\nbackward incompatible ways between minor releases. These include:\n\n\nExperimental APIs: The `tf.contrib` module and its submodules in Python\n    and any functions in the C API or fields in protocol buffers that are\n    explicitly commented as being experimental. In particular, any field in a\n    protocol buffer which is called \"experimental\" and all its fields and\n    submessages can change at any time.\n\n\nOther languages: TensorFlow APIs in languages other than Python and C,\n    such as:\n\n\nC++ (exposed through header files in\n    tensorflow/cc).\n\nJava,\nGo\n\nJavaScript\n\n\nDetails of composite ops: Many public functions in Python expand to\n    several primitive ops in the graph, and these details will be part of any\n    graphs saved to disk as `GraphDef`s. These details may change for\n    minor releases. In particular, regressions tests that check for exact\n    matching between graphs are likely to break across minor releases, even\n    though the behavior of the graph should be unchanged and existing\n    checkpoints will still work.\n\n\nFloating point numerical details: The specific floating point values\n    computed by ops may change at any time.  Users should rely only on\n    approximate accuracy and numerical stability, not on the specific bits\n    computed. Changes to numerical formulas in minor and patch releases should\n    result in comparable or improved accuracy, with the caveat that in machine\n    learning improved accuracy of specific formulas may result in decreased\n    accuracy for the overall system.\n\n\nRandom numbers: The specific random numbers computed by the\n    random ops may change at any time.\n    Users should rely only on approximately correct distributions and\n    statistical strength, not the specific bits computed. However, we will make\n    changes to random bits rarely (or perhaps never) for patch releases.  We\n    will, of course, document all such changes.\n\n\nVersion skew in distributed Tensorflow: Running two different versions\n    of TensorFlow in a single cluster is unsupported. There are no guarantees\n    about backwards compatibility of the wire protocol.\n\n\nBugs: We reserve the right to make backwards incompatible behavior\n    (though not API) changes if the current implementation is clearly broken,\n    that is, if it contradicts the documentation or if a well-known and\n    well-defined intended behavior is not properly implemented due to a bug.\n    For example, if an optimizer claims to implement a well-known optimization\n    algorithm but does not match that algorithm due to a bug, then we will fix\n    the optimizer. Our fix may break code relying on the wrong behavior for\n    convergence. We will note such changes in the release notes.\n\n\nError messages: We reserve the right to change the text of error\n    messages. In addition, the type of an error may change unless the type is\n    specified in the documentation. For example, a function documented to\n    raise an `InvalidArgument` exception will continue to\n    raise `InvalidArgument`, but the human-readable message contents can change.\n\n\nCompatibility of graphs and checkpoints\nYou'll sometimes need to preserve graphs and checkpoints.\nGraphs describe the data flow of ops to be run during training and\ninference, and checkpoints contain the saved tensor values of variables in a\ngraph.\nMany TensorFlow users save graphs and trained models to disk for\nlater evaluation or additional training, but end up running their saved graphs\nor models on a later release. In compliance with semver, any graph or checkpoint\nwritten out with one version of TensorFlow can be loaded and evaluated with a\nlater version of TensorFlow with the same major release.  However, we will\nendeavor to preserve backwards compatibility even across major releases when\npossible, so that the serialized files are usable over long periods of time.\nGraphs are serialized via the `GraphDef` protocol buffer.  To facilitate (rare)\nbackwards incompatible changes to graphs, each `GraphDef` has a version number\nseparate from the TensorFlow version.  For example, `GraphDef` version 17\ndeprecated the `inv` op in favor of `reciprocal`.  The semantics are:\n\n\nEach version of TensorFlow supports an interval of `GraphDef` versions.  This\n  interval will be constant across patch releases, and will only grow across\n  minor releases.  Dropping support for a `GraphDef` version will only occur\n  for a major release of TensorFlow.\n\n\nNewly created graphs are assigned the latest `GraphDef` version number.\n\n\nIf a given version of TensorFlow supports the `GraphDef` version of a graph,\n  it will load and evaluate with the same behavior as the TensorFlow version\n  used to generate it (except for floating point numerical details and random\n  numbers), regardless of the major version of TensorFlow.  In particular, all\n  checkpoint files will be compatible.\n\n\nIf the `GraphDef` upper bound is increased to X in a (minor) release, there\n  will be at least six months before the lower bound is increased to X.  For\n  example (we're using hypothetical version numbers here):\n\nTensorFlow 1.2 might support `GraphDef` versions 4 to 7.\nTensorFlow 1.3 could add `GraphDef` version 8 and support versions 4 to 8.\nAt least six months later, TensorFlow 2.0.0 could drop support for\n  versions 4 to 7, leaving version 8 only.\n\n\n\nFinally, when support for a `GraphDef` version is dropped, we will attempt to\nprovide tools for automatically converting graphs to a newer supported\n`GraphDef` version.\nGraph and checkpoint compatibility when extending TensorFlow\nThis section is relevant only when making incompatible changes to the `GraphDef`\nformat, such as when adding ops, removing ops, or changing the functionality\nof existing ops.  The previous section should suffice for most users.\n\nBackward and partial forward compatibility\nOur versioning scheme has three requirements:\n\nBackward compatibility to support loading graphs and checkpoints\n    created with older versions of TensorFlow.\nForward compatibility to support scenarios where the producer of a\n    graph or checkpoint is upgraded to a newer version of TensorFlow before\n    the consumer.\nEnable evolving TensorFlow in incompatible ways. For example, removing ops,\n    adding attributes, and removing attributes.\n\nNote that while the `GraphDef` version mechanism is separate from the TensorFlow\nversion, backwards incompatible changes to the `GraphDef` format are still\nrestricted by Semantic Versioning.  This means functionality can only be removed\nor changed between `MAJOR` versions of TensorFlow (such as `1.7` to `2.0`).\nAdditionally, forward compatibility is enforced within Patch releases (`1.x.1`\nto `1.x.2` for example).\nTo achieve backward and forward compatibility and to know when to enforce changes\nin formats, graphs and checkpoints have metadata that describes when they\nwere produced. The sections below detail the TensorFlow implementation and\nguidelines for evolving `GraphDef` versions.\nIndependent data version schemes\nThere are different data versions for graphs and checkpoints. The two data\nformats evolve at different rates from each other and also at different rates\nfrom TensorFlow. Both versioning systems are defined in\ncore/public/version.h.\nWhenever a new version is added, a note is added to the header detailing what\nchanged and the date.\nData, producers, and consumers\nWe distinguish between the following kinds of data version information:\n* producers: binaries that produce data.  Producers have a version\n  (`producer`) and a minimum consumer version that they are compatible with\n  (`min_consumer`).\n* consumers: binaries that consume data.  Consumers have a version\n  (`consumer`) and a minimum producer version that they are compatible with\n  (`min_producer`).\nEach piece of versioned data has a VersionDef\nversions\nfield which records the `producer` that made the data, the `min_consumer`\nthat it is compatible with, and a list of `bad_consumers` versions that are\ndisallowed.\nBy default, when a producer makes some data, the data inherits the producer's\n`producer` and `min_consumer` versions. `bad_consumers` can be set if specific\nconsumer versions are known to contain bugs and must be avoided. A consumer can\naccept a piece of data if the following are all true:\n\n`consumer` >= data's `min_consumer`\ndata's `producer` >= consumer's `min_producer`\n`consumer` not in data's `bad_consumers`\n\nSince both producers and consumers come from the same TensorFlow code base,\ncore/public/version.h\ncontains a main data version which is treated as either `producer` or\n`consumer` depending on context and both `min_consumer` and `min_producer`\n(needed by producers and consumers, respectively). Specifically,\n\nFor `GraphDef` versions, we have `TF_GRAPH_DEF_VERSION`,\n    `TF_GRAPH_DEF_VERSION_MIN_CONSUMER`, and\n    `TF_GRAPH_DEF_VERSION_MIN_PRODUCER`.\nFor checkpoint versions, we have `TF_CHECKPOINT_VERSION`,\n    `TF_CHECKPOINT_VERSION_MIN_CONSUMER`, and\n    `TF_CHECKPOINT_VERSION_MIN_PRODUCER`.\n\nAdd a new attribute with default to an existing op\nFollowing the guidance below gives you forward compatibility only if the set of\nops has not changed:\n\nIf forward compatibility is desired,  set `strip_default_attrs` to `True`\n   while exporting the model using either the\n   `tf.saved_model.builder.SavedModelBuilder.add_meta_graph_and_variables`\n   and `tf.saved_model.builder.SavedModelBuilder.add_meta_graph`\n   methods of the `SavedModelBuilder` class, or\n   `tf.estimator.Estimator.export_savedmodel`\nThis strips off the default valued attributes at the time of\n   producing/exporting the models. This makes sure that the exported\n   `tf.MetaGraphDef` does not contain the new op-attribute when the default\n   value is used.\nHaving this control could allow out-of-date consumers (for example, serving\n   binaries that lag behind training binaries) to continue loading the models\n   and prevent interruptions in model serving.\n\nEvolving GraphDef versions\nThis section explains how to use this versioning mechanism to make different\ntypes of changes to the `GraphDef` format.\nAdd an op\nAdd the new op to both consumers and producers at the same time, and do not\nchange any `GraphDef` versions. This type of change is automatically\nbackward compatible, and does not impact forward compatibility plan since\nexisting producer scripts will not suddenly use the new functionality.\nAdd an op and switch existing Python wrappers to use it\n\nImplement new consumer functionality and increment the `GraphDef` version.\nIf it is possible to make the wrappers use the new functionality only in\n    cases that did not work before, the wrappers can be updated now.\nChange Python wrappers to use the new functionality. Do not increment\n    `min_consumer`, since models that do not use this op should not break.\n\nRemove or restrict an op's functionality\n\nFix all producer scripts (not TensorFlow itself) to not use the banned op or\n    functionality.\nIncrement the `GraphDef` version and implement new consumer functionality\n    that bans the removed op or functionality for GraphDefs at the new version\n    and above. If possible, make TensorFlow stop producing `GraphDefs` with the\n    banned functionality. To do so, add the\n    REGISTER_OP(...).Deprecated(deprecated_at_version,\n    message).\nWait for a major release for backward compatibility purposes.\nIncrease `min_producer` to the GraphDef version from (2) and remove the\n    functionality entirely.\n\nChange an op's functionality\n\nAdd a new similar op named `SomethingV2` or similar and go through the\n    process of adding it and switching existing Python wrappers to use it.\n    To ensure forward compatibility use the checks suggested in\n    compat.py\n    when changing the Python wrappers.\nRemove the old op (Can only take place with a major version change due to\n    backward compatibility).\nIncrease `min_consumer` to rule out consumers with the old op, add back the\n    old op as an alias for `SomethingV2`, and go through the process to switch\n    existing Python wrappers to use it.\nGo through the process to remove `SomethingV2`.\n\nBan a single unsafe consumer version\n\nBump the `GraphDef` version and add the bad version to `bad_consumers` for\n    all new GraphDefs. If possible, add to `bad_consumers` only for GraphDefs\n    which contain a certain op or similar.\nIf existing consumers have the bad version, push them out as soon as\n",
    "tag": "tensorflow"
  },
  {
    "title": "Introduction",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/guide/low_level_intro.md",
    "content": "Introduction\nThis guide gets you started programming in the low-level TensorFlow APIs\n(TensorFlow Core), showing you how to:\n\nManage your own TensorFlow program (a `tf.Graph`) and TensorFlow\n    runtime (a `tf.Session`), instead of relying on Estimators to manage them.\nRun TensorFlow operations, using a `tf.Session`.\nUse high level components (datasets, layers, and\n    feature_columns) in this low level environment.\nBuild your own training loop, instead of using the one\n    provided by Estimators.\n\nWe recommend using the higher level APIs to build models when possible.\nKnowing TensorFlow Core is valuable for the following reasons:\n\nExperimentation and debugging are both more straight forward\n    when you can use low level TensorFlow operations directly.\nIt gives you a mental model of how things work internally when\n    using the higher level APIs.\n\nSetup\nBefore using this guide, install TensorFlow.\nTo get the most out of this guide, you should know the following:\n\nHow to program in Python.\nAt least a little bit about arrays.\nIdeally, something about machine learning.\n\nFeel free to launch `python` and follow along with this walkthrough.\nRun the following lines to set up your Python environment:\n```python\nfrom future import absolute_import\nfrom future import division\nfrom future import print_function\nimport numpy as np\nimport tensorflow as tf\n```\nTensor Values\nThe central unit of data in TensorFlow is the tensor. A tensor consists of a\nset of primitive values shaped into an array of any number of dimensions. A\ntensor's rank is its number of dimensions, while its shape is a tuple\nof integers specifying the array's length along each dimension. Here are some\nexamples of tensor values:\n`python\n3. # a rank 0 tensor; a scalar with shape [],\n[1., 2., 3.] # a rank 1 tensor; a vector with shape [3]\n[[1., 2., 3.], [4., 5., 6.]] # a rank 2 tensor; a matrix with shape [2, 3]\n[[[1., 2., 3.]], [[7., 8., 9.]]] # a rank 3 tensor with shape [2, 1, 3]`\nTensorFlow uses numpy arrays to represent tensor values.\nTensorFlow Core Walkthrough\nYou might think of TensorFlow Core programs as consisting of two discrete\nsections:\n\nBuilding the computational graph (a `tf.Graph`).\nRunning the computational graph (using a `tf.Session`).\n\nGraph\nA computational graph is a series of TensorFlow operations arranged into a\ngraph. The graph is composed of two types of objects.\n\n`tf.Operation` (or \"ops\"): The nodes of the graph.\n    Operations describe calculations that consume and produce tensors.\n`tf.Tensor`: The edges in the graph. These represent the values\n    that will flow through the graph. Most TensorFlow functions return\n    `tf.Tensors`.\n\nImportant: `tf.Tensors` do not have values, they are just handles to elements\nin the computation graph.\nLet's build a simple computational graph. The most basic operation is a\nconstant. The Python function that builds the operation takes a tensor value as\ninput. The resulting operation takes no inputs. When run, it outputs the\nvalue that was passed to the constructor. We can create two floating point\nconstants `a` and `b` as follows:\n`python\na = tf.constant(3.0, dtype=tf.float32)\nb = tf.constant(4.0) # also tf.float32 implicitly\ntotal = a + b\nprint(a)\nprint(b)\nprint(total)`\nThe print statements produce:\n`Tensor(\"Const:0\", shape=(), dtype=float32)\nTensor(\"Const_1:0\", shape=(), dtype=float32)\nTensor(\"add:0\", shape=(), dtype=float32)`\nNotice that printing the tensors does not output the values `3.0`, `4.0`, and\n`7.0` as you might expect. The above statements only build the computation\ngraph. These `tf.Tensor` objects just represent the results of the operations\nthat will be run.\nEach operation in a graph is given a unique name. This name is independent of\nthe names the objects are assigned to in Python. Tensors are named after the\noperation that produces them followed by an output index, as in\n`\"add:0\"` above.\nTensorBoard\nTensorFlow provides a utility called TensorBoard. One of TensorBoard's many\ncapabilities is visualizing a computation graph. You can easily do this with\na few simple commands.\nFirst you save the computation graph to a TensorBoard summary file as\nfollows:\n`writer = tf.summary.FileWriter('.')\nwriter.add_graph(tf.get_default_graph())\nwriter.flush()`\nThis will produce an `event` file in the current directory with a name in the\nfollowing format:\n`events.out.tfevents.{timestamp}.{hostname}`\nNow, in a new terminal, launch TensorBoard with the following shell command:\n`bsh\ntensorboard --logdir .`\nThen open TensorBoard's graphs page in your\nbrowser, and you should see a graph similar to the following:\n\nFor more about TensorBoard's graph visualization tools see TensorBoard: Graph Visualization.\nSession\nTo evaluate tensors, instantiate a `tf.Session` object, informally known as a\nsession. A session encapsulates the state of the TensorFlow runtime, and\nruns TensorFlow operations. If a `tf.Graph` is like a `.py` file, a `tf.Session`\nis like the `python` executable.\nThe following code creates a `tf.Session` object and then invokes its `run`\nmethod to evaluate the `total` tensor we created above:\n`python\nsess = tf.Session()\nprint(sess.run(total))`\nWhen you request the output of a node with `Session.run` TensorFlow backtracks\nthrough the graph and runs all the nodes that provide input to the requested\noutput node. So this prints the expected value of 7.0:\n`7.0`\nYou can pass multiple tensors to `tf.Session.run`. The `run` method\ntransparently handles any combination of tuples or dictionaries, as in the\nfollowing example:\n`python\nprint(sess.run({'ab':(a, b), 'total':total}))`\nwhich returns the results in a structure of the same layout:\n\n{'total': 7.0, 'ab': (3.0, 4.0)}\n\nDuring a call to `tf.Session.run` any `tf.Tensor` only has a single value.\nFor example, the following code calls `tf.random_uniform` to produce a\n`tf.Tensor` that generates a random 3-element vector (with values in `[0,1)`):\n`python\nvec = tf.random_uniform(shape=(3,))\nout1 = vec + 1\nout2 = vec + 2\nprint(sess.run(vec))\nprint(sess.run(vec))\nprint(sess.run((out1, out2)))`\nThe result shows a different random value on each call to `run`, but\na consistent value during a single `run` (`out1` and `out2` receive the same\nrandom input):\n`[ 0.52917576  0.64076328  0.68353939]\n[ 0.66192627  0.89126778  0.06254101]\n(\n  array([ 1.88408756,  1.87149239,  1.84057522], dtype=float32),\n  array([ 2.88408756,  2.87149239,  2.84057522], dtype=float32)\n)`\nSome TensorFlow functions return `tf.Operations` instead of `tf.Tensors`.\nThe result of calling `run` on an Operation is `None`. You run an operation\nto cause a side-effect, not to retrieve a value. Examples of this include the\ninitialization, and training ops\ndemonstrated later.\nFeeding\nAs it stands, this graph is not especially interesting because it always\nproduces a constant result. A graph can be parameterized to accept external\ninputs, known as placeholders. A placeholder is a promise to provide a\nvalue later, like a function argument.\n`python\nx = tf.placeholder(tf.float32)\ny = tf.placeholder(tf.float32)\nz = x + y`\nThe preceding three lines are a bit like a function in which we\ndefine two input parameters (`x` and `y`) and then an operation on them. We can\nevaluate this graph with multiple inputs by using the `feed_dict` argument of\nthe `tf.Session.run` method to feed concrete values to the placeholders:\n`python\nprint(sess.run(z, feed_dict={x: 3, y: 4.5}))\nprint(sess.run(z, feed_dict={x: [1, 3], y: [2, 4]}))`\nThis results in the following output:\n`7.5\n[ 3.  7.]`\nAlso note that the `feed_dict` argument can be used to overwrite any tensor in\nthe graph. The only difference between placeholders and other `tf.Tensors` is\nthat placeholders throw an error if no value is fed to them.\nDatasets\nPlaceholders work for simple experiments, but `tf.data` are the\npreferred method of streaming data into a model.\nTo get a runnable `tf.Tensor` from a Dataset you must first convert it to a\n`tf.data.Iterator`, and then call the Iterator's\n`tf.data.Iterator.get_next` method.\nThe simplest way to create an Iterator is with the\n`tf.data.Dataset.make_one_shot_iterator` method.\nFor example, in the following code the `next_item` tensor will return a row from\nthe `my_data` array on each `run` call:\n`python\nmy_data = [\n    [0, 1,],\n    [2, 3,],\n    [4, 5,],\n    [6, 7,],\n]\nslices = tf.data.Dataset.from_tensor_slices(my_data)\nnext_item = slices.make_one_shot_iterator().get_next()`\nReaching the end of the data stream causes `Dataset` to throw an\n`tf.errors.OutOfRangeError`. For example, the following code\nreads the `next_item` until there is no more data to read:\n`python\nwhile True:\n  try:\n    print(sess.run(next_item))\n  except tf.errors.OutOfRangeError:\n    break`\nIf the `Dataset` depends on stateful operations you may need to\ninitialize the iterator before using it, as shown below:\n``` python\nr = tf.random_normal([10,3])\ndataset = tf.data.Dataset.from_tensor_slices(r)\niterator = dataset.make_initializable_iterator()\nnext_row = iterator.get_next()\nsess.run(iterator.initializer)\nwhile True:\n  try:\n    print(sess.run(next_row))\n  except tf.errors.OutOfRangeError:\n    break\n```\nFor more details on Datasets and Iterators see: Importing Data.\nLayers\nA trainable model must modify the values in the graph to get new outputs with\nthe same input.  `tf.layers` are the preferred way to add trainable\nparameters to a graph.\nLayers package together both the variables and the operations that act\non them. For example a\ndensely-connected layer\nperforms a weighted sum across all inputs\nfor each output and applies an optional\nactivation function.\nThe connection weights and biases are managed by the layer object.\nCreating Layers\nThe following code creates a `tf.layers.Dense` layer that takes a\nbatch of input vectors, and produces a single output value for each. To apply a\nlayer to an input, call the layer as if it were a function. For example:\n`python\nx = tf.placeholder(tf.float32, shape=[None, 3])\nlinear_model = tf.layers.Dense(units=1)\ny = linear_model(x)`\nThe layer inspects its input to determine sizes for its internal variables. So\nhere we must set the shape of the `x` placeholder so that the layer can\nbuild a weight matrix of the correct size.\nNow that we have defined the calculation of the output, `y`, there is one more\ndetail we need to take care of before we run the calculation.\nInitializing Layers\nThe layer contains variables that must be initialized before they can be\nused. While it is possible to initialize variables individually, you can easily\ninitialize all the variables in a TensorFlow graph as follows:\n`python\ninit = tf.global_variables_initializer()\nsess.run(init)`\nImportant: Calling `tf.global_variables_initializer` only\ncreates and returns a handle to a TensorFlow operation. That op\nwill initialize all the global variables when we run it with `tf.Session.run`.\nAlso note that this `global_variables_initializer` only initializes variables\nthat existed in the graph when the  initializer was created. So the initializer\nshould be one of the last things added during graph construction.\nExecuting Layers\nNow that the layer is initialized, we can evaluate the `linear_model`'s output\ntensor as we would any other tensor. For example, the following code:\n`python\nprint(sess.run(y, {x: [[1, 2, 3],[4, 5, 6]]}))`\nwill generate a two-element output vector such as the following:\n`[[-3.41378999]\n [-9.14999008]]`\nLayer Function shortcuts\nFor each layer class (like `tf.layers.Dense`) TensorFlow also supplies a\nshortcut function (like `tf.layers.dense`). The only difference is that the\nshortcut function versions create and run the layer in a single call. For\nexample, the following code is equivalent to the earlier version:\n```python\nx = tf.placeholder(tf.float32, shape=[None, 3])\ny = tf.layers.dense(x, units=1)\ninit = tf.global_variables_initializer()\nsess.run(init)\nprint(sess.run(y, {x: [[1, 2, 3], [4, 5, 6]]}))\n```\nWhile convenient, this approach allows no access to the `tf.layers.Layer`\nobject. This makes introspection and debugging more difficult,\nand layer reuse impossible.\nFeature columns\nThe easiest way to experiment with feature columns is using the\n`tf.feature_column.input_layer` function. This function only accepts\ndense columns as inputs, so to view the result\nof a categorical column you must wrap it in an\n`tf.feature_column.indicator_column`. For example:\n``` python\nfeatures = {\n    'sales' : [[5], [10], [8], [9]],\n    'department': ['sports', 'sports', 'gardening', 'gardening']}\ndepartment_column = tf.feature_column.categorical_column_with_vocabulary_list(\n        'department', ['sports', 'gardening'])\ndepartment_column = tf.feature_column.indicator_column(department_column)\ncolumns = [\n    tf.feature_column.numeric_column('sales'),\n    department_column\n]\ninputs = tf.feature_column.input_layer(features, columns)\n```\nRunning the `inputs` tensor will parse the `features` into a batch of vectors.\nFeature columns can have internal state, like layers, so they often need to be\ninitialized. Categorical columns use `tf.contrib.lookup`\ninternally and these require a separate initialization op,\n`tf.tables_initializer`.\n`python\nvar_init = tf.global_variables_initializer()\ntable_init = tf.tables_initializer()\nsess = tf.Session()\nsess.run((var_init, table_init))`\nOnce the internal state has been initialized you can run `inputs` like any\nother `tf.Tensor`:\n`python\nprint(sess.run(inputs))`\nThis shows how the feature columns have packed the input vectors, with the\none-hot \"department\" as the first two indices and \"sales\" as the third.\n\n[[  1.   0.   5.]\n [  1.   0.  10.]\n [  0.   1.   8.]\n [  0.   1.   9.]]\n\nTraining\nNow that you're familiar with the basics of core TensorFlow, let's train a\nsmall regression model manually.\nDefine the data\nFirst let's define some inputs, `x`, and the expected output for each input,\n`y_true`:\n`python\nx = tf.constant([[1], [2], [3], [4]], dtype=tf.float32)\ny_true = tf.constant([[0], [-1], [-2], [-3]], dtype=tf.float32)`\nDefine the model\nNext, build a simple linear model, with 1 output:\n``` python\nlinear_model = tf.layers.Dense(units=1)\ny_pred = linear_model(x)\n```\nYou can evaluate the predictions as follows:\n``` python\nsess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run(init)\nprint(sess.run(y_pred))\n```\nThe model hasn't yet been trained, so the four \"predicted\" values aren't very\ngood. Here's what we got; your own output will almost certainly differ:\n\n[[ 0.02631879]\n [ 0.05263758]\n [ 0.07895637]\n [ 0.10527515]]\n\nLoss\nTo optimize a model, you first need to define the loss. We'll use the mean\nsquare error, a standard loss for regression problems.\nWhile you could do this manually with lower level math operations,\nthe `tf.losses` module provides a set of common loss functions. You can use it\nto calculate the mean square error as follows:\n``` python\nloss = tf.losses.mean_squared_error(labels=y_true, predictions=y_pred)\nprint(sess.run(loss))\n```\nThis will produce a loss value, something like:\n\n2.23962\n\nTraining\nTensorFlow provides\noptimizers\nimplementing standard optimization algorithms. These are implemented as\nsub-classes of `tf.train.Optimizer`. They incrementally change each\nvariable in order to minimize the loss. The simplest optimization algorithm is\ngradient descent,\nimplemented by `tf.train.GradientDescentOptimizer`. It modifies each\nvariable according to the magnitude of the derivative of loss with respect to\nthat variable. For example:\n`python\noptimizer = tf.train.GradientDescentOptimizer(0.01)\ntrain = optimizer.minimize(loss)`\nThis code builds all the graph components necessary for the optimization, and\nreturns a training operation. When run, the training op will update variables\nin the graph. You might run it as follows:\n`python\nfor i in range(100):\n  _, loss_value = sess.run((train, loss))\n  print(loss_value)`\nSince `train` is an op, not a tensor, it doesn't return a value when run.\nTo see the progression of the loss during training, we run the loss tensor at\nthe same time, producing output like the following:\n\n1.35659\n1.00412\n0.759167\n0.588829\n0.470264\n0.387626\n0.329918\n0.289511\n0.261112\n0.241046\n...\n\nComplete program\n```python\nx = tf.constant([[1], [2], [3], [4]], dtype=tf.float32)\ny_true = tf.constant([[0], [-1], [-2], [-3]], dtype=tf.float32)\nlinear_model = tf.layers.Dense(units=1)\ny_pred = linear_model(x)\nloss = tf.losses.mean_squared_error(labels=y_true, predictions=y_pred)\noptimizer = tf.train.GradientDescentOptimizer(0.01)\ntrain = optimizer.minimize(loss)\ninit = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init)\nfor i in range(100):\n  _, loss_value = sess.run((train, loss))\n  print(loss_value)\nprint(sess.run(y_pred))\n```\nNext steps\nTo learn more about building models with TensorFlow consider the following:\n\nCustom Estimators, to learn how to build\n  customized models with TensorFlow. Your knowledge of TensorFlow Core will\n  help you understand and debug your own models.\n\nIf you want to learn more about the inner workings of TensorFlow consider the\nfollowing documents, which go into more depth on many of the topics discussed\nhere:\n\nGraphs and Sessions\nTensors\n",
    "tag": "tensorflow"
  },
  {
    "title": "TensorBoard Histogram Dashboard",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/guide/tensorboard_histograms.md",
    "content": "TensorBoard Histogram Dashboard\nThe TensorBoard Histogram Dashboard displays how the distribution of some\n`Tensor` in your TensorFlow graph has changed over time. It does this by showing\nmany histograms visualizations of your tensor at different points in time.\nA Basic Example\nLet's start with a simple case: a normally-distributed variable, where the mean\nshifts over time.\nTensorFlow has an op\ntf.random_normal\nwhich is perfect for this purpose. As is usually the case with TensorBoard, we\nwill ingest data using a summary op; in this case,\n'tf.summary.histogram'.\nFor a primer on how summaries work, please see the\nTensorBoard.\nHere is a code snippet that will generate some histogram summaries containing\nnormally distributed data, where the mean of the distribution increases over\ntime.\n```python\nimport tensorflow as tf\nk = tf.placeholder(tf.float32)\nMake a normal distribution, with a shifting mean\nmean_moving_normal = tf.random_normal(shape=[1000], mean=(5*k), stddev=1)\nRecord that distribution into a histogram summary\ntf.summary.histogram(\"normal/moving_mean\", mean_moving_normal)\nSetup a session and summary writer\nsess = tf.Session()\nwriter = tf.summary.FileWriter(\"/tmp/histogram_example\")\nsummaries = tf.summary.merge_all()\nSetup a loop and write the summaries to disk\nN = 400\nfor step in range(N):\n  k_val = step/float(N)\n  summ = sess.run(summaries, feed_dict={k: k_val})\n  writer.add_summary(summ, global_step=step)\n```\nOnce that code runs, we can load the data into TensorBoard via the command line:\n`sh\ntensorboard --logdir=/tmp/histogram_example`\nOnce TensorBoard is running, load it in Chrome or Firefox and navigate to the\nHistogram Dashboard. Then we can see a histogram visualization for our normally\ndistributed data.\n\n`tf.summary.histogram` takes an arbitrarily sized and shaped Tensor, and\ncompresses it into a histogram data structure consisting of many bins with\nwidths and counts. For example, let's say we want to organize the numbers\n`[0.5, 1.1, 1.3, 2.2, 2.9, 2.99]` into bins. We could make three bins:\n* a bin\ncontaining everything from 0 to 1 (it would contain one element, 0.5),\n* a bin\ncontaining everything from 1-2 (it would contain two elements, 1.1 and 1.3),\n* a bin containing everything from 2-3 (it would contain three elements: 2.2,\n2.9 and 2.99).\nTensorFlow uses a similar approach to create bins, but unlike in our example, it\ndoesn't create integer bins. For large, sparse datasets, that might result in\nmany thousands of bins.\nInstead, the bins are exponentially distributed, with many bins close to 0 and\ncomparatively few bins for very large numbers.\nHowever, visualizing exponentially-distributed bins is tricky; if height is used\nto encode count, then wider bins take more space, even if they have the same\nnumber of elements. Conversely, encoding count in the area makes height\ncomparisons impossible. Instead, the histograms resample the data\ninto uniform bins. This can lead to unfortunate artifacts in some cases.\nEach slice in the histogram visualizer displays a single histogram.\nThe slices are organized by step;\nolder slices (e.g. step 0) are further \"back\" and darker, while newer slices\n(e.g. step 400) are close to the foreground, and lighter in color.\nThe y-axis on the right shows the step number.\nYou can mouse over the histogram to see tooltips with some more detailed\ninformation. For example, in the following image we can see that the histogram\nat timestep 176 has a bin centered at 2.25 with 177 elements in that bin.\n\nAlso, you may note that the histogram slices are not always evenly spaced in\nstep count or time. This is because TensorBoard uses\nreservoir sampling to keep a\nsubset of all the histograms, to save on memory. Reservoir sampling guarantees\nthat every sample has an equal likelihood of being included, but because it is\na randomized algorithm, the samples chosen don't occur at even steps.\nOverlay Mode\nThere is a control on the left of the dashboard that allows you to toggle the\nhistogram mode from \"offset\" to \"overlay\":\n\nIn \"offset\" mode, the visualization rotates 45 degrees, so that the individual\nhistogram slices are no longer spread out in time, but instead are all plotted\non the same y-axis.\n\nNow, each slice is a separate line on the chart, and the y-axis shows the item\ncount within each bucket. Darker lines are older, earlier steps, and lighter\nlines are more recent, later steps. Once again, you can mouse over the chart to\nsee some additional information.\n\nIn general, the overlay visualization is useful if you want to directly compare\nthe counts of different histograms.\nMultimodal Distributions\nThe Histogram Dashboard is great for visualizing multimodal\ndistributions. Let's construct a simple bimodal distribution by concatenating\nthe outputs from two different normal distributions. The code will look like\nthis:\n```python\nimport tensorflow as tf\nk = tf.placeholder(tf.float32)\nMake a normal distribution, with a shifting mean\nmean_moving_normal = tf.random_normal(shape=[1000], mean=(5*k), stddev=1)\nRecord that distribution into a histogram summary\ntf.summary.histogram(\"normal/moving_mean\", mean_moving_normal)\nMake a normal distribution with shrinking variance\nvariance_shrinking_normal = tf.random_normal(shape=[1000], mean=0, stddev=1-(k))\nRecord that distribution too\ntf.summary.histogram(\"normal/shrinking_variance\", variance_shrinking_normal)\nLet's combine both of those distributions into one dataset\nnormal_combined = tf.concat([mean_moving_normal, variance_shrinking_normal], 0)\nWe add another histogram summary to record the combined distribution\ntf.summary.histogram(\"normal/bimodal\", normal_combined)\nsummaries = tf.summary.merge_all()\nSetup a session and summary writer\nsess = tf.Session()\nwriter = tf.summary.FileWriter(\"/tmp/histogram_example\")\nSetup a loop and write the summaries to disk\nN = 400\nfor step in range(N):\n  k_val = step/float(N)\n  summ = sess.run(summaries, feed_dict={k: k_val})\n  writer.add_summary(summ, global_step=step)\n```\nYou already remember our \"moving mean\" normal distribution from the example\nabove. Now we also have a \"shrinking variance\" distribution. Side-by-side, they\nlook like this:\n\nWhen we concatenate them, we get a chart that clearly reveals the divergent,\nbimodal structure:\n\nSome more distributions\nJust for fun, let's generate and visualize a few more distributions, and then\ncombine them all into one chart. Here's the code we'll use:\n```python\nimport tensorflow as tf\nk = tf.placeholder(tf.float32)\nMake a normal distribution, with a shifting mean\nmean_moving_normal = tf.random_normal(shape=[1000], mean=(5*k), stddev=1)\nRecord that distribution into a histogram summary\ntf.summary.histogram(\"normal/moving_mean\", mean_moving_normal)\nMake a normal distribution with shrinking variance\nvariance_shrinking_normal = tf.random_normal(shape=[1000], mean=0, stddev=1-(k))\nRecord that distribution too\ntf.summary.histogram(\"normal/shrinking_variance\", variance_shrinking_normal)\nLet's combine both of those distributions into one dataset\nnormal_combined = tf.concat([mean_moving_normal, variance_shrinking_normal], 0)\nWe add another histogram summary to record the combined distribution\ntf.summary.histogram(\"normal/bimodal\", normal_combined)\nAdd a gamma distribution\ngamma = tf.random_gamma(shape=[1000], alpha=k)\ntf.summary.histogram(\"gamma\", gamma)\nAnd a poisson distribution\npoisson = tf.random_poisson(shape=[1000], lam=k)\ntf.summary.histogram(\"poisson\", poisson)\nAnd a uniform distribution\nuniform = tf.random_uniform(shape=[1000], maxval=k*10)\ntf.summary.histogram(\"uniform\", uniform)\nFinally, combine everything together!\nall_distributions = [mean_moving_normal, variance_shrinking_normal,\n                     gamma, poisson, uniform]\nall_combined = tf.concat(all_distributions, 0)\ntf.summary.histogram(\"all_combined\", all_combined)\nsummaries = tf.summary.merge_all()\nSetup a session and summary writer\nsess = tf.Session()\nwriter = tf.summary.FileWriter(\"/tmp/histogram_example\")\nSetup a loop and write the summaries to disk\nN = 400\nfor step in range(N):\n  k_val = step/float(N)\n  summ = sess.run(summaries, feed_dict={k: k_val})\n  writer.add_summary(summ, global_step=step)\n```\nGamma Distribution\n\nUniform Distribution\n\nPoisson Distribution\n\nThe poisson distribution is defined over the integers. So, all of the values\nbeing generated are perfect integers. The histogram compression moves the data\ninto floating-point bins, causing the visualization to show little\nbumps over the integer values rather than perfect spikes.\nAll Together Now\nFinally, we can concatenate all of the data into one funny-looking curve.\n",
    "tag": "tensorflow"
  },
  {
    "title": "Importing Data",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/guide/datasets.md",
    "content": "Importing Data\nThe `tf.data` API enables you to build complex input pipelines from\nsimple, reusable pieces. For example, the pipeline for an image model might\naggregate data from files in a distributed file system, apply random\nperturbations to each image, and merge randomly selected images into a batch\nfor training. The pipeline for a text model might involve extracting symbols\nfrom raw text data, converting them to embedding identifiers with a lookup\ntable, and batching together sequences of different lengths. The `tf.data` API\nmakes it easy to deal with large amounts of data, different data formats, and\ncomplicated transformations.\nThe `tf.data` API introduces two new abstractions to TensorFlow:\n\n\nA `tf.data.Dataset` represents a sequence of elements, in which\n  each element contains one or more `Tensor` objects. For example, in an image\n  pipeline, an element might be a single training example, with a pair of\n  tensors representing the image data and a label. There are two distinct\n  ways to create a dataset:\n\n\nCreating a source (e.g. `Dataset.from_tensor_slices()`) constructs a\ndataset from\none or more `tf.Tensor` objects.\n\n\nApplying a transformation (e.g. `Dataset.batch()`) constructs a dataset\nfrom one or more `tf.data.Dataset` objects.\n\n\n\n\nA `tf.data.Iterator` provides the main way to extract elements from a\n  dataset. The operation returned by `Iterator.get_next()` yields the next\n  element of a `Dataset` when executed, and typically acts as the interface\n  between input pipeline code and your model. The simplest iterator is a\n  \"one-shot iterator\", which is associated with a particular `Dataset` and\n  iterates through it once. For more sophisticated uses, the\n  `Iterator.initializer` operation enables you to reinitialize and parameterize\n  an iterator with different datasets, so that you can, for example, iterate\n  over training and validation data multiple times in the same program.\n\n\nBasic mechanics\nThis section of the guide describes the fundamentals of creating different kinds\nof `Dataset` and `Iterator` objects, and how to extract data from them.\nTo start an input pipeline, you must define a source. For example, to\nconstruct a `Dataset` from some tensors in memory, you can use\n`tf.data.Dataset.from_tensors()` or\n`tf.data.Dataset.from_tensor_slices()`. Alternatively, if your input\ndata are on disk in the recommended TFRecord format, you can construct a\n`tf.data.TFRecordDataset`.\nOnce you have a `Dataset` object, you can transform it into a new `Dataset` by\nchaining method calls on the `tf.data.Dataset` object. For example, you\ncan apply per-element transformations such as `Dataset.map()` (to apply a\nfunction to each element), and multi-element transformations such as\n`Dataset.batch()`. See the documentation for `tf.data.Dataset`\nfor a complete list of transformations.\nThe most common way to consume values from a `Dataset` is to make an\niterator object that provides access to one element of the dataset at a time\n(for example, by calling `Dataset.make_one_shot_iterator()`). A\n`tf.data.Iterator` provides two operations: `Iterator.initializer`,\nwhich enables you to (re)initialize the iterator's state; and\n`Iterator.get_next()`, which returns `tf.Tensor` objects that correspond to the\nsymbolic next element. Depending on your use case, you might choose a different\ntype of iterator, and the options are outlined below.\nDataset structure\nA dataset comprises elements that each have the same structure. An element\ncontains one or more `tf.Tensor` objects, called components. Each component\nhas a `tf.DType` representing the type of elements in the tensor, and a\n`tf.TensorShape` representing the (possibly partially specified) static shape of\neach element. The `Dataset.output_types` and `Dataset.output_shapes` properties\nallow you to inspect the inferred types and shapes of each component of a\ndataset element. The nested structure of these properties map to the structure\nof an element, which may be a single tensor, a tuple of tensors, or a nested\ntuple of tensors. For example:\n```python\ndataset1 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([4, 10]))\nprint(dataset1.output_types)  # ==> \"tf.float32\"\nprint(dataset1.output_shapes)  # ==> \"(10,)\"\ndataset2 = tf.data.Dataset.from_tensor_slices(\n   (tf.random_uniform([4]),\n    tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)))\nprint(dataset2.output_types)  # ==> \"(tf.float32, tf.int32)\"\nprint(dataset2.output_shapes)  # ==> \"((), (100,))\"\ndataset3 = tf.data.Dataset.zip((dataset1, dataset2))\nprint(dataset3.output_types)  # ==> (tf.float32, (tf.float32, tf.int32))\nprint(dataset3.output_shapes)  # ==> \"(10, ((), (100,)))\"\n```\nIt is often convenient to give names to each component of an element, for\nexample if they represent different features of a training example. In addition\nto tuples, you can use `collections.namedtuple` or a dictionary mapping strings\nto tensors to represent a single element of a `Dataset`.\n`python\ndataset = tf.data.Dataset.from_tensor_slices(\n   {\"a\": tf.random_uniform([4]),\n    \"b\": tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)})\nprint(dataset.output_types)  # ==> \"{'a': tf.float32, 'b': tf.int32}\"\nprint(dataset.output_shapes)  # ==> \"{'a': (), 'b': (100,)}\"`\nThe `Dataset` transformations support datasets of any structure. When using the\n`Dataset.map()`, `Dataset.flat_map()`, and `Dataset.filter()` transformations,\nwhich apply a function to each element, the element structure determines the\narguments of the function:\n```python\ndataset1 = dataset1.map(lambda x: ...)\ndataset2 = dataset2.flat_map(lambda x, y: ...)\nNote: Argument destructuring is not available in Python 3.\ndataset3 = dataset3.filter(lambda x, (y, z): ...)\n```\nCreating an iterator\nOnce you have built a `Dataset` to represent your input data, the next step is to\ncreate an `Iterator` to access elements from that dataset.  The `tf.data` API\ncurrently supports the following iterators, in increasing level of\nsophistication:\n\none-shot,\ninitializable,\nreinitializable, and\nfeedable.\n\nA one-shot iterator is the simplest form of iterator, which only supports\niterating once through a dataset, with no need for explicit initialization.\nOne-shot iterators handle almost all of the cases that the existing queue-based\ninput pipelines support, but they do not support parameterization. Using the\nexample of `Dataset.range()`:\n```python\ndataset = tf.data.Dataset.range(100)\niterator = dataset.make_one_shot_iterator()\nnext_element = iterator.get_next()\nfor i in range(100):\n  value = sess.run(next_element)\n  assert i == value\n```\nNote: Currently, one-shot iterators are the only type that is easily usable\nwith an `Estimator`.\nAn initializable iterator requires you to run an explicit\n`iterator.initializer` operation before using it. In exchange for this\ninconvenience, it enables you to parameterize the definition of the dataset,\nusing one or more `tf.placeholder()` tensors that can be fed when you\ninitialize the iterator. Continuing the `Dataset.range()` example:\n```python\nmax_value = tf.placeholder(tf.int64, shape=[])\ndataset = tf.data.Dataset.range(max_value)\niterator = dataset.make_initializable_iterator()\nnext_element = iterator.get_next()\nInitialize an iterator over a dataset with 10 elements.\nsess.run(iterator.initializer, feed_dict={max_value: 10})\nfor i in range(10):\n  value = sess.run(next_element)\n  assert i == value\nInitialize the same iterator over a dataset with 100 elements.\nsess.run(iterator.initializer, feed_dict={max_value: 100})\nfor i in range(100):\n  value = sess.run(next_element)\n  assert i == value\n```\nA reinitializable iterator can be initialized from multiple different\n`Dataset` objects. For example, you might have a training input pipeline that\nuses random perturbations to the input images to improve generalization, and\na validation input pipeline that evaluates predictions on unmodified data. These\npipelines will typically use different `Dataset` objects that have the same\nstructure (i.e. the same types and compatible shapes for each component).\n```python\nDefine training and validation datasets with the same structure.\ntraining_dataset = tf.data.Dataset.range(100).map(\n    lambda x: x + tf.random_uniform([], -10, 10, tf.int64))\nvalidation_dataset = tf.data.Dataset.range(50)\nA reinitializable iterator is defined by its structure. We could use the\n`output_types` and `output_shapes` properties of either `training_dataset`\nor `validation_dataset` here, because they are compatible.\niterator = tf.data.Iterator.from_structure(training_dataset.output_types,\n                                           training_dataset.output_shapes)\nnext_element = iterator.get_next()\ntraining_init_op = iterator.make_initializer(training_dataset)\nvalidation_init_op = iterator.make_initializer(validation_dataset)\nRun 20 epochs in which the training dataset is traversed, followed by the\nvalidation dataset.\nfor _ in range(20):\n  # Initialize an iterator over the training dataset.\n  sess.run(training_init_op)\n  for _ in range(100):\n    sess.run(next_element)\n# Initialize an iterator over the validation dataset.\n  sess.run(validation_init_op)\n  for _ in range(50):\n    sess.run(next_element)\n```\nA feedable iterator can be used together with `tf.placeholder` to select\nwhat `Iterator` to use in each call to `tf.Session.run`, via the familiar\n`feed_dict` mechanism. It offers the same functionality as a reinitializable\niterator, but it does not require you to initialize the iterator from the start\nof a dataset when you switch between iterators. For example, using the same\ntraining and validation example from above, you can use\n`tf.data.Iterator.from_string_handle` to define a feedable iterator\nthat allows you to switch between the two datasets:\n```python\nDefine training and validation datasets with the same structure.\ntraining_dataset = tf.data.Dataset.range(100).map(\n    lambda x: x + tf.random_uniform([], -10, 10, tf.int64)).repeat()\nvalidation_dataset = tf.data.Dataset.range(50)\nA feedable iterator is defined by a handle placeholder and its structure. We\ncould use the `output_types` and `output_shapes` properties of either\n`training_dataset` or `validation_dataset` here, because they have\nidentical structure.\nhandle = tf.placeholder(tf.string, shape=[])\niterator = tf.data.Iterator.from_string_handle(\n    handle, training_dataset.output_types, training_dataset.output_shapes)\nnext_element = iterator.get_next()\nYou can use feedable iterators with a variety of different kinds of iterator\n(such as one-shot and initializable iterators).\ntraining_iterator = training_dataset.make_one_shot_iterator()\nvalidation_iterator = validation_dataset.make_initializable_iterator()\nThe `Iterator.string_handle()` method returns a tensor that can be evaluated\nand used to feed the `handle` placeholder.\ntraining_handle = sess.run(training_iterator.string_handle())\nvalidation_handle = sess.run(validation_iterator.string_handle())\nLoop forever, alternating between training and validation.\nwhile True:\n  # Run 200 steps using the training dataset. Note that the training dataset is\n  # infinite, and we resume from where we left off in the previous `while` loop\n  # iteration.\n  for _ in range(200):\n    sess.run(next_element, feed_dict={handle: training_handle})\n# Run one pass over the validation dataset.\n  sess.run(validation_iterator.initializer)\n  for _ in range(50):\n    sess.run(next_element, feed_dict={handle: validation_handle})\n```\nConsuming values from an iterator\nThe `Iterator.get_next()` method returns one or more `tf.Tensor` objects that\ncorrespond to the symbolic next element of an iterator. Each time these tensors\nare evaluated, they take the value of the next element in the underlying\ndataset. (Note that, like other stateful objects in TensorFlow, calling\n`Iterator.get_next()` does not immediately advance the iterator. Instead you\nmust use the returned `tf.Tensor` objects in a TensorFlow expression, and pass\nthe result of that expression to `tf.Session.run()` to get the next elements and\nadvance the iterator.)\nIf the iterator reaches the end of the dataset, executing\nthe `Iterator.get_next()` operation will raise a `tf.errors.OutOfRangeError`.\nAfter this point the iterator will be in an unusable state, and you must\ninitialize it again if you want to use it further.\n```python\ndataset = tf.data.Dataset.range(5)\niterator = dataset.make_initializable_iterator()\nnext_element = iterator.get_next()\nTypically `result` will be the output of a model, or an optimizer's\ntraining operation.\nresult = tf.add(next_element, next_element)\nsess.run(iterator.initializer)\nprint(sess.run(result))  # ==> \"0\"\nprint(sess.run(result))  # ==> \"2\"\nprint(sess.run(result))  # ==> \"4\"\nprint(sess.run(result))  # ==> \"6\"\nprint(sess.run(result))  # ==> \"8\"\ntry:\n  sess.run(result)\nexcept tf.errors.OutOfRangeError:\n  print(\"End of dataset\")  # ==> \"End of dataset\"\n```\nA common pattern is to wrap the \"training loop\" in a `try`-`except` block:\n`python\nsess.run(iterator.initializer)\nwhile True:\n  try:\n    sess.run(result)\n  except tf.errors.OutOfRangeError:\n    break`\nIf each element of the dataset has a nested structure, the return value of\n`Iterator.get_next()` will be one or more `tf.Tensor` objects in the same\nnested structure:\n```python\ndataset1 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([4, 10]))\ndataset2 = tf.data.Dataset.from_tensor_slices((tf.random_uniform([4]), tf.random_uniform([4, 100])))\ndataset3 = tf.data.Dataset.zip((dataset1, dataset2))\niterator = dataset3.make_initializable_iterator()\nsess.run(iterator.initializer)\nnext1, (next2, next3) = iterator.get_next()\n```\nNote that `next1`, `next2`, and `next3` are tensors produced by the\nsame op/node (created by `Iterator.get_next()`). Therefore,  evaluating any of\nthese tensors will advance the iterator for all components. A typical consumer\nof an iterator will include all components in a single expression.\nSaving iterator state\nThe `tf.contrib.data.make_saveable_from_iterator` function creates a\n`SaveableObject` from an iterator, which can be used to save and\nrestore the current state of the iterator (and, effectively, the whole input\npipeline). A saveable object thus created can be added to `tf.train.Saver`\nvariables list or the `tf.GraphKeys.SAVEABLE_OBJECTS` collection for saving and\nrestoring in the same manner as a `tf.Variable`. Refer to\nSaving and Restoring for details on how to save and restore\nvariables.\n```python\nCreate saveable object from iterator.\nsaveable = tf.contrib.data.make_saveable_from_iterator(iterator)\nSave the iterator state by adding it to the saveable objects collection.\ntf.add_to_collection(tf.GraphKeys.SAVEABLE_OBJECTS, saveable)\nsaver = tf.train.Saver()\nwith tf.Session() as sess:\nif should_checkpoint:\n    saver.save(path_to_checkpoint)\nRestore the iterator state.\nwith tf.Session() as sess:\n  saver.restore(sess, path_to_checkpoint)\n```\nReading input data\nConsuming NumPy arrays\nIf all of your input data fit in memory, the simplest way to create a `Dataset`\nfrom them is to convert them to `tf.Tensor` objects and use\n`Dataset.from_tensor_slices()`.\n```python\nLoad the training data into two NumPy arrays, for example using `np.load()`.\nwith np.load(\"/var/data/training_data.npy\") as data:\n  features = data[\"features\"]\n  labels = data[\"labels\"]\nAssume that each row of `features` corresponds to the same row as `labels`.\nassert features.shape[0] == labels.shape[0]\ndataset = tf.data.Dataset.from_tensor_slices((features, labels))\n```\nNote that the above code snippet will embed the `features` and `labels` arrays\nin your TensorFlow graph as `tf.constant()` operations. This works well for a\nsmall dataset, but wastes memory---because the contents of the array will be\ncopied multiple times---and can run into the 2GB limit for the `tf.GraphDef`\nprotocol buffer.\nAs an alternative, you can define the `Dataset` in terms of `tf.placeholder()`\ntensors, and feed the NumPy arrays when you initialize an `Iterator` over the\ndataset.\n```python\nLoad the training data into two NumPy arrays, for example using `np.load()`.\nwith np.load(\"/var/data/training_data.npy\") as data:\n  features = data[\"features\"]\n  labels = data[\"labels\"]\nAssume that each row of `features` corresponds to the same row as `labels`.\nassert features.shape[0] == labels.shape[0]\nfeatures_placeholder = tf.placeholder(features.dtype, features.shape)\nlabels_placeholder = tf.placeholder(labels.dtype, labels.shape)\ndataset = tf.data.Dataset.from_tensor_slices((features_placeholder, labels_placeholder))\n[Other transformations on `dataset`...]\ndataset = ...\niterator = dataset.make_initializable_iterator()\nsess.run(iterator.initializer, feed_dict={features_placeholder: features,\n                                          labels_placeholder: labels})\n```\nConsuming TFRecord data\nThe `tf.data` API supports a variety of file formats so that you can process\nlarge datasets that do not fit in memory. For example, the TFRecord file format\nis a simple record-oriented binary format that many TensorFlow applications use\nfor training data. The `tf.data.TFRecordDataset` class enables you to\nstream over the contents of one or more TFRecord files as part of an input\npipeline.\n```python\nCreates a dataset that reads all of the examples from two files.\nfilenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\ndataset = tf.data.TFRecordDataset(filenames)\n```\nThe `filenames` argument to the `TFRecordDataset` initializer can either be a\nstring, a list of strings, or a `tf.Tensor` of strings. Therefore if you have\ntwo sets of files for training and validation purposes, you can use a\n`tf.placeholder(tf.string)` to represent the filenames, and initialize an\niterator from the appropriate filenames:\n```python\nfilenames = tf.placeholder(tf.string, shape=[None])\ndataset = tf.data.TFRecordDataset(filenames)\ndataset = dataset.map(...)  # Parse the record into tensors.\ndataset = dataset.repeat()  # Repeat the input indefinitely.\ndataset = dataset.batch(32)\niterator = dataset.make_initializable_iterator()\nYou can feed the initializer with the appropriate filenames for the current\nphase of execution, e.g. training vs. validation.\nInitialize `iterator` with training data.\ntraining_filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\nsess.run(iterator.initializer, feed_dict={filenames: training_filenames})\nInitialize `iterator` with validation data.\nvalidation_filenames = [\"/var/data/validation1.tfrecord\", ...]\nsess.run(iterator.initializer, feed_dict={filenames: validation_filenames})\n```\nConsuming text data\nMany datasets are distributed as one or more text files. The\n`tf.data.TextLineDataset` provides an easy way to extract lines from\none or more text files. Given one or more filenames, a `TextLineDataset` will\nproduce one string-valued element per line of those files. Like a\n`TFRecordDataset`, `TextLineDataset` accepts `filenames` as a `tf.Tensor`, so\nyou can parameterize it by passing a `tf.placeholder(tf.string)`.\n`python\nfilenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\"]\ndataset = tf.data.TextLineDataset(filenames)`\nBy default, a `TextLineDataset` yields every line of each file, which may\nnot be desirable, for example if the file starts with a header line, or contains\ncomments. These lines can be removed using the `Dataset.skip()` and\n`Dataset.filter()` transformations. To apply these transformations to each\nfile separately, we use `Dataset.flat_map()` to create a nested `Dataset` for\neach file.\n```python\nfilenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\"]\ndataset = tf.data.Dataset.from_tensor_slices(filenames)\nUse `Dataset.flat_map()` to transform each file as a separate nested dataset,\nand then concatenate their contents sequentially into a single \"flat\" dataset.\n* Skip the first line (header row).\n* Filter out lines beginning with \"#\" (comments).\ndataset = dataset.flat_map(\n    lambda filename: (\n        tf.data.TextLineDataset(filename)\n        .skip(1)\n        .filter(lambda line: tf.not_equal(tf.substr(line, 0, 1), \"#\"))))\n```\nConsuming CSV data\nThe CSV file format is a popular format for storing tabular data in plain text.\nThe `tf.data.experimental.CsvDataset` class provides a way to extract records from\none or more CSV files that comply with RFC 4180.\nGiven one or more filenames and a list of defaults, a `CsvDataset` will produce\na tuple of elements whose types correspond to the types of the defaults\nprovided, per CSV record. Like `TFRecordDataset` and `TextLineDataset`,\n`CsvDataset` accepts `filenames` as a `tf.Tensor`, so you can parameterize it\nby passing a  `tf.placeholder(tf.string)`.\n```\nCreates a dataset that reads all of the records from two CSV files, each with\neight float columns\nfilenames = [\"/var/data/file1.csv\", \"/var/data/file2.csv\"]\nrecord_defaults = [tf.float32] * 8   # Eight required float columns\ndataset = tf.data.experimental.CsvDataset(filenames, record_defaults)\n```\nIf some columns are empty, you can provide defaults instead of types.\n```\nCreates a dataset that reads all of the records from two CSV files, each with\nfour float columns which may have missing values\nrecord_defaults = [[0.0]] * 8\ndataset = tf.data.experimental.CsvDataset(filenames, record_defaults)\n```\nBy default, a `CsvDataset` yields every column of every line of the file,\nwhich may not be desirable, for example if the file starts with a header line\nthat should be ignored, or if some columns are not required in the input.\nThese lines and fields can be removed with the `header` and `select_cols`\narguments respectively.\n```\nCreates a dataset that reads all of the records from two CSV files with\nheaders, extracting float data from columns 2 and 4.\nrecord_defaults = [[0.0]] * 2  # Only provide defaults for the selected columns\ndataset = tf.data.experimental.CsvDataset(filenames, record_defaults, header=True, select_cols=[2,4])\n```\n\nPreprocessing data with `Dataset.map()`\nThe `Dataset.map(f)` transformation produces a new dataset by applying a given\nfunction `f` to each element of the input dataset. It is based on\nthe\nmap() function\nthat is commonly applied to lists (and other structures) in functional\nprogramming languages.  The function `f` takes the `tf.Tensor` objects that\nrepresent a single element in the input, and returns the `tf.Tensor` objects\nthat will represent a single element in the new dataset. Its implementation uses\nstandard TensorFlow operations to transform one element into another.\nThis section covers common examples of how to use `Dataset.map()`.\nParsing `tf.Example` protocol buffer messages\nMany input pipelines extract `tf.train.Example` protocol buffer messages from a\nTFRecord-format file (written, for example, using\n`tf.python_io.TFRecordWriter`). Each `tf.train.Example` record contains one or\nmore \"features\", and the input pipeline typically converts these features into\ntensors.\n```python\nTransforms a scalar string `example_proto` into a pair of a scalar string and\na scalar integer, representing an image and its label, respectively.\ndef _parse_function(example_proto):\n  features = {\"image\": tf.FixedLenFeature((), tf.string, default_value=\"\"),\n              \"label\": tf.FixedLenFeature((), tf.int64, default_value=0)}\n  parsed_features = tf.parse_single_example(example_proto, features)\n  return parsed_features[\"image\"], parsed_features[\"label\"]\nCreates a dataset that reads all of the examples from two files, and extracts\nthe image and label features.\nfilenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\ndataset = tf.data.TFRecordDataset(filenames)\ndataset = dataset.map(_parse_function)\n```\nDecoding image data and resizing it\nWhen training a neural network on real-world image data, it is often necessary\nto convert images of different sizes to a common size, so that they may be\nbatched into a fixed size.\n```python\nReads an image from a file, decodes it into a dense tensor, and resizes it\nto a fixed shape.\ndef _parse_function(filename, label):\n  image_string = tf.read_file(filename)\n  image_decoded = tf.image.decode_jpeg(image_string)\n  image_resized = tf.image.resize_images(image_decoded, [28, 28])\n  return image_resized, label\nA vector of filenames.\nfilenames = tf.constant([\"/var/data/image1.jpg\", \"/var/data/image2.jpg\", ...])\n`labels[i]` is the label for the image in `filenames[i].\nlabels = tf.constant([0, 37, ...])\ndataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\ndataset = dataset.map(_parse_function)\n```\nApplying arbitrary Python logic with `tf.py_func()`\nFor performance reasons, we encourage you to use TensorFlow operations for\npreprocessing your data whenever possible. However, it is sometimes useful to\ncall upon external Python libraries when parsing your input data. To do so,\ninvoke, the `tf.py_func()` operation in a `Dataset.map()` transformation.\n```python\nimport cv2\nUse a custom OpenCV function to read the image, instead of the standard\nTensorFlow `tf.read_file()` operation.\ndef _read_py_function(filename, label):\n  image_decoded = cv2.imread(filename.decode(), cv2.IMREAD_GRAYSCALE)\n  return image_decoded, label\nUse standard TensorFlow operations to resize the image to a fixed shape.\ndef _resize_function(image_decoded, label):\n  image_decoded.set_shape([None, None, None])\n  image_resized = tf.image.resize_images(image_decoded, [28, 28])\n  return image_resized, label\nfilenames = [\"/var/data/image1.jpg\", \"/var/data/image2.jpg\", ...]\nlabels = [0, 37, 29, 1, ...]\ndataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\ndataset = dataset.map(\n    lambda filename, label: tuple(tf.py_func(\n        _read_py_function, [filename, label], [tf.uint8, label.dtype])))\ndataset = dataset.map(_resize_function)\n```\n\nBatching dataset elements\nSimple batching\nThe simplest form of batching stacks `n` consecutive elements of a dataset into\na single element. The `Dataset.batch()` transformation does exactly this, with\nthe same constraints as the `tf.stack()` operator, applied to each component\nof the elements: i.e. for each component i, all elements must have a tensor\nof the exact same shape.\n```python\ninc_dataset = tf.data.Dataset.range(100)\ndec_dataset = tf.data.Dataset.range(0, -100, -1)\ndataset = tf.data.Dataset.zip((inc_dataset, dec_dataset))\nbatched_dataset = dataset.batch(4)\niterator = batched_dataset.make_one_shot_iterator()\nnext_element = iterator.get_next()\nprint(sess.run(next_element))  # ==> ([0, 1, 2,   3],   [ 0, -1,  -2,  -3])\nprint(sess.run(next_element))  # ==> ([4, 5, 6,   7],   [-4, -5,  -6,  -7])\nprint(sess.run(next_element))  # ==> ([8, 9, 10, 11],   [-8, -9, -10, -11])\n```\nBatching tensors with padding\nThe above recipe works for tensors that all have the same size. However, many\nmodels (e.g. sequence models) work with input data that can have varying size\n(e.g. sequences of different lengths). To handle this case, the\n`Dataset.padded_batch()` transformation enables you to batch tensors of\ndifferent shape by specifying one or more dimensions in which they may be\npadded.\n```python\ndataset = tf.data.Dataset.range(100)\ndataset = dataset.map(lambda x: tf.fill([tf.cast(x, tf.int32)], x))\ndataset = dataset.padded_batch(4, padded_shapes=(None,))\niterator = dataset.make_one_shot_iterator()\nnext_element = iterator.get_next()\nprint(sess.run(next_element))  # ==> [[0, 0, 0], [1, 0, 0], [2, 2, 0], [3, 3, 3]]\nprint(sess.run(next_element))  # ==> [[4, 4, 4, 4, 0, 0, 0],\n                               #      [5, 5, 5, 5, 5, 0, 0],\n                               #      [6, 6, 6, 6, 6, 6, 0],\n                               #      [7, 7, 7, 7, 7, 7, 7]]\n```\nThe `Dataset.padded_batch()` transformation allows you to set different padding\nfor each dimension of each component, and it may be variable-length (signified\nby `None` in the example above) or constant-length. It is also possible to\noverride the padding value, which defaults to 0.\n\nTraining workflows\nProcessing multiple epochs\nThe `tf.data` API offers two main ways to process multiple epochs of the same\ndata.\nThe simplest way to iterate over a dataset in multiple epochs is to use the\n`Dataset.repeat()` transformation. For example, to create a dataset that repeats\nits input for 10 epochs:\n`python\nfilenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\ndataset = tf.data.TFRecordDataset(filenames)\ndataset = dataset.map(...)\ndataset = dataset.repeat(10)\ndataset = dataset.batch(32)`\nApplying the `Dataset.repeat()` transformation with no arguments will repeat\nthe input indefinitely. The `Dataset.repeat()` transformation concatenates its\narguments without signaling the end of one epoch and the beginning of the next\nepoch.\nIf you want to receive a signal at the end of each epoch, you can write a\ntraining loop that catches the `tf.errors.OutOfRangeError` at the end of a\ndataset. At that point you might collect some statistics (e.g. the validation\nerror) for the epoch.\n```python\nfilenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\ndataset = tf.data.TFRecordDataset(filenames)\ndataset = dataset.map(...)\ndataset = dataset.batch(32)\niterator = dataset.make_initializable_iterator()\nnext_element = iterator.get_next()\nCompute for 100 epochs.\nfor _ in range(100):\n  sess.run(iterator.initializer)\n  while True:\n    try:\n      sess.run(next_element)\n    except tf.errors.OutOfRangeError:\n      break\n# [Perform end-of-epoch calculations here.]\n```\nRandomly shuffling input data\nThe `Dataset.shuffle()` transformation randomly shuffles the input dataset\nusing a similar algorithm to `tf.RandomShuffleQueue`: it maintains a fixed-size\nbuffer and chooses the next element uniformly at random from that buffer.\n`python\nfilenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\ndataset = tf.data.TFRecordDataset(filenames)\ndataset = dataset.map(...)\ndataset = dataset.shuffle(buffer_size=10000)\ndataset = dataset.batch(32)\ndataset = dataset.repeat()`\nUsing high-level APIs\nThe `tf.train.MonitoredTrainingSession` API simplifies many aspects of running\nTensorFlow in a distributed setting. `MonitoredTrainingSession` uses the\n`tf.errors.OutOfRangeError` to signal that training has completed, so to use it\nwith the `tf.data` API, we recommend using\n`Dataset.make_one_shot_iterator()`. For example:\n```python\nfilenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\ndataset = tf.data.TFRecordDataset(filenames)\ndataset = dataset.map(...)\ndataset = dataset.shuffle(buffer_size=10000)\ndataset = dataset.batch(32)\ndataset = dataset.repeat(num_epochs)\niterator = dataset.make_one_shot_iterator()\nnext_example, next_label = iterator.get_next()\nloss = model_function(next_example, next_label)\ntraining_op = tf.train.AdagradOptimizer(...).minimize(loss)\nwith tf.train.MonitoredTrainingSession(...) as sess:\n  while not sess.should_stop():\n    sess.run(training_op)\n```\nTo use a `Dataset` in the `input_fn` of a `tf.estimator.Estimator`, simply\nreturn the `Dataset` and the framework will take care of creating an iterator\nand initializing it for you. For example:\n```python\ndef dataset_input_fn():\n  filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n  dataset = tf.data.TFRecordDataset(filenames)\n# Use `tf.parse_single_example()` to extract data from a `tf.Example`\n  # protocol buffer, and perform any additional per-record preprocessing.\n  def parser(record):\n    keys_to_features = {\n        \"image_data\": tf.FixedLenFeature((), tf.string, default_value=\"\"),\n        \"date_time\": tf.FixedLenFeature((), tf.int64, default_value=\"\"),\n        \"label\": tf.FixedLenFeature((), tf.int64,\n                                    default_value=tf.zeros([], dtype=tf.int64)),\n    }\n    parsed = tf.parse_single_example(record, keys_to_features)\n\n\n```# Perform additional preprocessing on the parsed data.\nimage = tf.image.decode_jpeg(parsed[\"image_data\"])\nimage = tf.reshape(image, [299, 299, 1])\nlabel = tf.cast(parsed[\"label\"], tf.int32)\n\nreturn {\"image_data\": image, \"date_time\": parsed[\"date_time\"]}, label\n```\n\n\n# Use `Dataset.map()` to build a pair of a feature dictionary and a label\n  # tensor for each example.\n  dataset = dataset.map(parser)\n  dataset = dataset.shuffle(buffer_size=10000)\n  dataset = dataset.batch(32)\n  dataset = dataset.repeat(num_epochs)\n# Each element of `dataset` is tuple containing a dictionary of features\n  # (in which each value is a batch of values for that feature), and a batch of\n  # labels.\n  return dataset",
    "tag": "tensorflow"
  },
  {
    "title": "Checkpoints",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/guide/checkpoints.md",
    "content": "Checkpoints\nThis document examines how to save and restore TensorFlow models built with\nEstimators. TensorFlow provides two model formats:\n\ncheckpoints, which is a format dependent on the code that created\n    the model.\nSavedModel, which is a format independent of the code that created\n    the model.\n\nThis document focuses on checkpoints. For details on `SavedModel`, see the\nSaving and Restoring guide.\nSample code\nThis document relies on the same\nIris classification example detailed in Getting Started with TensorFlow.\nTo download and access the example, invoke the following two commands:\n`shell\ngit clone https://github.com/tensorflow/models/\ncd models/samples/core/get_started`\nMost of the code snippets in this document are minor variations\non `premade_estimator.py`.\nSaving partially-trained models\nEstimators automatically write the following to disk:\n\ncheckpoints, which are versions of the model created during training.\nevent files, which contain information that\n    TensorBoard\n    uses to create visualizations.\n\nTo specify the top-level directory in which the Estimator stores its\ninformation, assign a value to the optional `model_dir` argument of any\n`Estimator`'s constructor.\nTaking `DNNClassifier` as an example,\nthe following code sets the `model_dir`\nargument to the `models/iris` directory:\n`python\nclassifier = tf.estimator.DNNClassifier(\n    feature_columns=my_feature_columns,\n    hidden_units=[10, 10],\n    n_classes=3,\n    model_dir='models/iris')`\nSuppose you call the Estimator's `train` method. For example:\n`python\nclassifier.train(\n    input_fn=lambda: train_input_fn(train_x, train_y, batch_size=100),\n    steps=200)`\nAs suggested by the following diagrams, the first call to `train`\nadds checkpoints and other files to the `model_dir` directory:\n\n\n\n\nThe first call to train().\n\nTo see the objects in the created `model_dir` directory on a\nUNIX-based system, just call `ls` as follows:\n\n$ ls -1 models/iris\ncheckpoint\nevents.out.tfevents.timestamp.hostname\ngraph.pbtxt\nmodel.ckpt-1.data-00000-of-00001\nmodel.ckpt-1.index\nmodel.ckpt-1.meta\nmodel.ckpt-200.data-00000-of-00001\nmodel.ckpt-200.index\nmodel.ckpt-200.meta\n\nThe preceding `ls` command shows that the Estimator created checkpoints\nat steps 1 (the start of training) and 200 (the end of training).\nDefault checkpoint directory\nIf you don't specify `model_dir` in an Estimator's constructor, the Estimator\nwrites checkpoint files to a temporary directory chosen by Python's\ntempfile.mkdtemp\nfunction. For example, the following Estimator constructor does not specify\nthe `model_dir` argument:\n```python\nclassifier = tf.estimator.DNNClassifier(\n    feature_columns=my_feature_columns,\n    hidden_units=[10, 10],\n    n_classes=3)\nprint(classifier.model_dir)\n```\nThe `tempfile.mkdtemp` function picks a secure, temporary directory\nappropriate for your operating system. For example, a typical temporary\ndirectory on macOS might be something like the following:\n\n/var/folders/0s/5q9kfzfj3gx2knj0vj8p68yc00dhcr/T/tmpYm1Rwa\n\nCheckpointing Frequency\nBy default, the Estimator saves\ncheckpoints\nin the `model_dir` according to the following schedule:\n\nWrites a checkpoint every 10 minutes (600 seconds).\nWrites a checkpoint when the `train` method starts (first iteration)\n    and completes (final iteration).\nRetains only the 5 most recent checkpoints in the directory.\n\nYou may alter the default schedule by taking the following steps:\n\nCreate a `tf.estimator.RunConfig` object that defines the\n    desired schedule.\nWhen instantiating the Estimator, pass that `RunConfig` object to the\n    Estimator's `config` argument.\n\nFor example, the following code changes the checkpointing schedule to every\n20 minutes and retains the 10 most recent checkpoints:\n```python\nmy_checkpointing_config = tf.estimator.RunConfig(\n    save_checkpoints_secs = 20*60,  # Save checkpoints every 20 minutes.\n    keep_checkpoint_max = 10,       # Retain the 10 most recent checkpoints.\n)\nclassifier = tf.estimator.DNNClassifier(\n    feature_columns=my_feature_columns,\n    hidden_units=[10, 10],\n    n_classes=3,\n    model_dir='models/iris',\n    config=my_checkpointing_config)\n```\nRestoring your model\nThe first time you call an Estimator's `train` method, TensorFlow saves a\ncheckpoint to the `model_dir`. Each subsequent call to the Estimator's\n`train`, `evaluate`, or `predict` method causes the following:\n\nThe Estimator builds the model's\n    graph\n    by running the `model_fn()`.  (For details on the `model_fn()`, see\n    Creating Custom Estimators.)\nThe Estimator initializes the weights of the new model from the data\n    stored in the most recent checkpoint.\n\nIn other words, as the following illustration suggests, once checkpoints\nexist, TensorFlow rebuilds the model each time you call `train()`,\n`evaluate()`, or `predict()`.\n\n\n\n\nSubsequent calls to train(), evaluate(), or predict()\n\nAvoiding a bad restoration\nRestoring a model's state from a checkpoint only works if the model\nand checkpoint are compatible.  For example, suppose you trained a\n`DNNClassifier` Estimator containing two hidden layers,\neach having 10 nodes:\n```python\nclassifier = tf.estimator.DNNClassifier(\n    feature_columns=feature_columns,\n    hidden_units=[10, 10],\n    n_classes=3,\n    model_dir='models/iris')\nclassifier.train(\n    input_fn=lambda:train_input_fn(train_x, train_y, batch_size=100),\n        steps=200)\n```\nAfter training (and, therefore, after creating checkpoints in `models/iris`),\nimagine that you changed the number of neurons in each hidden layer from 10 to\n20 and then attempted to retrain the model:\n``` python\nclassifier2 = tf.estimator.DNNClassifier(\n    feature_columns=my_feature_columns,\n    hidden_units=[20, 20],  # Change the number of neurons in the model.\n    n_classes=3,\n    model_dir='models/iris')\nclassifier.train(\n    input_fn=lambda:train_input_fn(train_x, train_y, batch_size=100),\n        steps=200)\n```\nSince the state in the checkpoint is incompatible with the model described\nin `classifier2`, retraining fails with the following error:\n\n...\nInvalidArgumentError (see above for traceback): tensor_name =\ndnn/hiddenlayer_1/bias/t_0/Adagrad; shape in shape_and_slice spec [10]\ndoes not match the shape stored in checkpoint: [20]\n\nTo run experiments in which you train and compare slightly different\nversions of a model, save a copy of the code that created each\n`model_dir`, possibly by creating a separate git branch for each version.\nThis separation will keep your checkpoints recoverable.\nSummary\nCheckpoints provide an easy automatic mechanism for saving and restoring\nmodels created by Estimators.\nSee the Saving and Restoring guide for details about:\n\nSaving and restoring models using low-level TensorFlow APIs.\nExporting and importing models in the SavedModel format, which is a\n",
    "tag": "tensorflow"
  },
  {
    "title": "Using GPUs",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/guide/using_gpu.md",
    "content": "Using GPUs\nSupported devices\nOn a typical system, there are multiple computing devices. In TensorFlow, the\nsupported device types are `CPU` and `GPU`. They are represented as `strings`.\nFor example:\n\n`\"/cpu:0\"`: The CPU of your machine.\n`\"/device:GPU:0\"`: The GPU of your machine, if you have one.\n`\"/device:GPU:1\"`: The second GPU of your machine, etc.\n\nIf a TensorFlow operation has both CPU and GPU implementations, the GPU devices\nwill be given priority when the operation is assigned to a device. For example,\n`matmul` has both CPU and GPU kernels. On a system with devices `cpu:0` and\n`gpu:0`, `gpu:0` will be selected to run `matmul`.\nLogging Device placement\nTo find out which devices your operations and tensors are assigned to, create\nthe session with `log_device_placement` configuration option set to `True`.\n```python\nCreates a graph.\na = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\nb = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\nc = tf.matmul(a, b)\nCreates a session with log_device_placement set to True.\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\nRuns the op.\nprint(sess.run(c))\n```\nYou should see the following output:\n`Device mapping:\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla K40c, pci bus\nid: 0000:05:00.0\nb: /job:localhost/replica:0/task:0/device:GPU:0\na: /job:localhost/replica:0/task:0/device:GPU:0\nMatMul: /job:localhost/replica:0/task:0/device:GPU:0\n[[ 22.  28.]\n [ 49.  64.]]`\nManual device placement\nIf you would like a particular operation to run on a device of your choice\ninstead of what's automatically selected for you, you can use `with tf.device`\nto create a device context such that all the operations within that context will\nhave the same device assignment.\n```python\nCreates a graph.\nwith tf.device('/cpu:0'):\n  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\nc = tf.matmul(a, b)\nCreates a session with log_device_placement set to True.\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\nRuns the op.\nprint(sess.run(c))\n```\nYou will see that now `a` and `b` are assigned to `cpu:0`. Since a device was\nnot explicitly specified for the `MatMul` operation, the TensorFlow runtime will\nchoose one based on the operation and available devices (`gpu:0` in this\nexample) and automatically copy tensors between devices if required.\n`Device mapping:\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla K40c, pci bus\nid: 0000:05:00.0\nb: /job:localhost/replica:0/task:0/cpu:0\na: /job:localhost/replica:0/task:0/cpu:0\nMatMul: /job:localhost/replica:0/task:0/device:GPU:0\n[[ 22.  28.]\n [ 49.  64.]]`\nAllowing GPU memory growth\nBy default, TensorFlow maps nearly all of the GPU memory of all GPUs (subject to\nCUDA_VISIBLE_DEVICES)\nvisible to the process. This is done to more efficiently use the relatively\nprecious GPU memory resources on the devices by reducing memory\nfragmentation.\nIn some cases it is desirable for the process to only allocate a subset of the\navailable memory, or to only grow the memory usage as is needed by the process.\nTensorFlow provides two Config options on the Session to control this.\nThe first is the `allow_growth` option, which attempts to allocate only as much\nGPU memory based on runtime allocations: it starts out allocating very little\nmemory, and as Sessions get run and more GPU memory is needed, we extend the GPU\nmemory region needed by the TensorFlow process. Note that we do not release\nmemory, since that can lead to even worse memory fragmentation. To turn this\noption on, set the option in the ConfigProto by:\n`python\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsession = tf.Session(config=config, ...)`\nAnother way to enable this option is to set the environmental variable\n`TF_FORCE_GPU_ALLOW_GROWTH` to `true`. This configuration is platform specific.\nThe second method is the `per_process_gpu_memory_fraction` option, which\ndetermines the fraction of the overall amount of memory that each visible GPU\nshould be allocated. For example, you can tell TensorFlow to only allocate 40%\nof the total memory of each GPU by:\n`python\nconfig = tf.ConfigProto()\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.4\nsession = tf.Session(config=config, ...)`\nThis is useful if you want to truly bound the amount of GPU memory available to\nthe TensorFlow process.\nUsing a single GPU on a multi-GPU system\nIf you have more than one GPU in your system, the GPU with the lowest ID will be\nselected by default. If you would like to run on a different GPU, you will need\nto specify the preference explicitly:\n```python\nCreates a graph.\nwith tf.device('/device:GPU:2'):\n  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n  c = tf.matmul(a, b)\nCreates a session with log_device_placement set to True.\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\nRuns the op.\nprint(sess.run(c))\n```\nIf the device you have specified does not exist, you will get\n`InvalidArgumentError`:\n`InvalidArgumentError: Invalid argument: Cannot assign a device to node 'b':\nCould not satisfy explicit device specification '/device:GPU:2'\n   [[{{node b}} = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [3,2]\n   values: 1 2 3...>, _device=\"/device:GPU:2\"]()]]`\nIf you would like TensorFlow to automatically choose an existing and supported\ndevice to run the operations in case the specified one doesn't exist, you can\nset `allow_soft_placement` to `True` in the configuration option when creating\nthe session.\n```python\nCreates a graph.\nwith tf.device('/device:GPU:2'):\n  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n  c = tf.matmul(a, b)\nCreates a session with allow_soft_placement and log_device_placement set\nto True.\nsess = tf.Session(config=tf.ConfigProto(\n      allow_soft_placement=True, log_device_placement=True))\nRuns the op.\nprint(sess.run(c))\n```\nUsing multiple GPUs\nIf you would like to run TensorFlow on multiple GPUs, you can construct your\nmodel in a multi-tower fashion where each tower is assigned to a different GPU.\nFor example:\n``` python\nCreates a graph.\nc = []\nfor d in ['/device:GPU:2', '/device:GPU:3']:\n  with tf.device(d):\n    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3])\n    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2])\n    c.append(tf.matmul(a, b))\nwith tf.device('/cpu:0'):\n  sum = tf.add_n(c)\nCreates a session with log_device_placement set to True.\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\nRuns the op.\nprint(sess.run(sum))\n```\nYou will see the following output.\n`Device mapping:\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla K20m, pci bus\nid: 0000:02:00.0\n/job:localhost/replica:0/task:0/device:GPU:1 -> device: 1, name: Tesla K20m, pci bus\nid: 0000:03:00.0\n/job:localhost/replica:0/task:0/device:GPU:2 -> device: 2, name: Tesla K20m, pci bus\nid: 0000:83:00.0\n/job:localhost/replica:0/task:0/device:GPU:3 -> device: 3, name: Tesla K20m, pci bus\nid: 0000:84:00.0\nConst_3: /job:localhost/replica:0/task:0/device:GPU:3\nConst_2: /job:localhost/replica:0/task:0/device:GPU:3\nMatMul_1: /job:localhost/replica:0/task:0/device:GPU:3\nConst_1: /job:localhost/replica:0/task:0/device:GPU:2\nConst: /job:localhost/replica:0/task:0/device:GPU:2\nMatMul: /job:localhost/replica:0/task:0/device:GPU:2\nAddN: /job:localhost/replica:0/task:0/cpu:0\n[[  44.   56.]\n [  98.  128.]]`\nThe cifar10 tutorial is a good example",
    "tag": "tensorflow"
  },
  {
    "title": "Premade Estimators",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/guide/premade_estimators.md",
    "content": "Premade Estimators\nThis document introduces the TensorFlow programming environment and shows you\nhow to solve the Iris classification problem in TensorFlow.\nPrerequisites\nPrior to using the sample code in this document, you'll need to do the\nfollowing:\n\nInstall TensorFlow.\nIf you installed TensorFlow with venv or Anaconda, activate your\n  TensorFlow environment.\nInstall or upgrade pandas by issuing the following command:\n\n```pip install pandas\n```\n\n\n\n\nGetting the sample code\nTake the following steps to get the sample code we'll be going through:\n\n\nClone the TensorFlow Models repository from GitHub by entering the following\n   command:\n\n\n```git clone https://github.com/tensorflow/models\n```\n\n\n\n\nChange directory within that branch to the location containing the examples\n   used in this document:\n\n\n```cd models/samples/core/get_started/\n```\n\n\n\n\nThe program described in this document is\npremade_estimator.py.\nThis program uses\niris_data.py\nto fetch its training data.\nRunning the program\nYou run TensorFlow programs as you would run any Python program. For example:\n`bsh\npython premade_estimator.py`\nThe program should output training logs followed by some predictions against\nthe test set. For example, the first line in the following output shows that\nthe model thinks there is a 99.6% chance that the first example in the test\nset is a Setosa. Since the test set expected Setosa, this appears to be\na good prediction.\n\n...\nPrediction is \"Setosa\" (99.6%), expected \"Setosa\"\n\nPrediction is \"Versicolor\" (99.8%), expected \"Versicolor\"\n\nPrediction is \"Virginica\" (97.9%), expected \"Virginica\"\n\nIf the program generates errors instead of answers, ask yourself the following\nquestions:\n\nDid you install TensorFlow properly?\nAre you using the correct version of TensorFlow?\nDid you activate the environment you installed TensorFlow in? (This is\n  only relevant in certain installation mechanisms.)\n\nThe programming stack\nBefore getting into the details of the program itself, let's investigate the\nprogramming environment. As the following illustration shows, TensorFlow\nprovides a programming stack consisting of multiple API layers:\n\n\n\nWe strongly recommend writing TensorFlow programs with the following APIs:\n\nEstimators, which represent a complete model.\n  The Estimator API provides methods to train the model, to judge the model's\n  accuracy, and to generate predictions.\nDatasets for Estimators, which build a data input\n  pipeline. The Dataset API has methods to load and manipulate data, and feed\n  it into your model. The Dataset API meshes well with the Estimators API.\n\nClassifying irises: an overview\nThe sample program in this document builds and tests a model that\nclassifies Iris flowers into three different species based on the size of their\nsepals and\npetals.\n\n\n\nFrom left to right,\nIris setosa (by\nRadomil, CC BY-SA 3.0),\nIris versicolor (by\nDlanglois, CC BY-SA 3.0),\nand Iris virginica\n(by Frank Mayfield, CC BY-SA\n2.0).\nThe data set\nThe Iris data set contains four features and one\nlabel.\nThe four features identify the following botanical characteristics of\nindividual Iris flowers:\n\nsepal length\nsepal width\npetal length\npetal width\n\nOur model will represent these features as `float32` numerical data.\nThe label identifies the Iris species, which must be one of the following:\n\nIris setosa (0)\nIris versicolor (1)\nIris virginica (2)\n\nOur model will represent the label as `int32` categorical data.\nThe following table shows three examples in the data set:\n|sepal length | sepal width | petal length | petal width| species (label) |\n|------------:|------------:|-------------:|-----------:|:---------------:|\n|         5.1 |         3.3 |          1.7 |        0.5 |   0 (Setosa)   |\n|         5.0 |         2.3 |          3.3 |        1.0 |   1 (versicolor)|\n|         6.4 |         2.8 |          5.6 |        2.2 |   2 (virginica) |\nThe algorithm\nThe program trains a Deep Neural Network classifier model having the following\ntopology:\n\n2 hidden layers.\nEach hidden layer contains 10 nodes.\n\nThe following figure illustrates the features, hidden layers, and predictions\n(not all of the nodes in the hidden layers are shown):\n\n\n\nInference\nRunning the trained model on an unlabeled example yields three predictions,\nnamely, the likelihood that this flower is the given Iris species. The sum of\nthose output predictions will be 1.0. For example, the prediction on an\nunlabeled example might be something like the following:\n\n0.03 for Iris Setosa\n0.95 for Iris Versicolor\n0.02 for Iris Virginica\n\nThe preceding prediction indicates a 95% probability that the given unlabeled\nexample is an Iris Versicolor.\nOverview of programming with Estimators\nAn Estimator is TensorFlow's high-level representation of a complete model. It\nhandles the details of initialization, logging, saving and restoring, and many\nother features so you can concentrate on your model. For more details see\nEstimators.\nAn Estimator is any class derived from `tf.estimator.Estimator`. TensorFlow\nprovides a collection of\n`tf.estimator`\n(for example, `LinearRegressor`) to implement common ML algorithms. Beyond\nthose, you may write your own\ncustom Estimators.\nWe recommend using pre-made Estimators when just getting started.\nTo write a TensorFlow program based on pre-made Estimators, you must perform the\nfollowing tasks:\n\nCreate one or more input functions.\nDefine the model's feature columns.\nInstantiate an Estimator, specifying the feature columns and various\n  hyperparameters.\nCall one or more methods on the Estimator object, passing the appropriate\n  input function as the source of the data.\n\nLet's see how those tasks are implemented for Iris classification.\nCreate input functions\nYou must create input functions to supply data for training,\nevaluating, and prediction.\nAn input function is a function that returns a `tf.data.Dataset` object\nwhich outputs the following two-element tuple:\n\nfeatures - A Python dictionary in which:\nEach key is the name of a feature.\nEach value is an array containing all of that feature's values.\n\n\n`label` - An array containing the values of the\n  label for\n  every example.\n\nJust to demonstrate the format of the input function, here's a simple\nimplementation:\n`python\ndef input_evaluation_set():\n    features = {'SepalLength': np.array([6.4, 5.0]),\n                'SepalWidth':  np.array([2.8, 2.3]),\n                'PetalLength': np.array([5.6, 3.3]),\n                'PetalWidth':  np.array([2.2, 1.0])}\n    labels = np.array([2, 1])\n    return features, labels`\nYour input function may generate the `features` dictionary and `label` list any\nway you like. However, we recommend using TensorFlow's Dataset API, which can\nparse all sorts of data. At a high level, the Dataset API consists of the\nfollowing classes:\n\n\n\nWhere the individual members are:\n\n`Dataset` - Base class containing methods to create and transform\n  datasets. Also allows you to initialize a dataset from data in memory, or from\n  a Python generator.\n`TextLineDataset` - Reads lines from text files.\n`TFRecordDataset` - Reads records from TFRecord files.\n`FixedLengthRecordDataset` - Reads fixed size records from binary files.\n`Iterator` - Provides a way to access one data set element at a time.\n\nThe Dataset API can handle a lot of common cases for you. For example,\nusing the Dataset API, you can easily read in records from a large collection\nof files in parallel and join them into a single stream.\nTo keep things simple in this example we are going to load the data with\npandas, and build our input pipeline from this\nin-memory data.\nHere is the input function used for training in this program, which is available\nin iris_data.py:\n``` python\ndef train_input_fn(features, labels, batch_size):\n    \"\"\"An input function for training\"\"\"\n    # Convert the inputs to a Dataset.\n    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n\n\n```# Shuffle, repeat, and batch the examples.\nreturn dataset.shuffle(1000).repeat().batch(batch_size)\n```\n\n\n```\nDefine the feature columns\nA feature column\nis an object describing how the model should use raw input data from the\nfeatures dictionary. When you build an Estimator model, you pass it a list of\nfeature columns that describes each of the features you want the model to use.\nThe `tf.feature_column` module provides many options for representing data\nto the model.\nFor Iris, the 4 raw features are numeric values, so we'll build a list of\nfeature columns to tell the Estimator model to represent each of the four\nfeatures as 32-bit floating-point values. Therefore, the code to create the\nfeature column is:\n```python\nFeature columns describe how to use the input.\nmy_feature_columns = []\nfor key in train_x.keys():\n    my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n```\nFeature columns can be far more sophisticated than those we're showing here.  We\ndetail feature columns later on in our Getting\nStarted guide.\nNow that we have the description of how we want the model to represent the raw\nfeatures, we can build the estimator.\nInstantiate an estimator\nThe Iris problem is a classic classification problem. Fortunately, TensorFlow\nprovides several pre-made classifier Estimators, including:\n\n`tf.estimator.DNNClassifier` for deep models that perform multi-class\n  classification.\n`tf.estimator.DNNLinearCombinedClassifier` for wide & deep models.\n`tf.estimator.LinearClassifier` for classifiers based on linear models.\n\nFor the Iris problem, `tf.estimator.DNNClassifier` seems like the best choice.\nHere's how we instantiated this Estimator:\n```python\nBuild a DNN with 2 hidden layers and 10 nodes in each hidden layer.\nclassifier = tf.estimator.DNNClassifier(\n    feature_columns=my_feature_columns,\n    # Two hidden layers of 10 nodes each.\n    hidden_units=[10, 10],\n    # The model must choose between 3 classes.\n    n_classes=3)\n```\nTrain, Evaluate, and Predict\nNow that we have an Estimator object, we can call methods to do the following:\n\nTrain the model.\nEvaluate the trained model.\nUse the trained model to make predictions.\n\nTrain the model\nTrain the model by calling the Estimator's `train` method as follows:\n```python\nTrain the Model.\nclassifier.train(\n    input_fn=lambda:iris_data.train_input_fn(train_x, train_y, args.batch_size),\n    steps=args.train_steps)\n```\nHere we wrap up our `input_fn` call in a\nlambda\nto capture the arguments while providing an input function that takes no\narguments, as expected by the Estimator. The `steps` argument tells the method\nto stop training after a number of training steps.\nEvaluate the trained model\nNow that the model has been trained, we can get some statistics on its\nperformance. The following code block evaluates the accuracy of the trained\nmodel on the test data:\n```python\nEvaluate the model.\neval_result = classifier.evaluate(\n    input_fn=lambda:iris_data.eval_input_fn(test_x, test_y, args.batch_size))\nprint('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\n```\nUnlike our call to the `train` method, we did not pass the `steps`\nargument to evaluate. Our `eval_input_fn` only yields a single\nepoch of data.\nRunning this code yields the following output (or something similar):\n\nTest set accuracy: 0.967\n\nThe `eval_result` dictionary also contains the `average_loss` (mean loss per sample), the `loss` (mean loss per mini-batch) and the value of the estimator's `global_step` (the number of training iterations it underwent).\nMaking predictions (inferring) from the trained model\nWe now have a trained model that produces good evaluation results.\nWe can now use the trained model to predict the species of an Iris flower\nbased on some unlabeled measurements. As with training and evaluation, we make\npredictions using a single function call:\n```python\nGenerate predictions from the model\nexpected = ['Setosa', 'Versicolor', 'Virginica']\npredict_x = {\n    'SepalLength': [5.1, 5.9, 6.9],\n    'SepalWidth': [3.3, 3.0, 3.1],\n    'PetalLength': [1.7, 4.2, 5.4],\n    'PetalWidth': [0.5, 1.5, 2.1],\n}\npredictions = classifier.predict(\n    input_fn=lambda:iris_data.eval_input_fn(predict_x,\n                                            batch_size=args.batch_size))\n```\nThe `predict` method returns a Python iterable, yielding a dictionary of\nprediction results for each example. The following code prints a few\npredictions and their probabilities:\n``` python\ntemplate = ('\\nPrediction is \"{}\" ({:.1f}%), expected \"{}\"')\nfor pred_dict, expec in zip(predictions, expected):\n    class_id = pred_dict['class_ids'][0]\n    probability = pred_dict['probabilities'][class_id]\n\n\n```print(template.format(iris_data.SPECIES[class_id],\n                      100 * probability, expec))\n```\n\n\n```\nRunning the preceding code yields the following output:\n\n...\nPrediction is \"Setosa\" (99.6%), expected \"Setosa\"\n\nPrediction is \"Versicolor\" (99.8%), expected \"Versicolor\"\n\nPrediction is \"Virginica\" (97.9%), expected \"Virginica\"\n\nSummary\nPre-made Estimators are an effective way to quickly create standard models.\nNow that you've gotten started writing TensorFlow programs, consider the\nfollowing material:\n\nCheckpoints to learn how to save and restore models.\nDatasets for Estimators to learn more about importing\n  data into your model.\nCreating Custom Estimators to learn how to\n",
    "tag": "tensorflow"
  },
  {
    "title": "Feature Columns",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/guide/feature_columns.md",
    "content": "Feature Columns\nThis document details feature columns. Think of feature columns as the\nintermediaries between raw data and Estimators. Feature columns are very rich,\nenabling you to transform a diverse range of raw data into formats that\nEstimators can use, allowing easy experimentation.\nIn Premade Estimators, we used the premade\nEstimator, `tf.estimator.DNNClassifier` to train a model to\npredict different types of Iris flowers from four input features. That example\ncreated only numerical feature columns (of type\n`tf.feature_column.numeric_column`). Although numerical feature columns model\nthe lengths of petals and sepals effectively, real world data sets contain all\nkinds of features, many of which are non-numerical.\n\n\n\n\nSome real-world features (such as, longitude) are numerical, but many are not.\n\nInput to a Deep Neural Network\nWhat kind of data can a deep neural network operate on? The answer\nis, of course, numbers (for example, `tf.float32`). After all, every neuron in\na neural network performs multiplication and addition operations on weights and\ninput data. Real-life input data, however, often contains non-numerical\n(categorical) data. For example, consider a `product_class` feature that can\ncontain the following three non-numerical values:\n\n`kitchenware`\n`electronics`\n`sports`\n\nML models generally represent categorical values as simple vectors in which a\n1 represents the presence of a value and a 0 represents the absence of a value.\nFor example, when `product_class` is set to `sports`, an ML model would usually\nrepresent `product_class` as  `[0, 0, 1]`, meaning:\n\n`0`: `kitchenware` is absent\n`0`: `electronics` is absent\n`1`: `sports` is present\n\nSo, although raw data can be numerical or categorical, an ML model represents\nall features as numbers.\nFeature Columns\nAs the following figure suggests, you specify the input to a model through the\n`feature_columns` argument of an Estimator (`DNNClassifier` for Iris).\nFeature Columns bridge input data (as returned by `input_fn`) with your model.\n\n\n\n\nFeature columns bridge raw data with the data your model needs.\n\nTo create feature columns, call functions from the\n`tf.feature_column` module. This document explains nine of the functions in\nthat module. As the following figure shows, all nine functions return either a\nCategorical-Column or a Dense-Column object, except `bucketized_column`, which\ninherits from both classes:\n\n\n\n\nFeature column methods fall into two main categories and one hybrid category.\n\nLet's look at these functions in more detail.\nNumeric column\nThe Iris classifier calls the `tf.feature_column.numeric_column` function for\nall input features:\n\n`SepalLength`\n`SepalWidth`\n`PetalLength`\n`PetalWidth`\n\nAlthough `tf.numeric_column` provides optional arguments, calling\n`tf.numeric_column` without any arguments, as follows, is a fine way to specify\na numerical value with the default data type (`tf.float32`) as input to your\nmodel:\n```python\nDefaults to a tf.float32 scalar.\nnumeric_feature_column = tf.feature_column.numeric_column(key=\"SepalLength\")\n```\nTo specify a non-default numerical data type, use the `dtype` argument. For\nexample:\n``` python\nRepresent a tf.float64 scalar.\nnumeric_feature_column = tf.feature_column.numeric_column(key=\"SepalLength\",\n                                                          dtype=tf.float64)\n```\nBy default, a numeric column creates a single value (scalar). Use the shape\nargument to specify another shape. For example:\n\n```python\nRepresent a 10-element vector in which each cell contains a tf.float32.\nvector_feature_column = tf.feature_column.numeric_column(key=\"Bowling\",\n                                                         shape=10)\nRepresent a 10x5 matrix in which each cell contains a tf.float32.\nmatrix_feature_column = tf.feature_column.numeric_column(key=\"MyMatrix\",\n                                                         shape=[10,5])\n```\nBucketized column\nOften, you don't want to feed a number directly into the model, but instead\nsplit its value into different categories based on numerical ranges.  To do so,\ncreate a `tf.feature_column.bucketized_column`. For\nexample, consider raw data that represents the year a house was built. Instead\nof representing that year as a scalar numeric column, we could split the year\ninto the following four buckets:\n\n\n\n\nDividing year data into four buckets.\n\nThe model will represent the buckets as follows:\n|Date Range |Represented as... |\n|:----------|:-----------------|\n|< 1960               | [1, 0, 0, 0] |\n|>= 1960 but < 1980   | [0, 1, 0, 0] |\n|>= 1980 but < 2000   | [0, 0, 1, 0] |\n|>= 2000              | [0, 0, 0, 1] |\nWhy would you want to split a number\u2014a perfectly valid input to your\nmodel\u2014into a categorical value? Well, notice that the categorization splits a\nsingle input number into a four-element vector. Therefore, the model now can\nlearn four individual weights rather than just one; four weights creates a\nricher model than one weight. More importantly, bucketizing enables the model\nto clearly distinguish between different year categories since only one of the\nelements is set (1) and the other three elements are cleared (0). For example,\nwhen we just use a single number (a year) as input, a linear model can only\nlearn a linear relationship. So, bucketing provides the model with additional\nflexibility that the model can use to learn.\nThe following code demonstrates how to create a bucketized feature:\n\n```python\nFirst, convert the raw input to a numeric column.\nnumeric_feature_column = tf.feature_column.numeric_column(\"Year\")\nThen, bucketize the numeric column on the years 1960, 1980, and 2000.\nbucketized_feature_column = tf.feature_column.bucketized_column(\n    source_column = numeric_feature_column,\n    boundaries = [1960, 1980, 2000])\n```\nNote that specifying a three-element boundaries vector creates a\nfour-element bucketized vector.\nCategorical identity column\nCategorical identity columns can be seen as a special case of bucketized\ncolumns. In traditional bucketized columns, each bucket represents a range of\nvalues (for example, from 1960 to 1979). In a categorical identity column, each\nbucket represents a single, unique integer. For example, let's say you want to\nrepresent the integer range `[0, 4)`.  That is, you want to represent the\nintegers 0, 1, 2, or 3. In this case, the categorical identity mapping looks\nlike this:\n\n\n\n\nA categorical identity column mapping. Note that this is a one-hot\nencoding, not a binary numerical encoding.\n\nAs with bucketized columns, a model can learn a separate weight for each class\nin a categorical identity column. For example, instead of using a string to\nrepresent the `product_class`, let's represent each class with a unique integer\nvalue. That is:\n\n`0=\"kitchenware\"`\n`1=\"electronics\"`\n`2=\"sport\"`\n\nCall `tf.feature_column.categorical_column_with_identity` to implement a\ncategorical identity column. For example:\n``` python\nCreate categorical output for an integer feature named \"my_feature_b\",\nThe values of my_feature_b must be >= 0 and < num_buckets\nidentity_feature_column = tf.feature_column.categorical_column_with_identity(\n    key='my_feature_b',\n    num_buckets=4) # Values [0, 4)\nIn order for the preceding call to work, the input_fn() must return\na dictionary containing 'my_feature_b' as a key. Furthermore, the values\nassigned to 'my_feature_b' must belong to the set [0, 4).\ndef input_fn():\n    ...\n    return ({ 'my_feature_a':[7, 9, 5, 2], 'my_feature_b':[3, 1, 2, 2] },\n            [Label_values])\n```\nCategorical vocabulary column\nWe cannot input strings directly to a model. Instead, we must first map strings\nto numeric or categorical values. Categorical vocabulary columns provide a good\nway to represent strings as a one-hot vector. For example:\n\n\n\n\nMapping string values to vocabulary columns.\n\nAs you can see, categorical vocabulary columns are kind of an enum version of\ncategorical identity columns. TensorFlow provides two different functions to\ncreate categorical vocabulary columns:\n\n`tf.feature_column.categorical_column_with_vocabulary_list`\n`tf.feature_column.categorical_column_with_vocabulary_file`\n\n`categorical_column_with_vocabulary_list` maps each string to an integer based\non an explicit vocabulary list. For example:\n```python\nGiven input \"feature_name_from_input_fn\" which is a string,\ncreate a categorical feature by mapping the input to one of\nthe elements in the vocabulary list.\nvocabulary_feature_column =\n    tf.feature_column.categorical_column_with_vocabulary_list(\n        key=feature_name_from_input_fn,\n        vocabulary_list=[\"kitchenware\", \"electronics\", \"sports\"])\n```\nThe preceding function is pretty straightforward, but it has a significant\ndrawback. Namely, there's way too much typing when the vocabulary list is long.\nFor these cases, call\n`tf.feature_column.categorical_column_with_vocabulary_file` instead, which lets\nyou place the vocabulary words in a separate file. For example:\n```python\nGiven input \"feature_name_from_input_fn\" which is a string,\ncreate a categorical feature to our model by mapping the input to one of\nthe elements in the vocabulary file\nvocabulary_feature_column =\n    tf.feature_column.categorical_column_with_vocabulary_file(\n        key=feature_name_from_input_fn,\n        vocabulary_file=\"product_class.txt\",\n        vocabulary_size=3)\n```\n`product_class.txt` should contain one line for each vocabulary element. In our\ncase:\n\nkitchenware\nelectronics\nsports\n\nHashed Column\nSo far, we've worked with a naively small number of categories. For example,\nour product_class example has only 3 categories. Often though, the number of\ncategories can be so big that it's not possible to have individual categories\nfor each vocabulary word or integer because that would consume too much memory.\nFor these cases, we can instead turn the question around and ask, \"How many\ncategories am I willing to have for my input?\"  In fact, the\n`tf.feature_column.categorical_column_with_hash_bucket` function enables you\nto specify the number of categories. For this type of feature column the model\ncalculates a hash value of the input, then puts it into one of\nthe `hash_bucket_size` categories using the modulo operator, as in the following\npseudocode:\n```python\npseudocode\nfeature_id = hash(raw_feature) % hash_bucket_size\n```\nThe code to create the `feature_column` might look something like this:\n`python\nhashed_feature_column =\n    tf.feature_column.categorical_column_with_hash_bucket(\n        key = \"some_feature\",\n        hash_bucket_size = 100) # The number of categories`\nAt this point, you might rightfully think: \"This is crazy!\" After all, we are\nforcing the different input values to a smaller set of categories. This means\nthat two probably unrelated inputs will be mapped to the same\ncategory, and consequently mean the same thing to the neural network. The\nfollowing figure illustrates this dilemma, showing that kitchenware and sports\nboth get assigned to category (hash bucket) 12:\n\n\n\n\nRepresenting data with hash buckets.\n\nAs with many counterintuitive phenomena in machine learning, it turns out that\nhashing often works well in practice. That's because hash categories provide\nthe model with some separation. The model can use additional features to further\nseparate kitchenware from sports.\nCrossed column\nCombining features into a single feature, better known as\nfeature crosses,\nenables the model to learn separate weights for each combination of\nfeatures.\nMore concretely, suppose we want our model to calculate real estate prices in\nAtlanta, GA. Real-estate prices within this city vary greatly depending on\nlocation. Representing latitude and longitude as separate features isn't very\nuseful in identifying real-estate location dependencies; however, crossing\nlatitude and longitude into a single feature can pinpoint locations. Suppose we\nrepresent Atlanta as a grid of 100x100 rectangular sections, identifying each\nof the 10,000 sections by a feature cross of latitude and longitude. This\nfeature cross enables the model to train on pricing conditions related to each\nindividual section, which is a much stronger signal than latitude and longitude\nalone.\nThe following figure shows our plan, with the latitude & longitude values for\nthe corners of the city in red text:\n\n\n\n\nMap of Atlanta. Imagine this map divided into 10,000 sections of\nequal size.\n\nFor the solution, we used a combination of the `bucketized_column` we looked at\nearlier, with the `tf.feature_column.crossed_column` function.\n\n``` python\ndef make_dataset(latitude, longitude, labels):\n    assert latitude.shape == longitude.shape == labels.shape\n\n\n```features = {'latitude': latitude.flatten(),\n            'longitude': longitude.flatten()}\nlabels=labels.flatten()\n\nreturn tf.data.Dataset.from_tensor_slices((features, labels))\n```\n\n\nBucketize the latitude and longitude using the `edges`\nlatitude_bucket_fc = tf.feature_column.bucketized_column(\n    tf.feature_column.numeric_column('latitude'),\n    list(atlanta.latitude.edges))\nlongitude_bucket_fc = tf.feature_column.bucketized_column(\n    tf.feature_column.numeric_column('longitude'),\n    list(atlanta.longitude.edges))\nCross the bucketized columns, using 5000 hash bins.\ncrossed_lat_lon_fc = tf.feature_column.crossed_column(\n    [latitude_bucket_fc, longitude_bucket_fc], 5000)\nfc = [\n    latitude_bucket_fc,\n    longitude_bucket_fc,\n    crossed_lat_lon_fc]\nBuild and train the Estimator.\nest = tf.estimator.LinearRegressor(fc, ...)\n```\nYou may create a feature cross from either of the following:\n\nFeature names; that is, names from the `dict` returned from `input_fn`.\nAny categorical column, except `categorical_column_with_hash_bucket`\n  (since `crossed_column` hashes the input).\n\nWhen the feature columns `latitude_bucket_fc` and `longitude_bucket_fc` are\ncrossed, TensorFlow will create `(latitude_fc, longitude_fc)` pairs for each\nexample. This would produce a full grid of possibilities as follows:\n\n (0,0),  (0,1)...  (0,99)\n (1,0),  (1,1)...  (1,99)\n   ...     ...       ...\n(99,0), (99,1)...(99, 99)\n\nExcept that a full grid would only be tractable for inputs with limited\nvocabularies. Instead of building this, potentially huge, table of inputs,\nthe `crossed_column` only builds the number requested by the `hash_bucket_size`\nargument. The feature column assigns an example to a index by running a hash\nfunction on the tuple of inputs, followed by a modulo operation with\n`hash_bucket_size`.\nAs discussed earlier, performing the\nhash and modulo function limits the number of categories, but can cause category\ncollisions; that is, multiple (latitude, longitude) feature crosses will end\nup in the same hash bucket. In practice though, performing feature crosses\nstill adds significant value to the learning capability of your models.\nSomewhat counterintuitively, when creating feature crosses, you typically still\nshould include the original (uncrossed) features in your model (as in the\npreceding code snippet). The independent latitude and longitude features help the\nmodel distinguish between examples where a hash collision has occurred in the\ncrossed feature.\nIndicator and embedding columns\nIndicator columns and embedding columns never work on features directly, but\ninstead take categorical columns as input.\nWhen using an indicator column, we're telling TensorFlow to do exactly what\nwe've seen in our categorical product_class example. That is, an\nindicator column treats each category as an element in a one-hot vector,\nwhere the matching category has value 1 and the rest have 0s:\n\n\n\n\nRepresenting data in indicator columns.\n\nHere's how you create an indicator column by calling\n`tf.feature_column.indicator_column`:\n``` python\ncategorical_column = ... # Create any type of categorical column.\nRepresent the categorical column as an indicator column.\nindicator_column = tf.feature_column.indicator_column(categorical_column)\n```\nNow, suppose instead of having just three possible classes, we have a million.\nOr maybe a billion. For a number of reasons, as the number of categories grow\nlarge, it becomes infeasible to train a neural network using indicator columns.\nWe can use an embedding column to overcome this limitation. Instead of\nrepresenting the data as a one-hot vector of many dimensions, an\nembedding column represents that data as a lower-dimensional, ordinary\nvector in which each cell can contain any number, not just 0 or 1. By\npermitting a richer palette of numbers for every cell, an embedding column\ncontains far fewer cells than an indicator column.\nLet's look at an example comparing indicator and embedding columns. Suppose our\ninput examples consist of different words from a limited palette of only 81\nwords. Further suppose that the data set provides the following input\nwords in 4 separate examples:\n\n`\"dog\"`\n`\"spoon\"`\n`\"scissors\"`\n`\"guitar\"`\n\nIn that case, the following figure illustrates the processing path for\nembedding columns or indicator columns.\n\n\n\n\nAn embedding column stores categorical data in a lower-dimensional\nvector than an indicator column. (We just placed random numbers into the\nembedding vectors; training determines the actual numbers.)\n\nWhen an example is processed, one of the `categorical_column_with...` functions\nmaps the example string to a numerical categorical value. For example, a\nfunction maps \"spoon\" to `[32]`. (The 32 comes from our imagination\u2014the actual\nvalues depend on the mapping function.) You may then represent these numerical\ncategorical values in either of the following two ways:\n\n\nAs an indicator column. A function converts each numeric categorical value\n  into an 81-element vector (because our palette consists of 81 words), placing\n  a 1 in the index of the categorical value (0, 32, 79, 80) and a 0 in all the\n  other positions.\n\n\nAs an embedding column. A function uses the numerical categorical values\n  `(0, 32, 79, 80)` as indices to a lookup table. Each slot in that lookup table\n  contains a 3-element vector.\n\n\nHow do the values in the embeddings vectors magically get assigned? Actually,\nthe assignments happen during training. That is, the model learns the best way\nto map your input numeric categorical values to the embeddings vector value in\norder to solve your problem. Embedding columns increase your model's\ncapabilities, since an embeddings vector learns new relationships between\ncategories from the training data.\nWhy is the embedding vector size 3 in our example? Well, the following \"formula\"\nprovides a general rule of thumb about the number of embedding dimensions:\n`python\nembedding_dimensions =  number_of_categories**0.25`\nThat is, the embedding vector dimension should be the 4th root of the number of\ncategories. Since our vocabulary size in this example is 81, the recommended\nnumber of dimensions is 3:\n`python\n3 =  81**0.25`\nNote: This is just a general guideline; you can set the number of embedding\ndimensions as you please.\nCall `tf.feature_column.embedding_column` to create an `embedding_column` as\nsuggested by the following snippet:\n``` python\ncategorical_column = ... # Create any categorical column\nRepresent the categorical column as an embedding column.\nThis means creating an embedding vector lookup table with one element for each category.\nembedding_column = tf.feature_column.embedding_column(\n    categorical_column=categorical_column,\n    dimension=embedding_dimensions)\n```\nEmbeddings is a significant topic within machine\nlearning. This information was just to get you started using them as feature\ncolumns.\nPassing feature columns to Estimators\nAs the following list indicates, not all Estimators permit all types of\n`feature_columns` argument(s):\n\n`tf.estimator.LinearClassifier` and\n  `tf.estimator.LinearRegressor`: Accept all types of\n  feature column.\n`tf.estimator.DNNClassifier` and\n  `tf.estimator.DNNRegressor`: Only accept dense columns. Other\n  column types must be wrapped in either an `indicator_column` or\n  `embedding_column`.\n`tf.estimator.DNNLinearCombinedClassifier` and\n  `tf.estimator.DNNLinearCombinedRegressor`:\nThe `linear_feature_columns` argument accepts any feature column type.\nThe `dnn_feature_columns` argument only accepts dense columns.\n\n\n\nOther Sources\nFor more examples on feature columns, view the following:\n\nThe Low Level Introduction demonstrates how\n  experiment directly with `feature_columns` using TensorFlow's low level APIs.\nThe Estimator wide and deep learning tutorial\n  solves a binary classification problem using `feature_columns` on a variety of\n  input data types.\n\nTo learn more about embeddings, see the following:\n\nDeep Learning, NLP, and representations\n  (Chris Olah's blog)\n",
    "tag": "tensorflow"
  },
  {
    "title": "Variables",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/guide/variables.md",
    "content": "Variables\nA TensorFlow variable is the best way to represent shared, persistent state\nmanipulated by your program.\nVariables are manipulated via the `tf.Variable` class. A `tf.Variable`\nrepresents a tensor whose value can be changed by running ops on it. Unlike\n`tf.Tensor` objects, a `tf.Variable` exists outside the context of a single\n`session.run` call.\nInternally, a `tf.Variable` stores a persistent tensor. Specific ops allow you\nto read and modify the values of this tensor. These modifications are visible\nacross multiple `tf.Session`s, so multiple workers can see the same values for a\n`tf.Variable`.\nCreating a Variable\nThe best way to create a variable is to call the `tf.get_variable`\nfunction. This function requires you to specify the Variable's name. This name\nwill be used by other replicas to access the same variable, as well as to name\nthis variable's value when checkpointing and exporting models. `tf.get_variable`\nalso allows you to reuse a previously created variable of the same name, making it\neasy to define models which reuse layers.\nTo create a variable with `tf.get_variable`, simply provide the name and shape\n`python\nmy_variable = tf.get_variable(\"my_variable\", [1, 2, 3])`\nThis creates a variable named \"my_variable\" which is a three-dimensional tensor\nwith shape `[1, 2, 3]`. This variable will, by default, have the `dtype`\n`tf.float32` and its initial value will be randomized via\n`tf.glorot_uniform_initializer`.\nYou may optionally specify the `dtype` and initializer to `tf.get_variable`. For\nexample:\n`python\nmy_int_variable = tf.get_variable(\"my_int_variable\", [1, 2, 3], dtype=tf.int32,\n  initializer=tf.zeros_initializer)`\nTensorFlow provides many convenient initializers. Alternatively, you may\ninitialize a `tf.Variable` to have the value of a `tf.Tensor`. For example:\n`python\nother_variable = tf.get_variable(\"other_variable\", dtype=tf.int32,\n  initializer=tf.constant([23, 42]))`\nNote that when the initializer is a `tf.Tensor` you should not specify the\nvariable's shape, as the shape of the initializer tensor will be used.\n\nVariable collections\nBecause disconnected parts of a TensorFlow program might want to create\nvariables, it is sometimes useful to have a single way to access all of\nthem. For this reason TensorFlow provides collections, which are named lists\nof tensors or other objects, such as `tf.Variable` instances.\nBy default every `tf.Variable` gets placed in the following two collections:\n\n`tf.GraphKeys.GLOBAL_VARIABLES` --- variables that can be shared across\n   multiple devices,\n`tf.GraphKeys.TRAINABLE_VARIABLES` --- variables for which TensorFlow will\n   calculate gradients.\n\nIf you don't want a variable to be trainable, add it to the\n`tf.GraphKeys.LOCAL_VARIABLES` collection instead. For example, the following\nsnippet demonstrates how to add a variable named `my_local` to this collection:\n`python\nmy_local = tf.get_variable(\"my_local\", shape=(),\ncollections=[tf.GraphKeys.LOCAL_VARIABLES])`\nAlternatively, you can specify `trainable=False` as an argument to\n`tf.get_variable`:\n`python\nmy_non_trainable = tf.get_variable(\"my_non_trainable\",\n                                   shape=(),\n                                   trainable=False)`\nYou can also use your own collections. Any string is a valid collection name,\nand there is no need to explicitly create a collection. To add a variable (or\nany other object) to a collection after creating the variable, call\n`tf.add_to_collection`.  For example, the following code adds an existing\nvariable named `my_local` to a collection named `my_collection_name`:\n`python\ntf.add_to_collection(\"my_collection_name\", my_local)`\nAnd to retrieve a list of all the variables (or other objects) you've placed in\na collection you can use:\n`python\ntf.get_collection(\"my_collection_name\")`\nDevice placement\nJust like any other TensorFlow operation, you can place variables on particular\ndevices. For example, the following snippet creates a variable named `v` and\nplaces it on the second GPU device:\n`python\nwith tf.device(\"/device:GPU:1\"):\n  v = tf.get_variable(\"v\", [1])`\nIt is particularly important for variables to be in the correct device in\ndistributed settings. Accidentally putting variables on workers instead of\nparameter servers, for example, can severely slow down training or, in the worst\ncase, let each worker blithely forge ahead with its own independent copy of each\nvariable. For this reason we provide `tf.train.replica_device_setter`, which\ncan automatically place variables in parameter servers. For example:\n`python\ncluster_spec = {\n    \"ps\": [\"ps0:2222\", \"ps1:2222\"],\n    \"worker\": [\"worker0:2222\", \"worker1:2222\", \"worker2:2222\"]}\nwith tf.device(tf.train.replica_device_setter(cluster=cluster_spec)):\n  v = tf.get_variable(\"v\", shape=[20, 20])  # this variable is placed\n                                            # in the parameter server\n                                            # by the replica_device_setter`\nInitializing variables\nBefore you can use a variable, it must be initialized. If you are programming in\nthe low-level TensorFlow API (that is, you are explicitly creating your own\ngraphs and sessions), you must explicitly initialize the variables.  Most\nhigh-level frameworks such as `tf.contrib.slim`, `tf.estimator.Estimator` and\n`Keras` automatically initialize variables for you before training a model.\nExplicit initialization is otherwise useful because it allows you not to rerun\npotentially expensive initializers when reloading a model from a checkpoint as\nwell as allowing determinism when randomly-initialized variables are shared in a\ndistributed setting.\nTo initialize all trainable variables in one go, before training starts, call\n`tf.global_variables_initializer()`. This function returns a single operation\nresponsible for initializing all variables in the\n`tf.GraphKeys.GLOBAL_VARIABLES` collection. Running this operation initializes\nall variables. For example:\n``` python\nsession.run(tf.global_variables_initializer())\nNow all variables are initialized.\n```\nIf you do need to initialize variables yourself, you can run the variable's\ninitializer operation. For example:\n`python\nsession.run(my_variable.initializer)`\nYou can also ask which variables have still not been initialized. For example,\nthe following code prints the names of all variables which have not yet been\ninitialized:\n`python\nprint(session.run(tf.report_uninitialized_variables()))`\nNote that by default `tf.global_variables_initializer` does not specify the\norder in which variables are initialized. Therefore, if the initial value of a\nvariable depends on another variable's value, it's likely that you'll get an\nerror. Any time you use the value of a variable in a context in which not all\nvariables are initialized (say, if you use a variable's value while initializing\nanother variable), it is best to use `variable.initialized_value()` instead of\n`variable`:\n`python\nv = tf.get_variable(\"v\", shape=(), initializer=tf.zeros_initializer())\nw = tf.get_variable(\"w\", initializer=v.initialized_value() + 1)`\nUsing variables\nTo use the value of a `tf.Variable` in a TensorFlow graph, simply treat it like\na normal `tf.Tensor`:\n`python\nv = tf.get_variable(\"v\", shape=(), initializer=tf.zeros_initializer())\nw = v + 1  # w is a tf.Tensor which is computed based on the value of v.\n           # Any time a variable is used in an expression it gets automatically\n           # converted to a tf.Tensor representing its value.`\nTo assign a value to a variable, use the methods `assign`, `assign_add`, and\nfriends in the `tf.Variable` class. For example, here is how you can call these\nmethods:\n`python\nv = tf.get_variable(\"v\", shape=(), initializer=tf.zeros_initializer())\nassignment = v.assign_add(1)\ntf.global_variables_initializer().run()\nsess.run(assignment)  # or assignment.op.run(), or assignment.eval()`\nMost TensorFlow optimizers have specialized ops that efficiently update the\nvalues of variables according to some gradient descent-like algorithm. See\n`tf.train.Optimizer` for an explanation of how to use optimizers.\nBecause variables are mutable it's sometimes useful to know what version of a\nvariable's value is being used at any point in time. To force a re-read of the\nvalue of a variable after something has happened, you can use\n`tf.Variable.read_value`. For example:\n`python\nv = tf.get_variable(\"v\", shape=(), initializer=tf.zeros_initializer())\nassignment = v.assign_add(1)\nwith tf.control_dependencies([assignment]):\n  w = v.read_value()  # w is guaranteed to reflect v's value after the\n                      # assign_add operation.`\nSharing variables\nTensorFlow supports two ways of sharing variables:\n\nExplicitly passing `tf.Variable` objects around.\nImplicitly wrapping `tf.Variable` objects within `tf.variable_scope` objects.\n\nWhile code which explicitly passes variables around is very clear, it is\nsometimes convenient to write TensorFlow functions that implicitly use\nvariables in their implementations. Most of the functional layers from\n`tf.layers` use this approach, as well as all `tf.metrics`, and a few other\nlibrary utilities.\nVariable scopes allow you to control variable reuse when calling functions which\nimplicitly create and use variables. They also allow you to name your variables\nin a hierarchical and understandable way.\nFor example, let's say we write a function to create a convolutional / relu\nlayer:\n`python\ndef conv_relu(input, kernel_shape, bias_shape):\n    # Create variable named \"weights\".\n    weights = tf.get_variable(\"weights\", kernel_shape,\n        initializer=tf.random_normal_initializer())\n    # Create variable named \"biases\".\n    biases = tf.get_variable(\"biases\", bias_shape,\n        initializer=tf.constant_initializer(0.0))\n    conv = tf.nn.conv2d(input, weights,\n        strides=[1, 1, 1, 1], padding='SAME')\n    return tf.nn.relu(conv + biases)`\nThis function uses short names `weights` and `biases`, which is good for\nclarity. In a real model, however, we want many such convolutional layers, and\ncalling this function repeatedly would not work:\n`python\ninput1 = tf.random_normal([1,10,10,32])\ninput2 = tf.random_normal([1,20,20,32])\nx = conv_relu(input1, kernel_shape=[5, 5, 32, 32], bias_shape=[32])\nx = conv_relu(x, kernel_shape=[5, 5, 32, 32], bias_shape = [32])  # This fails.`\nSince the desired behavior is unclear (create new variables or reuse the\nexisting ones?) TensorFlow will fail. Calling `conv_relu` in different scopes,\nhowever, clarifies that we want to create new variables:\n`python\ndef my_image_filter(input_images):\n    with tf.variable_scope(\"conv1\"):\n        # Variables created here will be named \"conv1/weights\", \"conv1/biases\".\n        relu1 = conv_relu(input_images, [5, 5, 32, 32], [32])\n    with tf.variable_scope(\"conv2\"):\n        # Variables created here will be named \"conv2/weights\", \"conv2/biases\".\n        return conv_relu(relu1, [5, 5, 32, 32], [32])`\nIf you do want the variables to be shared, you have two options. First, you can\ncreate a scope with the same name using `reuse=True`:\n`python\nwith tf.variable_scope(\"model\"):\n  output1 = my_image_filter(input1)\nwith tf.variable_scope(\"model\", reuse=True):\n  output2 = my_image_filter(input2)`\nYou can also call `scope.reuse_variables()` to trigger a reuse:\n`python\nwith tf.variable_scope(\"model\") as scope:\n  output1 = my_image_filter(input1)\n  scope.reuse_variables()\n  output2 = my_image_filter(input2)`\nSince depending on exact string names of scopes can feel dangerous, it's also\npossible to initialize a variable scope based on another one:\n``` python\nwith tf.variable_scope(\"model\") as scope:\n  output1 = my_image_filter(input1)\nwith tf.variable_scope(scope, reuse=True):\n  output2 = my_image_filter(input2)",
    "tag": "tensorflow"
  },
  {
    "title": "Tensors",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/guide/tensors.md",
    "content": "Tensors\nTensorFlow, as the name indicates, is a framework to define and run computations\ninvolving tensors. A tensor is a generalization of vectors and matrices to\npotentially higher dimensions. Internally, TensorFlow represents tensors as\nn-dimensional arrays of base datatypes.\nWhen writing a TensorFlow program, the main object you manipulate and pass\naround is the `tf.Tensor`. A `tf.Tensor` object represents a partially defined\ncomputation that will eventually produce a value. TensorFlow programs work by\nfirst building a graph of `tf.Tensor` objects, detailing how each tensor is\ncomputed based on the other available tensors and then by running parts of this\ngraph to achieve the desired results.\nA `tf.Tensor` has the following properties:\n\na data type (`float32`, `int32`, or `string`, for example)\na shape\n\nEach element in the Tensor has the same data type, and the data type is always\nknown. The shape (that is, the number of dimensions it has and the size of each\ndimension) might be only partially known. Most operations produce tensors of\nfully-known shapes if the shapes of their inputs are also fully known, but in\nsome cases it's only possible to find the shape of a tensor at graph execution\ntime.\nSome types of tensors are special, and these will be covered in other\nunits of the TensorFlow guide. The main ones are:\n\n`tf.Variable`\n`tf.constant`\n`tf.placeholder`\n`tf.SparseTensor`\n\nWith the exception of `tf.Variable`, the value of a tensor is immutable, which\nmeans that in the context of a single execution tensors only have a single\nvalue. However, evaluating the same tensor twice can return different values;\nfor example that tensor can be the result of reading data from disk, or\ngenerating a random number.\nRank\nThe rank of a `tf.Tensor` object is its number of dimensions. Synonyms for\nrank include order or degree or n-dimension.\nNote that rank in TensorFlow is not the same as matrix rank in mathematics.\nAs the following table shows, each rank in TensorFlow corresponds to a\ndifferent mathematical entity:\nRank | Math entity\n--- | ---\n0 | Scalar (magnitude only)\n1 | Vector (magnitude and direction)\n2 | Matrix (table of numbers)\n3 | 3-Tensor (cube of numbers)\nn | n-Tensor (you get the idea)\nRank 0\nThe following snippet demonstrates creating a few rank 0 variables:\n`python\nmammal = tf.Variable(\"Elephant\", tf.string)\nignition = tf.Variable(451, tf.int16)\nfloating = tf.Variable(3.14159265359, tf.float64)\nits_complicated = tf.Variable(12.3 - 4.85j, tf.complex64)`\nNote: A string is treated as a single object in TensorFlow, not as a sequence of\ncharacters. It is possible to have scalar strings, vectors of strings, etc.\nRank 1\nTo create a rank 1 `tf.Tensor` object, you can pass a list of items as the\ninitial value. For example:\n`python\nmystr = tf.Variable([\"Hello\"], tf.string)\ncool_numbers  = tf.Variable([3.14159, 2.71828], tf.float32)\nfirst_primes = tf.Variable([2, 3, 5, 7, 11], tf.int32)\nits_very_complicated = tf.Variable([12.3 - 4.85j, 7.5 - 6.23j], tf.complex64)`\nHigher ranks\nA rank 2 `tf.Tensor` object consists of at least one row and at least\none column:\n`python\nmymat = tf.Variable([[7],[11]], tf.int16)\nmyxor = tf.Variable([[False, True],[True, False]], tf.bool)\nlinear_squares = tf.Variable([[4], [9], [16], [25]], tf.int32)\nsquarish_squares = tf.Variable([ [4, 9], [16, 25] ], tf.int32)\nrank_of_squares = tf.rank(squarish_squares)\nmymatC = tf.Variable([[7],[11]], tf.int32)`\nHigher-rank Tensors, similarly, consist of an n-dimensional array. For example,\nduring image processing, many tensors of rank 4 are used, with dimensions\ncorresponding to example-in-batch, image height, image width, and color channel.\n`python\nmy_image = tf.zeros([10, 299, 299, 3])  # batch x height x width x color`\nGetting a `tf.Tensor` object's rank\nTo determine the rank of a `tf.Tensor` object, call the `tf.rank` method.\nFor example, the following method programmatically determines the rank\nof the `tf.Tensor` defined in the previous section:\n```python\nr = tf.rank(my_image)\nAfter the graph runs, r will hold the value 4.\n```\nReferring to `tf.Tensor` slices\nSince a `tf.Tensor` is an n-dimensional array of cells, to access a single cell\nin a `tf.Tensor` you need to specify n indices.\nFor a rank 0 tensor (a scalar), no indices are necessary, since it is already a\nsingle number.\nFor a rank 1 tensor (a vector), passing a single index allows you to access a\nnumber:\n`python\nmy_scalar = my_vector[2]`\nNote that the index passed inside the `[]` can itself be a scalar `tf.Tensor`, if\nyou want to dynamically choose an element from the vector.\nFor tensors of rank 2 or higher, the situation is more interesting. For a\n`tf.Tensor` of rank 2, passing two numbers returns a scalar, as expected:\n`python\nmy_scalar = my_matrix[1, 2]`\nPassing a single number, however, returns a subvector of a matrix, as follows:\n`python\nmy_row_vector = my_matrix[2]\nmy_column_vector = my_matrix[:, 3]`\nThe `:` notation is python slicing syntax for \"leave this dimension alone\". This\nis useful in higher-rank Tensors, as it allows you to access its subvectors,\nsubmatrices, and even other subtensors.\nShape\nThe shape of a tensor is the number of elements in each dimension.\nTensorFlow automatically infers shapes during graph construction. These inferred\nshapes might have known or unknown rank. If the rank is known, the sizes of each\ndimension might be known or unknown.\nThe TensorFlow documentation uses three notational conventions to describe\ntensor dimensionality: rank, shape, and dimension number. The following table\nshows how these relate to one another:\nRank | Shape | Dimension number | Example\n--- | --- | --- | ---\n0 | [] | 0-D | A 0-D tensor.  A scalar.\n1 | [D0] | 1-D | A 1-D tensor with shape [5].\n2 | [D0, D1] | 2-D | A 2-D tensor with shape [3, 4].\n3 | [D0, D1, D2] | 3-D | A 3-D tensor with shape [1, 4, 3].\nn | [D0, D1, ... Dn-1] | n-D | A tensor with shape [D0, D1, ... Dn-1].\nShapes can be represented via Python lists / tuples of ints, or with the\n`tf.TensorShape`.\nGetting a `tf.Tensor` object's shape\nThere are two ways of accessing the shape of a `tf.Tensor`. While building the\ngraph, it is often useful to ask what is already known about a tensor's\nshape. This can be done by reading the `shape` property of a `tf.Tensor` object.\nThis method returns a `TensorShape` object, which is a convenient way of\nrepresenting partially-specified shapes (since, when building the graph, not all\nshapes will be fully known).\nIt is also possible to get a `tf.Tensor` that will represent the fully-defined\nshape of another `tf.Tensor` at runtime. This is done by calling the `tf.shape`\noperation. This way, you can build a graph that manipulates the shapes of\ntensors by building other tensors that depend on the dynamic shape of the input\n`tf.Tensor`.\nFor example, here is how to make a vector of zeros with the same size as the\nnumber of columns in a given matrix:\n`python\nzeros = tf.zeros(my_matrix.shape[1])`\nChanging the shape of a `tf.Tensor`\nThe number of elements of a tensor is the product of the sizes of all its\nshapes. The number of elements of a scalar is always `1`. Since there are often\nmany different shapes that have the same number of elements, it's often\nconvenient to be able to change the shape of a `tf.Tensor`, keeping its elements\nfixed. This can be done with `tf.reshape`.\nThe following examples demonstrate how to reshape tensors:\n```python\nrank_three_tensor = tf.ones([3, 4, 5])\nmatrix = tf.reshape(rank_three_tensor, [6, 10])  # Reshape existing content into\n                                                 # a 6x10 matrix\nmatrixB = tf.reshape(matrix, [3, -1])  #  Reshape existing content into a 3x20\n                                       # matrix. -1 tells reshape to calculate\n                                       # the size of this dimension.\nmatrixAlt = tf.reshape(matrixB, [4, 3, -1])  # Reshape existing content into a\n                                             #4x3x5 tensor\nNote that the number of elements of the reshaped Tensors has to match the\noriginal number of elements. Therefore, the following example generates an\nerror because no possible value for the last dimension will match the number\nof elements.\nyet_another = tf.reshape(matrixAlt, [13, 2, -1])  # ERROR!\n```\nData types\nIn addition to dimensionality, Tensors have a data type. Refer to the\n`tf.DType` page for a complete list of the data types.\nIt is not possible to have a `tf.Tensor` with more than one data type. It is\npossible, however, to serialize arbitrary data structures as `string`s and store\nthose in `tf.Tensor`s.\nIt is possible to cast `tf.Tensor`s from one datatype to another using\n`tf.cast`:\n``` python\nCast a constant integer tensor into floating point.\nfloat_tensor = tf.cast(tf.constant([1, 2, 3]), dtype=tf.float32)\n```\nTo inspect a `tf.Tensor`'s data type use the `Tensor.dtype` property.\nWhen creating a `tf.Tensor` from a python object you may optionally specify the\ndatatype. If you don't, TensorFlow chooses a datatype that can represent your\ndata. TensorFlow converts Python integers to `tf.int32` and python floating\npoint numbers to `tf.float32`. Otherwise TensorFlow uses the same rules numpy\nuses when converting to arrays.\nEvaluating Tensors\nOnce the computation graph has been built, you can run the computation that\nproduces a particular `tf.Tensor` and fetch the value assigned to it. This is\noften useful for debugging as well as being required for much of TensorFlow to\nwork.\nThe simplest way to evaluate a Tensor is using the `Tensor.eval` method. For\nexample:\n`python\nconstant = tf.constant([1, 2, 3])\ntensor = constant * constant\nprint(tensor.eval())`\nThe `eval` method only works when a default `tf.Session` is active (see\nGraphs and Sessions for more information).\n`Tensor.eval` returns a numpy array with the same contents as the tensor.\nSometimes it is not possible to evaluate a `tf.Tensor` with no context because\nits value might depend on dynamic information that is not available. For\nexample, tensors that depend on `placeholder`s can't be evaluated without\nproviding a value for the `placeholder`.\n`python\np = tf.placeholder(tf.float32)\nt = p + 1.0\nt.eval()  # This will fail, since the placeholder did not get a value.\nt.eval(feed_dict={p:2.0})  # This will succeed because we're feeding a value\n                           # to the placeholder.`\nNote that it is possible to feed any `tf.Tensor`, not just placeholders.\nOther model constructs might make evaluating a `tf.Tensor`\ncomplicated. TensorFlow can't directly evaluate `tf.Tensor`s defined inside\nfunctions or inside control flow constructs. If a `tf.Tensor` depends on a value\nfrom a queue, evaluating the `tf.Tensor` will only work once something has been\nenqueued; otherwise, evaluating it will hang. When working with queues, remember\nto call `tf.train.start_queue_runners` before evaluating any `tf.Tensor`s.\nPrinting Tensors\nFor debugging purposes you might want to print the value of a `tf.Tensor`. While\n tfdbg provides advanced debugging support, TensorFlow also has an\n operation to directly print the value of a `tf.Tensor`.\nNote that you rarely want to use the following pattern when printing a\n`tf.Tensor`:\n`python\nt = <<some tensorflow operation>>\nprint(t)  # This will print the symbolic tensor when the graph is being built.\n          # This tensor does not have a value in this context.`\nThis code prints the `tf.Tensor` object (which represents deferred computation)\nand not its value. Instead, TensorFlow provides the `tf.Print` operation, which\nreturns its first tensor argument unchanged while printing the set of\n`tf.Tensor`s it is passed as the second argument.\nTo correctly use `tf.Print` its return value must be used. See the example below\n`python\nt = <<some tensorflow operation>>\ntf.Print(t, [t])  # This does nothing\nt = tf.Print(t, [t])  # Here we are using the value returned by tf.Print\nresult = t + 1  # Now when result is evaluated the value of `t` will be printed.`\nWhen you evaluate `result` you will evaluate everything `result` depends\nupon. Since `result` depends upon `t`, and evaluating `t` has the side effect of\nprinting its input (the old value of `t`), `t` gets printed.",
    "tag": "tensorflow"
  },
  {
    "title": "Reading custom file and record formats",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/guide/extend/formats.md",
    "content": "Reading custom file and record formats\nPREREQUISITES:\n\nSome familiarity with C++.\nMust have downloaded TensorFlow source, and be\n    able to build it.\n\nWe divide the task of supporting a file format into two pieces:\n\nFile formats: We use a reader `tf.data.Dataset` to read raw records (which\n    are typically represented by scalar string tensors, but can have more\n    structure) from a file.\nRecord formats: We use decoder or parsing ops to turn a string record\n    into tensors usable by TensorFlow.\n\nFor example, to re-implement `tf.contrib.data.make_csv_dataset` function, we\ncould use `tf.data.TextLineDataset` to extract the records, and then\nuse `tf.data.Dataset.map` and `tf.decode_csv` to parses the CSV records from\neach line of text in the dataset.\n[TOC]\nWriting a `Dataset` for a file format\nA `tf.data.Dataset` represents a sequence of elements, which can be the\nindividual records in a file. There are several examples of \"reader\" datasets\nthat are already built into TensorFlow:\n\n`tf.data.TFRecordDataset`\n    (source in kernels/data/reader_dataset_ops.cc)\n`tf.data.FixedLengthRecordDataset`\n    (source in kernels/data/reader_dataset_ops.cc)\n`tf.data.TextLineDataset`\n    (source in kernels/data/reader_dataset_ops.cc)\n\nEach of these implementations comprises three related classes:\n\n\nA `tensorflow::DatasetOpKernel` subclass (e.g. `TextLineDatasetOp`), which\n  tells TensorFlow how to construct a dataset object from the inputs to and\n  attrs of an op, in its `MakeDataset()` method.\n\n\nA `tensorflow::GraphDatasetBase` subclass (e.g. `TextLineDatasetOp::Dataset`),\n  which represents the immutable definition of the dataset itself, and tells\n  TensorFlow how to construct an iterator object over that dataset, in its\n  `MakeIteratorInternal()` method.\n\n\nA `tensorflow::DatasetIterator<Dataset>` subclass (e.g.\n  `TextLineDatasetOp::Dataset::Iterator`), which represents the mutable state\n  of an iterator over a particular dataset, and tells TensorFlow how to get the\n  next element from the iterator, in its `GetNextInternal()` method.\n\n\nThe most important method is the `GetNextInternal()` method, since it defines\nhow to actually read records from the file and represent them as one or more\n`Tensor` objects.\nTo create a new reader dataset called (for example) `MyReaderDataset`, you will\nneed to:\n\nIn C++, define subclasses of `tensorflow::DatasetOpKernel`,\n   `tensorflow::GraphDatasetBase`, and `tensorflow::DatasetIterator<Dataset>`\n   that implement the reading logic.\nIn C++, register a new reader op and kernel with the name\n   `\"MyReaderDataset\"`.\nIn Python, define a subclass of `tf.data.Dataset` called `MyReaderDataset`.\n\nYou can put all the C++ code in a single file, such as\n`my_reader_dataset_op.cc`. It will help if you are\nfamiliar with the adding an op how-to. The following skeleton\ncan be used as a starting point for your implementation:\n```c++\ninclude \"tensorflow/core/framework/common_shape_fns.h\"\ninclude \"tensorflow/core/framework/dataset.h\"\ninclude \"tensorflow/core/framework/op.h\"\ninclude \"tensorflow/core/framework/shape_inference.h\"\nnamespace myproject {\nnamespace {\nusing ::tensorflow::DT_STRING;\nusing ::tensorflow::PartialTensorShape;\nusing ::tensorflow::Status;\nclass MyReaderDatasetOp : public tensorflow::DatasetOpKernel {\n public:\nMyReaderDatasetOp(tensorflow::OpKernelConstruction* ctx)\n      : DatasetOpKernel(ctx) {\n    // Parse and validate any attrs that define the dataset using\n    // `ctx->GetAttr()`, and store them in member variables.\n  }\nvoid MakeDataset(tensorflow::OpKernelContext ctx,\n                   tensorflow::DatasetBase* output) override {\n    // Parse and validate any input tensors that define the dataset using\n    // `ctx->input()` or the utility function\n    // `ParseScalarArgument<T>(ctx, &arg)`.\n\n\n```// Create the dataset object, passing any (already-validated) arguments from\n// attrs or input tensors.\n*output = new Dataset(ctx);\n```\n\n\n}\nprivate:\n  class Dataset : public tensorflow::GraphDatasetBase {\n   public:\n    Dataset(tensorflow::OpKernelContext* ctx) : GraphDatasetBase(ctx) {}\n\n\n```std::unique_ptr<tensorflow::IteratorBase> MakeIteratorInternal(\n    const string& prefix) const override {\n  return std::unique_ptr<tensorflow::IteratorBase>(new Iterator(\n      {this, tensorflow::strings::StrCat(prefix, \"::MyReader\")}));\n}\n\n// Record structure: Each record is represented by a scalar string tensor.\n//\n// Dataset elements can have a fixed number of components of different\n// types and shapes; replace the following two methods to customize this\n// aspect of the dataset.\nconst tensorflow::DataTypeVector& output_dtypes() const override {\n  static auto* const dtypes = new tensorflow::DataTypeVector({DT_STRING});\n  return *dtypes;\n}\nconst std::vector<PartialTensorShape>& output_shapes() const override {\n  static std::vector<PartialTensorShape>* shapes =\n      new std::vector<PartialTensorShape>({{}});\n  return *shapes;\n}\n\nstring DebugString() const override { return \"MyReaderDatasetOp::Dataset\"; }\n```\n\n\nprotected:\n    // Optional: Implementation of `GraphDef` serialization for this dataset.\n    //\n    // Implement this method if you want to be able to save and restore\n    // instances of this dataset (and any iterators over it).\n    Status AsGraphDefInternal(DatasetGraphDefBuilder b,\n                              tensorflow::Node* output) const override {\n      // Construct nodes to represent any of the input tensors from this\n      // object's member variables using `b->AddScalar()` and `b->AddVector()`.\n      std::vector input_tensors;\n      TF_RETURN_IF_ERROR(b->AddDataset(this, input_tensors, output));\n      return Status::OK();\n    }\nprivate:\n    class Iterator : public tensorflow::DatasetIterator {\n     public:\n      explicit Iterator(const Params& params)\n          : DatasetIterator(params), i_(0) {}\n\n\n```  // Implementation of the reading logic.\n  //\n  // The example implementation in this file yields the string \"MyReader!\"\n  // ten times. In general there are three cases:\n  //\n  // 1. If an element is successfully read, store it as one or more tensors\n  //    in `*out_tensors`, set `*end_of_sequence = false` and return\n  //    `Status::OK()`.\n  // 2. If the end of input is reached, set `*end_of_sequence = true` and\n  //    return `Status::OK()`.\n  // 3. If an error occurs, return an error status using one of the helper\n  //    functions from \"tensorflow/core/lib/core/errors.h\".\n  Status GetNextInternal(tensorflow::IteratorContext* ctx,\n                         std::vector<tensorflow::Tensor>* out_tensors,\n                         bool* end_of_sequence) override {\n    // NOTE: `GetNextInternal()` may be called concurrently, so it is\n    // recommended that you protect the iterator state with a mutex.\n    tensorflow::mutex_lock l(mu_);\n    if (i_ < 10) {\n      // Create a scalar string tensor and add it to the output.\n      tensorflow::Tensor record_tensor(ctx->allocator({}), DT_STRING, {});\n      record_tensor.scalar<string>()() = \"MyReader!\";\n      out_tensors->emplace_back(std::move(record_tensor));\n      ++i_;\n      *end_of_sequence = false;\n    } else {\n      *end_of_sequence = true;\n    }\n    return Status::OK();\n  }\n\n protected:\n  // Optional: Implementation of iterator state serialization for this\n  // iterator.\n  //\n  // Implement these two methods if you want to be able to save and restore\n  // instances of this iterator.\n  Status SaveInternal(tensorflow::IteratorStateWriter* writer) override {\n    tensorflow::mutex_lock l(mu_);\n    TF_RETURN_IF_ERROR(writer->WriteScalar(full_name(\"i\"), i_));\n    return Status::OK();\n  }\n  Status RestoreInternal(tensorflow::IteratorContext* ctx,\n                         tensorflow::IteratorStateReader* reader) override {\n    tensorflow::mutex_lock l(mu_);\n    TF_RETURN_IF_ERROR(reader->ReadScalar(full_name(\"i\"), &i_));\n    return Status::OK();\n  }\n\n private:\n  tensorflow::mutex mu_;\n  int64 i_ GUARDED_BY(mu_);\n};\n```\n\n\n};\n};\n// Register the op definition for MyReaderDataset.\n//\n// Dataset ops always have a single output, of type `variant`, which represents\n// the constructed `Dataset` object.\n//\n// Add any attrs and input tensors that define the dataset here.\nREGISTER_OP(\"MyReaderDataset\")\n    .Output(\"handle: variant\")\n    .SetIsStateful()\n    .SetShapeFn(tensorflow::shape_inference::ScalarShape);\n// Register the kernel implementation for MyReaderDataset.\nREGISTER_KERNEL_BUILDER(Name(\"MyReaderDataset\").Device(tensorflow::DEVICE_CPU),\n                        MyReaderDatasetOp);\n}  // namespace\n}  // namespace myproject\n```\nThe last step is to build the C++ code and add a Python wrapper. The easiest way\nto do this is by compiling a dynamic\nlibrary (e.g. called `\"my_reader_dataset_op.so\"`), and adding a Python class\nthat subclasses `tf.data.Dataset` to wrap it. An example Python program is\ngiven here:\n```python\nimport tensorflow as tf\nAssumes the file is in the current working directory.\nmy_reader_dataset_module = tf.load_op_library(\"./my_reader_dataset_op.so\")\nclass MyReaderDataset(tf.data.Dataset):\ndef init(self):\n    super(MyReaderDataset, self).init()\n    # Create any input attrs or tensors as members of this class.\ndef _as_variant_tensor(self):\n    # Actually construct the graph node for the dataset op.\n    #\n    # This method will be invoked when you create an iterator on this dataset\n    # or a dataset derived from it.\n    return my_reader_dataset_module.my_reader_dataset()\n# The following properties define the structure of each element: a scalar\n  # `tf.string` tensor. Change these properties to match the `output_dtypes()`\n  # and `output_shapes()` methods of `MyReaderDataset::Dataset` if you modify\n  # the structure of each element.\n  @property\n  def output_types(self):\n    return tf.string\n@property\n  def output_shapes(self):\n    return tf.TensorShape([])\n@property\n  def output_classes(self):\n    return tf.Tensor\nif name == \"main\":\n  # Create a MyReaderDataset and print its elements.\n  with tf.Session() as sess:\n    iterator = MyReaderDataset().make_one_shot_iterator()\n    next_element = iterator.get_next()\n    try:\n      while True:\n        print(sess.run(next_element))  # Prints \"MyReader!\" ten times.\n    except tf.errors.OutOfRangeError:\n      pass\n```\nYou can see some examples of `Dataset` wrapper classes in\ntensorflow/python/data/ops/dataset_ops.py.\nWriting an Op for a record format\nGenerally this is an ordinary op that takes a scalar string record as input, and\nso follow the instructions to add an Op.\nYou may optionally take a scalar string key as input, and include that in error\nmessages reporting improperly formatted data.  That way users can more easily\ntrack down where the bad data came from.\nExamples of Ops useful for decoding records:\n\n`tf.parse_single_example` (and `tf.parse_example`)\n`tf.decode_csv`\n`tf.decode_raw`\n\nNote that it can be useful to use multiple Ops to decode a particular record\nformat.  For example, you may have an image saved as a string in\na tf.train.Example protocol buffer.\nDepending on the format of that image, you might take the corresponding output\nfrom a `tf.parse_single_example` op and call `tf.image.decode_jpeg`,\n`tf.image.decode_png`, or `tf.decode_raw`.  It is common to take the output",
    "tag": "tensorflow"
  },
  {
    "title": "TensorFlow in other languages",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/guide/extend/bindings.md",
    "content": "TensorFlow in other languages\nBackground\nThis document is intended as a guide for those interested in the creation or\ndevelopment of TensorFlow functionality in other programming languages. It\ndescribes the features of TensorFlow and recommended steps for making the same\navailable in other programming languages.\nPython was the first client language supported by TensorFlow and currently\nsupports the most features. More and more of that functionality is being moved\ninto the core of TensorFlow (implemented in C++) and exposed via a [C API].\nClient languages should use the language's foreign function interface\n(FFI) to call into\nthis [C API] to provide TensorFlow functionality.\nOverview\nProviding TensorFlow functionality in a programming language can be broken down\ninto broad categories:\n\nRun a predefined graph: Given a `GraphDef` (or\n    `MetaGraphDef`) protocol message, be able to create a session, run queries,\n    and get tensor results. This is sufficient for a mobile app or server that\n    wants to run inference on a pre-trained model.\nGraph construction: At least one function per defined\n    TensorFlow op that adds an operation to the graph. Ideally these functions\n    would be automatically generated so they stay in sync as the op definitions\n    are modified.\nGradients (AKA automatic differentiation): Given a graph and a list of\n    input and output operations, add operations to the graph that compute the\n    partial derivatives (gradients) of the inputs with respect to the outputs.\n    Allows for customization of the gradient function for a particular operation\n    in the graph.\nFunctions: Define a subgraph that may be called in multiple places in the\n    main `GraphDef`. Defines a `FunctionDef` in the `FunctionDefLibrary`\n    included in a `GraphDef`.\nControl Flow: Construct \"If\" and \"While\" with user-specified subgraphs.\n    Ideally these work with gradients (see above).\nNeural Network library: A number of components that together support the\n    creation of neural network models and training them (possibly in a\n    distributed setting). While it would be convenient to have this available in\n    other languages, there are currently no plans to support this in languages\n    other than Python. These libraries are typically wrappers over the features\n    described above.\n\nAt a minimum, a language binding should support running a predefined graph, but\nmost should also support graph construction. The TensorFlow Python API provides\nall these features.\nCurrent Status\nNew language support should be built on top of the [C API]. However, as you can\nsee in the table below, not all functionality is available in C yet. Providing\nmore functionality in the [C API] is an ongoing project.\nFeature                                        | Python                                                      | C\n:--------------------------------------------- | :---------------------------------------------------------- | :--\nRun a predefined Graph                         | `tf.import_graph_def`, `tf.Session`                         | `TF_GraphImportGraphDef`, `TF_NewSession`\nGraph construction with generated op functions | Yes                                                         | Yes (The C API supports client languages that do this)\nGradients                                      | `tf.gradients`                                              |\nFunctions                                      | `tf.python.framework.function.Defun`                        |\nControl Flow                                   | `tf.cond`, `tf.while_loop`                                  |\nNeural Network library                         | `tf.train`, `tf.nn`, `tf.contrib.layers`, `tf.contrib.slim` |\nRecommended Approach\nRun a predefined graph\nA language binding is expected to define the following classes:\n\n`Graph`: A graph representing a TensorFlow computation. Consists of\n    operations (represented in the client language by `Operation`s) and\n    corresponds to a `TF_Graph` in the C API. Mainly used as an argument when\n    creating new `Operation` objects and when starting a `Session`. Also\n    supports iterating through the operations in the graph\n    (`TF_GraphNextOperation`), looking up operations by name\n    (`TF_GraphOperationByName`), and converting to and from a `GraphDef`\n    protocol message (`TF_GraphToGraphDef` and `TF_GraphImportGraphDef` in the C\n    API).\n`Operation`: Represents a computation node in the graph. Corresponds to a\n    `TF_Operation` in the C API.\n`Output`: Represents one of the outputs of an operation in the graph. Has a\n    `DataType` (and eventually a shape). May be passed as an input argument to a\n    function for adding operations to a graph, or to a `Session`'s `Run()`\n    method to fetch that output as a tensor. Corresponds to a `TF_Output` in the\n    C API.\n`Session`: Represents a client to a particular instance of the TensorFlow\n    runtime. Its main job is to be constructed with a `Graph` and some options\n    and then field calls to `Run()` the graph. Corresponds to a `TF_Session` in\n    the C API.\n`Tensor`: Represents an N-dimensional (rectangular) array with elements all\n    the same `DataType`. Gets data into and out of a `Session`'s `Run()` call.\n    Corresponds to a `TF_Tensor` in the C API.\n`DataType`: An enumerant with all the possible tensor types supported by\n    TensorFlow. Corresponds to `TF_DataType` in the C API and often referred to\n    as `dtype` in the Python API.\n\nGraph construction\nTensorFlow has many ops, and the list is not static, so we recommend generating\nthe functions for adding ops to a graph instead of writing them by individually\nby hand (though writing a few by hand is a good way to figure out what the\ngenerator should generate). The information needed to generate a function is\ncontained in an `OpDef` protocol message.\nThere are a few ways to get a list of the `OpDef`s for the registered ops:\n\n`TF_GetAllOpList` in the C API retrieves all registered `OpDef` protocol\n    messages. This can be used to write the generator in the client language.\n    This requires that the client language have protocol buffer support in order\n    to interpret the `OpDef` messages.\nThe C++ function `OpRegistry::Global()->GetRegisteredOps()` returns the same\n    list of all registered `OpDef`s (defined in\n    tensorflow/core/framework/op.h). This can be used to write the generator\n    in C++ (particularly useful for languages that do not have protocol buffer\n    support).\nThe ASCII-serialized version of that list is periodically checked in to\n    tensorflow/core/ops/ops.pbtxt by an automated process.\n\nThe `OpDef` specifies the following:\n\nName of the op in CamelCase. For generated functions follow the conventions\n    of the language. For example, if the language uses snake_case, use that\n    instead of CamelCase for the op's function name.\nA list of inputs and outputs. The types for these may be polymorphic by\n    referencing attributes, as described in the inputs and outputs section of\n    Adding an     op.\nA list of attributes, along with their default values (if any). Note that\n    some of these will be inferred (if they are determined by an input), some\n    will be optional (if they have a default), and some will be required (no\n    default).\nDocumentation for the op in general and the inputs, outputs, and\n    non-inferred attributes.\nSome other fields that are used by the runtime and can be ignored by the\n    code generators.\n\nAn `OpDef` can be converted into the text of a function that adds that op to the\ngraph using the `TF_OperationDescription` C API (wrapped in the language's FFI):\n\nStart with `TF_NewOperation()` to create the `TF_OperationDescription*`.\nCall `TF_AddInput()` or `TF_AddInputList()` once per input (depending on\n    whether the input has a list type).\nCall `TF_SetAttr*()` functions to set non-inferred attributes. May skip\n    attributes with defaults if you don't want to override the default value.\nSet optional fields if necessary:\n`TF_SetDevice()`: force the operation onto a specific device.\n`TF_AddControlInput()`: add requirements that another operation finish\n    before this operation starts running\n`TF_SetAttrString(\"_kernel\")` to set the kernel label (rarely used)\n`TF_ColocateWith()` to colocate one op with another\n\n\nCall `TF_FinishOperation()` when done. This adds the operation to the graph,\n    after which it can't be modified.\n\nThe existing examples run the code generator as part of the build process (using\na Bazel genrule). Alternatively, the code generator can be run by an automated\ncron process, possibly checking in the result. This creates a risk of divergence\nbetween the generated code and the `OpDef`s checked into the repository, but is\nuseful for languages where code is expected to be generated ahead of time like\n`go get` for Go and `cargo ops` for Rust. At the other end of the spectrum, for\nsome languages the code could be generated dynamically from\ntensorflow/core/ops/ops.pbtxt.\nHandling Constants\nCalling code will be much more concise if users can provide constants to input\narguments. The generated code should convert those constants to operations that\nare added to the graph and used as input to the op being instantiated.\nOptional parameters\nIf the language allows for optional parameters to a function (like keyword\narguments with defaults in Python), use them for optional attributes, operation\nnames, devices, control inputs etc. In some languages, these optional parameters\ncan be set using dynamic scopes (like \"with\" blocks in Python). Without these\nfeatures, the library may resort to the \"builder pattern\", as is done in the C++\nversion of the TensorFlow API.\nName scopes\nIt is a good idea to have support for naming graph operations using some sort of\nscoping hierarchy, especially considering the fact that TensorBoard relies on it\nto display large graphs in a reasonable way. The existing Python and C++ APIs\ntake different approaches: In Python, the \"directory\" part of the name\n(everything up to the last \"/\") comes from `with` blocks. In effect, there is a\nthread-local stack with the scopes defining the name hierarchy. The last\ncomponent of the name is either supplied explicitly by the user (using the\noptional `name` keyword argument) or defaults to the name of the type of the op\nbeing added. In C++ the \"directory\" part of the name is stored in an explicit\n`Scope` object. The `NewSubScope()` method appends to that part of the name and\nreturns a new `Scope`. The last component of the name is set using the\n`WithOpName()` method, and like Python defaults to the name of the type of op\nbeing added. `Scope` objects are explicitly passed around to specify the name of\nthe context.\nWrappers\nIt may make sense to keep the generated functions private for some ops so that\nwrapper functions that do a little bit of additional work can be used instead.\nThis also gives an escape hatch for supporting features outside the scope of\ngenerated code.\nOne use of a wrapper is for supporting `SparseTensor` input and output. A\n`SparseTensor` is a tuple of 3 dense tensors: indices, values, and shape. values\nis a vector size [n], shape is a vector size [rank], and indices is a matrix\nsize [n, rank]. There are some sparse ops that use this triple to represent a\nsingle sparse tensor.\nAnother reason to use wrappers is for ops that hold state. There are a few such\nops (e.g. a variable) that have several companion ops for operating on that\nstate. The Python API has classes for these ops where the constructor creates\nthe op, and methods on that class add operations to the graph that operate on\nthe state.\nOther Considerations\n\nIt is good to have a list of keywords used to rename op functions and\n    arguments that collide with language keywords (or other symbols that will\n    cause trouble, like the names of library functions or variables referenced\n    in the generated code).\nThe function for adding a `Const` operation to a graph typically is a\n    wrapper since the generated function will typically have redundant\n    `DataType` inputs.\n\nGradients, functions and control flow\nAt this time, support for gradients, functions and control flow operations (\"if\"\nand \"while\") is not available in languages other than Python. This will be\nupdated when the [C API] provides necessary support.",
    "tag": "tensorflow"
  },
  {
    "title": "TensorFlow Architecture",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/guide/extend/architecture.md",
    "content": "TensorFlow Architecture\nWe designed TensorFlow for large-scale distributed training and inference, but\nit is also flexible enough to support experimentation with new machine\nlearning models and system-level optimizations.\nThis document describes the system architecture that makes this\ncombination of scale and flexibility possible. It assumes that you have basic familiarity\nwith TensorFlow programming concepts such as the computation graph, operations,\nand sessions. See this document for an introduction to\nthese topics. Some familiarity with distributed TensorFlow\nwill also be helpful.\nThis document is for developers who want to extend TensorFlow in some way not\nsupported by current APIs, hardware engineers who want to optimize for\nTensorFlow, implementers of machine learning systems working on scaling and\ndistribution, or anyone who wants to look under Tensorflow's hood. By the end of this document \nyou should understand the TensorFlow architecture well enough to read\nand modify the core TensorFlow code.\nOverview\nThe TensorFlow runtime is a cross-platform library. Figure 1 illustrates its\ngeneral architecture. A C API separates user level code in different languages\nfrom the core runtime.\n\nFigure 1\nThis document focuses on the following layers:\n\nClient:\nDefines the computation as a dataflow graph.\nInitiates graph execution using a session.\nDistributed Master\nPrunes a specific subgraph from the graph, as defined by the arguments\n      to Session.run().\nPartitions the subgraph into multiple pieces that run in different\n      processes and devices.\nDistributes the graph pieces to worker services.\nInitiates graph piece execution by worker services.\nWorker Services (one for each task)\nSchedule the execution of graph operations using kernel implementations\n      appropriate to the available hardware (CPUs, GPUs, etc).\nSend and receive operation results to and from other worker services.\nKernel Implementations\nPerform the computation for individual graph operations.\n\nFigure 2 illustrates the interaction of these components. \"/job:worker/task:0\" and\n\"/job:ps/task:0\" are both tasks with worker services. \"PS\" stands for \"parameter\nserver\": a task responsible for storing and updating the model's parameters.\nOther tasks send updates to these parameters as they work on optimizing the\nparameters. This particular division of labor between tasks is not required, but\n is common for distributed training.\n\nFigure 2\nNote that the Distributed Master and Worker Service only exist in\ndistributed TensorFlow. The single-process version of TensorFlow includes a\nspecial Session implementation that does everything the distributed master does\nbut only communicates with devices in the local process.\nThe following sections describe the core TensorFlow layers in greater detail and\nstep through the processing of an example graph.\nClient\nUsers write the client TensorFlow program that builds the computation graph.\nThis program can either directly compose individual operations or use a\nconvenience library like the Estimators API to compose neural network layers and\nother higher-level abstractions. TensorFlow supports multiple client\nlanguages, and we have prioritized Python and C++, because our internal users\nare most familiar with these languages. As features become more established,\nwe typically port them to C++, so that users can access an optimized\nimplementation from all client languages. Most of the training libraries are\nstill Python-only, but C++ does have support for efficient inference.\nThe client creates a session, which sends the graph definition to the\ndistributed master as a `tf.GraphDef`\nprotocol buffer. When the client evaluates a node or nodes in the\ngraph, the evaluation triggers a call to the distributed master to initiate\ncomputation.\nIn Figure 3, the client has built a graph that applies weights (w) to a\nfeature vector (x), adds a bias term (b) and saves the result in a variable\n(s).\n\nFigure 3\nCode\n\n`tf.Session`\n\nDistributed master\nThe distributed master:\n\nprunes the graph to obtain the subgraph required to evaluate the nodes\n   requested by the client,\npartitions the graph to obtain graph pieces for\n   each participating device, and\ncaches these pieces so that they may be re-used in subsequent steps.\n\nSince the master sees the overall computation for\na step, it applies standard optimizations such as common subexpression\nelimination and constant folding. It then coordinates execution of the\noptimized subgraphs across a set of tasks.\n\nFigure 4\nFigure 5 shows a possible partition of our example graph. The distributed\nmaster has grouped the model parameters in order to place them together on the\nparameter server.\n\nFigure 5\nWhere graph edges are cut by the partition, the distributed master inserts\nsend and receive nodes to pass information between the distributed tasks\n(Figure 6).\n\nFigure 6\nThe distributed master then ships the graph pieces to the distributed tasks.\n\nFigure 7\nCode\n\nMasterService API definition\nMaster interface\n\nWorker Service\nThe worker service in each task:\n\nhandles requests from the master,\nschedules the execution of the kernels for the operations that comprise a\n   local subgraph, and\nmediates direct communication between tasks.\n\nWe optimize the worker service for running large graphs with low overhead. Our\ncurrent implementation can execute tens of thousands of subgraphs per second,\nwhich enables a large number of replicas to make rapid, fine-grained training\nsteps. The worker service dispatches kernels to local devices and runs kernels\nin parallel when possible, for example by using multiple CPU cores or GPU\nstreams.\nWe specialize Send and Recv operations for each pair of source and destination\ndevice types:\n\nTransfers between local CPU and GPU devices use the\n   `cudaMemcpyAsync()` API to overlap computation and data transfer.\nTransfers between two local GPUs use peer-to-peer DMA, to avoid an expensive\n   copy via the host CPU.\n\nFor transfers between tasks, TensorFlow uses multiple protocols, including:\n\ngRPC over TCP.\nRDMA over Converged Ethernet.\n\nWe also have preliminary support for NVIDIA's NCCL library for multi-GPU\ncommunication, see:\ntf.contrib.nccl.\n\nFigure 8\nCode\n\nWorkerService API definition\nWorker interface\nRemote rendezvous (for Send and Recv implementations)\n\nKernel Implementations\nThe runtime contains over 200 standard operations including mathematical, array\nmanipulation, control flow, and state management operations. Each of these\noperations can have kernel implementations optimized for a variety of devices.\nMany of the operation kernels are implemented using Eigen::Tensor, which uses\nC++ templates to generate efficient parallel code for multicore CPUs and GPUs;\nhowever, we liberally use libraries like cuDNN where a more efficient kernel\nimplementation is possible. We have also implemented\nquantization, which enables\nfaster inference in environments such as mobile devices and high-throughput\ndatacenter applications, and use the\ngemmlowp low-precision matrix library to\naccelerate quantized computation.\nIf it is difficult or inefficient to represent a subcomputation as a composition\nof operations, users can register additional kernels that provide an efficient\nimplementation written in C++. For example, we recommend registering your own\nfused kernels for some performance critical operations, such as the ReLU and\nSigmoid activation functions and their corresponding gradients. The XLA Compiler has an\nexperimental implementation of automatic kernel fusion.\nCode",
    "tag": "tensorflow"
  },
  {
    "title": "A Tool Developer's Guide to TensorFlow Model Files",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/guide/extend/model_files.md",
    "content": "A Tool Developer's Guide to TensorFlow Model Files\nMost users shouldn't need to care about the internal details of how TensorFlow\nstores data on disk, but you might if you're a tool developer. For example, you\nmay want to analyze models, or convert back and forth between TensorFlow and\nother formats. This guide tries to explain some of the details of how you can\nwork with the main files that hold model data, to make it easier to develop\nthose kind of tools.\n[TOC]\nProtocol Buffers\nAll of TensorFlow's file formats are based on\nProtocol Buffers, so to\nstart it's worth getting familiar with how they work. The summary is that you\ndefine data structures in text files, and the protobuf tools generate classes in\nC, Python, and other languages that can load, save, and access the data in a\nfriendly way. We often refer to Protocol Buffers as protobufs, and I'll use\nthat convention in this guide.\nGraphDef\nThe foundation of computation in TensorFlow is the `Graph` object. This holds a\nnetwork of nodes, each representing one operation, connected to each other as\ninputs and outputs. After you've created a `Graph` object, you can save it out\nby calling `as_graph_def()`, which returns a `GraphDef` object.\nThe GraphDef class is an object created by the ProtoBuf library from the\ndefinition in\ntensorflow/core/framework/graph.proto. The protobuf tools parse\nthis text file, and generate the code to load, store, and manipulate graph\ndefinitions. If you see a standalone TensorFlow file representing a model, it's\nlikely to contain a serialized version of one of these `GraphDef` objects\nsaved out by the protobuf code.\nThis generated code is used to save and load the GraphDef files from disk. The code that actually loads the model looks like this:\n`python\ngraph_def = graph_pb2.GraphDef()`\nThis line creates an empty `GraphDef` object, the class that's been created\nfrom the textual definition in graph.proto. This is the object we're going to\npopulate with the data from our file.\n`python\nwith open(FLAGS.graph, \"rb\") as f:`\nHere we get a file handle for the path we've passed in to the script\n`python\n  if FLAGS.input_binary:\n    graph_def.ParseFromString(f.read())\n  else:\n    text_format.Merge(f.read(), graph_def)`\nText or Binary?\nThere are actually two different formats that a ProtoBuf can be saved in.\nTextFormat is a human-readable form, which makes it nice for debugging and\nediting, but can get large when there's numerical data like weights stored in\nit. You can see a small example of that in\ngraph_run_run2.pbtxt.\nBinary format files are a lot smaller than their text equivalents, even though\nthey're not as readable for us. In this script, we ask the user to supply a\nflag indicating whether the input file is binary or text, so we know the right\nfunction to call. You can find an example of a large binary file inside the\ninception_v3 archive,\nas `inception_v3_2016_08_28_frozen.pb`.\nThe API itself can be a bit confusing - the binary call is actually\n`ParseFromString()`, whereas you use a utility function from the `text_format`\nmodule to load textual files.\nNodes\nOnce you've loaded a file into the `graph_def` variable, you can now access the\ndata inside it. For most practical purposes, the important section is the list\nof nodes stored in the node member. Here's the code that loops through those:\n`python\nfor node in graph_def.node`\nEach node is a `NodeDef` object, defined in\ntensorflow/core/framework/node_def.proto. These\nare the fundamental building blocks of TensorFlow graphs, with each one defining\na single operation along with its input connections. Here are the members of a\n`NodeDef`, and what they mean.\n`name`\nEvery node should have a unique identifier that's not used by any other nodes\nin the graph. If you don't specify one as you're building a graph using the\nPython API, one reflecting the name of operation, such as \"MatMul\",\nconcatenated with a monotonically increasing number, such as \"5\", will be\npicked for you. The name is used when defining the connections between nodes,\nand when setting inputs and outputs for the whole graph when it's run.\n`op`\nThis defines what operation to run, for example `\"Add\"`, `\"MatMul\"`, or\n`\"Conv2D\"`. When a graph is run, this op name is looked up in a registry to\nfind an implementation. The registry is populated by calls to the\n`REGISTER_OP()` macro, like those in\ntensorflow/core/ops/nn_ops.cc.\n`input`\nA list of strings, each one of which is the name of another node, optionally\nfollowed by a colon and an output port number. For example, a node with two\ninputs might have a list like `[\"some_node_name\", \"another_node_name\"]`, which\nis equivalent to `[\"some_node_name:0\", \"another_node_name:0\"]`, and defines the\nnode's first input as the first output from the node with the name\n`\"some_node_name\"`, and a second input from the first output of\n`\"another_node_name\"`\n`device`\nIn most cases you can ignore this, since it defines where to run a node in a\ndistributed environment, or when you want to force the operation onto CPU or\nGPU.\n`attr`\nThis is a key/value store holding all the attributes of a node. These are the\npermanent properties of nodes, things that don't change at runtime such as the\nsize of filters for convolutions, or the values of constant ops. Because there\ncan be so many different types of attribute values, from strings, to ints, to\narrays of tensor values, there's a separate protobuf file defining the data\nstructure that holds them, in\ntensorflow/core/framework/attr_value.proto.\nEach attribute has a unique name string, and the expected attributes are listed\nwhen the operation is defined. If an attribute isn't present in a node, but it\nhas a default listed in the operation definition, that default is used when the\ngraph is created.\nYou can access all of these members by calling `node.name`, `node.op`, etc. in\nPython. The list of nodes stored in the `GraphDef` is a full definition of the\nmodel architecture.\nFreezing\nOne confusing part about this is that the weights usually aren't stored inside\nthe file format during training. Instead, they're held in separate checkpoint\nfiles, and there are `Variable` ops in the graph that load the latest values\nwhen they're initialized. It's often not very convenient to have separate files\nwhen you're deploying to production, so there's the\nfreeze_graph.py script that takes a graph definition and a set\nof checkpoints and freezes them together into a single file.\nWhat this does is load the `GraphDef`, pull in the values for all the variables\nfrom the latest checkpoint file, and then replace each `Variable` op with a\n`Const` that has the numerical data for the weights stored in its attributes.\nIt then strips away all the extraneous nodes that aren't used for forward\ninference, and saves out the resulting `GraphDef` into an output file.\nWeight Formats\nIf you're dealing with TensorFlow models that represent neural networks, one of\nthe most common problems is extracting and interpreting the weight values. A\ncommon way to store them, for example in graphs created by the freeze_graph\nscript, is as `Const` ops containing the weights as `Tensors`. These are\ndefined in\ntensorflow/core/framework/tensor.proto, and contain information\nabout the size and type of the data, as well as the values themselves. In\nPython, you get a `TensorProto` object from a `NodeDef` representing a `Const`\nop by calling something like `some_node_def.attr['value'].tensor`.\nThis will give you an object representing the weights data. The data itself\nwill be stored in one of the lists with the suffix _val as indicated by the\ntype of the object, for example `float_val` for 32-bit float data types.\nThe ordering of convolution weight values is often tricky to deal with when\nconverting between different frameworks. In TensorFlow, the filter weights for\nthe `Conv2D` operation are stored on the second input, and are expected to be\nin the order `[filter_height, filter_width, input_depth, output_depth]`, where\nfilter_count increasing by one means moving to an adjacent value in memory.\nHopefully this rundown gives you a better idea of what's going on inside",
    "tag": "tensorflow"
  },
  {
    "title": "Adding a New Op",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/guide/extend/op.md",
    "content": "Adding a New Op\nNote: To guarantee that your C++ custom ops are ABI compatible with TensorFlow's\nofficial pip packages, please follow the guide at\nCustom op repository. It has an\nend-to-end code example, as well as Docker images for building and distributing\nyour custom ops.\nIf you'd like to create an op that isn't covered by the existing TensorFlow\nlibrary, we recommend that you first try writing the op in Python as\na composition of existing Python ops or functions. If that isn't possible, you\ncan create a custom C++ op. There are several reasons why you might want to\ncreate a custom C++ op:\n\nIt's not easy or possible to express your operation as a composition of\n    existing ops.\nIt's not efficient to express your operation as a composition of existing\n    primitives.\nYou want to hand-fuse a composition of primitives that a future compiler\n    would find difficult fusing.\n\nFor example, imagine you want to implement something like \"median pooling\",\nsimilar to the \"MaxPool\" operator, but computing medians over sliding windows\ninstead of maximum values.  Doing this using a composition of operations may be\npossible (e.g., using ExtractImagePatches and TopK), but may not be as\nperformance- or memory-efficient as a native operation where you can do\nsomething more clever in a single, fused operation. As always, it is typically\nfirst worth trying to express what you want using operator composition, only\nchoosing to add a new operation if that proves to be difficult or inefficient.\nTo incorporate your custom op you'll need to:\n\nRegister the new op in a C++ file. Op registration defines an interface\n    (specification) for the op's functionality, which is independent of the\n    op's implementation. For example, op registration defines the op's name and\n    the op's inputs and outputs. It also defines the shape function\n    that is used for tensor shape inference.\nImplement the op in C++. The implementation of an op is known\n    as a kernel, and it is the concrete implementation of the specification you\n    registered in Step 1. There can be multiple kernels for different input /\n    output types or architectures (for example, CPUs, GPUs).\nCreate a Python wrapper (optional). This wrapper is the public API that's\n    used to create the op in Python. A default wrapper is generated from the\n    op registration, which can be used directly or added to.\nWrite a function to compute gradients for the op (optional).\nTest the op. We usually do this in Python for convenience, but you can also\n    test the op in C++. If you define gradients, you can verify them with the\n    Python `tf.test.compute_gradient_error`.\n    See\n    relu_op_test.py as\n    an example that tests the forward functions of Relu-like operators and\n    their gradients.\n\nPREREQUISITES:\n\nSome familiarity with C++.\nMust have installed the\n    TensorFlow binary, or must have\n    downloaded TensorFlow source,\n    and be able to build it.\n\n[TOC]\nDefine the op's interface\nYou define the interface of an op by registering it with the TensorFlow system.\nIn the registration, you specify the name of your op, its inputs (types and\nnames) and outputs (types and names), as well as docstrings and\nany attrs the op might require.\nTo see how this works, suppose you'd like to create an op that takes a tensor of\n`int32`s and outputs a copy of the tensor, with all but the first element set to\nzero. To do this, create a file named `zero_out.cc`. Then add a call to the\n`REGISTER_OP` macro that defines the interface for your op:\n```c++\ninclude \"tensorflow/core/framework/op.h\"\ninclude \"tensorflow/core/framework/shape_inference.h\"\nusing namespace tensorflow;\nREGISTER_OP(\"ZeroOut\")\n    .Input(\"to_zero: int32\")\n    .Output(\"zeroed: int32\")\n    .SetShapeFn( {\n      c->set_output(0, c->input(0));\n      return Status::OK();\n    });\n```\nThis `ZeroOut` op takes one tensor `to_zero` of 32-bit integers as input, and\noutputs a tensor `zeroed` of 32-bit integers. The op also uses a shape function\nto ensure that the output tensor is the same shape as the input tensor. For\nexample, if the input is a tensor of shape [10, 20], then this shape function\nspecifies that the output shape is also [10, 20].\n\nA note on naming: The op name must be in CamelCase and it must be unique\n  among all other ops that are registered in the binary.\n\nImplement the kernel for the op\nAfter you define the interface, provide one or more implementations of the op.\nTo create one of these kernels, create a class that extends `OpKernel` and\noverrides the `Compute` method. The `Compute` method provides one `context`\nargument of type `OpKernelContext*`, from which you can access useful things\nlike the input and output tensors.\nAdd your kernel to the file you created above. The kernel might look something\nlike this:\n```c++\ninclude \"tensorflow/core/framework/op_kernel.h\"\nusing namespace tensorflow;\nclass ZeroOutOp : public OpKernel {\n public:\n  explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}\nvoid Compute(OpKernelContext* context) override {\n    // Grab the input tensor\n    const Tensor& input_tensor = context->input(0);\n    auto input = input_tensor.flat();\n\n\n```// Create an output tensor\nTensor* output_tensor = NULL;\nOP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),\n                                                 &output_tensor));\nauto output_flat = output_tensor->flat<int32>();\n\n// Set all but the first element of the output tensor to 0.\nconst int N = input.size();\nfor (int i = 1; i < N; i++) {\n  output_flat(i) = 0;\n}\n\n// Preserve the first input value if possible.\nif (N > 0) output_flat(0) = input(0);\n```\n\n\n}\n};\n```\nAfter implementing your kernel, you register it with the TensorFlow system. In\nthe registration, you specify different constraints under which this kernel\nwill run. For example, you might have one kernel made for CPUs, and a separate\none for GPUs.\nTo do this for the `ZeroOut` op, add the following to `zero_out.cc`:\n`c++\nREGISTER_KERNEL_BUILDER(Name(\"ZeroOut\").Device(DEVICE_CPU), ZeroOutOp);`\n\nImportant: Instances of your OpKernel may be accessed concurrently.\n  Your `Compute` method must be thread-safe. Guard any access to class\n  members with a mutex. Or better yet, don't share state via class members!\n  Consider using a ResourceMgr\n  to keep track of op state.\n\nMulti-threaded CPU kernels\nTo write a multi-threaded CPU kernel, the Shard function in\nwork_sharder.h\ncan be used. This function shards a computation function across the\nthreads configured to be used for intra-op threading (see\nintra_op_parallelism_threads in\nconfig.proto).\nGPU kernels\nA GPU kernel is implemented in two parts: the OpKernel and the CUDA kernel and\nits launch code.\nSometimes the OpKernel implementation is common between a CPU and GPU kernel,\nsuch as around inspecting inputs and allocating outputs.  In that case, a\nsuggested implementation is to:\n\nDefine the OpKernel templated on the Device and the primitive type of the\n   tensor.\nTo do the actual computation of the output, the Compute function calls a\n    templated functor struct.\nThe specialization of that functor for the CPUDevice is defined in the same\n   file, but the specialization for the GPUDevice is defined in a .cu.cc file,\n   since it will be compiled with the CUDA compiler.\n\nHere is an example implementation.\n```c++\n// kernel_example.h\nifndef KERNEL_EXAMPLE_H_\ndefine KERNEL_EXAMPLE_H_\ntemplate \nstruct ExampleFunctor {\n  void operator()(const Device& d, int size, const T in, T out);\n};\nif GOOGLE_CUDA\n// Partially specialize functor for GpuDevice.\ntemplate \nstruct ExampleFunctor {\n  void operator()(const Eigen::GpuDevice& d, int size, const T in, T out);\n};\nendif\nendif KERNEL_EXAMPLE_H_\n```\n```c++\n// kernel_example.cc\ninclude \"kernel_example.h\"\ninclude \"tensorflow/core/framework/op_kernel.h\"\nusing namespace tensorflow;\nusing CPUDevice = Eigen::ThreadPoolDevice;\nusing GPUDevice = Eigen::GpuDevice;\n// CPU specialization of actual computation.\ntemplate \nstruct ExampleFunctor {\n  void operator()(const CPUDevice& d, int size, const T in, T out) {\n    for (int i = 0; i < size; ++i) {\n      out[i] = 2 * in[i];\n    }\n  }\n};\n// OpKernel definition.\n// template parameter  is the datatype of the tensors.\ntemplate \nclass ExampleOp : public OpKernel {\n public:\n  explicit ExampleOp(OpKernelConstruction* context) : OpKernel(context) {}\nvoid Compute(OpKernelContext* context) override {\n    // Grab the input tensor\n    const Tensor& input_tensor = context->input(0);\n\n\n```// Create an output tensor\nTensor* output_tensor = NULL;\nOP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),\n                                                 &output_tensor));\n\n// Do the computation.\nOP_REQUIRES(context, input_tensor.NumElements() <= tensorflow::kint32max,\n            errors::InvalidArgument(\"Too many elements in tensor\"));\nExampleFunctor<Device, T>()(\n    context->eigen_device<Device>(),\n    static_cast<int>(input_tensor.NumElements()),\n    input_tensor.flat<T>().data(),\n    output_tensor->flat<T>().data());\n```\n\n\n}\n};\n// Register the CPU kernels.\n#define REGISTER_CPU(T)                                          \\\n  REGISTER_KERNEL_BUILDER(                                       \\\n      Name(\"Example\").Device(DEVICE_CPU).TypeConstraint(\"T\"), \\\n      ExampleOp);\nREGISTER_CPU(float);\nREGISTER_CPU(int32);\n// Register the GPU kernels.\nifdef GOOGLE_CUDA\n#define REGISTER_GPU(T)                                          \\\n  / Declare explicit instantiations in kernel_example.cu.cc. / \\\n  extern template ExampleFunctor;                  \\\n  REGISTER_KERNEL_BUILDER(                                       \\\n      Name(\"Example\").Device(DEVICE_GPU).TypeConstraint(\"T\"), \\\n      ExampleOp);\nREGISTER_GPU(float);\nREGISTER_GPU(int32);\nendif  // GOOGLE_CUDA\n```\n```c++\n// kernel_example.cu.cc\nifdef GOOGLE_CUDA\ndefine EIGEN_USE_GPU\ninclude \"example.h\"\ninclude \"tensorflow/core/util/gpu_kernel_helper.h\"\nusing namespace tensorflow;\nusing GPUDevice = Eigen::GpuDevice;\n// Define the CUDA kernel.\ntemplate \nglobal void ExampleCudaKernel(const int size, const T in, T out) {\n  for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < size;\n       i += blockDim.x * gridDim.x) {\n    out[i] = 2 * ldg(in + i);\n  }\n}\n// Define the GPU implementation that launches the CUDA kernel.\ntemplate \nvoid ExampleFunctor::operator()(\n    const GPUDevice& d, int size, const T in, T out) {\n  // Launch the cuda kernel.\n  //\n  // See core/util/gpu_kernel_helper.h for example of computing\n  // block count and thread_per_block count.\n  int block_count = 1024;\n  int thread_per_block = 20;\n  ExampleCudaKernel\n      <<>>(size, in, out);\n}\n// Explicitly instantiate functors for the types of OpKernels registered.\ntemplate struct ExampleFunctor;\ntemplate struct ExampleFunctor;\nendif  // GOOGLE_CUDA\n```\nBuild the op library\nCompile the op using your system compiler (TensorFlow binary installation)\nYou should be able to compile `zero_out.cc` with a `C++` compiler such as `g++`\nor `clang` available on your system. The binary PIP package installs the header\nfiles and the library that you need to compile your op in locations that are\nsystem specific. However, the TensorFlow python library provides the\n`get_include` function to get the header directory, and the `get_lib` directory\nhas a shared object to link against.\nHere are the outputs of these functions on an Ubuntu machine.\n```bash\n$ python\n\n\n\nimport tensorflow as tf\ntf.sysconfig.get_include()\n'/usr/local/lib/python3.6/site-packages/tensorflow/include'\ntf.sysconfig.get_lib()\n'/usr/local/lib/python3.6/site-packages/tensorflow'\n```\n\n\n\nAssuming you have `g++` installed, here is the sequence of commands you can use\nto compile your op into a dynamic library.\n`bash\nTF_CFLAGS=( $(python -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_compile_flags()))') )\nTF_LFLAGS=( $(python -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_link_flags()))') )\ng++ -std=c++11 -shared zero_out.cc -o zero_out.so -fPIC ${TF_CFLAGS[@]} ${TF_LFLAGS[@]} -O2`\nOn macOS, the additional flag \"-undefined dynamic_lookup\" is required when\nbuilding the `.so` file.\n\nNote on `gcc` version `>=5`: gcc uses the new C++\n  ABI since version `5`. The binary pip\n  packages available on the TensorFlow website are built with `gcc4` that uses\n  the older ABI. If you compile your op library with `gcc>=5`, add\n  `-D_GLIBCXX_USE_CXX11_ABI=0` to the command line to make the library\n  compatible with the older abi.\n\nCompile the op using bazel (TensorFlow source installation)\nIf you have TensorFlow sources installed, you can make use of TensorFlow's build\nsystem to compile your op. Place a BUILD file with following Bazel build rule in\nthe tensorflow/core/user_ops directory.\n```python\nload(\"//tensorflow:tensorflow.bzl\", \"tf_custom_op_library\")\ntf_custom_op_library(\n    name = \"zero_out.so\",\n    srcs = [\"zero_out.cc\"],\n)\n```\nRun the following command to build `zero_out.so`.\n`bash\n$ bazel build --config opt //tensorflow/core/user_ops:zero_out.so`\n\nAs explained above, if you are compiling with gcc>=5 add `--cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"`\n  to the bazel command line.\nNote: Although you can create a shared library (a `.so` file) with the\n  standard `cc_library` rule, we strongly recommend that you use the\n  `tf_custom_op_library` macro. It adds some required dependencies, and\n  performs checks to ensure that the shared library is compatible with\n  TensorFlow's plugin loading mechanism.\n\nUse the op in Python\nTensorFlow Python API provides the\n`tf.load_op_library` function to\nload the dynamic library and register the op with the TensorFlow\nframework. `load_op_library` returns a Python module that contains the Python\nwrappers for the op and the kernel. Thus, once you have built the op, you can\ndo the following to run it from Python:\n```python\nimport tensorflow as tf\nzero_out_module = tf.load_op_library('./zero_out.so')\nwith tf.Session(''):\n  zero_out_module.zero_out([[1, 2], [3, 4]]).eval()\nPrints\narray([[1, 0], [0, 0]], dtype=int32)\n```\nKeep in mind, the generated function will be given a snake_case name (to comply\nwith PEP8). So, if your op is\nnamed `ZeroOut` in the C++ files, the python function will be called `zero_out`.\nTo make the op available as a regular function `import`-able from a Python\nmodule, it maybe useful to have the `load_op_library` call in a Python source\nfile as follows:\n```python\nimport tensorflow as tf\nzero_out_module = tf.load_op_library('./zero_out.so')\nzero_out = zero_out_module.zero_out\n```\nVerify that the op works\nA good way to verify that you've successfully implemented your op is to write a\ntest for it. Create the file\n`zero_out_op_test.py` with the contents:\n```python\nimport tensorflow as tf\nclass ZeroOutTest(tf.test.TestCase):\n  def testZeroOut(self):\n    zero_out_module = tf.load_op_library('./zero_out.so')\n    with self.test_session():\n      result = zero_out_module.zero_out([5, 4, 3, 2, 1])\n      self.assertAllEqual(result.eval(), [5, 0, 0, 0, 0])\nif name == \"main\":\n  tf.test.main()\n```\nThen run your test (assuming you have tensorflow installed):\n`sh\n$ python zero_out_op_test.py`\nBuilding advanced features into your op\nNow that you know how to build a basic (and somewhat restricted) op and\nimplementation, we'll look at some of the more complicated things you will\ntypically need to build into your op. This includes:\n\nConditional checks and validation\nOp registration\nAttrs\nAttr types\nPolymorphism\nInputs and outputs\nBackwards compatibility\n\n\nGPU support\nCompiling the kernel for the GPU device\n\n\nImplement the gradient in Python\nShape functions in C++\n\nConditional checks and validation\nThe example above assumed that the op applied to a tensor of any shape.  What\nif it only applied to vectors?  That means adding a check to the above OpKernel\nimplementation.\n```c++\n  void Compute(OpKernelContext* context) override {\n    // Grab the input tensor\n    const Tensor& input_tensor = context->input(0);\n\n\n```OP_REQUIRES(context, TensorShapeUtils::IsVector(input_tensor.shape()),\n            errors::InvalidArgument(\"ZeroOut expects a 1-D vector.\"));\n// ...\n```\n\n\n}\n```\nThis asserts that the input is a vector, and returns having set the\n`InvalidArgument` status if it isn't.  The\nOP_REQUIRES macro takes three arguments:\n\nThe `context`, which can either be an `OpKernelContext` or\n    `OpKernelConstruction` pointer (see\n    tensorflow/core/framework/op_kernel.h),\n    for its `SetStatus()` method.\nThe condition.  For example, there are functions for validating the shape\n    of a tensor in\n    tensorflow/core/framework/tensor_shape.h\nThe error itself, which is represented by a `Status` object, see\n    tensorflow/core/lib/core/status.h. A\n    `Status` has both a type (frequently `InvalidArgument`, but see the list of\n    types) and a message.  Functions for constructing an error may be found in\n    tensorflow/core/lib/core/errors.h.\n\nAlternatively, if you want to test whether a `Status` object returned from some\nfunction is an error, and if so return it, use\nOP_REQUIRES_OK.  Both of these macros return from the\nfunction on error.\nOp registration\nAttrs\nOps can have attrs, whose values are set when the op is added to a graph. These\nare used to configure the op, and their values can be accessed both within the\nkernel implementation and in the types of inputs and outputs in the op\nregistration. Prefer using an input instead of an attr when possible, since\ninputs are more flexible. This is because attrs are constants and must be\ndefined at graph construction time. In contrast, inputs are Tensors whose\nvalues can be dynamic; that is, inputs can change every step, be set using a\nfeed, etc. Attrs are used for things that can't be done with inputs: any\nconfiguration that affects the signature (number or type of inputs or outputs)\nor that can't change from step-to-step.\nYou define an attr when you register the op, by specifying its name and type\nusing the `Attr` method, which expects a spec of the form:\n`<name>: <attr-type-expr>`\nwhere `<name>` begins with a letter and can be composed of alphanumeric\ncharacters and underscores, and `<attr-type-expr>` is a type expression of the\nform described below.\nFor example, if you'd like the `ZeroOut` op to preserve a user-specified index,\ninstead of only the 0th element, you can register the op like so:\n`c++\nREGISTER_OP(\"ZeroOut\")\n    .Attr(\"preserve_index: int\")\n    .Input(\"to_zero: int32\")\n    .Output(\"zeroed: int32\");`\n(Note that the set of attribute types is different from the\n`tf.DType` used for inputs and outputs.)\nYour kernel can then access this attr in its constructor via the `context`\nparameter:\n`c++\nclass ZeroOutOp : public OpKernel {\n public:\n  explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {\n    // Get the index of the value to preserve\n    OP_REQUIRES_OK(context,\n                   context->GetAttr(\"preserve_index\", &preserve_index_));\n    // Check that preserve_index is positive\n    OP_REQUIRES(context, preserve_index_ >= 0,\n                errors::InvalidArgument(\"Need preserve_index >= 0, got \",\n                                        preserve_index_));\n  }\n  void Compute(OpKernelContext* context) override {\n    // ...\n  }\n private:\n  int preserve_index_;\n};`\nwhich can then be used in the `Compute` method:\n```c++\n  void Compute(OpKernelContext* context) override {\n    // ...\n\n\n```// We're using saved attr to validate potentially dynamic input\n// So we check that preserve_index is in range\nOP_REQUIRES(context, preserve_index_ < input.dimension(0),\n            errors::InvalidArgument(\"preserve_index out of range\"));\n\n// Set all the elements of the output tensor to 0\nconst int N = input.size();\nfor (int i = 0; i < N; i++) {\n  output_flat(i) = 0;\n}\n\n// Preserve the requested input value\noutput_flat(preserve_index_) = input(preserve_index_);\n```\n\n\n}\n```\nAttr types\nThe following types are supported in an attr:\n\n`string`: Any sequence of bytes (not required to be UTF8).\n`int`: A signed integer.\n`float`: A floating point number.\n`bool`: True or false.\n`type`: One of the (non-ref) values of DataType.\n`shape`: A TensorShapeProto.\n`tensor`: A [`TensorProto`][TensorProto].\n`list(<type>)`: A list of `<type>`, where `<type>` is one of the above types.\n  Note that `list(list(<type>))` is invalid.\n\nSee also: op_def_builder.cc:FinalizeAttr for a definitive list.\nDefault values & constraints\nAttrs may have default values, and some types of attrs can have constraints. To\ndefine an attr with constraints, you can use the following `<attr-type-expr>`s:\n\n`{'<string1>', '<string2>'}`: The value must be a string that has either the\n  value `<string1>` or `<string2>`.  The name of the type, `string`, is implied\n  when you use this syntax.  This emulates an enum:\n\n`c++\n  REGISTER_OP(\"EnumExample\")\n      .Attr(\"e: {'apple', 'orange'}\");`\n\n`{<type1>, <type2>}`: The value is of type `type`, and must be one of\n  `<type1>` or `<type2>`, where `<type1>` and `<type2>` are supported\n  `tf.DType`.  You don't specify\n  that the type of the attr is `type`. This is implied when you have a list of\n  types in `{...}`.  For example, in this case the attr `t` is a type that must\n  be an `int32`, a `float`, or a `bool`:\n\n`c++\n  REGISTER_OP(\"RestrictedTypeExample\")\n      .Attr(\"t: {int32, float, bool}\");`\n\n\nThere are shortcuts for common type constraints:\n\n`numbertype`: Type `type` restricted to the numeric (non-string and\n  non-bool) types.\n`realnumbertype`: Like `numbertype` without complex types.\n`quantizedtype`: Like `numbertype` but just the quantized number types.\n\nThe specific lists of types allowed by these are defined by the functions\n(like `NumberTypes()`) in\ntensorflow/core/framework/types.h.\nIn this example the attr `t` must be one of the numeric types:\n`c++\nREGISTER_OP(\"NumberType\")\n    .Attr(\"t: numbertype\");`\nFor this op:\n`python\ntf.number_type(t=tf.int32)  # Valid\ntf.number_type(t=tf.bool)   # Invalid`\nLists can be combined with other lists and single types.  The following\nop allows attr `t` to be any of the numeric types, or the bool type:\n`c++\nREGISTER_OP(\"NumberOrBooleanType\")\n    .Attr(\"t: {numbertype, bool}\");`\nFor this op:\n`python\ntf.number_or_boolean_type(t=tf.int32)  # Valid\ntf.number_or_boolean_type(t=tf.bool)   # Valid\ntf.number_or_boolean_type(t=tf.string) # Invalid`\n\n\n`int >= <n>`: The value must be an int whose value is greater than or equal to\n  `<n>`, where `<n>` is a natural number.\n\n\nFor example, the following op registration specifies that the attr `a` must\n  have a value that is at least `2`:\n`c++\n  REGISTER_OP(\"MinIntExample\")\n      .Attr(\"a: int >= 2\");`\n\n`list(<type>) >= <n>`: A list of type `<type>` whose length is greater than\n  or equal to `<n>`.\n\nFor example, the following op registration specifies that the attr `a` is a\n  list of types (either `int32` or `float`), and that there must be at least 3\n  of them:\n`c++\n  REGISTER_OP(\"TypeListExample\")\n      .Attr(\"a: list({int32, float}) >= 3\");`\nTo set a default value for an attr (making it optional in the generated code),\nadd `= <default>` to the end, as in:\n`c++\nREGISTER_OP(\"AttrDefaultExample\")\n    .Attr(\"i: int = 0\");`\nThe supported syntax of the default value is what would be used in the proto\nrepresentation of the resulting GraphDef definition.\nHere are examples for how to specify a default for all types:\n`c++\nREGISTER_OP(\"AttrDefaultExampleForAllTypes\")\n   .Attr(\"s: string = 'foo'\")\n   .Attr(\"i: int = 0\")\n   .Attr(\"f: float = 1.0\")\n   .Attr(\"b: bool = true\")\n   .Attr(\"ty: type = DT_INT32\")\n   .Attr(\"sh: shape = { dim { size: 1 } dim { size: 2 } }\")\n   .Attr(\"te: tensor = { dtype: DT_INT32 int_val: 5 }\")\n   .Attr(\"l_empty: list(int) = []\")\n   .Attr(\"l_int: list(int) = [2, 3, 5, 7]\");`\nNote in particular that the values of type `type`\nuse `tf.DType`.\nPolymorphism\nType Polymorphism\nFor ops that can take different types as input or produce different output\ntypes, you can specify an attr in\nan input or output type in the op registration.  Typically\nyou would then register an `OpKernel` for each supported type.\nFor instance, if you'd like the `ZeroOut` op to work on `float`s\nin addition to `int32`s, your op registration might look like:\n`c++\nREGISTER_OP(\"ZeroOut\")\n    .Attr(\"T: {float, int32}\")\n    .Input(\"to_zero: T\")\n    .Output(\"zeroed: T\");`\nYour op registration now specifies that the input's type must be `float`, or\n`int32`, and that its output will be the same type, since both have type `T`.\n\nA note on naming: Inputs, outputs, and attrs generally should be\ngiven snake_case names.  The one exception is attrs that are used as the type\nof an input or in the type of an input. Those attrs can be inferred when the\nop is added to the graph and so don't appear in the op's function.  For\nexample, this last definition of ZeroOut will generate a Python function that\nlooks like:\n```python\ndef zero_out(to_zero, name=None):\n  \"\"\"...\n  Args:\n    to_zero: A`Tensor`. Must be one of the following types:`float32`,`int32`.\n    name: A name for the operation (optional).\nReturns:\n    A `Tensor`. Has the same type as `to_zero`.\n  \"\"\"\n```\nIf `to_zero` is passed an `int32` tensor, then `T` is automatically set to\n`int32` (well, actually `DT_INT32`). Those inferred attrs are given\nCapitalized or CamelCase names.\nCompare this with an op that has a type attr that determines the output\ntype:\n`c++\nREGISTER_OP(\"StringToNumber\")\n    .Input(\"string_tensor: string\")\n    .Output(\"output: out_type\")\n    .Attr(\"out_type: {float, int32} = DT_FLOAT\");\n    .Doc(R\"doc(\nConverts each string in the input Tensor to the specified numeric type.\n)doc\");`\nIn this case, the user has to specify the output type, as in the generated\nPython:\n```python\ndef string_to_number(string_tensor, out_type=None, name=None):\n  \"\"\"Converts each string in the input Tensor to the specified numeric type.\nArgs:\n    string_tensor: A `Tensor` of type `string`.\n    out_type: An optional `tf.DType` from: `tf.float32, tf.int32`.\n      Defaults to `tf.float32`.\n    name: A name for the operation (optional).\nReturns:\n    A `Tensor` of type `out_type`.\n  \"\"\"\n```\n\n```c++\ninclude \"tensorflow/core/framework/op_kernel.h\"\nclass ZeroOutInt32Op : public OpKernel {\n  // as before\n};\nclass ZeroOutFloatOp : public OpKernel {\n public:\n  explicit ZeroOutFloatOp(OpKernelConstruction* context)\n      : OpKernel(context) {}\nvoid Compute(OpKernelContext* context) override {\n    // Grab the input tensor\n    const Tensor& input_tensor = context->input(0);\n    auto input = input_tensor.flat();\n\n\n```// Create an output tensor\nTensor* output = NULL;\nOP_REQUIRES_OK(context,\n               context->allocate_output(0, input_tensor.shape(), &output));\nauto output_flat = output->template flat<float>();\n\n// Set all the elements of the output tensor to 0\nconst int N = input.size();\nfor (int i = 0; i < N; i++) {\n  output_flat(i) = 0;\n}\n\n// Preserve the first input value\nif (N > 0) output_flat(0) = input(0);\n```\n\n\n}\n};\n// Note that TypeConstraint(\"T\") means that attr \"T\" (defined\n// in the op registration above) must be \"int32\" to use this template\n// instantiation.\nREGISTER_KERNEL_BUILDER(\n    Name(\"ZeroOut\")\n    .Device(DEVICE_CPU)\n    .TypeConstraint(\"T\"),\n    ZeroOutInt32Op);\nREGISTER_KERNEL_BUILDER(\n    Name(\"ZeroOut\")\n    .Device(DEVICE_CPU)\n    .TypeConstraint(\"T\"),\n    ZeroOutFloatOp);\n```\n\nTo preserve backwards compatibility, you should\nspecify a default value when adding an attr to\nan existing op:\n`c++\nREGISTER_OP(\"ZeroOut\")\n  .Attr(\"T: {float, int32} = DT_INT32\")\n  .Input(\"to_zero: T\")\n  .Output(\"zeroed: T\")`\n\nLet's say you wanted to add more types, say `double`:\n`c++\nREGISTER_OP(\"ZeroOut\")\n    .Attr(\"T: {float, double, int32}\")\n    .Input(\"to_zero: T\")\n    .Output(\"zeroed: T\");`\nInstead of writing another `OpKernel` with redundant code as above, often you\nwill be able to use a C++ template instead.  You will still have one kernel\nregistration (`REGISTER_KERNEL_BUILDER` call) per overload.\n```c++\ntemplate \nclass ZeroOutOp : public OpKernel {\n public:\n  explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}\nvoid Compute(OpKernelContext* context) override {\n    // Grab the input tensor\n    const Tensor& input_tensor = context->input(0);\n    auto input = input_tensor.flat();\n\n\n```// Create an output tensor\nTensor* output = NULL;\nOP_REQUIRES_OK(context,\n               context->allocate_output(0, input_tensor.shape(), &output));\nauto output_flat = output->template flat<T>();\n\n// Set all the elements of the output tensor to 0\nconst int N = input.size();\nfor (int i = 0; i < N; i++) {\n  output_flat(i) = 0;\n}\n\n// Preserve the first input value\nif (N > 0) output_flat(0) = input(0);\n```\n\n\n}\n};\n// Note that TypeConstraint(\"T\") means that attr \"T\" (defined\n// in the op registration above) must be \"int32\" to use this template\n// instantiation.\nREGISTER_KERNEL_BUILDER(\n    Name(\"ZeroOut\")\n    .Device(DEVICE_CPU)\n    .TypeConstraint(\"T\"),\n    ZeroOutOp);\nREGISTER_KERNEL_BUILDER(\n    Name(\"ZeroOut\")\n    .Device(DEVICE_CPU)\n    .TypeConstraint(\"T\"),\n    ZeroOutOp);\nREGISTER_KERNEL_BUILDER(\n    Name(\"ZeroOut\")\n    .Device(DEVICE_CPU)\n    .TypeConstraint(\"T\"),\n    ZeroOutOp);\n```\nIf you have more than a couple overloads, you can put the registration in a\nmacro.\n```c++\ninclude \"tensorflow/core/framework/op_kernel.h\"\n#define REGISTER_KERNEL(type)                                       \\\n  REGISTER_KERNEL_BUILDER(                                          \\\n      Name(\"ZeroOut\").Device(DEVICE_CPU).TypeConstraint(\"T\"), \\\n      ZeroOutOp)\nREGISTER_KERNEL(int32);\nREGISTER_KERNEL(float);\nREGISTER_KERNEL(double);\nundef REGISTER_KERNEL\n```\nDepending on the list of types you are registering the kernel for, you may be\nable to use a macro provided by\ntensorflow/core/framework/register_types.h:\n```c++\ninclude \"tensorflow/core/framework/op_kernel.h\"\ninclude \"tensorflow/core/framework/register_types.h\"\nREGISTER_OP(\"ZeroOut\")\n    .Attr(\"T: realnumbertype\")\n    .Input(\"to_zero: T\")\n    .Output(\"zeroed: T\");\ntemplate \nclass ZeroOutOp : public OpKernel { ... };\n#define REGISTER_KERNEL(type)                                       \\\n  REGISTER_KERNEL_BUILDER(                                          \\\n      Name(\"ZeroOut\").Device(DEVICE_CPU).TypeConstraint(\"T\"), \\\n      ZeroOutOp)\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNEL);\nundef REGISTER_KERNEL\n```\nList Inputs and Outputs\nIn addition to being able to accept or produce different types, ops can consume\nor produce a variable number of tensors.\nIn the next example, the attr `T` holds a list of types, and is used as the\ntype of both the input `in` and the output `out`.  The input and output are\nlists of tensors of that type (and the number and types of tensors in the output\nare the same as the input, since both have type `T`).\n`c++\nREGISTER_OP(\"PolymorphicListExample\")\n    .Attr(\"T: list(type)\")\n    .Input(\"in: T\")\n    .Output(\"out: T\");`\nYou can also place restrictions on what types can be specified in the list. In\nthis next case, the input is a list of `float` and `double` tensors. The op\naccepts, for example, input types `(float, double, float)` and in that case the\noutput type would also be `(float, double, float)`.\n`c++\nREGISTER_OP(\"ListTypeRestrictionExample\")\n    .Attr(\"T: list({float, double})\")\n    .Input(\"in: T\")\n    .Output(\"out: T\");`\nIf you want all the tensors in a list to be of the same type, you might do\nsomething like:\n`c++\nREGISTER_OP(\"IntListInputExample\")\n    .Attr(\"N: int\")\n    .Input(\"in: N * int32\")\n    .Output(\"out: int32\");`\nThis accepts a list of `int32` tensors, and uses an `int` attr `N` to\nspecify the length of the list.\nThis can be made type polymorphic as well.  In the next\nexample, the input is a list of tensors (with length `\"N\"`) of the same (but\nunspecified) type (`\"T\"`), and the output is a single tensor of matching type:\n`c++\nREGISTER_OP(\"SameListInputExample\")\n    .Attr(\"N: int\")\n    .Attr(\"T: type\")\n    .Input(\"in: N * T\")\n    .Output(\"out: T\");`\nBy default, tensor lists have a minimum length of 1. You can change that default\nusing\na \">=\" constraint on the corresponding attr.\nIn this next example, the input is a list of at least 2 `int32` tensors:\n`c++\nREGISTER_OP(\"MinLengthIntListExample\")\n    .Attr(\"N: int >= 2\")\n    .Input(\"in: N * int32\")\n    .Output(\"out: int32\");`\nThe same syntax works with `\"list(type)\"` attrs:\n`c++\nREGISTER_OP(\"MinimumLengthPolymorphicListExample\")\n    .Attr(\"T: list(type) >= 3\")\n    .Input(\"in: T\")\n    .Output(\"out: T\");`\nInputs and Outputs\nTo summarize the above, an op registration can have multiple inputs and outputs:\n`c++\nREGISTER_OP(\"MultipleInsAndOuts\")\n    .Input(\"y: int32\")\n    .Input(\"z: float\")\n    .Output(\"a: string\")\n    .Output(\"b: int32\");`\nEach input or output spec is of the form:\n`<name>: <io-type-expr>`\nwhere `<name>` begins with a letter and can be composed of alphanumeric\ncharacters and underscores. `<io-type-expr>` is one of the following type\nexpressions:\n\n`<type>`, where `<type>` is a supported input type (e.g. `float`, `int32`,\n  `string`). This specifies a single tensor of the given type.\n\nSee\n  `tf.DType`.\n`c++\n  REGISTER_OP(\"BuiltInTypesExample\")\n      .Input(\"integers: int32\")\n      .Input(\"complex_numbers: complex64\");`\n\n`<attr-type>`, where `<attr-type>` is the name of an Attr with type\n  `type` or `list(type)` (with a possible type restriction). This syntax allows\n  for polymorphic ops.\n\n```c++\n  REGISTER_OP(\"PolymorphicSingleInput\")\n      .Attr(\"T: type\")\n      .Input(\"in: T\");\nREGISTER_OP(\"RestrictedPolymorphicSingleInput\")\n      .Attr(\"T: {int32, int64}\")\n      .Input(\"in: T\");\n  ```\nReferencing an attr of type `list(type)` allows you to accept a sequence of\n  tensors.\n```c++\n  REGISTER_OP(\"ArbitraryTensorSequenceExample\")\n      .Attr(\"T: list(type)\")\n      .Input(\"in: T\")\n      .Output(\"out: T\");\nREGISTER_OP(\"RestrictedTensorSequenceExample\")\n      .Attr(\"T: list({int32, int64})\")\n      .Input(\"in: T\")\n      .Output(\"out: T\");\n  ```\nNote that the number and types of tensors in the output `out` is the same as\n  in the input `in`, since both are of type `T`.\n\nFor a sequence of tensors with the same type: `<number> * <type>`, where\n  `<number>` is the name of an Attr with type `int`.  The `<type>` can\n  either be a `tf.DType`,\n  or the name of an attr with type `type`.  As an example of the first, this\n  op accepts a list of `int32` tensors:\n\n`c++\n  REGISTER_OP(\"Int32SequenceExample\")\n      .Attr(\"NumTensors: int\")\n      .Input(\"in: NumTensors * int32\")`\nWhereas this op accepts a list of tensors of any type, as long as they are all\n  the same:\n`c++\n  REGISTER_OP(\"SameTypeSequenceExample\")\n      .Attr(\"NumTensors: int\")\n      .Attr(\"T: type\")\n      .Input(\"in: NumTensors * T\")`\n\nFor a reference to a tensor: `Ref(<type>)`, where `<type>` is one of the\n  previous types.\n\n\nA note on naming: Any attr used in the type of an input will be inferred.  By\nconvention those inferred attrs use capital names (like `T` or `N`).\nOtherwise inputs, outputs, and attrs have names like function parameters\n(e.g. `num_outputs`).  For more details, see the\nearlier note on naming.\n\nFor more details, see\ntensorflow/core/framework/op_def_builder.h.\nBackwards compatibility\nLet's assume you have written a nice, custom op and shared it with others, so\nyou have happy customers using your operation.  However, you'd like to make\nchanges to the op in some way.\nIn general, changes to existing, checked-in specifications must be\nbackwards-compatible: changing the specification of an op must not break prior\nserialized `GraphDef` protocol buffers constructed from older specifications.\nThe details of `GraphDef` compatibility are\ndescribed here.\nThere are several ways to preserve backwards-compatibility.\n\n\nAny new attrs added to an operation must have default values defined, and\n   with that default value the op must have the original behavior. To change an\n   operation from not polymorphic to polymorphic, you must give a default\n   value to the new type attr to preserve the original signature by default. For\n   example, if your operation was:\nREGISTER_OP(\"MyGeneralUnaryOp\")\n       .Input(\"in: float\")\n       .Output(\"out: float\");\n\n\nyou can make it polymorphic in a backwards-compatible way using:\n\n\n```   REGISTER_OP(\"MyGeneralUnaryOp\")\n       .Input(\"in: T\")\n       .Output(\"out: T\")\n       .Attr(\"T: numerictype = DT_FLOAT\");\n```\n\n\n\n\nYou can safely make a constraint on an attr less restrictive.  For example,\n   you can change from `{int32, int64}` to `{int32, int64, float}` or `type`.\n   Or you may change from `{\"apple\", \"orange\"}` to `{\"apple\", \"banana\",\n   \"orange\"}` or `string`.\n\n\nYou can change single inputs / outputs into list inputs / outputs, as long as\n   the default for the list type matches the old signature.\n\n\nYou can add a new list input / output, if it defaults to empty.\n\n\nNamespace any new ops you create, by prefixing the op names with something\n   unique to your project. This avoids having your op colliding with any ops\n   that might be included in future versions of TensorFlow.\n\n\nPlan ahead! Try to anticipate future uses for the op. Some signature changes\n   can't be done in a compatible way (for example, making a list of the same\n   type into a list of varying types).\n\n\nThe full list of safe and unsafe changes can be found in\ntensorflow/core/framework/op_compatibility_test.cc.\nIf you cannot make your change to an operation backwards compatible, then create\na new operation with a new name with the new semantics.\nAlso note that while these changes can maintain `GraphDef` compatibility, the\ngenerated Python code may change in a way that isn't compatible with old\ncallers.  The Python API may be kept compatible by careful changes in a\nhand-written Python wrapper, by keeping the old signature except possibly adding\nnew optional arguments to the end.  Generally incompatible changes may only be\nmade when TensorFlow changes major versions, and must conform to the\nGraphDef version semantics.\nGPU Support\nYou can implement different OpKernels and register one for CPU and another for\nGPU, just like you can register kernels for different types.\nThere are several examples of kernels with GPU support in\ntensorflow/core/kernels/.\nNotice some kernels have a CPU version in a `.cc` file, a GPU version in a file\nending in `_gpu.cu.cc`, and some code shared in common in a `.h` file.\nFor example, the `tf.pad` has\neverything but the GPU kernel in tensorflow/core/kernels/pad_op.cc.\nThe GPU kernel is in\ntensorflow/core/kernels/pad_op_gpu.cu.cc,\nand the shared code is a templated class defined in\ntensorflow/core/kernels/pad_op.h.\nWe organize the code this way for two reasons: it allows you to share common\ncode among the CPU and GPU implementations, and it puts the GPU implementation\ninto a separate file so that it can be compiled only by the GPU compiler.\nOne thing to note, even when the GPU kernel version of `pad` is used, it still\nneeds its `\"paddings\"` input in CPU memory.  To mark that inputs or outputs are\nkept on the CPU, add a `HostMemory()` call to the kernel registration, e.g.:\n`c++\n#define REGISTER_GPU_KERNEL(T)                         \\\n  REGISTER_KERNEL_BUILDER(Name(\"Pad\")                  \\\n                              .Device(DEVICE_GPU)      \\\n                              .TypeConstraint<T>(\"T\")  \\\n                              .HostMemory(\"paddings\"), \\\n                          PadOp<GPUDevice, T>)`\nCompiling the kernel for the GPU device\nLook at\ncuda_op_kernel.cu.cc\nfor an example that uses a CUDA kernel to implement an op. The\n`tf_custom_op_library` accepts a `gpu_srcs` argument in which the list of source\nfiles containing the CUDA kernels (`*.cu.cc` files) can be specified. For use\nwith a binary installation of TensorFlow, the CUDA kernels have to be compiled\nwith NVIDIA's `nvcc` compiler. Here is the sequence of commands you can use to\ncompile the\ncuda_op_kernel.cu.cc\nand\ncuda_op_kernel.cc\ninto a single dynamically loadable library:\n```bash\nnvcc -std=c++11 -c -o cuda_op_kernel.cu.o cuda_op_kernel.cu.cc \\\n  ${TF_CFLAGS[@]} -D GOOGLE_CUDA=1 -x cu -Xcompiler -fPIC\ng++ -std=c++11 -shared -o cuda_op_kernel.so cuda_op_kernel.cc \\\n  cuda_op_kernel.cu.o ${TF_CFLAGS[@]} -fPIC -lcudart ${TF_LFLAGS[@]}\n```\n`cuda_op_kernel.so` produced above can be loaded as usual in Python, using the\n`tf.load_op_library` function.\nNote that if your CUDA libraries are not installed in `/usr/local/lib64`,\nyou'll need to specify the path explicitly in the second (g++) command above.\nFor example, add `-L /usr/local/cuda-8.0/lib64/` if your CUDA is installed in\n`/usr/local/cuda-8.0`.\n\nNote in some linux settings, additional options to `nvcc` compiling step are needed. Add `-D_MWAITXINTRIN_H_INCLUDED` to the `nvcc` command line to avoid errors from `mwaitxintrin.h`.\n\nImplement the gradient in Python\nGiven a graph of ops, TensorFlow uses automatic differentiation\n(backpropagation) to add new ops representing gradients with respect to the\nexisting ops.\nTo make automatic differentiation work for new ops, you must register a gradient\nfunction which computes gradients with respect to the ops' inputs given\ngradients with respect to the ops' outputs.\nMathematically, if an op computes \\(y = f(x)\\) the registered gradient op\nconverts gradients \\(\\partial L/ \\partial y\\) of loss \\(L\\) with respect to\n\\(y\\) into gradients \\(\\partial L/ \\partial x\\) with respect to \\(x\\) via\nthe chain rule:\n$$\\frac{\\partial L}{\\partial x}\n    = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial x}\n    = \\frac{\\partial L}{\\partial y} \\frac{\\partial f}{\\partial x}.$$\nIn the case of `ZeroOut`, only one entry in the input affects the output, so the\ngradient with respect to the input is a sparse \"one hot\" tensor.  This is\nexpressed as follows:\n```python\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import sparse_ops\n@ops.RegisterGradient(\"ZeroOut\")\ndef _zero_out_grad(op, grad):\n  \"\"\"The gradients for `zero_out`.\nArgs:\n    op: The `zero_out` `Operation` that we are differentiating, which we can use\n      to find the inputs and outputs of the original op.\n    grad: Gradient with respect to the output of the `zero_out` op.\nReturns:\n    Gradients with respect to the input of `zero_out`.\n  \"\"\"\n  to_zero = op.inputs[0]\n  shape = array_ops.shape(to_zero)\n  index = array_ops.zeros_like(shape)\n  first_grad = array_ops.reshape(grad, [-1])[0]\n  to_zero_grad = sparse_ops.sparse_to_dense([index], shape, first_grad, 0)\n  return [to_zero_grad]  # List of one Tensor, since we have one input\n```\nDetails about registering gradient functions with\n`tf.RegisterGradient`:\n\n\nFor an op with one output, the gradient function will take an\n  `tf.Operation` `op` and a\n  `tf.Tensor` `grad` and build new ops\n  out of the tensors\n  op.inputs[i],\n  op.outputs[i], and `grad`.  Information\n  about any attrs can be found via\n  `tf.Operation.get_attr`.\n\n\nIf the op has multiple outputs, the gradient function will take `op` and\n  `grads`, where `grads` is a list of gradients with respect to each output.\n  The result of the gradient function must be a list of `Tensor` objects\n  representing the gradients with respect to each input.\n\n\nIf there is no well-defined gradient for some input, such as for integer\n  inputs used as indices, the corresponding returned gradient should be\n  `None`.  For example, for an op taking a floating point tensor `x` and an\n  integer index `i`, the gradient function would `return [x_grad, None]`.\n\n\nIf there is no meaningful gradient for the op at all, you often will not have\n  to register any gradient, and as long as the op's gradient is never needed,\n  you will be fine. In some cases, an op has no well-defined gradient but can\n  be involved in the computation of the gradient. Here you can use\n  `ops.NotDifferentiable` to automatically propagate zeros backwards.\n\n\nNote that at the time the gradient function is called, only the data flow graph\nof ops is available, not the tensor data itself.  Thus, all computation must be\nperformed using other tensorflow ops, to be run at graph execution time.\nShape functions in C++\nThe TensorFlow API has a feature called \"shape inference\" that provides\ninformation about the shapes of tensors without having to execute the\ngraph. Shape inference is supported by \"shape functions\" that are registered for\neach op type in the C++ `REGISTER_OP` declaration, and perform two roles:\nasserting that the shapes of the inputs are compatible during graph\nconstruction, and specifying the shapes for the outputs.\nShape functions are defined as operations on the\n`shape_inference::InferenceContext` class. For example, in the shape function\nfor ZeroOut:\n`c++\n    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\n      c->set_output(0, c->input(0));\n      return Status::OK();\n    });`\n`c->set_output(0, c->input(0));` declares that the first output's shape should\nbe set to the first input's shape. If the output is selected by its index as in the above example, the second parameter of `set_output` should be a `ShapeHandle` object. You can create an empty `ShapeHandle` object by its default constructor. The `ShapeHandle` object for an input with index `idx` can be obtained by `c->input(idx)`.\nThere are a number of common shape functions\nthat apply to many ops, such as `shape_inference::UnchangedShape` which can be\nfound in common_shape_fns.h and used as follows:\n`c++\nREGISTER_OP(\"ZeroOut\")\n    .Input(\"to_zero: int32\")\n    .Output(\"zeroed: int32\")\n    .SetShapeFn(::tensorflow::shape_inference::UnchangedShape);`\nA shape function can also constrain the shape of an input. For the version of\nZeroOut with a vector shape constraint, the shape function\nwould be as follows:\n`c++\n    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\n      ::tensorflow::shape_inference::ShapeHandle input;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 1, &input));\n      c->set_output(0, input);\n      return Status::OK();\n    });`\nThe `WithRank` call validates that the input shape `c->input(0)` has\na shape with exactly one dimension (or if the input shape is unknown,\nthe output shape will be a vector with one unknown dimension).\nIf your op is polymorphic with multiple inputs, you can use\nmembers of `InferenceContext` to determine the number of shapes to check, and\n`Merge` to validate that the shapes are all compatible (alternatively, access\nattributes that indicate the lengths, with `InferenceContext::GetAttr`, which\nprovides access to the attributes of the op).\n`c++\n    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\n      ::tensorflow::shape_inference::ShapeHandle input;\n      ::tensorflow::shape_inference::ShapeHandle output;\n      for (size_t i = 0; i < c->num_inputs(); ++i) {\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(i), 2, &input));\n        TF_RETURN_IF_ERROR(c->Merge(output, input, &output));\n      }\n      c->set_output(0, output);\n      return Status::OK();\n    });`\nSince shape inference is an optional feature, and the shapes of tensors may vary\ndynamically, shape functions must be robust to incomplete shape information for\nany of the inputs. The `Merge` method in InferenceContext\nallows the caller to assert that two shapes are the same, even if either\nor both of them do not have complete information. Shape functions are defined\nfor all of the core TensorFlow ops and provide many different usage examples.\nThe `InferenceContext` class has a number of functions that can be used to\ndefine shape function manipulations.  For example, you can validate that a\nparticular dimension has a very specific value using `InferenceContext::Dim` and\n`InferenceContext::WithValue`; you can specify that an output dimension is the\nsum / product of two input dimensions using `InferenceContext::Add` and\n`InferenceContext::Multiply`. See the `InferenceContext` class for\nall of the various shape manipulations you can specify. The following example sets\nshape of the first output to (n, 3), where first input has shape (n, ...)\n`c++\n.SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\n    c->set_output(0, c->Matrix(c->Dim(c->input(0), 0), 3));\n    return Status::OK();\n});`\nIf you have a complicated shape function, you should consider adding a test for\nvalidating that various input shape combinations produce the expected output\nshape combinations.  You can see examples of how to write these tests in some\nour\ncore ops tests.\n(The syntax of `INFER_OK` and `INFER_ERROR` are a little cryptic, but try to be\ncompact in representing input and output shape specifications in tests.  For\nnow, see the surrounding comments in those tests to get a sense of the shape\nstring specification).\nBuild a pip package for your custom op\nTo build a `pip` package for your op, see the\ntensorflow/custom-op example. This\nguide shows how to build custom ops from the TensorFlow pip package instead\nof building TensorFlow from source.",
    "tag": "tensorflow"
  },
  {
    "title": "C++ API",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/guide/extend/cc.md",
    "content": "C++ API\nNote: The instructions in this doc require\nbuilding from source. You probably want to build from\nthe `master` branch of TensorFlow.\nTensorFlow's C++ API provides mechanisms for constructing and executing a data\nflow graph. The API is designed to be simple and concise: graph operations are\nclearly expressed using a \"functional\" construction style, including easy\nspecification of names, device placement, etc., and the resulting graph can be\nefficiently run and the desired outputs fetched in a few lines of code. This\nguide explains the basic concepts and data structures needed to get started with\nTensorFlow graph construction and execution in C++.\nThe C++ API is only designed to work with TensorFlow `bazel build`.\nIf you need a stand-alone option, use the C API.\nSee these instructions\nfor details on how to include TensorFlow as a subproject (instead of building\nyour project from inside TensorFlow, as in this example).\nThe Basics\nLet's start with a simple example that illustrates graph construction and\nexecution using the C++ API.\n```c++\n// tensorflow/cc/example/example.cc\ninclude \"tensorflow/cc/client/client_session.h\"\ninclude \"tensorflow/cc/ops/standard_ops.h\"\ninclude \"tensorflow/core/framework/tensor.h\"\nint main() {\n  using namespace tensorflow;\n  using namespace tensorflow::ops;\n  Scope root = Scope::NewRootScope();\n  // Matrix A = [3 2; -1 0]\n  auto A = Const(root, { {3.f, 2.f}, {-1.f, 0.f} });\n  // Vector b = [3 5]\n  auto b = Const(root, { {3.f, 5.f} });\n  // v = Ab^T\n  auto v = MatMul(root.WithOpName(\"v\"), A, b, MatMul::TransposeB(true));\n  std::vector outputs;\n  ClientSession session(root);\n  // Run and fetch v\n  TF_CHECK_OK(session.Run({v}, &outputs));\n  // Expect outputs[0] == [19; -3]\n  LOG(INFO) << outputs[0].matrix();\n  return 0;\n}\n```\nPlace this example code in the file `tensorflow/cc/example/example.cc` inside a\nclone of the\nTensorFlow\nGitHub repository. Also place a\n`BUILD` file in the same directory with the following contents:\n```python\nload(\"//tensorflow:tensorflow.bzl\", \"tf_cc_binary\")\ntf_cc_binary(\n    name = \"example\",\n    srcs = [\"example.cc\"],\n    deps = [\n        \"//tensorflow/cc:cc_ops\",\n        \"//tensorflow/cc:client_session\",\n        \"//tensorflow/core:tensorflow\",\n    ],\n)\n```\nUse `tf_cc_binary` rather than Bazel's native `cc_binary` to link in necessary\nsymbols from `libtensorflow_framework.so`. You should be able to build and run\nthe example using the following command (be sure to run `./configure` in your\nbuild sandbox first):\n`shell\nbazel run -c opt //tensorflow/cc/example:example`\nThis example shows some of the important features of the C++ API such as the\nfollowing:\n\nConstructing tensor constants from C++ nested initializer lists\nConstructing and naming of TensorFlow operations\nSpecifying optional attributes to operation constructors\nExecuting and fetching the tensor values from the TensorFlow session.\n\nWe will delve into the details of each below.\nGraph Construction\nScope\n`tensorflow::Scope` is the main data structure that holds the current state\nof graph construction. A `Scope` acts as a handle to the graph being\nconstructed, as well as storing TensorFlow operation properties. The `Scope`\nobject is the first argument to operation constructors, and operations that use\na given `Scope` as their first argument inherit that `Scope`'s properties, such\nas a common name prefix. Multiple `Scope`s can refer to the same graph, as\nexplained further below.\nCreate a new `Scope` object by calling `Scope::NewRootScope`. This creates\nsome resources such as a graph to which operations are added. It also creates a\n`tensorflow::Status` object which will be used to indicate errors encountered\nwhen constructing operations. The `Scope` class has value semantics, thus, a\n`Scope` object can be freely copied and passed around.\nThe `Scope` object returned by `Scope::NewRootScope` is referred\nto as the root scope. \"Child\" scopes can be constructed from the root scope by\ncalling various member functions of the `Scope` class, thus forming a hierarchy\nof scopes. A child scope inherits all of the properties of the parent scope and\ntypically has one property added or changed. For instance, `NewSubScope(name)`\nappends `name` to the prefix of names for operations created using the returned\n`Scope` object.\nHere are some of the properties controlled by a `Scope` object:\n\nOperation names\nSet of control dependencies for an operation\nDevice placement for an operation\nKernel attribute for an operation\n\nPlease refer to `tensorflow::Scope` for the complete list of member functions\nthat let you create child scopes with new properties.\nOperation Constructors\nYou can create graph operations with operation constructors, one C++ class per\nTensorFlow operation. Unlike the Python API which uses snake-case to name the\noperation constructors, the C++ API uses camel-case to conform to C++ coding\nstyle. For instance, the `MatMul` operation has a C++ class with the same name.\nUsing this class-per-operation method, it is possible, though not recommended,\nto construct an operation as follows:\n`c++\n// Not recommended\nMatMul m(scope, a, b);`\nInstead, we recommend the following \"functional\" style for constructing\noperations:\n`c++\n// Recommended\nauto m = MatMul(scope, a, b);`\nThe first parameter for all operation constructors is always a `Scope` object.\nTensor inputs and mandatory attributes form the rest of the arguments.\nFor optional arguments, constructors have an optional parameter that allows\noptional attributes.  For operations with optional arguments, the constructor's\nlast optional parameter is a `struct` type called `[operation]:Attrs` that\ncontains data members for each optional attribute. You can construct such\n`Attrs` in multiple ways:\n\nYou can specify a single optional attribute by constructing an `Attrs` object\nusing the `static` functions provided in the C++ class for the operation. For\nexample:\n\n`c++\nauto m = MatMul(scope, a, b, MatMul::TransposeA(true));`\n\nYou can specify multiple optional attributes by chaining together functions\n  available in the `Attrs` struct. For example:\n\n```c++\nauto m = MatMul(scope, a, b, MatMul::TransposeA(true).TransposeB(true));\n// Or, alternatively\nauto m = MatMul(scope, a, b, MatMul::Attrs().TransposeA(true).TransposeB(true));\n```\nThe arguments and return values of operations are handled in different ways\ndepending on their type:\n\nFor operations that return single tensors, the object returned by\n  the operation object can be passed directly to other operation\n  constructors. For example:\n\n`c++\nauto m = MatMul(scope, x, W);\nauto sum = Add(scope, m, bias);`\n\nFor operations producing multiple outputs, the object returned by the\n  operation constructor has a member for each of the outputs. The names of those\n  members are identical to the names present in the `OpDef` for the\n  operation. For example:\n\n`c++\nauto u = Unique(scope, a);\n// u.y has the unique values and u.idx has the unique indices\nauto m = Add(scope, u.y, b);`\n\nOperations producing a list-typed output return an object that can\n  be indexed using the `[]` operator. That object can also be directly passed to\n  other constructors that expect list-typed inputs. For example:\n\n`c++\nauto s = Split(scope, 0, a, 2);\n// Access elements of the returned list.\nauto b = Add(scope, s[0], s[1]);\n// Pass the list as a whole to other constructors.\nauto c = Concat(scope, s, 0);`\nConstants\nYou may pass many different types of C++ values directly to tensor\nconstants. You may explicitly create a tensor constant by calling the\n`tensorflow::ops::Const` function from various kinds of C++ values. For\nexample:\n\nScalars\n\n`c++\nauto f = Const(scope, 42.0f);\nauto s = Const(scope, \"hello world!\");`\n\nNested initializer lists\n\n`c++\n// 2x2 matrix\nauto c1 = Const(scope, { {1, 2}, {2, 4} });\n// 1x3x1 tensor\nauto c2 = Const(scope, { { {1}, {2}, {3} } });\n// 1x2x0 tensor\nauto c3 = ops::Const(scope, { { {}, {} } });`\n\nShapes explicitly specified\n\n`c++\n// 2x2 matrix with all elements = 10\nauto c1 = Const(scope, 10, /* shape */ {2, 2});\n// 1x3x2x1 tensor\nauto c2 = Const(scope, {1, 2, 3, 4, 5, 6}, /* shape */ {1, 3, 2, 1});`\nYou may directly pass constants to other operation constructors, either by\nexplicitly constructing one using the `Const` function, or implicitly as any of\nthe above types of C++ values. For example:\n`c++\n// [1 1] * [41; 1]\nauto x = MatMul(scope, { {1, 1} }, { {41}, {1} });\n// [1 2 3 4] + 10\nauto y = Add(scope, {1, 2, 3, 4}, 10);`\nGraph Execution\nWhen executing a graph, you will need a session. The C++ API provides a\n`tensorflow::ClientSession` class that will execute ops created by the\noperation constructors. TensorFlow will automatically determine which parts of\nthe graph need to be executed, and what values need feeding. For example:\n```c++\nScope root = Scope::NewRootScope();\nauto c = Const(root, { {1, 1} });\nauto m = MatMul(root, c, { {41}, {1} });\nClientSession session(root);\nstd::vector outputs;\nsession.Run({m}, &outputs);\n// outputs[0] == {42}\n```\nSimilarly, the object returned by the operation constructor can be used as the\nargument to specify a value being fed when executing the graph. Furthermore, the\nvalue to feed can be specified with the different kinds of C++ values used to\nspecify tensor constants. For example:\n```c++\nScope root = Scope::NewRootScope();\nauto a = Placeholder(root, DT_INT32);\n// [3 3; 3 3]\nauto b = Const(root, 3, {2, 2});\nauto c = Add(root, a, b);\nClientSession session(root);\nstd::vector outputs;\n// Feed a <- [1 2; 3 4]\nsession.Run({ {a, { {1, 2}, {3, 4} } } }, {c}, &outputs);\n// outputs[0] == [4 5; 6 7]\n```\nPlease see the `tensorflow::Tensor` documentation for more information on how",
    "tag": "tensorflow"
  },
  {
    "title": "Adding a Custom Filesystem Plugin",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/guide/extend/filesystem.md",
    "content": "Adding a Custom Filesystem Plugin\nBackground\nThe TensorFlow framework is often used in multi-process and\nmulti-machine environments, such as Google data centers, Google Cloud\nMachine Learning, Amazon Web Services (AWS), and on-site distributed clusters.\nIn order to both share and save certain types of state produced by TensorFlow,\nthe framework assumes the existence of a reliable, shared filesystem. This\nshared filesystem has numerous uses, for example:\n\nCheckpoints of state are often saved to a distributed filesystem for\n    reliability and fault-tolerance.\nTraining processes communicate with TensorBoard by writing event files\n    to a directory, which TensorBoard watches. A shared filesystem allows this\n    communication to work even when TensorBoard runs in a different process or\n    machine.\n\nThere are many different implementations of shared or distributed filesystems in\nthe real world, so TensorFlow provides an ability for users to implement a\ncustom FileSystem plugin that can be registered with the TensorFlow runtime.\nWhen the TensorFlow runtime attempts to write to a file through the `FileSystem`\ninterface, it uses a portion of the pathname to dynamically select the\nimplementation that should be used for filesystem operations. Thus, adding\nsupport for your custom filesystem requires implementing a `FileSystem`\ninterface, building a shared object containing that implementation, and loading\nthat object at runtime in whichever process needs to write to that filesystem.\nNote that TensorFlow already includes many filesystem implementations, such as:\n\n\nA standard POSIX filesystem\nNote: NFS filesystems often mount as a POSIX interface, and so standard\nTensorFlow can work on top of NFS-mounted remote filesystems.\n\n\nHDFS - the Hadoop File System\n\nGCS - Google Cloud Storage filesystem\nS3 - Amazon Simple Storage Service filesystem\nA \"memory-mapped-file\" filesystem\n\nThe rest of this guide describes how to implement a custom filesystem.\nImplementing a custom filesystem plugin\nTo implement a custom filesystem plugin, you must do the following:\n\nImplement subclasses of `RandomAccessFile`, `WriteableFile`,\n    `AppendableFile`, and `ReadOnlyMemoryRegion`.\nImplement the `FileSystem` interface as a subclass.\nRegister the `FileSystem` implementation with an appropriate prefix pattern.\nLoad the filesystem plugin in a process that wants to write to that\n    filesystem.\n\nThe FileSystem interface\nThe `FileSystem` interface is an abstract C++ interface defined in\nfile_system.h.\nAn implementation of the `FileSystem` interface should implement all relevant\nthe methods defined by the interface. Implementing the interface requires\ndefining operations such as creating `RandomAccessFile`, `WritableFile`, and\nimplementing standard filesystem operations such as `FileExists`, `IsDirectory`,\n`GetMatchingPaths`, `DeleteFile`, and so on. An implementation of these\ninterfaces will often involve translating the function's input arguments to\ndelegate to an already-existing library function implementing the equivalent\nfunctionality in your custom filesystem.\nFor example, the `PosixFileSystem` implementation implements `DeleteFile` using\nthe POSIX `unlink()` function; `CreateDir` simply calls `mkdir()`; `GetFileSize`\ninvolves calling `stat()` on the file and then returns the filesize as reported\nby the return of the stat object. Similarly, for the `HDFSFileSystem`\nimplementation, these calls simply delegate to the `libHDFS` implementation of\nsimilar functionality, such as `hdfsDelete` for\nDeleteFile.\nWe suggest looking through these code examples to get an idea of how different\nfilesystem implementations call their existing libraries. Examples include:\n\nPOSIX\n    plugin\nHDFS\n    plugin\nGCS\n    plugin\nS3\n    plugin\n\nThe File interfaces\nBeyond operations that allow you to query and manipulate files and directories\nin a filesystem, the `FileSystem` interface requires you to implement factories\nthat return implementations of abstract objects such as the\nRandomAccessFile,\nthe `WritableFile`, so that TensorFlow code and read and write to files in that\n`FileSystem` implementation.\nTo implement a `RandomAccessFile`, you must implement a single interface called\n`Read()`, in which the implementation must provide a way to read from an offset\nwithin a named file.\nFor example, below is the implementation of RandomAccessFile for the POSIX\nfilesystem, which uses the `pread()` random-access POSIX function to implement\nread. Notice that the particular implementation must know how to retry or\npropagate errors from the underlying filesystem.\n```C++\n    class PosixRandomAccessFile : public RandomAccessFile {\n     public:\n      PosixRandomAccessFile(const string& fname, int fd)\n          : filename_(fname), fd_(fd) {}\n      ~PosixRandomAccessFile() override { close(fd_); }\n\n\n```  Status Read(uint64 offset, size_t n, StringPiece* result,\n              char* scratch) const override {\n    Status s;\n    char* dst = scratch;\n    while (n > 0 && s.ok()) {\n      ssize_t r = pread(fd_, dst, n, static_cast<off_t>(offset));\n      if (r > 0) {\n        dst += r;\n        n -= r;\n        offset += r;\n      } else if (r == 0) {\n        s = Status(error::OUT_OF_RANGE, \"Read less bytes than requested\");\n      } else if (errno == EINTR || errno == EAGAIN) {\n        // Retry\n      } else {\n        s = IOError(filename_, errno);\n      }\n    }\n    *result = StringPiece(scratch, dst - scratch);\n    return s;\n  }\n\n private:\n  string filename_;\n  int fd_;\n};\n```\n\n\n```\nTo implement the WritableFile sequential-writing abstraction, one must implement\na few interfaces, such as `Append()`, `Flush()`, `Sync()`, and `Close()`.\nFor example, below is the implementation of WritableFile for the POSIX\nfilesystem, which takes a `FILE` object in its constructor and uses standard\nposix functions on that object to implement the interface.\n```C++\n    class PosixWritableFile : public WritableFile {\n     public:\n      PosixWritableFile(const string& fname, FILE* f)\n          : filename_(fname), file_(f) {}\n\n\n```  ~PosixWritableFile() override {\n    if (file_ != NULL) {\n      fclose(file_);\n    }\n  }\n\n  Status Append(const StringPiece& data) override {\n    size_t r = fwrite(data.data(), 1, data.size(), file_);\n    if (r != data.size()) {\n      return IOError(filename_, errno);\n    }\n    return Status::OK();\n  }\n\n  Status Close() override {\n    Status result;\n    if (fclose(file_) != 0) {\n      result = IOError(filename_, errno);\n    }\n    file_ = NULL;\n    return result;\n  }\n\n  Status Flush() override {\n    if (fflush(file_) != 0) {\n      return IOError(filename_, errno);\n    }\n    return Status::OK();\n  }\n\n  Status Sync() override {\n    Status s;\n    if (fflush(file_) != 0) {\n      s = IOError(filename_, errno);\n    }\n    return s;\n  }\n\n private:\n  string filename_;\n  FILE* file_;\n};\n```\n\n\n```\nFor more details, please see the documentations of those interfaces, and look at\nexample implementations for inspiration.\nRegistering and loading the filesystem\nOnce you have implemented the `FileSystem` implementation for your custom\nfilesystem, you need to register it under a \"scheme\" so that paths prefixed with\nthat scheme are directed to your implementation. To do this, you call\n`REGISTER_FILE_SYSTEM`::\n`REGISTER_FILE_SYSTEM(\"foobar\", FooBarFileSystem);`\nWhen TensorFlow tries to operate on a file whose path starts with `foobar://`,\nit will use the `FooBarFileSystem` implementation.\n```C++\n    string filename = \"foobar://path/to/file.txt\";\n    std::unique_ptr file;\n\n\n```// Calls FooBarFileSystem::NewWritableFile to return\n// a WritableFile class, which happens to be the FooBarFileSystem's\n// WritableFile implementation.\nTF_RETURN_IF_ERROR(env->NewWritableFile(filename, &file));\n```\n\n\n```\nNext, you must build a shared object containing this implementation. An example\nof doing so using bazel's `cc_binary` rule can be found\nhere,\nbut you may use any build system to do so. See the section on building the op library for similar\ninstructions.\nThe result of building this target is a `.so` shared object file.\nLastly, you must dynamically load this implementation in the process. In Python,\nyou can call the `tf.load_file_system_library(file_system_library)` function,\npassing the path to the shared object. Calling this in your client program loads\nthe shared object in the process, thus registering your implementation as\navailable for any file operations going through the `FileSystem` interface. You\ncan see\ntest_file_system.py\nfor an example.\nWhat goes through this interface?\nAlmost all core C++ file operations within TensorFlow use the `FileSystem`\ninterface, such as the `CheckpointWriter`, the `EventsWriter`, and many other\nutilities. This means implementing a `FileSystem` implementation allows most of\nyour TensorFlow programs to write to your shared filesystem.\nIn Python, the `gfile` and `file_io` classes bind underneath to the `FileSystem\nimplementation via SWIG, which means that once you have loaded this filesystem\nlibrary, you can do:\n```\nwith gfile.Open(\"foobar://path/to/file.txt\") as w:\nw.write(\"hi\")\n```\nWhen you do this, a file containing \"hi\" will appear in the \"/path/to/file.txt\"",
    "tag": "tensorflow"
  },
  {
    "title": "Performance",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/guide/performance/overview.md",
    "content": "Performance\nBetter TensorFlow performance comes out-of-the-box by using the high-level APIs.\nThe sections below detail the high-level APIs to use as well a few tips for\ndebugging, a little history, and a few instances where manual tuning is\nbeneficial. It covers best practices that are relevant to a variety of hardware\nand models.\nInput pipeline\nThe input pipeline extracts data from a location, transforms it, and loads it\nonto an accelerator for processing. As accelerators become faster, it is\nimportant that the input pipeline keeps up with the demand. The `tf.data` API is\ndesigned with flexibility, ease of use, and performance in mind. For using and\nmaximizing performance with the `tf.data` API, see the\ndata input pipeline guide.\nReading large numbers of small files significantly impacts I/O performance.\nOne approach to get maximum I/O throughput is to preprocess input data into\nlarger (~100MB) `TFRecord` files. For smaller data sets (200MB-1GB), the best\napproach is often to load the entire data set into memory. The document\nDownloading and converting to TFRecord format\nincludes information and scripts for creating `TFRecord`s, and this\nscript\nconverts the CIFAR-10 dataset into `TFRecord`s.\nWhile feeding data using a `feed_dict` offers a high level of flexibility, in\ngeneral, `feed_dict` does not provide a scalable solution. Avoid using `feed_dict`\nfor all but trivial examples.\n```python\nfeed_dict often results in suboptimal performance.\nsess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n```\nRNN Performance\nThere are many ways to specify an RNN computation in TensorFlow and they have\ntrade-offs with respect to model flexibility and performance. The\n`tf.nn.rnn_cell.BasicLSTMCell` should be considered a reference implementation\nand used only as a last resort when no other options will work.\nWhen using one of the cells, rather than the fully fused RNN layers, you have a\nchoice of whether to use `tf.nn.static_rnn` or `tf.nn.dynamic_rnn`. There\nshouldn't generally be a performance difference at runtime, but large unroll\namounts can increase the graph size of the `tf.nn.static_rnn` and cause long\ncompile times. An additional advantage of `tf.nn.dynamic_rnn` is that it can\noptionally swap memory from the GPU to the CPU to enable training of very long\nsequences. Depending on the model and hardware configuration, this can come at\na performance cost. It is also possible to run multiple iterations of\n`tf.nn.dynamic_rnn` and the underlying `tf.while_loop` construct in parallel,\nalthough this is rarely useful with RNN models as they are inherently\nsequential.\nOn NVIDIA GPUs, the use of `tf.contrib.cudnn_rnn` should always be preferred\nunless you want layer normalization, which it doesn't support. It is often at\nleast an order of magnitude faster than `tf.contrib.rnn.BasicLSTMCell` and\n`tf.contrib.rnn.LSTMBlockCell` and uses 3-4x less memory than\n`tf.contrib.rnn.BasicLSTMCell`.\nIf you need to run one step of the RNN at a time, as might be the case in\nreinforcement learning with a recurrent policy, then you should use the\n`tf.contrib.rnn.LSTMBlockCell` with your own environment interaction loop\ninside a `tf.while_loop` construct. Running one step of the RNN at a time and\nreturning to Python is possible, but it will be slower.\nOn CPUs, mobile devices, and if `tf.contrib.cudnn_rnn` is not available on\nyour GPU, the fastest and most memory efficient option is\n`tf.contrib.rnn.LSTMBlockFusedCell`.\nFor all of the less common cell types like `tf.contrib.rnn.NASCell`,\n`tf.contrib.rnn.PhasedLSTMCell`, `tf.contrib.rnn.UGRNNCell`,\n`tf.contrib.rnn.GLSTMCell`, `tf.contrib.rnn.Conv1DLSTMCell`,\n`tf.contrib.rnn.Conv2DLSTMCell`, `tf.contrib.rnn.LayerNormBasicLSTMCell`,\netc., be aware that they are implemented in the graph like\n`tf.contrib.rnn.BasicLSTMCell` and will suffer from the same poor\nperformance and high memory usage. Consider whether or not those\ntrade-offs are worth it before using these cells. For example, while layer\nnormalization can speed up convergence, because cuDNN is 20x faster, the fastest\nwall clock time to convergence is usually obtained without it.\nManual tuning\nOptimizing for CPU\nCPUs, which includes Intel\u00ae Xeon Phi\u2122, achieve optimal performance when\nTensorFlow is built from source with all of the\ninstructions supported by the target CPU.\nBeyond using the latest instruction sets, Intel\u00ae has added support for the\nIntel\u00ae Math Kernel Library for Deep Neural Networks (Intel\u00ae MKL-DNN) to\nTensorFlow. While the name is not completely accurate, these optimizations are\noften simply referred to as MKL or TensorFlow with MKL.\nTensorFlow with Intel\u00ae MKL-DNN contains details\nabout the MKL optimizations.\nThe two configurations listed below are used to optimize CPU performance by\nadjusting the thread pools.\n\n`intra_op_parallelism_threads`: Nodes that can use multiple threads to\n  parallelize their execution will schedule the individual pieces into this pool.\n`inter_op_parallelism_threads`: All ready nodes are scheduled in this pool.\n\nThese configurations are set using the `tf.ConfigProto` and passed to `tf.Session`\nin the `config` attribute as shown in the snippet below. For both configuration\noptions, if they are unset or set to 0, will default to the number of logical\nCPU cores. Testing has shown that the default is effective for systems ranging\nfrom one CPU with 4 cores to multiple CPUs with 70+ combined logical cores.\nA common alternative optimization is to set the number of threads in both pools\nequal to the number of physical cores rather than logical cores.\n`python\nconfig = tf.ConfigProto()\nconfig.intra_op_parallelism_threads = 44\nconfig.inter_op_parallelism_threads = 44\ntf.Session(config=config)`\nTensorFlow with Intel\u00ae MKL DNN\nIntel\u00ae has added optimizations to TensorFlow for Intel\u00ae Xeon\u00ae and Intel\u00ae Xeon\nPhi\u2122 through the use of the Intel\u00ae Math Kernel Library for Deep Neural Networks\n(Intel\u00ae MKL-DNN) optimized primitives. The optimizations also provide speedups\nfor the consumer line of processors, e.g. i5 and i7 Intel processors. The Intel\npublished paper\nTensorFlow* Optimizations on Modern Intel\u00ae Architecture\ncontains additional details on the implementation.\nNote: MKL was added as of TensorFlow 1.2 and currently only works on Linux. It\nalso does not work when also using `--config=cuda`.\nIn addition to providing significant performance improvements for training CNN\nbased models, compiling with the MKL creates a binary that is optimized for AVX\nand AVX2. The result is a single binary that is optimized and compatible with\nmost modern (post-2011) processors.\nTensorFlow can be compiled with the MKL optimizations using the following\ncommands that depend on the version of the TensorFlow source used.\nFor TensorFlow source versions after 1.3.0:\n```bash\n./configure\nPick the desired options\nbazel build --config=mkl --config=opt //tensorflow/tools/pip_package:build_pip_package\n```\nFor TensorFlow versions 1.2.0 through 1.3.0:\n```bash\n./configure\nDo you wish to build TensorFlow with MKL support? [y/N] Y\nDo you wish to download MKL LIB from the web? [Y/n] Y\nSelect the defaults for the rest of the options.\nbazel build --config=mkl --copt=\"-DEIGEN_USE_VML\" -c opt //tensorflow/tools/pip_package:build_pip_package\n```\nTuning MKL for the best performance\nThis section details the different configurations and environment variables that\ncan be used to tune the MKL to get optimal performance. Before tweaking various\nenvironment variables make sure the model is using the `NCHW` (`channels_first`)\ndata format. The MKL is optimized for `NCHW` and Intel is\nworking to get near performance parity when using `NHWC`.\nMKL uses the following environment variables to tune performance:\n\n`KMP_BLOCKTIME` - Sets the time, in milliseconds, that a thread should wait,\n  after completing the execution of a parallel region, before sleeping.\n`KMP_AFFINITY` - Enables the run-time library to bind threads to physical\n  processing units.\n`KMP_SETTINGS` - Enables (true) or disables (false) the printing of OpenMP*\n  run-time library environment variables during program execution.\n`OMP_NUM_THREADS` - Specifies the number of threads to use.\n\nMore details on the KMP variables are on\nIntel's site and the OMP\nvariables on\ngnu.org\nWhile there can be substantial gains from adjusting the environment variables,\nwhich is discussed below, the simplified advice is to set the\n`inter_op_parallelism_threads` equal to the number of physical CPUs and to set\nthe following environment variables:\n\n`KMP_BLOCKTIME=0`\n`KMP_AFFINITY=granularity=fine,verbose,compact,1,0`\n\nExample setting MKL variables with command-line arguments:\n`bash\nKMP_BLOCKTIME=0 KMP_AFFINITY=granularity=fine,verbose,compact,1,0 \\\nKMP_SETTINGS=1 python your_python_script.py`\nExample setting MKL variables with python `os.environ`:\n`python\nos.environ[\"KMP_BLOCKTIME\"] = str(FLAGS.kmp_blocktime)\nos.environ[\"KMP_SETTINGS\"] = str(FLAGS.kmp_settings)\nos.environ[\"KMP_AFFINITY\"]= FLAGS.kmp_affinity\nif FLAGS.num_intra_threads > 0:\n  os.environ[\"OMP_NUM_THREADS\"]= str(FLAGS.num_intra_threads)`\nThere are models and hardware platforms that benefit from different settings.\nEach variable that impacts performance is discussed below.\n\n`KMP_BLOCKTIME`: The MKL default is 200ms, which was not optimal in our\n  testing. 0 (0ms) was a good default for CNN based models that were tested.\n  The best performance for AlexNet was achieved at 30ms and both GoogleNet and\n  VGG11 performed best set at 1ms.\n`KMP_AFFINITY`: The recommended setting is `granularity=fine,verbose,compact,1,0`.\n`OMP_NUM_THREADS`: This defaults to the number of physical cores.\n  Adjusting this parameter beyond matching the number of cores can have an\n  impact when using Intel\u00ae Xeon Phi\u2122 (Knights Landing) for some models. See\n  TensorFlow* Optimizations on Modern Intel\u00ae Architecture\n  for optimal settings.\n`intra_op_parallelism_threads`: Setting this equal to the number of\n  physical cores is recommended. Setting the value to 0, which is the default,\n  results in the value being set to the number of logical cores - this is an\n  alternate option to try for some architectures. This value and `OMP_NUM_THREADS`\n  should be equal.\n`inter_op_parallelism_threads`: Setting this equal to the number of\n  sockets is recommended. Setting the value to 0, which is the default,\n  results in the value being set to the number of logical cores.\n\nBuilding and installing from source\nThe default TensorFlow binaries target the broadest range of hardware to make\nTensorFlow accessible to everyone. If using CPUs for training or inference, it\nis recommended to compile TensorFlow with all of the optimizations available for\nthe CPU in use.\nTo install the most optimized version of TensorFlow, build and install\nfrom source. If there is a need to build TensorFlow\non a platform that has different hardware than the target, then cross-compile\nwith the highest optimizations for the target platform. The following command is\nan example of using `bazel` to compile for a specific platform:\n```python\nThis command optimizes for Intel\u2019s Broadwell processor\nbazel build -c opt --copt=-march=\"broadwell\" --config=cuda //tensorflow/tools/pip_package:build_pip_package\n```\nEnvironment, build, and install tips\n\n`./configure` asks which compute capability to include in the build. This\n  does not impact overall performance but does impact initial startup. After\n  running TensorFlow once, the compiled kernels are cached by CUDA. If using\n  a docker container, the data is not cached and the penalty is paid each time\n  TensorFlow starts. The best practice is to include the\n  compute capabilities\n  of the GPUs that will be used, e.g. P100: 6.0, Titan X (Pascal): 6.1,\n  Titan X (Maxwell): 5.2, and K80: 3.7.\nUse a version of `gcc` that supports all of the optimizations of the target\n  CPU. The recommended minimum gcc version is 4.8.3. On macOS, upgrade to the\n  latest Xcode version and use the version of clang that comes with Xcode.\nInstall the latest stable CUDA platform and cuDNN libraries supported by\n  TensorFlow.\n\nHistory\nData formats\nData formats refers to the structure of the Tensor passed to a given op. The\ndiscussion below is specifically about 4D Tensors representing images. In\nTensorFlow the parts of the 4D tensor are often referred to by the following\nletters:\n\nN refers to the number of images in a batch.\nH refers to the number of pixels in the vertical (height) dimension.\nW refers to the number of pixels in the horizontal (width) dimension.\nC refers to the channels. For example, 1 for black and white or grayscale\n  and 3 for RGB.\n\nWithin TensorFlow there are two naming conventions representing the two most\ncommon data formats:\n\n`NCHW` or `channels_first`\n`NHWC` or `channels_last`\n\n`NHWC` is the TensorFlow default and `NCHW` is the optimal format to use when\ntraining on NVIDIA GPUs using cuDNN.\nThe best practice is to build models that work with both data formats. This\nsimplifies training on GPUs and then running inference on CPUs. If TensorFlow is\ncompiled with the Intel MKL optimizations,\nmany operations, especially those related to CNN based models, will be optimized\nand support `NCHW`. If not using the MKL, some operations are not supported on\nCPU when using `NCHW`.\nThe brief history of these two formats is that TensorFlow started by using\n`NHWC` because it was a little faster on CPUs. In the long term, we are working\non tools to auto rewrite graphs to make switching between the formats\ntransparent and take advantages of micro optimizations where a GPU op may be\nfaster using `NHWC` than the normally most efficient `NCHW`.\nDebugging\nDebug input pipeline optimization\nTypical models retrieve data from disk and preprocess it before sending the data\nthrough the network. For example, models that process JPEG images will follow\nthis flow: load image from disk, decode JPEG into a tensor, crop and pad,\npossibly flip and distort, and then batch. This flow is referred to as the input\npipeline. As GPUs and other hardware accelerators get faster, preprocessing of\ndata can be a bottleneck.\nDetermining if the input pipeline is the bottleneck can be complicated. One of\nthe most straightforward methods is to reduce the model to a single operation\n(trivial model) after the input pipeline and measure the examples per second. If\nthe difference in examples per second for the full model and the trivial model\nis minimal then the input pipeline is likely a bottleneck. Below are some other\napproaches to identifying issues:\n\nCheck if a GPU is underutilized by running `nvidia-smi -l 2`. If GPU\n  utilization is not approaching 80-100%, then the input pipeline may be the\n  bottleneck.\nGenerate a timeline and look for large blocks of white space (waiting). An\n  example of generating a timeline exists as part of the\n  XLA jit tutorial.\nCheck CPU usage. It is possible to have an optimized input pipeline and lack\n  the CPU cycles to process the pipeline.\nEstimate the throughput needed and verify the disk used is capable of that\n  level of throughput. Some cloud solutions have network attached disks that\n  start as low as 50 MB/sec, which is slower than spinning disks (150 MB/sec),\n  SATA SSDs (500 MB/sec), and PCIe SSDs (2,000+ MB/sec).\n\nPreprocessing on the CPU\nPlacing input pipeline operations on the CPU can significantly improve\nperformance. Utilizing the CPU for the input pipeline frees the GPU to focus on\ntraining. To ensure preprocessing is on the CPU, wrap the preprocessing\noperations as shown below:\n`python\nwith tf.device('/cpu:0'):\n  # function to get and process images or data.\n  distorted_inputs = load_and_distort_images()`\nIf using `tf.estimator.Estimator` the input function is automatically placed on",
    "tag": "tensorflow"
  },
  {
    "title": "Benchmarks",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/guide/performance/benchmarks.md",
    "content": "Benchmarks\nOverview\nA selection of image classification models were tested across multiple platforms\nto create a point of reference for the TensorFlow community. The\nMethodology section details how the tests were executed and has\nlinks to the scripts used.\nResults for image classification models\nInceptionV3 (arXiv:1512.00567), ResNet-50\n(arXiv:1512.03385), ResNet-152\n(arXiv:1512.03385), VGG16\n(arXiv:1409.1556), and\nAlexNet\nwere tested using the ImageNet data set. Tests were\nrun on Google Compute Engine, Amazon Elastic Compute Cloud (Amazon EC2), and an\nNVIDIA\u00ae DGX-1\u2122. Most of the tests were run with both synthetic and real data.\nTesting with synthetic data was done by using a `tf.Variable` set to the same\nshape as the data expected by each model for ImageNet. We believe it is\nimportant to include real data measurements when benchmarking a platform. This\nload tests both the underlying hardware and the framework at preparing data for\nactual training. We start with synthetic data to remove disk I/O as a variable\nand to set a baseline. Real data is then used to verify that the TensorFlow\ninput pipeline and the underlying disk I/O are saturating the compute units.\nTraining with NVIDIA\u00ae DGX-1\u2122 (NVIDIA\u00ae Tesla\u00ae P100)\n\n\n\nDetails and additional results are in the Details for NVIDIA\u00ae DGX-1\u2122 (NVIDIA\u00ae\nTesla\u00ae P100) section.\nTraining with NVIDIA\u00ae Tesla\u00ae K80\n\n\n\nDetails and additional results are in the Details for Google Compute Engine\n(NVIDIA\u00ae Tesla\u00ae K80) and\nDetails for Amazon EC2 (NVIDIA\u00ae Tesla\u00ae\nK80) sections.\nDistributed training with NVIDIA\u00ae Tesla\u00ae K80\n\n\n\nDetails and additional results are in the Details for Amazon EC2 Distributed\n(NVIDIA\u00ae Tesla\u00ae K80)\nsection.\nCompare synthetic with real data training\nNVIDIA\u00ae Tesla\u00ae P100\n\n\n\n\nNVIDIA\u00ae Tesla\u00ae K80\n\n\n\n\nDetails for NVIDIA\u00ae DGX-1\u2122 (NVIDIA\u00ae Tesla\u00ae P100)\nEnvironment\n\nInstance type: NVIDIA\u00ae DGX-1\u2122\nGPU: 8x NVIDIA\u00ae Tesla\u00ae P100\nOS: Ubuntu 16.04 LTS with tests run via Docker\nCUDA / cuDNN: 8.0 / 5.1\nTensorFlow GitHub hash: b1e174e\nBenchmark GitHub hash: 9165a70\nBuild Command: `bazel build -c opt --copt=-march=\"haswell\" --config=cuda\n    //tensorflow/tools/pip_package:build_pip_package`\nDisk: Local SSD\nDataSet: ImageNet\nTest Date: May 2017\n\nBatch size and optimizer used for each model are listed in the table below. In\naddition to the batch sizes listed in the table, InceptionV3, ResNet-50,\nResNet-152, and VGG16 were tested with a batch size of 32. Those results are in\nthe other results section.\nOptions            | InceptionV3 | ResNet-50 | ResNet-152 | AlexNet | VGG16\n------------------ | ----------- | --------- | ---------- | ------- | -----\nBatch size per GPU | 64          | 64        | 64         | 512     | 64\nOptimizer          | sgd         | sgd       | sgd        | sgd     | sgd\nConfiguration used for each model.\nModel       | variable_update        | local_parameter_device\n----------- | ---------------------- | ----------------------\nInceptionV3 | parameter_server       | cpu\nResNet50    | parameter_server       | cpu\nResNet152   | parameter_server       | cpu\nAlexNet     | replicated (with NCCL) | n/a\nVGG16       | replicated (with NCCL) | n/a\nResults\n\n\n\n\n\n\n\nTraining synthetic data\nGPUs | InceptionV3 | ResNet-50 | ResNet-152 | AlexNet | VGG16\n---- | ----------- | --------- | ---------- | ------- | -----\n1    | 142         | 219       | 91.8       | 2987    | 154\n2    | 284         | 422       | 181        | 5658    | 295\n4    | 569         | 852       | 356        | 10509   | 584\n8    | 1131        | 1734      | 716        | 17822   | 1081\nTraining real data\nGPUs | InceptionV3 | ResNet-50 | ResNet-152 | AlexNet | VGG16\n---- | ----------- | --------- | ---------- | ------- | -----\n1    | 142         | 218       | 91.4       | 2890    | 154\n2    | 278         | 425       | 179        | 4448    | 284\n4    | 551         | 853       | 359        | 7105    | 534\n8    | 1079        | 1630      | 708        | N/A     | 898\nTraining AlexNet with real data on 8 GPUs was excluded from the graph and table\nabove due to it maxing out the input pipeline.\nOther Results\nThe results below are all with a batch size of 32.\nTraining synthetic data\nGPUs | InceptionV3 | ResNet-50 | ResNet-152 | VGG16\n---- | ----------- | --------- | ---------- | -----\n1    | 128         | 195       | 82.7       | 144\n2    | 259         | 368       | 160        | 281\n4    | 520         | 768       | 317        | 549\n8    | 995         | 1485      | 632        | 820\nTraining real data\nGPUs | InceptionV3 | ResNet-50 | ResNet-152 | VGG16\n---- | ----------- | --------- | ---------- | -----\n1    | 130         | 193       | 82.4       | 144\n2    | 257         | 369       | 159        | 253\n4    | 507         | 760       | 317        | 457\n8    | 966         | 1410      | 609        | 690\nDetails for Google Compute Engine (NVIDIA\u00ae Tesla\u00ae K80)\nEnvironment\n\nInstance type: n1-standard-32-k80x8\nGPU: 8x NVIDIA\u00ae Tesla\u00ae K80\nOS: Ubuntu 16.04 LTS\nCUDA / cuDNN: 8.0 / 5.1\nTensorFlow GitHub hash: b1e174e\nBenchmark GitHub hash: 9165a70\nBuild Command: `bazel build -c opt --copt=-march=\"haswell\" --config=cuda\n    //tensorflow/tools/pip_package:build_pip_package`\nDisk: 1.7 TB Shared SSD persistent disk (800 MB/s)\nDataSet: ImageNet\nTest Date: May 2017\n\nBatch size and optimizer used for each model are listed in the table below. In\naddition to the batch sizes listed in the table, InceptionV3 and ResNet-50 were\ntested with a batch size of 32. Those results are in the other results\nsection.\nOptions            | InceptionV3 | ResNet-50 | ResNet-152 | AlexNet | VGG16\n------------------ | ----------- | --------- | ---------- | ------- | -----\nBatch size per GPU | 64          | 64        | 32         | 512     | 32\nOptimizer          | sgd         | sgd       | sgd        | sgd     | sgd\nThe configuration used for each model was `variable_update` equal to\n`parameter_server` and `local_parameter_device` equal to `cpu`.\nResults\n\n\n\n\nTraining synthetic data\nGPUs | InceptionV3 | ResNet-50 | ResNet-152 | AlexNet | VGG16\n---- | ----------- | --------- | ---------- | ------- | -----\n1    | 30.5        | 51.9      | 20.0       | 656     | 35.4\n2    | 57.8        | 99.0      | 38.2       | 1209    | 64.8\n4    | 116         | 195       | 75.8       | 2328    | 120\n8    | 227         | 387       | 148        | 4640    | 234\nTraining real data\nGPUs | InceptionV3 | ResNet-50 | ResNet-152 | AlexNet | VGG16\n---- | ----------- | --------- | ---------- | ------- | -----\n1    | 30.6        | 51.2      | 20.0       | 639     | 34.2\n2    | 58.4        | 98.8      | 38.3       | 1136    | 62.9\n4    | 115         | 194       | 75.4       | 2067    | 118\n8    | 225         | 381       | 148        | 4056    | 230\nOther Results\nTraining synthetic data\nGPUs | InceptionV3 (batch size 32) | ResNet-50 (batch size 32)\n---- | --------------------------- | -------------------------\n1    | 29.3                        | 49.5\n2    | 55.0                        | 95.4\n4    | 109                         | 183\n8    | 216                         | 362\nTraining real data\nGPUs | InceptionV3 (batch size 32) | ResNet-50 (batch size 32)\n---- | --------------------------- | -------------------------\n1    | 29.5                        | 49.3\n2    | 55.4                        | 95.3\n4    | 110                         | 186\n8    | 216                         | 359\nDetails for Amazon EC2 (NVIDIA\u00ae Tesla\u00ae K80)\nEnvironment\n\nInstance type: p2.8xlarge\nGPU: 8x NVIDIA\u00ae Tesla\u00ae K80\nOS: Ubuntu 16.04 LTS\nCUDA / cuDNN: 8.0 / 5.1\nTensorFlow GitHub hash: b1e174e\nBenchmark GitHub hash: 9165a70\nBuild Command: `bazel build -c opt --copt=-march=\"haswell\" --config=cuda\n    //tensorflow/tools/pip_package:build_pip_package`\nDisk: 1TB Amazon EFS (burst 100 MiB/sec for 12 hours, continuous 50\n    MiB/sec)\nDataSet: ImageNet\nTest Date: May 2017\n\nBatch size and optimizer used for each model are listed in the table below. In\naddition to the batch sizes listed in the table, InceptionV3 and ResNet-50 were\ntested with a batch size of 32. Those results are in the other results\nsection.\nOptions            | InceptionV3 | ResNet-50 | ResNet-152 | AlexNet | VGG16\n------------------ | ----------- | --------- | ---------- | ------- | -----\nBatch size per GPU | 64          | 64        | 32         | 512     | 32\nOptimizer          | sgd         | sgd       | sgd        | sgd     | sgd\nConfiguration used for each model.\nModel       | variable_update           | local_parameter_device\n----------- | ------------------------- | ----------------------\nInceptionV3 | parameter_server          | cpu\nResNet-50   | replicated (without NCCL) | gpu\nResNet-152  | replicated (without NCCL) | gpu\nAlexNet     | parameter_server          | gpu\nVGG16       | parameter_server          | gpu\nResults\n\n\n\n\nTraining synthetic data\nGPUs | InceptionV3 | ResNet-50 | ResNet-152 | AlexNet | VGG16\n---- | ----------- | --------- | ---------- | ------- | -----\n1    | 30.8        | 51.5      | 19.7       | 684     | 36.3\n2    | 58.7        | 98.0      | 37.6       | 1244    | 69.4\n4    | 117         | 195       | 74.9       | 2479    | 141\n8    | 230         | 384       | 149        | 4853    | 260\nTraining real data\nGPUs | InceptionV3 | ResNet-50 | ResNet-152 | AlexNet | VGG16\n---- | ----------- | --------- | ---------- | ------- | -----\n1    | 30.5        | 51.3      | 19.7       | 674     | 36.3\n2    | 59.0        | 94.9      | 38.2       | 1227    | 67.5\n4    | 118         | 188       | 75.2       | 2201    | 136\n8    | 228         | 373       | 149        | N/A     | 242\nTraining AlexNet with real data on 8 GPUs was excluded from the graph and table\nabove due to our EFS setup not providing enough throughput.\nOther Results\nTraining synthetic data\nGPUs | InceptionV3 (batch size 32) | ResNet-50 (batch size 32)\n---- | --------------------------- | -------------------------\n1    | 29.9                        | 49.0\n2    | 57.5                        | 94.1\n4    | 114                         | 184\n8    | 216                         | 355\nTraining real data\nGPUs | InceptionV3 (batch size 32) | ResNet-50 (batch size 32)\n---- | --------------------------- | -------------------------\n1    | 30.0                        | 49.1\n2    | 57.5                        | 95.1\n4    | 113                         | 185\n8    | 212                         | 353\nDetails for Amazon EC2 Distributed (NVIDIA\u00ae Tesla\u00ae K80)\nEnvironment\n\nInstance type: p2.8xlarge\nGPU: 8x NVIDIA\u00ae Tesla\u00ae K80\nOS: Ubuntu 16.04 LTS\nCUDA / cuDNN: 8.0 / 5.1\nTensorFlow GitHub hash: b1e174e\nBenchmark GitHub hash: 9165a70\nBuild Command: `bazel build -c opt --copt=-march=\"haswell\" --config=cuda\n    //tensorflow/tools/pip_package:build_pip_package`\nDisk: 1.0 TB EFS (burst 100 MB/sec for 12 hours, continuous 50 MB/sec)\nDataSet: ImageNet\nTest Date: May 2017\n\nThe batch size and optimizer used for the tests are listed in the table. In\naddition to the batch sizes listed in the table, InceptionV3 and ResNet-50 were\ntested with a batch size of 32. Those results are in the other results\nsection.\nOptions            | InceptionV3 | ResNet-50 | ResNet-152\n------------------ | ----------- | --------- | ----------\nBatch size per GPU | 64          | 64        | 32\nOptimizer          | sgd         | sgd       | sgd\nConfiguration used for each model.\nModel       | variable_update        | local_parameter_device | cross_replica_sync\n----------- | ---------------------- | ---------------------- | ------------------\nInceptionV3 | distributed_replicated | n/a                    | True\nResNet-50   | distributed_replicated | n/a                    | True\nResNet-152  | distributed_replicated | n/a                    | True\nTo simplify server setup, EC2 instances (p2.8xlarge) running worker servers also\nran parameter servers. Equal numbers of parameter servers and worker servers were\nused with the following exceptions:\n\nInceptionV3: 8 instances / 6 parameter servers\nResNet-50: (batch size 32) 8 instances / 4 parameter servers\nResNet-152: 8 instances / 4 parameter servers\n\nResults\n\n\n\n\n\n\nTraining synthetic data\nGPUs | InceptionV3 | ResNet-50 | ResNet-152\n---- | ----------- | --------- | ----------\n1    | 29.7        | 52.4      | 19.4\n8    | 229         | 378       | 146\n16   | 459         | 751       | 291\n32   | 902         | 1388      | 565\n64   | 1783        | 2744      | 981\nOther Results\n\n\n\nTraining synthetic data\nGPUs | InceptionV3 (batch size 32) | ResNet-50 (batch size 32)\n---- | --------------------------- | -------------------------\n1    | 29.2                        | 48.4\n8    | 219                         | 333\n16   | 427                         | 667\n32   | 820                         | 1180\n64   | 1608                        | 2315\nMethodology\nThis\nscript\nwas run on the various platforms to generate the above results.\nIn order to create results that are as repeatable as possible, each test was run\n5 times and then the times were averaged together. GPUs are run in their default\nstate on the given platform. For NVIDIA\u00ae Tesla\u00ae K80 this means leaving on GPU\nBoost.\nFor each test, 10 warmup steps are done and then the next 100 steps are",
    "tag": "tensorflow"
  },
  {
    "title": "Data Input Pipeline Performance",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/guide/performance/datasets.md",
    "content": "Data Input Pipeline Performance\nGPUs and TPUs can radically reduce the time required to execute a single\ntraining step. Achieving peak performance requires an efficient input pipeline\nthat delivers data for the next step before the current step has finished. The\n`tf.data` API helps to build flexible and efficient input pipelines. This\ndocument explains the `tf.data` API's features and best practices for building\nhigh performance TensorFlow input pipelines across a variety of models and\naccelerators.\nThis guide does the following:\n\nIllustrates that TensorFlow input pipelines are essentially an\n    ETL process.\nDescribes common performance optimizations in the context of the `tf.data`\n    API.\nDiscusses the performance implications of the order in which you apply\n    transformations.\nSummarizes the best practices for designing performant TensorFlow input\n    pipelines.\n\nInput Pipeline Structure\nA typical TensorFlow training input pipeline can be framed as an ETL process:\n\nExtract: Read data from persistent storage -- either local (e.g. HDD or\n    SSD) or remote (e.g. GCS or\n    HDFS).\nTransform: Use CPU cores to parse and perform preprocessing operations\n    on the data such as image decompression, data augmentation transformations\n    (such as random crop, flips, and color distortions), shuffling, and batching.\nLoad: Load the transformed data onto the accelerator device(s) (for\n    example, GPU(s) or TPU(s)) that execute the machine learning model.\n\nThis pattern effectively utilizes the CPU, while reserving the accelerator for\nthe heavy lifting of training your model. In addition, viewing input pipelines\nas an ETL process provides structure that facilitates the application of\nperformance optimizations.\nWhen using the `tf.estimator.Estimator` API, the first two phases (Extract and\nTransform) are captured in the `input_fn` passed to\n`tf.estimator.Estimator.train`. In code, this might look like the following\n(naive, sequential) implementation:\n```\ndef parse_fn(example):\n  \"Parse TFExample records and perform simple data augmentation.\"\n  example_fmt = {\n    \"image\": tf.FixedLengthFeature((), tf.string, \"\"),\n    \"label\": tf.FixedLengthFeature((), tf.int64, -1)\n  }\n  parsed = tf.parse_single_example(example, example_fmt)\n  image = tf.image.decode_image(parsed[\"image\"])\n  image = _augment_helper(image)  # augments image using slice, reshape, resize_bilinear\n  return image, parsed[\"label\"]\ndef input_fn():\n  files = tf.data.Dataset.list_files(\"/path/to/dataset/train-*.tfrecord\")\n  dataset = files.interleave(tf.data.TFRecordDataset)\n  dataset = dataset.shuffle(buffer_size=FLAGS.shuffle_buffer_size)\n  dataset = dataset.map(map_func=parse_fn)\n  dataset = dataset.batch(batch_size=FLAGS.batch_size)\n  return dataset\n```\nThe next section builds on this input pipeline, adding performance\noptimizations.\nOptimizing Performance\nAs new computing devices (such as GPUs and TPUs) make it possible to train\nneural networks at an increasingly fast rate, the CPU processing is prone to\nbecoming the bottleneck. The `tf.data` API provides users with building blocks\nto design input pipelines that effectively utilize the CPU, optimizing each step\nof the ETL process.\nPipelining\nTo perform a training step, you must first extract and transform the training\ndata and then feed it to a model running on an accelerator. However, in a naive\nsynchronous implementation, while the CPU is preparing the data, the accelerator\nis sitting idle. Conversely, while the accelerator is training the model, the\nCPU is sitting idle. The training step time is thus the sum of both CPU\npre-processing time and the accelerator training time.\nPipelining overlaps the preprocessing and model execution of a training\nstep. While the accelerator is performing training step `N`, the CPU is\npreparing the data for step `N+1`. Doing so reduces the step time to the maximum\n(as opposed to the sum) of the training and the time it takes to extract and\ntransform the data.\nWithout pipelining, the CPU and the GPU/TPU sit idle much of the time:\n\nWith pipelining, idle time diminishes significantly:\n\nThe `tf.data` API provides a software pipelining mechanism through the\n`tf.data.Dataset.prefetch` transformation, which can be used to decouple the\ntime data is produced from the time it is consumed. In particular, the\ntransformation uses a background thread and an internal buffer to prefetch\nelements from the input dataset ahead of the time they are requested. Thus, to\nachieve the pipelining effect illustrated above, you can add `prefetch(1)` as\nthe final transformation to your dataset pipeline (or `prefetch(n)` if a single\ntraining step consumes n elements).\nTo apply this change to our running example, change:\n`dataset = dataset.batch(batch_size=FLAGS.batch_size)\nreturn dataset`\nto:\n`dataset = dataset.batch(batch_size=FLAGS.batch_size)\ndataset = dataset.prefetch(buffer_size=FLAGS.prefetch_buffer_size)\nreturn dataset`\nNote that the prefetch transformation will yield benefits any time there is an\nopportunity to overlap the work of a \"producer\" with the work of a \"consumer.\"\nThe preceding recommendation is simply the most common application.\nParallelize Data Transformation\nWhen preparing a batch, input elements may need to be pre-processed. To this\nend, the `tf.data` API offers the `tf.data.Dataset.map` transformation, which\napplies a user-defined function (for example, `parse_fn` from the running\nexample) to each element of the input dataset. Because input elements are\nindependent of one another, the pre-processing can be parallelized across\nmultiple CPU cores. To make this possible, the `map` transformation provides the\n`num_parallel_calls` argument to specify the level of parallelism. For example,\nthe following diagram illustrates the effect of setting `num_parallel_calls=2`\nto the `map` transformation:\n\nChoosing the best value for the `num_parallel_calls` argument depends on your\nhardware, characteristics of your training data (such as its size and shape),\nthe cost of your map function, and what other processing is happening on the\nCPU at the same time; a simple heuristic is to use the number of available CPU\ncores. For instance, if the machine executing the example above had 4 cores, it\nwould have been more efficient to set `num_parallel_calls=4`. On the other hand,\nsetting `num_parallel_calls` to a value much greater than the number of\navailable CPUs can lead to inefficient scheduling, resulting in a slowdown.\nTo apply this change to our running example, change:\n`dataset = dataset.map(map_func=parse_fn)`\nto:\n`dataset = dataset.map(map_func=parse_fn, num_parallel_calls=FLAGS.num_parallel_calls)`\nFurthermore, if your batch size is in the hundreds or thousands, your pipeline\nwill likely additionally benefit from parallelizing the batch creation. To this\nend, the `tf.data` API provides the `tf.data.experimental.map_and_batch`\ntransformation, which effectively \"fuses\" the map and batch transformations.\nTo apply this change to our running example, change:\n`dataset = dataset.map(map_func=parse_fn, num_parallel_calls=FLAGS.num_parallel_calls)\ndataset = dataset.batch(batch_size=FLAGS.batch_size)`\nto:\n`dataset = dataset.apply(tf.contrib.data.map_and_batch(\n    map_func=parse_fn, batch_size=FLAGS.batch_size))`\nParallelize Data Extraction\nIn a real-world setting, the input data may be stored remotely (for example,\nGCS or HDFS), either because the input data would not fit locally or because the\ntraining is distributed and it would not make sense to replicate the input data\non every machine. A dataset pipeline that works well when reading data locally\nmight become bottlenecked on I/O when reading data remotely because of the\nfollowing differences between local and remote storage:\n\nTime-to-first-byte: Reading the first byte of a file from remote storage\n    can take orders of magnitude longer than from local storage.\nRead throughput: While remote storage typically offers large aggregate\n    bandwidth, reading a single file might only be able to utilize a small\n    fraction of this bandwidth.\n\nIn addition, once the raw bytes are read into memory, it may also be necessary\nto deserialize or decrypt the data\n(e.g. protobuf), which adds\nadditional overhead. This overhead is present irrespective of whether the data\nis stored locally or remotely, but can be worse in the remote case if data is\nnot prefetched effectively.\nTo mitigate the impact of the various data extraction overheads, the `tf.data`\nAPI offers the `tf.data.experimental.parallel_interleave` transformation. Use this\ntransformation to parallelize the execution of and interleave the contents of\nother datasets (such as data file readers). The\nnumber of datasets to overlap can be specified by the `cycle_length` argument.\nThe following diagram illustrates the effect of supplying `cycle_length=2` to\nthe `parallel_interleave` transformation:\n\nTo apply this change to our running example, change:\n`dataset = files.interleave(tf.data.TFRecordDataset)`\nto:\n`dataset = files.apply(tf.contrib.data.parallel_interleave(\n    tf.data.TFRecordDataset, cycle_length=FLAGS.num_parallel_readers))`\nThe throughput of remote storage systems can vary over time due to load or\nnetwork events. To account for this variance, the `parallel_interleave`\ntransformation can optionally use prefetching. (See\n`tf.data.experimental.parallel_interleave` for details).\nBy default, the `parallel_interleave` transformation provides a deterministic\nordering of elements to aid reproducibility. As an alternative to prefetching\n(which may be ineffective in some cases), the `parallel_interleave`\ntransformation also provides an option that can boost performance at the expense\nof ordering guarantees. In particular, if the `sloppy` argument is set to true,\nthe transformation may depart from its otherwise deterministic ordering, by\ntemporarily skipping over files whose elements are not available when the next\nelement is requested.\nPerformance Considerations\nThe `tf.data` API is designed around composable transformations to provide its\nusers with flexibility. Although many of these transformations are commutative,\nthe ordering of certain transformations has performance implications.\nMap and Batch\nInvoking the user-defined function passed into the `map` transformation has\noverhead related to scheduling and executing the user-defined function.\nNormally, this overhead is small compared to the amount of computation performed\nby the function. However, if `map` does little work, this overhead can dominate\nthe total cost. In such cases, we recommend vectorizing the user-defined\nfunction (that is, have it operate over a batch of inputs at once) and apply the\n`batch` transformation before the `map` transformation.\nMap and Cache\nThe `tf.data.Dataset.cache` transformation can cache a dataset, either in\nmemory or on local storage. If the user-defined function passed into the `map`\ntransformation is expensive, apply the cache transformation after the map\ntransformation as long as the resulting dataset can still fit into memory or\nlocal storage. If the user-defined function increases the space required to\nstore the dataset beyond the cache capacity, consider pre-processing your data\nbefore your training job to reduce resource usage.\nMap and Interleave / Prefetch / Shuffle\nA number of transformations, including `interleave`, `prefetch`, and `shuffle`,\nmaintain an internal buffer of elements. If the user-defined function passed\ninto the `map` transformation changes the size of the elements, then the\nordering of the map transformation and the transformations that buffer elements\naffects the memory usage. In general, we recommend choosing the order that\nresults in lower memory footprint, unless different ordering is desirable for\nperformance (for example, to enable fusing of the map and batch transformations).\nRepeat and Shuffle\nThe `tf.data.Dataset.repeat` transformation repeats the input data a finite (or\ninfinite) number of times; each repetition of the data is typically referred to\nas an epoch. The `tf.data.Dataset.shuffle` transformation randomizes the\norder of the dataset's examples.\nIf the `repeat` transformation is applied before the `shuffle` transformation,\nthen the epoch boundaries are blurred. That is, certain elements can be repeated\nbefore other elements appear even once. On the other hand, if the `shuffle`\ntransformation is applied before the repeat transformation, then performance\nmight slow down at the beginning of each epoch related to initialization of the\ninternal state of the `shuffle` transformation. In other words, the former\n(`repeat` before `shuffle`) provides better performance, while the latter\n(`shuffle` before `repeat`) provides stronger ordering guarantees.\nWhen possible, we recommend using the fused\n`tf.data.experimental.shuffle_and_repeat` transformation, which combines the best of\nboth worlds (good performance and strong ordering guarantees). Otherwise, we\nrecommend shuffling before repeating.\nSummary of Best Practices\nHere is a summary of the best practices for designing input pipelines:\n\nUse the `prefetch` transformation to overlap the work of a producer and\n    consumer. In particular, we recommend adding prefetch(n) (where n is the\n    number of elements / batches consumed by a training step) to the end of your\n    input pipeline to overlap the transformations performed on the CPU with the\n    training done on the accelerator.\nParallelize the `map` transformation by setting the `num_parallel_calls`\n    argument. We recommend using the number of available CPU cores for its value.\nIf you are combining pre-processed elements into a batch using the `batch`\n    transformation, we recommend using the fused `map_and_batch` transformation;\n    especially if you are using large batch sizes.\nIf you are working with data stored remotely and / or requiring\n    deserialization, we recommend using the `parallel_interleave`\n    transformation to overlap the reading (and deserialization) of data from\n    different files.\nVectorize cheap user-defined functions passed in to the `map` transformation\n    to amortize the overhead associated with scheduling and executing the\n    function.\nIf your data can fit into memory, use the `cache` transformation to cache it\n    in memory during the first epoch, so that subsequent epochs can avoid the\n    overhead associated with reading, parsing, and transforming it.\nIf your pre-processing increases the size of your data, we recommend\n    applying the `interleave`, `prefetch`, and `shuffle` first (if possible) to\n    reduce memory usage.\nWe recommend applying the `shuffle` transformation before the `repeat`\n",
    "tag": "tensorflow"
  },
  {
    "title": "TensorFlow 1.x tutorials (archived)",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/tutorials",
    "content": "TensorFlow 1.x tutorials (archived)\nNote: Please use the latest tutorials at https://www.tensorflow.org/tutorials\nTensorFlow is an open-source machine learning library for research and\nproduction. TensorFlow offers APIs for beginners and experts to develop for\ndesktop, mobile, web, and cloud. See the sections below to get started.\nLearn and use ML\nThe high-level Keras API provides building blocks to create and\ntrain deep learning models. Start with these beginner-friendly\nnotebook examples, then read the TensorFlow Keras guide.\n\nBasic classification\nText classification\nRegression\nOverfitting and underfitting\nSave and load\n\n```python\nimport tensorflow as tf\nmnist = tf.keras.datasets.mnist\n(x_train, y_train),(x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\n  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5)\nmodel.evaluate(x_test, y_test)\n```\nRun this code in\nGoogle's interactive notebook.\nResearch and experimentation\nEager execution provides an imperative, define-by-run interface for advanced\noperations. Write custom layers, forward passes, and training loops with\nauto\u2011differentiation. Start with these notebooks, then read the\neager execution guide.\n\nEager execution basics\nAutomatic differentiation and gradient tape\nCustom training: basics\nCustom layers\nCustom training: walkthrough\n\nML at production scale\nEstimators can train large models on multiple machines in a production\nenvironment. TensorFlow provides a collection of pre-made Estimators to\nimplement common ML algorithms. See the\nEstimators guide.\n\nBuild a linear model with Estimators\nBoosted trees\nGradient Boosted Trees: Model understanding\nBuild a Convolutional Neural Network using Estimators\n",
    "tag": "tensorflow"
  },
  {
    "title": "Learn and use machine learning",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/tutorials/keras",
    "content": "Learn and use machine learning\nThis notebook collection is inspired by the book\nDeep Learning with Python.\nThese tutorials use `tf.keras`, TensorFlow's high-level Python API for building\nand training deep learning models. To learn more about using Keras with\nTensorFlow, see the TensorFlow Keras Guide.\nPublisher's note: Deep Learning with Python introduces the field of deep\nlearning using the Python language and the powerful Keras library. Written by\nKeras creator and Google AI researcher Fran\u00e7ois Chollet, this book builds your\nunderstanding through intuitive explanations and practical examples.\nTo learn about machine learning fundamentals and concepts, consider taking the\nMachine Learning Crash Course.\n\nBasic classification\nText classification\nRegression\nOverfitting and underfitting\n",
    "tag": "tensorflow"
  },
  {
    "title": "Advanced Convolutional Neural Networks",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/tutorials/images/deep_cnn.md",
    "content": "Advanced Convolutional Neural Networks\nOverview\nCIFAR-10 classification is a common benchmark problem in machine learning.  The\nproblem is to classify RGB 32x32 pixel images across 10 categories:\n`airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck.`\nFor more details refer to the CIFAR-10 page\nand a Tech Report\nby Alex Krizhevsky.\nGoals\nThe goal of this tutorial is to build a relatively small convolutional neural\nnetwork (CNN) for\nrecognizing images. In the process, this tutorial:\n\n\nHighlights a canonical organization for network architecture,\n   training and evaluation.\n\n\nProvides a template for constructing larger and more sophisticated models.\n\n\nThe reason CIFAR-10 was selected was that it is complex enough to exercise\nmuch of TensorFlow's ability to scale to large models. At the same time,\nthe model is small enough to train fast, which is ideal for trying out\nnew ideas and experimenting with new techniques.\nHighlights of the Tutorial\nThe CIFAR-10 tutorial demonstrates several important constructs for\ndesigning larger and more sophisticated models in TensorFlow:\n\nCore mathematical components including `tf.nn.conv2d`\n(wiki),\n`tf.nn.relu`\n(wiki),\n`tf.nn.max_pool`\n(wiki)\nand `tf.nn.local_response_normalization`\n(section 3.3 in\nAlexNet paper).\nVisualization\nof network activities during training, including input images,\nlosses and distributions of activations and gradients.\nRoutines for calculating the\n`tf.train.ExponentialMovingAverage`\nof learned parameters and using these averages\nduring evaluation to boost predictive performance.\nImplementation of a\n`tf.train.exponential_decay`\nthat systematically decrements over time.\nPrefetching input data to isolate the model from disk latency and expensive\nimage pre-processing.\n\nWe also provide a multi-GPU version\nof the model which demonstrates:\n\nConfiguring a model to train across multiple GPU cards in parallel.\nSharing and updating variables among multiple GPUs.\n\nWe hope that this tutorial provides a launch point for building larger CNNs for\nvision tasks on TensorFlow.\nModel Architecture\nThe model in this CIFAR-10 tutorial is a multi-layer architecture consisting of\nalternating convolutions and nonlinearities. These layers are followed by fully\nconnected layers leading into a softmax classifier.  The model follows the\narchitecture described by\nAlex Krizhevsky, with a few\ndifferences in the top few layers.\nThis model achieves a peak performance of about 86% accuracy within a few hours\nof training time on a GPU. Please see below and the code\nfor details.  It consists of 1,068,298 learnable parameters and requires about\n19.5M multiply-add operations to compute inference on a single image.\nCode Organization\nThe code for this tutorial resides in\nmodels/tutorials/image/cifar10/.\nFile | Purpose\n--- | ---\ncifar10_input.py | Loads CIFAR-10 dataset using tensorflow-datasets library.\ncifar10.py | Builds the CIFAR-10 model.\ncifar10_train.py | Trains a CIFAR-10 model on a CPU or GPU.\ncifar10_multi_gpu_train.py | Trains a CIFAR-10 model on multiple GPUs.\ncifar10_eval.py | Evaluates the predictive performance of a CIFAR-10 model.\nTo run this tutorial, you will need to:\n`shell\npip install tensorflow-datasets`\nCIFAR-10 Model\nThe CIFAR-10 network is largely contained in\ncifar10.py.\nThe complete training\ngraph contains roughly 765 operations. We find that we can make the code most\nreusable by constructing the graph with the following modules:\n\nModel inputs: `inputs()` and `distorted_inputs()` add\noperations that read and preprocess CIFAR images for evaluation and training,\nrespectively.\nModel prediction: `inference()`\nadds operations that perform inference, i.e. classification, on supplied images.\nModel training: `loss()` and `train()`\nadd operations that compute the loss,\ngradients, variable updates and visualization summaries.\n\nModel Inputs\nThe input part of the model is built by the functions `_get_images_labels()` and\n`Transformer()`.\nThe images are processed as follows:\n\nThey are cropped to 24 x 24 pixels, centrally for evaluation or\n   `tf.random_crop` for training.\nThey are `tf.image.per_image_standardization`\n   to make the model insensitive to dynamic range.\n\nFor training, we additionally apply a series of random distortions to\nartificially increase the data set size:\n\n`tf.image.random_flip_left_right` the image from left to right.\nRandomly distort the image with `tf.image.random_brightness`.\nRandomly distort the image with `tf.image.random_contrast`.\n\nPlease see the Images page for the list of\navailable distortions. We also attach an\n`tf.summary.image` to the images\nso that we may visualize them in TensorBoard.\nThis is a good practice to verify that inputs are built correctly.\n\n\n\nReading images from disk and distorting them can use a non-trivial amount of\nprocessing time. To prevent these operations from slowing down training, we\napply the transformation in parallel (`num_parallel_calls` argument of\n`dataset.map()`), and `prefetch` the data.\nModel Prediction\nThe prediction part of the model is constructed by the `inference()` function\nwhich adds operations to compute the logits of the predictions. That part of\nthe model is organized as follows:\nLayer Name | Description\n--- | ---\n`conv1` | `tf.nn.conv2d` and `tf.nn.relu` activation.\n`pool1` | `tf.nn.max_pool`.\n`norm1` | `tf.nn.local_response_normalization`.\n`conv2` | `tf.nn.conv2d` and `tf.nn.relu` activation.\n`norm2` | `tf.nn.local_response_normalization`.\n`pool2` | `tf.nn.max_pool`.\n`local3` | fully connected layer with rectified linear activation.\n`local4` | fully connected layer with rectified linear activation.\n`softmax_linear` | linear transformation to produce logits.\nHere is a graph generated from TensorBoard describing the inference operation:\n\n\n\n\nEXERCISE: The output of `inference` are un-normalized logits. Try editing\nthe network architecture to return normalized predictions using\n`tf.nn.softmax`.\n\nThe `inputs()` and `inference()` functions provide all the components\nnecessary to perform an evaluation of a model. We now shift our focus towards\nbuilding operations for training a model.\n\nEXERCISE: The model architecture in `inference()` differs slightly from\nthe CIFAR-10 model specified in\ncuda-convnet.  In particular, the top\nlayers of Alex's original model are locally connected and not fully connected.\nTry editing the architecture to exactly reproduce the locally connected\narchitecture in the top layer.\n\nModel Training\nThe usual method for training a network to perform N-way classification is\nmultinomial logistic regression,\naka. softmax regression. Softmax regression applies a\n`tf.nn.softmax` nonlinearity to the\noutput of the network and calculates the\n`tf.nn.sparse_softmax_cross_entropy_with_logits`\nbetween the normalized predictions and the label index.\nFor regularization, we also apply the usual\n`tf.nn.l2_loss` losses to all learned\nvariables.  The objective function for the model is the sum of the cross entropy\nloss and all these weight decay terms, as returned by the `loss()` function.\nWe visualize it in TensorBoard with a `tf.summary.scalar`:\n\nWe train the model using standard\ngradient descent\nalgorithm (see Training for other methods)\nwith a learning rate that\n`tf.train.exponential_decay`\nover time.\n\nThe `train()` function adds the operations needed to minimize the objective by\ncalculating the gradient and updating the learned variables (see\n`tf.train.GradientDescentOptimizer`\nfor details).  It returns an operation that executes all the calculations\nneeded to train and update the model for one batch of images.\nLaunching and Training the Model\nWe have built the model, let's now launch it and run the training operation with\nthe script `cifar10_train.py`.\n`shell\npython cifar10_train.py`\n\nNOTE: The first time you run any target in the CIFAR-10 tutorial,\nthe CIFAR-10 dataset is automatically downloaded. The dataset is ~160MB\nso you may want to grab a quick cup of coffee for your first run.\n\nYou should see the output:\n`shell\n2015-11-04 11:45:45.927302: step 0, loss = 4.68 (2.0 examples/sec; 64.221 sec/batch)\n2015-11-04 11:45:49.133065: step 10, loss = 4.66 (533.8 examples/sec; 0.240 sec/batch)\n2015-11-04 11:45:51.397710: step 20, loss = 4.64 (597.4 examples/sec; 0.214 sec/batch)\n2015-11-04 11:45:54.446850: step 30, loss = 4.62 (391.0 examples/sec; 0.327 sec/batch)\n2015-11-04 11:45:57.152676: step 40, loss = 4.61 (430.2 examples/sec; 0.298 sec/batch)\n2015-11-04 11:46:00.437717: step 50, loss = 4.59 (406.4 examples/sec; 0.315 sec/batch)\n...`\nThe script reports the total loss every 10 steps as well as the speed at which\nthe last batch of data was processed. A few comments:\n\n\nThe first batch of data can be inordinately slow (e.g. several minutes) as the\npreprocessing threads fill up the shuffling queue with 20,000 processed CIFAR\nimages.\n\n\nThe reported loss is the average loss of the most recent batch. Remember that\nthis loss is the sum of the cross entropy and all weight decay terms.\n\n\nKeep an eye on the processing speed of a batch. The numbers shown above were\nobtained on a Tesla K40c. If you are running on a CPU, expect slower performance.\n\n\n`cifar10_train.py` periodically uses a  `tf.train.Saver` to save\nall model parameters in\ncheckpoint files\nbut it does not evaluate the model. The checkpoint file\nwill be used by `cifar10_eval.py` to measure the predictive\nperformance (see Evaluating a Model below).\nIf you followed the previous steps, then you have now started training\na CIFAR-10 model. Congratulations!\nThe terminal text returned from `cifar10_train.py` provides minimal insight into\nhow the model is training. We want more insight into the model during training:\n\nIs the loss really decreasing or is that just noise?\nIs the model being provided appropriate images?\nAre the gradients, activations and weights reasonable?\nWhat is the learning rate currently at?\n\nTensorBoard provides this\nfunctionality, displaying data exported periodically from `cifar10_train.py` via\na `tf.summary.FileWriter`. You can view the results in TensorBoard by running:\n`shell\ntensorboard --logdir /tmp/cifar10_train`\nFor instance, we can watch how the distribution of activations and degree of\nsparsity in `local3` features evolve during training:\n\n\n\n\nIndividual loss functions, as well as the total loss, are particularly\ninteresting to track over time. However, the loss exhibits a considerable amount\nof noise due to the small batch size employed by training.  In practice we find\nit extremely useful to visualize their moving averages in addition to their raw\nvalues.  See how the scripts use\n`tf.train.ExponentialMovingAverage`\nfor this purpose.\nEvaluating a Model\nLet us now evaluate how well the trained model performs on a hold-out data set.\nThe model is evaluated by the script `cifar10_eval.py`.  It constructs the model\nwith the `inference()` function and uses all 10,000 images in the evaluation set\nof CIFAR-10. It calculates the precision at 1: how often the top prediction\nmatches the true label of the image.\nTo monitor how the model improves during training, the evaluation script runs\nperiodically on the latest checkpoint files created by the `cifar10_train.py`.\n`shell\npython cifar10_eval.py`\n\nBe careful not to run the evaluation and training binary on the same GPU or\nelse you might run out of memory. Consider running the evaluation on\na separate GPU if available or suspending the training binary while running\nthe evaluation on the same GPU.\n\nYou should see the output:\n`shell\n2015-11-06 08:30:44.391206: precision @ 1 = 0.860\n...`\nThe script merely returns the precision @ 1 periodically -- in this case\nit returned 86% accuracy. `cifar10_eval.py` also\nexports summaries that may be visualized in TensorBoard. These summaries\nprovide additional insight into the model during evaluation. You can view \nthe results in TensorBoard by running:\n`shell\ntensorboard --logdir /tmp/cifar10_eval`\nThe training script calculates the\n`tf.train.ExponentialMovingAverage` of all learned variables.\nThe evaluation script substitutes\nall learned model parameters with the moving average version. This\nsubstitution boosts model performance at evaluation time.\n\nEXERCISE: Employing averaged parameters may boost predictive performance\nby about 3% as measured by precision @ 1. Edit `cifar10_eval.py` to not employ\nthe averaged parameters for the model and verify that the predictive performance\ndrops.\n\nTraining a Model Using Multiple GPU Cards\nModern workstations may contain multiple GPUs for scientific computation.\nTensorFlow can leverage this environment to run the training operation\nconcurrently across multiple cards.\nTraining a model in a parallel, distributed fashion requires\ncoordinating training processes. For what follows we term model replica\nto be one copy of a model training on a subset of data.\nNaively employing asynchronous updates of model parameters\nleads to sub-optimal training performance\nbecause an individual model replica might be trained on a stale\ncopy of the model parameters. Conversely, employing fully synchronous\nupdates will be as slow as the slowest model replica.\nIn a workstation with multiple GPU cards, each GPU will have similar speed\nand contain enough memory to run an entire CIFAR-10 model. Thus, we opt to\ndesign our training system in the following manner:\n\nPlace an individual model replica on each GPU.\nUpdate model parameters synchronously by waiting for all GPUs to finish\nprocessing a batch of data.\n\nHere is a diagram of this model:\n\n\n\nNote that each GPU computes inference as well as the gradients for a unique\nbatch of data. This setup effectively permits dividing up a larger batch\nof data across the GPUs.\nThis setup requires that all GPUs share the model parameters. A well-known\nfact is that transferring data to and from GPUs is quite slow. For this\nreason, we decide to store and update all model parameters on the CPU (see\ngreen box). A fresh set of model parameters is transferred to the GPU\nwhen a new batch of data is processed by all GPUs.\nThe GPUs are synchronized in operation. All gradients are accumulated from\nthe GPUs and averaged (see green box). The model parameters are updated with\nthe gradients averaged across all model replicas.\nPlacing Variables and Operations on Devices\nPlacing operations and variables on devices requires some special\nabstractions.\nThe first abstraction we require is a function for computing inference and\ngradients for a single model replica. In the code we term this abstraction\na \"tower\". We must set two attributes for each tower:\n\n\nA unique name for all operations within a tower.\n`tf.name_scope` provides\nthis unique name by prepending a scope. For instance, all operations in\nthe first tower are prepended with `tower_0`, e.g. `tower_0/conv1/Conv2D`.\n\n\nA preferred hardware device to run the operation within a tower.\n`tf.device` specifies this. For\ninstance, all operations in the first tower reside within `device('/device:GPU:0')`\nscope indicating that they should be run on the first GPU.\n\n\nAll variables are pinned to the CPU and accessed via\n`tf.get_variable`\nin order to share them in a multi-GPU version.\nSee how-to on Sharing Variables.\nLaunching and Training the Model on Multiple GPU cards\nIf you have several GPU cards installed on your machine you can use them to\ntrain the model faster with the `cifar10_multi_gpu_train.py` script.  This\nversion of the training script parallelizes the model across multiple GPU cards.\n`shell\npython cifar10_multi_gpu_train.py --num_gpus=2`\nNote that the number of GPU cards used defaults to 1. Additionally, if only 1\nGPU is available on your machine, all computations will be placed on it, even if\nyou ask for more.\n\nEXERCISE: The default settings for `cifar10_train.py` is to\nrun on a batch size of 128. Try running `cifar10_multi_gpu_train.py` on 2 GPUs\nwith a batch size of 64 and compare the training speed.\n\nNext Steps\nIf you are now interested in developing and training your own image\nclassification system, we recommend forking this tutorial and replacing\ncomponents to address your image classification problem.\n\nEXERCISE: Download the\nStreet View House Numbers (SVHN) data set.\nFork the CIFAR-10 tutorial and swap in the SVHN as the input data. Try adapting\n",
    "tag": "tensorflow"
  },
  {
    "title": "Image Recognition",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/tutorials/images/image_recognition.md",
    "content": "Image Recognition\nOur brains make vision seem easy. It doesn't take any effort for humans to\ntell apart a lion and a jaguar, read a sign, or recognize a human's face.\nBut these are actually hard problems to solve with a computer: they only\nseem easy because our brains are incredibly good at understanding images.\nIn the last few years, the field of machine learning has made tremendous\nprogress on addressing these difficult problems. In particular, we've\nfound that a kind of model called a deep\nconvolutional neural network\ncan achieve reasonable performance on hard visual recognition tasks --\nmatching or exceeding human performance in some domains.\nResearchers have demonstrated steady progress\nin computer vision by validating their work against\nImageNet -- an academic benchmark for computer vision.\nSuccessive models continue to show improvements, each time achieving\na new state-of-the-art result:\nQuocNet, AlexNet, Inception (GoogLeNet), BN-Inception-v2.\nResearchers both internal and external to Google have published papers describing all\nthese models but the results are still hard to reproduce.\nWe're now taking the next step by releasing code for running image recognition\non our latest model, Inception-v3.\nInception-v3 is trained for the ImageNet Large Visual Recognition Challenge\nusing the data from 2012. This is a standard task in computer vision,\nwhere models try to classify entire\nimages into 1000 classes, like \"Zebra\", \"Dalmatian\", and \"Dishwasher\".\nFor example, here are the results from AlexNet classifying some images:\n\n\n\nTo compare models, we examine how often the model fails to predict the\ncorrect answer as one of their top 5 guesses -- termed \"top-5 error rate\".\nAlexNet achieved by setting a top-5 error rate of 15.3% on the 2012\nvalidation data set; Inception (GoogLeNet) achieved 6.67%;\nBN-Inception-v2 achieved 4.9%; Inception-v3 reaches 3.46%.\n\nHow well do humans do on ImageNet Challenge? There's a blog post by\nAndrej Karpathy who attempted to measure his own performance. He reached\n5.1% top-5 error rate.\n\nThis tutorial will teach you how to use Inception-v3. You'll learn how to\nclassify images into 1000 classes in Python or C++. We'll also discuss how to\nextract higher level features from this model which may be reused for other\nvision tasks.\nWe're excited to see what the community will do with this model.\nUsage with Python API\n`classify_image.py` downloads the trained model from `tensorflow.org`\nwhen the program is run for the first time. You'll need about 200M of free space\navailable on your hard disk.\nStart by cloning the TensorFlow models repo from GitHub. Run the following commands:\n\n\n```cd models/tutorials/image/imagenet\npython classify_image.py\n```\n\n\nThe above command will classify a supplied image of a panda bear.\n\n\n\nIf the model runs correctly, the script will produce the following output:\n\n\n```giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca (score = 0.88493)\nindri, indris, Indri indri, Indri brevicaudatus (score = 0.00878)\nlesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens (score = 0.00317)\ncustard apple (score = 0.00149)\nearthstar (score = 0.00127)\n```\n\n\nIf you wish to supply other JPEG images, you may do so by editing\nthe `--image_file` argument.\n\nIf you download the model data to a different directory, you\nwill need to point `--model_dir`  to the directory used.\n\nUsage with the C++ API\nYou can run the same Inception-v3 model in C++ for use in production\nenvironments. You can download the archive containing the GraphDef that defines\nthe model like this (running from the root directory of the TensorFlow\nrepository):\n`bash\ncurl -L \"https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz\" |\n  tar -C tensorflow/examples/label_image/data -xz`\nNext, we need to compile the C++ binary that includes the code to load and run the graph.\nIf you've followed\nthe instructions to download the source installation of TensorFlow\nfor your platform, you should be able to build the example by\nrunning this command from your shell terminal:\n`bash\nbazel build tensorflow/examples/label_image/...`\nThat should create a binary executable that you can then run like this:\n`bash\nbazel-bin/tensorflow/examples/label_image/label_image`\nThis uses the default example image that ships with the framework, and should\noutput something similar to this:\n`I tensorflow/examples/label_image/main.cc:206] military uniform (653): 0.834306\nI tensorflow/examples/label_image/main.cc:206] mortarboard (668): 0.0218692\nI tensorflow/examples/label_image/main.cc:206] academic gown (401): 0.0103579\nI tensorflow/examples/label_image/main.cc:206] pickelhaube (716): 0.00800814\nI tensorflow/examples/label_image/main.cc:206] bulletproof vest (466): 0.00535088`\nIn this case, we're using the default image of\nAdmiral Grace Hopper, and you can\nsee the network correctly identifies she's wearing a military uniform, with a high\nscore of 0.8.\n\n\n\nNext, try it out on your own images by supplying the --image= argument, e.g.\n`bash\nbazel-bin/tensorflow/examples/label_image/label_image --image=my_image.png`\nIf you look inside the tensorflow/examples/label_image/main.cc\nfile, you can find out\nhow it works. We hope this code will help you integrate TensorFlow into\nyour own applications, so we will walk step by step through the main functions:\nThe command line flags control where the files are loaded from, and properties of the input images.\nThe model expects to get square 299x299 RGB images, so those are the `input_width`\nand `input_height` flags. We also need to scale the pixel values from integers that\nare between 0 and 255 to the floating point values that the graph operates on.\nWe control the scaling with the `input_mean` and `input_std` flags: we first subtract\n`input_mean` from each pixel value, then divide it by `input_std`.\nThese values probably look somewhat magical, but they are just defined by the\noriginal model author based on what he/she wanted to use as input images for\ntraining. If you have a graph that you've trained yourself, you'll just need\nto adjust the values to match whatever you used during your training process.\nYou can see how they're applied to an image in the\nReadTensorFromImageFile()\nfunction.\n`C++\n// Given an image file name, read in the data, try to decode it as an image,\n// resize it to the requested size, and then scale the values as desired.\nStatus ReadTensorFromImageFile(string file_name, const int input_height,\n                               const int input_width, const float input_mean,\n                               const float input_std,\n                               std::vector<Tensor>* out_tensors) {\n  tensorflow::GraphDefBuilder b;`\nWe start by creating a `GraphDefBuilder`, which is an object we can use to\nspecify a model to run or load.\n`C++\n  string input_name = \"file_reader\";\n  string output_name = \"normalized\";\n  tensorflow::Node* file_reader =\n      tensorflow::ops::ReadFile(tensorflow::ops::Const(file_name, b.opts()),\n                                b.opts().WithName(input_name));`\nWe then start creating nodes for the small model we want to run\nto load, resize, and scale the pixel values to get the result the main model\nexpects as its input. The first node we create is just a `Const` op that holds a\ntensor with the file name of the image we want to load. That's then passed as the\nfirst input to the `ReadFile` op. You might notice we're passing `b.opts()` as the last\nargument to all the op creation functions. The argument ensures that the node is added to\nthe model definition held in the `GraphDefBuilder`. We also name the `ReadFile`\noperator by making the `WithName()` call to `b.opts()`. This gives a name to the node,\nwhich isn't strictly necessary since an automatic name will be assigned if you don't\ndo this, but it does make debugging a bit easier.\n`C++\n  // Now try to figure out what kind of file it is and decode it.\n  const int wanted_channels = 3;\n  tensorflow::Node* image_reader;\n  if (tensorflow::StringPiece(file_name).ends_with(\".png\")) {\n    image_reader = tensorflow::ops::DecodePng(\n        file_reader,\n        b.opts().WithAttr(\"channels\", wanted_channels).WithName(\"png_reader\"));\n  } else {\n    // Assume if it's not a PNG then it must be a JPEG.\n    image_reader = tensorflow::ops::DecodeJpeg(\n        file_reader,\n        b.opts().WithAttr(\"channels\", wanted_channels).WithName(\"jpeg_reader\"));\n  }\n  // Now cast the image data to float so we can do normal math on it.\n  tensorflow::Node* float_caster = tensorflow::ops::Cast(\n      image_reader, tensorflow::DT_FLOAT, b.opts().WithName(\"float_caster\"));\n  // The convention for image ops in TensorFlow is that all images are expected\n  // to be in batches, so that they're four-dimensional arrays with indices of\n  // [batch, height, width, channel]. Because we only have a single image, we\n  // have to add a batch dimension of 1 to the start with ExpandDims().\n  tensorflow::Node* dims_expander = tensorflow::ops::ExpandDims(\n      float_caster, tensorflow::ops::Const(0, b.opts()), b.opts());\n  // Bilinearly resize the image to fit the required dimensions.\n  tensorflow::Node* resized = tensorflow::ops::ResizeBilinear(\n      dims_expander, tensorflow::ops::Const({input_height, input_width},\n                                            b.opts().WithName(\"size\")),\n      b.opts());\n  // Subtract the mean and divide by the scale.\n  tensorflow::ops::Div(\n      tensorflow::ops::Sub(\n          resized, tensorflow::ops::Const({input_mean}, b.opts()), b.opts()),\n      tensorflow::ops::Const({input_std}, b.opts()),\n      b.opts().WithName(output_name));`\nWe then keep adding more nodes, to decode the file data as an image, to cast the\nintegers into floating point values, to resize it, and then finally to run the\nsubtraction and division operations on the pixel values.\n`C++\n  // This runs the GraphDef network definition that we've just constructed, and\n  // returns the results in the output tensor.\n  tensorflow::GraphDef graph;\n  TF_RETURN_IF_ERROR(b.ToGraphDef(&graph));`\nAt the end of this we have\na model definition stored in the b variable, which we turn into a full graph\ndefinition with the `ToGraphDef()` function.\n`C++\n  std::unique_ptr<tensorflow::Session> session(\n      tensorflow::NewSession(tensorflow::SessionOptions()));\n  TF_RETURN_IF_ERROR(session->Create(graph));\n  TF_RETURN_IF_ERROR(session->Run({}, {output_name}, {}, out_tensors));\n  return Status::OK();`\nThen we create a `tf.Session`\nobject, which is the interface to actually running the graph, and run it,\nspecifying which node we want to get the output from, and where to put the\noutput data.\nThis gives us a vector of `Tensor` objects, which in this case we know will only be a\nsingle object long. You can think of a `Tensor` as a multi-dimensional array in this\ncontext, and it holds a 299 pixel high, 299 pixel wide, 3 channel image as float\nvalues. If you have your own image-processing framework in your product already, you\nshould be able to use that instead, as long as you apply the same transformations\nbefore you feed images into the main graph.\nThis is a simple example of creating a small TensorFlow graph dynamically in C++,\nbut for the pre-trained Inception model we want to load a much larger definition from\na file. You can see how we do that in the `LoadGraph()` function.\n`C++\n// Reads a model graph definition from disk, and creates a session object you\n// can use to run it.\nStatus LoadGraph(string graph_file_name,\n                 std::unique_ptr<tensorflow::Session>* session) {\n  tensorflow::GraphDef graph_def;\n  Status load_graph_status =\n      ReadBinaryProto(tensorflow::Env::Default(), graph_file_name, &graph_def);\n  if (!load_graph_status.ok()) {\n    return tensorflow::errors::NotFound(\"Failed to load compute graph at '\",\n                                        graph_file_name, \"'\");\n  }`\nIf you've looked through the image loading code, a lot of the terms should seem familiar. Rather than\nusing a `GraphDefBuilder` to produce a `GraphDef` object, we load a protobuf file that\ndirectly contains the `GraphDef`.\n`C++\n  session->reset(tensorflow::NewSession(tensorflow::SessionOptions()));\n  Status session_create_status = (*session)->Create(graph_def);\n  if (!session_create_status.ok()) {\n    return session_create_status;\n  }\n  return Status::OK();\n}`\nThen we create a Session object from that `GraphDef` and\npass it back to the caller so that they can run it at a later time.\nThe `GetTopLabels()` function is a lot like the image loading, except that in this case\nwe want to take the results of running the main graph, and turn it into a sorted list\nof the highest-scoring labels. Just like the image loader, it creates a\n`GraphDefBuilder`, adds a couple of nodes to it, and then runs the short graph to get a\npair of output tensors. In this case they represent the sorted scores and index\npositions of the highest results.\n`C++\n// Analyzes the output of the Inception graph to retrieve the highest scores and\n// their positions in the tensor, which correspond to categories.\nStatus GetTopLabels(const std::vector<Tensor>& outputs, int how_many_labels,\n                    Tensor* indices, Tensor* scores) {\n  tensorflow::GraphDefBuilder b;\n  string output_name = \"top_k\";\n  tensorflow::ops::TopK(tensorflow::ops::Const(outputs[0], b.opts()),\n                        how_many_labels, b.opts().WithName(output_name));\n  // This runs the GraphDef network definition that we've just constructed, and\n  // returns the results in the output tensors.\n  tensorflow::GraphDef graph;\n  TF_RETURN_IF_ERROR(b.ToGraphDef(&graph));\n  std::unique_ptr<tensorflow::Session> session(\n      tensorflow::NewSession(tensorflow::SessionOptions()));\n  TF_RETURN_IF_ERROR(session->Create(graph));\n  // The TopK node returns two outputs, the scores and their original indices,\n  // so we have to append :0 and :1 to specify them both.\n  std::vector<Tensor> out_tensors;\n  TF_RETURN_IF_ERROR(session->Run({}, {output_name + \":0\", output_name + \":1\"},\n                                  {}, &out_tensors));\n  *scores = out_tensors[0];\n  *indices = out_tensors[1];\n  return Status::OK();`\nThe `PrintTopLabels()` function takes those sorted results, and prints them out in a\nfriendly way. The `CheckTopLabel()` function is very similar, but just makes sure that\nthe top label is the one we expect, for debugging purposes.\nAt the end, main()\nties together all of these calls.\n```C++\nint main(int argc, char* argv[]) {\n  // We need to call this to set up global state for TensorFlow.\n  tensorflow::port::InitMain(argv[0], &argc, &argv);\n  Status s = tensorflow::ParseCommandLineFlags(&argc, argv);\n  if (!s.ok()) {\n    LOG(ERROR) << \"Error parsing command line flags: \" << s.ToString();\n    return -1;\n  }\n// First we load and initialize the model.\n  std::unique_ptr session;\n  string graph_path = tensorflow::io::JoinPath(FLAGS_root_dir, FLAGS_graph);\n  Status load_graph_status = LoadGraph(graph_path, &session);\n  if (!load_graph_status.ok()) {\n    LOG(ERROR) << load_graph_status;\n    return -1;\n  }\n```\nWe load the main graph.\n`C++\n  // Get the image from disk as a float array of numbers, resized and normalized\n  // to the specifications the main graph expects.\n  std::vector<Tensor> resized_tensors;\n  string image_path = tensorflow::io::JoinPath(FLAGS_root_dir, FLAGS_image);\n  Status read_tensor_status = ReadTensorFromImageFile(\n      image_path, FLAGS_input_height, FLAGS_input_width, FLAGS_input_mean,\n      FLAGS_input_std, &resized_tensors);\n  if (!read_tensor_status.ok()) {\n    LOG(ERROR) << read_tensor_status;\n    return -1;\n  }\n  const Tensor& resized_tensor = resized_tensors[0];`\nLoad, resize, and process the input image.\n`C++\n  // Actually run the image through the model.\n  std::vector<Tensor> outputs;\n  Status run_status = session->Run({{FLAGS_input_layer, resized_tensor}},\n                                   {FLAGS_output_layer}, {}, &outputs);\n  if (!run_status.ok()) {\n    LOG(ERROR) << \"Running model failed: \" << run_status;\n    return -1;\n  }`\nHere we run the loaded graph with the image as an input.\n`C++\n  // This is for automated testing to make sure we get the expected result with\n  // the default settings. We know that label 866 (military uniform) should be\n  // the top label for the Admiral Hopper image.\n  if (FLAGS_self_test) {\n    bool expected_matches;\n    Status check_status = CheckTopLabel(outputs, 866, &expected_matches);\n    if (!check_status.ok()) {\n      LOG(ERROR) << \"Running check failed: \" << check_status;\n      return -1;\n    }\n    if (!expected_matches) {\n      LOG(ERROR) << \"Self-test failed!\";\n      return -1;\n    }\n  }`\nFor testing purposes we can check to make sure we get the output we expect here.\n`C++\n  // Do something interesting with the results we've generated.\n  Status print_status = PrintTopLabels(outputs, FLAGS_labels);`\nFinally we print the labels we found.\n`C++\n  if (!print_status.ok()) {\n    LOG(ERROR) << \"Running print failed: \" << print_status;\n    return -1;\n  }`\nThe error handling here is using TensorFlow's `Status`\nobject, which is very convenient because it lets you know whether any error has\noccurred with the `ok()` checker, and then can be printed out to give a readable error\nmessage.\nIn this case we are demonstrating object recognition, but you should be able to\nuse very similar code on other models you've found or trained yourself, across\nall\nsorts of domains. We hope this small example gives you some ideas on how to use\nTensorFlow within your own products.\n\nEXERCISE: Transfer learning is the idea that, if you know how to solve a task well, you\nshould be able to transfer some of that understanding to solving related\nproblems.  One way to perform transfer learning is to remove the final\nclassification layer of the network and extract\nthe next-to-last layer of the CNN, in this case a 2048 dimensional vector.\n\nResources for Learning More\nTo learn about neural networks in general, Michael Nielsen's\nfree online book\nis an excellent resource. For convolutional neural networks in particular,\nChris Olah has some\nnice blog posts,\nand Michael Nielsen's book has a\ngreat chapter\ncovering them.\nTo find out more about implementing convolutional neural networks, you can jump\nto the TensorFlow deep convolutional networks tutorial,\nor start a bit more gently with our Estimator MNIST tutorial.\nFinally, if you want to get up to speed on research in this area, you can\nread the recent work of all the papers referenced in this tutorial.",
    "tag": "tensorflow"
  },
  {
    "title": "Improving Linear Models Using Explicit Kernel Methods",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/tutorials/representation/kernel_methods.md",
    "content": "Improving Linear Models Using Explicit Kernel Methods\nNote: This document uses a deprecated version of `tf.estimator`,\n`tf.contrib.learn.Estimator`, which has a different interface. It also uses\nother `contrib` methods whose API may not be stable.\nIn this tutorial, we demonstrate how combining (explicit) kernel methods with\nlinear models can drastically increase the latters' quality of predictions\nwithout significantly increasing training and inference times. Unlike dual\nkernel methods, explicit (primal) kernel methods scale well with the size of the\ntraining dataset both in terms of training/inference times and in terms of\nmemory requirements.\nIntended audience: Even though we provide a high-level overview of concepts\nrelated to explicit kernel methods, this tutorial primarily targets readers who\nalready have at least basic knowledge of kernel methods and Support Vector\nMachines (SVMs). If you are new to kernel methods, refer to either of the\nfollowing sources for an introduction:\n\nIf you have a strong mathematical background:\nKernel Methods in Machine Learning\nKernel method wikipedia page\n\nCurrently, TensorFlow supports explicit kernel mappings for dense features only;\nTensorFlow will provide support for sparse features at a later release.\nThis tutorial uses tf.contrib.learn\n(TensorFlow's high-level Machine Learning API) Estimators for our ML models.\nIf you are not familiar with this API, The Estimator guide\nis a good place to start. We will use the MNIST dataset. The tutorial consists\nof the following steps:\n\nLoad and prepare MNIST data for classification.\nConstruct a simple linear model, train it, and evaluate it on the eval data.\nReplace the linear model with a kernelized linear model, re-train, and\nre-evaluate.\n\nLoad and prepare MNIST data for classification\nRun the following utility command to load the MNIST dataset:\n`python\ndata = tf.contrib.learn.datasets.mnist.load_mnist()`\nThe preceding method loads the entire MNIST dataset (containing 70K samples) and\nsplits it into train, validation, and test data with 55K, 5K, and 10K samples\nrespectively. Each split contains one numpy array for images (with shape\n[sample_size, 784]) and one for labels (with shape [sample_size, 1]). In this\ntutorial, we only use the train and validation splits to train and evaluate our\nmodels respectively.\nIn order to feed data to a `tf.contrib.learn Estimator`, it is helpful to convert\nit to Tensors. For this, we will use an `input function` which adds Ops to the\nTensorFlow graph that, when executed, create mini-batches of Tensors to be used\ndownstream. For more background on input functions, check\nthis section on input functions.\nIn this example, we will use the `tf.train.shuffle_batch` Op which, besides\nconverting numpy arrays to Tensors, allows us to specify the batch_size and\nwhether to randomize the input every time the input_fn Ops are executed\n(randomization typically expedites convergence during training). The full code\nfor loading and preparing the data is shown in the snippet below. In this\nexample, we use mini-batches of size 256 for training and the entire sample\n(5K entries) for evaluation. Feel free to experiment with different batch sizes.\n```python\nimport numpy as np\nimport tensorflow as tf\ndef get_input_fn(dataset_split, batch_size, capacity=10000, min_after_dequeue=3000):\ndef _input_fn():\n    images_batch, labels_batch = tf.train.shuffle_batch(\n        tensors=[dataset_split.images, dataset_split.labels.astype(np.int32)],\n        batch_size=batch_size,\n        capacity=capacity,\n        min_after_dequeue=min_after_dequeue,\n        enqueue_many=True,\n        num_threads=4)\n    features_map = {'images': images_batch}\n    return features_map, labels_batch\nreturn _input_fn\ndata = tf.contrib.learn.datasets.mnist.load_mnist()\ntrain_input_fn = get_input_fn(data.train, batch_size=256)\neval_input_fn = get_input_fn(data.validation, batch_size=5000)\n```\nTraining a simple linear model\nWe can now train a linear model over the MNIST dataset. We will use the\n`tf.contrib.learn.LinearClassifier` estimator with 10 classes representing the\n10 digits. The input features form a 784-dimensional dense vector which can\nbe specified as follows:\n`python\nimage_column = tf.contrib.layers.real_valued_column('images', dimension=784)`\nThe full code for constructing, training and evaluating a LinearClassifier\nestimator is as follows:\n```python\nimport time\nSpecify the feature(s) to be used by the estimator.\nimage_column = tf.contrib.layers.real_valued_column('images', dimension=784)\nestimator = tf.contrib.learn.LinearClassifier(feature_columns=[image_column], n_classes=10)\nTrain.\nstart = time.time()\nestimator.fit(input_fn=train_input_fn, steps=2000)\nend = time.time()\nprint('Elapsed time: {} seconds'.format(end - start))\nEvaluate and report metrics.\neval_metrics = estimator.evaluate(input_fn=eval_input_fn, steps=1)\nprint(eval_metrics)\n```\nThe following table summarizes the results on the eval data.\nmetric        | value\n:------------ | :------------\nloss          | 0.25 to 0.30\naccuracy      | 92.5%\ntraining time | ~25 seconds on my machine\nNote: Metrics will vary depending on various factors.\nIn addition to experimenting with the (training) batch size and the number of\ntraining steps, there are a couple other parameters that can be tuned as well.\nFor instance, you can change the optimization method used to minimize the loss\nby explicitly selecting another optimizer from the collection of\navailable optimizers.\nAs an example, the following code constructs a LinearClassifier estimator that\nuses the Follow-The-Regularized-Leader (FTRL) optimization strategy with a\nspecific learning rate and L2-regularization.\n`python\noptimizer = tf.train.FtrlOptimizer(learning_rate=5.0, l2_regularization_strength=1.0)\nestimator = tf.contrib.learn.LinearClassifier(\n    feature_columns=[image_column], n_classes=10, optimizer=optimizer)`\nRegardless of the values of the parameters, the maximum accuracy a linear model\ncan achieve on this dataset caps at around 93%.\nUsing explicit kernel mappings with the linear model.\nThe relatively high error (~7%) of the linear model over MNIST indicates that\nthe input data is not linearly separable. We will use explicit kernel mappings\nto reduce the classification error.\nIntuition: The high-level idea is to use a non-linear map to transform the\ninput space to another feature space (of possibly higher dimension) where the\n(transformed) features are (almost) linearly separable and then apply a linear\nmodel on the mapped features. This is shown in the following figure:\n\n\n\nTechnical details\nIn this example we will use Random Fourier Features, introduced in the\n\"Random Features for Large-Scale Kernel Machines\"\npaper by Rahimi and Recht, to map the input data. Random Fourier Features map a\nvector \\(\\mathbf{x} \\in \\mathbb{R}^d\\) to \\(\\mathbf{x'} \\in \\mathbb{R}^D\\)\nvia the following mapping:\n$$\nRFFM(\\cdot): \\mathbb{R}^d \\to \\mathbb{R}^D, \\quad\nRFFM(\\mathbf{x}) =  \\cos(\\mathbf{\\Omega} \\cdot \\mathbf{x}+ \\mathbf{b})\n$$\nwhere \\(\\mathbf{\\Omega} \\in \\mathbb{R}^{D \\times d}\\),\n\\(\\mathbf{x} \\in \\mathbb{R}^d,\\) \\(\\mathbf{b} \\in \\mathbb{R}^D\\) and the\ncosine is applied element-wise.\nIn this example, the entries of \\(\\mathbf{\\Omega}\\) and \\(\\mathbf{b}\\) are\nsampled from distributions such that the mapping satisfies the following\nproperty:\n$$\nRFFM(\\mathbf{x})^T \\cdot RFFM(\\mathbf{y}) \\approx\ne^{-\\frac{\\|\\mathbf{x} - \\mathbf{y}\\|^2}{2 \\sigma^2}}\n$$\nThe right-hand-side quantity of the expression above is known as the RBF (or\nGaussian) kernel function. This function is one of the most-widely used kernel\nfunctions in Machine Learning and implicitly measures similarity in a different,\nmuch higher dimensional space than the original one. See\nRadial basis function kernel\nfor more details.\nKernel classifier\n`tf.contrib.kernel_methods.KernelLinearClassifier` is a pre-packaged\n`tf.contrib.learn` estimator that combines the power of explicit kernel mappings\nwith linear models. Its constructor is almost identical to that of the\nLinearClassifier estimator with the additional option to specify a list of\nexplicit kernel mappings to be applied to each feature the classifier uses. The\nfollowing code snippet demonstrates how to replace LinearClassifier with\nKernelLinearClassifier.\n```python\nSpecify the feature(s) to be used by the estimator. This is identical to the\ncode used for the LinearClassifier.\nimage_column = tf.contrib.layers.real_valued_column('images', dimension=784)\noptimizer = tf.train.FtrlOptimizer(\n   learning_rate=50.0, l2_regularization_strength=0.001)\nkernel_mapper = tf.contrib.kernel_methods.RandomFourierFeatureMapper(\n  input_dim=784, output_dim=2000, stddev=5.0, name='rffm')\nkernel_mappers = {image_column: [kernel_mapper]}\nestimator = tf.contrib.kernel_methods.KernelLinearClassifier(\n   n_classes=10, optimizer=optimizer, kernel_mappers=kernel_mappers)\nTrain.\nstart = time.time()\nestimator.fit(input_fn=train_input_fn, steps=2000)\nend = time.time()\nprint('Elapsed time: {} seconds'.format(end - start))\nEvaluate and report metrics.\neval_metrics = estimator.evaluate(input_fn=eval_input_fn, steps=1)\nprint(eval_metrics)\n```\nThe only additional parameter passed to`KernelLinearClassifier` is a dictionary\nfrom feature_columns to a list of kernel mappings to be applied to the\ncorresponding feature column. The following lines instruct the classifier to\nfirst map the initial 784-dimensional images to 2000-dimensional vectors using\nrandom Fourier features and then learn a linear model on the transformed\nvectors:\n`python\nkernel_mapper = tf.contrib.kernel_methods.RandomFourierFeatureMapper(\n  input_dim=784, output_dim=2000, stddev=5.0, name='rffm')\nkernel_mappers = {image_column: [kernel_mapper]}\nestimator = tf.contrib.kernel_methods.KernelLinearClassifier(\n   n_classes=10, optimizer=optimizer, kernel_mappers=kernel_mappers)`\nNotice the `stddev` parameter. This is the standard deviation (\\(\\sigma\\)) of\nthe approximated RBF kernel and controls the similarity measure used in\nclassification. `stddev` is typically determined via hyperparameter tuning.\nThe results of running the preceding code are summarized in the following table.\nWe can further increase the accuracy by increasing the output dimension of the\nmapping and tuning the standard deviation.\nmetric        | value\n:------------ | :------------\nloss          | 0.10\naccuracy      | 97%\ntraining time | ~35 seconds on my machine\nstddev\nThe classification quality is very sensitive to the value of stddev. The\nfollowing table shows the accuracy of the classifier on the eval data for\ndifferent values of stddev. The optimal value is stddev=5.0. Notice how too\nsmall or too high stddev values can dramatically decrease the accuracy of the\nclassification.\nstddev | eval accuracy\n:----- | :------------\n1.0    | 0.1362\n2.0    | 0.4764\n4.0    | 0.9654\n5.0    | 0.9766\n8.0    | 0.9714\n16.0   | 0.8878\nOutput dimension\nIntuitively, the larger the output dimension of the mapping, the closer the\ninner product of two mapped vectors approximates the kernel, which typically\ntranslates to better classification accuracy. Another way to think about this is\nthat the output dimension equals the number of weights of the linear model; the\nlarger this dimension, the larger the \"degrees of freedom\" of the model.\nHowever, after a certain threshold, higher output dimensions increase the\naccuracy by very little, while making training take more time. This is shown in\nthe following two Figures which depict the eval accuracy as a function of the\noutput dimension and the training time, respectively.\n\n\nSummary\nExplicit kernel mappings combine the predictive power of nonlinear models with\nthe scalability of linear models. Unlike traditional dual kernel methods,\nexplicit kernel methods can scale to millions or hundreds of millions of\nsamples. When using explicit kernel mappings, consider the following tips:\n\nRandom Fourier Features can be particularly effective for datasets with dense\nfeatures.\nThe parameters of the kernel mapping are often data-dependent. Model quality\ncan be very sensitive to these parameters. Use hyperparameter tuning to find the\noptimal values.\nIf you have multiple numerical features, concatenate them into a single\nmulti-dimensional feature and apply the kernel mapping to the concatenated\n",
    "tag": "tensorflow"
  },
  {
    "title": "Large-scale Linear Models with TensorFlow",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/tutorials/representation/linear.md",
    "content": "Large-scale Linear Models with TensorFlow\n`tf.estimator` provides (among other things) a rich set of tools for\nworking with linear models in TensorFlow. This document provides an overview of\nthose tools. It explains:\n\nWhat a linear model is.\nWhy you might want to use a linear model.\nHow Estimators make it easy to build linear models in TensorFlow.\nHow you can use Estimators to combine linear models with\n     deep learning to get the advantages of both.\n\nRead this overview to decide whether the Estimator's linear model tools  might\nbe useful to you. Then work through the\nEstimator wide and deep learning tutorial\nto give it a try. This overview uses code samples from the tutorial, but the\ntutorial walks through the code in greater detail.\nTo understand this overview it will help to have some familiarity\nwith basic machine learning concepts, and also with\nEstimators.\n[TOC]\nWhat is a linear model?\nA linear model uses a single weighted sum of features to make a prediction.\nFor example, if you have data\non age, years of education, and weekly hours of\nwork for a population, a model can learn weights for each of those numbers so that\ntheir weighted sum estimates a person's salary. You can also use linear models\nfor classification.\nSome linear models transform the weighted sum into a more convenient form. For\nexample, logistic regression plugs the weighted sum into the logistic\nfunction to turn the output into a value between 0 and 1. But you still just\nhave one weight for each input feature.\nWhy would you want to use a linear model?\nWhy would you want to use so simple a model when recent research has\ndemonstrated the power of more complex neural networks with many layers?\nLinear models:\n\ntrain quickly, compared to deep neural nets.\ncan work well on very large feature sets.\ncan be trained with algorithms that don't require a lot of fiddling\n   with learning rates, etc.\ncan be interpreted and debugged more easily than neural nets.\n   You can examine the weights assigned to each feature to figure out what's\n   having the biggest impact on a prediction.\nprovide an excellent starting point for learning about machine learning.\nare widely used in industry.\n\nHow do Estimators help you build linear models?\nYou can build a linear model from scratch in TensorFlow without the help of a\nspecial API. But Estimators provides some tools that make it easier to build\neffective large-scale linear models.\nFeature columns and transformations\nMuch of the work of designing a linear model consists of transforming raw data\ninto suitable input features. TensorFlow uses the `FeatureColumn` abstraction to\nenable these transformations.\nA `FeatureColumn` represents a single feature in your data. A `FeatureColumn`\nmay represent a quantity like 'height', or it may represent a category like\n'eye_color' where the value is drawn from a set of discrete possibilities like\n{'blue', 'brown', 'green'}.\nIn the case of both continuous features like 'height' and categorical\nfeatures like 'eye_color', a single value in the data might get transformed\ninto a sequence of numbers before it is input into the model. The\n`FeatureColumn` abstraction lets you manipulate the feature as a single\nsemantic unit in spite of this fact. You can specify transformations and\nselect features to include without dealing with specific indices in the\ntensors you feed into the model.\nSparse columns\nCategorical features in linear models are typically translated into a sparse\nvector in which each possible value has a corresponding index or id. For\nexample, if there are only three possible eye colors you can represent\n'eye_color' as a length 3 vector: 'brown' would become [1, 0, 0], 'blue' would\nbecome [0, 1, 0] and 'green' would become [0, 0, 1]. These vectors are called\n\"sparse\" because they may be very long, with many zeros, when the set of\npossible values is very large (such as all English words).\nWhile you don't need to use categorical columns to use the linear model tools\nprovided by Estimators, one of the strengths of linear models is their ability\nto deal with large sparse vectors. Sparse features are a primary use case for\nthe linear model tools provided by Estimators.\nEncoding sparse columns\n`FeatureColumn` handles the conversion of categorical values into vectors\nautomatically, with code like this:\n`python\neye_color = tf.feature_column.categorical_column_with_vocabulary_list(\n    \"eye_color\", vocabulary_list=[\"blue\", \"brown\", \"green\"])`\nwhere `eye_color` is the name of a column in your source data.\nYou can also generate `FeatureColumn`s for categorical features for which you\ndon't know all possible values. For this case you would use\n`categorical_column_with_hash_bucket()`, which uses a hash function to assign\nindices to feature values.\n`python\neducation = tf.feature_column.categorical_column_with_hash_bucket(\n    \"education\", hash_bucket_size=1000)`\nFeature Crosses\nBecause linear models assign independent weights to separate features, they\ncan't learn the relative importance of specific combinations of feature\nvalues. If you have a feature 'favorite_sport' and a feature 'home_city' and\nyou're trying to predict whether a person likes to wear red, your linear model\nwon't be able to learn that baseball fans from St. Louis especially like to\nwear red.\nYou can get around this limitation by creating a new feature\n'favorite_sport_x_home_city'. The value of this feature for a given person is\njust the concatenation of the values of the two source features:\n'baseball_x_stlouis', for example. This sort of combination feature is called\na feature cross.\nThe `crossed_column()` method makes it easy to set up feature crosses:\n`python\nsport_x_city = tf.feature_column.crossed_column(\n    [\"sport\", \"city\"], hash_bucket_size=int(1e4))`\nContinuous columns\nYou can specify a continuous feature like so:\n`python\nage = tf.feature_column.numeric_column(\"age\")`\nAlthough, as a single real number, a continuous feature can often be input\ndirectly into the model, TensorFlow offers useful transformations for this sort\nof column as well.\nBucketization\nBucketization turns a continuous column into a categorical column. This\ntransformation lets you use continuous features in feature crosses, or learn\ncases where specific value ranges have particular importance.\nBucketization divides the range of possible values into subranges called\nbuckets:\n`python\nage_buckets = tf.feature_column.bucketized_column(\n    age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])`\nThe bucket into which a value falls becomes the categorical label for\nthat value.\nInput function\n`FeatureColumn`s provide a specification for the input data for your model,\nindicating how to represent and transform the data. But they do not provide\nthe data itself. You provide the data through an input function.\nThe input function must return a dictionary of tensors. Each key corresponds to\nthe name of a `FeatureColumn`. Each key's value is a tensor containing the\nvalues of that feature for all data instances. See\nPremade Estimators for a\nmore comprehensive look at input functions, and `input_fn` in the\nwide and deep learning tutorial\nfor an example implementation of an input function.\nThe input function is passed to the `train()` and `evaluate()` calls that\ninitiate training and testing, as described in the next section.\nLinear estimators\nTensorFlow estimator classes provide a unified training and evaluation harness\nfor regression and classification models. They take care of the details of the\ntraining and evaluation loops and allow the user to focus on model inputs and\narchitecture.\nTo build a linear estimator, you can use either the\n`tf.estimator.LinearClassifier` estimator or the\n`tf.estimator.LinearRegressor` estimator, for classification and\nregression respectively.\nAs with all TensorFlow estimators, to run the estimator you just:\n\nInstantiate the estimator class. For the two linear estimator classes,\n   you pass a list of `FeatureColumn`s to the constructor.\nCall the estimator's `train()` method to train it.\nCall the estimator's `evaluate()` method to see how it does.\n\nFor example:\n```python\ne = tf.estimator.LinearClassifier(\n    feature_columns=[\n        native_country, education, occupation, workclass, marital_status,\n        race, age_buckets, education_x_occupation,\n        age_buckets_x_race_x_occupation],\n    model_dir=YOUR_MODEL_DIRECTORY)\ne.train(input_fn=input_fn_train, steps=200)\nEvaluate for one step (one pass through the test data).\nresults = e.evaluate(input_fn=input_fn_test)\nPrint the stats for the evaluation.\nfor key in sorted(results):\n    print(\"%s: %s\" % (key, results[key]))\n```\nWide and deep learning\nThe `tf.estimator` module also provides an estimator class that lets you jointly\ntrain a linear model and a deep neural network. This novel approach combines the\nability of linear models to \"memorize\" key features with the generalization\nability of neural nets. Use `tf.estimator.DNNLinearCombinedClassifier` to\ncreate this sort of \"wide and deep\" model:\n`python\ne = tf.estimator.DNNLinearCombinedClassifier(\n    model_dir=YOUR_MODEL_DIR,\n    linear_feature_columns=wide_columns,\n    dnn_feature_columns=deep_columns,\n    dnn_hidden_units=[100, 50])`\nFor more information, see the",
    "tag": "tensorflow"
  },
  {
    "title": "Vector Representations of Words",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/tutorials/representation/word2vec.md",
    "content": "Vector Representations of Words\nIn this tutorial we look at the word2vec model by\nMikolov et al.\nThis model is used for learning vector representations of words, called \"word\nembeddings\".\nHighlights\nThis tutorial is meant to highlight the interesting, substantive parts of\nbuilding a word2vec model in TensorFlow.\n\nWe start by giving the motivation for why we would want to\nrepresent words as vectors.\nWe look at the intuition behind the model and how it is trained\n(with a splash of math for good measure).\nWe also show a simple implementation of the model in TensorFlow.\nFinally, we look at ways to make the naive version scale better.\n\nWe walk through the code later during the tutorial, but if you'd prefer to dive\nstraight in, feel free to look at the minimalistic implementation in\ntensorflow/examples/tutorials/word2vec/word2vec_basic.py\nThis basic example contains the code needed to download some data, train on it a\nbit and visualize the result. Once you get comfortable with reading and running\nthe basic version, you can graduate to\nmodels/tutorials/embedding/word2vec.py\nwhich is a more serious implementation that showcases some more advanced\nTensorFlow principles about how to efficiently use threads to move data into a\ntext model, how to checkpoint during training, etc.\nBut first, let's look at why we would want to learn word embeddings in the first\nplace. Feel free to skip this section if you're an Embedding Pro and you'd just\nlike to get your hands dirty with the details.\nMotivation: Why learn word embeddings?\nImage and audio processing systems work with rich, high-dimensional datasets\nencoded as vectors of the individual raw pixel-intensities for image data, or\ne.g. power spectral density coefficients for audio data. For tasks like object\nor speech recognition we know that all the information required to successfully\nperform the task is encoded in the data (because humans can perform these tasks\nfrom the raw data).  However, natural language processing systems traditionally\ntreat words as discrete atomic symbols, and therefore 'cat' may be represented\nas  `Id537` and 'dog' as `Id143`.  These encodings are arbitrary, and provide\nno useful information to the system regarding the relationships that may exist\nbetween the individual symbols. This means that the model can leverage\nvery little of what it has learned about 'cats' when it is processing data about\n'dogs' (such that they are both animals, four-legged, pets, etc.). Representing\nwords as unique, discrete ids furthermore leads to data sparsity, and usually\nmeans that we may need more data in order to successfully train statistical\nmodels.  Using vector representations can overcome some of these obstacles.\n\n\n\nVector space models (VSMs)\nrepresent (embed) words in a continuous vector space where semantically\nsimilar words are mapped to nearby points ('are embedded nearby each other').\nVSMs have a long, rich history in NLP, but all methods depend in some way or\nanother on the\nDistributional Hypothesis,\nwhich states that words that appear in the same contexts share\nsemantic meaning. The different approaches that leverage this principle can be\ndivided into two categories: count-based methods (e.g.\nLatent Semantic Analysis),\nand predictive methods (e.g.\nneural probabilistic language models).\nThis distinction is elaborated in much more detail by\nBaroni et al.,\nbut in a nutshell: Count-based methods compute the statistics of\nhow often some word co-occurs with its neighbor words in a large text corpus,\nand then map these count-statistics down to a small, dense vector for each word.\nPredictive models directly try to predict a word from its neighbors in terms of\nlearned small, dense embedding vectors (considered parameters of the\nmodel).\nWord2vec is a particularly computationally-efficient predictive model for\nlearning word embeddings from raw text. It comes in two flavors, the Continuous\nBag-of-Words model (CBOW) and the Skip-Gram model (Section 3.1 and 3.2 in Mikolov et al.). Algorithmically, these\nmodels are similar, except that CBOW predicts target words (e.g. 'mat') from\nsource context words ('the cat sits on the'), while the skip-gram does the\ninverse and predicts source context-words from the target words. This inversion\nmight seem like an arbitrary choice, but statistically it has the effect that\nCBOW smoothes over a lot of the distributional information (by treating an\nentire context as one observation). For the most part, this turns out to be a\nuseful thing for smaller datasets. However, skip-gram treats each context-target\npair as a new observation, and this tends to do better when we have larger\ndatasets. We will focus on the skip-gram model in the rest of this tutorial.\nScaling up with noise-contrastive training\nNeural probabilistic language models are traditionally trained using the\nmaximum likelihood (ML)\nprinciple  to maximize the probability of the next word \\(w_t\\) (for \"target\")\ngiven the previous words \\(h\\) (for \"history\") in terms of a\nsoftmax function,\n$$\n\\begin{align}\nP(w_t | h) &= \\text{softmax}(\\text{score}(w_t, h)) \\\n           &= \\frac{\\exp { \\text{score}(w_t, h) } }\n             {\\sum_\\text{Word w' in Vocab} \\exp { \\text{score}(w', h) } }\n\\end{align}\n$$\nwhere \\(\\text{score}(w_t, h)\\) computes the compatibility of word \\(w_t\\)\nwith the context \\(h\\) (a dot product is commonly used). We train this model\nby maximizing its log-likelihood\non the training set, i.e. by maximizing\n$$\n\\begin{align}\n J_\\text{ML} &= \\log P(w_t | h) \\\n  &= \\text{score}(w_t, h) -\n     \\log \\left( \\sum_\\text{Word w' in Vocab} \\exp { \\text{score}(w', h) } \\right).\n\\end{align}\n$$\nThis yields a properly normalized probabilistic model for language modeling.\nHowever this is very expensive, because we need to compute and normalize each\nprobability using the score for all other \\(V\\) words \\(w'\\) in the current\ncontext \\(h\\), at every training step.\n\n\n\nOn the other hand, for feature learning in word2vec we do not need a full\nprobabilistic model. The CBOW and skip-gram models are instead trained using a\nbinary classification objective (logistic regression)\nto discriminate the real target words \\(w_t\\) from \\(k\\) imaginary (noise) words \\(\\tilde w\\), in the\nsame context. We illustrate this below for a CBOW model. For skip-gram the\ndirection is simply inverted.\n\n\n\nMathematically, the objective (for each example) is to maximize\n$$J_\\text{NEG} = \\log Q_\\theta(D=1 |w_t, h) +\n  k \\mathop{\\mathbb{E}}{\\tilde w \\sim P\\text{noise}}\n     \\left[ \\log Q_\\theta(D = 0 |\\tilde w, h) \\right]$$\nwhere \\(Q_\\theta(D=1 | w, h)\\) is the binary logistic regression probability\nunder the model of seeing the word \\(w\\) in the context \\(h\\) in the dataset\n\\(D\\), calculated in terms of the learned embedding vectors \\(\\theta\\). In\npractice we approximate the expectation by drawing \\(k\\) contrastive words\nfrom the noise distribution (i.e. we compute a\nMonte Carlo average).\nThis objective is maximized when the model assigns high probabilities\nto the real words, and low probabilities to noise words. Technically, this is\ncalled\nNegative Sampling,\nand there is good mathematical motivation for using this loss function:\nThe updates it proposes approximate the updates of the softmax function in the\nlimit. But computationally it is especially appealing because computing the\nloss function now scales only with the number of noise words that we\nselect (\\(k\\)), and not all words in the vocabulary (\\(V\\)). This makes it\nmuch faster to train. We will actually make use of the very similar\nnoise-contrastive estimation (NCE)\nloss, for which TensorFlow has a handy helper function `tf.nn.nce_loss()`.\nLet's get an intuitive feel for how this would work in practice!\nThe skip-gram model\nAs an example, let's consider the dataset\n`the quick brown fox jumped over the lazy dog`\nWe first form a dataset of words and the contexts in which they appear. We\ncould define 'context' in any way that makes sense, and in fact people have\nlooked at syntactic contexts (i.e. the syntactic dependents of the current\ntarget word, see e.g.\nLevy et al.),\nwords-to-the-left of the target, words-to-the-right of the target, etc. For now,\nlet's stick to the vanilla definition and define 'context' as the window\nof words to the left and to the right of a target word. Using a window\nsize of 1, we then have the dataset\n`([the, brown], quick), ([quick, fox], brown), ([brown, jumped], fox), ...`\nof `(context, target)` pairs. Recall that skip-gram inverts contexts and\ntargets, and tries to predict each context word from its target word, so the\ntask becomes to predict 'the' and 'brown' from 'quick', 'quick' and 'fox' from\n'brown', etc. Therefore our dataset becomes\n`(quick, the), (quick, brown), (brown, quick), (brown, fox), ...`\nof `(input, output)` pairs.  The objective function is defined over the entire\ndataset, but we typically optimize this with\nstochastic gradient descent\n(SGD) using one example at a time (or a 'minibatch' of `batch_size` examples,\nwhere typically `16 <= batch_size <= 512`). So let's look at one step of\nthis process.\nLet's imagine at training step \\(t\\) we observe the first training case above,\nwhere the goal is to predict `the` from `quick`. We select `num_noise` number\nof noisy (contrastive) examples by drawing from some noise distribution,\ntypically the unigram distribution, \\(P(w)\\). For simplicity let's say\n`num_noise=1` and we select `sheep` as a noisy example. Next we compute the\nloss for this pair of observed and noisy examples, i.e. the objective at time\nstep \\(t\\) becomes\n$$J^{(t)}\\text{NEG} = \\log Q\\theta(D=1 | \\text{the, quick}) +\n  \\log(Q_\\theta(D=0 | \\text{sheep, quick}))$$\nThe goal is to make an update to the embedding parameters \\(\\theta\\) to improve\n(in this case, maximize) this objective function.  We do this by deriving the\ngradient of the loss with respect to the embedding parameters \\(\\theta\\), i.e.\n\\(\\frac{\\partial}{\\partial \\theta} J_\\text{NEG}\\) (luckily TensorFlow provides\neasy helper functions for doing this!). We then perform an update to the\nembeddings by taking a small step in the direction of the gradient. When this\nprocess is repeated over the entire training set, this has the effect of\n'moving' the embedding vectors around for each word until the model is\nsuccessful at discriminating real words from noise words.\nWe can visualize the learned vectors by projecting them down to 2 dimensions\nusing for instance something like the\nt-SNE dimensionality reduction technique.\nWhen we inspect these visualizations it becomes apparent that the vectors\ncapture some general, and in fact quite useful, semantic information about\nwords and their relationships to one another. It was very interesting when we\nfirst discovered that certain directions in the induced vector space specialize\ntowards certain semantic relationships, e.g. male-female, verb tense and\neven country-capital relationships between words, as illustrated in the figure\nbelow (see also for example\nMikolov et al., 2013).\n\n\n\nThis explains why these vectors are also useful as features for many canonical\nNLP prediction tasks, such as part-of-speech tagging or named entity recognition\n(see for example the original work by\nCollobert et al., 2011\n(pdf), or follow-up work by\nTurian et al., 2010).\nBut for now, let's just use them to draw pretty pictures!\nBuilding the graph\nThis is all about embeddings, so let's define our embedding matrix.\nThis is just a big random matrix to start.  We'll initialize the values to be\nuniform in the unit cube.\n`python\nembeddings = tf.Variable(\n    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))`\nThe noise-contrastive estimation loss is defined in terms of a logistic regression\nmodel. For this, we need to define the weights and biases for each word in the\nvocabulary (also called the `output weights` as opposed to the `input\nembeddings`). So let's define that.\n`python\nnce_weights = tf.Variable(\n  tf.truncated_normal([vocabulary_size, embedding_size],\n                      stddev=1.0 / math.sqrt(embedding_size)))\nnce_biases = tf.Variable(tf.zeros([vocabulary_size]))`\nNow that we have the parameters in place, we can define our skip-gram model\ngraph. For simplicity, let's suppose we've already integerized our text corpus\nwith a vocabulary so that each word is represented as an integer (see\ntensorflow/examples/tutorials/word2vec/word2vec_basic.py\nfor the details). The skip-gram model takes two inputs. One is a batch full of\nintegers representing the source context words, the other is for the target\nwords. Let's create placeholder nodes for these inputs, so that we can feed in\ndata later.\n```python\nPlaceholders for inputs\ntrain_inputs = tf.placeholder(tf.int32, shape=[batch_size])\ntrain_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n```\nNow what we need to do is look up the vector for each of the source words in\nthe batch.  TensorFlow has handy helpers that make this easy.\n`python\nembed = tf.nn.embedding_lookup(embeddings, train_inputs)`\nOk, now that we have the embeddings for each word, we'd like to try to predict\nthe target word using the noise-contrastive training objective.\n```python\nCompute the NCE loss, using a sample of the negative labels each time.\nloss = tf.reduce_mean(\n  tf.nn.nce_loss(weights=nce_weights,\n                 biases=nce_biases,\n                 labels=train_labels,\n                 inputs=embed,\n                 num_sampled=num_sampled,\n                 num_classes=vocabulary_size))\n```\nNow that we have a loss node, we need to add the nodes required to compute\ngradients and update the parameters, etc. For this we will use stochastic\ngradient descent, and TensorFlow has handy helpers to make this easy as well.\n```python\nWe use the SGD optimizer.\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)\n```\nTraining the model\nTraining the model is then as simple as using a `feed_dict` to push data into\nthe placeholders and calling\n`tf.Session.run` with this new data\nin a loop.\n`python\nfor inputs, labels in generate_batch(...):\n  feed_dict = {train_inputs: inputs, train_labels: labels}\n  _, cur_loss = session.run([optimizer, loss], feed_dict=feed_dict)`\nSee the full example code in\ntensorflow/examples/tutorials/word2vec/word2vec_basic.py.\nVisualizing the learned embeddings\nAfter training has finished we can visualize the learned embeddings using\nt-SNE.\n\n\n\nEt voila! As expected, words that are similar end up clustering nearby each\nother. For a more heavyweight implementation of word2vec that showcases more of\nthe advanced features of TensorFlow, see the implementation in\nmodels/tutorials/embedding/word2vec.py.\nEvaluating embeddings: analogical reasoning\nEmbeddings are useful for a wide variety of prediction tasks in NLP. Short of\ntraining a full-blown part-of-speech model or named-entity model, one simple way\nto evaluate embeddings is to directly use them to predict syntactic and semantic\nrelationships like `king is to queen as father is to ?`. This is called\nanalogical reasoning and the task was introduced by\nMikolov and colleagues\n.\nDownload the dataset for this task from\ndownload.tensorflow.org.\nTo see how we do this evaluation, have a look at the `build_eval_graph()` and\n`eval()` functions in\nmodels/tutorials/embedding/word2vec.py.\nThe choice of hyperparameters can strongly influence the accuracy on this task.\nTo achieve state-of-the-art performance on this task requires training over a\nvery large dataset, carefully tuning the hyperparameters and making use of\ntricks like subsampling the data, which is out of the scope of this tutorial.\nOptimizing the implementation\nOur vanilla implementation showcases the flexibility of TensorFlow. For\nexample, changing the training objective is as simple as swapping out the call\nto `tf.nn.nce_loss()` for an off-the-shelf alternative such as\n`tf.nn.sampled_softmax_loss()`. If you have a new idea for a loss function, you\ncan manually write an expression for the new objective in TensorFlow and let\nthe optimizer compute its derivatives. This flexibility is invaluable in the\nexploratory phase of machine learning model development, where we are trying\nout several different ideas and iterating quickly.\nOnce you have a model structure you're satisfied with, it may be worth\noptimizing your implementation to run more efficiently (and cover more data in\nless time). For example, the naive code we used in this tutorial would suffer\ncompromised speed because we use Python for reading and feeding data items --\neach of which require very little work on the TensorFlow back-end. If you find\nyour model is seriously bottlenecked on input data, you may want to implement a\ncustom data reader for your problem, as described in\nNew Data Formats. For the case of Skip-Gram\nmodeling, we've actually already done this for you as an example in\nmodels/tutorials/embedding/word2vec.py.\nIf your model is no longer I/O bound but you want still more performance, you\ncan take things further by writing your own TensorFlow Ops, as described in\nAdding a New Op. Again we've provided an example of\nthis for the Skip-Gram case\nmodels/tutorials/embedding/word2vec_optimized.py.\nFeel free to benchmark these against each other to measure performance\nimprovements at each stage.\nConclusion\nIn this tutorial we covered the word2vec model, a computationally efficient\nmodel for learning word embeddings. We motivated why embeddings are useful,\ndiscussed efficient training techniques and showed how to implement all of this\nin TensorFlow. Overall, we hope that this has show-cased how TensorFlow affords\nyou the flexibility you need for early experimentation, and the control you",
    "tag": "tensorflow"
  },
  {
    "title": "Simple Audio Recognition",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/tutorials/sequences/audio_recognition.md",
    "content": "Simple Audio Recognition\nThis tutorial will show you how to build a basic speech recognition network that\nrecognizes ten different words. It's important to know that real speech and\naudio recognition systems are much more complex, but like MNIST for images, it\nshould give you a basic understanding of the techniques involved. Once you've\ncompleted this tutorial, you'll have a model that tries to classify a one second\naudio clip as either silence, an unknown word, \"yes\", \"no\", \"up\", \"down\",\n\"left\", \"right\", \"on\", \"off\", \"stop\", or \"go\". You'll also be able to take this\nmodel and run it in an Android application.\nPreparation\nYou should make sure you have TensorFlow installed, and since the script\ndownloads over 1GB of training data, you'll need a good internet connection and\nenough free space on your machine. The training process itself can take several\nhours, so make sure you have a machine available for that long.\nTraining\nTo begin the training process, go to the TensorFlow source tree and run:\n`bash\npython tensorflow/examples/speech_commands/train.py`\nThe script will start off by downloading the Speech Commands\ndataset,\nwhich consists of over 105,000 WAVE audio files of people saying thirty\ndifferent words. This data was collected by Google and released under a CC BY\nlicense, and you can help improve it by contributing five minutes of your own\nvoice. The archive is\nover 2GB, so this part may take a while, but you should see progress logs, and\nonce it's been downloaded once you won't need to do this step again. You can\nfind more information about this dataset in this\nSpeech Commands paper.\nOnce the downloading has completed, you'll see logging information that looks\nlike this:\n`I0730 16:53:44.766740   55030 train.py:176] Training from step: 1\nI0730 16:53:47.289078   55030 train.py:217] Step #1: rate 0.001000, accuracy 7.0%, cross entropy 2.611571`\nThis shows that the initialization process is done and the training loop has\nbegun. You'll see that it outputs information for every training step. Here's a\nbreak down of what it means:\n`Step #1` shows that we're on the first step of the training loop. In this case\nthere are going to be 18,000 steps in total, so you can look at the step number\nto get an idea of how close it is to finishing.\n`rate 0.001000` is the learning rate that's controlling the speed of the\nnetwork's weight updates. Early on this is a comparatively high number (0.001),\nbut for later training cycles it will be reduced 10x, to 0.0001.\n`accuracy 7.0%` is the how many classes were correctly predicted on this\ntraining step. This value will often fluctuate a lot, but should increase on\naverage as training progresses. The model outputs an array of numbers, one for\neach label, and each number is the predicted likelihood of the input being that\nclass. The predicted label is picked by choosing the entry with the highest\nscore. The scores are always between zero and one, with higher values\nrepresenting more confidence in the result.\n`cross entropy 2.611571` is the result of the loss function that we're using to\nguide the training process. This is a score that's obtained by comparing the\nvector of scores from the current training run to the correct labels, and this\nshould trend downwards during training.\nAfter a hundred steps, you should see a line like this:\n`I0730 16:54:41.813438 55030 train.py:252] Saving to\n\"/tmp/speech_commands_train/conv.ckpt-100\"`\nThis is saving out the current trained weights to a checkpoint file. If your\ntraining script gets interrupted, you can look for the last saved checkpoint and\nthen restart the script with\n`--start_checkpoint=/tmp/speech_commands_train/conv.ckpt-100` as a command line\nargument to start from that point.\nConfusion Matrix\nAfter four hundred steps, this information will be logged:\n`I0730 16:57:38.073667   55030 train.py:243] Confusion Matrix:\n [[258   0   0   0   0   0   0   0   0   0   0   0]\n [  7   6  26  94   7  49   1  15  40   2   0  11]\n [ 10   1 107  80  13  22   0  13  10   1   0   4]\n [  1   3  16 163   6  48   0   5  10   1   0  17]\n [ 15   1  17 114  55  13   0   9  22   5   0   9]\n [  1   1   6  97   3  87   1  12  46   0   0  10]\n [  8   6  86  84  13  24   1   9   9   1   0   6]\n [  9   3  32 112   9  26   1  36  19   0   0   9]\n [  8   2  12  94   9  52   0   6  72   0   0   2]\n [ 16   1  39  74  29  42   0   6  37   9   0   3]\n [ 15   6  17  71  50  37   0   6  32   2   1   9]\n [ 11   1   6 151   5  42   0   8  16   0   0  20]]`\nThe first section is a confusion\nmatrix. To\nunderstand what it means, you first need to know the labels being used, which in\nthis case are \"silence\", \"unknown\", \"yes\", \"no\", \"up\", \"down\", \"left\",\n\"right\", \"on\", \"off\", \"stop\", and \"go\". Each column represents a set of samples\nthat were predicted to be each label, so the first column represents all the\nclips that were predicted to be silence, the second all those that were\npredicted to be unknown words, the third \"yes\", and so on.\nEach row represents clips by their correct, ground truth labels. The first row\nis all the clips that were silence, the second clips that were unknown words,\nthe third \"yes\", etc.\nThis matrix can be more useful than just a single accuracy score because it\ngives a good summary of what mistakes the network is making. In this example you\ncan see that all of the entries in the first row are zero, apart from the\ninitial one. Because the first row is all the clips that are actually silence,\nthis means that none of them were mistakenly labeled as words, so we have no\nfalse negatives for silence. This shows the network is already getting pretty\ngood at distinguishing silence from words.\nIf we look down the first column though, we see a lot of non-zero values. The\ncolumn represents all the clips that were predicted to be silence, so positive\nnumbers outside of the first cell are errors. This means that some clips of real\nspoken words are actually being predicted to be silence, so we do have quite a\nfew false positives.\nA perfect model would produce a confusion matrix where all of the entries were\nzero apart from a diagonal line through the center. Spotting deviations from\nthat pattern can help you figure out how the model is most easily confused, and\nonce you've identified the problems you can address them by adding more data or\ncleaning up categories.\nValidation\nAfter the confusion matrix, you should see a line like this:\n`I0730 16:57:38.073777 55030 train.py:245] Step 400: Validation accuracy = 26.3%\n(N=3093)`\nIt's good practice to separate your data set into three categories. The largest\n(in this case roughly 80% of the data) is used for training the network, a\nsmaller set (10% here, known as \"validation\") is reserved for evaluation of the\naccuracy during training, and another set (the last 10%, \"testing\") is used to\nevaluate the accuracy once after the training is complete.\nThe reason for this split is that there's always a danger that networks will\nstart memorizing their inputs during training. By keeping the validation set\nseparate, you can ensure that the model works with data it's never seen before.\nThe testing set is an additional safeguard to make sure that you haven't just\nbeen tweaking your model in a way that happens to work for both the training and\nvalidation sets, but not a broader range of inputs.\nThe training script automatically separates the data set into these three\ncategories, and the logging line above shows the accuracy of model when run on\nthe validation set. Ideally, this should stick fairly close to the training\naccuracy. If the training accuracy increases but the validation doesn't, that's\na sign that overfitting is occurring, and your model is only learning things\nabout the training clips, not broader patterns that generalize.\nTensorboard\nA good way to visualize how the training is progressing is using Tensorboard. By\ndefault, the script saves out events to /tmp/retrain_logs, and you can load\nthese by running:\n`tensorboard --logdir /tmp/retrain_logs`\nThen navigate to http://localhost:6006 in your browser,\nand you'll see charts and graphs showing your models progress.\n\n\n\nTraining Finished\nAfter a few hours of training (depending on your machine's speed), the script\nshould have completed all 18,000 steps. It will print out a final confusion\nmatrix, along with an accuracy score, all run on the testing set. With the\ndefault settings, you should see an accuracy of between 85% and 90%.\nBecause audio recognition is particularly useful on mobile devices, next we'll\nexport it to a compact format that's easy to work with on those platforms. To do\nthat, run this command line:\n`python tensorflow/examples/speech_commands/freeze.py \\\n--start_checkpoint=/tmp/speech_commands_train/conv.ckpt-18000 \\\n--output_file=/tmp/my_frozen_graph.pb`\nOnce the frozen model has been created, you can test it with the `label_wav.py`\nscript, like this:\n`python tensorflow/examples/speech_commands/label_wav.py \\\n--graph=/tmp/my_frozen_graph.pb \\\n--labels=/tmp/speech_commands_train/conv_labels.txt \\\n--wav=/tmp/speech_dataset/left/a5d485dc_nohash_0.wav`\nThis should print out three labels:\n`left (score = 0.81477)\nright (score = 0.14139)\n_unknown_ (score = 0.03808)`\nHopefully \"left\" is the top score since that's the correct label, but since the\ntraining is random it may not for the first file you try. Experiment with some\nof the other .wav files in that same folder to see how well it does.\nThe scores are between zero and one, and higher values mean the model is more\nconfident in its prediction.\nRunning the Model in an Android App\nThe easiest way to see how this model works in a real application is to download\nthe prebuilt Android demo applications\nand install them on your phone. You'll see 'TF Speech' appear in your app list,\nand opening it will show you the same list of action words we've just trained\nour model on, starting with \"Yes\" and \"No\". Once you've given the app permission\nto use the microphone, you should be able to try saying those words and see them\nhighlighted in the UI when the model recognizes one of them.\nYou can also build this application yourself, since it's open source and\navailable as part of the TensorFlow repository on github.\nBy default it downloads\na pretrained model from tensorflow.org,\nbut you can easily\nreplace it with a model you've trained yourself.\nIf you do this, you'll need to make sure that the constants in\nthe main SpeechActivity Java source file\nlike `SAMPLE_RATE` and `SAMPLE_DURATION` match any changes you've made to the\ndefaults while training. You'll also see that there's a\nJava version of the RecognizeCommands module\nthat's very similar to the C++ version in this tutorial. If you've tweaked\nparameters for that, you can also update them in SpeechActivity to get the same\nresults as in your server testing.\nThe demo app updates its UI list of results automatically based on the labels\ntext file you copy into assets alongside your frozen graph, which means you can\neasily try out different models without needing to make any code changes. You\nwill need to update `LABEL_FILENAME` and `MODEL_FILENAME` to point to the files\nyou've added if you change the paths though.\nHow does this Model Work?\nThe architecture used in this tutorial is based on some described in the paper\nConvolutional Neural Networks for Small-footprint Keyword\nSpotting.\nIt was chosen because it's comparatively simple, quick to train, and easy to\nunderstand, rather than being state of the art. There are lots of different\napproaches to building neural network models to work with audio, including\nrecurrent networks or dilated\n(atrous)\nconvolutions.\nThis tutorial is based on the kind of convolutional network that will feel very\nfamiliar to anyone who's worked with image recognition. That may seem surprising\nat first though, since audio is inherently a one-dimensional continuous signal\nacross time, not a 2D spatial problem.\nWe solve that issue by defining a window of time we believe our spoken words\nshould fit into, and converting the audio signal in that window into an image.\nThis is done by grouping the incoming audio samples into short segments, just a\nfew milliseconds long, and calculating the strength of the frequencies across a\nset of bands. Each set of frequency strengths from a segment is treated as a\nvector of numbers, and those vectors are arranged in time order to form a\ntwo-dimensional array. This array of values can then be treated like a\nsingle-channel image, and is known as a\nspectrogram. If you want to view\nwhat kind of image an audio sample produces, you can run the `wav_to_spectrogram\ntool:\n`bazel run tensorflow/examples/wav_to_spectrogram:wav_to_spectrogram -- \\\n--input_wav=/tmp/speech_dataset/happy/ab00c4b2_nohash_0.wav \\\n--output_image=/tmp/spectrogram.png`\nIf you open up `/tmp/spectrogram.png` you should see something like this:\n\n\n\nBecause of TensorFlow's memory order, time in this image is increasing from top\nto bottom, with frequencies going from left to right, unlike the usual\nconvention for spectrograms where time is left to right. You should be able to\nsee a couple of distinct parts, with the first syllable \"Ha\" distinct from\n\"ppy\".\nBecause the human ear is more sensitive to some frequencies than others, it's\nbeen traditional in speech recognition to do further processing to this\nrepresentation to turn it into a set of Mel-Frequency Cepstral\nCoefficients, or MFCCs\nfor short. This is also a two-dimensional, one-channel representation so it can\nbe treated like an image too. If you're targeting general sounds rather than\nspeech you may find you can skip this step and operate directly on the\nspectrograms.\nThe image that's produced by these processing steps is then fed into a\nmulti-layer convolutional neural network, with a fully-connected layer followed\nby a softmax at the end. You can see the definition of this portion in\ntensorflow/examples/speech_commands/models.py.\nStreaming Accuracy\nMost audio recognition applications need to run on a continuous stream of audio,\nrather than on individual clips. A typical way to use a model in this\nenvironment is to apply it repeatedly at different offsets in time and average\nthe results over a short window to produce a smoothed prediction. If you think\nof the input as an image, it's continuously scrolling along the time axis. The\nwords we want to recognize can start at any time, so we need to take a series of\nsnapshots to have a chance of having an alignment that captures most of the\nutterance in the time window we feed into the model. If we sample at a high\nenough rate, then we have a good chance of capturing the word in multiple\nwindows, so averaging the results improves the overall confidence of the\nprediction.\nFor an example of how you can use your model on streaming data, you can look at\ntest_streaming_accuracy.cc.\nThis uses the\nRecognizeCommands\nclass to run through a long-form input audio, try to spot words, and compare\nthose predictions against a ground truth list of labels and times. This makes it\na good example of applying a model to a stream of audio signals over time.\nYou'll need a long audio file to test it against, along with labels showing\nwhere each word was spoken. If you don't want to record one yourself, you can\ngenerate some synthetic test data using the `generate_streaming_test_wav`\nutility. By default this will create a ten minute .wav file with words roughly\nevery three seconds, and a text file containing the ground truth of when each\nword was spoken. These words are pulled from the test portion of your current\ndataset, mixed in with background noise. To run it, use:\n`bazel run tensorflow/examples/speech_commands:generate_streaming_test_wav`\nThis will save a .wav file to `/tmp/speech_commands_train/streaming_test.wav`,\nand a text file listing the labels to\n`/tmp/speech_commands_train/streaming_test_labels.txt`. You can then run\naccuracy testing with:\n`bazel run tensorflow/examples/speech_commands:test_streaming_accuracy -- \\\n--graph=/tmp/my_frozen_graph.pb \\\n--labels=/tmp/speech_commands_train/conv_labels.txt \\\n--wav=/tmp/speech_commands_train/streaming_test.wav \\\n--ground_truth=/tmp/speech_commands_train/streaming_test_labels.txt \\\n--verbose`\nThis will output information about the number of words correctly matched, how\nmany were given the wrong labels, and how many times the model triggered when\nthere was no real word spoken. There are various parameters that control how the\nsignal averaging works, including `--average_window_ms` which sets the length of\ntime to average results over, `--clip_stride_ms` which is the time between\napplications of the model, `--suppression_ms` which stops subsequent word\ndetections from triggering for a certain time after an initial one is found, and\n`--detection_threshold`, which controls how high the average score must be\nbefore it's considered a solid result.\nYou'll see that the streaming accuracy outputs three numbers, rather than just\nthe one metric used in training. This is because different applications have\nvarying requirements, with some being able to tolerate frequent incorrect\nresults as long as real words are found (high recall), while others very focused\non ensuring the predicted labels are highly likely to be correct even if some\naren't detected (high precision). The numbers from the tool give you an idea of\nhow your model will perform in an application, and you can try tweaking the\nsignal averaging parameters to tune it to give the kind of performance you want.\nTo understand what the right parameters are for your application, you can look\nat generating an ROC\ncurve to help\nyou understand the tradeoffs.\nRecognizeCommands\nThe streaming accuracy tool uses a simple decoder contained in a small C++ class\ncalled\nRecognizeCommands.\nThis class is fed the output of running the TensorFlow model over time, it\naverages the signals, and returns information about a label when it has enough\nevidence to think that a recognized word has been found. The implementation is\nfairly small, just keeping track of the last few predictions and averaging them,\nso it's easy to port to other platforms and languages as needed. For example,\nit's convenient to do something similar at the Java level on Android, or Python\non the Raspberry Pi. As long as these implementations share the same logic, you\ncan tune the parameters that control the averaging using the streaming test\ntool, and then transfer them over to your application to get similar results.\nAdvanced Training\nThe defaults for the training script are designed to produce good end to end\nresults in a comparatively small file, but there are a lot of options you can\nchange to customize the results for your own requirements.\nCustom Training Data\nBy default the script will download the Speech Commands\ndataset, but\nyou can also supply your own training data. To train on your own data, you\nshould make sure that you have at least several hundred recordings of each sound\nyou would like to recognize, and arrange them into folders by class. For\nexample, if you were trying to recognize dog barks from cat miaows, you would\ncreate a root folder called `animal_sounds`, and then within that two\nsub-folders called `bark` and `miaow`. You would then organize your audio files\ninto the appropriate folders.\nTo point the script to your new audio files, you'll need to set `--data_url=` to\ndisable downloading of the Speech Commands dataset, and\n`--data_dir=/your/data/folder/` to find the files you've just created.\nThe files themselves should be 16-bit little-endian PCM-encoded WAVE format. The\nsample rate defaults to 16,000, but as long as all your audio is consistently\nthe same rate (the script doesn't support resampling) you can change this with\nthe `--sample_rate` argument. The clips should also all be roughly the same\nduration. The default expected duration is one second, but you can set this with\nthe `--clip_duration_ms` flag. If you have clips with variable amounts of\nsilence at the start, you can look at word alignment tools to standardize them\n(here's a quick and dirty approach you can use\ntoo).\nOne issue to watch out for is that you may have very similar repetitions of the\nsame sounds in your dataset, and these can give misleading metrics if they're\nspread across your training, validation, and test sets. For example, the Speech\nCommands set has people repeating the same word multiple times. Each one of\nthose repetitions is likely to be pretty close to the others, so if training was\noverfitting and memorizing one, it could perform unrealistically well when it\nsaw a very similar copy in the test set. To avoid this danger, Speech Commands\ntries to ensure that all clips featuring the same word spoken by a single person\nare put into the same partition. Clips are assigned to training, test, or\nvalidation sets based on a hash of their filename, to ensure that the\nassignments remain steady even as new clips are added and avoid any training\nsamples migrating into the other sets. To make sure that all a given speaker's\nwords are in the same bucket,\nthe hashing function\nignores anything in a filename after 'nohash' when calculating the\nassignments. This means that if you have file names like `pete_nohash_0.wav` and\n`pete_nohash_1.wav`, they're guaranteed to be in the same set.\nUnknown Class\nIt's likely that your application will hear sounds that aren't in your training\nset, and you'll want the model to indicate that it doesn't recognize the noise\nin those cases. To help the network learn what sounds to ignore, you need to\nprovide some clips of audio that are neither of your classes. To do this, you'd\ncreate `quack`, `oink`, and `moo` subfolders and populate them with noises from\nother animals your users might encounter. The `--wanted_words` argument to the\nscript defines which classes you care about, all the others mentioned in\nsubfolder names will be used to populate an `_unknown_` class during training.\nThe Speech Commands dataset has twenty words in its unknown classes, including\nthe digits zero through nine and random names like \"Sheila\".\nBy default 10% of the training examples are picked from the unknown classes, but\nyou can control this with the `--unknown_percentage` flag. Increasing this will\nmake the model less likely to mistake unknown words for wanted ones, but making\nit too large can backfire as the model might decide it's safest to categorize\nall words as unknown!\nBackground Noise\nReal applications have to recognize audio even when there are other irrelevant\nsounds happening in the environment. To build a model that's robust to this kind\nof interference, we need to train against recorded audio with similar\nproperties. The files in the Speech Commands dataset were captured on a variety\nof devices by users in many different environments, not in a studio, so that\nhelps add some realism to the training. To add even more, you can mix in random\nsegments of environmental audio to the training inputs. In the Speech Commands\nset there's a special folder called `_background_noise_` which contains\nminute-long WAVE files with white noise and recordings of machinery and everyday\nhousehold activity.\nSmall snippets of these files are chosen at random and mixed at a low volume\ninto clips during training. The loudness is also chosen randomly, and controlled\nby the `--background_volume` argument as a proportion where 0 is silence, and 1\nis full volume. Not all clips have background added, so the\n`--background_frequency` flag controls what proportion have them mixed in.\nYour own application might operate in its own environment with different\nbackground noise patterns than these defaults, so you can supply your own audio\nclips in the `_background_noise_` folder. These should be the same sample rate\nas your main dataset, but much longer in duration so that a good set of random\nsegments can be selected from them.\nSilence\nIn most cases the sounds you care about will be intermittent and so it's\nimportant to know when there's no matching audio. To support this, there's a\nspecial `_silence_` label that indicates when the model detects nothing\ninteresting. Because there's never complete silence in real environments, we\nactually have to supply examples with quiet and irrelevant audio. For this, we\nreuse the `_background_noise_` folder that's also mixed in to real clips,\npulling short sections of the audio data and feeding those in with the ground\ntruth class of `_silence_`. By default 10% of the training data is supplied like\nthis, but the `--silence_percentage` can be used to control the proportion. As\nwith unknown words, setting this higher can weight the model results in favor of\ntrue positives for silence, at the expense of false negatives for words, but too\nlarge a proportion can cause it to fall into the trap of always guessing\nsilence.\nTime Shifting\nAdding in background noise is one way of distorting the training data in a\nrealistic way to effectively increase the size of the dataset, and so increase\noverall accuracy, and time shifting is another. This involves a random offset in\ntime of the training sample data, so that a small part of the start or end is\ncut off and the opposite section is padded with zeroes. This mimics the natural\nvariations in starting time in the training data, and is controlled with the\n`--time_shift_ms` flag, which defaults to 100ms. Increasing this value will\nprovide more variation, but at the risk of cutting off important parts of the\naudio. A related way of augmenting the data with realistic distortions is by\nusing time stretching and pitch\nscaling,\nbut that's outside the scope of this tutorial.\nCustomizing the Model\nThe default model used for this script is pretty large, taking over 800 million\nFLOPs for each inference and using 940,000 weight parameters. This runs at\nusable speeds on desktop machines or modern phones, but it involves too many\ncalculations to run at interactive speeds on devices with more limited\nresources. To support these use cases, there's a couple of alternatives\navailable:\nlow_latency_conv\nBased on the 'cnn-one-fstride4' topology described in the Convolutional\nNeural Networks for Small-footprint Keyword Spotting\npaper.\nThe accuracy is slightly lower than 'conv' but the number of weight parameters\nis about the same, and it only needs 11 million FLOPs to run one prediction,\nmaking it much faster.\nTo use this model, you specify `--model_architecture=low_latency_conv` on\nthe command line. You'll also need to update the training rates and the number\nof steps, so the full command will look like:\n`python tensorflow/examples/speech_commands/train \\\n--model_architecture=low_latency_conv \\\n--how_many_training_steps=20000,6000 \\\n--learning_rate=0.01,0.001`\nThis asks the script to train with a learning rate of 0.01 for 20,000 steps, and\nthen do a fine-tuning pass of 6,000 steps with a 10x smaller rate.\nlow_latency_svdf\nBased on the topology presented in the Compressing Deep Neural Networks using a\nRank-Constrained Topology paper.\nThe accuracy is also lower than 'conv' but it only uses about 750 thousand\nparameters, and most significantly, it allows for an optimized execution at\ntest time (i.e. when you will actually use it in your application), resulting\nin 750 thousand FLOPs.\nTo use this model, you specify `--model_architecture=low_latency_svdf` on\nthe command line, and update the training rates and the number\nof steps, so the full command will look like:\n`python tensorflow/examples/speech_commands/train \\\n--model_architecture=low_latency_svdf \\\n--how_many_training_steps=100000,35000 \\\n--learning_rate=0.01,0.005`\nNote that despite requiring a larger number of steps than the previous two\ntopologies, the reduced number of computations means that training should take\nabout the same time, and at the end reach an accuracy of around 85%.\nYou can also further tune the topology fairly easily for computation and\naccuracy by changing these parameters in the SVDF layer:\n\nrank - The rank of the approximation (higher typically better, but results in\n         more computation).\nnum_units - Similar to other layer types, specifies the number of nodes in\n              the layer (more nodes better quality, and more computation).\n\nRegarding runtime, since the layer allows optimizations by caching some of the\ninternal neural network activations, you need to make sure to use a consistent\nstride (e.g. 'clip_stride_ms' flag) both when you freeze the graph, and when\nexecuting the model in streaming mode (e.g. test_streaming_accuracy.cc).\nOther parameters to customize If you want to experiment with customizing\nmodels, a good place to start is by tweaking the spectrogram creation\nparameters. This has the effect of altering the size of the input image to the\nmodel, and the creation code in\nmodels.py\nwill adjust the number of computations and weights automatically to fit with\ndifferent dimensions. If you make the input smaller, the model will need fewer\ncomputations to process it, so it can be a great way to trade off some accuracy\nfor improved latency. The `--window_stride_ms` controls how far apart each\nfrequency analysis sample is from the previous. If you increase this value, then\nfewer samples will be taken for a given duration, and the time axis of the input\nwill shrink. The `--dct_coefficient_count` flag controls how many buckets are\nused for the frequency counting, so reducing this will shrink the input in the\nother dimension. The `--window_size_ms` argument doesn't affect the size, but\ndoes control how wide the area used to calculate the frequencies is for each\nsample. Reducing the duration of the training samples, controlled by\n`--clip_duration_ms`, can also help if the sounds you're looking for are short,\nsince that also reduces the time dimension of the input. You'll need to make\nsure that all your training data contains the right audio in the initial portion\nof the clip though.\nIf you have an entirely different model in mind for your problem, you may find\nthat you can plug it into\nmodels.py\nand have the rest of the script handle all of the preprocessing and training\nmechanics. You would add a new clause to `create_model`, looking for the name of\nyour architecture and then calling a model creation function. This function is\ngiven the size of the spectrogram input, along with other model information, and\nis expected to create TensorFlow ops to read that in and produce an output\nprediction vector, and a placeholder to control the dropout rate. The rest of\nthe script will handle integrating this model into a larger graph doing the\ninput calculations and applying softmax and a loss function to train it.\nOne common problem when you're adjusting models and training hyper-parameters is\nthat not-a-number values can creep in, thanks to numerical precision issues. In\ngeneral you can solve these by reducing the magnitude of things like learning\nrates and weight initialization functions, but if they're persistent you can\nenable the `--check_nans` flag to track down the source of the errors. This will\ninsert check ops between most regular operations in TensorFlow, and abort the",
    "tag": "tensorflow"
  },
  {
    "title": "Recurrent Neural Networks for Drawing Classification",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/tutorials/sequences/recurrent_quickdraw.md",
    "content": "Recurrent Neural Networks for Drawing Classification\nQuick, Draw! is a game where a player is challenged to draw a number of\nobjects and see if a computer can recognize the drawing.\nThe recognition in Quick, Draw! is performed by a classifier that takes the\nuser input, given as a sequence of strokes of points in x and y, and recognizes\nthe object category that the user tried to draw.\nIn this tutorial we'll show how to build an RNN-based recognizer for this\nproblem. The model will use a combination of convolutional layers, LSTM layers,\nand a softmax output layer to classify the drawings:\n\nThe figure above shows the structure of the model that we will build in this\ntutorial. The input is a drawing that is encoded as a sequence of strokes of\npoints in x, y, and n, where n indicates whether a the point is the first point\nin a new stroke.\nThen, a series of 1-dimensional convolutions is applied. Then LSTM layers are\napplied and the sum of the outputs of all LSTM steps is fed into a softmax layer\nto make a classification decision among the classes of drawings that we know.\nThis tutorial uses the data from actual Quick, Draw! games that is publicly\navailable. This dataset contains of 50M\ndrawings in 345 categories.\nRun the tutorial code\nTo try the code for this tutorial:\n\nInstall TensorFlow if you haven't already.\nDownload the\n    tutorial code.\n\nDownload the data in `TFRecord` format from\n    here\n    and unzip it. More details about\n    how to obtain the original Quick, Draw! data\n    and how to convert that to TFRecord files\n    is available below.\n\n\nExecute the tutorial code with the following command to train the RNN-based\n    model described in this tutorial. Make sure to adjust the paths to point to\n    the unzipped data from the download in step 3.\n\n\n`shell\n  python train_model.py \\\n    --training_data=rnn_tutorial_data/training.tfrecord-?????-of-????? \\\n    --eval_data=rnn_tutorial_data/eval.tfrecord-?????-of-????? \\\n    --classes_file=rnn_tutorial_data/training.tfrecord.classes`\nTutorial details\nDownload the data\nWe make the data that we use in this tutorial available as `TFRecord` files\ncontaining `TFExamples`. You can download the data from here:\nhttp://download.tensorflow.org/data/quickdraw_tutorial_dataset_v1.tar.gz (~1GB).\nAlternatively you can download the original data in `ndjson` format from the\nGoogle cloud and convert it to the `TFRecord` files containing `TFExamples`\nyourself as described in the next section.\nOptional: Download the full Quick Draw Data\nThe full Quick, Draw!\ndataset is available on Google Cloud\nStorage as ndjson files separated by category. You can\nbrowse the list of files in Cloud\nConsole.\nTo download the data we recommend using\ngsutil to\ndownload the entire dataset. Note that the original .ndjson files require\ndownloading ~22GB.\nThen use the following command to check that your gsutil installation works and\nthat you can access the data bucket:\n`shell\ngsutil ls -r \"gs://quickdraw_dataset/full/simplified/*\"`\nwhich will output a long list of files like the following:\n`shell\ngs://quickdraw_dataset/full/simplified/The Eiffel Tower.ndjson\ngs://quickdraw_dataset/full/simplified/The Great Wall of China.ndjson\ngs://quickdraw_dataset/full/simplified/The Mona Lisa.ndjson\ngs://quickdraw_dataset/full/simplified/aircraft carrier.ndjson\n...`\nThen create a folder and download the dataset there.\n`shell\nmkdir rnn_tutorial_data\ncd rnn_tutorial_data\ngsutil -m cp \"gs://quickdraw_dataset/full/simplified/*\" .`\nThis download will take a while and download a bit more than 23GB of data.\nOptional: Converting the data\nTo convert the `ndjson` files to\nTFRecord files containing\ntf.train.Example\nprotos run the following command.\n`shell\n   python create_dataset.py --ndjson_path rnn_tutorial_data \\\n      --output_path rnn_tutorial_data`\nThis will store the data in 10 shards of\nTFRecord files with 10000 items\nper class for the training data and 1000 items per class as eval data.\nThis conversion process is described in more detail in the following.\nThe original QuickDraw data is formatted as `ndjson` files where each line\ncontains a JSON object like the following:\n`json\n{\"word\":\"cat\",\n \"countrycode\":\"VE\",\n \"timestamp\":\"2017-03-02 23:25:10.07453 UTC\",\n \"recognized\":true,\n \"key_id\":\"5201136883597312\",\n \"drawing\":[\n   [\n     [130,113,99,109,76,64,55,48,48,51,59,86,133,154,170,203,214,217,215,208,186,176,162,157,132],\n     [72,40,27,79,82,88,100,120,134,152,165,184,189,186,179,152,131,114,100,89,76,0,31,65,70]\n   ],[\n     [76,28,7],\n     [136,128,128]\n   ],[\n     [76,23,0],\n     [160,164,175]\n   ],[\n     [87,52,37],\n     [175,191,204]\n   ],[\n     [174,220,246,251],\n     [134,132,136,139]\n   ],[\n     [175,255],\n     [147,168]\n   ],[\n     [171,208,215],\n     [164,198,210]\n   ],[\n     [130,110,108,111,130,139,139,119],\n     [129,134,137,144,148,144,136,130]\n   ],[\n     [107,106],\n     [96,113]\n   ]\n ]\n}`\nFor our purpose of building a classifier we only care about the fields \"`word`\"\nand \"`drawing`\". While parsing the ndjson files, we process them line by line\nusing a function that converts the strokes from the `drawing` field into a\ntensor of size `[number of points, 3]` containing the differences of consecutive\npoints. This function also returns the class name as a string.\n`python\ndef parse_line(ndjson_line):\n  \"\"\"Parse an ndjson line and return ink (as np array) and classname.\"\"\"\n  sample = json.loads(ndjson_line)\n  class_name = sample[\"word\"]\n  inkarray = sample[\"drawing\"]\n  stroke_lengths = [len(stroke[0]) for stroke in inkarray]\n  total_points = sum(stroke_lengths)\n  np_ink = np.zeros((total_points, 3), dtype=np.float32)\n  current_t = 0\n  for stroke in inkarray:\n    for i in [0, 1]:\n      np_ink[current_t:(current_t + len(stroke[0])), i] = stroke[i]\n    current_t += len(stroke[0])\n    np_ink[current_t - 1, 2] = 1  # stroke_end\n  # Preprocessing.\n  # 1. Size normalization.\n  lower = np.min(np_ink[:, 0:2], axis=0)\n  upper = np.max(np_ink[:, 0:2], axis=0)\n  scale = upper - lower\n  scale[scale == 0] = 1\n  np_ink[:, 0:2] = (np_ink[:, 0:2] - lower) / scale\n  # 2. Compute deltas.\n  np_ink[1:, 0:2] -= np_ink[0:-1, 0:2]\n  np_ink = np_ink[1:, :]\n  return np_ink, class_name`\nSince we want the data to be shuffled for writing we read from each of the\ncategory files in random order and write to a random shard.\nFor the training data we read the first 10000 items for each class and for the\neval data we read the next 1000 items for each class.\nThis data is then reformatted into a tensor of shape `[num_training_samples,\nmax_length, 3]`. Then we determine the bounding box of the original drawing in\nscreen coordinates and normalize the size such that the drawing has unit height.\n\nFinally, we compute the differences between consecutive points and store these\nas a `VarLenFeature` in a\ntensorflow.Example\nunder the key `ink`. In addition we store the `class_index` as a single entry\n`FixedLengthFeature` and the `shape` of the `ink` as a `FixedLengthFeature` of\nlength 2.\nDefining the model\nTo define the model we create a new `Estimator`. If you want to read more about\nestimators, we recommend this tutorial.\nTo build the model, we:\n\n\nreshape the input back into the original shape - where the mini batch is\n    padded to the maximal length of its contents. In addition to the ink data we\n    also have the lengths for each example and the target class. This happens in\n    the function _get_input_tensors.\n\n\npass the input through to a series of convolution layers in\n    _add_conv_layers.\n\n\npass the output of the convolutions into a series of bidirectional LSTM\n    layers in _add_rnn_layers. At the end of that, the\n    outputs for each time step are summed up to have a compact, fixed length\n    embedding of the input.\n\n\nclassify this embedding using a softmax layer in\n    _add_fc_layers.\n\n\nIn code this looks like:\n`python\ninks, lengths, targets = _get_input_tensors(features, targets)\nconvolved = _add_conv_layers(inks)\nfinal_state = _add_rnn_layers(convolved, lengths)\nlogits =_add_fc_layers(final_state)`\n_get_input_tensors\nTo obtain the input features we first obtain the shape from the features dict\nand then create a 1D tensor of size `[batch_size]` containing the lengths of the\ninput sequences. The ink is stored as a SparseTensor in the features dict which\nwe convert into a dense tensor and then reshape to be `[batch_size, ?, 3]`. And\nfinally, if targets were passed in we make sure they are stored as a 1D tensor\nof size `[batch_size]`\nIn code this looks like this:\n`python\nshapes = features[\"shape\"]\nlengths = tf.squeeze(\n    tf.slice(shapes, begin=[0, 0], size=[params[\"batch_size\"], 1]))\ninks = tf.reshape(\n    tf.sparse_tensor_to_dense(features[\"ink\"]),\n    [params[\"batch_size\"], -1, 3])\nif targets is not None:\n  targets = tf.squeeze(targets)`\n_add_conv_layers\nThe desired number of convolution layers and the lengths of the filters is\nconfigured through the parameters `num_conv` and `conv_len` in the `params`\ndict.\nThe input is a sequence where each point has dimensionality 3. We are going to\nuse 1D convolutions where we treat the 3 input features as channels. That means\nthat the input is a `[batch_size, length, 3]` tensor and the output will be a\n`[batch_size, length, number_of_filters]` tensor.\n`python\nconvolved = inks\nfor i in range(len(params.num_conv)):\n  convolved_input = convolved\n  if params.batch_norm:\n    convolved_input = tf.layers.batch_normalization(\n        convolved_input,\n        training=(mode == tf.estimator.ModeKeys.TRAIN))\n  # Add dropout layer if enabled and not first convolution layer.\n  if i > 0 and params.dropout:\n    convolved_input = tf.layers.dropout(\n        convolved_input,\n        rate=params.dropout,\n        training=(mode == tf.estimator.ModeKeys.TRAIN))\n  convolved = tf.layers.conv1d(\n      convolved_input,\n      filters=params.num_conv[i],\n      kernel_size=params.conv_len[i],\n      activation=None,\n      strides=1,\n      padding=\"same\",\n      name=\"conv1d_%d\" % i)\nreturn convolved, lengths`\n_add_rnn_layers\nWe pass the output from the convolutions into bidirectional LSTM layers for\nwhich we use a helper function from contrib.\n`python\noutputs, _, _ = contrib_rnn.stack_bidirectional_dynamic_rnn(\n    cells_fw=[cell(params.num_nodes) for _ in range(params.num_layers)],\n    cells_bw=[cell(params.num_nodes) for _ in range(params.num_layers)],\n    inputs=convolved,\n    sequence_length=lengths,\n    dtype=tf.float32,\n    scope=\"rnn_classification\")`\nsee the code for more details and how to use `CUDA` accelerated implementations.\nTo create a compact, fixed-length embedding, we sum up the output of the LSTMs.\nWe first zero out the regions of the batch where the sequences have no data.\n`python\nmask = tf.tile(\n    tf.expand_dims(tf.sequence_mask(lengths, tf.shape(outputs)[1]), 2),\n    [1, 1, tf.shape(outputs)[2]])\nzero_outside = tf.where(mask, outputs, tf.zeros_like(outputs))\noutputs = tf.reduce_sum(zero_outside, axis=1)`\n_add_fc_layers\nThe embedding of the input is passed into a fully connected layer which we then\nuse as a softmax layer.\n`python\ntf.layers.dense(final_state, params.num_classes)`\nLoss, predictions, and optimizer\nFinally, we need to add a loss, a training op, and predictions to create the\n`ModelFn`:\n```python\ncross_entropy = tf.reduce_mean(\n    tf.nn.sparse_softmax_cross_entropy_with_logits(\n        labels=targets, logits=logits))\nAdd the optimizer.\ntrain_op = tf.contrib.layers.optimize_loss(\n    loss=cross_entropy,\n    global_step=tf.train.get_global_step(),\n    learning_rate=params.learning_rate,\n    optimizer=\"Adam\",\n    # some gradient clipping stabilizes training in the beginning.\n    clip_gradients=params.gradient_clipping_norm,\n    summaries=[\"learning_rate\", \"loss\", \"gradients\", \"gradient_norm\"])\npredictions = tf.argmax(logits, axis=1)\nreturn model_fn_lib.ModelFnOps(\n    mode=mode,\n    predictions={\"logits\": logits,\n                 \"predictions\": predictions},\n    loss=cross_entropy,\n    train_op=train_op,\n    eval_metric_ops={\"accuracy\": tf.metrics.accuracy(targets, predictions)})\n```\nTraining and evaluating the model\nTo train and evaluate the model we can rely on the functionalities of the\n`Estimator` APIs and easily run training and evaluation with the `Experiment`\nAPIs:\n`python\n  estimator = tf.estimator.Estimator(\n      model_fn=model_fn,\n      model_dir=output_dir,\n      config=config,\n      params=model_params)\n  # Train the model.\n  tf.contrib.learn.Experiment(\n      estimator=estimator,\n      train_input_fn=get_input_fn(\n          mode=tf.contrib.learn.ModeKeys.TRAIN,\n          tfrecord_pattern=FLAGS.training_data,\n          batch_size=FLAGS.batch_size),\n      train_steps=FLAGS.steps,\n      eval_input_fn=get_input_fn(\n          mode=tf.contrib.learn.ModeKeys.EVAL,\n          tfrecord_pattern=FLAGS.eval_data,\n          batch_size=FLAGS.batch_size),\n      min_eval_frequency=1000)`\nNote that this tutorial is just a quick example on a relatively small dataset to\nget you familiar with the APIs of recurrent neural networks and estimators. Such\nmodels can be even more powerful if you try them on a large dataset.\nWhen training the model for 1M steps you can expect to get an accuracy of\napproximately of approximately 70% on the top-1 candidate. Note that this\naccuracy is sufficient to build the quickdraw game because of the game dynamics\nthe user will be able to adjust their drawing until it is ready. Also, the game\ndoes not use the top-1 candidate only but accepts a drawing as correct if the",
    "tag": "tensorflow"
  },
  {
    "title": "Recurrent Neural Networks",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/r1/tutorials/sequences/recurrent.md",
    "content": "Recurrent Neural Networks\nIntroduction\nSee Understanding LSTM Networks{:.external}\nfor an introduction to recurrent neural networks and LSTMs.\nLanguage Modeling\nIn this tutorial we will show how to train a recurrent neural network on\na challenging task of language modeling. The goal of the problem is to fit a\nprobabilistic model which assigns probabilities to sentences. It does so by\npredicting next words in a text given a history of previous words. For this\npurpose we will use the Penn Tree Bank\n(PTB) dataset, which is a popular benchmark for measuring the quality of these\nmodels, whilst being small and relatively fast to train.\nLanguage modeling is key to many interesting problems such as speech\nrecognition, machine translation, or image captioning. It is also fun --\ntake a look here.\nFor the purpose of this tutorial, we will reproduce the results from\nZaremba et al., 2014\n(pdf), which achieves very good quality\non the PTB dataset.\nTutorial Files\nThis tutorial references the following files from `models/tutorials/rnn/ptb` in the TensorFlow models repo:\nFile | Purpose\n--- | ---\n`ptb_word_lm.py` | The code to train a language model on the PTB dataset.\n`reader.py` | The code to read the dataset.\nDownload and Prepare the Data\nThe data required for this tutorial is in the `data/` directory of the\nPTB dataset from Tomas Mikolov's webpage.\nThe dataset is already preprocessed and contains overall 10000 different words,\nincluding the end-of-sentence marker and a special symbol (\\<unk>) for rare\nwords. In `reader.py`, we convert each word to a unique integer identifier,\nin order to make it easy for the neural network to process the data.\nThe Model\nLSTM\nThe core of the model consists of an LSTM cell that processes one word at a\ntime and computes probabilities of the possible values for the next word in the\nsentence. The memory state of the network is initialized with a vector of zeros\nand gets updated after reading each word. For computational reasons, we will\nprocess data in mini-batches of size `batch_size`.  In this example, it is\nimportant to note that `current_batch_of_words` does not correspond to a\n\"sentence\" of words.  Every word in a batch should correspond to a time t.\nTensorFlow will automatically sum the gradients of each batch for you.\nFor example:\n```\n t=0  t=1    t=2  t=3     t=4\n[The, brown, fox, is,     quick]\n[The, red,   fox, jumped, high]\nwords_in_dataset[0] = [The, The]\nwords_in_dataset[1] = [brown, red]\nwords_in_dataset[2] = [fox, fox]\nwords_in_dataset[3] = [is, jumped]\nwords_in_dataset[4] = [quick, high]\nbatch_size = 2, time_steps = 5\n```\nThe basic pseudocode is as follows:\n```python\nwords_in_dataset = tf.placeholder(tf.float32, [time_steps, batch_size, num_features])\nlstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\nInitial state of the LSTM memory.\nstate = lstm.zero_state(batch_size, dtype=tf.float32)\nprobabilities = []\nloss = 0.0\nfor current_batch_of_words in words_in_dataset:\n    # The value of state is updated after processing each batch of words.\n    output, state = lstm(current_batch_of_words, state)\n\n\n```# The LSTM output can be used to make next word predictions\nlogits = tf.matmul(output, softmax_w) + softmax_b\nprobabilities.append(tf.nn.softmax(logits))\nloss += loss_function(probabilities, target_words)\n```\n\n\n```\nTruncated Backpropagation\nBy design, the output of a recurrent neural network (RNN) depends on arbitrarily\ndistant inputs. Unfortunately, this makes backpropagation computation difficult.\nIn order to make the learning process tractable, it is common practice to create\nan \"unrolled\" version of the network, which contains a fixed number\n(`num_steps`) of LSTM inputs and outputs. The model is then trained on this\nfinite approximation of the RNN. This can be implemented by feeding inputs of\nlength `num_steps` at a time and performing a backward pass after each\nsuch input block.\nHere is a simplified block of code for creating a graph which performs\ntruncated backpropagation:\n```python\nPlaceholder for the inputs in a given iteration.\nwords = tf.placeholder(tf.int32, [batch_size, num_steps])\nlstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\nInitial state of the LSTM memory.\ninitial_state = state = lstm.zero_state(batch_size, dtype=tf.float32)\nfor i in range(num_steps):\n    # The value of state is updated after processing each batch of words.\n    output, state = lstm(words[:, i], state)\n\n\n```# The rest of the code.\n# ...\n```\n\n\nfinal_state = state\n```\nAnd this is how to implement an iteration over the whole dataset:\n```python\nA numpy array holding the state of LSTM after each batch of words.\nnumpy_state = initial_state.eval()\ntotal_loss = 0.0\nfor current_batch_of_words in words_in_dataset:\n    numpy_state, current_loss = session.run([final_state, loss],\n        # Initialize the LSTM state from the previous iteration.\n        feed_dict={initial_state: numpy_state, words: current_batch_of_words})\n    total_loss += current_loss\n```\nInputs\nThe word IDs will be embedded into a dense representation (see the\nVector Representations Tutorial) before feeding to\nthe LSTM. This allows the model to efficiently represent the knowledge about\nparticular words. It is also easy to write:\n```python\nembedding_matrix is a tensor of shape [vocabulary_size, embedding size]\nword_embeddings = tf.nn.embedding_lookup(embedding_matrix, word_ids)\n```\nThe embedding matrix will be initialized randomly and the model will learn to\ndifferentiate the meaning of words just by looking at the data.\nLoss Function\nWe want to minimize the average negative log probability of the target words:\n$$ \\text{loss} = -\\frac{1}{N}\\sum_{i=1}^{N} \\ln p_{\\text{target}_i} $$\nIt is not very difficult to implement but the function\n`sequence_loss_by_example` is already available, so we can just use it here.\nThe typical measure reported in the papers is average per-word perplexity (often\njust called perplexity), which is equal to\n$$e^{-\\frac{1}{N}\\sum_{i=1}^{N} \\ln p_{\\text{target}_i}} = e^{\\text{loss}} $$\nand we will monitor its value throughout the training process.\nStacking multiple LSTMs\nTo give the model more expressive power, we can add multiple layers of LSTMs\nto process the data. The output of the first layer will become the input of\nthe second and so on.\nWe have a class called `MultiRNNCell` that makes the implementation seamless:\n```python\ndef lstm_cell():\n  return tf.contrib.rnn.BasicLSTMCell(lstm_size)\nstacked_lstm = tf.contrib.rnn.MultiRNNCell(\n    [lstm_cell() for _ in range(number_of_layers)])\ninitial_state = state = stacked_lstm.zero_state(batch_size, tf.float32)\nfor i in range(num_steps):\n    # The value of state is updated after processing each batch of words.\n    output, state = stacked_lstm(words[:, i], state)\n\n\n```# The rest of the code.\n# ...\n```\n\n\nfinal_state = state\n```\nRun the Code\nBefore running the code, download the PTB dataset, as discussed at the beginning\nof this tutorial.  Then, extract the PTB dataset underneath your home directory\nas follows:\n`bsh\ntar xvfz simple-examples.tgz -C $HOME`\n(Note: On Windows, you may need to use\nother tools.)\nNow, clone the TensorFlow models repo\nfrom GitHub. Run the following commands:\n`bsh\ncd models/tutorials/rnn/ptb\npython ptb_word_lm.py --data_path=$HOME/simple-examples/data/ --model=small`\nThere are 3 supported model configurations in the tutorial code: \"small\",\n\"medium\" and \"large\". The difference between them is in size of the LSTMs and\nthe set of hyperparameters used for training.\nThe larger the model, the better results it should get. The `small` model should\nbe able to reach perplexity below 120 on the test set and the `large` one below\n80, though it might take several hours to train.\nWhat Next?\nThere are several tricks that we haven't mentioned that make the model better,\nincluding:\n\ndecreasing learning rate schedule,\ndropout between the LSTM layers.\n",
    "tag": "tensorflow"
  },
  {
    "title": "Mailing lists",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/community/mailing-lists.md",
    "content": "Mailing lists\nAs a community, we do much of our collaboration on public mailing lists. Please\nnote that if you're looking for help using TensorFlow,\nTensorFlow Forum,\nStack Overflow, and\nGitHub issues are the best\ninitial places to look. To receive a roundup of updates from the TensorFlow team each quarter, subscribe to the TensorFlow newsletter.\nGeneral TensorFlow lists and forums\n\nannounce -\n    Low-volume announcements of new releases.\ndiscuss -\n    General community discussion around TensorFlow.\ndevelopers -\n    Discussion for developers contributing to TensorFlow.\ndocumentation -\n    Discussion for contributing to TensorFlow documentation. See\n    community translations\n    for language-specific docs lists.\ntesting -\n    Discussion and questions on TensorFlow 2 testing.\n\nProject-specific lists\nThese projects inside the TensorFlow GitHub organization have lists dedicated to their communities:\n\nhub - Discussion\n    and collaboration around\n    TensorFlow Hub.\nmagenta-discuss -\n    General discussion about Magenta\n    development and directions.\ntensor2tensor -\n    Discussion and peer support for Tensor2Tensor.\ntfjs-announce -\n    Announcements of new TensorFlow.js releases.\ntfjs - Discussion\n    and peer support for TensorFlow.js.\ntflite -\n    Discussion and peer support for TensorFlow Lite.\ntfprobability -\n    Discussion and peer support for TensorFlow Probability.\ntfx -\n    Discussion and collaboration around TensorFlow Extended (TFX).\ntpu-users -\n    Community discussion and support for TPU users.\nxla-dev - Discussion for\n    developers contributing to the XLA\n    compiler.\n\nSpecial Interest Groups\nTensorFlow's\nSpecial Interest Groups (SIGs)\nsupport community collaboration on particular project focuses. Members of these\ngroups work together to build and support TensorFlow related projects. While their\narchives are public, different SIGs have their own membership policies.\n\naddons -\n    Supporting SIG Addons, for extensions to TensorFlow that confirm to the\n    stable API.\nbuild -\n    Supporting SIG Build, for build, distribution and packaging of TensorFlow.\nio - Supporting SIG\n    IO, for file systems and formats not available in core TensorFlow.\njvm - Supporting\n    SIG JVM, building Java and JVM support for TensorFlow.\nkeras - Keras users\n    mailing list, for design reviews and discussions relating to SIG Keras.\nmicro -\n    Supporting SIG Micro, focusing on low power TF Lite deployment.\nmlir - Supporting\n    SIG MLIR, collaboration around MLIR, Multi-Level Intermediate\n    Representation.\nnetworking -\n    Supporting SIG Networking, for adding network protocols other than gRPC.\nrust - Supporting\n    SIG Rust, for the Rust language bindings.\nswift -\n    Supporting SIG Swift, developing Swift for TensorFlow.\ntensorboard -\n",
    "tag": "tensorflow"
  },
  {
    "title": "SIG playbook",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/community/sig_playbook.md",
    "content": "SIG playbook\nScope of a SIG\nTensorFlow hosts Special Interest Groups (SIGs) to focus collaboration on particular areas. SIGs do their work in public. To join and contribute, review the work of the group, and get in touch with the SIG leader. Membership policies vary on a per-SIG basis.\nThe ideal scope for a SIG meets a well-defined domain, where the majority of\nparticipation is from the community. Additionally, there should be\nsufficient evidence that there are community members willing to engage and\ncontribute should the interest group be established.\nNot all SIGs will have the same level of energy, breadth of scope, or governance\nmodels, so expect some variability.\nSee the complete list of TensorFlow SIGs.\nNon-goals: What a SIG is not\nSIGs are intended is to facilitate collaboration on shared work. A SIG is\ntherefore:\n\nNot a support forum: a mailing list and a SIG is not the same thing.\nNot immediately required: early on in a project's life, you may not know\n    if you have shared work or collaborators.\nNot free labor: energy is required to grow and coordinate the work\n    collaboratively.\n\nOur approach to SIG creation will be conservative\u2014thanks to the ease of starting projects on GitHub, there are many avenues where collaboration can happen without the need for a SIG.\nSIG lifecycle\nResearch and consultation\nProposers of groups should gather evidence for approval, as specified below.\nSome possible avenues to consider are:\n\nA well-defined problem or set of problems the group would solve.\nConsultation with community members who would benefit, assessing both the\n    benefit and their willingness to commit.\nFor existing projects, evidence from issues and PRs that contributors care\n    about the topic.\nPotential goals for the group to achieve.\nResource requirements of running the group.\n\nEven if the need for a SIG seems self-evident, the research and consultation is\nstill important to the success of the group.\nCreating the new group\nThe new group should follow the below process for chartering. In particular, it\nmust demonstrate:\n\nA clear purpose and benefit to TensorFlow (either around a sub-project or\n    application area)\nTwo or more contributors willing to act as group leads, existence of other\n    contributors, and evidence of demand for the group\nResources it will initially require (usually, mailing list and regular VC\n    call.) \n\nApproval for the group will be given by a decision of the TF Community Team,\ndefined as being the maintainers of the tensorflow/community project. The team\nwill consult other stakeholders as necessary.\nBefore entering the formal parts of the process, it is advisable to consult with\nthe TensorFlow community team, community-team@tensorflow.org. It is highly\nlikely that conversation and iteration will be required before the SIG request\nis ready.\nThe formal request for the new group is done by submitting a charter as a PR to\ntensorflow/community, and including the request in the comments on the PR (see\ntemplate below). On approval, the PR for the group will be merged and the\nrequired resources created.\nTemplate Request for New SIG\nThis template will be available in the community repo:\nSIG-request-template.md.\nChartering\nEach group will be established with a charter, and be governed by the TensorFlow\ncode of conduct. Archives of the group will be public. Membership may either be\nopen to all without approval, or available on request, pending approval of the\ngroup administrator.\nThe charter must nominate an administrator. As well as an administrator, the\ngroup must include at least one person as lead (these may be the same person),\nwho will serve as point of contact for coordination as required with the TensorFlow\ncommunity team.\nThis charter will be posted initially to the group mailing list. The community\nrepository in the TensorFlow GitHub organization will archive such documents and\npolicies (example from Kubernetes).\nAs any group evolves its practices and conventions, we expect it to document\nthese within the relevant part of the community repository.\nCollaboration and inclusion\nWhile it is not mandated, the group should choose to make use of collaboration\nvia scheduled conference call or chat channels to conduct meetings. Any such\nmeetings should be advertised on the mailing list, and notes posted to the\nmailing list afterwards. Regular meeting helps drive accountability and progress\nin a SIG.\nTensorFlow community team members will proactively monitor and encourage the\ngroup to discussion and action as appropriate.\nLaunching\nRequired activities:\n\nNotifying TensorFlow general discussion groups\n    (discuss@,\n    developers@).\nAdding SIG to the community pages on TensorFlow web site. \n\nOptional activities:\n\nCreating a blog post for the TensorFlow blog community.\n\nHealth and termination of SIGs\nThe TensorFlow community team will make a best effort to ensure the health of\nSIGs. From time to time it will request the SIG lead to provide a report of the\nSIG's work, which will be used to inform the broader TensorFlow community of the\nactivity of the group.\nIf a SIG no longer has a useful purpose or interested community, it may be\narchived and cease operation. The TF community team reserves the right to\narchive such inactive SIGs, in order to maintain the health of the project at\nlarge, though it is a less preferable outcome. A SIG may also opt to disband if",
    "tag": "tensorflow"
  },
  {
    "title": "The TensorFlow RFC process",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/community/contribute/rfc_process.md",
    "content": "The TensorFlow RFC process\nEvery new TensorFlow feature begins life as a Request for Comment (RFC).\nAn RFC is a document that describes a requirement and the proposed changes that\nwill solve it. Specifically, the RFC will:\n\nBe formatted according to the\n    RFC template.\nBe submitted as a pull request to the\n    community/rfcs\n    directory.\nBe subject to discussion and a review meeting prior to acceptance.\n\nThe purpose of a TensorFlow Request for Comments (RFC) is to engage the\nTensorFlow community in development, by getting feedback from stakeholders and\nexperts, and communicating design changes broadly.\nHow to submit an RFC\n\n\nBefore submitting an RFC, discuss your aims with project contributors and\n    maintainers and get early feedback. Use the developer mailing list for the\n    project concerned (developers@tensorflow.org, or the list for the relevant\n    SIG).\n\n\nDraft your RFC.\n\nRead the design review criteria\nFollow the\n    RFC template.\nName your RFC file `YYYYMMDD-descriptive-name.md`, where `YYYYMMDD` is\n    the date of submission, and `descriptive-name` relates to the title of\n    your RFC. (For instance, if your RFC is titled Parallel Widgets API,\n    you might use the filename `20180531-parallel-widgets.md`.\nIf you have images or other auxiliary files, create a directory of the\n    form `YYYYMMDD-descriptive-name` in which to store those files.\n\nAfter writing the RFC draft, get feedback from maintainers and contributors\nbefore submitting it.\nWriting implementation code is not a requirement, but it may help design\ndiscussions.\n\n\nRecruit a sponsor.\n\nA sponsor must be a maintainer of the project.\nIdentify the sponsor in the RFC, before posting the PR.\n\nYou may post an RFC without a sponsor, but if within a month of posting\nthe PR there is still no sponsor, it will be closed.\n\n\nSubmit your RFC as a pull request to\n    tensorflow/community/rfcs.\nInclude the header table and the contents of the Objective section in the\ncomment of your pull request, using Markdown. For an example, please see\nthis example RFC. Include\nthe GitHub handles of co-authors, reviewers, and sponsors.\nAt the top of the PR identify how long the comment period will be. This\nshould be a minimum of two weeks from posting the PR.\n\n\nEmail the developer mailing list with a brief description, a link to the PR\n    and a request for review. Follow the format of previous mailings, as you can\n    see in\n    this example.\n\n\nThe sponsor will request a review committee meeting, no sooner than two\n    weeks after the RFC PR is posted. If discussion is lively, wait until it has\n    settled before going to review. The goal of the review meeting is to resolve\n    minor issues; consensus should be reached on major issues beforehand.\n\n\nThe meeting may approve the RFC, reject it, or require changes before it can\n    be considered again. Approved RFCs will be merged into\n    community/rfcs,\n    and rejected RFCs will have their PRs closed.\n\n\nRFC participants\nMany people are involved in the RFC process:\n\n\nRFC author \u2014 one or more community members who write an RFC and are\n    committed to championing it through the process\n\n\nRFC sponsor \u2014 a maintainer who sponsors the RFC and will shepherd it\n    through the RFC review process\n\n\nreview committee \u2014 a group of maintainers who have the responsibility of\n    recommending the adoption of the RFC\n\n\nAny community member may help by providing feedback on whether the RFC\n    will meet their needs.\n\n\nRFC sponsors\nA sponsor is a project maintainer responsible for ensuring the best possible\noutcome of the RFC process. This includes:\n\nAdvocating for the proposed design.\nGuiding the RFC to adhere to existing design and style conventions.\nGuiding the review committee to come to a productive consensus.\nIf changes are requested by the review committee, ensure these are made and\n    seek subsequent approval from the committee members.\nIf the RFC moves to implementation:\nEnsuring proposed implementation adheres to the design.\nCoordinate with appropriate parties to successfully land implementation.\n\n\n\nRFC review committees\nThe review committee decides on a consensus basis whether to approve, reject, or\nrequest changes. They are responsible for:\n\nEnsuring that substantive items of public feedback have been accounted for.\nAdding their meeting notes as comments to the PR.\nProviding reasons for their decisions.\n\nThe constitution of a review committee may change according to the particular\ngovernance style and leadership of each project. For core TensorFlow, the\ncommittee will consist of contributors to the TensorFlow project who have\nexpertise in the domain area concerned.\nCommunity members and the RFC process\nThe purpose of RFCs is to ensure the community is well represented and served by\nnew changes to TensorFlow. It is the responsibility of community members to\nparticipate in reviewing RFCs where they have an interest in the outcome.\nCommunity members who are interested in an RFC should:\n\nProvide feedback as soon as possible to allow adequate time for\n    consideration.\nRead RFCs thoroughly before providing feedback.\nBe civil and constructive.\n\nImplementing new features\nOnce an RFC has been approved, implementation can begin.\nIf you are working on new code to implement an RFC:\n\nMake sure you understand the feature and the design approved in the RFC. Ask\n    questions and discuss the approach before beginning work.\nNew features must include new unit tests that verify the feature works as\n    expected. It's a good idea to write these tests before writing the code.\nFollow the TensorFlow Code Style Guide\nAdd or update relevant API documentation. Reference the RFC in the new\n    documentation.\nFollow any other guidelines laid out in the `CONTRIBUTING.md` file in the\n    project repo you're contributing to.\nRun unit tests before submitting your code.\nWork with the RFC sponsor to successfully land the new code.\n\nKeeping the bar high\nWhile we encourage and celebrate every contributor, the bar for RFC acceptance\nis kept intentionally high. A new feature may be rejected or need significant\nrevision at any one of these stages:\n\nInitial design conversations on the relevant mailing list.\nFailure to recruit a sponsor.\nCritical objections during the feedback phase.\nFailure to achieve consensus during the design review.\nConcerns raised during implementation (for example: inability to achieve\n    backwards compatibility, concerns about maintenance).\n\nIf this process is functioning well, RFCs are expected to fail in the earlier,\nrather than later, stages. An approved RFC is no guarantee of a commitment to\nimplement, and acceptance of a proposed RFC implementation is still subject to\nthe usual code review process.\nIf you have any questions about this process, feel free to ask on the developers\nmailing list or file an issue in",
    "tag": "tensorflow"
  },
  {
    "title": "Contribute to the TensorFlow community",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/community/contribute/community.md",
    "content": "Contribute to the TensorFlow community\nAn open source project isn't just about the code, it's also about the community of users, developers, writers, researchers, and other contributors. You can help grow and support this community.\nPlease read the TensorFlow Code and Collaboration governance.\nCommunity support\nMany people ask questions about TensorFlow on the TensorFlow Forum. Answering those questions and pointing people to the relevant documentation is a great service to the community.\nSome users also ask support questions as GitHub issues. We try to discourage this, as GitHub issues are not the best place to ask for technical support. However, if you notice these issues, you are encouraged to answer them and point people to the relevant documentation.\nTensorFlow Forum\nThe TensorFlow Forum is a central platform for community discussion and support. It brings our community together to share ideas, best practices and use cases related to TensorFlow. We foster an open and welcoming environment according to the TensorFlow Code of Conduct.\nThe TensorFlow Forum is organized by categories, subcategories and tags. We encourage you to create an account and follow categories and tags of interest. When you create a new post, select the most appropriate category or subcategory and tags to help other users find your topic.\nFor more information on Discourse features, read the Discourse New User Guide.\nBecome a Forum expert\nDiscourse uses trust levels to reward increasing levels of participation in the forum. The Forum facilitates learning by doing, letting you to collect badges that are displayed on your profile. This is a great way to be recognized for helping fellow community members. The more you invest in helping community members, the more badges and forum tools you will unlock.\nCertain groups, such as TensorFlow Team members and Machine Learning GDEs, display a special icon for easier identification.\nCommunication\nThe TensorFlow community has a number of formal and informal ways of keeping in touch.\nGitHub\nThe primary communication about work on TensorFlow happens in the TensorFlow repositories on GitHub. This is the place to discuss bugs, new features, and in-progress work.\nMailing lists\nMost communication happens on the TensorFlow Forum. The following mailing lists are still used for announcements and contributor conversations. Note that they are not intended to provide technical support.\n\nannounce@tensorflow.org \u2014 All major releases and important announcements are sent to this mailing group. We recommend that you join this list if you depend on TensorFlow in any way.\ndevelopers@tensorflow.org \u2014 Discussion for developers who are contributing to TensorFlow.\n\nFor more information on project-specific communication, visit the Contribute to SIGs page.\nBlog and social media\nThe TensorFlow Blog is full of great content both from our team at Google and the broader community. We'd love to hear what you have to say, so if you would like to submit an article for review, please contact us at tensorflow-blog@google.com. Note that we receive many great submissions, and setting expectations, we can only publish a few.\nOn Twitter we share the latest and greatest from our community, and our YouTube channel has free educational content to help you create, understand and deploy models for a variety of applications.\nTensorFlow Community Spotlight\nThe TensorFlow Community Spotlight Program provides an opportunity to showcase your passion projects using TensorFlow. Submit your project for a chance to be featured and recognized on TensorFlow\u2019s Twitter account.\nFollow the #TFCommunitySpotlight hashtag and find out more about past winners here.\nUser groups\nTensorFlow User Groups (or TFUGs, for short) are local communities of developers and researchers around the world. If you don\u2019t have a TFUG in your country or city, we encourage you to start one by reaching out to tfug-help@tensorflow.org.\nEvents\nThe TensorFlow team hosts and supports events all around the world! If your TFUG is planning an upcoming event or meetup, please let our Community know by posting about it on the TensorFlow Forum under the Events category.",
    "tag": "tensorflow"
  },
  {
    "title": "Contribute to TensorFlow Special Interest Groups (SIGs)",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/community/contribute/sigs.md",
    "content": "Contribute to TensorFlow Special Interest Groups (SIGs)\nThe TensorFlow Special Interest Groups (TF SIGs) organize community contributions to key parts of the TensorFlow ecosystem. SIG leads and members work together to build and support important TensorFlow use cases.\nSIGs are led by members of the open source community, including industry collaborators and Machine Learning Google Developer Experts (ML GDEs). TensorFlow's success is due in large part to their hard work and contributions.\nWe encourage you to join a SIG working on the area of TensorFlow's ecosystem you care most about. Not all SIGs will have the same level of energy, breadth of scope, or governance models \u2014 browse our SIG charters to learn more. Stay connected with SIG leads and members on the TensorFlow Forum, where you can subscribe to preferred tags and learn more about the regular SIG meetings.\nSIG Addons\nSIG Addons builds and maintains a repository of community contributions that conform to well-established API patterns, but implement new functionality not available in core TensorFlow. \nTensorFlow natively supports a large number of operators, layers, metrics, losses, optimizers, and more. However, in a fast-moving field like ML, there are many new developments that cannot be integrated into core TensorFlow (because their broad applicability is not yet clear, or it is mostly used by a smaller subset of the community). SIG Addons enables users to introduce new extensions to the TensorFlow ecosystem in a sustainable manner.\nSIG Addons on GitHub Contributing Discuss on the Forum\nSIG Build\nSIG Build improves and extends the TensorFlow build process. SIG Build maintains a repository showcasing resources, guides, tools, and builds contributed by the community, for the community.\nSIG Build on GitHub Contributing Discuss on the Forum\nSIG IO\nSIG IO maintains TensorFlow I/O, a collection of file systems and file formats that are not available in TensorFlow's built-in support.\nSIG IO on GitHub Contributing Discuss on the Forum\nSIG JVM\nSIG JVM maintains the TF Java bindings to let users use JVM for building, training and running machine learning models.\nJava and other JVM languages, such as Scala or Kotlin, are frequently used in small-to-large enterprises all over the world, which makes TensorFlow a strategic choice for adopting machine learning at a large scale.\nSIG JVM on GitHub Contributing Discuss on the Forum\nSIG Models\nSIG Models focuses on enabling contributions to the state-of-the-art model implementation in TensorFlow 2, and sharing best practices of using TensorFlow 2 for state-of-the-art research. Subgroups orient around different machine learning applications (Vision, NLP, etc.).\nSIG Models host discussions and collaborations around the TensorFlow Model Garden and TensorFlow Hub. Learn how to contribute on GitHub below, or discuss Research & Models on the Forum.\nTensorFlow Model Garden on GitHub Contributing \nTensorFlow Hub on GitHub Contributing \nSIG Micro\nSIG Micro discusses and shares updates on TensorFlow Lite for Microcontrollers, a port of TensorFlow Lite designed to run machine learning models on DSPs, microcontrollers and other devices with limited memory.\nTensorFlow Lite Micro on GitHub Contributing Discuss on the Forum\nSIG MLIR\nSIG MLIR maintains MLIR dialects and utilities for TensorFlow, XLA and TF Lite, providing high performance compilers and optimization techniques that can be applied to TensorFlow graphs and code generation. Their overarching goal is to create common intermediate representation (IR) that reduces the cost to bring up new hardware, and improve usability for existing TensorFlow users.\nSIG MLIR on GitHub Contributing Discuss on the Forum\nSIG Networking\nSIG Networking maintains the TensorFlow Networking repository for platform-specific networking extensions to core TensorFlow and related utilities.\nSIG Networking on GitHub Discuss on the Forum\nSIG Recommenders\nSIG Recommenders maintains a collection of projects related to large-scale recommendation systems built upon TensorFlow contributed and maintained by the community. Those contributions are complementary to TensorFlow Core and TensorFlow Recommenders.\nSIG Recommenders on GitHub Contributing Discuss on the Forum\nSIG Rust\nSIG Rust maintains idiomatic Rust language bindings for TensorFlow.\nSIG Rust on GitHub Contributing Discuss on the Forum\nSIG TensorBoard\nSIG TensorBoard facilitates discussion around TensorBoard\u2014a suite of tools for inspecting, debugging and optimizing TensorFlow programs.\nTensorBoard on GitHub Contributing Discuss on the Forum\nSIG TF.js\nSIG TF.js facilitates community-contributed components to TensorFlow.js and offers project support through the SIG.\nTensorFlow.js on GitHub Contributing Discuss on the Forum\nSIG TFX-Addons\nSIG TFX-Addons accelerates the sharing of customizations and additions to meet the needs of production ML, expand the vision, and help drive new directions for TensorFlow Extended (TFX) and the ML community.\nSIG TFX-Addons on GitHub Contributing Discuss on the Forum\nNew SIGs",
    "tag": "tensorflow"
  },
  {
    "title": "TensorFlow testing best practices",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/community/contribute/tests.md",
    "content": "TensorFlow testing best practices\nThese are the recommended practices for testing code in the\nTensorFlow repository.\nBefore you get started\nBefore you contribute source code to a TensorFlow project, please review the\n`CONTRIBUTING.md` file in the GitHub repo of the project. (For example, see the\nCONTRIBUTING.md file for the core TensorFlow repo.)\nAll code contributors are required to sign a\nContributor License Agreement (CLA).\nGeneral principles\nOnly depend on what you use in your BUILD rules\nTensorFlow is a large library, and depending on the full package when\nwriting a unit test for its submodules has been a common practice. However, this\ndisables the `bazel` dependency-based analysis. This means that continuous\nintegration systems cannot intelligently eliminate unrelated tests for\npresubmit/postsubmit runs. If you only depend on the submodules that you are\ntesting in your `BUILD` file, you will save time for all TensorFlow developers,\nand a lot of valuable computation power.\nHowever, modifying your build dependency to omit the full TF targets brings some\nlimitations for what you can import in your Python code. You will not be able to\nuse the `import tensorflow as tf` statement in your unit tests anymore. But this\nis a worthwhile tradeoff since as it saves all developers from running thousands\nof unnecessary tests.\nAll code should have unit tests\nFor any code you write, you should also write its unit tests. If you write a new\nfile `foo.py`, you should place its unit tests in `foo_test.py` and submit it\nwithin the same change. Aim for >90% incremental test coverage for all your\ncode.\nAvoid using native bazel test rules in TF\nTF has a lot of subtleties when running tests. We have worked to hide all of\nthose complexities in our bazel macros. To avoid having to deal with those, use\nthe following instead of the native test rules. Note that all of these are\ndefined in `tensorflow/tensorflow.bzl`\nFor CC tests, use `tf_cc_test`, `tf_gpu_cc_test`, `tf_gpu_only_cc_test`.\nFor python tests, use `tf_py_test` or `gpu_py_test`.\nIf you need something really close to the native `py_test` rule, please use the\none defined in tensorflow.bzl instead. You just need to add the following line\nat the top of the BUILD file: `load(\u201ctensorflow/tensorflow.bzl\u201d, \u201cpy_test\u201d)`\nBe aware where the test executes\nWhen you write a test, our test infra can take care of running your tests on\nCPU, GPU and accelerators if you write them accordingly. We have automated tests\nthat run on Linux, macos, windows, that have systems with or without GPUs. You\nsimply need to pick one of the macros listed above, and then use tags to limit\nwhere they are executed.\n\n\n`manual` tag will exclude your test from running anywhere. This includes\nmanual test executions that use patterns such as `bazel test tensorflow/\u2026`\n\n\n`no_oss` will exclude your test from running in the official TF OSS test\ninfrastructure.\n\n\n`no_mac` or `no_windows` tags can be used to exclude your test from relevant\noperating system test suites.\n\n`no_gpu` tag can be used to exclude your test from running in GPU test suites.\n\nVerify tests run in expected test suites\nTF has quite a few test suites. Sometimes, they may be confusing to set up.\nThere might be different problems that cause your tests to be omitted from\ncontinuous builds. Thus, you should verify your tests are executing as expected.\nTo do this:\n\nWait for your presubmits on your Pull Request(PR) to run to completion.\nScroll to the bottom of your PR to see the status checks.\nClick the \u201cDetails\u201d link at the right side of any Kokoro check.\nCheck the \u201cTargets\u201d list to find your newly added targets.\n\nEach class/unit should have its own unit test file\nSeparate test classes help us better isolate failures and resources. They lead\nto much shorter and easier to read test files. Therefore, all your Python files\nshould have at least one corresponding test file (For each `foo.py`, it should\nhave `foo_test.py`). For more elaborate tests, such as integration tests that\nrequire different setups, it is fine to add more test files.\nSpeed and running times\nSharding should be used as little as possible\nInstead of sharding please consider:\n* Making your tests smaller\n* If the above is not possible, split the tests up\nSharding helps reduce the overall latency of a test, but the same can be\nachieved by breaking up tests to smaller targets. Splitting tests gives us a\nfiner level of control on each test, minimizing unnecessary presubmit runs and\nreducing the coverage loss from a buildcop disabling an entire target due to a\nmisbehaving testcase. Moreover, sharding incurs hidden costs that are not so\nobvious, such as running all test initialization code for all shards. This issue\nhas been escalated to us by infra teams as a source that creates extra load.\nSmaller tests are better\nThe quicker your tests run, the more likely people will be to run your tests.\nOne extra second for your test can accumulate to hours of extra time spent\nrunning your test by developers and our infrastructure. Try to make your tests\nrun under 30 seconds (in non-opt mode!), and make them small. Only mark your\ntests as medium as a last resort. The infra does not run any large tests as\npresubmits or postsubmits! Therefore, only write a large test if you are going\nto arrange where it is going to run. Some tips to make tests run faster:\n\nRun less iterations of training in your test\nConsider using dependency injection to replace heavy dependencies of system\nunder test with simple fakes.\nConsider using smaller input data in unit tests\nIf nothing else works, try splitting up your test file.\n\nTest times should aim for half of test size timeout to avoid flakes\nWith `bazel` test targets, small tests have 1 minute timeouts. Medium test\ntimeouts are 5 minutes. Large tests are just not executed by the TensorFlow test\ninfra. However, many tests are not deterministic in the amount of time they\ntake. For various reasons your tests might take more time every now and then.\nAnd, if you mark a test that runs for 50 seconds on the average as small, your\ntest will flake if it schedules on a machine with an old CPU. Therefore, aim for\n30 second average running time for small tests. Aim for 2 minutes 30 seconds of\naverage running time for medium tests.\nReduce the number of samples and increase tolerances for training\nSlow running tests deter contributors. Running training in tests can be very\nslow. Prefer higher tolerances to be able to use less samples in your tests to\nkeep your tests sufficiently fast (2.5 minutes max).\nEliminate non-determinism and flakes\nWrite deterministic tests\nUnit tests should always be deterministic. All tests running on TAP and guitar\nshould run the same way every single time, if there is no code change affecting\nthem. To ensure this, below are some points to consider.\nAlways seed any source of stochasticity\nAny random number generator, or any other sources of stochasticity can cause\nflakiness. Therefore, each of these must be seeded. In addition to making tests\nless flaky, this makes all tests reproducible. Different ways to set some seeds\nyou may need to set in TF tests are:\n```python\nPython RNG\nimport random\nrandom.seed(42)\nNumpy RNG\nimport numpy as np\nnp.random.seed(42)\nTF RNG\nfrom tensorflow.python.framework import random_seed\nrandom_seed.set_seed(42)\n```\nAvoid using `sleep` in multithreaded tests\nUsing `sleep` function in tests can be a major cause of flakiness. Especially\nwhen using multiple threads, using sleep to wait for another thread will never\nbe determistic. This is due to system not being able to guarantee any ordering\nof execution of different threads or processes. Therefore, prefer deterministic\nsynchronization constructs such as mutexes.\nCheck if the test is flaky\nFlakes cause buildcops and developers to lose many hours. They are difficult to\ndetect, and they are difficult to debug. Even though there are automated systems\nto detect flakiness, they need to accumulate hundreds of test runs before they\ncan accurately denylist tests. Even when they detect, they denylist your tests\nand test coverage is lost. Therefore, test authors should check if their tests\nare flaky when writing tests. This can be easily done by running your test with\nthe flag: `--runs_per_test=1000`\nUse TensorFlowTestCase\nTensorFlowTestCase takes necessary precautions such as seeding all random number\ngenerators used to reduce flakiness as much as possible. As we discover and fix\nmore flakiness sources, these all will be added to TensorFlowTestCase.\nTherefore, you should use TensorFlowTestCase when writing tests for tensorflow.\nTensorFlowTestCase is defined here: `tensorflow/python/framework/test_util.py`\nWrite hermetic tests\nHermetic tests do not need any outside resources. They are packed with\neverything they need, and they just start any fake services they might need. Any\nservices other than your tests are sources for non determinism. Even with 99%\navailability of other services, network can flake, rpc response can be delayed,\nand you might end up with an inexplicable error message.",
    "tag": "tensorflow"
  },
  {
    "title": "Contribute to the TensorFlow API documentation",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/community/contribute/docs_ref.md",
    "content": "Contribute to the TensorFlow API documentation\n\nTestable docstrings\nTensorFlow uses DocTest to\ntest code snippets in Python docstrings. The snippet must be executable Python\ncode. To enable testing, prepend the line with `>>>` (three left-angle\nbrackets). For example, here's a excerpt from the `tf.concat` function in the\narray_ops.py\nsource file:\n```\ndef concat(values, axis, name=\"concat\"):\n  \"\"\"Concatenates tensors along one dimension.\n  ...\n\n\n\nt1 = [[1, 2, 3], [4, 5, 6]]\nt2 = [[7, 8, 9], [10, 11, 12]]\nconcat([t1, t2], 0)\n  \n\n\n\n<... more description or code snippets ...>\nArgs:\n    values: A list of `tf.Tensor` objects or a single `tf.Tensor`.\n    axis: 0-D `int32` `Tensor`.  Dimension along which to concatenate. Must be\n      in the range `[-rank(values), rank(values))`. As in Python, indexing for\n      axis is 0-based. Positive axis in the rage of `[0, rank(values))` refers\n      to `axis`-th dimension. And negative axis refers to `axis +\n      rank(values)`-th dimension.\n    name: A name for the operation (optional).\n\n\n```Returns:\n  A `tf.Tensor` resulting from concatenation of the input tensors.\n```\n\n\n\"\"\"\n`\n````\nNote: TensorFlow DocTest uses TensorFlow 2 and Python 3.\nTo assess reference documentation quality, see the example section of the\nTensorFlow 2 API Docs advice.\n(Be aware that the Task Tracker on this sheet is no longer in use.)\nMake the code testable with DocTest\nCurrently, many docstrings use backticks (```) to identify code. To make the\ncode testable with DocTest:\n\nRemove the backticks (```) and use the left-brackets (>>>) in front of each\n    line. Use (...) in front of continued lines.\nAdd a newline to separate DocTest snippets from Markdown text to\n    render properly on tensorflow.org.\n\nCustomizations\nTensorFlow uses a few customizations to the builtin doctest logic:\n\nIt does not compare float values as text: Float values are extracted from\n    the text and compared using `allclose` with liberal `atol` and `rtol`\n    tolerences. This allows :\nClearer docs - Authors don't need to include all decimal places.\nMore robust tests - Numerical changes in the underlying implementation\n    should never cause a doctest to fail.\n\n\nIt only checks the output if the author includes output for a line. This\n    allows for clearer docs because authors usually don't need to capture\n    irrelevant intermediate values to prevent them from being printed.\n\nDocstring considerations\n\nOverall: The goal of doctest is to provide documentation, and confirm that\n    the documentation works. This is different from unit-testing. So:\nKeep examples simple.\nAvoid long or complicated outputs.\nUse round numbers if possible.\n\n\nOutput format: The output of the snippet needs to be directly beneath the\n    code that\u2019s generating the output. Also, the output in the docstring has to\n    be exactly equal to what the output would be after the code is executed. See\n    the above example. Also, check out\n    this part in the\n    DocTest documentation. If the output exceeds the 80 line limit, you can put\n    the extra output on the new line and DocTest will recognize it. For example,\n    see multi-line blocks below.\nGlobals: The ``tf``, `np` and `os` modules are always\n    available in TensorFlow's DocTest.\n\nUse symbols: In DocTest you can directly access symbols defined in the\n    same file. To use a symbol that\u2019s not defined in the current file, please\n    use TensorFlow\u2019s public API `tf.xxx` instead of `xxx`. As you can see in the\n    example below, ``random.normal`` is accessed via\n    ``tf.random.normal``. This is because\n    ``random.normal`` is not visible in `NewLayer`.\n```\ndef NewLayer():\n  \"\"\"This layer does cool stuff.\nExample usage:\n\n\n\nx = tf.random.normal((1, 28, 28, 3))\nnew_layer = NewLayer(x)\nnew_layer\n  \n  \"\"\"\n```\n\n\n\n\n\nFloating point values: The TensorFlow doctest extracts float values from\n    the result strings, and compares using `np.allclose` with reasonable\n    tolerances (`atol=1e-6`, `rtol=1e-6`). This way authors do not need to worry\n    about overly precise docstrings causing failures due to numerical issues.\n    Simply paste in the expected value.\n\n\nNon-deterministic output: Use ellipsis(`...`) for the uncertain parts and\n    DocTest will ignore that substring.\n```\n\n\n\nx = tf.random.normal((1,))\nprint(x)\n\n```\n\n\n\n\n\nMulti-line blocks: DocTest is strict about the difference between a single\n    and a multi-line statement. Note the usage of (...) below:\n```\n\n\n\nif x > 0:\n...   print(\"X is positive\")\nmodel.compile(\n...   loss=\"mse\",\n...   optimizer=\"adam\")\n```\n\n\n\n\n\nExceptions: Exception details are ignored except the Exception that\u2019s\n    raised. See\n    this\n    for more details.\n```\n\n\n\nnp_var = np.array([1, 2])\ntf.keras.backend.is_keras_tensor(np_var)\nTraceback (most recent call last):\n...\nValueError: Unexpectedly found an instance of type `<class 'numpy.ndarray'>`.\n```\n\n\n\n\n\nUse a project-local copy of tf-doctest.\nNote: The tf-doctest utility is only setup to test source files within the\n`tensorflow` repository. If the files you are editing are in TensorFlow you can\nskip to the next section. Otherwise keep reading this section.\nSome API's in TensorFlow come from an external project:\n\n`tf.estimator` (from\n    tensorflow_estimator)\n`tf.summary` tensorboard)\n`tf.keras.preprocessing` (from\n    keras-preprocessing)\n\nIf you're working on an external project, or on TensorFlow APIs that are housed\nin an external project, these instructions won't work unless that project has\nits own local copy of `tf_doctest`, and you use that copy instead of\nTensorFlow's.\nFor example:\ntf_estimator_doctest.py.\nTest on your local machine\nThere are two ways to test the code in the docstring locally:\n\n\nIf you are only changing the docstring of a class/function/method, then you\n    can test it by passing that file's path to\n    tf_doctest.py.\n    For example:\n\n\n\n```python tf_doctest.py --file=<file_path>```\n\n\n\nThis will run it using your installed version of TensorFlow. To be sure\nyou're running the same code that you're testing:\n\nUse an up to date tf-nightly\n`pip install -U tf-nightly`\nRebase your pull request onto a recent pull from\n    TensorFlow's master branch.\n\n\n\nIf you are changing the code and the docstring of a class/function/method,\n    then you will need to\n    build TensorFlow from source. Once you are setup\n    to build from source, you can run the tests:\n\n\n\n```bazel run //tensorflow/tools/docs:tf_doctest```\n\n\n\nor\n\n\n\n```bazel run //tensorflow/tools/docs:tf_doctest -- --module=ops.array_ops```\n\n\n\n\n",
    "tag": "tensorflow"
  },
  {
    "title": "Contribute to TensorFlow",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/community/contribute/index.md",
    "content": "Contribute to TensorFlow\nThe TensorFlow ecosystem can only grow through the contributions of this\ncommunity. Thanks so much for your enthusiasm and your work\u2014we appreciate\neverything you do!\nCommunity values\nIn the interest of fostering an open and welcoming environment, contributors and\nmaintainers pledge to make participation in our project and our community a\nharassment-free experience for everyone\u2014regardless of age, body size,\ndisability, ethnicity, gender identity and expression, level of experience,\nnationality, personal appearance, race, religion, or sexual identity and\norientation.\nExamples of behaviors that contribute to creating a positive environment\ninclude:\n\nUse welcome and inclusive language.\nBe respectful of differing viewpoints and experiences.\nGracefully accept constructive criticism.\nFoster what's best for the community.\nShow empathy for other community members.\n\nDecisions are made based on technical merit and consensus. The TensorFlow\ncommunity aspires to treat everyone equally, and to value all contributions. For\nmore information on best practices in the TensorFlow community, please review\nour\nCode of Conduct.\nMake your first contribution\nThere are many ways to contribute to TensorFlow! You can contribute code, make\nimprovements to the TensorFlow API documentation, or add your Jupyter notebooks\nto the tensorflow/examples repo.\nThis guide provides everything you need to get started. Our most common\ncontributions include code, documentation, and community support.\n\nWrite code.\nImprove tests.\nImprove documentation.\nAnswer questions on\n  Stack Overflow.\nParticipate in the discussion on our\n  mailing lists.\nContribute example notebooks.\nInvestigate bugs and issues\n  on GitHub.\nReview and comment on\n  pull requests from other\n  developers.\nReport an issue.\nGive a \u201cthumbs up\u201d \ud83d\udc4d on issues that are relevant to you.\nReference TensorFlow in your blogs, papers, and articles.\nTalk about TensorFlow on social media.\n... even just starring/forking the repos you like on GitHub!\n\nTensorFlow was originally developed by researchers and engineers from the Google\nBrain team within Google's AI organization. Google open\nsourced TensorFlow in the hope of sharing technology with the external community\nand encouraging collaboration between researchers and industry. Since then,\nTensorFlow has grown into a thriving ecosystem of products, on a wide range of\nplatforms. But our goal is still to make machine learning accessible to anyone,",
    "tag": "tensorflow"
  },
  {
    "title": "TensorFlow documentation style guide",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/community/contribute/docs_style.md",
    "content": "TensorFlow documentation style guide\nBest practices\n\nFocus on user intent and audience.\nUse every-day words and keep sentences short.\nUse consistent sentence construction, wording, and capitalization.\nUse headings and lists to make your docs easier to scan.\nThe\n    Google Developer Docs Style Guide\n    is helpful.\n\nMarkdown\nWith a few exceptions, TensorFlow uses a Markdown syntax similar to\nGitHub Flavored Markdown\n(GFM). This section explains differences between GFM Markdown syntax and the\nMarkdown used for TensorFlow documentation.\nWrite about code\nInline mentions of code\nPut ``backticks`` around the following symbols when used in\ntext:\n\nArgument names: ``input``, ``x``,\n    ``tensor``\nReturned tensor names: ``output``,\n    ``idx``, ``out``\nData types: ``int32``, ``float``,\n    ``uint8``\nOther op names reference in text: ``list_diff()``,\n    ``shuffle()``\nClass names: ``tf.Tensor``, ``Strategy``\nFile name: ``image_ops.py``,\n    ``/path_to_dir/file_name``\nMath expressions or conditions: ``-1-input.dims() <= dim <=\n    input.dims()``\n\nCode blocks\nUse three backticks to open and close a code block. Optionally, specify the programming\nlanguage after the first backtick group, for example:\n\n\n```\n```python\n# some python code here\n```\n```\n\n\nLinks in Markdown and notebooks\nLinks between files in a repository\nUse relative links between files in a single GitHub repository. Include the file\nextension.\nFor example, this file you're reading is from the\nhttps://github.com/tensorflow/docs\nrepository. Therefore, it can use relative paths to link to other files in the same\nrepository like this:\n\n`[Basics](../../guide/basics.ipynb)` produces\nBasics.\n\nThis is the prefered approach because this way the links on\ntensorflow.org,\nGitHub{:.external} and\nColab{:.external}\nall work. Also, the reader stays in the same site when they click a link.\nNote: You should include the file extension\u2014such as `.ipynb` or `.md`\u2014for\nrelative links. It will rendered on `tensorflow.org` without an extension.\nExternal links\nFor links to files that are not in the current repository, use standard Markdown\nlinks with the full URI. Prefer to link to the\ntensorflow.org URI if it's available.\nTo link to source code, use a link starting with\nhttps://www.github.com/tensorflow/tensorflow/blob/master/, followed\nby the file name starting at the GitHub root.\nWhen linking off of tensorflow.org, include a\n`{:.external}` on the Markdown link so that the \"external link\" symbol is shown.\n\n`[GitHub](https://github.com/tensorflow/docs){:.external}` produces\n  GitHub{:.external}\n\nDo not include URI query parameters in the link:\n\nUse: `https://www.tensorflow.org/guide/data`\nNot: `https://www.tensorflow.org/guide/data?hl=en`\n\nImages\nThe advice in the previous section is for links to pages. Images are handled\ndifferently.\nGenerally, you should not check in images, and instead add the\nTensorFlow-Docs team to your PR, and ask\nthem to host the images on tensorflow.org.\nThis helps keep the size of your repository down.\nIf you do submit images to your repository, note that some systems do not handle\nrelative paths to images. Prefer to use a full URL pointing to the image's\neventual location on tensorflow.org.\nLinks to API documentation\nAPI links are converted when the site is published. To link to a symbol's API\nreference page, enclose the symbol path in backticks:\n\n``tf.data.Dataset`` produces\n    tf.data.Dataset\n\nFull paths are slightly preferred except for long paths. Paths\ncan be abbreviated by dropping the leading path components. Partial paths will\nbe converted to links if:\n\nThere is at least one `.` in the path, and\nThe partial path is unique within the project.\n\nAPI paths are linked for every project with a Python API published on\ntensorflow.org. You can easily link to multiple\nsubprojects from a single file by wrapping the API names with backticks.\nFor example:\n\n``tf.metrics``, ``tf_agents.metrics``,\n    ``text.metrics`` produces: `tf.metrics`,\n    `tf_agents.metrics`, `text.metrics`.\n\nFor symbols with multiple path aliases there is a slight preference for the\npath that matches the API-page on tensorflow.org.\nAll aliases will redirect to the correct page.\nMath in Markdown\nYou may use MathJax within TensorFlow when editing Markdown files, but note the\nfollowing:\n\nMathJax renders properly on tensorflow.org.\nMathJax does not render properly on GitHub.\nThis notation can be off-putting to unfamiliar developers.\nFor consistency tensorflow.org follows the\n    same  rules as Jupyter/Colab.\n\nUse `$$` around a block of MathJax:\n\n\n```$$\nE=\\frac{1}{2n}\\sum_x\\lVert (y(x)-y'(x)) \\rVert^2\n$$```\n\n\n$$\nE=\\frac{1}{2n}\\sum_x\\lVert (y(x)-y'(x)) \\rVert^2\n$$\nWrap inline MathJax expressions with `$ ... $`:\n\n\n```\nThis is an example of an inline MathJax expression: $ 2 \\times 2 = 4 $\n```\n\n\nThis is an example of an inline MathJax expression: $ 2 \\times 2 = 4 $\n`\\\\( ... \\\\)` delimiters also work for inline math,\nbut the \\$ form is sometimes more readable.\nNote: If you need to use a dollar sign in text or MathJax expressions, escape it\nwith a leading slash: `\\$`. Dollar signs within code blocks (such as Bash\nvariable names) do not need to be escaped.\nProse style\nIf you are going to write or edit substantial portions of the narrative\ndocumentation, please read the\nGoogle Developer Documentation Style Guide.\nPrinciples of good style\n\nCheck the spelling and grammar in your contributions. Most editors\n    include a spell checker or have an available spell-checking plugin. You can\n    also paste your text into a Google Doc or other document software for a more\n    robust spelling and grammar check.\nUse a casual and friendly voice. Write TensorFlow documentation like a\n    conversation\u2014as if you're talking to another person one-on-one. Use a\n    supportive tone in the article.\n\nNote: Being less formal does not mean being less technical. Simplify your prose,\nnot the technical content.\n\nAvoid disclaimers, opinions, and value judgements. Words like \"easily\",\n    \"just\", and \"simple\" are loaded with assumptions. Something might seem easy\n    to you, but be difficult for another person. Try to avoid these whenever\n    possible.\nUse simple, to the point sentences without complicated jargon. Compound\n    sentences, chains of clauses, and location-specific idioms can make text\n    hard to understand and translate. If a sentence can be split in two, it\n    probably should. Avoid semicolons. Use bullet lists when appropriate.\nProvide context. Don't use abbreviations without explaining them. Don't\n    mention non-TensorFlow projects without linking to them. Explain why the\n    code is written the way it is.\n\nUsage guide\nOps\nIn markdown files, use `# \u21d2` instead of a single equal sign when you want to\nshow what an op returns.\n```python\n'input' is a tensor of shape [2, 3, 5]\ntf.expand_dims(input, 0)  # \u21d2 [1, 2, 3, 5]\n```\nIn notebooks, display the result instead of adding a comment (If the last\nexpression in a notebook cell is not assigned to a variable, it is automatically\ndisplayed.)\nIn API reference docs prefer using doctest to show\nresults.\nTensors\nWhen you're talking about a tensor in general, don't capitalize the word\ntensor. When you're talking about the specific object that's provided to or\nreturned from an op, then you should capitalize the word Tensor and add\nbackticks around it because you're talking about a `Tensor` object.\nDon't use the word Tensors (plural) to describe multiple `Tensor` objects\nunless you really are talking about a `Tensors` object. Instead, say \"a list (or\ncollection) of `Tensor` objects\".\nUse the word shape to detail the axes of a tensor, and show the shape in\nsquare brackets with backticks. For example:\n\n\n```\nIf `input` is a three-axis `Tensor` with shape `[3, 4, 3]`, this operation\nreturns a three-axis `Tensor` with shape `[6, 8, 6]`.\n```\n\n\nAs above, prefer \"axis\" or \"index\" over \"dimension\" when talking about the\nelements of a `Tensor`'s shape. Otherwise it's easy to confuse \"dimension\" with\nthe dimension of a vector space. A \"three-dimensional vector\" has a single axis",
    "tag": "tensorflow"
  },
  {
    "title": "TensorFlow code style guide",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/community/contribute/code_style.md",
    "content": "TensorFlow code style guide\nPython style\nFollow the PEP 8 Python style\nguide, except TensorFlow uses 2\nspaces instead of 4. Please conform to the\nGoogle Python Style Guide,\nand use pylint to check your Python changes.\npylint\nTo install `pylint`:\n`bash\n$ pip install pylint`\nTo check a file with `pylint` from the TensorFlow source code root directory:\n`bash\n$ pylint --rcfile=tensorflow/tools/ci_build/pylintrc tensorflow/python/keras/losses.py`\nSupported Python versions\nFor supported Python versions, see the TensorFlow\ninstallation guide.\nSee the TensorFlow\ncontinuous build status\nfor official and community supported builds.\nC++ coding style\nChanges to TensorFlow C++ code should conform to the Google C++ Style\nGuide and TensorFlow specific style details. Use `clang-format` to check your C/C++ changes.\nTo install on Ubuntu 16+, do:\n`bash\n$ apt-get install -y clang-format`\nYou can check the format of a C/C++ file with the following:\n`bash\n$ clang-format <my_cc_file> --style=google > /tmp/my_cc_file.cc\n$ diff <my_cc_file> /tmp/my_cc_file.cc`\nOther languages\n\nGoogle Java Style Guide\nGoogle JavaScript Style Guide\nGoogle Shell Style Guide\nGoogle Objective-C Style Guide\n\nTensorFlow conventions and special uses\nPython operations\nA TensorFlow operation is a function that, given input tensors returns output\ntensors (or adds an op to a graph when building graphs).\n\nThe first argument should be tensors, followed by basic Python parameters.\n    The last argument is `name` with a default value of `None`.\nTensor arguments should be either a single tensor or an iterable of tensors. That is, a \"Tensor or list of Tensors\" is too broad. See `assert_proper_iterable`.\nOperations that take tensors as arguments should call `convert_to_tensor` to\n    convert non-tensor inputs into tensors if they are using C++ operations.\n    Note that the arguments are still described as a `Tensor` object of a\n    specific dtype in the documentation.\nEach Python operation should have a `name_scope`. As seen below, pass the name\n    of the op as a string.\nOperations should contain an extensive Python comment with Args and Returns\n    declarations that explain both the type and meaning of each value. Possible\n    shapes, dtypes, or ranks should be specified in the description. See\n    documentation details.\nFor increased usability, include an example of usage with inputs / outputs\n    of the op in Example section.\nAvoid making explicit use of `tf.Tensor.eval` or `tf.Session.run`. For\n    example, to write logic that depends on the Tensor value, use the TensorFlow\n    control flow. Alternatively, restrict the operation to only run when eager\n    execution is enabled (`tf.executing_eagerly()`).\n\nExample:\n```python\ndef my_op(tensor_in, other_tensor_in, my_param, other_param=0.5,\n          output_collections=(), name=None):\n  \"\"\"My operation that adds two tensors with given coefficients.\nArgs:\n    tensor_in: `Tensor`, input tensor.\n    other_tensor_in: `Tensor`, same shape as `tensor_in`, other input tensor.\n    my_param: `float`, coefficient for `tensor_in`.\n    other_param: `float`, coefficient for `other_tensor_in`.\n    output_collections: `tuple` of `string`s, name of the collection to\n                        collect result of this op.\n    name: `string`, name of the operation.\nReturns:\n    `Tensor` of same shape as `tensor_in`, sum of input values with coefficients.\nExample:\n    >>> my_op([1., 2.], [3., 4.], my_param=0.5, other_param=0.6,\n              output_collections=['MY_OPS'], name='add_t1t2')\n    [2.3, 3.4]\n  \"\"\"\n  with tf.name_scope(name or \"my_op\"):\n    tensor_in = tf.convert_to_tensor(tensor_in)\n    other_tensor_in = tf.convert_to_tensor(other_tensor_in)\n    result = my_param * tensor_in + other_param * other_tensor_in\n    tf.add_to_collection(output_collections, result)\n    return result\n```\nUsage:\n```python\noutput = my_op(t1, t2, my_param=0.5, other_param=0.6,\n               output_collections=['MY_OPS'], name='add_t1t2')",
    "tag": "tensorflow"
  },
  {
    "title": "Contribute to the TensorFlow documentation",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/community/contribute/docs.md",
    "content": "Contribute to the TensorFlow documentation\nTensorFlow welcomes documentation contributions\u2014if you improve the\ndocumentation, you improve the TensorFlow library itself. Documentation on\ntensorflow.org falls into the following categories:\n\nAPI reference \u2014The API reference docs\n  are generated from docstrings in the\n  TensorFlow source code.\nNarrative documentation \u2014These are tutorials,\n  guides, and other writing that's not part\n  of the TensorFlow code. This documentation is in the\n  tensorflow/docs GitHub repository.\nCommunity translations \u2014These are guides and tutorials translated by the\n  community. All community translations live in the\n  tensorflow/docs repo.\n\nSome TensorFlow projects keep documentation\nsource files near the code in a separate repository, usually in a `docs/`\ndirectory. See the project's `CONTRIBUTING.md` file or contact the maintainer to\ncontribute.\nTo participate in the TensorFlow docs community:\n\nWatch the tensorflow/docs GitHub\n  repository.\nFollow the docs tag on the\n  TensorFlow Forum.\n\nAPI reference\nFor details, use the TensorFlow API docs contributor guide. This\nshows you how to find the\nsource file\nand edit the symbol's\ndocstring.\nMany API reference pages on tensorflow.org include a link to the source file\nwhere the symbol is defined. Docstrings support\nMarkdown\nand can be (approximately) previewed using any\nMarkdown previewer.\nVersions and branches\nThe site's API reference\nversion defaults to the latest stable binary\u2014this matches the package installed\nwith `pip install tensorflow`.\nThe default TensorFlow package is built from the stable branch `rX.x` in the\nmain\ntensorflow/tensorflow\nrepo. The reference documentation is generated from code comments\nand docstrings in the source code for\nPython,\nC++, and\nJava.\nPrevious versions of the TensorFlow documentation are available as\nrX.x branches in the TensorFlow\nDocs repository. These branches are added when a new version is released.\nBuild API docs\nNote: This step is not required to edit or preview API docstrings, only to\ngenerate the HTML used on tensorflow.org.\nPython reference\nThe `tensorflow_docs` package includes the generator for the\nPython API reference docs. To\ninstall:\n\n\n\n```pip install git+https://github.com/tensorflow/docs```\n\n\n\nTo generate the TensorFlow 2 reference docs, use the\n`tensorflow/tools/docs/generate2.py` script:\n\n\n\n```git clone https://github.com/tensorflow/tensorflow tensorflow```\n\n\n\n\n```cd tensorflow/tensorflow/tools/docs```\n\n\n\n\n```pip install tensorflow```\n\n\n\n\n```python generate2.py --output_dir=/tmp/out```\n\n\n\nNote: This script uses the installed TensorFlow package to generate docs and\nonly works for TensorFlow 2.x.\nNarrative documentation\nTensorFlow guides and\ntutorials are written as\nMarkdown\nfiles and interactive\nJupyter notebooks. Notebooks\ncan be run in your browser using\nGoogle Colaboratory.\nThe narrative docs on tensorflow.org are built\nfrom the\ntensorflow/docs\n`master` branch. Older versions are available in GitHub on the `rX.x` release\nbranches.\nSimple changes\nThe easiest way to make straightforward documentation updates to Markdown files\nis to use GitHub's\nweb-based\nfile editor. Browse the\ntensorflow/docs\nrepository to find the Markdown that roughly corresponds to the\ntensorflow.org URL structure. In the\nupper right corner of the file view, click the pencil icon\n\nto open the file editor. Edit the file and then submit a new pull request.\nSet up a local Git repo\nFor multi-file edits or more complex updates, it's better to use a local Git\nworkflow to create a pull request.\nNote: Git is the open source\nversion control system (VCS) used to track changes to source code.\nGitHub is an online service\nthat provides collaboration tools that work with Git. See the\nGitHub Help to set up\nyour GitHub account and get started.\nThe following Git steps are only required the first time you set up a local\nproject.\nFork the tensorflow/docs repo\nOn the\ntensorflow/docs\nGitHub page, click the Fork button\n\nto create your own repo copy under your GitHub account. Once forked, you're\nresponsible for keeping your repo copy up-to-date with the upstream TensorFlow\nrepo.\nClone your repo\nDownload a copy of your remote username/docs repo to your local\nmachine. This is the working directory where you will make changes:\n\n\n\n```git clone git@github.com:username/docs```\n\n\n\n\n```cd ./docs```\n\n\n\nAdd an upstream repo to keep up-to-date (optional)\nTo keep your local repository in sync with `tensorflow/docs`, add an upstream\nremote to download the latest changes.\nNote: Make sure to update your local repo before starting a contribution.\nRegular syncs to upstream reduce the chance of a\nmerge conflict\nwhen you submit your pull request.\nAdd a remote:\n\n\n\n```git remote add upstream git@github.com:tensorflow/docs.git```\n\n\n\n# View remote repos\n\n\n```git remote -v```\n\n\norigin    git@github.com:username/docs.git (fetch)\norigin    git@github.com:username/docs.git (push)\nupstream  git@github.com:tensorflow/docs.git (fetch)\nupstream  git@github.com:tensorflow/docs.git (push)\n\nTo update:\n\n\n\n```git checkout master```\n\n\n\n\n```git pull upstream master```\n\n\n\n\n\n```git push```\n\n  # Push changes to your GitHub account (defaults to origin)\n\nGitHub workflow\n1. Create a new branch\nAfter you update your repo from `tensorflow/docs`, create a new branch from the\nlocal master branch:\n\n\n\n```git checkout -b feature-name```\n\n\n\n\n\n```git branch```\n\n  # List local branches\n  master\n* feature-name\n\n2. Make changes\nEdit files in your favorite editor and please follow the\nTensorFlow documentation style guide.\nCommit your file change:\n\n# View changes\n\n\n```git status```\n\n  # See which files have changed\n\n\n```git diff```\n\n    # See changes within files\n\n\n\n```git add path/to/file.md```\n\n\n\n\n```git commit -m \"Your meaningful commit message for the change.\"```\n\n\n\nAdd more commits, as necessary.\n3. Create a pull request\nUpload your local branch to your remote GitHub repo\n(github.com/username/docs):\n\n\n\n```git push```\n\n\n\nAfter the push completes, a message may display a URL to automatically\nsubmit a pull request to the upstream repo. If not, go to the\ntensorflow/docs\nrepo\u2014or your own repo\u2014and GitHub will prompt you to create a pull request.\n4. Review\nMaintainers and other contributors will review your pull request. Please\nparticipate in the discussion and make the requested changes. When your pull\nrequest is approved, it will be merged into the upstream TensorFlow docs repo.\nSuccess: Your changes have been accepted to the TensorFlow documentation.\nThere is a separate publishing step to update\ntensorflow.org from the GitHub repo. Typically,\nchanges are batched together and the site is updated on a regular cadence.\nInteractive notebooks\nWhile it's possible to edit the notebook JSON file with GitHub's\nweb-based file editor,\nit's not recommended since malformed JSON can corrupt the file. Make sure to\ntest the notebook before submitting a pull request.\nGoogle Colaboratory\nis a hosted notebook environment that makes it easy to edit\u2014and run\u2014notebook\ndocumentation. Notebooks in GitHub are loaded in Google Colab by passing the\npath to the Colab URL, for example,\nthe notebook located in GitHub here:\nhttps://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/classification.ipynb\ncan be loaded into Google Colab at this URL:\nhttps://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/classification.ipynb\n\nThere is an\nOpen in Colab\nChrome extension that performs this URL substitution when browsing a notebook on\nGitHub. This is useful when opening a notebook in your repo fork, because the\ntop buttons always link to the TensorFlow Docs `master` branch.\nNotebook formatting\nA notebook formatting tool makes Jupyter notebook source diffs consistent and\neasier to review. Since notebook authoring environments differ with regards to\nfile output, indentation, metadata and other non-specified fields; `nbfmt` uses\nopinionated defaults with a preference for the TensorFlow docs Colab workflow.\nTo format a notebook, install the\nTensorFlow\ndocs notebook tools and run the `nbfmt` tool:\n```\nInstall the tensorflow-docs package:\n$ python3 -m pip install -U [--user] git+https://github.com/tensorflow/docs\n$ python3 -m tensorflow_docs.tools.nbfmt [options] notebook.ipynb [...]\n```\nFor TensorFlow docs projects, notebooks without output cells are executed and\ntested; notebooks with saved output cells are published as-is. `nbfmt`\nrespects the notebook state and uses the `--remove_outputs` option to explicitly\nremove output cells.\nTo create a new notebook, copy and edit the\nTensorFlow\ndocs notebook template.\nEdit in Colab\nWithin the Google Colab environment, double-click cells to edit text and code\nblocks. Text cells use Markdown and should follow the\nTensorFlow docs style guide.\nDownload notebook files from Colab with File > Download .pynb. Commit\nthis file to your local Git repo and send a pull\nrequest.\nTo create a new notebook, copy and edit the\nTensorFlow notebook template.\nColab-GitHub workflow\nInstead of downloading a notebook file and using a local Git workflow, you can\nedit and update your forked GitHub repo directly from Google Colab:\n\nIn your forked username/docs repo, use the GitHub web UI to\n   create a new branch.\nNavigate to the notebook file to edit.\nOpen the notebook in Google Colab: use the URL swap or the Open in Colab\n   Chrome extension.\nEdit the notebook in Colab.\nCommit the changes to your repo from Colab with\n   File > Save a copy in GitHub.... The save dialog should link to the\n   appropriate repo and branch. Add a meaningful commit message.\nAfter saving, browse to your repo or the\n   tensorflow/docs\n   repo, GitHub should prompt you to create a pull request.\nThe pull request is reviewed by maintainers.\n\nSuccess: Your changes have been accepted to the TensorFlow documentation.\nTranslations\nThe TensorFlow team works with the community and vendors to provide translations\nfor tensorflow.org. Translations of notebooks and other technical content are\nlocated in the\ntensorflow/docs-l10n\nGitHub repo. Please submit pull requests through the\nTensorFlow\nGitLocalize project.\nThe English docs are the source-of-truth and translations should follow these\nguides as close as possible. That said, translations are written for the\ncommunities they serve. If the English terminology, phrasing, style, or tone\ndoes not translate to another language, please use a translation appropriate for\nthe reader.\nLanguage support is determined by a number of factors including\u2014but not limited\nto\u2014site metrics and demand, community support,\nEnglish\nproficiency, audience preference, and other indicators. Since each supported\nlanguage incurs a cost, unmaintained languages are removed. Support for new\nlanguages will be announced on the\nTensorFlow blog or\nTwitter.\nIf your preferred language is not supported, you are welcome to maintain a\ncommunity fork for open source contributors. These are not published to",
    "tag": "tensorflow"
  },
  {
    "title": "Contribute to the TensorFlow code",
    "source": "https://github.com/tensorflow/docs/tree/master/site/en/community/contribute/code.md",
    "content": "Contribute to the TensorFlow code\nWhether you are adding a loss function, improving test coverage, or writing an\nRFC for a major design change, this portion of the contributor guide will help\nyou get started. Thank you for work and interest in improving TensorFlow.\nBefore you get started\nBefore you contribute source code to a TensorFlow project, please review the\n`CONTRIBUTING.md` file in the GitHub repo of the project. For example, see the\nCONTRIBUTING.md\nfile in the core TensorFlow repo. All code contributors are required to sign a\nContributor License Agreement (CLA).\nTo avoid duplicating work, please review\ncurrent or\nproposed\nRFCs and contact the developers on the TensorFlow forums\n(developers@tensorflow.org)\nbefore you start work on a non-trivial feature. We are somewhat selective when\ndeciding to add new functionality, and the best way to contribute and help the\nproject is to work on known issues.\nIssues for new contributors\nNew contributors should look for the following tags when searching for a first\ncontribution to the TensorFlow code base. We strongly recommend that new\ncontributors tackle \u201cgood first issue\u201d and \"contributions welcome\" projects\nfirst; this helps the contributor become familiar with the contribution\nworkflow, and for the core devs to become acquainted with the contributor.\n\ngood first issue\ncontributions welcome\n\nIf you are interested in recruiting a team to help tackle a large-scale problem\nor a new feature, please email the\ndevelopers@ group\nand review our current list of RFCs.\nCode review\nNew features, bug fixes, and any other changes to the code base are subject to\ncode review.\nReviewing code contributed to the project as pull requests is a crucial\ncomponent of TensorFlow development. We encourage anyone to start reviewing code\nsubmitted by other developers, especially if the feature is something that you\nare likely to use.\nHere are some questions to keep in mind during the code review process:\n\nDo we want this in TensorFlow? Is it likely to be used? Do you, as a TensorFlow user, like the change and intend to use it? Is this change in the scope of TensorFlow? Will the cost of maintaining a new feature be worth its benefits?\nIs the code consistent with the TensorFlow API? Are public functions, classes, and parameters well-named and intuitively designed?\n\nDoes it include documentation? Are all public functions, classes, parameters, return types, and stored attributes named according to TensorFlow conventions and clearly documented? Is new functionality described in TensorFlow's documentation and illustrated with examples, whenever possible? Does the documentation render properly?\n\n\nIs the code human-readable? Is it low on redundancy? Should variable names be improved for clarity or consistency? Should comments be added? Should any comments be removed as unhelpful or extraneous?\n\nIs the code efficient? Could it be rewritten easily to run more efficiently?\nIs the code backwards compatible with previous versions of TensorFlow?\nWill the new code add new dependencies on other libraries?\n\nTest and improve test coverage\nHigh-quality unit testing is a corner-stone of the TensorFlow development\nprocess. For this purpose, we use Docker images. The test functions are\nappropriately named, and are responsible for checking the validity of algorithms\nas well as different options of the code.\nAll new features and bug fixes must include adequate test coverage. We also\nwelcome contributions of new test cases or improvements to existing tests. If\nyou discover that our existing tests are not complete \u2014 even if that is not\ncurrently causing a bug \u2014 please file an issue and, if possible, a pull request.\nFor the specific details of testing procedures in each TensorFlow project, see\nthe `README.md` and `CONTRIBUTING.md` files in the project repo on GitHub.\nOf particular concerns in adequate testing:\n\nIs every public function and class tested?\nAre a reasonable set of parameters, their values, value types, and\n    combinations tested?\nDo the tests validate that the code is correct, and that it is doing what\n    the documentation says the code is intended to do?\nIf the change is a bug fix, is a non-regression test included?\nDo the tests pass the continuous integration build?\nDo the tests cover every line of code? If not, are the exceptions\n    reasonable and explicit?\n\nIf you find any problems, please consider helping the contributor understand\nthose problems and resolve them.\nImprove error messages or logs\nWe welcome contributions that improve error messages and logging.\nContribution workflow\nCode contributions\u2014bug fixes, new development, test improvement\u2014all follow a\nGitHub-centered workflow. To participate in TensorFlow development, set up a\nGitHub account. Then:\n\n\nFork the repo you plan to work on. Go to the project repo page and use the\n    Fork button. This will create a copy of the repo, under your username.\n    (For more details on how to fork a repository see\n    this guide.)\n\n\nClone down the repo to your local system.\n`$ git clone git@github.com:your-user-name/project-name.git`\n\n\nCreate a new branch to hold your work.\n`$ git checkout -b new-branch-name`\n\n\nWork on your new code. Write and run tests.\n\n\nCommit your changes.\n`$ git add -A`\n`$ git commit -m \"commit message here\"`\n\n\nPush your changes to your GitHub repo.\n`$ git push origin branch-name`\n\n\nOpen a Pull Request (PR). Go to the original project repo on GitHub. There\n    will be a message about your recently pushed branch, asking if you would\n    like to open a pull request. Follow the prompts, compare across\n    repositories, and submit the PR. This will send an email to the committers.\n    You may want to consider sending an email to the mailing list for more\n    visibility. (For more details, see the\n    GitHub guide on PRs.\n\n\nMaintainers and other contributors will review your PR. Please participate\n    in the conversation, and try to make any requested changes. Once the PR is\n    approved, the code will be merged.\n\n\nBefore working on your next contribution, make sure your local repository is\nup to date.\n\n\nSet the upstream remote. (You only have to do this once per project, not\n    every time.)\n`$ git remote add upstream git@github.com:tensorflow/project-repo-name`\n\n\nSwitch to the local master branch.\n`$ git checkout master`\n\n\nPull down the changes from upstream.\n`$ git pull upstream master`\n\n\nPush the changes to your GitHub account. (Optional, but a good practice.)\n`$ git push origin master`\n\n\nCreate a new branch if you are starting new work.\n`$ git checkout -b branch-name`\n\n\nAdditional `git` and GitHub resources:\n\nGit documentation\nGit development workflow\nResolving merge conflicts.\n\nContributor checklist\n\nRead the contributing guidelines.\nRead the Code of Conduct.\nEnsure you have signed the Contributor License Agreement (CLA).\nCheck if your changes are consistent with the guidelines.\nCheck if your changes are consistent with the TensorFlow coding style.\n",
    "tag": "tensorflow"
  }
]