[
  {
    "title": "BentoML Documentation",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/",
    "content": "BentoML Documentation\nA guide for docs contributors\nThe `docs` directory contains the sphinx source text for BentoML docs, visit\nhttp://docs.bentoml.org/ to read the full documentation.\nThis guide is made for anyone who's interested in running BentoML documentation locally,\nmaking changes to it and make contributions. BentoML is made by the thriving community\nbehind it, and you're always welcome to make contributions to the project and the \ndocumentation. \nBefore starting to make a contribution to the docs, make sure to check the \nissues page and the `#bentoml-contributors` \nchannel in the community slack, to make sure no one \nelse is working on the same thing and to get feedback from the community for larger \nproposals.\n\nBuild Docs\nIf you haven't already, clone the BentoML Github repo to a local directory:\n`bash\ngit clone https://github.com/bentoml/BentoML.git && cd BentoML`\nInstall all dependencies required for building docs (mainly `sphinx` and its extension):\n`bash\npip install -r requirements/docs-requirements.txt`\nBuild the sphinx docs:\n`bash\nmake clean html -C ./docs`\nThe docs HTML files are now generated under `docs/build/html` directory, you can preview\nit locally with the following command:\n`bash\npython -m http.server 8000 -d docs/build/html`\nAnd open your browser at http://0.0.0.0:8000/ to view the generated docs.\nSpellcheck\nInstall spellchecker dependencies:\n`bash\nmake install-spellchecker-deps`\nTo run spellchecker locally:\n`bash\nmake spellcheck-doc`\nWatch Docs\nWe recommend using sphinx-autobuild during development, which provides a live-reloading \nserver, that rebuilds the documentation and refreshes any open pages automatically when \nchanges are saved. This enables a much shorter feedback loop which can help boost \nproductivity when writing documentation.\nSimply run the following command from BentoML project's root directory: \n`bash\nsphinx-autobuild docs/source docs/build/html`\nIf you have `make` installed, you may also run:\n`bash\nmake watch-docs`\nWriting Documentation\nWriting .rst (ReStructuredText) in BentoML docs\nBentoML docs is built with Sphinx, which natively supports ReStructuredText.\nDocument titles and section headers\nIn reStructuredText, there are no heading levels assigned to certain characters as the \nstructure is determined from the succession of headings. However in BentoML docs, we\nfollow the following convention:\n```rst\nDocument Title\nTop Level Headings\n2nd level headings\n~~~~~~~~~~~~~~~~~~\n3rd level headings\n^^^^^^^^^^^^^^^^^^\n4th level heading - avoid this if possible\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n```\nAdding Reference Links\nWhen writing documentation, it is common to mention or link to other parts of the docs.\nIf you need to refer to a specific documentation page, use `:doc:` plus path to the \ntarget documentation file under the `docs/source/`. e.g.:\n`rst\n:doc:`tutorial`\n:doc:`/frameworks/pytorch``\nBy default, this will show the title of the target document and link to it. You may also\nchange the title shown on current page:\n`rst\n:doc:`\ud83d\udcd6 Main Concepts <concepts/index>``\nIt is also possible to refer to a specific section of other document pages. We use the\nautosectionlabel sphinx plugin\nto generate labels for every section in the documentation.\nFor example:\n`rst\n:ref:`frameworks/pytorch:Section Title`\nAdmonitions\nA `note` section can be created with the following syntax:\n```rst\n.. note:: This is what the most basic admonitions look like.\n.. note::\n   It is possible to have multiple paragraphs in the same admonition.\nIf you really want, you can even have lists, or code, or tables.\n```\nThere are other admonition types such as `caution`, `danger`, `hint`, `important`, \n`seealso`, and `tip`. Learn more about it here.\nCode Blocks\n```rst\nCode blocks in reStructuredText can be created in various ways::\n\n\n```Indenting content by 4 spaces, after a line ends with \"::\".\nThis will have default syntax highlighting (highlighting a few words and \"strings\").\n```\n\n\n.. code::\n\n\n```You can also use the code directive, or an alias: code-block, sourcecode.\nThis will have default syntax highlighting (highlighting a few words and \"strings\").\n```\n\n\n.. code:: python\n\n\n```print(\"And with the directive syntax, you can have syntax highlighting.\")\n```\n\n\n.. code:: none\n\n\n```print(\"Or disable all syntax highlighting.\")\n```\n\n\n```\nThere's a lot more forms of \"blocks\" in reStructuredText that can be used, as\nseen in https://docutils.sourceforge.io/docs/ref/rst/restructuredtext.html#literal-blocks.\nTabs\nFor most scenarios in BentoML docs, use the tabs view provided by `sphinx-design`:\nhttps://sphinx-design.readthedocs.io/en/furo-theme/tabs.html\n```rst\n.. tab-set::\n\n\n```.. tab-item:: Label1\n\n    Content 1\n\n.. tab-item:: Label2\n\n    Content 2\n```\n\n\n```\nDocumenting Source Code\nBentoML docs relies heavily on the Python docstrings defined together with the source\ncode. We ask our contributors to document every public facing APIs and CLIs, including\ntheir signatures, options, and example usage. Sphinx can then use these inline docs to\ngenerate API References pages. \nBentoML uses the sphinx.ext.autodoc\nextension to include documentation from docstring. For example, a `.rst` document can \ncreate a section made from a Python Class's docstring, using the following syntax:\n`rst\n.. autoclass:: bentoml.Service\n    :members: api`\nSimilarly, for functions:\n`rst\n.. autofunction:: bentoml.models.list`\nLearn more about this syntax here.\nBentoML codebase follows the Google's docstring style\nfor writing inline docstring. Below are some examples.\nDefine arguments in a method\nArguments should be defined with `Args:` prefix, followed by a line with indentation. Each argument should be followed by\nits type, a new indentation for description of given field. Each argument should follow the below definition:\n`markdown\n    Args:\n        bento_name (:code:`str`):\n            :class:`~bentoml.BentoService` identifier with name format :obj:`NAME:VERSION`.\n            ``NAME`` can be accessed via :meth:`~bentoml.BentoService.name` and ``VERSION`` can\n            be accessed via :meth:`~bentoml.BentoService.version``\nFor optional arguments, follow the following syntax. For example a function `func()` with following signature:\n`python\ndef func(x: str=None, a: Optional[bool]=None):\n    ...`\nthen documentation should look like:\n`markdown\n    Args:\n        x (:code:`str`, `optional`):\n            Description of x ...\n        a (`bool`, `optional`):\n            Description of a ...`\nDefine a multiline code block in a method\nMake sure to define something like:\n```markdown\nExample::\n\n\n```# example code here\n# ...\n```\n\n\n```\nThe `Example` can be replaced with any word of choice as long as there are two semicolons following. Read more about doctest\nDefine a return block in a method\nIf a function returns value, returns should be defined with `Returns:`, followed by a line with indentation. The first line\nshould be the type of the return, followed by a line return. An example for a return statement:\n```markdown\n    Returns:\n        :obj:`Dict[str,str]`with keys are :class:`~bentoml.BentoService` nametag following with saved bundle path.",
    "tag": "bentoml"
  },
  {
    "title": "index.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/index.rst",
    "content": "===============================\nUnified Model Serving Framework\n===============================\n|github_stars| |pypi_status| |downloads| |actions_status| |documentation_status| |join_slack|\n\nWhat is BentoML?\n`BentoML <https://github.com/bentoml/BentoML>`_ makes it easy to create ML-powered prediction services that are ready to deploy and scale.\nData Scientists and ML Engineers use BentoML to:\n\nAccelerate and standardize the process of taking ML models to production\nBuild scalable and high performance prediction services\nContinuously deploy, monitor, and operate prediction services in production\n\nLearn BentoML\n.. grid:: 1 2 2 2\n    :gutter: 3\n    :margin: 0\n    :padding: 3 4 0 0\n\n\n```.. grid-item-card:: :doc:`\ud83d\udcbb Tutorial: Intro to BentoML <tutorial>`\n    :link: tutorial\n    :link-type: doc\n\n    A simple example of using BentoML in action. In under 10 minutes, you'll be able to serve your ML model over an HTTP API endpoint, and build a docker image that is ready to be deployed in production.\n\n.. grid-item-card:: :doc:`\ud83d\udcd6 Main Concepts <concepts/index>`\n    :link: concepts/index\n    :link-type: doc\n\n    A step-by-step tour of BentoML's components and introduce you to its philosophy. After reading, you will see what drives BentoML's design, and know what `bento` and `runner` stands for.\n\n.. grid-item-card:: :doc:`\ud83e\uddee ML Framework Guides <frameworks/index>`\n    :link: frameworks/index\n    :link-type: doc\n\n    Best practices and example usages by the ML framework used for model training.\n\n.. grid-item-card:: `\ud83c\udfa8 Examples <https://github.com/bentoml/BentoML/tree/main/examples>`_\n    :link: https://github.com/bentoml/BentoML/tree/main/examples\n    :link-type: url\n\n    Example projects demonstrating BentoML usage in a variety of different scenarios.\n\n.. grid-item-card:: :doc:`\ud83d\udcaa Advanced Guides <guides/index>`\n    :link: guides/index\n    :link-type: doc\n\n    Dive into BentoML's advanced features, internals, and architecture, including GPU support, inference graph, monitoring, and performance optimization.\n\n.. grid-item-card:: :doc:`\u2699\ufe0f Integrations & Ecosystem <integrations/index>`\n    :link: integrations/index\n    :link-type: doc\n\n    Learn how BentoML works together with other tools and products in the Data/ML ecosystem\n\n.. grid-item-card:: `\ud83d\udcac BentoML Community <https://l.linklyhq.com/l/ktOX>`_\n    :link: https://l.linklyhq.com/l/ktOX\n    :link-type: url\n\n    Join us in our Slack community where hundreds of ML practitioners are contributing to the project, helping other users, and discuss all things MLOps.\n```\n\n\nBeyond Model Serving\n.. grid:: 1 2 2 2\n    :gutter: 3\n    :margin: 0\n    :padding: 3 4 0 0\n\n\n```.. grid-item-card:: `\ud83e\udd84\ufe0f Yatai <https://github.com/bentoml/Yatai>`_\n    :link: https://github.com/bentoml/Yatai\n    :link-type: url\n\n    Model Deployment at scale on Kubernetes.\n\n.. grid-item-card:: `\ud83d\ude80 bentoctl <https://github.com/bentoml/bentoctl>`_\n    :link: https://github.com/bentoml/bentoctl\n    :link-type: url\n\n    Fast model deployment on any cloud platform.\n```\n\n\nStaying Informed\nThe `BentoML Blog <http://modelserving.com>` and `@bentomlai <http://twitt\ner.com/bentomlai>` on Twitter are the official source for\nupdates from the BentoML team. Anything important, including major releases and announcements, will be posted there. We also frequently\nshare tutorials, case studies, and community updates there.\nTo receive release notification, star & watch the `BentoML project on GitHub <https://github.com/bentoml/bentoml>`. For release\nnotes and detailed changelog, see the `Releases <https://github.com/bentoml/BentoML/releases>` page.\n\nWhy are we building BentoML?\nModel deployment is one of the last and most important stages in the machine learning\nlife cycle: only by putting a machine learning model into a production environment and\nmaking predictions for end applications, the full potential of ML can be realized.\nSitting at the intersection of data science and engineering, model deployment\nintroduces new operational challenges between these teams. Data scientists, who are\ntypically responsible for building and training the model, often don\u2019t have the\nexpertise to bring it into production. At the same time, engineers, who aren\u2019t used to\nworking with models that require continuous iteration and improvement, find it\nchallenging to leverage their know-how and common practices (like CI/CD) to deploy them.\nAs the two teams try to meet halfway to get the model over the finish line,\ntime-consuming and error-prone workflows can often be the result, slowing down the pace\nof progress.\nWe at BentoML want to get your ML models shipped in a fast, repeatable, and scalable\nway. BentoML is designed to streamline the handoff to production deployment, making it\neasy for developers and data scientists alike to test, deploy, and integrate their\nmodels with other systems.\nWith BentoML, data scientists can focus primarily on creating and improving their\nmodels, while giving deployment engineers peace of mind that nothing in the deployment\nlogic is changing and that production service is stable.\n\nGetting Involved\nBentoML has a thriving open source community where hundreds of ML practitioners are\ncontributing to the project, helping other users and discuss all things MLOps.\n`\ud83d\udc49 Join us on slack today! <https://l.linklyhq.com/l/ktOX>`_\n.. toctree::\n   :hidden:\ninstallation\n   tutorial\n   concepts/index\n   frameworks/index\n   guides/index\n   integrations/index\n   reference/index\n   Examples https://github.com/bentoml/BentoML/tree/main/examples\n   Community https://l.linklyhq.com/l/ktOX\n   GitHub https://github.com/bentoml/BentoML\n   Blog https://modelserving.com\n.. spelling::\n.. |pypi_status| image:: https://img.shields.io/pypi/v/bentoml.svg?style=flat-square\n   :target: https://pypi.org/project/BentoML\n.. |downloads| image:: https://pepy.tech/badge/bentoml?style=flat-square\n   :target: https://pepy.tech/project/bentoml\n.. |actions_status| image:: https://github.com/bentoml/bentoml/workflows/CI/badge.svg\n   :target: https://github.com/bentoml/bentoml/actions\n.. |documentation_status| image:: https://readthedocs.org/projects/bentoml/badge/?version=latest&style=flat-square\n   :target: https://docs.bentoml.org/\n.. |join_slack| image:: https://badgen.net/badge/Join/BentoML%20Slack/cyan?icon=slack&style=flat-square\n   :target: https://l.linklyhq.com/l/ktOX\n.. |github_stars| image:: https://img.shields.io/github/stars/bentoml/BentoML?color=%23c9378a&label=github&logo=github&style=flat-square",
    "tag": "bentoml"
  },
  {
    "title": "tutorial.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/tutorial.rst",
    "content": "==========================\nTutorial: Intro to BentoML\n==========================\ntime expected: 10 minutes\nIn this tutorial, we will focus on online model serving with BentoML, using a\nclassification model trained with `scikit-learn <https://scikit-learn.org/stable/>`_ and the Iris dataset.\nBy the end of this tutorial, we will have a Bento that can be served easily using HTTP or gRPC for handling inference requests, and a docker\ncontainer image for deployment.\n.. note::\n\n\n```You might be tempted to skip this tutorial because you are not using scikit-learn,\nbut give it a chance. The concepts you will learn in the tutorial are fundamental to\nmodel serving with any ML framework using BentoML, and mastering it will give you a\ndeep understanding of BentoML.\n```\n\n\nSetup for the tutorial\nThere are three ways to complete this tutorial:\n. Run with Google Colab in your browser\n\ud83d\udc49 `Open Tutorial Notebook on Colab <https://colab.research.google.com/github/bentoml/BentoML/blob/main/examples/quickstart/iris_classifier.ipynb>`_\n   side by side with this guide. As you go through this guide, you can simply run the\n   sample code from the Colab Notebook.\nYou will be able to try out most of the content in the tutorial on Colab besides\n   the docker container part towards the end. This is because Google Colab currently\n   does not support docker.\n. Run the tutorial notebook from Docker\nIf you have Docker installed, you can run the tutorial notebook from a pre-configured\n   docker image with:\n.. code-block:: bash\n\n\n```  \u00bb docker run -it --rm -p 8888:8888 -p 3000:3000 -p 3001:3001 bentoml/quickstart:latest\n```\n\n\n. Local Development Environment\nDownload the source code of this tutorial from `bentoml/examples <https://github.com/bentoml/BentoML/tree/main/examples>`_:\n.. code-block:: bash\n\n\n```  \u00bb git clone --depth=1 git@github.com:bentoml/BentoML.git\n  \u00bb cd bentoml/examples/quickstart/\n```\n\n\nBentoML supports Linux, Windows and MacOS. You will need Python 3.7 or above to run\n   this tutorial. We recommend using `virtual environment <https://docs.python.org/3/library/venv.html>`_\n   to create an isolated local environment. However this is not required.\nInstall all dependencies required for this tutorial:\n.. code-block:: bash\n\n\n```  \u00bb pip install bentoml scikit-learn pandas\n```\n\n\n.. note::\nBentoML provides gRPC support, and we will provide gRPC examples alongside the HTTP\n   ones in this tutorial. However, these examples are optional and you don't have to\n   know about gRPC to get started with BentoML.\nIf you are interested in trying the gRPC examples in this tutorial, install\n   the gRPC dependencies for BentoML:\n.. code-block:: bash\n\n\n```  \u00bb pip install \"bentoml[grpc]\"\n```\n\n\nSaving Models with BentoML\nTo begin with BentoML, you will need to save your trained models with BentoML API in\nits model store (a local directory managed by BentoML). The model store is used for\nmanaging all your trained models locally as well as accessing them for serving.\n.. code-block:: python\n   :emphasize-lines: 14,15\nimport bentoml\nfrom sklearn import svm\n   from sklearn import datasets\n# Load training data set\n   iris = datasets.load_iris()\n   X, y = iris.data, iris.target\n# Train the model\n   clf = svm.SVC(gamma='scale')\n   clf.fit(X, y)\n# Save model to the BentoML local model store\n   saved_model = bentoml.sklearn.save_model(\"iris_clf\", clf)\n   print(f\"Model saved: {saved_model}\")\n# Model saved: Model(tag=\"iris_clf:zy3dfgxzqkjrlgxi\")\nThe model is now saved under the name `iris_clf` with an automatically generated\nversion. The name and version pair can then be used for retrieving the model. For\ninstance, the original model object can be loaded back into memory for testing via:\n.. code-block:: python\nmodel = bentoml.sklearn.load_model(\"iris_clf:2uo5fkgxj27exuqj\")\n# Alternatively, use `latest` to find the newest version\n   model = bentoml.sklearn.load_model(\"iris_clf:latest\")\nThe `bentoml.sklearn.save_model` API is built specifically for the Scikit-Learn\nframework and uses its native saved model format under the hood for best compatibility\nand performance. This goes the same for other ML frameworks, e.g.\n`bentoml.pytorch.save_model`, see the :doc:`frameworks/index` to learn more.\n.. seealso::\nIt is possible to use pre-trained models directly with BentoML or import existing\n   trained model files to BentoML. Learn more about it from :doc:`concepts/model`.\nSaved models can be managed via the `bentoml models` CLI command or Python API,\nlearn about it here: :ref:`concepts/model:Managing Models`.\nCreating a Service\nServices are the core components of BentoML, where the serving logic is defined. Create\na file `service.py` with:\n.. code-block:: python\n   :caption: `service.py`\n\n\n```import numpy as np\nimport bentoml\nfrom bentoml.io import NumpyNdarray\n\niris_clf_runner = bentoml.sklearn.get(\"iris_clf:latest\").to_runner()\n\nsvc = bentoml.Service(\"iris_classifier\", runners=[iris_clf_runner])\n\n@svc.api(input=NumpyNdarray(), output=NumpyNdarray())\ndef classify(input_series: np.ndarray) -> np.ndarray:\n    result = iris_clf_runner.predict.run(input_series)\n    return result\n```\n\n\nWe can now run the BentoML server for our new service in development mode:\n.. tab-set::\n\n\n```.. tab-item:: HTTP\n   :sync: http\n\n   .. code-block:: bash\n\n      \u00bb bentoml serve service:svc --reload\n      2022-09-18T21:11:22-0700 [INFO] [cli] Prometheus metrics for HTTP BentoServer from \"service.py:svc\" can be accessed at http://localhost:3000/metrics.\n      2022-09-18T21:11:22-0700 [INFO] [cli] Starting development HTTP BentoServer from \"service.py:svc\" listening on 0.0.0.0:3000 (Press CTRL+C to quit)\n      2022-09-18 21:11:23 circus[80177] [INFO] Loading the plugin...\n      2022-09-18 21:11:23 circus[80177] [INFO] Endpoint: 'tcp://127.0.0.1:61825'\n      2022-09-18 21:11:23 circus[80177] [INFO] Pub/sub: 'tcp://127.0.0.1:61826'\n      2022-09-18T21:11:23-0700 [INFO] [observer] Watching directories: ['~/workspace/bentoml/examples/quickstart', '~/bentoml/models']\n\n.. tab-item:: gRPC\n   :sync: grpc\n\n   .. code-block:: bash\n\n      \u00bb bentoml serve-grpc service:svc --reload --enable-reflection\n      2022-09-18T21:12:18-0700 [INFO] [cli] Prometheus metrics for gRPC BentoServer from \"service.py:svc\" can be accessed at http://localhost:3001.\n      2022-09-18T21:12:18-0700 [INFO] [cli] Starting development gRPC BentoServer from \"service.py:svc\" listening on 0.0.0.0:3000 (Press CTRL+C to quit)\n      2022-09-18 21:12:19 circus[81102] [INFO] Loading the plugin...\n      2022-09-18 21:12:19 circus[81102] [INFO] Endpoint: 'tcp://127.0.0.1:61849'\n      2022-09-18 21:12:19 circus[81102] [INFO] Pub/sub: 'tcp://127.0.0.1:61850'\n      2022-09-18T21:12:19-0700 [INFO] [observer] Watching directories: ['~/workspace/bentoml/examples/quickstart', '~/bentoml/models']\n```\n\n\nSend prediction request to the service:\n.. tab-set::\n.. tab-item:: HTTP\n      :sync: http\n\n\n```  .. tab-set::\n\n     .. tab-item:: Python\n        :sync: python-client\n\n        .. code-block:: python\n\n           import requests\n\n           requests.post(\n              \"http://127.0.0.1:3000/classify\",\n              headers={\"content-type\": \"application/json\"},\n              data=\"[[5.9, 3, 5.1, 1.8]]\",\n           ).text\n\n     .. tab-item:: CURL\n        :sync: curl-client\n\n        .. code-block:: bash\n\n           \u00bb curl -X POST \\\n              -H \"content-type: application/json\" \\\n              --data \"[[5.9, 3, 5.1, 1.8]]\" \\\n              http://127.0.0.1:3000/classify\n\n     .. tab-item:: Browser\n        :sync: browser-client\n\n        Open http://127.0.0.1:3000 in your browser and send test request from the web UI.\n```\n\n\n.. tab-item:: gRPC\n      :sync: grpc\n\n\n```  .. tab-set::\n\n     .. tab-item:: Python\n        :sync: python-client\n\n        .. code-block:: python\n\n           import grpc\n           import numpy as np\n           from bentoml.grpc.utils import import_generated_stubs\n\n           pb, services = import_generated_stubs()\n\n           with grpc.insecure_channel(\"localhost:3000\") as channel:\n              stub = services.BentoServiceStub(channel)\n\n              req: pb.Response = stub.Call(\n                 request=pb.Request(\n                       api_name=\"classify\",\n                       ndarray=pb.NDArray(\n                          dtype=pb.NDArray.DTYPE_FLOAT,\n                          shape=(1, 4),\n                          float_values=[5.9, 3, 5.1, 1.8],\n                       ),\n                 )\n              )\n              print(req)\n\n     .. tab-item:: grpcURL\n        :sync: curl-client\n\n        We will use `fullstorydev/grpcurl <https://github.com/fullstorydev/grpcurl>`_ to send a CURL-like request to the gRPC BentoServer.\n\n        Note that we will use `docker <https://docs.docker.com/get-docker/>`_ to run the ``grpcurl`` command.\n\n        .. tab-set::\n\n           .. tab-item:: MacOS/Windows\n              :sync: macwin\n\n              .. code-block:: bash\n\n                 \u00bb docker run -i --rm fullstorydev/grpcurl -d @ -plaintext host.docker.internal:3000 bentoml.grpc.v1.BentoService/Call <<EOM\n                 {\n                    \"apiName\": \"classify\",\n                    \"ndarray\": {\n                       \"shape\": [1, 4],\n                       \"floatValues\": [5.9, 3, 5.1, 1.8]\n                    }\n                 }\n                 EOM\n\n           .. tab-item:: Linux\n              :sync: Linux\n\n              .. code-block:: bash\n\n                 \u00bb docker run -i --rm --network=host fullstorydev/grpcurl -d @ -plaintext 0.0.0.0:3000 bentoml.grpc.v1.BentoService/Call <<EOM\n                 {\n                    \"apiName\": \"classify\",\n                    \"ndarray\": {\n                       \"shape\": [1, 4],\n                       \"floatValues\": [5.9, 3, 5.1, 1.8]\n                    }\n                 }\n                 EOM\n\n     .. tab-item:: Browser\n        :sync: browser-client\n\n        We will use `fullstorydev/grpcui <https://github.com/fullstorydev/grpcui>`_ to send request from a web browser.\n\n        Note that we will use `docker <https://docs.docker.com/get-docker/>`_ to run the ``grpcui`` command.\n\n        .. tab-set::\n\n           .. tab-item:: MacOS/Windows\n              :sync: macwin\n\n              .. code-block:: bash\n\n                 \u00bb docker run --init --rm -p 8080:8080 fullstorydev/grpcui -plaintext host.docker.internal:3000\n\n           .. tab-item:: Linux\n              :sync: Linux\n\n              .. code-block:: bash\n\n                 \u00bb docker run --init --rm -p 8080:8080 --network=host fullstorydev/grpcui -plaintext 0.0.0.0:3000\n\n\n        Proceed to http://127.0.0.1:8080 in your browser and send test request from the web UI.\n```\n\n\nUsing Models in a Service\n~~~~~~~~~~~~~~~~~~~~~~~~~\nIn this example, `bentoml.sklearn.get` creates a reference to the saved model\nin the model store, and `to_runner` creates a Runner instance from the model.\nThe Runner abstraction gives BentoServer more flexibility in terms of how to schedule\nthe inference computation, how to dynamically batch inference calls and better take\nadvantage of all hardware resource available.\nYou can test out the Runner interface this way:\n.. code-block:: python\nimport bentoml\niris_clf_runner = bentoml.sklearn.get(\"iris_clf:latest\").to_runner()\n   iris_clf_runner.init_local()\n   iris_clf_runner.predict.run([[5.9, 3., 5.1, 1.8]])\n.. note::\nFor custom Runners and advanced runner options, see :doc:`concepts/runner` and :doc:`guides/batching`.\nService API and IO Descriptor\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nThe `svc.api` decorator adds a function to the `bentoml.Service` object's\nAPIs list. The `input` and `output` parameter takes an\n:doc:`IO Descriptor <reference/api_io_descriptors>` object, which specifies the API\nfunction's expected input/output types, and is used for generating HTTP endpoints.\nIn this example, both `input` and `output` are defined with\n:ref:`bentoml.io.NumpyNdarray <reference/api_io_descriptors:NumPy \\``ndarray``>`, which means\nthe API function being decorated, takes a `numpy.ndarray` as input, and returns a\n`numpy.ndarray` as output.\n.. note::\nMore options, such as `pandas.DataFrame`, `JSON`, and `PIL.Image`\n   are also supported. An IO Descriptor object can also be configured with a schema or\n   a shape for input/output validation. Learn more about them in\n   :doc:`reference/api_io_descriptors`.\nInside the API function, users can define any business logic, feature fetching, and\nfeature transformation code. Model inference calls are made directly through runner\nobjects, that are passed into `bentoml.Service(name=.., runners=[..])` call when\ncreating the service object.\n.. tip::\nBentoML supports both :ref:`sync and async endpoints <concepts/service:Sync vs Async APIs>`.\n   For performance sensitive use cases, especially when working with IO-intense\n   workloads (e.g. fetching features from multiple sources) or when\n   :ref:`composing multiple models <concepts/runner:Serving Multiple Models via Runner>` , you may consider defining an\n   `async` API instead.\nHere's an example of the same endpoint above defined with `async`:\n.. code-block:: python\n\n\n```  @svc.api(input=NumpyNdarray(), output=NumpyNdarray())\n  async def classify(input_series: np.ndarray) -> np.ndarray:\n     result = await iris_clf_runner.predict.async_run(input_series)\n     return result\n```\n\n\nBuilding a Bento \ud83c\udf71\nOnce the service definition is finalized, we can build the model and service into a\n`bento`. Bento is the distribution format for a service. It is a self-contained\narchive that contains all the source code, model files and dependency specifications\nrequired to run the service.\nTo build a Bento, first create a `bentofile.yaml` file in your project directory:\n.. tab-set::\n\n\n```.. tab-item:: HTTP\n   :sync: http\n\n   .. code-block:: yaml\n\n      service: \"service:svc\"  # Same as the argument passed to `bentoml serve`\n      labels:\n         owner: bentoml-team\n         stage: dev\n      include:\n      - \"*.py\"  # A pattern for matching which files to include in the bento\n      python:\n         packages:  # Additional pip packages required by the service\n         - scikit-learn\n         - pandas\n\n.. tab-item:: gRPC\n   :sync: grpc\n\n   .. code-block:: yaml\n\n      service: \"service:svc\"  # Same as the argument passed to `bentoml serve`\n      labels:\n         owner: bentoml-team\n         stage: dev\n      include:\n      - \"*.py\"  # A pattern for matching which files to include in the bento\n      python:\n         packages:  # Additional pip packages required by the service\n         - bentoml[grpc]\n         - scikit-learn\n         - pandas\n```\n\n\n.. tip::\nBentoML provides lots of build options in `bentofile.yaml` for customizing the\n   Python dependencies, cuda installation, docker image distro, etc. Read more about it\n   on the :doc:`concepts/bento` page.\nNext, run the `bentoml build` CLI command from the same directory:\n.. code-block:: bash\n\n\n```\u00bb bentoml build\n\nBuilding BentoML service \"iris_classifier:6otbsmxzq6lwbgxi\" from build context \"/home/user/gallery/quickstart\"\nPacking model \"iris_clf:zy3dfgxzqkjrlgxi\"\nLocking PyPI package versions..\n\n\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2557\u2591\u2591\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2591\u2588\u2588\u2588\u2588\u2588\u2557\u2591\u2588\u2588\u2588\u2557\u2591\u2591\u2591\u2588\u2588\u2588\u2557\u2588\u2588\u2557\u2591\u2591\u2591\u2591\u2591\n\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2588\u2588\u2557\u2591\u2588\u2588\u2551\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2557\u2591\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551\u2591\u2591\u2591\u2591\u2591\n\u2588\u2588\u2588\u2588\u2588\u2588\u2566\u255d\u2588\u2588\u2588\u2588\u2588\u2557\u2591\u2591\u2588\u2588\u2554\u2588\u2588\u2557\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2588\u2588\u2551\u2588\u2588\u2554\u2588\u2588\u2588\u2588\u2554\u2588\u2588\u2551\u2588\u2588\u2551\u2591\u2591\u2591\u2591\u2591\n\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u255d\u2591\u2591\u2588\u2588\u2551\u255a\u2588\u2588\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2551\u2591\u2591\u2591\u2591\u2591\n\u2588\u2588\u2588\u2588\u2588\u2588\u2566\u255d\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551\u2591\u255a\u2588\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2591\u255a\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2591\u255a\u2550\u255d\u2591\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\n\u255a\u2550\u2550\u2550\u2550\u2550\u255d\u2591\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d\u2591\u2591\u255a\u2550\u2550\u255d\u2591\u2591\u2591\u255a\u2550\u255d\u2591\u2591\u2591\u2591\u255a\u2550\u2550\u2550\u2550\u255d\u2591\u255a\u2550\u255d\u2591\u2591\u2591\u2591\u2591\u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\nSuccessfully built Bento(tag=\"iris_classifier:6otbsmxzq6lwbgxi\")\n```\n\n\n\ud83c\udf89 You've just created your first Bento, and it is now ready for serving in production!\nFor starters, you can now serve it with the `bentoml serve` CLI command:\n.. tab-set::\n\n\n```.. tab-item:: HTTP\n   :sync: http\n\n   .. code-block:: bash\n\n      \u00bb bentoml serve iris_classifier:latest --production\n\n      2022-09-18T21:22:17-0700 [INFO] [cli] Environ for worker 0: set CPU thread count to 10\n      2022-09-18T21:22:17-0700 [INFO] [cli] Prometheus metrics for HTTP BentoServer from \"iris_classifier:latest\" can be accessed at http://0.0.0.0:3000/metrics.\n      2022-09-18T21:22:18-0700 [INFO] [cli] Starting production HTTP BentoServer from \"iris_classifier:latest\" running on http://0.0.0.0:3000 (Press CTRL+C to quit)\n\n.. tab-item:: gRPC\n   :sync: grpc\n\n   .. code-block:: bash\n\n      \u00bb bentoml serve-grpc iris_classifier:latest --production\n\n      2022-09-18T21:23:11-0700 [INFO] [cli] Environ for worker 0: set CPU thread count to 10\n      2022-09-18T21:23:11-0700 [INFO] [cli] Prometheus metrics for gRPC BentoServer from \"iris_classifier:latest\" can be accessed at http://0.0.0.0:3001.\n      2022-09-18T21:23:11-0700 [INFO] [cli] Starting production gRPC BentoServer from \"iris_classifier:latest\" running on http://0.0.0.0:3000 (Press CTRL+C to quit)\n```\n\n\n.. note::\nThe build process resolves `iris_clf:latest` and packages the latest version of the `iris_clf` model in the model store to ensure the same version of the model gets deployed every time.\nBento is the unit of deployment in BentoML, one of the most important artifacts to keep\ntrack of in your model deployment workflow. BentoML provides CLI commands and APIs for\nmanaging Bentos and moving them around, see the :ref:`concepts/bento:Managing Bentos`\nsection to learn more.\nGenerate Docker Image\nA docker image can be automatically generated from a Bento for production deployment,\nvia the `bentoml containerize` CLI command:\n.. tab-set::\n\n\n```.. tab-item:: HTTP\n   :sync: http\n\n   .. code-block:: bash\n\n      \u00bb bentoml containerize iris_classifier:latest\n\n      Building docker image for Bento(tag=\"iris_classifier:6otbsmxzq6lwbgxi\")...\n      Successfully built docker image for \"iris_classifier:6otbsmxzq6lwbgxi\" with tags \"iris_classifier:6otbsmxzq6lwbgxi\"\n      To run your newly built Bento container, pass \"iris_classifier:6otbsmxzq6lwbgxi\" to \"docker run\". For example: \"docker run -it --rm -p 3000:3000 iris_classifier:6otbsmxzq6lwbgxi serve --production\".\n\n.. tab-item:: gRPC\n   :sync: grpc\n\n   .. code-block:: bash\n\n      \u00bb bentoml containerize iris_classifier:latest --enable-features grpc\n\n      Building docker image for Bento(tag=\"iris_classifier:6otbsmxzq6lwbgxi\")...\n      Successfully built docker image for \"iris_classifier:6otbsmxzq6lwbgxi\" with tags \"iris_classifier:6otbsmxzq6lwbgxi\"\n      To run your newly built Bento container, pass \"iris_classifier:6otbsmxzq6lwbgxi\" to \"docker run\". For example: \"docker run -it --rm -p 3000:3000 iris_classifier:6otbsmxzq6lwbgxi serve --production\".\n      Additionally, to run your Bento container as a gRPC server, do: \"docker run -it --rm -p 3000:3000 -p 3001:3001 iris_classifier:6otbsmxzq6lwbgxi serve-grpc --production\"\n```\n\n\n.. note::\nYou will need to `install Docker <https://docs.docker.com/get-docker/>`_ before\n   running this command.\n.. dropdown:: For Mac with Apple Silicon\n   :icon: cpu\nSpecify the `--platform` to avoid potential compatibility issues with some\n   Python libraries.\n.. code-block:: bash\n\n\n```  \u00bb bentoml containerize --platform=linux/amd64 iris_classifier:latest\n```\n\n\nThis creates a docker image that includes the Bento, and has all its dependencies\ninstalled. The docker image tag will be same as the Bento tag by default:\n.. code-block:: bash\n\u00bb docker images\nREPOSITORY         TAG                 IMAGE ID        CREATED          SIZE\n   iris_classifier    6otbsmxzq6lwbgxi    0b4f5ec01941    10 seconds ago   1.06GB\nRun the docker image to start the BentoServer:\n.. tab-set::\n\n\n```.. tab-item:: HTTP\n   :sync: http\n\n   .. code-block:: bash\n\n      \u00bb docker run -it --rm -p 3000:3000 iris_classifier:6otbsmxzq6lwbgxi serve --production\n\n      2022-09-19T05:27:31+0000 [INFO] [cli] Service loaded from Bento directory: bentoml.Service(tag=\"iris_classifier:6otbsmxzq6lwbgxi\", path=\"/home/bentoml/bento/\")\n      2022-09-19T05:27:31+0000 [WARNING] [cli] GPU not detected. Unable to initialize pynvml lib.\n      2022-09-19T05:27:31+0000 [INFO] [cli] Environ for worker 0: set CPU thread count to 4\n      2022-09-19T05:27:31+0000 [INFO] [cli] Prometheus metrics for HTTP BentoServer from \"/home/bentoml/bento\" can be accessed at http://0.0.0.0:3000/metrics.\n      2022-09-19T05:27:32+0000 [INFO] [cli] Starting production HTTP BentoServer from \"/home/bentoml/bento\" running on http://0.0.0.0:3000 (Press CTRL+C to quit)\n      2022-09-19T05:27:32+0000 [INFO] [api_server:2] Service loaded from Bento directory: bentoml.Service(tag=\"iris_classifier:6otbsmxzq6lwbgxi\", path=\"/home/bentoml/bento/\")\n      2022-09-19T05:27:32+0000 [INFO] [api_server:1] Service loaded from Bento directory: bentoml.Service(tag=\"iris_classifier:6otbsmxzq6lwbgxi\", path=\"/home/bentoml/bento/\")\n      2022-09-19T05:27:32+0000 [INFO] [runner:iris_clf:1] Service loaded from Bento directory: bentoml.Service(tag=\"iris_classifier:6otbsmxzq6lwbgxi\", path=\"/home/bentoml/bento/\")\n      2022-09-19T05:27:32+0000 [INFO] [api_server:3] Service loaded from Bento directory: bentoml.Service(tag=\"iris_classifier:6otbsmxzq6lwbgxi\", path=\"/home/bentoml/bento/\")\n      2022-09-19T05:27:32+0000 [INFO] [api_server:4] Service loaded from Bento directory: bentoml.Service(tag=\"iris_classifier:6otbsmxzq6lwbgxi\", path=\"/home/bentoml/bento/\")\n\n.. tab-item:: gRPC\n   :sync: grpc\n\n   .. code-block:: bash\n\n      \u00bb docker run -it --rm -p 3000:3000 -p 3001:3001 iris_classifier:6otbsmxzq6lwbgxi serve-grpc --production\n\n      2022-09-19T05:28:29+0000 [INFO] [cli] Service loaded from Bento directory: bentoml.Service(tag=\"iris_classifier:6otbsmxzq6lwbgxi\", path=\"/home/bentoml/bento/\")\n      2022-09-19T05:28:29+0000 [WARNING] [cli] GPU not detected. Unable to initialize pynvml lib.\n      2022-09-19T05:28:29+0000 [INFO] [cli] Environ for worker 0: set CPU thread count to 4\n      2022-09-19T05:28:29+0000 [INFO] [cli] Prometheus metrics for gRPC BentoServer from \"/home/bentoml/bento\" can be accessed at http://0.0.0.0:3001.\n      2022-09-19T05:28:30+0000 [INFO] [cli] Starting production gRPC BentoServer from \"/home/bentoml/bento\" running on http://0.0.0.0:3000 (Press CTRL+C to quit)\n      2022-09-19T05:28:30+0000 [INFO] [grpc_api_server:2] Service loaded from Bento directory: bentoml.Service(tag=\"iris_classifier:6otbsmxzq6lwbgxi\", path=\"/home/bentoml/bento/\")\n      2022-09-19T05:28:30+0000 [INFO] [grpc_api_server:4] Service loaded from Bento directory: bentoml.Service(tag=\"iris_classifier:6otbsmxzq6lwbgxi\", path=\"/home/bentoml/bento/\")\n      2022-09-19T05:28:30+0000 [INFO] [grpc_api_server:3] Service loaded from Bento directory: bentoml.Service(tag=\"iris_classifier:6otbsmxzq6lwbgxi\", path=\"/home/bentoml/bento/\")\n      2022-09-19T05:28:30+0000 [INFO] [grpc_api_server:1] Service loaded from Bento directory: bentoml.Service(tag=\"iris_classifier:6otbsmxzq6lwbgxi\", path=\"/home/bentoml/bento/\")\n      2022-09-19T05:28:30+0000 [INFO] [runner:iris_clf:1] Service loaded from Bento directory: bentoml.Service(tag=\"iris_classifier:6otbsmxzq6lwbgxi\", path=\"/home/bentoml/bento/\")\n```\n\n\nMost of the deployment tools built on top of BentoML use Docker under the hood. It is\nrecommended to test out serving from a containerized Bento docker image first, before\nmoving to a production deployment. This helps verify the correctness of all the docker\nand dependency configs specified in the `bentofile.yaml`.\nDeploying Bentos\nBentoML standardizes the saved model format, service API definition and the Bento build\nprocess, which opens up many different deployment options for ML teams.\nThe Bento we built and the docker image created in the previous steps are designed to\nbe DevOps friendly and ready for deployment in a production environment. If your team\nhas existing infrastructure for running docker, it's likely that the Bento generated\ndocker images can be directly deployed to your infrastructure without any modification.\n.. note::\nTo streamline the deployment process, BentoServer follows most common best practices\n   found in a backend service: it provides\n   :doc:`health check and prometheus metrics <guides/monitoring>`\n   endpoints for monitoring out-of-the-box; It provides configurable\n   :doc:`distributed tracing <guides/tracing>` and :doc:`logging <guides/logging>` for\n   performance analysis and debugging; and it can be easily\n   :doc:`integrated with other tools <integrations/index>` that are commonly used by\n   Data Engineers and DevOps engineers.\nFor teams looking for an end-to-end solution, with more powerful deployment features\nspecific for ML, the BentoML team has also created Yatai and bentoctl:\n.. grid::  1 2 2 2\n    :gutter: 3\n    :margin: 0\n    :padding: 0\n\n\n```.. grid-item-card:: `\ud83e\udd84\ufe0f Yatai <https://github.com/bentoml/Yatai>`_\n    :link: https://github.com/bentoml/Yatai\n    :link-type: url\n\n    Model Deployment at scale on Kubernetes.\n\n.. grid-item-card:: `\ud83d\ude80 bentoctl <https://github.com/bentoml/bentoctl>`_\n    :link: https://github.com/bentoml/bentoctl\n    :link-type: url\n\n    Fast model deployment on any cloud platform.\n```\n\n\nLearn more about different deployment options with BentoML from the\n:doc:`concepts/deploy` page.\n\n.. button-ref:: concepts/index\n   :ref-type: doc\n   :color: secondary\n   :expand:",
    "tag": "bentoml"
  },
  {
    "title": "installation.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/installation.rst",
    "content": "============\nInstallation\n============\n\ud83c\udf71 BentoML is distributed as a Python Package available `on PyPI <https://pypi.org/project/bentoml/>`_.\nInstall BentoML alongside with whichever deep learning library you are working with, and you are ready to go!\n\nBentoML supports Linux/UNIX, Windows, and MacOS.\nBentoML requires Python 3.7 or above.\n\n.. code-block::\npip install bentoml\nTo install all additional features in BentoML, such as gRPC, S3 support, and more, use the `all` variant. Features can also be installed separate later.\n.. code-block:: bash\n\n\n```pip install \"bentoml[all]\"\n```\n\n\nInstall from Source\nIf you want to install BentoML from source, run the following command:\n.. code-block:: bash\n\n\n```pip install git+https://github.com/bentoml/bentoml\n```\n\n\nThis will install the bleeding edge `main` version of BentoML. The `main` version is\nuseful for stay-up-to-date with the latest features and bug fixes. However, this means\nthat `main` version is not always stable. If you run into any issues, please either\ncreate `an issue <https://github.com/bentoml/BentoML/issues/new/choose>` or join our\n`community Slack <https://l.linklyhq.com/l/ktOX>` to get help.\nEditable Install\nYou may want an editable install if:\n\nYou want to stay-up-to-date with the latest features and bug fixes\nYou want to contribute to \ud83c\udf71 BentoML and test code changes\n\n.. note::\nMake sure that you have the following requirements:\n    - `Git <https://git-scm.com/>`\n    - `pip <https://pip.pypa.io/en/stable/installation/>`\n    - `Python3.7+ <https://www.python.org/downloads/>`_\n.. seealso::\nYou're always welcome to make contributions to the project and its documentation. Check out the\n    `BentoML development guide <https://github.com/bentoml/BentoML/blob/main/DEVELOPMENT.md>`\n    and `documentation guide <https://github.com/bentoml/BentoML/blob/main/docs/README.md>`\n    to get started.\nClone the repository to your local folder and install \ud83c\udf71 BentoML with the following command:\n.. code-block:: bash\n\n\n```git clone https://github.com/bentoml/bentoml.git\ncd bentoml\npip install -e .\n```\n\n\nThis command will install \ud83c\udf71 BentoML in `editable mode\n<https://pip.pypa.io/en/stable/topics/local-project-installs/#editable-installs>`_,\nwhich allows you to install the project without copying any files. Python will link this\nfolder and add it to Python library paths. This means that any changes you make to the\nfolder will and can be tested immediately.\n.. dropdown:: For user using `setuptools>=64.0.0`\n   :icon: question\nBentoML uses `setuptools <https://setuptools.pypa.io/en/latest/>` to build and\n   package the project. Since `setuptools>=64.0.0`, setuptools implemented `PEP 660 <https://peps.python.org/pep-0660/>`, which changes the behavior of editable install in comparison with previous version of setuptools.\nCurrently, BentoML is not compatible with this new behavior. To install BentoML in editable mode, you have to pass `--config-settings editable_mode=compat` to `pip`.\n.. code-block:: bash\n\n\n```  pip install -e \".[grpc]\" --config-settings editable_mode=compat\n```\n\n\nSee setuptools' `development mode guide <https://setuptools.pypa.io/en/latest/userguide/development_mode.html>`_ for more information.\n.. warning::\nYou must not remove `bentoml` folder after installing in editable mode to keep using\n   the library.\nAfter that you can easily update your clone with the latest changes on `main` branch\nwith the following command:\n.. code-block:: bash\n\n\n```cd bentoml\n```\n\n",
    "tag": "bentoml"
  },
  {
    "title": "mlflow.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/integrations/mlflow.rst",
    "content": "======\nMLflow\n======\n`MLflow <https://mlflow.org/>`_ is an open source framework for tracking ML experiments,\npackaging ML code for training pipelines, and capturing models logged from experiments.\nIt enables data scientists to iterate quickly during model development while keeping\ntheir experiments and training pipelines reproducible.\nBentoML, on the other hand, focuses on ML in production. By design, BentoML is agnostic\nto the experimentation platform and the model development environment.\nComparing to the MLflow model registry, BentoML's model format and model store is\ndesigned for managing model artifacts that will be used for building, testing, and\ndeploying prediction services. It is best fitted to manage your \u201cfinalized model\u201d, sets\nof models that yield the best outcomes from your periodic training pipelines and are\nmeant for running in production.\nBentoML integrates with MLflow natively. Users can not only port over models logged with\nMLflow Tracking to BentoML for high-performance model serving but also combine MLFlow\nprojects and pipelines with BentoML's model deployment workflow in an efficient manner.\nCompatibility\nBentoML supports MLflow 0.9 and above.\nExamples\nBesides this documentation, also check out code samples demonstrating BentoML and MLflow\nintegration at: `bentoml/examples: MLflow Examples <https://github.com/bentoml/BentoML/tree/main/examples/mlflow>`_.\nImport an MLflow model\n`MLflow Model <https://www.mlflow.org/docs/latest/models.html>`_ is a format for saving\ntrained model artifacts in MLflow experiments and pipelines. BentoML supports importing\nMLflow model to its own format for model serving. For example:\n.. code-block:: python\n\n\n```mlflow.sklearn.save_model(model, \"./my_model\")\nbentoml.mlflow.import_model(\"my_sklearn_model\", model_uri=\"./my_model\")\n```\n\n\n.. code-block:: python\n\n\n```with mlflow.start_run():\n    mlflow.pytorch.log_model(model, artifact_path=\"pytorch-model\")\n\n    model_uri = mlflow.get_artifact_uri(\"pytorch-model\")\n    bento_model = bentoml.mlflow.import_model(\n        'mlflow_pytorch_mnist',\n        model_uri,\n        signatures={'predict': {'batchable': True}}\n    )\n```\n\n\nThe `bentoml.mlflow.import_model` API is similar to the other `save_model` APIs\nfound in BentoML, where the first argument represent the model name in BentoML model\nstore. A new version will be automatically generated when a new MLflow model is\nimported. Users can manage imported MLflow models same as models saved with other ML\nframeworks:\n.. code-block:: bash\n\n\n```bentoml models list mlflow_pytorch_mnist\n```\n\n\nThe second argument `model_uri` takes a URI to the MLflow model. It can be a local\npath, a `'runs:/'` URI, or a remote storage URI (e.g., an `'s3://'` URI). Here are\nsome example `model_uri` values commonly used in MLflow:\n.. code-block::\n\n\n```/Users/me/path/to/local/model\n../relative/path/to/local/model\ns3://my_bucket/path/to/model\nruns:/<mlflow_run_id>/run-relative/path/to/model\nmodels:/<model_name>/<model_version>\nmodels:/<model_name>/<stage>\n```\n\n\nRunning Imported Model\nMLflow models imported to BentoML can be loaded back for running inference in a various\nof ways.\nLoading original model flavor\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nFor evaluation and testing purpose, sometimes it's convenient to load the model in its\nnative form\n.. code-block:: python\n\n\n```bento_model = bentoml.mlflow.get(\"mlflow_pytorch_mnist:latest\")\nmlflow_model_path = bento_model.path_of(bentoml.mlflow.MLFLOW_MODEL_FOLDER)\n\nloaded_pytorch_model = mlflow.pytorch.load_model(mlflow_model_path)\nloaded_pytorch_model.to(device)\nloaded_pytorch_model.eval()\nwith torch.no_grad():\n    input_tensor = torch.from_numpy(test_input_arr).to(device)\n    predictions = loaded_pytorch_model(input_tensor)\n```\n\n\nLoading Pyfunc flavor\n~~~~~~~~~~~~~~~~~~~~~\nBy default, `bentoml.mflow.load_model` will load the imported MLflow model using the\n`python_function flavor <https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html>`_\nfor best compatibility across all ML frameworks supported by MLflow.\n.. code-block:: python\n\n\n```pyfunc_model: mlflow.pyfunc.PyFuncModel = bentoml.mlflow.load_model(\"mlflow_pytorch_mnist:latest\")\npredictions = pyfunc_model.predict(test_input_arr)\n```\n\n\nUsing Model Runner\n~~~~~~~~~~~~~~~~~~\nImported MLflow models can be loaded as BentoML Runner for best performance in building\nprediction service with BentoML. To test out the runner API:\n.. code-block:: python\n\n\n```runner = bentoml.mlflow.get(\"mlflow_pytorch_mnist:latest\").to_runner()\nrunner.init_local()\nrunner.predict.run(input_df)\n```\n\n\nLearn more about BentoML Runner at :doc:`/concepts/runner`.\nRunner created from an MLflow model supports the following input types. Note that for\nsome ML frameworks, only a subset of this list is supported.\n.. code-block:: python\n\n\n```MLflowRunnerInput = Union[pandas.DataFrame, np.ndarray, List[Any], Dict[str, Any]]\nMLflowRunnerOutput = Union[pandas.DataFrame, pandas.Series, np.ndarray, list]\n```\n\n\n.. note::\n\n\n```To use adaptive batching with a MLflow Runner, make sure to set\n``signatures={'predict': {'batchable': True}}`` when importing the model:\n\n.. code-block:: python\n\n    bento_model = bentoml.mlflow.import_model(\n        'mlflow_pytorch_mnist',\n        model_uri,\n        signatures={'predict': {'batchable': True}}\n    )\n```\n\n\nOptimizations\n~~~~~~~~~~~~~\nThere are two major limitations of using MLflow Runner in BentoML:\n\nLack of support for GPU\nLack of support for multiple inference method\n\nA common optimization we recommend, is to save trained model instance directly with BentoML,\ninstead of importing MLflow pyfunc model. This makes it possible to run GPU inference and expose \nmultiple inference signatures.\n\nSave model directly with bentoml\n\n.. code-block:: python\n\n\n```mlflow.sklearn.log_model(clf, \"model\")\nbentoml.sklearn.save_model(\"iris_clf\", clf)\n```\n\n\n\nLoad original flavor and save with BentoML\n\n.. code-block:: python\n\n\n```loaded_model = mlflow.sklearn.load_model(model_uri)\nbentoml.sklearn.save_model(\"iris_clf\", loaded_model)\n```\n\n\nThis way, it goes back to a typically BentoML workflow, which allow users to use a\nRunner specifically built for the target ML framework, with GPU support and multiple\nsignatures available.\nBuild Prediction Service\nHere's an example `bentoml.Service` built with a MLflow model:\n.. code-block:: python\n\n\n```import bentoml\nimport mlflow\nimport torch\n\nmnist_runner = bentoml.mlflow.get('mlflow_pytorch_mnist:latest').to_runner()\n\nsvc = bentoml.Service('mlflow_pytorch_mnist', runners=[ mnist_runner ])\n\ninput_spec = bentoml.io.NumpyNdarray(\n    dtype=\"float32\",\n    shape=[-1, 1, 28, 28],\n    enforce_shape=True,\n    enforce_dtype=True,\n)\n\n@svc.api(input=input_spec, output=bentoml.io.NumpyNdarray())\ndef predict(input_arr):\n    return mnist_runner.predict.run(input_arr)\n```\n\n\nTo try out the full example, visit `bentoml/examples: MLflow Pytorch Example <https://github.com/bentoml/BentoML/tree/main/examples/mlflow/pytorch>`_.\nMLflow \ud83e\udd1d BentoML Workflow\nThere are numerous ways you can integrate BentoML with your MLflow workflow for model serving and deployment.\n\nFind `model_uri` from a MLflow model instance returned from `log_model`:\n\n.. code-block:: python\n\n\n```# https://github.com/bentoml/BentoML/tree/main/examples/mlflow/sklearn_logistic_regression\nlogged_model = mlflow.sklearn.log_model(lr, \"model\")\nprint(\"Model saved in run %s\" % mlflow.active_run().info.run_uuid)\n\n# Import logged mlflow model to BentoML model store for serving:\nbento_model = bentoml.mlflow.import_model('logistic_regression_model', logged_model.model_uri)\nprint(\"Model imported to BentoML: %s\" % bento_model)\n```\n\n\n\nFind model artifact path inside current `mlflow.run` scope:\n\n.. code-block:: python\n\n\n```# https://github.com/bentoml/BentoML/tree/main/examples/mlflow/pytorch\nwith mlflow.start_run():\n    ...\n    mlflow.pytorch.log_model(model, artifact_path=\"pytorch-model\")\n    model_uri = mlflow.get_artifact_uri(\"pytorch-model\")\n    bento_model = bentoml.mlflow.import_model('mlflow_pytorch_mnist', model_uri)\n```\n\n\n\nWhen using `autolog`, find `model_uri` by last active `run_id`:\n\n.. code-block:: python\n\n\n```import mlflow\nimport bentoml\nfrom sklearn.linear_model import LinearRegression\n\n# enable autologging\nmlflow.sklearn.autolog()\n\n# prepare training data\nX = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\ny = np.dot(X, np.array([1, 2])) + 3\n\n# train a model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# import logged MLflow model to BentoML\nrun_id = mlflow.last_active_run().info.run_id\nartifact_path = \"model\"\nmodel_uri = f\"runs:/{run_id}/{artifact_path}\"\nbento_model = bentoml.mlflow.import_model('logistic_regression_model', model_uri)\nprint(f\"Model imported to BentoML: {bento_model}\")\n```\n\n\n\nImport a registered model on MLflow server\n\nWhen using a MLflow tracking server, users can also import\n`registered models <https://www.mlflow.org/docs/latest/model-registry.html#registering-a-model>`_\ndirectly to BentoML for serving.\n.. code-block:: python\n\n\n```# Import from a version:\nmodel_name = \"sk-learn-random-forest-reg-model\"\nmodel_version = 1\nmodel_uri=f\"models:/{model_name}/{model_version}\"\nbentoml.mlflow.import_model('my_mlflow_model', model_uri)\n\n# Import from a stage:\nmodel_name = \"sk-learn-random-forest-reg-model\"\nstage = 'Staging'\nmodel_uri=f\"models:/{model_name}/{stage}\"\nbentoml.mlflow.import_model('my_mlflow_model', model_uri)\n```\n\n\nAdditional Tips\nUse MLflow model dependencies config\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nMost MLflow models bundles dependency information that is required for running framework model. \nIf no additional dependencies are required in the :obj:`~bentoml.Service` definition code, users may\npass through dependency requirements from within MLflow model to BentoML.\nFirst, put the following in your `bentofile.yaml` build file:\n.. code-block:: yaml\n\n\n```python:\n    requirements_txt: $BENTOML_MLFLOW_MODEL_PATH/mlflow_model/requirements.txt\n    lock_packages: False\n```\n\n\nAlternatively, one can also use MLflow model's generated conda environment file:\n.. code-block:: yaml\n\n\n```conda:\n    environment_yml: $BENTOML_MLFLOW_MODEL_PATH/mlflow_model/conda.yaml\n```\n\n\nThis allows BentoML to dynamically find the given dependency file based on a user-defined\nenvironment variable. In this case, the `bentoml get` CLI returns the path to the target\nMLflow model folder and expose it to `bentoml build` via the environment variable\n`BENTOML_MLFLOW_MODEL_PATH`:\n.. code-block:: bash\n\n\n```export BENTOML_MLFLOW_MODEL_PATH=$(bentoml models get my_mlflow_model:latest -o path)\nbentoml build\n```\n\n\nAttach model params, metrics, and tags\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nMLflow model format encapsulates lots of context information regarding the training metrics\nand parameters. The following code snippet demonstrates how to package metadata logged from a given MLflow model to the BentoML model store.\n.. code-block:: python\n\n\n```run_id = '0e4425ecbf3e4672ba0c1741651bb47a'\nrun = mlflow.get_run(run_id)\nmodel_uri = f\"{run.info.artifact_uri}/model\"\nbentoml.mlflow.import_model(\n    \"my_mlflow_model\",\n    model_uri,\n    labels=run.data.tags,\n    metadata={\n        \"metrics\": run.data.metrics,\n        \"params\": run.data.params,\n    }\n```\n\n",
    "tag": "bentoml"
  },
  {
    "title": "airflow.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/integrations/airflow.rst",
    "content": "=======\nAirflow\n=======\nApache Airflow is a platform to programmatically author, schedule and monitor workflows.\nIt is a commonly used framework for building model training pipelines in ML projects.\nBentoML provides a flexible set of APIs for integrating natively with Apache Airflow.\nUsers can use Airflow to schedule their model training pipelines and use BentoML to keep\ntracked of trained model artifacts and optionally deploy them to production in an\nautomated fashion.\nThis is especially userful for teams that can benefit from retraining models often with\nnewly arrived data, and want to update their production models regularly with\nconfidence.\nFor more in-depth Airflow tutorials, please visit `the Airflow documentation <https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html>`_.\nOverview\nA typical Airflow pipeline with a BentoML serving & deployment workflow look like this:\n\nFetch new data batches from a data source\nSplit the data in train and test sets\nPerform feature extraction on training data set\nTrain a new model using the training data set\nPerform model evaluation and validation\n:doc:`Save model with BentoML </concepts/model>`\n:ref:`Push saved model to Yatai registry (or export model to s3) <concepts/model:Managing Models>`\n:doc:`Build a new Bento using the newly trained model </concepts/bento>`\nRun integration test on the Bento to verify the entire serving pipeline\n:ref:`Push the Bento to a Yatai (or export bento to s3) <concepts/bento:Managing Bentos>`\n(Optional) Trigger a redeployment via Yatai, bentoctl, or custom deploy script\n\nPro Tips\nPipeline Dependencies\n~~~~~~~~~~~~~~~~~~~~~\nThe default PythonOperator requires all the dependencies to be installed on the Airflow\nenvironment. This can be challenging to manage when the pipeline is running on a remote\nAirflow deployment and running a mix of different tasks.\nTo avoid this, we recommend managing dependencies of your ML pipeline with the\n`PythonVirtualenvOperator <https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/python.html#pythonvirtualenvoperator>`_,\nwhich runs your code in a virtual environment. This allows you to define your Bento's\ndependencies in a `requirements.txt` file and use it across training pipeline and the\nbento build process. For example:\n.. code-block:: python\n\n\n```from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.decorators import task\n\nwith DAG(\n    dag_id='example_bento_build_operator',\n    description='A simple tutorial DAG with BentoML',\n    schedule_interval=timedelta(days=1),\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=['example'],\n) as dag:\n\n    @task.virtualenv(\n        task_id=\"bento_build\",\n        requirements='./requirements.txt',\n        system_site_packages=False,\n        provide_context=True,\n    )\n    def build_bento(**context):\n        \"\"\"\n        Perform Bento build in a virtual environment.\n        \"\"\"\n        import bentoml\n        bento = bentoml.bentos.build(\n            \"service.py:svc\",\n            labels={\n                \"job_id\": context.run_id\n            },\n            python={\n                requirements_txt: \"./requirements.txt\"\n            },\n            include=[\"*\"],\n        )\n\n    build_bento_task = build_bento()\n```\n\n\nArtifact Management\n~~~~~~~~~~~~~~~~~~~\nSince Airflow is a distributed system, it is important to save the\n:doc:`Models </concepts/model>` and :doc:`Bentos </concepts/bento>` produced in your\nAirflow pipeline to a central location that is accessible by all the nodes in the\nAirflow cluster, and also by the workers in your production deployment environment.\nFor a simple setup, we recommend using the Import/Export API for\n:ref:`Model <concepts/model:Managing Models>` and\n:ref:`Bento <concepts/bento:Managing Bentos>`. This allows you to export the model files\ndirectly to cloud storage, and import them from the same location when needed. E.g:\n.. code-block:: python\n\n\n```bentoml.models.export_model('s3://my_bucket/folder/')\nbentoml.models.import_model('s3://my_bucket/folder/iris_clf-3vl5n7qkcwqe5uqj.bentomodel')\n\nbentoml.export_bento('s3://my_bucket/bentos/')\nbentoml.import_bento('s3://my_bucket/bentos/iris_classifier-7soszfq53sv6huqj.bento')\n```\n\n\nFor a more advanced setup, we recommend using the Model and Bento Registry feature\nprovided in `Yatai <https://github.com/bentoml/Yatai>`_, which provides additional\nmanagement features such as filtering, labels, and a web UI for browsing and managing\nmodels. E.g:\n.. code-block:: python\n\n\n```bentoml.models.push(\"iris_clf:latest\")\nbentoml.models.pull(\"iris_clf:3vl5n7qkcwqe5uqj\")\n\nbentoml.push(\"iris_classifier:latest\")\nbentoml.pull(\"iris_classifier:mcjbijq6j2yhiusu\")\n```\n\n\nPython API or CLI\n~~~~~~~~~~~~~~~~~\nBentoML provides both Python APIs and CLI commands for most workflow management tasks,\nsuch as building Bento, managing Models/Bentos, and deploying to production.\nWhen using the Python APIs, you can organize your code in a Airflow PythonOperator task.\nAnd for CLI commands, you can use the `BashOperator <https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/bash.html>`_\ninstead.\nValidating new Bento\n~~~~~~~~~~~~~~~~~~~~\nIt is important to validate the new Bento before deploying it to production. The\n`bentoml.testing` module provides a set of utility functions for building behavior tests\nfor your BentoML Service, by launching the API server in a docker container and sending\ntest requests to it.\nThe BentoML community is also building a standardized way of defining and running\ntest cases for your Bento, that can be easily integrated with your CI/CD pipeline in\nan Airflow job. See `#2967 <https://github.com/bentoml/BentoML/issues/2967>`_ for the\nlatest progress.\nSaving model metadata\n~~~~~~~~~~~~~~~~~~~~~\nWhen saving a model with BentoML, you can pass in a dictionary of metadata to be saved\ntogether with the model. This can be useful for tracking model evaluation metrics and\ntraining context, such as the training dataset timestamp, training code version, or\ntraining parameters.\nSample Project\nThe following is a sample project created by the BentoML community member Sarah Floris\uff0c\nthat demonstrates how to use BentoML with Airflow:\n\n\ud83d\udcd6 `Deploying BentoML using Airflow <https://medium.com/codex/deploying-bentoml-using-airflow-28972343ac68>`_\n\ud83d\udcbb `Source Code <https://github.com/sdf94/bentoml-airflow>`_\n",
    "tag": "bentoml"
  },
  {
    "title": "flink.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/integrations/flink.rst",
    "content": "============\nApache Flink\n============\nApache Flink DataStream\nBentoML support stream model inferencing in \n`Apache Flink DataStream API <https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/overview/>`_ \nthrough either embedded runners or remote calls to a separated deployed Bento Service. This guide assumes prior knowledge \non using runners and service APIs.\nEmbedded Model Runners\n^^^^^^^^^^^^^^^^^^^^^^\nIn BentoML, a :ref:`Runner <concepts/runner:Using Runners>` \nrepresents a unit of computation, such as model inferencing, that can executed on either a remote runner process or an \nembedded runner instance. If available system resources allow loading the ML model in memory, invoking runners as embedded \ninstances can typically achieve a better performance by avoiding the overhead incurred in the interprocess communication.\nRunners can be initialized as embedded instances by calling `init_local()`. Once a runner is initialized, inference functions \ncan be invoked on the runners.\n.. code:: python\n\n\n```import bentoml\n\niris_runner = bentoml.transformers.get(\"text-classification:latest\")).to_runner()\niris_runner.init_local()\niris_runner.predict.run(INPUT_TEXT)\n```\n\n\nTo integrate with Flink DataRunners API, runners can be used in `ProcessWindowFunction`` for iterative inferencing or a`WindowFunction` for batched inferencing.\n.. code:: python\n\n\n```import bentoml\n\nfrom pyflink.datastream import StreamExecutionEnvironment\nfrom pyflink.datastream.functions import RuntimeContext, MapFunction\n\nclass ClassifyFunction(MapFunction):\n    def open(self, runtime_context: RuntimeContext):\n        self.runner = bentoml.transformers.get(\n            \"text-classification:latest\"\n        ).to_runner()\n        self.runner.init_local()\n\n    def map(self, data):\n        # transform(data)\n        return data[0], self.runner.run(data[1])\n```\n\n\nThe following is an end-to-end word classification example of using embedded runners in a Flink DataStream program. \nFor simplicity, the input stream and output sink are abstracted out using in-memory collections and stdout sink.\n.. code:: python\n\n\n```import bentoml\nimport logging\nimport sys\n\nfrom pyflink.datastream import StreamExecutionEnvironment\nfrom pyflink.datastream.functions import RuntimeContext, MapFunction\n\n\nclass ClassifyFunction(MapFunction):\n    def open(self, runtime_context: RuntimeContext):\n        self.runner = bentoml.transformers.get(\"text-classification:latest\").to_runner()\n        self.runner.init_local()\n\n    def map(self, data):\n        # transform(data)\n        return data[0], self.runner.run(data[1])\n\n\ndef classify_tweets():\n    # Create a StreamExecutionEnvironment\n    env = StreamExecutionEnvironment.get_execution_environment()\n    env.set_parallelism(1)\n\n    # Create source DataStream, e.g. Kafka, Table, etc. Example uses data collection for simplicity.\n    ds = env.from_collection(\n        collection=[\n            (1, \"BentoML: Create an ML Powered Prediction Service in Minutes via @TDataScience https://buff.ly/3srhTw9 #Python #MachineLearning #BentoML\"),\n            (2, \"Top MLOps Serving frameworks\u200a\u2014\u200a2021 https://link.medium.com/5Elq6Aw52ib #mlops #TritonInferenceServer #opensource #nvidia #machincelearning  #serving #tensorflow #PyTorch #Bodywork #BentoML #KFServing #kubeflow #Cortex #Seldon #Sagify #Syndicai\"),\n            (3, \"#MLFlow provides components for experimentation management, ML project management. #BentoML only focuses on serving and deploying trained models\"),\n            (4, \"2000 and beyond #OpenSource #bentoml\"),\n            (5, \"Model Serving Made Easy https://github.com/bentoml/BentoML \u2b50 1.1K #Python #Bentoml #BentoML #Modelserving #Modeldeployment #Modelmanagement #Mlplatform #Mlinfrastructure #Ml #Ai #Machinelearning #Awssagemaker #Awslambda #Azureml #Mlops #Aiops #Machinelearningoperations #Turn\")\n        ]\n    )\n\n    # Define the execution logic\n    ds = ds.map(ClassifyFunction())\n\n    # Create sink and emit result to sink, e.g. Kafka, File, Table, etc. Example prints to stdout for simplicity.\n    ds.print()\n\n    # Submit for execution\n    env.execute()\n\n\nif __name__ == '__main__':\n    logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(message)s\")\n    classify_tweets()\n```\n\n\nRemote Bento Service\n^^^^^^^^^^^^^^^^^^^^\nModel runners can also be invoked remotely as a separately deployed Bento Service. Calling a remote Bento Service may be \npreferred if the model cannot be loaded into memory of the Flink DataStream program. This options is also advantageous because \nmodel runners can be scaled more easily with deployment frameworks like :ref:`Yatai <concepts/deploy:Deploy with Yatai>`.\nTo send a prediction request to a remotely deployed Bento Service in the DataStream program, you can use any HTTP client \nimplementation of your choice inside the `MapFunction` or `ProcessWindowFunction`.\n.. code:: python\n\n\n```class ClassifyFunction(MapFunction):\n    def map(self, data):\n        return requests.post(\n            \"http://127.0.0.1:3000/classify\",\n            headers={\"content-type\": \"text/plain\"},\n            data=TEXT_INPUT,\n        ).text\n```\n\n\nUsing a client with asynchronous IO support combined with Flink AsyncFunction is recommended to handle requests and responses ",
    "tag": "bentoml"
  },
  {
    "title": "spark.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/integrations/spark.rst",
    "content": "=====\nSpark\n=====\n`Apache Spark <https://spark.apache.org/>`_ is a general-purpose distributed processing system used\nfor big data workloads. It allows for processing large datasets through an in-memory computation\nmodel, which can improve the performance of big data processing tasks. It also provides a wide range\nof APIs and a feature-rich set of tools for structured data processing, machine learning, and stream\nprocessing for big-data applications.\nBentoML now supports running your Bentos with batch data via Spark. The following tutorial assumes\nbasic understanding of BentoML. If you'd like to learn more about BentoML, see the\n:ref:`BentoML tutorial <tutorial:Creating a Service>`.\nPrerequisites\n\nMake sure to have at least BentoML 1.0.13 and Spark version 3.3.0 available in your system.\n.. code-block:: bash\n\n\n```$ pip install -U \"bentoml>=1.0.13\"\n```\n\n\nIn addition, both BentoML and your service's dependencies (including model dependencies) must also\nbe installed in the Spark cluster. Most likely, the service you are hosting Spark on has its own\nmechanisms for doing this. If you are using a standalone cluster, you should install those\ndependencies on every node you expect to use.\nFinally, we use the quickstart bento from the :ref:`aforementioned tutorial <tutorial>`. If you have\nalready followed that tutorial, you should already have that bento. If you have note, simply run the\nfollowing:\n.. code-block:: python\n\n\n```import urllib.request\nurllib.request.urlretrieve(\"https://bentoml-public.s3.us-west-1.amazonaws.com/quickstart/iris_classifier.bento\", \"iris_classifier.bento\")\nbentoml.import_bento(\"iris_classifier.bento\")\n```\n\n\nRun Bentos in Spark\n\n.. note::\n\n\n```All of the following commands/APIs should work for bentos with\n:ref:`IO Descriptor <reference/api_io_descriptors:API IO Descriptors>` that support batch\ninference. Currently, those descriptors are\n:ref:`bentoml.io.NumpyNdarray <reference/api_io_descriptors:NumPy \\`\\`ndarray\\`\\`>`,\n:ref:`bentoml.io.PandasDataFrame, and bentoml.io.PandasSeries <reference/api_io_descriptors:Tabular Data with Pandas>`.\n```\n\n\n:bdg-warning:`IMPORTANT:` your Bento API must be capable of accepting multiple inputs. For example,\n`batch_classify(np.array([[input_1], [input_2]]))` must work, and return\n`np.array([[output_1], [output_2]])`. The quickstart bento supports this pattern because the iris\nclassifier model it contains does.\nCreate a PySpark SparkSession object\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nThis will be used to create a DataFrame from the input\ndata, and to run the batch inference job. If you're running in a notebook with spark already\n(e.g. a VertexAI PySpark notebook or a Databricks Notebook), you can skip this step.\n.. code-block:: python\n\n\n```from pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n```\n\n\nLoad the input data into a PySpark DataFrame\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nIf you are using multipart input, or your dataframe\nrequires column names, you must also provide a schema for your DataFrame as you load it. You can\ndo this using the `spark.read.csv()` method, which takes a file path as input and returns a\nDataFrame containing the data from the file.\n.. code-block:: python\n\n\n```from pyspark.sql.types import StructType, StructField, FloatType, StringType\nimport urllib.request\n\nurllib.request.urlretrieve(\"https://docs.bentoml.org/en/latest/_static/examples/batch/input.csv\", \"input.csv\")\n\nschema = StructType([\n    StructField(\"sepal_length\", FloatType(), False),\n    StructField(\"sepal_width\", FloatType(), False),\n    StructField(\"petal_length\", FloatType(), False),\n    StructField(\"petal_width\", FloatType(), False),\n])\ndf = spark.read.csv(\"input.csv\", schema=schema)\n```\n\n\nCreate a BentoService object\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nCreate a BentoService object using the BentoML service you want to use for the batch inference\njob. Here, we first try to use `bentoml.get` to get the bento from the local BentoML store. If it\nis not found, we retrieve the bento from the BentoML public S3 and import it.\n.. code-block:: python\n\n\n```import bentoml\n\nbento = bentoml.get(\"iris_classifier:latest\")\n```\n\n\nRun the batch inference job\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRun the batch inference job using the `bentoml.batch.run_in_spark()` method. This method takes\nthe API name, the Spark DataFrame containing the input data, and the Spark session itself as\nparameters, and it returns a DataFrame containing the results of the batch inference job.\n.. code-block:: python\n\n\n```results_df = bentoml.batch.run_in_spark(bento, \"classify\", df, spark)\n```\n\n\nInternally, what happens when you run `run_in_spark` is as follows:\n\n\nFirst, the bento is distributed to the cluster. Note that if the bento has already been\n  distributed, i.e. you have already run a computation with that bento, this step is skipped.\n\n\nNext, a process function is created, which runs the API method on every Spark batch given it. The\n  batch size can be controlled by setting `spark.sql.execution.arrow.maxRecordsPerBatch`. PySpark\n  pickles this process function and dispatches it, along with the relevant data, to the workers.\n\n\nFinally, the function is evaluated on the given dataframe. Once all methods that the user defined\n  in the script have been executed, the data is returned to the master node.\n\n\nSave the results\n^^^^^^^^^^^^^^^^\nFinally, save the results of the batch inference job to a file using the\n`DataFrame.write.csv()` method. This method takes a file path as input and saves the contents\nof the DataFrame to the specified file.\n.. code-block:: python\n\n\n```results_df.write.csv(\"output\")\n```\n\n\nUpon success, you should see multiple files in the output folder: an empty `_SUCCESS` file and\none or more `part-*.csv` files containing your output.\n.. code-block:: bash\n\n\n```$ ls output\n_SUCCESS  part-00000-85fe41df-4005-4991-a6ad-98b6ed549993-c000.csv\n$ head output/part-00000-d8fe59de-0233-4a80-8bda-519ce98223ea-c000.csv\n1.0\n0.0\n2.0\n0.0\n```\n\n\nSpark supports many formats other than CSV; see the `Spark documentation\n<https://spark.apache.org/docs/latest/api/python//reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.html#pyspark.sql.DataFrameWriter>`_",
    "tag": "bentoml"
  },
  {
    "title": "arize.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/integrations/arize.rst",
    "content": "========\nArize AI\n========\nArize AI provides a unified platform for data scientists, data engineers, and ML engineers to monitor, analyze, and debug ML models in production.\nOur collaboration with Arize AI makes it easy to integrate end-to-end solutions for data/model monitoring with BentoML Deployments.\n.. seealso::\n:ref:`The Arize section under the monitoring guide <guides/monitoring:Plugins and Third-party Monitoring Data Collectors>` demonstrates how to use the integration.\nPreview\n~~~~~~~\n\nDrift Detection & Monitoring\n\n.. image:: ../_static/img/arize_drift_monitoring.png\n   :width: 100%\n   :alt: Arize Drift Monitoring\n\nData Quality Monitoring\n\n.. image:: ../_static/img/arize_data_quality_monitoring.png\n   :width: 100%\n   :alt: Arize Data Quality Monitoring\n\nModel Explainability\n\n.. image:: ../_static/img/arize_model_explainability.png\n   :width: 100%\n   :alt: Arize Model Explainability\n\nAlerting & Notification\n\n.. image:: ../_static/img/arize_alerting.png\n   :width: 100%",
    "tag": "bentoml"
  },
  {
    "title": "server.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/guides/server.rst",
    "content": "============\nBento Server\n============\nBentoML Server runs the Service API in an `ASGI <https://asgi.readthedocs.io/en/latest/>`_\nweb serving layer and puts Runners in a separate worker process pool managed by BentoML. The ASGI web\nserving layer will expose REST endpoints for inference APIs, such as `POST /predict` and common\ninfrastructure APIs, such as `GET /metrics` for monitoring.\nBentoML offers a number of ways to customize the behaviors of the web serving layer to meet the needs of the consumers.\nCustom Endpoint URL\nBy default, the inference APIs are generated from the `@api` defined within a\n`bentoml.Service`. The URL route for the inference API is determined by the function\nname. Take the sample service from our tutorial for example, the function name `classify`\nwill be used as the REST API URL `/classify`:\n.. code-block:: python\n\n\n```svc = bentoml.Service(\"iris_classifier\", runners=[iris_clf_runner])\n\n@svc.api(input=NumpyNdarray(), output=NumpyNdarray())\ndef classify(input_arr):\n    ...\n```\n\n\nHowever, user can customize this URL endpoint via the `route` option in the\n`bentoml.Service#api` decorator. For example, the following code will assign the\nendpoint with URL `/v1/models/iris_classifier/predict`, regardless of the API function name:\n.. code-block:: python\n\n\n```import numpy as np\nimport bentoml\nfrom bentoml.io import NumpyNdarray\n\niris_clf_runner = bentoml.sklearn.get(\"iris_clf:latest\").to_runner()\n\nsvc = bentoml.Service(\"iris_classifier\", runners=[iris_clf_runner])\n\n@svc.api(\n    input=NumpyNdarray(),\n    output=NumpyNdarray(),\n    route=\"v1/models/iris_classifier/predict\"\n)\ndef any_func_name(input_series: np.ndarray) -> np.ndarray:\n    result = iris_clf_runner.predict.run(input_series)\n    return result\n```\n\n\nASGI Middleware\nSince the web serving layer is built with the Python ASGI protocol, users can use the\n:code:`bentoml.Service#add_asgi_middleware` API to mount arbitrary\n`ASGI middleware <https://asgi.readthedocs.io/en/latest/specs/main.html>`_ to change\nanything they may need to customize in the HTTP request to response lifecycle, such as\nmanipulating the request headers, modifying the response status code, or authorizing access to an endpoint.\nUsers can not only implement their own ASGI middleware class,\nbut also use existing middleware built by the Python web development community, such as:\n\nFastAPI middlewares: https://fastapi.tiangolo.com/advanced/middleware/\nStarlette middlewares: https://www.starlette.io/middleware/\n\nFor example, you can add do:\n.. code::\n\n\n```from starlette.middleware.httpsredirect import HTTPSRedirectMiddleware\nfrom starlette.middleware.trustedhost import TrustedHostMiddleware\n\nsvc = bentoml.Service('my_service', runners=[...])\n\nsvc.add_asgi_middleware(TrustedHostMiddleware, allowed_hosts=['example.com', '*.example.com'])\nsvc.add_asgi_middleware(HTTPSRedirectMiddleware)\n```\n\n\nFully Customized Endpoints\nBentoML provides first-class support for mounting existing WSGI or ASGI apps onto the\nweb serving layer, to enable common use cases such as serving existing Python web applications alongside\nthe models, performing custom authentication and authorization, handling GET requests and web UIs, or\nproviding streaming capabilities.\nBundle ASGI app (e.g. FastAPI)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nBentoML's web serving layer is ASGI native, existing ASGI apps can be mounted directly\nto and serving side-by-side with your BentoML Service.\nHere\u2019s an example of mounting BentoML Service with an ASGI app built with FastAPI:\n.. code-block:: python\n\n\n```import numpy as np\nimport pandas as pd\nimport bentoml\nfrom bentoml.io import NumpyNdarray, JSON\nfrom pydantic import BaseModel\nfrom fastapi import FastAPI\n\nclass IrisFeatures(BaseModel):\n    sepal_len: float\n    sepal_width: float\n    petal_len: float\n    petal_width: float\n\nbento_model = bentoml.sklearn.get(\"iris_clf_with_feature_names:latest\")\niris_clf_runner = bento_model.to_runner()\n\nsvc = bentoml.Service(\"iris_fastapi_demo\", runners=[iris_clf_runner])\n\n@svc.api(input=JSON(pydantic_model=IrisFeatures), output=NumpyNdarray())\ndef predict_bentoml(input_data: IrisFeatures) -> np.ndarray:\n    input_df = pd.DataFrame([input_data.dict()])\n    return iris_clf_runner.predict.run(input_df)\n\nfastapi_app = FastAPI()\nsvc.mount_asgi_app(fastapi_app)\n\n@fastapi_app.get(\"/metadata\")\ndef metadata():\n    return {\"name\": bento_model.tag.name, \"version\": bento_model.tag.version}\n\n# For demo purpose, here's an identical inference endpoint implemented via FastAPI\n@fastapi_app.post(\"/predict_fastapi\")\ndef predict(features: IrisFeatures):\n    input_df = pd.DataFrame([features.dict()])\n    results = iris_clf_runner.predict.run(input_df)\n    return { \"prediction\": results.tolist()[0] }\n\n# BentoML Runner's async API is recommended for async endpoints\n@fastapi_app.post(\"/predict_fastapi_async\")\nasync def predict_async(features: IrisFeatures):\n    input_df = pd.DataFrame([features.dict()])\n    results = await iris_clf_runner.predict.async_run(input_df)\n    return { \"prediction\": results.tolist()[0] }\n```\n\n\nIn addition to FastAPI, application mounting is supported for any ASGI web applications built with any frameworks adhering to the ASGI standards.\nBundle WSGI app (e.g. Flask)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFor WSGI web apps, such as a Flask app, BentoML provides a different API `mount_wsgi_app`\nwhich will internally convert the provided WSGI app into an ASGI app and serve side-by-side\nwith your BentoML Service.\nHere\u2019s an example of mounting BentoML Service with an WSGI app built with Flask:\n.. code-block:: python\n\n\n```import numpy as np\nimport bentoml\nfrom bentoml.io import NumpyNdarray\nfrom flask import Flask, request, jsonify\n\nbento_model = bentoml.sklearn.get(\"iris_clf:latest\")\niris_clf_runner = bento_model.to_runner()\n\nsvc = bentoml.Service(\"iris_flask_demo\", runners=[iris_clf_runner])\n\n\n@svc.api(input=NumpyNdarray(), output=NumpyNdarray())\ndef predict_bentoml(input_series: np.ndarray) -> np.ndarray:\n    return iris_clf_runner.predict.run(input_series)\n\nflask_app = Flask(__name__)\nsvc.mount_wsgi_app(flask_app)\n\n@flask_app.route(\"/metadata\")\ndef metadata():\n    return {\"name\": bento_model.tag.name, \"version\": bento_model.tag.version}\n\n# For demo purpose, here's an identical inference endpoint implemented via FastAPI\n@flask_app.route(\"/predict_flask\", methods=[\"POST\"])\ndef predict():\n    content_type = request.headers.get('Content-Type')\n    if (content_type == 'application/json'):\n        input_arr = np.array(request.json, dtype=float)\n        return jsonify(iris_clf_runner.predict.run(input_arr).tolist())\n    else:\n```\n\n",
    "tag": "bentoml"
  },
  {
    "title": "ci.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/guides/ci.rst",
    "content": "=====================================\nTraining Pipeline Integration (CI/CD)\n=====================================\n.. TODO::\n    Document how to build CI pipelines with BentoML.\nBefore a more detailed documentation is coming, check out the APIs for\n:doc:`/concepts/bento`, :ref:`concepts/bento:Managing Bentos` and\n:ref:`concepts/model:Managing Models`. They are essential to building a CI pipeline\nwith BentoML.\n.. admonition:: Help us improve the project!\n\n\n```Found an issue or a TODO item? You're always welcome to make contributions to the\nproject and its documentation. Check out the\n`BentoML development guide <https://github.com/bentoml/BentoML/blob/main/DEVELOPMENT.md>`_\nand `documentation guide <https://github.com/bentoml/BentoML/blob/main/docs/README.md>`_\nto get started.\n```\n\n",
    "tag": "bentoml"
  },
  {
    "title": "index.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/guides/index.rst",
    "content": "===============\nAdvanced Guides\n===============\nThis guide introduces advanced features in BentoML.\nMake sure to go through the :doc:`/tutorial` and :doc:`/concepts/index` before diving\ninto this part of the documentation.\n.. grid:: 1 2 2 2\n    :gutter: 3\n    :margin: 0\n    :padding: 3 4 0 0\n\n\n```.. grid-item-card:: :doc:`/guides/batching`\n    :link: /guides/batching\n    :link-type: doc\n\n.. grid-item-card:: :doc:`/guides/containerization`\n    :link: /guides/containerization\n    :link-type: doc\n\n.. grid-item-card:: :doc:`/guides/client`\n    :link: /guides/client\n    :link-type: doc\n\n.. grid-item-card:: :doc:`/guides/server`\n    :link: /guides/server\n    :link-type: doc\n\n.. grid-item-card:: :doc:`/guides/grpc`\n    :link: /guides/grpc\n    :link-type: doc\n\n.. grid-item-card:: :doc:`/guides/configuration`\n    :link: /guides/configuration\n    :link-type: doc\n\n.. grid-item-card:: :doc:`/guides/graph`\n    :link: /guides/graph\n    :link-type: doc\n\n.. grid-item-card:: :doc:`/guides/monitoring`\n    :link: /guides/monitoring\n    :link-type: doc\n\n.. grid-item-card:: :doc:`/guides/logging`\n    :link: /guides/logging\n    :link-type: doc\n\n.. grid-item-card:: :doc:`/guides/metrics`\n    :link: /guides/metrics\n    :link-type: doc\n\n.. grid-item-card:: :doc:`/guides/performance`\n    :link: /guides/performance\n    :link-type: doc\n\n.. grid-item-card:: :doc:`/guides/gpu`\n    :link: /guides/gpu\n    :link-type: doc\n\n.. grid-item-card:: :doc:`/guides/security`\n    :link: /guides/security\n    :link-type: doc\n\n.. grid-item-card:: :doc:`/guides/tracing`\n    :link: /guides/tracing\n    :link-type: doc\n\n.. grid-item-card:: :doc:`/guides/envmanager`\n    :link: /guides/envmanager\n    :link-type: doc\n\n.. grid-item-card:: :doc:`/guides/migration`\n    :link: /guides/migration\n    :link-type: doc\n```\n\n\n.. toctree::\n    :hidden:\n\n\n```batching\ncontainerization\nclient\nserver\nconfiguration\nenvmanager\ngraph\nmonitoring\nlogging\nmetrics\nperformance\ngrpc\ngpu\nsecurity\ntracing\nmigration\n```\n\n\n.. admonition:: Help us improve the project!\n\n\n```Found an issue or a TODO item? You're always welcome to make contributions to the\nproject and its documentation. Check out the\n`BentoML development guide <https://github.com/bentoml/BentoML/blob/main/DEVELOPMENT.md>`_\nand `documentation guide <https://github.com/bentoml/BentoML/blob/main/docs/README.md>`_\n```\n\n",
    "tag": "bentoml"
  },
  {
    "title": "gpu.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/guides/gpu.rst",
    "content": "================\nServing with GPU\n================\nMost popular deep learning frameworks (TensorFlow, PyTorch, ONNX, etc.) have supports\nfor GPU, both for training and inference. This guide demonstrates how to serve models\nwith BentoML on GPU.\nDocker Images Options\nSee :ref:`concepts/bento:Docker Options` for all options related to setting up docker\nimage options related to GPU. Here's a sample :code:`bentofile.yaml` config for serving\nwith GPU:\n.. code:: yaml\n\n\n```service: \"service:svc\"\ninclude:\n- \"*.py\"\npython:\n    packages:\n    - torch\n    - torchvision\n    - torchaudio\n    extra_index_url:\n    - \"https://download.pytorch.org/whl/cu113\"\ndocker:\n    distro: debian\n    python_version: \"3.8.12\"\n    cuda_version: \"11.6.2\"\n```\n\n\nWhen containerize a saved bento with a :code:`cuda_version` configured, BentoML will\ninstall the corresponding cuda version onto the docker image created:\n.. code-block:: bash\n\n\n```$ bentoml containerize MyTFService:latest -t tf_svc\n```\n\n\nIf the desired :code:`cuda_version` is not natively supported by BentoML, users can\nstill customize the installation of cuda driver and libraries via the\n:code:`system_packages`, :code:`setup_script`, or :code:`base_image` options under the\n:ref:`Bento build docker options<concepts/bento:Docker Options>`.\nRunning Docker with GPU\nThe NVIDIA Container Toolkit is required for running docker containers with Nvidia GPU.\nNVIDIA provides `detailed instructions <https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker>`_\nfor installing both :code:`Docker CE` and :code:`nvidia-docker`.\nStart bento generated image and check for GPU usages:\n.. code-block:: bash\n\n\n```$ docker run --gpus all ${DEVICE_ARGS} -p 3000:3000 tf_svc:latest --workers=2\n```\n\n\n.. seealso::\n    For more information, check out the `nvidia-docker wiki <https://github.com/NVIDIA/nvidia-docker/wiki>`_.\n.. note::\n    It is recommended to append device location to `--device` when running the\n    container:\n\n\n```.. code-block:: bash\n\n    $ docker run --gpus all --device /dev/nvidia0 \\\n                   --device /dev/nvidia-uvm --device /dev/nvidia-uvm-tools \\\n                   --device /dev/nvidia-modeset --device /dev/nvidiactl <docker-args>\n```\n\n\n.. tip::\n    In order to check for GPU usage, one can run `nvidia-smi` to check whether BentoService is using GPU. e.g\n\n\n```.. code:: bash\n\n    \u00bb nvidia-smi\n    Thu Jun 10 15:30:28 2021\n    +-----------------------------------------------------------------------------+\n    | NVIDIA-SMI 465.31       Driver Version: 465.31       CUDA Version: 11.3     |\n    |-------------------------------+----------------------+----------------------+\n    | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n    | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n    |                               |                      |               MIG M. |\n    |===============================+======================+======================|\n    |   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\n    | N/A   49C    P8     6W /  N/A |    753MiB /  6078MiB |      0%      Default |\n    |                               |                      |                  N/A |\n    +-------------------------------+----------------------+----------------------+\n\n    +-----------------------------------------------------------------------------+\n    | Processes:                                                                  |\n    |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n    |        ID   ID                                                   Usage      |\n    |=============================================================================|\n    |    0   N/A  N/A    179346      C   /opt/conda/bin/python             745MiB |\n```\n\n",
    "tag": "bentoml"
  },
  {
    "title": "envmanager.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/guides/envmanager.rst",
    "content": "===================\nEnvironment Manager\n===================\n:bdg-info:`Note:` This feature is currently only supported on UNIX/MacOS.\nEnvironment manager is a utility that helps create an isolated environment to\nrun the BentoML CLI. Dependencies are pulled from your defined\n`bentofile.yaml` and the environment is built upon request. This means by\npassing `--env` to supported CLI commands (such as :ref:`bentoml serve\n<reference/cli:serve>`), such commands will then be run in an sandbox\nenvironment that mimics the behaviour during production.\n.. code-block:: bash\n\u00bb bentoml serve --env conda iris_classifier:latest\nThis creates and isolated conda environment from the dependencies in the bento\nand runs `bentoml serve` from that environment.\n.. note:: The current implementation will try to install the given dependencies\n   before running the CLI command. Therefore, the environment startup will be a\n   blocking call.\nBentoML CLI Commands that support Environment Manager\n    - :ref:`serve <reference/cli:serve>`\n    - :ref:`serve-grpc <reference/cli:serve-grpc>`\nSupported Environments\n    - conda\nCaching strategies\nCurrently, there are two types of environments that are supported by the\nenvironment manager:\n\n\nPersistent environment: If the given target is a Bento, then the created\n   environment will be stored locally to `$BENTOML_HOME/env`. Such an\n   environment will then be cached and later used by subsequent invocations.\n\n\nEphemeral environment: In cases where the given target is not a Bento (import\n   path to `bentoml.Service`, project directory containing a valid\n   `bentofile.yaml`), the environment will be created and cleanup up on\n   demand.\n\n\n.. note::",
    "tag": "bentoml"
  },
  {
    "title": "monitoring.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/guides/monitoring.rst",
    "content": "============================================\nInference Data Collection & Model Monitoring\n============================================\nData-Centric Artificial Intelligence is an industrial leading paradigm that puts\ndata at the forefront of AI systems. It is a new way of thinking\nabout AI that is based on the idea that data is the most important\ncomponent of AI systems.\nBentoML embraces this new paradigm by providing APIs that make a data-centric workflow easy to implement.\n.. image:: ../_static/img/monitoring_workflow.png\n   :width: 600px\n   :align: center\nIn this guide, we will focus on the online data collection and model monitoring. BentoML provides a unified interface for that.\nThe benefits of having a data collection and model monitoring workflow includes:\n\nMonitor key statistical business metrics.\nIdentify early data drift events to determine whether retraining is required.\nEnable QA for the previous untracked metrics, such as model performance, accuracy, degradation, etc.\nBetter interoperability for training or model iteration.\n\nBuild an ML Application with monitoring API\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nThe following examples are excerpted from :github:`bentoml/BentoML/tree/main/examples/quickstart`.\nGiven the following service definition:\n.. code-block:: python\n    :caption: `service.py`\n\n\n```import numpy as np\nimport bentoml\nfrom bentoml.io import Text\nfrom bentoml.io import NumpyNdarray\n\nCLASS_NAMES = [\"setosa\", \"versicolor\", \"virginica\"]\n\niris_clf_runner = bentoml.sklearn.get(\"iris_clf:latest\").to_runner()\n\nsvc = bentoml.Service(\"iris_classifier\", runners=[iris_clf_runner])\n\n@svc.api(\n    input=NumpyNdarray.from_sample(np.array([4.9, 3.0, 1.4, 0.2], dtype=np.double)),\n    output=Text(),\n)\nasync def classify(features: np.ndarray) -> str:\n    results = await iris_clf_runner.predict.async_run([features])\n    result = results[0]\n    category = CLASS_NAMES[result]\n    return category\n```\n\n\nBefore we go to the production, we will only need one more step to add monitoring:\n.. code-block:: python\n    :caption: `service.py`\n    :emphasize-lines: 17-21,27\n\n\n```import numpy as np\n\nimport bentoml\nfrom bentoml.io import Text\nfrom bentoml.io import NumpyNdarray\n\nCLASS_NAMES = [\"setosa\", \"versicolor\", \"virginica\"]\n\niris_clf_runner = bentoml.sklearn.get(\"iris_clf:latest\").to_runner()\nsvc = bentoml.Service(\"iris_classifier\", runners=[iris_clf_runner])\n\n@svc.api(\n    input=NumpyNdarray.from_sample(np.array([4.9, 3.0, 1.4, 0.2], dtype=np.double)),\n    output=Text(),\n)\nasync def classify(features: np.ndarray) -> str:\n    with bentoml.monitor(\"iris_classifier_prediction\") as mon:\n        mon.log(features[0], name=\"sepal length\", role=\"feature\", data_type=\"numerical\")\n        mon.log(features[1], name=\"sepal width\", role=\"feature\", data_type=\"numerical\")\n        mon.log(features[2], name=\"petal length\", role=\"feature\", data_type=\"numerical\")\n        mon.log(features[3], name=\"petal width\", role=\"feature\", data_type=\"numerical\")\n\n        results = await iris_clf_runner.predict.async_run([features])\n        result = results[0]\n        category = CLASS_NAMES[result]\n\n        mon.log(category, name=\"pred\", role=\"prediction\", data_type=\"categorical\")\n    return category\n```\n\n\nThe Monitor object has a `log()` API that allows users to log request features and prediction information. Given data type can be one of the following: `[\"numerical\", \"categorical\", \"numerical_sequence\"]` with each role to be one of `[\"feature\", \"prediction\", \"target\"]`.\nWith a complete service definition, we can proceed to build the bento.\n.. code-block:: bash\n\n\n```$ bentoml build\n\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2557\u2591\u2591\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2591\u2588\u2588\u2588\u2588\u2588\u2557\u2591\u2588\u2588\u2588\u2557\u2591\u2591\u2591\u2588\u2588\u2588\u2557\u2588\u2588\u2557\u2591\u2591\u2591\u2591\u2591\n\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2588\u2588\u2557\u2591\u2588\u2588\u2551\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2557\u2591\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551\u2591\u2591\u2591\u2591\u2591\n\u2588\u2588\u2588\u2588\u2588\u2588\u2566\u255d\u2588\u2588\u2588\u2588\u2588\u2557\u2591\u2591\u2588\u2588\u2554\u2588\u2588\u2557\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2588\u2588\u2551\u2588\u2588\u2554\u2588\u2588\u2588\u2588\u2554\u2588\u2588\u2551\u2588\u2588\u2551\u2591\u2591\u2591\u2591\u2591\n\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u255d\u2591\u2591\u2588\u2588\u2551\u255a\u2588\u2588\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2551\u2591\u2591\u2591\u2591\u2591\n\u2588\u2588\u2588\u2588\u2588\u2588\u2566\u255d\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551\u2591\u255a\u2588\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2591\u255a\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2591\u255a\u2550\u255d\u2591\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\n\u255a\u2550\u2550\u2550\u2550\u2550\u255d\u2591\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d\u2591\u2591\u255a\u2550\u2550\u255d\u2591\u2591\u2591\u255a\u2550\u255d\u2591\u2591\u2591\u2591\u255a\u2550\u2550\u2550\u2550\u255d\u2591\u255a\u2550\u255d\u2591\u2591\u2591\u2591\u2591\u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\nSuccessfully built Bento(tag=\"iris_classifier:6aqnksdbuouf2usu\").\n```\n\n\nDeploy the service and collect monitoring data\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nWith BentoML, once we have the bento, it's easy to :ref:`deploy <concepts/deploy:Deploying Bento>` the ML application to any target.\nUse `serve --production` to start the bento in production mode as a standalone server:\n.. code-block:: bash\n\n\n```$ bentoml serve iris_classifier --production\n```\n\n\nThen we can send a request to the server to get the prediction. BentoML will log the request features and predictions to the configured place.\nBy default BentoML will export the data to the `monitoring/<your_monitor_name>` directory. To preview:\n.. code-block:: bash\n\n\n```$ tail -f monitoring/iris_classifier_prediction/data/*.log\n==> monitoring/iris_classifier_prediction/data/data.1.log <==\n{\"sepal length\": 6.3, \"sepal width\": 2.3, \"petal length\": 4.4, \"petal width\": 1.3, \"pred\": \"versicolor\", \"timestamp\": \"2022-11-09T15:31:26.781914\", \"request_id\": \"10655923893485958044\"}\n{\"sepal length\": 4.9, \"sepal width\": 3.6, \"petal length\": 1.4, \"petal width\": 0.1, \"pred\": \"setosa\", \"timestamp\": \"2022-11-09T15:31:26.786670\", \"request_id\": \"16263733333988780524\"}\n{\"sepal length\": 7.7, \"sepal width\": 3.0, \"petal length\": 6.1, \"petal width\": 2.3, \"pred\": \"virginica\", \"timestamp\": \"2022-11-09T15:31:26.788535\", \"request_id\": \"9077185615468445403\"}\n{\"sepal length\": 7.4, \"sepal width\": 2.8, \"petal length\": 6.1, \"petal width\": 1.9, \"pred\": \"virginica\", \"timestamp\": \"2022-11-09T15:31:26.795290\", \"request_id\": \"1949956912055125154\"}\n{\"sepal length\": 5.0, \"sepal width\": 2.3, \"petal length\": 3.3, \"petal width\": 1.0, \"pred\": \"versicolor\", \"timestamp\": \"2022-11-09T15:31:26.797957\", \"request_id\": \"5892192931675972870\"}\n{\"sepal length\": 5.1, \"sepal width\": 3.5, \"petal length\": 1.4, \"petal width\": 0.3, \"pred\": \"setosa\", \"timestamp\": \"2022-11-09T15:31:26.801006\", \"request_id\": \"11124174524929195678\"}\n{\"sepal length\": 5.4, \"sepal width\": 3.4, \"petal length\": 1.7, \"petal width\": 0.2, \"pred\": \"setosa\", \"timestamp\": \"2022-11-09T15:31:26.805018\", \"request_id\": \"1977947867380701804\"}\n{\"sepal length\": 5.4, \"sepal width\": 3.0, \"petal length\": 4.5, \"petal width\": 1.5, \"pred\": \"versicolor\", \"timestamp\": \"2022-11-09T15:31:26.809391\", \"request_id\": \"5170522495321543267\"}\n{\"sepal length\": 6.5, \"sepal width\": 3.2, \"petal length\": 5.1, \"petal width\": 2.0, \"pred\": \"virginica\", \"timestamp\": \"2022-11-09T15:31:26.813963\", \"request_id\": \"746111233619919779\"}\n{\"sepal length\": 5.4, \"sepal width\": 3.7, \"petal length\": 1.5, \"petal width\": 0.2, \"pred\": \"setosa\", \"timestamp\": \"2022-11-09T15:31:26.816515\", \"request_id\": \"10451493838968794158\"}\n\n==> monitoring/iris_classifier_prediction/data/data.2.log <==\n{\"sepal length\": 5.6, \"sepal width\": 2.5, \"petal length\": 3.9, \"petal width\": 1.1, \"pred\": \"versicolor\", \"timestamp\": \"2022-11-09T15:31:26.768545\", \"request_id\": \"12581333339958540887\"}\n{\"sepal length\": 4.5, \"sepal width\": 2.3, \"petal length\": 1.3, \"petal width\": 0.3, \"pred\": \"setosa\", \"timestamp\": \"2022-11-09T15:31:26.770188\", \"request_id\": \"14803218836235991321\"}\n{\"sepal length\": 6.3, \"sepal width\": 2.9, \"petal length\": 5.6, \"petal width\": 1.8, \"pred\": \"virginica\", \"timestamp\": \"2022-11-09T15:31:26.771554\", \"request_id\": \"3898998431725264845\"}\n{\"sepal length\": 4.7, \"sepal width\": 3.2, \"petal length\": 1.6, \"petal width\": 0.2, \"pred\": \"setosa\", \"timestamp\": \"2022-11-09T15:31:26.775306\", \"request_id\": \"16171654492399963820\"}\n{\"sepal length\": 4.9, \"sepal width\": 3.0, \"petal length\": 1.4, \"petal width\": 0.2, \"pred\": \"setosa\", \"timestamp\": \"2022-11-09T15:31:26.778971\", \"request_id\": \"12433921846139166785\"}\n{\"sepal length\": 6.9, \"sepal width\": 3.1, \"petal length\": 5.4, \"petal width\": 2.1, \"pred\": \"virginica\", \"timestamp\": \"2022-11-09T15:31:26.783441\", \"request_id\": \"3868728687839356795\"}\n{\"sepal length\": 5.1, \"sepal width\": 3.4, \"petal length\": 1.5, \"petal width\": 0.2, \"pred\": \"setosa\", \"timestamp\": \"2022-11-09T15:31:26.803871\", \"request_id\": \"4920762203256166127\"}\n{\"sepal length\": 4.5, \"sepal width\": 2.3, \"petal length\": 1.3, \"petal width\": 0.3, \"pred\": \"setosa\", \"timestamp\": \"2022-11-09T15:31:26.807770\", \"request_id\": \"562712759995883379\"}\n{\"sepal length\": 5.1, \"sepal width\": 3.8, \"petal length\": 1.6, \"petal width\": 0.2, \"pred\": \"setosa\", \"timestamp\": \"2022-11-09T15:31:26.810136\", \"request_id\": \"15755243536090754018\"}\n{\"sepal length\": 6.4, \"sepal width\": 3.1, \"petal length\": 5.5, \"petal width\": 1.8, \"pred\": \"virginica\", \"timestamp\": \"2022-11-09T15:31:26.812188\", \"request_id\": \"15915060852312696387\"}\n```\n\n\nShipping the collected data\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\nBentoML has a general monitoring data collecting API. This makes it possible to ship collected data to anywhere without code changes.\nFor example to a data warehouse, data analyze pipelines or to a monitoring & drift detection solution.\nTo achieve this, we just neet to provide a deployment configuration to bentoml.\nBuilt-in Monitoring Data Collectors\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nThrough log files\n~~~~~~~~~~~~~~~~~\nThe most common way to collect monitoring data is to write it to log files. Many utils like `fluentbit <https://fluentbit.io/>`, `filebeat <https://www.elastic.co/beats/filebeat>`, `logstash <https://www.elastic.co/logstash/>`_, etc. can be used to collect log files and ship them to a data warehouse or a monitoring system.\nThis is also the default way BentoML exports monitoring data:\n.. code-block:: yaml\n    :caption: \u2699\ufe0f `configuration.yml`\n\n\n```monitoring:\n  enabled: true\n  type: default\n  options:\n    log_path: path/to/log/file\n```\n\n\nFor Docker deployments, user can mount the log directory to a volume to persist the log files.\nFor K8s deployments, user can mount the log directory, and deploy a fluentbit daemonset or sidecar container to collect the log files to target destinations.\nThrough a OTLP endpoint\n~~~~~~~~~~~~~~~~~~~~~~~\n.. code-block:: yaml\n    :caption: \u2699\ufe0f `configuration.yml`\n\n\n```monitoring:\n  enable: true\n  type: otlp\n  options:\n    endpoint: http://localhost:5000\n    insecure: true\n    credentials: null\n    headers: null\n    timeout: 10\n    compression: null\n    meta_sample_rate: 1.0,\n```\n\n\nFor some deployment platforms, it's not easy to collect log files. For example, AWS Lambda doesn't support log files. In this case, BentoML can export monitoring data to an OTLP endpoint.\nSome log collectors like fluentbit also supports OTLP input.\nPlugins and Third-party Monitoring Data Collectors\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nBentoML also supports plugins and third-party monitoring data collectors. User can write a custom monitoring data collector and publish it as a python package.\nUnlike built-ins are more protocol specific for general use cases,\nplugins could be more platform specific.\n.. note::\n    To use plugins, you need to install the plugin and include it in the dependencies section of the bentofile.\n    For example, it is required to add `bentoml-plugins-arize` to the `python:packages` to use the Arize plugin.\n    See :ref:`the build command<concepts/bento:The Build Command>` for more details.\nArize AI\n~~~~~~~~\nFor end-to-end solutions for data/model monitoring, BentoML colaborates with `Arize AI <https://arize.com/docs/>`_ to provide a plugin for Arize.\nIf you don't want to deploy a pipeline by yourself but still need data and model monitoring for the bussiness, Arize AI is a good choice.\nArize AI provides a unified platform for data scientists, data engineers, and ML engineers to monitor, analyze, and debug ML models in production.\nAnd the `bentoml-plugins-arize` makes it easy to work with BentoML.\n.. code-block:: yaml\n    :caption: \u2699\ufe0f `configuration.yml`\n\n\n```monitoring:\n    enable: true\n    type: bentoml_plugins.arize.ArizeMonitor\n    options:\n        space_key: <your_space_key>\n        api_key: <your_api_key>\n        # ... more arize options\n        # see https://docs.arize.com/arize/data-ingestion/api-reference/python-sdk/arize.init#keyword-arguments\n        # and https://docs.arize.com/arize/sending-data-to-arize/data-ingestion-methods/sdk-reference/python-sdk/arize.log\n```\n\n",
    "tag": "bentoml"
  },
  {
    "title": "configuration.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/guides/configuration.rst",
    "content": "=============\nConfiguration\n=============\ntime expected: 11 minutes\nBentoML provides a configuration interface that allows you to customize the runtime\nbehaviour of your BentoService.  This article highlight and consolidates the configuration\nfields definition, as well as some of recommendation for best-practice when configuring\nyour BentoML.\nConfiguration is best used for scenarios where the customizations can be specified once\n   and applied anywhere among your organization using BentoML.\nBentoML comes with out-of-the-box configuration that should work for most use cases.\nHowever, for more advanced users who wants to fine-tune the feature suites BentoML has to offer,\nusers can configure such runtime variables and settings via a configuration file, often referred to as\n`bentoml_configuration.yaml`.\n.. note::\nThis is not to be mistaken with the `bentofile.yaml` which is used to define and\n   package your :ref:`Bento \ud83c\udf71 <concepts/bento:What is a Bento?>`\nThis configuration file are for BentoML runtime configuration.\nProviding configuration during serve runtime\nBentoML configuration is a :wiki:`YAML` file which can then be specified via the environment variable `BENTOML_CONFIG`.\nFor example, given the following `bentoml_configuration.yaml` that specify that the\nserver should only use 4 workers:\n.. code-block:: yaml\n   :caption: `~/bentoml_configuration.yaml`\nversion: 1\n   api_server:\n     workers: 4\nSaid configuration then can be parsed to :ref:`bentoml serve <reference/cli:serve>` like\nbelow:\n.. code-block:: bash\n\u00bb BENTOML_CONFIG=~/bentoml_configuration.yaml bentoml serve iris_classifier:latest --production\n.. note::\nUsers will only have to specify a partial configuration with properties they wish to customize. BentoML\n   will then fill in the rest of the configuration with the default values [#default_configuration]_.\nIn the example above, the number of API workers count is overridden to 4.\n   Remaining properties will take their defaults values.\n.. seealso::\n:ref:`guides/configuration:Configuration fields`\nOverrding configuration with environment variables\nUsers can also override configuration fields with environment variables. by defining\nan oneline value of a \"flat\" JSON via `BENTOML_CONFIG_OPTIONS`:\n.. code-block:: yaml\n$ BENTOML_CONFIG_OPTIONS='runners.pytorch_mnist.resources.\"nvidia.com/gpu\"[0]=0 runners.pytorch_mnist.resources.\"nvidia.com/gpu\"[1]=2' \\\n            bentoml serve pytorch_mnist_demo:latest --production\nWhich the override configuration will be intepreted as:\n.. code-block:: yaml\nrunners:\n    pytorch_mnist:\n      resources:\n        nvidia.com/gpu: [0, 2]\n.. note::\nFor fields that represents a iterable type, the override array must have a space\n   separating each element:\n.. image:: /_static/img/configuration-override-env.png\n      :alt: Configuration override environment variable\nMounting configuration to containerized Bento\nTo mount a configuration file to a containerized BentoService, user can use the\n|volume_mount| option to mount the configuration file to the container and\n|env_flag| option to set the `BENTOML_CONFIG` environment variable:\n.. code-block:: bash\n$ docker run --rm -v /path/to/configuration.yml:/home/bentoml/configuration.yml \\\n                -e BENTOML_CONFIG=/home/bentoml/configuration.yml \\\n                iris_classifier:6otbsmxzq6lwbgxi serve --production\nVoila! You have successfully mounted a configuration file to your containerized BentoService.\n.. _env_flag: https://docs.docker.com/engine/reference/commandline/run/#set-environment-variables--e---env---env-file\n.. |env_flag| replace:: `-e`\n.. _volume_mount: https://docs.docker.com/storage/volumes/#choose-the--v-or---mount-flag\n.. |volume_mount| replace:: `-v`\nConfiguration fields\nOn the top level, BentoML configuration [#default_configuration]_ has three fields:\n\n\n`version`: The version of the configuration file. This is used to determine the\n  compatibility of the configuration file with the current BentoML version.\n\n\n`api_server`: Configuration for BentoML API server.\n\n\n`runners` [#runners_configuration]_: Configuration for BentoService runners.\n\n\n`version`\n^^^^^^^^^^^\nBentoML configuration provides a `version` field, which enables users to easily specify\nand upgrade their configuration file as BentoML evolves.\nThis field will follow BentoML major version number. For every patch releases that\nintroduces new configuration fields, a compatibility layer will be provided to ensure\nthere is no breaking changes.\n.. epigraph::\nNote that `version` is not a required field, and BentoML will default to version 1 if\n   it is not specified.\nHowever, we encourage users to always version their BentoML configuration.\n`api_server`\n^^^^^^^^^^^^^^\nThe following options are available for the `api_server` section:\n+-------------+-------------------------------------------------------------+-------------------------------------------------+\n| Option      | Description                                                 | Default                                         |\n+=============+=============================================================+=================================================+\n| `workers` | Number of API workers for to spawn                          | null [#default_workers]_                        |\n+-------------+-------------------------------------------------------------+-------------------------------------------------+\n| `timeout` | Timeout for API server in seconds                           | 60                                              |\n+-------------+-------------------------------------------------------------+-------------------------------------------------+\n| `backlog` | Maximum number of connections to hold in backlog            | 2048                                            |\n+-------------+-------------------------------------------------------------+-------------------------------------------------+\n| `metrics` | Key and values to enable metrics feature                    | See :ref:`guides/configuration:\\``metrics```|\n+-------------+-------------------------------------------------------------+-------------------------------------------------+\n| ``logging`` | Key and values to enable logging feature                    | See :ref:`guides/logging:Logging Configuration`|\n+-------------+-------------------------------------------------------------+-------------------------------------------------+\n| ``http``    | Key and values to configure HTTP API server                 | See :ref:`guides/configuration:``http```|\n+-------------+-------------------------------------------------------------+-------------------------------------------------+\n| ``grpc``    | Key and values to configure gRPC API server                 | See :ref:`guides/configuration:``grpc```|\n+-------------+-------------------------------------------------------------+-------------------------------------------------+\n| ``ssl``     | Key and values to configure SSL                             | See :ref:`guides/configuration:``ssl```|\n+-------------+-------------------------------------------------------------+-------------------------------------------------+\n| ``tracing`` | Key and values to configure tracing exporter for API server | See :doc:`/guides/tracing`                      |\n+-------------+-------------------------------------------------------------+-------------------------------------------------+\n`metrics`\n\"\"\"\"\"\"\"\"\"\"\"\nBentoML utilises `Prometheus <https://prometheus.io/>`_ to collect metrics from the API server. By default, this feature is enabled.\nTo disable this feature, set `api_server.metrics.enabled` to `false`:\n.. code-block:: yaml\napi_server:\n     metrics:\n       enabled: false\nFollowing `labeling convention <https://prometheus.io/docs/practices/naming/#metric-and-label-naming>`_ set by Prometheus, metrics generated\nby BentoML API server components will have `namespace` `bentoml_api_server`, which can\nalso be overridden by setting `api_server.metrics.namespace`:\n.. code-block:: yaml\napi_server:\n     metrics:\n       namespace: custom_namespace\n.. epigraph::\n:bdg-info:`Note:` for most use cases, users should not need to change the default `namespace` value.\nThere are three types of metrics every BentoML API server will generate:\n\n`request_duration_seconds`: This is a `Histogram <https://prometheus.io/docs/concepts/metric_types/#histogram>`_ that measures the HTTP request duration in seconds.\n\nThere are two ways for users to customize `duration bucket size <https://prometheus.io/docs/practices/histograms/#count-and-sum-of-observations>`_ for this metrics:\n\n\nProvides a manual bucket steps via `api_server.metrics.duration.buckets`:\n.. code-block:: yaml\napi_server:\n     metrics:\n       duration:\n         buckets: [0.1, 0.2, 0.5, 1, 2, 5, 10]\n\n\nAutomatically generate an exponential buckets with any given `min`, `max` and `factor`:\n.. code-block:: yaml\napi_server:\n     metrics:\n       duration:\n         min: 0.1\n         max: 10\n         factor: 1.2\n\n\n.. note::\n\n\n``` - ``duration.min``, ``duration.max`` and ``duration.factor`` are mutually exclusive with ``duration.buckets``.\n\n - ``duration.factor`` must be greater than 1.\n```\n\n\nBy default, BentoML will respect the default `duration buckets <https://github.com/prometheus/client_python/blob/f17a8361ad3ed5bc47f193ac03b00911120a8d81/prometheus_client/metrics.py#L544>`_ provided by Prometheus.\n\n\n`request_total`: This is a `Counter <https://prometheus.io/docs/concepts/metric_types/#counter>`_ that measures the total number of HTTP requests.\n\n\n`request_in_progress`: This is a `Gauge <https://prometheus.io/docs/concepts/metric_types/#gauge>`_ that measures the number of HTTP requests in progress.\n\n\nThe following options are available for the `metrics` section:\n+----------------------+-------------------------------------+-------------------------------------------------------+\n| Option               | Description                         | Default                                               |\n+======================+=====================================+=======================================================+\n| `enabled`          | Enable metrics feature              | `true`                                              |\n+----------------------+-------------------------------------+-------------------------------------------------------+\n| `namespace`        | Namespace for metrics               | `bentoml_api_server`                                |\n+----------------------+-------------------------------------+-------------------------------------------------------+\n| `duration.buckets` | Duration buckets for Histogram      | Prometheus bucket value [#prometheus_default_bucket]_ |\n+----------------------+-------------------------------------+-------------------------------------------------------+\n| `duration.factor`  | factor for exponential buckets      | null                                                  |\n+----------------------+-------------------------------------+-------------------------------------------------------+\n| `duration.max`     | upper bound for exponential buckets | null                                                  |\n+----------------------+-------------------------------------+-------------------------------------------------------+\n| `duration.min`     | lower bound for exponential buckets | null                                                  |\n+----------------------+-------------------------------------+-------------------------------------------------------+\n`http`\n\"\"\"\"\"\"\"\"\nConfiguration under `api_server.http` will be used to configure the HTTP API server.\nBy default, BentoML will start an HTTP API server on port 3000. To change the port, set `api_server.http.port`:\n.. code-block:: yaml\napi_server:\n     http:\n       port: 5000\nUsers can also configure `CORS <https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS>`_ via `api_server.http.cors`. By default CORS is disabled.\nIf specified, all fields under `api_server.http.cors` will then be parsed to `CORSMiddleware <https://www.starlette.io/middleware/#corsmiddleware>`_:\n.. code-block:: yaml\napi_server:\n     http:\n       cors:\n         enabled: true\n         allow_origin: [\"myorg.com\"]\n         allow_methods: [\"GET\", \"OPTIONS\", \"POST\", \"HEAD\", \"PUT\"]\n         allow_credentials: true\n         allow_headers: \"\"\n         allow_origin_regex: 'https://..my_org.com'\n         max_age: 1200\n         expose_headers: [\"Content-Length\"]\n`grpc`\n\"\"\"\"\"\"\"\"\nThis section will go through configuration that is not yet coverred in :ref:`our guides on performance tuning <guides/grpc:Performance tuning>`.\nSimilar to HTTP API server, BentoML will start a gRPC API server on port 3000 by default. To change the port, set `api_server.grpc.port`:\n.. code-block:: yaml\napi_server:\n     grpc:\n       port: 5000\nNote that when using :ref:`bentoml serve-grpc <reference/cli:serve-grpc>` and metrics is\nenabled, a Prometheus metrics server will be started as a sidecar on port 3001. To change the port, set `api_server.grpc.metrics.port`:\n.. code-block:: yaml\napi_server:\n     grpc:\n       metrics:\n         port: 50051\nBy default, the gRPC API server will disable reflection. To always enable :github:`server reflection <grpc/grpc/blob/master/doc/server-reflection.md>`,\nset `api_server.grpc.reflection.enabled` to `true`:\n.. code-block:: yaml\napi_server:\n     grpc:\n       reflection:\n         enabled: true\n.. note::\nUser can already enable reflection by passing `--enable-reflection` to :ref:`bentoml serve-grpc <reference/cli:serve-grpc>` CLI command.\nHowever, we also provide this option in the config file to make it easier for users who wish to always enable reflection.\n`ssl`\n\"\"\"\"\"\"\"\nBentoML supports SSL/TLS for both HTTP and gRPC API server. To enable SSL/TLS, set `api_server.ssl.enabled` to `true`:\n.. code-block:: yaml\napi_server:\n     ssl:\n       enabled: true\nWhen using HTTP API server, BentoML will parse all of the available fields directly to `Uvicorn <https://www.uvicorn.org/settings/#https>`_.\n.. TODO::\n\nAdd instruction how one can setup SSL for gRPC API server.\n\n\n.. rubric:: Notes\n.. [#default_workers] The default number of workers is the number of CPUs count.\n.. [#default_configuration] The default configuration can also be found under :github:`configuration folder <bentoml/BentoML/tree/main/bentoml/_internal/configuration>`.\n.. dropdown:: `Expands for default configuration`\n      :icon: code\n\n\n```  .. literalinclude:: ../../../src/bentoml/_internal/configuration/v1/default_configuration.yaml\n     :language: yaml\n```\n\n\n.. [#prometheus_default_bucket] The default buckets is specified `here <https://github.com/prometheus/client_python/blob/f17a8361ad3ed5bc47f193ac03b00911120a8d81/prometheus_client/metrics.py#L544>`_ for Python client.",
    "tag": "bentoml"
  },
  {
    "title": "grpc.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/guides/grpc.rst",
    "content": "=================\nServing with gRPC\n=================\ntime expected: 12 minutes\nThis guide will demonstrate advanced features that BentoML offers for you to get started\nwith `gRPC <https://grpc.io/>`_:\n\nFirst-class support for :ref:`custom gRPC Servicer <guides/grpc:Mounting Servicer>`, :ref:`custom interceptors <guides/grpc:Mounting gRPC Interceptors>`, handlers.\nSeemlessly adding gRPC support to existing Bento.\n\nThis guide will also walk you through tradeoffs of serving with gRPC, as well as\nrecommendation on scenarios where gRPC might be a good fit.\n:bdg-info:`Requirements:` This guide assumes that you have basic knowledge of gRPC and protobuf. If you aren't\nfamilar with gRPC, you can start with gRPC `quick start guide <https://grpc.io/docs/languages/python/quickstart/>`_.\n.. seealso::\nFor quick introduction to serving with gRPC, see :ref:`Intro to BentoML <tutorial:Tutorial: Intro to BentoML>`\nGet started with gRPC in BentoML\nWe will be using the example from :ref:`the quickstart<tutorial:Tutorial: Intro to BentoML>` to\ndemonstrate BentoML capabilities with gRPC.\nRequirements\n~~~~~~~~~~~~\nBentoML supports for gRPC are introduced in version 1.0.6 and above.\nInstall BentoML with gRPC support with :pypi:`pip`:\n.. code-block:: bash\n\u00bb pip install -U \"bentoml[grpc]\"\nThats it! You can now serve your Bento with gRPC via :ref:`bentoml serve-grpc <reference/cli:serve-grpc>` without having to modify your current service definition \ud83d\ude03.\n.. code-block:: bash\n\u00bb bentoml serve-grpc iris_classifier:latest --production\nUsing your gRPC BentoService\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nThere are two ways to interact with your gRPC BentoService:\n\nUse tools such as :github:`fullstorydev/grpcurl`, :github:`fullstorydev/grpcui`: \n   The server requires :github:`reflection <grpc/grpc/blob/master/doc/server-reflection.md>` to be enabled for those tools to work.\n   Pass in `--enable-reflection` to enable reflection:\n\n.. code-block:: bash\n\n\n```  \u00bb bentoml serve-grpc iris_classifier:latest --production --enable-reflection\n```\n\n\n.. include:: ./snippets/grpc/grpc_tools.rst\nOpen a different terminal and use one of the following:\n\nUse one of the below :ref:`client implementations <guides/grpc:Client Implementation>` to send test requests to your BentoService.\n\n.. _workspace: https://bazel.build/concepts/build-ref\n.. |workspace| replace:: `WORKSPACE`\n.. _build: https://bazel.build/concepts/build-files\n.. |build| replace:: `BUILD`\n.. _bazel: https://bazel.build\n.. |bazel| replace:: `bazel`\nClient Implementation\n~~~~~~~~~~~~~~~~~~~~~\n.. note::\nAll of the following client implementations are :github:`available on GitHub <bentoml/BentoML/tree/main/grpc-client/>`.\n:raw-html:`<br />`\nFrom another terminal, use one of the following client implementation to send request to the\ngRPC server:\n.. note::\ngRPC comes with supports for multiple languages. In the upcoming sections\n   we will demonstrate two workflows of generating stubs and implementing clients:\n\nUsing |bazel|_ to manage and isolate dependencies (recommended)\nA manual approach using `protoc` its language-specific plugins\n\n.. tab-set::\n.. tab-item:: Python\n      :sync: python\n\n\n```  We will create our Python client in the directory ``~/workspace/iris_python_client/``:\n\n  .. code-block:: bash\n\n     \u00bb mkdir -p ~/workspace/iris_python_client\n     \u00bb cd ~/workspace/iris_python_client\n\n  Create a ``client.py`` file with the following content:\n\n  .. literalinclude:: ../../../grpc-client/python/client.py\n     :language: python\n     :caption: `client.py`\n```\n\n\n.. tab-item:: Go\n      :sync: golang\n\n\n```  :bdg-info:`Requirements:` Make sure to install the `prerequisites <https://grpc.io/docs/languages/go/quickstart/#prerequisites>`_ before using Go.\n\n  We will create our Golang client in the directory ``~/workspace/iris_go_client/``:\n\n  .. code-block:: bash\n\n     \u00bb mkdir -p ~/workspace/iris_go_client\n     \u00bb cd ~/workspace/iris_go_client\n\n  .. tab-set::\n\n     .. tab-item:: Using bazel (recommended)\n        :sync: bazel-workflow\n\n        Define a |workspace|_ file:\n\n        .. dropdown:: ``WORKSPACE``\n           :icon: code\n\n           .. literalinclude:: ./snippets/grpc/go/WORKSPACE.snippet.bzl\n              :language: python\n\n        Followed by defining a |build|_ file:\n\n        .. dropdown:: ``BUILD``\n           :icon: code\n\n           .. literalinclude:: ./snippets/grpc/go/BUILD.snippet.bzl\n              :language: python\n\n     .. tab-item:: Using protoc and language-specific plugins\n        :sync: protoc-and-plugins\n\n        Create a Go module:\n\n        .. code-block:: bash\n\n           \u00bb go mod init iris_go_client && go mod tidy\n\n        Add the following lines to ``~/workspace/iris_go_client/go.mod``:\n\n        .. code-block:: go\n\n           require github.com/bentoml/bentoml/grpc/v1 v0.0.0-unpublished\n\n           replace github.com/bentoml/bentoml/grpc/v1 v0.0.0-unpublished => ./github.com/bentoml/bentoml/grpc/v1\n\n        By using `replace directive <https://go.dev/ref/mod#go-mod-file-replace>`_, we\n        ensure that Go will know where our generated stubs to be imported from. (since we don't host the generate gRPC stubs on `pkg.go.dev` \ud83d\ude04)\n\n        .. include:: ./snippets/grpc/additional_setup.rst\n\n        Here is the ``protoc`` command to generate the gRPC Go stubs:\n\n        .. code-block:: bash\n\n           \u00bb protoc -I. -I thirdparty/protobuf/src  \\\n                    --go_out=. --go_opt=paths=import \\\n                    --go-grpc_out=. --go-grpc_opt=paths=import \\\n                    bentoml/grpc/v1/service.proto\n\n        Then run the following to make sure the generated stubs are importable:\n\n        .. code-block:: bash\n\n           \u00bb pushd github.com/bentoml/bentoml/grpc/v1\n           \u00bb go mod init v1 && go mod tidy\n           \u00bb popd\n\n  Create a ``client.go`` file with the following content:\n\n  .. literalinclude:: ../../../grpc-client/go/client.go\n     :language: go\n     :caption: `client.go`\n```\n\n\n.. tab-item:: C++\n      :sync: cpp\n\n\n```  :bdg-info:`Requirements:` Make sure follow the `instructions <https://grpc.io/docs/languages/cpp/quickstart/#install-grpc>`_ to install gRPC and Protobuf locally.\n\n  We will create our C++ client in the directory ``~/workspace/iris_cc_client/``:\n\n  .. code-block:: bash\n\n     \u00bb mkdir -p ~/workspace/iris_cc_client\n     \u00bb cd ~/workspace/iris_cc_client\n\n  .. tab-set::\n\n     .. tab-item:: Using bazel (recommended)\n        :sync: bazel-workflow\n\n        Define a |workspace|_ file:\n\n        .. dropdown:: ``WORKSPACE``\n           :icon: code\n\n           .. literalinclude:: ./snippets/grpc/cpp/WORKSPACE.snippet.bzl\n              :language: python\n\n        Followed by defining a |build|_ file:\n\n        .. dropdown:: ``BUILD``\n           :icon: code\n\n           .. literalinclude:: ./snippets/grpc/cpp/BUILD.snippet.bzl\n              :language: python\n\n     .. tab-item:: Using protoc and language-specific plugins\n        :sync: protoc-and-plugins\n\n        .. include:: ./snippets/grpc/additional_setup.rst\n\n        Here is the ``protoc`` command to generate the gRPC C++ stubs:\n\n        .. code-block:: bash\n\n           \u00bb protoc -I . -I ./thirdparty/protobuf/src \\\n                    --cpp_out=. --grpc_out=. \\\n                    --plugin=protoc-gen-grpc=$(which grpc_cpp_plugin) \\\n                    bentoml/grpc/v1/service.proto\n\n  Create a ``client.cpp`` file with the following content:\n\n  .. literalinclude:: ../../../grpc-client/cpp/client.cc\n     :language: cpp\n     :caption: `client.cpp`\n```\n\n\n.. tab-item:: Java\n      :sync: java\n\n\n```  :bdg-info:`Requirements:` Make sure to have `JDK>=7 <https://jdk.java.net/>`_.\n\n  :bdg-info:`Optional:`  follow the :github:`instructions <grpc/grpc-java/tree/master/compiler>` to install ``protoc`` plugin for gRPC Java if you plan to use ``protoc`` standalone.\n\n  .. note::\n\n     Feel free to use any Java build tools of choice (Maven, Gradle, Bazel, etc.) to build and run the client you find fit.\n\n     In this tutorial we will be using |bazel|_.\n\n  We will create our Java client in the directory ``~/workspace/iris_java_client/``:\n\n  .. code-block:: bash\n\n     \u00bb mkdir -p ~/workspace/iris_java_client\n     \u00bb cd ~/workspace/iris_java_client\n\n  Create the client Java package (``com.client.BentoServiceClient``):\n\n  .. code-block:: bash\n\n     \u00bb mkdir -p src/main/java/com/client\n\n  .. tab-set::\n\n     .. tab-item:: Using bazel (recommended)\n        :sync: bazel-workflow\n\n        Define a |workspace|_ file:\n\n        .. dropdown:: ``WORKSPACE``\n           :icon: code\n\n           .. literalinclude:: ./snippets/grpc/java/WORKSPACE.snippet.bzl\n              :language: python\n\n        Followed by defining a |build|_ file:\n\n        .. dropdown:: ``BUILD``\n           :icon: code\n\n           .. literalinclude:: ./snippets/grpc/java/BUILD.snippet.bzl\n              :language: python\n\n     .. tab-item:: Using others build system\n        :sync: protoc-and-plugins\n\n        One simply can't manually running ``javac`` to compile the Java class, since\n        there are way too many dependencies to be resolved.\n\n        Provided below is an example of how one can use `gradle <https://gradle.org/>`_ to build the Java client.\n\n        .. code-block:: bash\n\n           \u00bb gradle init --project-dir .\n\n        The following ``build.gradle`` should be able to help you get started:\n\n        .. literalinclude:: ../../../grpc-client/java/build.gradle\n           :language: text\n           :caption: build.gradle\n\n        To build the client, run:\n\n        .. code-block:: bash\n\n           \u00bb ./gradlew build\n\n  Proceed to create a ``src/main/java/com/client/BentoServiceClient.java`` file with the following content:\n\n  .. literalinclude:: ../../../grpc-client/java/src/main/java/com/client/BentoServiceClient.java\n     :language: java\n     :caption: `BentoServiceClient.java`\n\n  .. dropdown:: On running ``protoc`` standalone (optional)\n     :icon: book\n\n     .. include:: ./snippets/grpc/additional_setup.rst\n\n     Here is the ``protoc`` command to generate the gRPC Java stubs if you need to use ``protoc`` standalone:\n\n     .. code-block:: bash\n\n        \u00bb protoc -I . \\\n                 -I ./thirdparty/protobuf/src \\\n                 --java_out=./src/main/java \\\n                 --grpc-java_out=./src/main/java \\\n                 bentoml/grpc/v1/service.proto\n```\n\n\n.. tab-item:: Kotlin\n      :sync: kotlin\n\n\n```  :bdg-info:`Requirements:` Make sure to have the `prequisites <https://grpc.io/docs/languages/kotlin/quickstart/#prerequisites>`_ to get started with :github:`grpc/grpc-kotlin`.\n\n  :bdg-info:`Optional:` feel free to install :github:`Kotlin gRPC codegen <grpc/grpc-kotlin/blob/master/compiler/README.md>` in order to generate gRPC stubs if you plan to use ``protoc`` standalone.\n\n  To bootstrap the Kotlin client, feel free to use either `gradle <https://gradle.org/>`_ or\n  `maven <https://maven.apache.org/>`_ to build and run the following client code.\n\n  In this example, we will use |bazel|_ to build and run the client.\n\n  We will create our Kotlin client in the directory ``~/workspace/iris_kotlin_client/``, followed by creating the client directory structure:\n\n  .. code-block:: bash\n\n     \u00bb mkdir -p ~/workspace/iris_kotlin_client\n     \u00bb cd ~/workspace/iris_kotlin_client\n     \u00bb mkdir -p src/main/kotlin/com/client\n\n  .. tab-set::\n\n     .. tab-item:: Using bazel (recommended)\n        :sync: bazel-workflow\n\n        Define a |workspace|_ file:\n\n        .. dropdown:: ``WORKSPACE``\n\n           .. literalinclude:: ./snippets/grpc/kotlin/WORKSPACE.snippet.bzl\n              :language: python\n\n        Followed by defining a |build|_ file:\n\n        .. dropdown:: ``BUILD``\n\n           .. literalinclude:: ./snippets/grpc/kotlin/BUILD.snippet.bzl\n              :language: python\n\n     .. tab-item:: Using others build system\n        :sync: protoc-and-plugins\n\n        One simply can't manually compile all the Kotlin files, since there are way too many dependencies to be resolved.\n\n        Provided below is an example of how one can use `gradle <https://gradle.org/>`_ to build the Kotlin client.\n\n        .. code-block:: bash\n\n           \u00bb gradle init --project-dir .\n\n        The following ``build.gradle.kts`` should be able to help you get started:\n\n        .. literalinclude:: ../../../grpc-client/kotlin/build.gradle.kts\n           :language: text\n           :caption: build.gradle.kts\n\n        To build the client, run:\n\n        .. code-block:: bash\n\n           \u00bb ./gradlew build\n\n  Proceed to create a ``src/main/kotlin/com/client/BentoServiceClient.kt`` file with the following content:\n\n  .. literalinclude:: ../../../grpc-client/kotlin/src/main/kotlin/com/client/BentoServiceClient.kt\n     :language: java\n     :caption: `BentoServiceClient.kt`\n\n  .. dropdown:: On running ``protoc`` standalone (optional)\n     :icon: book\n\n     .. include:: ./snippets/grpc/additional_setup.rst\n\n     Here is the ``protoc`` command to generate the gRPC Kotlin stubs if you need to use ``protoc`` standalone:\n\n     .. code-block:: bash\n\n        \u00bb protoc -I. -I ./thirdparty/protobuf/src \\\n                 --kotlin_out ./kotlin/src/main/kotlin/ \\\n                 --grpc-kotlin_out ./kotlin/src/main/kotlin \\\n                 --plugin=protoc-gen-grpc-kotlin=$(which protoc-gen-grpc-kotlin) \\\n                 bentoml/grpc/v1/service.proto\n```\n\n\n.. tab-item:: Node.js\n      :sync: nodejs\n\n\n```  :bdg-info:`Requirements:` Make sure to have `Node.js <https://nodejs.org/en/>`_\n  installed in your system.\n\n  We will create our Node.js client in the directory ``~/workspace/iris_node_client/``:\n\n  .. code-block:: bash\n\n     \u00bb mkdir -p ~/workspace/iris_node_client\n     \u00bb cd ~/workspace/iris_node_client\n\n  .. dropdown:: Initialize the project and use the following ``package.json``:\n\n     .. literalinclude:: ../../../grpc-client/node/package.json\n        :language: json\n        :caption: `package.json`\n\n  Install the dependencies with either ``npm`` or ``yarn``:\n\n  .. code-block:: bash\n\n     \u00bb yarn install --add-devs\n\n  .. note::\n\n     If you are using M1, you might also have to prepend ``npm_config_target_arch=x64`` to ``yarn`` command:\n\n     .. code-block:: bash\n\n        \u00bb npm_config_target_arch=x64 yarn install --add-devs\n\n  .. include:: ./snippets/grpc/additional_setup.rst\n\n  Here is the ``protoc`` command to generate the gRPC Javascript stubs:\n\n  .. code-block:: bash\n\n     \u00bb $(npm bin)/grpc_tools_node_protoc \\\n              -I . -I ./thirdparty/protobuf/src \\\n              --js_out=import_style=commonjs,binary:. \\\n              --grpc_out=grpc_js:js \\\n              bentoml/grpc/v1/service.proto\n\n  Proceed to create a ``client.js`` file with the following content:\n\n  .. literalinclude:: ../../../grpc-client/node/client.js\n     :language: javascript\n     :caption: `client.js`\n```\n\n\n.. tab-item:: Swift\n      :sync: swift\n\n\n```  :bdg-info:`Requirements:` Make sure to have the :github:`prequisites <grpc/grpc-swift/blob/main/docs/quick-start.md#prerequisites>` to get started with :github:`grpc/grpc-swift`.\n\n  We will create our Swift client in the directory ``~/workspace/iris_swift_client/``:\n\n  .. code-block:: bash\n\n     \u00bb mkdir -p ~/workspace/iris_swift_client\n     \u00bb cd ~/workspace/iris_swift_client\n\n  We will use `Swift Package Manager <https://swift.org/package-manager/>`_ to build and run the client.\n\n  .. code-block:: bash\n\n     \u00bb swift package init --type executable\n\n  .. dropdown:: Initialize the project and use the following ``Package.swift``:\n\n     .. literalinclude:: ../../../grpc-client/swift/Package.swift\n        :language: swift\n        :caption: `Package.swift`\n\n  .. include:: ./snippets/grpc/additional_setup.rst\n\n  Here is the ``protoc`` command to generate the gRPC Swift stubs:\n\n  .. code-block:: bash\n\n     \u00bb protoc -I. -I ./thirdparty/protobuf/src \\\n              --swift_out=Sources --swift_opt=Visibility=Public \\\n              --grpc-swift_out=Sources --grpc-swift_opt=Visibility=Public \\\n              --plugin=protoc-gen-grpc-swift=$(which protoc-gen-grpc-swift) \\\n              bentoml/grpc/v1/service.proto\n\n  Proceed to create a ``Sources/BentoServiceClient/main.swift`` file with the following content:\n\n  .. literalinclude:: ../../../grpc-client/swift/Sources/BentoServiceClient/main.swift\n     :language: swift\n     :caption: `main.swift`\n```\n\n\n.. tab-item:: PHP\n      :sync: php\n\n\n```  :bdg-info:`Requirements:` Make sure to follow the :github:`instructions <grpc/grpc/blob/master/src/php/README.md>` to install ``grpc`` via either `pecl <https://pecl.php.net/>`_ or from source.\n\n  .. note::\n\n     You will also have to symlink the built C++ extension to the PHP extension directory for it to be loaded by PHP.\n\n  We will then use |bazel|_, `composer <https://getcomposer.org/>`_ to build and run the client.\n\n  We will create our PHP client in the directory ``~/workspace/iris_php_client/``:\n\n  .. code-block:: bash\n\n     \u00bb mkdir -p ~/workspace/iris_php_client\n     \u00bb cd ~/workspace/iris_php_client\n\n  Create a new PHP package:\n\n  .. code-block:: bash\n\n     \u00bb composer init\n\n  .. dropdown:: An example ``composer.json`` for the client:\n     :icon: code\n\n     .. literalinclude:: ../../../grpc-client/php/composer.json\n        :language: json\n\n  .. include:: ./snippets/grpc/additional_setup.rst\n\n  Here is the ``protoc`` command to generate the gRPC swift stubs:\n\n  .. code-block:: bash\n\n     \u00bb protoc -I . -I ./thirdparty/protobuf/src \\\n              --php_out=. \\\n              --grpc_out=. \\\n              --plugin=protoc-gen-grpc=$(which grpc_php_plugin) \\\n              bentoml/grpc/v1/service.proto\n\n  Proceed to create a ``BentoServiceClient.php`` file with the following content:\n\n  .. literalinclude:: ../../../grpc-client/php/BentoServiceClient.php\n     :language: php\n     :caption: `BentoServiceClient.php`\n```\n\n\n.. TODO::\nBazel instruction for `swift`, `nodejs`, `python`\n:raw-html:`<br />`\nThen you can proceed to run the client scripts:\n.. tab-set::\n.. tab-item:: Python\n      :sync: python\n\n\n```  .. code-block:: bash\n\n     \u00bb python -m client\n```\n\n\n.. tab-item:: Go\n      :sync: golang\n\n\n```  .. tab-set::\n\n     .. tab-item:: Using bazel (recommended)\n        :sync: bazel-workflow\n\n        .. code-block:: bash\n\n           \u00bb bazel run //:client_go\n\n     .. tab-item:: Using protoc and language-specific plugins\n        :sync: protoc-and-plugins\n\n        .. code-block:: bash\n\n           \u00bb go run ./client.go\n```\n\n\n.. tab-item:: C++\n      :sync: cpp\n\n\n```  .. tab-set::\n\n     .. tab-item:: Using bazel (recommended)\n        :sync: bazel-workflow\n\n        .. code-block:: bash\n\n           \u00bb bazel run :client_cc\n\n     .. tab-item:: Using protoc and language-specific plugins\n        :sync: protoc-and-plugins\n\n        Refer to :github:`grpc/grpc` for instructions on using CMake and other similar build tools.\n\n  .. note::\n\n     See the :github:`instructions on GitHub <bentoml/BentoML/tree/main/grpc-client/README.md>` for working C++ client.\n```\n\n\n.. tab-item:: Java\n      :sync: java\n\n\n```  .. tab-set::\n\n     .. tab-item:: Using bazel (recommended)\n        :sync: bazel-workflow\n\n        .. code-block:: bash\n\n           \u00bb bazel run :client_java\n\n     .. tab-item:: Using others build system\n        :sync: protoc-and-plugins\n\n        We will use ``gradlew`` to build the client and run it:\n\n        .. code-block:: bash\n\n           \u00bb ./gradlew build && \\\n              ./build/tmp/scripts/bentoServiceClient/bento-service-client\n\n  .. note::\n\n     See the :github:`instructions on GitHub <bentoml/BentoML/tree/main/grpc-client/README.md>` for working Java client.\n```\n\n\n.. tab-item:: Kotlin\n      :sync: kotlin\n\n\n```  .. tab-set::\n\n     .. tab-item:: Using bazel (recommended)\n        :sync: bazel-workflow\n\n        .. code-block:: bash\n\n           \u00bb bazel run :client_kt\n\n     .. tab-item:: Using others build system\n        :sync: protoc-and-plugins\n\n        We will use ``gradlew`` to build the client and run it:\n\n        .. code-block:: bash\n\n           \u00bb ./gradlew build && \\\n              ./build/tmp/scripts/bentoServiceClient/bento-service-client\n\n  .. note::\n\n     See the :github:`instructions on GitHub <bentoml/BentoML/tree/main/grpc-client/README.md>` for working Kotlin client.\n```\n\n\n.. tab-item:: Node.js\n      :sync: nodejs\n\n\n```  .. code-block:: bash\n\n     \u00bb node client.js\n```\n\n\n.. tab-item:: Swift\n      :sync: swift\n\n\n```  .. code-block:: bash\n\n     \u00bb swift run BentoServiceClient\n```\n\n\n.. tab-item:: PHP\n      :sync: php\n\n\n```  .. code-block:: bash\n\n     \u00bb php -d extension=/path/to/grpc.so -d max_execution_time=300 BentoServiceClient.php\n```\n\n\n.. dropdown:: Additional language support for client implementation\n   :icon: triangle-down\n.. tab-set::\n\n\n```  .. tab-item:: Ruby\n     :sync: ruby\n\n     :bdg-primary:`Note:` Please check out the :github:`gRPC Ruby <grpc/grpc/blob/master/src/ruby/README.md#grpc-ruby>` for how to install from source.\n     Check out the :github:`examples folder <grpc/grpc/blob/master/examples/ruby/README.md#prerequisites>` for Ruby client implementation.\n\n  .. tab-item:: .NET\n     :sync: dotnet\n\n     :bdg-primary:`Note:` Please check out the :github:`gRPC .NET <grpc/grpc-dotnet/tree/master/examples>` examples folder for :github:`grpc/grpc-dotnet` client implementation.\n\n  .. tab-item:: Dart\n     :sync: dart\n\n     :bdg-primary:`Note:` Please check out the :github:`gRPC Dart <grpc/grpc-dart/tree/master/examples>` examples folder for :github:`grpc/grpc-dart` client implementation.\n\n  .. tab-item:: Rust\n     :sync: rust\n\n     :bdg-primary:`Note:` Currently there are no official gRPC Rust client implementation. Please check out the :github:`tikv/grpc-rs` as one of the unofficial implementation.\n```\n\n\nAfter successfully running the client, proceed to build the bento as usual:\n.. code-block:: bash\n\u00bb bentoml build\n:raw-html:`<br />`\nContainerize your Bento \ud83c\udf71 with gRPC support\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nTo containerize the Bento with gRPC features, pass in `--enable-features=grpc` to\n:ref:`bentoml containerize <reference/cli:containerize>` to add additional gRPC\ndependencies to your Bento\n.. code-block:: bash\n\u00bb bentoml containerize iris_classifier:latest --enable-features=grpc\n`--enable-features` allows users to containerize any of the existing Bentos with :ref:`additional features <concepts/bento:Enable features for your Bento>` that BentoML provides without having to rebuild the Bento.\n.. note::\n`--enable-features` accepts a comma-separated list of features or multiple arguments.\nAfter containerization, your Bento container can now be used with gRPC:\n.. code-block:: bash\n\u00bb docker run -it --rm \\\n                -p 3000:3000 -p 3001:3001 \\\n                iris_classifier:6otbsmxzq6lwbgxi serve-grpc --production\nCongratulations! You have successfully served, containerized and tested your BentoService with gRPC.\n\nUsing gRPC in BentoML\nWe will dive into some of the details of how gRPC is implemented in BentoML.\nProtobuf definition\n~~~~~~~~~~~~~~~~~~~\nLet's take a quick look at `protobuf <https://developers.google.com/protocol-buffers/>`_  definition of the BentoService:\n.. code-block:: protobuf\nservice BentoService {\n     rpc Call(Request) returns (Response) {}\n   }\n.. dropdown:: `Expands for current protobuf definition.`\n   :icon: code\n.. tab-set::\n\n\n```  .. tab-item:: v1\n\n     .. literalinclude:: ../../../src/bentoml/grpc/v1/service.proto\n        :language: protobuf\n\n  .. tab-item:: v1alpha1\n\n     .. literalinclude:: ../../../src/bentoml/grpc/v1alpha1/service.proto\n        :language: protobuf\n```\n\n\nAs you can see, BentoService defines a `simple rpc` `Call` that sends a `Request` message and returns a `Response` message.\nA `Request` message takes in:\n\n`api_name`: the name of the API function defined inside your BentoService. \n`oneof <https://developers.google.com/protocol-buffers/docs/proto3#oneof>`_ `content`: the field can be one of the following types:\n\n+------------------------------------------------------------------+-------------------------------------------------------------------------------------------+\n| Protobuf definition                                              | IO Descriptor                                                                             |\n+------------------------------------------------------------------+-------------------------------------------------------------------------------------------+\n| :ref:`guides/grpc:Array representation via ``NDArray```          | :ref:`bentoml.io.NumpyNdarray `|\n+------------------------------------------------------------------+-------------------------------------------------------------------------------------------+\n| :ref:`guides/grpc:Tabular data representation via `DataFrame``` | :ref:`bentoml.io.PandasDataFrame <reference/api_io_descriptors:Tabular Data with Pandas>` |\n+------------------------------------------------------------------+-------------------------------------------------------------------------------------------+\n| :ref:`guides/grpc:Series representation via`Series`| :ref:`bentoml.io.PandasDataFrame <reference/api_io_descriptors:Tabular Data with Pandas>` |\n+------------------------------------------------------------------+-------------------------------------------------------------------------------------------+\n| :ref:`guides/grpc:File-like object via ``File`                 | :ref:`bentoml.io.File <reference/api_io_descriptors:Files>`                               |\n+------------------------------------------------------------------+-------------------------------------------------------------------------------------------+\n| |google_protobuf_string_value|                                  | :ref:`bentoml.io.Text <reference/api_io_descriptors:Texts>`                               |\n+------------------------------------------------------------------+-------------------------------------------------------------------------------------------+\n| |google_protobuf_value|                                         | :ref:`bentoml.io.JSON <reference/api_io_descriptors:Structured Data with JSON>`           |\n+------------------------------------------------------------------+-------------------------------------------------------------------------------------------+\n| :ref:`guides/grpc:Complex payload via ``Multipart```             | :ref:`bentoml.io.Multipart `|\n+------------------------------------------------------------------+-------------------------------------------------------------------------------------------+\n| :ref:`guides/grpc:Compact data format via ``serialized_bytes```  | (See below)                                                                               |\n+------------------------------------------------------------------+-------------------------------------------------------------------------------------------+\n.. note::\n`Series` is currently not yet supported.\n.. _google_protobuf_value: https://developers.google.com/protocol-buffers/docs/reference/google.protobuf#google.protobuf.Value\n.. |google_protobuf_value| replace:: `google.protobuf.Value`\n.. _google_protobuf_string_value: https://developers.google.com/protocol-buffers/docs/reference/google.protobuf#stringvalue\n.. |google_protobuf_string_value| replace:: `google.protobuf.StringValue`\nThe `Response` message will then return one of the aforementioned types as result.\n:raw-html:`<br />`\n:bdg-info:`Example:` In the :ref:`quickstart guide<tutorial:Creating a Service>`, we defined a `classify` API that takes in a :ref:`bentoml.io.NumpyNdarray <reference/api_io_descriptors:NumPy \\``ndarray``>`.\nTherefore, our `Request` message would have the following structure:\n.. tab-set::\n.. tab-item:: Python\n      :sync: python\n\n\n```  .. literalinclude:: ./snippets/grpc/python/request.py\n     :language: python\n```\n\n\n.. tab-item:: Go\n      :sync: golang\n\n\n```  .. literalinclude:: ./snippets/grpc/go/request.go\n     :language: go\n```\n\n\n.. tab-item:: C++\n      :sync: cpp\n\n\n```  .. literalinclude:: ./snippets/grpc/cpp/request.cc\n     :language: cpp\n```\n\n\n.. tab-item:: Java\n      :sync: java\n\n\n```  .. literalinclude:: ./snippets/grpc/java/Request.java\n     :language: java\n```\n\n\n.. tab-item:: Kotlin\n      :sync: kotlin\n\n\n```  .. literalinclude:: ./snippets/grpc/kotlin/Request.kt\n     :language: java\n```\n\n\n.. tab-item:: Node.js\n      :sync: nodejs\n\n\n```  .. literalinclude:: ./snippets/grpc/node/request.js\n     :language: javascript\n```\n\n\n.. tab-item:: Swift\n      :sync: swift\n\n\n```  .. literalinclude:: ./snippets/grpc/swift/Request.swift\n     :language: swift\n```\n\n\nArray representation via `NDArray`\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n:bdg-info:`Description:` `NDArray` represents a flattened n-dimensional array of arbitrary type. It accepts the following fields:\n\n`dtype`\n\nThe data type of given input. This is a `Enum <https://developers.google.com/protocol-buffers/docs/proto3#enum>`_ field that provides 1-1 mapping with Protobuf data types to NumPy data types:\n+-----------------------+---------------+------------+\n  | pb.NDArray.DType      | numpy.dtype   | Enum value |\n  +=======================+===============+============+\n  | `DTYPE_UNSPECIFIED` | `None`      | 0          |\n  +-----------------------+---------------+------------+\n  | `DTYPE_FLOAT`       | `np.float`  | 1          |\n  +-----------------------+---------------+------------+\n  | `DTYPE_DOUBLE`      | `np.double` | 2          |\n  +-----------------------+---------------+------------+\n  | `DTYPE_BOOL`        | `np.bool_`  | 3          |\n  +-----------------------+---------------+------------+\n  | `DTYPE_INT32`       | `np.int32`  | 4          |\n  +-----------------------+---------------+------------+\n  | `DTYPE_INT64`       | `np.int64`  | 5          |\n  +-----------------------+---------------+------------+\n  | `DTYPE_UINT32`      | `np.uint32` | 6          |\n  +-----------------------+---------------+------------+\n  | `DTYPE_UINT64`      | `np.uint64` | 7          |\n  +-----------------------+---------------+------------+\n  | `DTYPE_STRING`      | `np.str_`   | 8          |\n  +-----------------------+---------------+------------+\n\n`shape`\n\nA list of `int32` that represents the shape of the flattened array. the :ref:`bentoml.io.NumpyNdarray <reference/api_io_descriptors:NumPy \\``ndarray``>` will\n  then reshape the given payload into expected shape.\nNote that this value will always takes precendence over the `shape` field in the :ref:`bentoml.io.NumpyNdarray <reference/api_io_descriptors:NumPy \\``ndarray``>`descriptor,\n  meaning the array will be reshaped to this value first if given. Refer to :meth:`bentoml.io.NumpyNdarray.from_proto` for implementation details.\n\n`string_values`, `float_values`, `double_values`, `bool_values`, `int32_values`, `int64_values`, `uint32_values`, `unit64_values`\n\nEach of the fields is a `list` of the corresponding data type. The list is a flattened array, and will be reconstructed\n  alongside with `shape` field to the original payload.\nPer request sent, one message should only contain ONE of the aforementioned fields.\nThe interaction among the above fields and `dtype` are as follows:\n\n\nif `dtype` is not present in the message:\n\nAll of the fields are empty, then we return a `np.empty`.\n\nWe will loop through all of the provided fields, and only allows one field per message.\nIf here are more than one field (i.e. `string_values` and `float_values`), then we will raise an error, as we don't know how to deserialize the data.\n\n\n\n\notherwise:\n\nWe will use the provided dtype-to-field map to get the data from the given message.\n\n+------------------+-------------------+\n  | DType            | field             |\n  +------------------+-------------------+\n  | `DTYPE_BOOL`   | `bool_values`   |\n  +------------------+-------------------+\n  | `DTYPE_DOUBLE` | `double_values` |\n  +------------------+-------------------+\n  | `DTYPE_FLOAT`  | `float_values`  |\n  +------------------+-------------------+\n  | `DTYPE_INT32`  | `int32_values`  |\n  +------------------+-------------------+\n  | `DTYPE_INT64`  | `int64_values`  |\n  +------------------+-------------------+\n  | `DTYPE_STRING` | `string_values` |\n  +------------------+-------------------+\n  | `DTYPE_UINT32` | `uint32_values` |\n  +------------------+-------------------+\n  | `DTYPE_UINT64` | `uint64_values` |\n  +------------------+-------------------+\n\n\nFor example, if `dtype` is `DTYPE_FLOAT`, then the payload expects to have `float_values` field.\n.. grid:: 2\n\n\n```.. grid-item-card::  ``Python API``\n\n  .. code-block:: python\n\n     NumpyNdarray.from_sample(\n        np.array([[5.4, 3.4, 1.5, 0.4]])\n     )\n\n.. grid-item-card::  ``pb.NDArray``\n\n  .. code-block:: none\n\n     ndarray {\n       dtype: DTYPE_FLOAT\n       shape: 1\n       shape: 4\n       float_values: 5.4\n       float_values: 3.4\n       float_values: 1.5\n       float_values: 0.4\n     }\n```\n\n\n:bdg-primary:`API reference:` :meth:`bentoml.io.NumpyNdarray.from_proto`\n:raw-html:`<br />`\nTabular data representation via `DataFrame`\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n:bdg-info:`Description:` `DataFrame` represents any tabular data type. Currently we only support the columns orientation\nsince it is best for preserving the input order.\nIt accepts the following fields:\n\n`column_names`\n\nA list of `string` that represents the column names of the given tabular data.\n\n`column_values`\n\nA list of `Series` where `Series` represents a series of arbitrary data type. The allowed fields for\n  `Series` as similar to the ones in `NDArray`:\n\none of [`string_values`, `float_values`, `double_values`, `bool_values`, `int32_values`, `int64_values`, `uint32_values`, `unit64_values`]\n\n.. grid:: 2\n\n\n```.. grid-item-card::  ``Python API``\n\n  .. code-block:: python\n\n     PandasDataFrame.from_sample(\n         pd.DataFrame({\n           \"age\": [3, 29],\n           \"height\": [94, 170],\n           \"weight\": [31, 115]\n         }),\n         orient=\"columns\",\n     )\n\n.. grid-item-card::  ``pb.DataFrame``\n\n  .. code-block:: none\n\n     dataframe {\n       column_names: \"age\"\n       column_names: \"height\"\n       column_names: \"weight\"\n       columns {\n         int32_values: 3\n         int32_values: 29\n       }\n       columns {\n         int32_values: 40\n         int32_values: 190\n       }\n       columns {\n         int32_values: 140\n         int32_values: 178\n       }\n     }\n```\n\n\n:bdg-primary:`API reference:` :meth:`bentoml.io.PandasDataFrame.from_proto`\nSeries representation via `Series`\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n:bdg-info:`Description:` `Series` portrays a series of values. This can be used for representing Series types in tabular data.\nIt accepts the following fields:\n\n`string_values`, `float_values`, `double_values`, `bool_values`, `int32_values`, `int64_values`\n\nSimilar to NumpyNdarray, each of the fields is a `list` of the corresponding data type. The list is a 1-D array, and will be then pass to `pd.Series`.\nEach request should only contain ONE of the aforementioned fields.\nThe interaction among the above fields and `dtype` from `PandasSeries` are as follows:\n\n\nif `dtype` is not present in the descriptor:\n\nAll of the fields are empty, then we return an empty `pd.Series`.\n\nWe will loop through all of the provided fields, and only allows one field per message.\nIf here are more than one field (i.e. `string_values` and `float_values`), then we will raise an error, as we don't know how to deserialize the data.\n\n\n\n\notherwise:\n\nWe will use the provided dtype-to-field map to get the data from the given message.\n\n\n\n.. grid:: 2\n\n\n```.. grid-item-card::  ``Python API``\n\n  .. code-block:: python\n\n     PandasSeries.from_sample([5.4, 3.4, 1.5, 0.4])\n\n.. grid-item-card::  ``pb.Series``\n\n  .. code-block:: none\n\n     series {\n       float_values: 5.4\n       float_values: 3.4\n       float_values: 1.5\n       float_values: 0.4\n     }\n```\n\n\n:bdg-primary:`API reference:` :meth:`bentoml.io.PandasSeries.from_proto`\n:raw-html:`<br />`\nFile-like object via `File`\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n:bdg-info:`Description:` `File` represents any arbitrary file type. this can be used\nto send in any file type, including images, videos, audio, etc.\n.. note::\nCurrently both :class:`bentoml.io.File` and :class:`bentoml.io.Image` are using\n   `pb.File`\nIt accepts the following fields:\n\n`content`\n\nA `bytes` field that represents the content of the file.\n\n`kind`\n\nAn optional `string` field that represents the file type. If specified, it will raise an error if\n  `mime_type` specified in :ref:`bentoml.io.File <reference/api_io_descriptors:Files>` is not matched.\n.. grid:: 2\n\n\n```.. grid-item-card::  ``Python API``\n\n  .. code-block:: python\n\n     Image(mime_type=\"application/pdf\")\n\n.. grid-item-card::  ``pb.File``\n\n  .. code-block:: none\n\n     file {\n       kind: \"application/pdf\"\n       content: <bytes>\n     }\n```\n\n\n:ref:`bentoml.io.Image <reference/api_io_descriptors:Images>` will also be using `pb.File`.\n.. grid:: 2\n\n\n```.. grid-item-card::  ``Python API``\n\n  .. code-block:: python\n\n     File(mime_type=\"image/png\")\n\n.. grid-item-card::  ``pb.File``\n\n  .. code-block:: none\n\n     file {\n       kind: \"image/png\"\n       content: <bytes>\n     }\n```\n\n\nComplex payload via `Multipart`\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n:bdg-info:`Description:` `Multipart` represents a complex payload that can contain\nmultiple different fields. It takes a `fields`, which is a dictionary of input name to\nits coresponding :class:`bentoml.io.IODescriptor`\n.. grid:: 2\n\n\n```.. grid-item-card::  ``Python API``\n\n  .. code-block:: python\n\n     Multipart(\n        meta=Text(),\n        arr=NumpyNdarray(\n           dtype=np.float16,\n           shape=[2,2]\n        )\n     )\n\n.. grid-item-card::  ``pb.Multipart``\n\n  .. code-block:: none\n\n     multipart {\n        fields {\n           key: \"arr\"\n           value {\n              ndarray {\n              dtype: DTYPE_FLOAT\n              shape: 2\n              shape: 2\n              float_values: 1.0\n              float_values: 2.0\n              float_values: 3.0\n              float_values: 4.0\n              }\n           }\n        }\n        fields {\n           key: \"meta\"\n           value {\n              text {\n              value: \"nlp\"\n              }\n           }\n        }\n     }\n```\n\n\n:bdg-primary:`API reference:` :meth:`bentoml.io.Multipart.from_proto`\nCompact data format via `serialized_bytes`\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nThe `serialized_bytes` field in both `Request` and `Response`  is reserved for pre-established protocol encoding between client and server.\nBentoML leverages the field to improve serialization performance between BentoML client and server. Thus the field is not recommended for use directly.\nMounting Servicer\n~~~~~~~~~~~~~~~~~\ngRPC service :ref:`multiplexing <guides/grpc:Demystifying the misconception of gRPC vs. REST>` enables us to mount additional custom servicers alongside with BentoService,\nand serve them under the same port.\n.. code-block:: python\n   :caption: `service.py`\n   :emphasize-lines: 13\nimport route_guide_pb2\n   import route_guide_pb2_grpc\n   from servicer_impl import RouteGuideServicer\nsvc = bentoml.Service(\"iris_classifier\", runners=[iris_clf_runner])\nservices_name = [\n       v.full_name for v in route_guide_pb2.DESCRIPTOR.services_by_name.values()\n   ]\n   svc.mount_grpc_servicer(\n       RouteGuideServicer,\n       add_servicer_fn=add_RouteGuideServicer_to_server,\n       service_names=services_name,\n   )\nServe your service with :ref:`bentoml serve-grpc <reference/cli:serve-grpc>` command:\n.. code-block:: bash\n\u00bb bentoml serve-grpc service.py:svc --reload --enable-reflection\nNow your `RouteGuide` service can also be accessed through `localhost:3000`.\n.. note::\n`service_names` is REQUIRED here, as this will be used for :github:`server reflection <grpc/grpc/blob/master/doc/server-reflection.md>`\n   when `--enable-reflection` is passed to `bentoml serve-grpc`.\nMounting gRPC Interceptors\n~~~~~~~~~~~~~~~~~~~~~~~~~~\nInteceptors are a component of gRPC that allows us to intercept and interact with the\nproto message and service context either before - or after - the actual RPC call was\nsent/received by client/server.\nInterceptors to gRPC is what middleware is to HTTP. The most common use-case for interceptors\nare authentication, :ref:`tracing <guides/tracing:Tracing>`, access logs, and more.\nBentoML comes with a sets of built-in async interceptors to provide support for access logs,\n`OpenTelemetry <https://opentelemetry.io/>`, and `Prometheus <https://prometheus.io/>`.\nThe following diagrams demonstrates the flow of a gRPC request from client to server:\n.. image:: /_static/img/interceptor-flow.png\n   :alt: Interceptor Flow\nSince interceptors are executed in the order they are added, users interceptors will be executed after the built-in interceptors.\nUsers interceptors shouldn't modify the existing headers and data of the incoming `Request`.\nBentoML currently only support async interceptors (via `grpc.aio.ServerInterceptor <https://grpc.github.io/grpc/python/grpc_asyncio.html#grpc.aio.ServerInterceptor>`, as opposed to `grpc.ServerInterceptor <https://grpc.github.io/grpc/python/grpc_asyncio.html#grpc.aio.ServerInterceptor>`). This is\nbecause BentoML gRPC server is an async implementation of gRPC server.\n.. note::\nIf you are using `grpc.ServerInterceptor`, you will need to migrate it over\n   to use the new `grpc.aio.ServerInterceptor` in order to use this feature.\nFeel free to reach out to us at `#support on Slack <https://l.linklyhq.com/l/ktOX>`_\n.. dropdown:: A toy implementation `AppendMetadataInterceptor`\n.. code-block:: python\n      :caption: metadata_interceptor.py\n\n\n```  from __future__ import annotations\n\n  import typing as t\n  import functools\n  import dataclasses\n  from typing import TYPE_CHECKING\n\n  from grpc import aio\n\n  if TYPE_CHECKING:\n      from bentoml.grpc.types import Request\n      from bentoml.grpc.types import Response\n      from bentoml.grpc.types import RpcMethodHandler\n      from bentoml.grpc.types import AsyncHandlerMethod\n      from bentoml.grpc.types import HandlerCallDetails\n      from bentoml.grpc.types import BentoServicerContext\n\n\n  @dataclasses.dataclass\n  class Context:\n      usage: str\n      accuracy_score: float\n\n\n  class AppendMetadataInterceptor(aio.ServerInterceptor):\n       def __init__(self, *, usage: str, accuracy_score: float) -> None:\n           self.context = Context(usage=usage, accuracy_score=accuracy_score)\n           self._record: set[str] = set()\n\n       async def intercept_service(\n           self,\n           continuation: t.Callable[[HandlerCallDetails], t.Awaitable[RpcMethodHandler]],\n           handler_call_details: HandlerCallDetails,\n       ) -> RpcMethodHandler:\n           from bentoml.grpc.utils import wrap_rpc_handler\n\n           handler = await continuation(handler_call_details)\n\n           if handler and (handler.response_streaming or handler.request_streaming):\n               return handler\n\n           def wrapper(behaviour: AsyncHandlerMethod[Response]):\n               @functools.wraps(behaviour)\n               async def new_behaviour(\n                  request: Request, context: BentoServicerContext\n               ) -> Response | t.Awaitable[Response]:\n                   self._record.update(\n                     {f\"{self.context.usage}:{self.context.accuracy_score}\"}\n                   )\n                   resp = await behaviour(request, context)\n                   context.set_trailing_metadata(\n                      tuple(\n                            [\n                               (k, str(v).encode(\"utf-8\"))\n                               for k, v in dataclasses.asdict(self.context).items()\n                            ]\n                      )\n                   )\n                   return resp\n\n               return new_behaviour\n\n           return wrap_rpc_handler(wrapper, handler)\n```\n\n\nTo add your intercptors to existing BentoService, use `svc.add_grpc_interceptor`:\n.. code-block:: python\n   :caption: `service.py`\nfrom custom_interceptor import CustomInterceptor\nsvc.add_grpc_interceptor(CustomInterceptor)\n.. note::\n`add_grpc_interceptor` also supports `partial` class as well as multiple arguments\n   interceptors:\n.. tab-set::\n\n\n```  .. tab-item:: multiple arguments\n\n     .. code-block:: python\n\n        from metadata_interceptor import AppendMetadataInterceptor\n\n        svc.add_grpc_interceptor(AppendMetadataInterceptor, usage=\"NLP\", accuracy_score=0.867)\n\n  .. tab-item:: partial method\n\n     .. code-block:: python\n\n        from functools import partial\n\n        from metadata_interceptor import AppendMetadataInterceptor\n\n        svc.add_grpc_interceptor(partial(AppendMetadataInterceptor, usage=\"NLP\", accuracy_score=0.867))\n```\n\n\n\nRecommendations\ngRPC is designed to be high performance framework for inter-service communications. This\nmeans that it is a perfect fit for building microservices. The following are some\nrecommendation we have for using gRPC for model serving:\n:raw-html:`<br />`\nDemystifying the misconception of gRPC vs. REST\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nYou might stumble upon articles comparing gRPC to REST, and you might get the impression\nthat gRPC is a better choice than REST when building services. This is not entirely\ntrue.\ngRPC is built on top of HTTP/2, and it addresses some of the shortcomings of HTTP/1.1,\nsuch as :wiki:`head-of-line blocking <Head-of-line_blocking>`, and :wiki:`HTTP pipelining <HTTP_pipelining>`.\nHowever, gRPC is not a replacement for REST, and indeed it is not a replacement for\nmodel serving. gRPC comes with its own set of trade-offs, such as:\n\n\nLimited browser support: It is impossible to call a gRPC service directly from any\n  browser. You will end up using tools such as :github:`gRPCUI <fullstorydev/grpcui>` in order to interact\n  with your service, or having to go through the hassle of implementing a gRPC client in\n  your language of choice.\n\n\nBinary protocol format: While :github:`Protobuf <protocolbuffers/protobuf>` is\n  efficient to send and receive over the wire, it is not human-readable. This means\n  additional toolin for debugging and analyzing protobuf messages are required.\n\n\nKnowledge gap: gRPC comes with its own concepts and learning curve, which requires\n  teams to invest time in filling those knowledge gap to be effectively use gRPC. This\n  often leads to a lot of friction and sometimes increase friction to the development\n  agility.\n\n\nLack of support for additional content types: gRPC depends on protobuf, its content\n  type are restrictive, in comparison to out-of-the-box support from HTTP+REST.\n\n\n.. seealso::\n`gRPC on HTTP/2 <https://grpc.io/blog/grpc-on-http2/>` dives into how gRPC is built\n   on top of HTTP/2, and this `article <https://www.cncf.io/blog/2018/07/03/http-2-smarter-at-scale/>`\n   goes into more details on how HTTP/2 address the problem from HTTP/1.1\nFor HTTP/2 specification, see `RFC 7540 <https://tools.ietf.org/html/rfc7540>`_.\n:raw-html:`<br />`\nShould I use gRPC instead of REST for model serving?\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nYes and no.\nIf your organization is already using gRPC for inter-service communications, using\nyour Bento with gRPC is a no-brainer. You will be able to seemlessly integrate your\nBento with your existing gRPC services without having to worry about the overhead of\nimplementing :github:`grpc-gateway <grpc-ecosystem/grpc-gateway>`.\nHowever, if your organization is not using gRPC, we recommend to keep using REST for\nmodel serving. This is because REST is a well-known and well-understood protocol,\nmeaning there is no knowledge gap for your team, which will increase developer agility, and\nfaster go-to-market strategy.\n:raw-html:`<br />`\nPerformance tuning\n~~~~~~~~~~~~~~~~~~\nBentoML allows user to tune the performance of gRPC via :ref:`bentoml_configuration.yaml <guides/configuration:Configuration>` via `api_server.grpc`.\nA quick overview of the available configuration for gRPC:\n.. code-block:: yaml\n   :caption: `bentoml_configuration.yaml`\napi_server:\n     grpc:\n       host: 0.0.0.0\n       port: 3000\n       max_concurrent_streams: ~\n       maximum_concurrent_rpcs: ~\n       max_message_length: -1\n       reflection:\n         enabled: false\n       metrics:\n         host: 0.0.0.0\n         port: 3001\n:raw-html:`<br />`\n`max_concurrent_streams`\n^^^^^^^^^^^^^^^^^^^^^^^^^^\n.. epigraph::\n   :bdg-info:`Definition:` Maximum number of concurrent incoming streams to allow on a HTTP2 connection.\nBy default we don't set a limit cap. HTTP/2 connections typically has limit of `maximum concurrent streams <httpwg.org/specs/rfc7540.html#rfc.section.5.1.2>`_\non a connection at one time.\n.. dropdown:: Some notes about fine-tuning `max_concurrent_streams`\nNote that a gRPC channel uses a single HTTP/2 connection, and concurrent calls are multiplexed on said connection.\n   When the number of active calls reaches the connection stream limit, any additional\n   calls are queued to the client. Queued calls then wait for active calls to complete before being sent. This means that\n   application will higher load and long running streams could see a performance degradation caused by queuing because of the limit.\nSetting a limit cap on the number of concurrent streams will prevent this from happening, but it also means that\n   you need to tune the limit cap to the right number. \n\n\nIf the limit cap is too low, you will sooner or later running into the issue mentioned above.\n\n\nNot setting a limit cap are also NOT RECOMMENDED. Too many streams on a single\n     HTTP/2 connection introduces `thread contention` between streams trying to write\n     to the connection, `packet loss` which causes all call to be blocked.\n\n\n:bdg-info:`Remarks:` We recommend you to play around with the limit cap, starting with 100, and increase if needed.\n:raw-html:`<br />`\n`maximum_concurrent_rpcs`\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.. epigraph::\n   :bdg-info:`Definition:` The maximum number of concurrent RPCs this server will service before returning `RESOURCE_EXHAUSTED` status.\nBy default we set to `None` to indicate no limit, and let gRPC to decide the limit.\n:raw-html:`<br />`\n`max_message_length`\n^^^^^^^^^^^^^^^^^^^^^^\n.. epigraph::\n   :bdg-info:`Definition:` The maximum message length in bytes allowed to be received on/can be send to the server.\nBy default we set to `-1` to indicate no limit.\nMessage size limits via this options is a way to prevent gRPC from consuming excessive\nresources. By default, gRPC uses per-message limits to manage inbound and outbound\nmessage.\n.. dropdown:: Some notes about fine-tuning `max_message_length`\nThis options sets two values: :github:`grpc.max_receive_message_length <grpc/grpc/blob/e8df8185e521b518a8f608b8a5cf98571e2d0925/include/grpc/impl/codegen/grpc_types.h#L153>`\n   and :github:`grpc.max_send_message_length <grpc/grpc/blob/e8df8185e521b518a8f608b8a5cf98571e2d0925/include/grpc/impl/codegen/grpc_types.h#L159>`.\n.. code-block:: cpp\n\n\n```  #define GRPC_ARG_MAX_RECEIVE_MESSAGE_LENGTH \"grpc.max_receive_message_length\"\n\n  #define GRPC_ARG_MAX_SEND_MESSAGE_LENGTH \"grpc.max_send_message_length\"\n```\n\n\nBy default, gRPC sets incoming message to be 4MB, and no limit on outgoing message.\n   We recommend you to only set this option if you want to limit the size of outcoming message. Otherwise, you should let gRPC to determine the limit.\nWe recommend you to also check out `gRPC performance best practice <https://grpc.io/docs/guides/performance/>`_ to learn about best practice for gRPC.",
    "tag": "bentoml"
  },
  {
    "title": "containerization.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/guides/containerization.rst",
    "content": "=========================\nAdvanced Containerization\n=========================\ntime expected: 12 minutes\nThis guide describes advanced containerization options \nprovided by BentoML:\n\n:ref:`Using base image <guides/containerization:Custom Base Image>`\n:ref:`Using dockerfile template <guides/containerization:Dockerfile Template>`\n\nThis is an advanced feature for user to customize container environment that are not directly supported in BentoML.\nFor basic containerizing options, see :ref:`Docker Options <concepts/bento:Docker Options>`.\nWhy you may need this?\n\nIf you want to customize the containerization process of your Bento.\nIf you need a certain tools, configs, prebuilt binaries that is available across all your Bento generated container images.\nA big difference with :ref:`base image <concepts/bento:Docker Options Table>` features is that you don't have to setup a custom base image and then push it to a remote registry.\n\nCustom Base Image\nIf none of the provided distros work for your use case, e.g. if your infrastructure\nrequires all docker images to be derived from the same base image with certain security\nfixes and libraries, you can config BentoML to use your base image instead:\n.. code:: yaml\n\n\n```docker:\n    base_image: \"my_custom_image:latest\"\n```\n\n\nWhen a :code:`base_image` is provided, all other docker options will be ignored,\n(distro, cuda_version, system_packages, python_version). :code:`bentoml containerize`\nwill build a new image on top of the base_image with the following steps:\n\nsetup env vars\nrun the :code:`setup_script` if provided\ninstall the required Python packages\ncopy over the Bento file\nsetup the entrypoint command for serving.\n\n.. note::\n\n\n```:bdg-warning:`Warning:` user must ensure that the provided base image has desired\nPython version installed. If the base image you have doesn't have Python, you may\ninstall python via a :code:`setup_script`. The implementation of the script depends\non the base image distro or the package manager available.\n\n.. code:: yaml\n\n    docker:\n        base_image: \"my_custom_image:latest\"\n        setup_script: \"./setup.sh\"\n```\n\n\n.. warning::\n\n\n```By default, BentoML supports multi-platform docker image build out-of-the-box.\nHowever, when a custom :code:`base_image` is provided, the generated Dockerfile can\nonly be used for building linux/amd64 platform docker images.\n\nIf you are running BentoML from an Apple M1 device or an ARM based computer, make\nsure to pass the :code:`--opt platform=linux/amd64` parameter when containerizing a Bento. e.g.:\n\n.. code:: bash\n\n    bentoml containerize iris_classifier:latest --opt platform=linux/amd64\n```\n\n\nDockerfile Template\nThe :code:`dockerfile_template` field gives the user full control over how the\n:code:`Dockerfile` is generated for a Bento by extending the template used by\nBentoML.\nFirst, create a :code:`Dockerfile.template` file next to your :code:`bentofile.yaml`\nbuild file. This file should follow the\n`Jinja2 <https://jinja.palletsprojects.com/en/3.1.x/>` template language, and extend\nBentoML's base template and blocks. The template should render a valid\n`Dockerfile <https://docs.docker.com/engine/reference/builder/>`. For example:\n.. code-block:: jinja\n{% extends bento_base_template %}\n   {% block SETUP_BENTO_COMPONENTS %}\n   {{ super() }}\n   RUN echo \"We are running this during bentoml containerize!\"\n   {% endblock %}\nThen add the path to your template file to the :code:`dockerfile_template` field in\nyour :code: `bentofile.yaml`:\n.. code:: yaml\n\n\n```docker:\n    dockerfile_template: \"./Dockerfile.template\"\n```\n\n\nNow run :code:`bentoml build` to build a new Bento. It will contain a Dockerfile\ngenerated with the custom template. To confirm the generated Dockerfile works as\nexpected, run :code:`bentoml containerize <bento>` to build a docker image with it.\n.. dropdown:: View the generated Dockerfile content\n    :icon: code\n\n\n```During development and debugging, you may want to see the generated Dockerfile.\nHere's shortcut for that:\n\n.. code-block:: bash\n\n    cat \"$(bentoml get <bento>:<tag> -o path)/env/docker/Dockerfile\"\n```\n\n\nExamples\n\n:ref:`guides/containerization:Building TensorFlow custom op`\n:ref:`guides/containerization:Access AWS credentials during image build`\n\nBuilding TensorFlow custom op\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nLet's start with an example that builds a `custom TensorFlow op <https://www.tensorflow.org/guide/create_op>` binary into a Bento, which is based on |zero_out|:\n.. _zero_out: https://www.tensorflow.org/guide/create_op#define_the_op_interface\n.. |zero_out| replace:: :code:`zero_out.cc` implementation details\nDefine the following :code:`Dockerfile.template`:\n.. literalinclude:: ./snippets/containerization/tf_ops.template\n   :language: jinja\n   :caption: `Dockerfile.template`\nThen add the following to your :code:`bentofile.yaml`:\n.. code-block:: yaml\ninclude:\n     - \"zero_out.cc\"\n   python:\n     packages:\n     - tensorflow\n   docker:\n     dockerfile_template: ./Dockerfile.template\nProceed to build your Bento with :code:`bentoml build` and containerize with :code:`bentoml containerize`:\n.. code-block:: bash\nbentoml build\nbentoml containerize :\n.. tip:: \nYou can also provide :code:`--progress plain` to see the progress from\n   `buildkit <https://github.com/moby/buildkit>`_ in plain text\n.. code-block:: yaml\n\n\n```  bentoml containerize --progress plain <bento>:<tag>\n```\n\n\nAccess AWS credentials during image build\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nWe will now demonstrate how to provide AWS credentials to a Bento via two approaches:\n\n:ref:`guides/containerization:Using environment variables`.\n:ref:`guides/containerization:Mount credentials from host`.\n\n.. note::\n:bdg-info:`Remarks:` We recommend for most cases \n   to use the second option (:ref:`guides/containerization:Mount credentials from host`)\n   as it prevents any securities leak.\nBy default BentoML uses the latest `dockerfile frontend <https://hub.docker.com/r/docker/dockerfile>`_ which\n   allows mounting secrets to container.\nFor both examples, you will need to add the following to your :code:`bentofile.yaml`:\n.. code-block:: yaml\npython:\n     packages:\n     - awscli\n   docker:\n     dockerfile_template: ./Dockerfile.template\nUsing environment variables\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\nDefine the following :code:`Dockerfile.template`:\n.. code-block:: jinja\n{% extends bento_base_template %}\n   {% block SETUP_BENTO_BASE_IMAGE %}\n   ARG AWS_SECRET_ACCESS_KEY\n   ARG AWS_ACCESS_KEY_ID\n   {{ super() }}\nARG AWS_SECRET_ACCESS_KEY\n   ARG AWS_ACCESS_KEY_ID\nENV AWS_SECRET_ACCESS_KEY=$ARG AWS_SECRET_ACCESS_KEY\n   ENV AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID\n   {% endblock %}\n   {% block SETUP_BENTO_COMPONENTS %}\n   {{ super() }}\nRUN aws s3 cp s3://path/to/file {{ bento__path }}\n{% endblock %}\nAfter building the bento with :code:`bentoml build`, you can then\npass :code:`AWS_SECRET_ACCESS_KEY` and :code:`AWS_ACCESS_KEY_ID` as arguments to :code:`bentoml containerize`:\n.. code-block:: bash\nbentoml containerize --build-arg AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \\\n                        --build-arg AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \\\n                        :\nMount credentials from host\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\nDefine the following :code:`Dockerfile.template`:\n.. code-block:: jinja\n{% extends bento_base_template %}\n   {% block SETUP_BENTO_COMPONENTS %}\n   {{ super() }}\nRUN --mount=type=secret,id=aws,target=/root/.aws/credentials \\\n        aws s3 cp s3://path/to/file {{ bento__path }}\n{% endblock %}\nFollow the above addition to :code:`bentofile.yaml` to include `awscli` and\nthe custom dockerfile template.\nTo pass in secrets to the Bento, pass it via :code:`--secret` to :code:`bentoml\ncontainerize`:\n.. code-block:: bash\nbentoml containerize --secret id=aws,src=$HOME/.aws/credentials :\n.. seealso::\n`Mounting Secrets <https://github.com/moby/buildkit/blob/master/frontend/dockerfile/docs/syntax.md#run---mounttypesecret>`_\nWriting :code:`dockerfile_template`\nBentoML utilize `Jinja2 <https://jinja.palletsprojects.com/en/3.1.x/>`_ to\nstructure a :code:`Dockerfile.template`.\nThe Dockerfile template is a mix between :code:`Jinja2` syntax and :code:`Dockerfile`\nsyntax. BentoML set both `trim_blocks` and `lstrip_blocks` in Jinja\ntemplates environment to :code:`True`. \n.. note::\nMake sure that your Dockerfile instruction is unindented as if you are writting a normal Dockerfile.\n.. seealso::\n`Jinja Whitespace Control <https://jinja.palletsprojects.com/en/3.1.x/templates/#whitespace-control>`_.\nAn example of a Dockerfile template takes advantage of multi-stage build to\nisolate the installation of a local library :code:`mypackage`:\n.. code-block:: jinja\n{% extends bento_base_template %}\n   {% block SETUP_BENTO_BASE_IMAGE %}\n   FROM --platform=$BUILDPLATFORM python:3.7-slim as buildstage\n   RUN mkdir /tmp/mypackage\nWORKDIR /tmp/mypackage/\n   COPY mypackage .\n   RUN python setup.py sdist && mv dist/mypackage-0.0.1.tar.gz mypackage.tar.gz\n{{ super() }}\n   {% endblock %}\n   {% block SETUP_BENTO_COMPONENTS %}\n   {{ super() }}\n   COPY --from=buildstage mypackage.tar.gz /tmp/wheels/\n   RUN --network=none pip install --find-links /tmp/wheels mypackage\n   {% endblock %}\n.. note::\nNotice how for all Dockerfile instruction, we consider as if the Jinja\n   logics aren't there \ud83d\ude80.\nJinja templates\n~~~~~~~~~~~~~~~\nOne of the powerful features Jinja offers is its `template inheritance <https://jinja.palletsprojects.com/en/3.1.x/templates/#template-inheritance>`_.\nThis allows BentoML to enable users to fully customize how to structure a Bento's Dockerfile.\n.. note::\nTo use a custom Dockerfile template, users have to provide a file with a format\n   that follows the Jinja2 template syntax. The template file should have\n   extensions of :code:`.j2`, :code:`.template`, :code:`.jinja`.\n.. note::\nThis section is not meant to be a complete reference on Jinja2.\n   For any advanced features from on Jinja2, please refers to their `Templates Design Documentation <https://jinja.palletsprojects.com/en/3.1.x/templates/>`_.\nTo construct a custom :code:`Dockerfile` template, users have to provide an `extends block <https://jinja.palletsprojects.com/en/3.1.x/templates/#extends>`_ at the beginning of the Dockerfile template :code:`Dockerfile.template` followed by the given base template name :code:`bento_base_template`:\n.. code-block:: jinja\n{% extends bento_base_template %}\n.. tip::\n:bdg-warning:`Warning:` If you pass in a generic :code:`Dockerfile` file, and then run :code:`bentoml build` to build a Bento and it doesn't throw any errors.\nHowever, when you try to run :code:`bentoml containerize`, this won't work.\nThis is an expected behaviour from Jinja2, where Jinja2 accepts any file as a template.\nWe decided not to put any restrictions to validate the template file, simply because we want to enable \n   users to customize to their own needs. \n:code:`{{ super() }}`\n^^^^^^^^^^^^^^^^^^^^^\nAs you can notice throughout this guides, we use a special function :code:`{{ super() }}`. This is a Jinja\nfeatures that allow users to call content of `parent block <https://jinja.palletsprojects.com/en/3.1.x/templates/#super-blocks>`_. This \nenables users to fully extend base templates provided by BentoML to ensure that\nthe result Bentos can be containerized.\n.. seealso::\n|super_tag|_ for more information on template inheritance.\n.. _super_tag: https://jinja.palletsprojects.com/en/3.1.x/templates/#super-blocks\n.. |super_tag| replace:: :code:`{{ super() }}` Syntax\nBlocks\n^^^^^^\nBentoML defines a sets of `Blocks <https://jinja.palletsprojects.com/en/3.1.x/templates/#base-template>`_ under the object :code:`bento_base_template`.\nAll exported blocks that users can use to extend are as follow:\n+---------------------------------+----------------------------------------------------------------------------------------------------------------------------------+\n| Blocks                          | Definition                                                                                                                       |\n+=================================+==================================================================================================================================+\n| :code:`SETUP_BENTO_BASE_IMAGE`  | Instructions to set up multi architecture supports, base images as well as installing system packages that is defined by users.  |\n+---------------------------------+----------------------------------------------------------------------------------------------------------------------------------+\n| :code:`SETUP_BENTO_USER`        | Setup bento users with correct UID, GID and directory for a \ud83c\udf71.                                                                  |\n+---------------------------------+----------------------------------------------------------------------------------------------------------------------------------+\n| :code:`SETUP_BENTO_ENVARS`      | Add users environment variables (if specified) and other required variables from BentoML.                                        |\n+---------------------------------+----------------------------------------------------------------------------------------------------------------------------------+\n| :code:`SETUP_BENTO_COMPONENTS`  | Setup components for a \ud83c\udf71 , including installing pip packages, running setup scripts, installing bentoml, etc.                   |\n+---------------------------------+----------------------------------------------------------------------------------------------------------------------------------+\n| :code:`SETUP_BENTO_ENTRYPOINT`  | Finalize ports and set :code:`ENTRYPOINT` and :code:`CMD` for the \ud83c\udf71.                                                            |\n+---------------------------------+----------------------------------------------------------------------------------------------------------------------------------+\n.. note::\nAll the defined blocks are prefixed with :code:`SETUP_BENTO_*`. This is to\n   ensure that users can extend blocks defined by BentoML without sacrificing\n   the flexibility of a Jinja template.\nTo extend any given block, users can do so by adding :code:`{{ super() }}` at\nany point inside block.\nDockerfile instruction\n~~~~~~~~~~~~~~~~~~~~~~\n.. seealso::\n`Dockerfile reference <https://docs.docker.com/engine/reference/builder>`_ for writing a Dockerfile.\nWe recommend that users should use the following Dockerfile instructions in\ntheir custom Dockerfile templates: :code:`ENV`, :code:`RUN`, :code:`ARG`. These\ninstructions are mostly used and often times will get the jobs done.\nThe use of the following instructions can be potentially harmful. They should be reserved for specialized advanced use cases.\n+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Instruction    | Reasons not to use                                                                                                                                                                                                                                        |\n+================+===========================================================================================================================================================================================================================================================+\n| :code:`FROM`   | Since the containerized Bento is a multi-stage builds container, adding :code:`FROM` statement will result in failure to containerize the given Bento.                                                                                                    |\n+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| :code:`SHELL`  | BentoML uses `heredoc syntax <https://github.com/moby/buildkit/blob/master/frontend/dockerfile/docs/syntax.md#user-content-here-documents>`_ and using :code:`bash` in our containerization process. Hence changing :code:`SHELL` will result in failure. |\n+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| :code:`CMD`    | Changing :code:`CMD` will inherently modify the behaviour of the bento container where docker won't be able to run the bento inside the container. More :ref:`below <guides/containerization:\\:code\\:\\`entrypoint`>`                                     |\n+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\nThe following instructions should be used with caution:\n:code:`WORKDIR`\n^^^^^^^^^^^^^^^\n.. seealso::\n`WORKDIR reference <https://docs.docker.com/engine/reference/builder/#workdir>`_\nSince :code:`WORKDIR` determines the working directory for any :code:`RUN`, :code:`CMD`, :code:`ENTRYPOINT`, :code:`COPY` and :code:`ADD` instructions that follow it in the Dockerfile,\nmake sure that your instructions define the correct path to any working files.\n.. note::\nBy default, all paths for Bento-related files will be generated to its\n   fspath, which ensures that Bento will work regardless of :code:`WORKDIR`\n:code:`ENTRYPOINT`\n^^^^^^^^^^^^^^^^^^\n.. seealso::\n`ENTRYPOINT reference <https://docs.docker.com/engine/reference/builder/#entrypoint>`_\nThe flexibility of a Jinja template also brings up the flexibility of setting up :code:`ENTRYPOINT` and :code:`CMD`.\nFrom `Dockerfile documentation <https://docs.docker.com/engine/reference/builder/#entrypoint>`_:\n\n\n```Only the last :code:`ENTRYPOINT` instruction in the Dockerfile will have an effect.\n```\n\n\nBy default, a Bento sets:\n.. code-block:: jinja\n\n\n```ENTRYPOINT [ \"{{ bento__entrypoint }}\" ]\n\nCMD [\"bentoml\", \"serve\", \"{{ bento__path }}\", \"--production\"]\n```\n\n\nThis aboved instructions ensure that whenever :code:`docker run` is invoked on the \ud83c\udf71 container, :code:`bentoml` is called correctly. \nIn scenarios where one needs to setup a custom :code:`ENTRYPOINT`, make sure to use\nthe :code:`ENTRYPOINT` instruction under the :code:`SETUP_BENTO_ENTRYPOINT` block as follows:\n.. code-block:: jinja\n\n\n```{% extends bento_base_template %}\n{% block SETUP_BENTO_ENTRYPOINT %}\n{{ super() }}\n\n...\nENTRYPOINT [ \"{{ bento__entrypoint }}\", \"python\", \"-m\", \"awslambdaric\" ]\n{% endblock %}\n```\n\n\n.. tip::\n\n\n```:code:`{{ bento__entrypoint }}` is the path the BentoML entrypoint,\nnothinig special here \ud83d\ude0f.\n```\n\n\nRead more about :code:`CMD` and :code:`ENTRYPOINT` interaction `here <https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact>`_.\nAdvanced Options\nThe next part goes into advanced options. Skip this part if you are not\ncomfortable with using it.\nDockerfile variables\n~~~~~~~~~~~~~~~~~~~~\nBentoML does expose some variables that user can modify to fit their needs.\nThe following are the variables that users can set in their custom Dockerfile template:\n+-------------------------+---------------------------------------------------------------------+\n| Variables               | Description                                                         |\n+=========================+=====================================================================+\n| :code:`bento__home`     | Setup bento home, default to :code:`/home/{{ bento__user }}`        |\n+-------------------------+---------------------------------------------------------------------+\n| :code:`bento__user`     | Setup bento user, default to :code:`bentoml`                        |\n+-------------------------+---------------------------------------------------------------------+\n| :code:`bento__uid_gid`  | Setup UID and GID for the user, default to :code:`1034:1034`        |\n+-------------------------+---------------------------------------------------------------------+\n| :code:`bento__path`     | Setup bento path, default to :code:`/home/{{ bento__user }}/bento`  |\n+-------------------------+---------------------------------------------------------------------+\nIf any of the aforementioned fields are set with :code:`{% set ... %}`, then we\nwill use your value instead, otherwise a default value will be used.\nAdding :code:`conda` to CUDA-enabled Bento\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n.. tip::\n:bdg-warning:`Warning:` miniconda install scripts provided by ContinuumIO (the parent company of Anaconda) supports Python 3.7 to 3.9. Make sure that you are using the correct python version under :code:`docker.python_version`.\nIf you need to use conda for CUDA images, use the following template ( partially extracted from |conda_docker|_ ):\n.. dropdown:: Expands me\n   :class-title: sd-text-primary\n   :icon: code\n.. literalinclude:: ./snippets/containerization/conda_cuda.template\n      :language: jinja\n      :caption: `Dockerfile.template`\nContainerization with different container engines.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nIn BentoML version 1.0.11 [#pr_3164]_, we support different container engines aside from docker.\nBentoML-generated Dockerfiles from version 1.0.11 onward will be OCI-compliant and can be built with:\n\n`Docker <https://www.docker.com/>`_\n`Podman <https://podman.io/>`_\n`Buildah <https://buildah.io/>`_\n`nerdctl <https://github.com/containerd/nerdctl>`_\n:github:`buildctl <moby/buildkit/blob/master/docs/buildctl.md>`\n`Docker buildx <https://docs.docker.com/engine/reference/commandline/buildx/>`_\n\nTo use any of the aforementioned backends, they must be installed on your system. Refer to their documentation for installation and setup.\n.. note::\nBy default, BentoML will use Docker as the container backend. \n   To use other container engines, please set the environment variable `BENTOML_CONTAINERIZE_BACKEND` or\n   pass in `--backend` to :ref:`bentoml containerize <reference/cli:containerize>`:\n.. code-block:: bash\n\n\n```  # set environment variable\n  BENTOML_CONTAINERIZE_BACKEND=buildah bentoml containerize pytorch-mnist\n\n  # or pass in --backend\n  bentoml containerize pytorch-mnist:latest --backend buildah\n```\n\n\nTo build a BentoContainer in Python, you can use the :ref:`Container SDK <reference/container:Container APIs>` method :meth:`bentoml.container.build`:\n.. code-block:: python\nimport bentoml\nbentoml.container.build(\n      \"pytorch-mnist:latest\",\n      backend=\"podman\",\n      features=[\"grpc\",\"grpc-reflection\"],\n      cache_from=\"registry.com/my_cache:v1\",\n   )\nRegister custom backend\n^^^^^^^^^^^^^^^^^^^^^^^\nTo register a new backend, there are two functions that need to be implemented:\n\n`arg_parser_func`: a function that takes in keyword arguments that represents the builder\n  commandline arguments and returns a `list[str]`:\n\n.. code-block:: python\n\n\n``` def arg_parser_func(\n     *,\n     context_path: str = \".\",\n     cache_from: Optional[str] = None,\n     **kwargs,\n ) -> list[str]:\n     if cache_from:\n         args.extend([\"--cache-from\", cache_from])\n     args.append(context_path)\n     return args\n```\n\n\n\n`health_func`: a function that returns a `bool` to indicate if the backend is available:\n\n.. code-block:: python\n\n\n``` import shutil\n\n def health_func() -> bool:\n     return shutil.which(\"limactl\") is not None\n```\n\n\nTo register a new backend, use :meth:`bentoml.container.register_backend`:\n.. code-block:: python\nfrom bentoml.container import register_backend\nregister_backend(\n      \"lima\",\n      binary=\"/usr/bin/limactl\",\n      buildkit_support=True,\n      health=health_func,\n      construct_build_args=arg_parser_func,\n      env={\"DOCKER_BUILDKIT\": \"1\"},\n   )\n.. dropdown:: Backward compatibility with `bentoml.bentos.containerize`\n   :class-title: sd-text-primary\nBefore 1.0.11, BentoML uses :meth:`bentoml.bentos.containerize` to containerize Bento. This method is now deprecated and will be removed in the future.\nBuildKit interop\n^^^^^^^^^^^^^^^^\nBentoML leverages `BuildKit <https://github.com/moby/buildkit>`_ for a more extensive feature set. However, we recognise that\nBuildKit has come with a lot of friction for migration purposes as well as restrictions to use with other build tools (such as podman, buildah, kaniko).\nTherefore, since BentoML version 1.0.11, BuildKit will be an opt-out. To disable BuildKit, pass `DOCKER_BUILDKIT=0` to\n:ref:`bentoml containerize <reference/cli:containerize>`, which aligns with the behaviour of `docker build`:\n.. code-block:: bash\n\n\n```$ DOCKER_BUILDKIT=0 bentoml containerize ...\n```\n\n\n.. note::\n\n\n```All Bento container will now be following OCI spec instead of Docker spec. The difference is that in OCI spec, there is no SHELL argument.\n```\n\n\n.. note::\nThe generated Dockerfile included inside the Bento will be a minimal Dockerfile, which ensures compatibility among build tools. We encourage users to always use\n   :ref:`bentoml containerize <reference/cli:containerize>`.\nIf you wish to use the generated Dockerfile, make sure that you know what you are doing!\nCLI enhancement\n^^^^^^^^^^^^^^^\nTo better support different backends, :ref:`bentoml containerize <reference/cli:containerize>`\nwill be more agnostic when it comes to parsing options.\nOne can pass in options for specific backend with `--opt`:\n.. code-block:: bash\n$ bentoml containerize pytorch-mnist:latest --backend buildx --opt platform=linux/arm64\n`--opt` also accepts parsing `:`\n.. code-block:: bash\n$ bentoml containerize pytorch-mnist:latest --backend buildx --opt platform:linux/arm64\n.. note::\nIf you are seeing a warning message like:\n.. code-block:: prolog\n\n\n```   '--platform=linux/arm64' is now deprecated, use the equivalent '--opt platform=linux/arm64' instead.\n```\n\n\nBentoML used to depends on Docker buildx. These options are now backward compatible with `--opt`. You can safely ignore this warning and use\n   `--opt` to pass options for `--backend=buildx`.\n\n.. rubric:: Notes\n.. [#pr_3164] Introduction of container builder to build Bento into OCI-compliant image: :github:`bentoml/BentoML/pull/3164`\n.. _conda_docker: https://github.com/ContinuumIO/docker-images/blob/master/miniconda3/debian/Dockerfile",
    "tag": "bentoml"
  },
  {
    "title": "tracing.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/guides/tracing.rst",
    "content": "=======\nTracing\n=======\ntime expected: 8 minutes\nThis guide dives into the :wiki:`tracing <Tracing_(software)>` capabilities that BentoML offers.\nBentoML allows user to export trace with `Zipkin <https://zipkin.io/>`,\n`Jaeger <https://www.jaegertracing.io/>` and `OTLP <https://opentelemetry.io/>`_.\nThis guide will also provide a simple example of how to use BentoML tracing with `Jaeger <https://www.jaegertracing.io/>`_\nWhy do you need this?\nDebugging models and services in production is hard. Adding logs and identifying\nthe root cause of the problem is time consuming and error prone. Additionally, tracking\nlogs across multiple services is difficult, which takes a lot of time, and slow down\nyour development agility. As a result, logs won\u2019t always provide the required information to solve regressions.\nTracing encompasses a much wider, continuous view of an application. The goal of tracing is to following a program\u2019s flow and data progression.\nAs such, there is a lot more information at play; tracing can be a lot noisier than logging \u2013 and that\u2019s intentional.\nBentoML comes with built-in tracing support, with :ref:`OpenTelemetry <guides/logging:OpenTelemetry Compatible>`. This means users\ncan then use any of the OpenTelemetry compatible tracing tools to visualize and analyze the traces.\nRunning a BentoService\n:bdg-info:`Requirements:` bentoml must be installed with the extras dependencies for\ntracing exporters. The following command will install BentoML with its coresponding\ntracing exporter:\n.. tab-set::\n.. tab-item:: Jaeger\n\n\n```   .. code-block:: bash\n\n      pip install \"bentoml[tracing-jaeger]\"\n```\n\n\n.. tab-item:: Zipkin\n\n\n```  .. code-block:: bash\n\n     pip install \"bentoml[tracing-zipkin]\"\n```\n\n\n.. tab-item:: OpenTelemetry Protocol\n\n\n```  .. code-block:: bash\n\n     pip install \"bentoml[tracing-otlp]\"\n```\n\n\nWe will be using the example from :ref:`the quickstart <tutorial:Tutorial: Intro to BentoML>`.\nRun the Jaeger `all-in-one <https://www.jaegertracing.io/docs/1.38/getting-started/#all-in-one>`_ docker image:\n.. code-block:: bash\n\u00bb docker run -d --name jaeger \\\n      -e COLLECTOR_ZIPKIN_HOST_PORT=:9411 \\\n      -e COLLECTOR_OTLP_ENABLED=true \\\n      -p 6831:6831/udp \\\n      -p 6832:6832/udp \\\n      -p 5778:5778 \\\n      -p 16686:16686 \\\n      -p 4317:4317 \\\n      -p 4318:4318 \\\n      -p 14250:14250 \\\n      -p 14268:14268 \\\n      -p 14269:14269 \\\n      -p 9411:9411 \\\n      jaegertracing/all-in-one:1.38\n.. dropdown:: For our Mac users\n   :icon: cpu\nIf you are running into this error:\n.. parsed-literal::\n\n\n```  2022-10-05T01:32:21-0700 [WARNING] [api_server:iris_classifier:8] Data exceeds the max UDP packet size; size 216659, max 65000\n  2022-10-05T01:32:24-0700 [ERROR] [api_server:iris_classifier:3] Exception while exporting Span batch.\n  Traceback (most recent call last):\n    File \"~/venv/lib/python3.10/site-packages/opentelemetry/sdk/trace/export/__init__.py\", line 367, in _export_batch\n      self.span_exporter.export(self.spans_list[:idx])  # exporter_type: ignore\n    File \"~/venv/lib/python3.10/site-packages/opentelemetry/exporter/jaeger/thrift/__init__.py\", line 219, in export\n      self._agent_client.emit(batch)\n    File \"~/venv/lib/python3.10/site-packages/opentelemetry/exporter/jaeger/thrift/send.py\", line 95, in emit\n      udp_socket.sendto(buff, self.address)\n  OSError: [Errno 40] Message too long\n```\n\n\nThis is because the default UDP packet size on Mac is set 9216 bytes, which is described `under Jaeger reporters <https://www.jaegertracing.io/docs/1.19/client-libraries/#emsgsize-and-udp-buffer-limits>`_. To increase the UDP packet size, run the following command:\n.. code-block:: bash\n\n\n```  % sysctl net.inet.udp.maxdgram\n  # net.inet.udp.maxdgram: 9216\n  % sudo sysctl net.inet.udp.maxdgram=65536\n  # net.inet.udp.maxdgram: 9216 -> 65536\n  % sudo sysctl net.inet.udp.maxdgram\n  # net.inet.udp.maxdgram: 65536\n```\n\n\nTo configure Jaeger exporter, user can provide a config :wiki:`YAML` file specifying the tracer type and tracing server information under `api_server.tracing`:\n.. literalinclude: ./snippets/tracing/bentoml_configuration.yaml\n   :language: yaml\n   :caption: `bentoml_configuration.yaml`\nProvide this configuration via environment variable `BENTOML_CONFIG` to `bentoml serve`:\n.. code-block:: bash\n\u00bb BENTOML_CONFIG=bentoml_configuration.yaml bentoml serve iris_classifier:latest --production\nSend any request to the BentoService, and then you can visit the `Jaeger UI <http://localhost:16686>`_ to see the traces.\n.. image:: /_static/img/jaeger-ui.png\n   :alt: Jaeger UI\nTracing your containerized BentoService\nIf you are running your BentoService within a container, you can use the following `docker-compose` configuration to run Jaeger and your BentoService together:\n.. literalinclude:: ./snippets/tracing/docker-compose.yml\n   :language: yaml\n   :caption: `docker-compose.yml`\nStart the services with `docker-compose -f ./docker-compose.yml up`\nTo shutdown the services, run `docker-compose -f ./docker-compose.yml down`\nExporter Configuration\n.. note::\nBentoML implements OpenTelemetry APIs, which means OpenTelemetry environment variables\n   will take precedence over the configuration file.\nFor example, if you have the following configuration in your config file:\n.. code-block:: yaml\n\n\n```  api_server:\n    tracing:\n      exporter_type: jaeger\n      sample_rate: 1.0\n      jaeger:\n        protocol: thrift\n        thrift:\n          agent_host_name: localhost\n```\n\n\nThen environment variable `OTEL_EXPORTER_JAEGER_AGENT_HOST` will take precedence over the\n   `agent_host_name` setting in the config file.\nThe following section describes the configuration options for each tracing exporter.\nBy default, no traces will be collected. Set `sample_rate` to your desired fraction in order to start collecting them:\n.. code-block:: yaml\n\n\n```api_server:\n  tracing:\n    exporter_type: zipkin\n    sample_rate: 1.0\n```\n\n\nIf you would like to exclude some routes from tracing, you can specify them using\nthe :code:`excluded_urls` parameter. This parameter can be either a comma-separated \nstring of routes, or a list of strings.\n.. code-block:: yaml\n\n\n```tracing:\n  exporter_type: jaeger\n  sample_rate: 1.0\n  jaeger:\n    address: localhost\n    port: 6831\n  excluded_urls: readyz,livez,healthz,static_content,docs,metrics\n```\n\n\nTo set a timeout for the exporter, where it will wait for each batch export, use the `timeout` parameter:\n.. code-block:: yaml\n\n\n```tracing:\n  exporter_type: jaeger\n  sample_rate: 1.0\n  timeout: 5 [#default_timeout]_\n```\n\n\nTo set a maximum length string attribute values can have, use the `max_tag_value_length` parameter:\n.. code-block:: yaml\n\n\n```tracing:\n  exporter_type: jaeger\n  sample_rate: 1.0\n  max_tag_value_length: 256\n```\n\n\n.. note::\nAll of the above value are shared values among the exporters. This means it will be\n   applied to corresponding exporter that is set via `exporter_type`.\nZipkin\n^^^^^^\nWhen using Zipkin, BentoML only supports its V2 protocol. If you are reporting to\nthe an OpenZipkin server directly, make sure to add the URL path `/api/v2/spans`\nto the server address.\nConfiguration fields are passed through the OpenTelemetry Zipkin exporter\n[#otlp_zipkin_exporter_docs]_.\n.. code-block:: yaml\n\n\n```tracing:\n  exporter_type: zipkin\n  sample_rate: 1.0\n  zipkin:\n    endpoint: http://localhost:9411/api/v2/spans\n    local_node_ipv4: \"192.168.0.1\"\n    local_node_ipv6: \"2001:db8::c001\"\n    local_node_port: 31313\n```\n\n\nJaeger\n^^^^^^\nThe Jaeger exporter supports sending trace over both the Thrift and gRPC protocol. By default, BentoML \nwill use the Thrift protocol.\n.. note::\nWhen it is not feasible to deploy Jaeger Agent next to the application, for example, when the\n   application code is running as Lambda function, a collector can be configured to send spans\n   using Thrift over HTTP. If both agent and collector are configured, the exporter sends traces\n   only to the collector to eliminate the duplicate entries. [#otlp_jaeger_exporter_docs]_.\nTo setup the collector endpoint that will be used to receive either Thrift or Protobuf\nover HTTP/gRPC, use the `collector_endpoint` parameter:\n.. tab-set::\n.. tab-item:: Thrift over HTTP\n      :sync: http\n\n\n```  .. code-block:: yaml\n\n      tracing:\n        exporter_type: jaeger\n        sample_rate: 1.0\n        jaeger:\n          collector_endpoint: http://localhost:14268/api/traces?format=jaeger.thrift\n```\n\n\n.. tab-item:: Protobuf over gRPC\n      :sync: grpc\n\n\n```  .. code-block:: yaml\n\n      tracing:\n        exporter_type: jaeger\n        sample_rate: 1.0\n        jaeger:\n          collector_endpoint: http://localhost:14250\n```\n\n\nConfiguration fields are passed through the OpenTelemetry Zipkin exporter\n[#jaeger_source]_.\n.. tab-set::\n.. tab-item:: Thrift\n      :sync: http\n\n\n```  .. code-block:: yaml\n\n      tracing:\n        exporter_type: jaeger\n        sample_rate: 1.0\n        jaeger:\n          protocol: thrift\n          thrift:\n            agent_host_name: localhost\n            agent_port: 6831\n            udp_split_oversized_batches: true\n\n  .. note::\n\n     if ``udp_split_oversized_batches`` [#default_udp_split_oversized_batches]_ is\n     True, the oversized batch will be split into smaller batch over the UDP max\n     packets size (default: `65000`) if given buffer is larger than max\n     packet size:\n\n     .. math::\n\n        \\mathrm{packets}\\triangleq \\left\\lceil{\\frac{\\text{len}\\left(\\text{buff}\\right)}{\\text{max_packet_size}}}\\right\\rceil\n```\n\n\n.. tab-item:: gRPC\n      :sync: grpc\n\n\n```  .. code-block:: yaml\n\n      tracing:\n        exporter_type: jaeger\n        sample_rate: 1.0\n        jaeger:\n          protocol: grpc\n          grpc:\n            endpoint: http://localhost:14250\n            insecure: true  # Whether or not collector has encryption or authentication.\n```\n\n\nOTLP Exporter\n^^^^^^^^^^^^^\nBentoML supports OTLP exporter for easy integration with an OpenTelemetry Traces receiver.\nOTLP provides both a gRPC and HTTP protocol that uses Protobuf to send traces.\nYou may use either HTTP or gRPC as protocol. By default, gRPC is the default protocol.\n.. note::\nYou may also use HTTP protocol as it provides an easier way to configure proxy and\n   load balancer.\nTo change the protocol, use the `protocol` parameter:\n.. code-block:: yaml\napi_server:\n     tracing:\n       exporter_type: otlp\n       sample_rate: 1.0\n       otlp:\n         protocol: http\nConfiguration fields are passed through the OpenTelemetry Zipkin exporter\n[#otlp_source]_.\n.. tab-set::\n.. tab-item:: HTTP\n      :sync: http\n\n\n```  .. note::\n\n     Make sure to set ``endpoint`` to have traces export path ``/v1/traces`` appended.\n\n  .. code-block:: yaml\n\n      tracing:\n        exporter_type: otlp\n        sample_rate: 1.0\n        otlp:\n          protocol: http\n          endpoint: http://localhost:4318/v1/traces\n          http:\n            certificate_file: /path/to/cert.pem\n            headers:\n              Keep-Alive: timeout=5, max=1000\n```\n\n\n.. tab-item:: gRPC\n      :sync: grpc\n\n\n```  .. code-block:: yaml\n\n      tracing:\n        exporter_type: otlp\n        sample_rate: 1.0\n        otlp:\n          protocol: grpc\n          endpoint: http://localhost:4317\n          grpc:\n            insecure: true\n            headers:\n              - [\"grpc-encoding\", \"gzip\"]\n```\n\n\n\n.. rubric:: Notes\n.. [#otlp_zipkin_exporter_docs]  `OpenTelemetry Zipkin Exporter API docs <https://opentelemetry-python.readthedocs.io/en/latest/exporter/zipkin/zipkin.html#opentelemetry.exporter.zipkin.json.ZipkinExporter>`_\n.. [#otlp_jaeger_exporter_docs]  `OpenTelemetry Jaeger Exporter API docs <https://opentelemetry-python.readthedocs.io/en/latest/exporter/jaeger/jaeger.html#module-opentelemetry.exporter.jaeger>`_\n.. [#jaeger_source]  Jaeger exporter source code for :github:`Thrift <open-telemetry/opentelemetry-python/blob/main/exporter/opentelemetry-exporter-jaeger-thrift/src/opentelemetry/exporter/jaeger/thrift/__init__.py>` and \n   :github:`gRPC <open-telemetry/opentelemetry-python/blob/main/exporter/opentelemetry-exporter-jaeger-proto-grpc/src/opentelemetry/exporter/jaeger/proto/grpc/__init__.py>`.\n.. [#default_timeout] The default timeout is 10 seconds. For most use cases, you don't need to change this value.\n.. [#default_udp_split_oversized_batches] Whether or not to re-emit oversized batches in smaller chunks. By default this is not set.\n.. [#otlp_source] OTLP exporter source code for :github:`HTTP <open-telemetry/opentelemetry-python/blob/main/exporter/opentelemetry-exporter-otlp-proto-http/src/opentelemetry/exporter/otlp/proto/http/trace_exporter/__init__.py>`",
    "tag": "bentoml"
  },
  {
    "title": "metrics.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/guides/metrics.rst",
    "content": "=======\nMetrics\n=======\nMetrics are measurements of statistics about your service, which can provide information about the usage and performance of your bentos in production.\nBentoML allows users to define custom metrics with `Prometheus <https://prometheus.io/docs/introduction/overview/>`_ to easily enable monitoring for their Bentos.\nThis article will dive into the default metrics and how to add custom metrics for\neither a :ref:`concepts/runner:Custom Runner` or :ref:`Service <concepts/service:Service and APIs>`.\nHaving a `Prometheus server <https://prometheus.io/docs/prometheus/latest/getting_started/>`_ available will help visualize the examples in this guide.\n.. note::\nThis article assumes that you have a base understanding of a BentoService. If you\n   are new to BentoML, please start with :ref:`the quickstart tutorial <tutorial:Tutorial: Intro to BentoML>`.\n.. seealso::\nAll `metrics types <https://prometheus.io/docs/concepts/metric_types/>`_ supported by Prometheus are supported in BentoML. See :ref:`reference/metrics:Metrics API` for more information on `bentoml.metrics`.\nDefault Metrics\n~~~~~~~~~~~~~~~\nBentoML automatically collects the following metrics for all API Server and Runners by default across the following dimensions.\n.. list-table::\n   :header-rows: 1\n\n\n\nDescription\nMetric Name\nMetric Type\nDimensions\n\n\n\n\nAPI Server request in progress\n`bentoml_api_server_request_in_progress`\nGauge\n`endpoint`, `service_name`, `service_version`\n\n\n\n\nRunner request in progress\n`bentoml_runner_request_in_progress`\nGauge\n`endpoint`, `runner_name`, `service_name`, `service_version`\n\n\n\n\nAPI Server request total\n`bentoml_api_server_request_total`\nCounter\n`endpoint`, `service_name`, `service_version`, `http_response_code`\n\n\n\n\nRunner request total\n`bentoml_runner_request_total`\nCounter\n`endpoint`, `service_name`, `runner_name`, `service_version`, `http_response_code`\n\n\n\n\nAPI Server request duration in seconds\n`bentoml_api_server_request_duration_seconds_sum`, `bentoml_api_server_request_duration_seconds_count`, `bentoml_api_server_request_duration_seconds_bucket`\nHistogram\n`endpoint`, `service_name`, `service_version`, `http_response_code`\n\n\n\n\nRunner request duration in seconds\n`bentoml_runner_request_duration_seconds_sum`, `bentoml_runner_request_duration_seconds_count`, `bentoml_runner_request_duration_seconds_bucket`\nHistogram\n`endpoint`, `service_name`, `runner_name`, `service_version`, `http_response_code`\n\n\n\n\nRunner adaptive batch size\n`bentoml_runner_adaptive_batch_size_sum`, `bentoml_runner_adaptive_batch_size_count`, `bentoml_runner_adaptive_batch_size_bucket`\nHistogram\n`method_name`, `service_name`, `runner_name`, `worker_index`\n\n\n\nRequest In-Progress\n^^^^^^^^^^^^^^^^^^^\nMeasures the number of requests currently being processed by the API Server or Runner.\nRequest Total\n^^^^^^^^^^^^^\nMeasures the total number of requests processed by the API Server or Runner. The following PromQL expression returns the average request count\nper-second over the last 1 minute for the `/classify` endpoint on the `iris_classifier` service.\n.. code-block:: text\nrate(bentoml_api_server_request_total{service_name=\"iris_classifier\", endpoint=\"/classify\"}[1m])\nRequest Duration\n^^^^^^^^^^^^^^^^\nMeasures the durations of requests processed by the API Server or Runner. The accuracy of the histogram depends on the range and\ngranularity of the histogram buckets. By default, the Prometheus buckets covering the range from 0.005s to 10s are used. The following\nconfiguration can be used to update the buckets configuration for the request duration metric. The configuration keys `min` and `max` indicates\nthe expected range of request duration to be tracked. The configuration key `factor` controls the granularity of the buckets and is used as\nthe exponential factor to generate the buckets. For example, the configuration below will generate the following buckets\n`(0.1, 0.2, 0.4, 0.8, 1.6, 3.2, 5.0, inf)`. See the :ref:`configuration <guides/configuration:Configuration>` guide for more information on\nhow to configure BentoML.\n.. code-block:: yaml\n   :caption: \u2699\ufe0f `configuration.yml`\napi_server:\n     metrics:\n       duration:\n         min: 0.1\n         max: 5.0\n         factor: 2.0\nThe following PromQL expression returns the 99th percentile of the request duration over the last 1 minute for the `/classify` endpoint\non the `iris_classifier` service.\n.. code-block:: text\nhistogram_quantile(0.99, rate(bentoml_api_server_request_duration_seconds_bucket{service_name=\"iris_classifier\", endpoint=\"/classify\"}[1m]))\nAdaptive Batch Size\n^^^^^^^^^^^^^^^^^^^\nMeasures the batch size used by the :ref:`adaptive batching <guides/batching:Adaptive Batching>` feature in the :ref:`runner <concepts/runner:Using Runners>`.\nThe following PromQL expression returns the 75th percentile of the batch size over the last 1 minute for the `iris_classifier` service.\n.. code-block:: text\nhistogram_quantile(0.75, rate(bentoml_runner_adaptive_batch_size_bucket{service_name=\"iris_classifier\"}[1m]))\nCustom Metrics\n~~~~~~~~~~~~~~\nWe will build a custom histogram to track the latency of our :ref:`pretrained NLTK runner <concepts/runner:Custom Runner>`, a custom\ncounter to measure the total amount of time our endpoint is invoked.\n.. note::\nThe source code for this custom runner is :github:`available on GitHub <bentoml/BentoML/tree/main/examples/custom_runner/nltk_pretrained_model>`.\nInitialize our metrics as follow:\n.. literalinclude:: ./snippets/metrics/metric_defs.py\n   :language: python\n   :caption: `service.py`\n`inference_duration` is a :meth:`bentoml.metrics.Histogram`, which tracks how long it\ntakes for our model to run inference.\nThe :attr:`bentoml.metrics.Histogram.buckets` argument is used to determine the granularity of histogram tracking. The range of the buckets should cover the range of values the histogram is expected track. Number of buckets is positively correlated to the the granularity of tracking. The last value of the bucket should always be the positive infinity. See Prometheus documentation on `Histogram <https://prometheus.io/docs/practices/histograms/>`_ for more details.\n`polarity_counter` is a :meth:`bentoml.metrics.Counter`, which tracks the total number\nof analysis by the polarity scores.\n.. epigraph::\n:bdg-info:`Note:` This also applies to any other metric type, including :meth:`bentoml.metrics.Gauge` and :meth:`bentoml.metrics.Summary`.\nCreate our NLTK custom runner:\n.. literalinclude:: ./snippets/metrics/runner_impl.py\n   :language: python\n   :caption: `service.py`\nThis runnable implementation creates a custom NLTK runner, that use the `inference_duration`\nhistogram to track the latency of polarity scores from a given sentence.\nInitialize our NLTK runner, and add it to the service:\n.. code-block:: python\nnltk_runner = t.cast(\n      \"RunnerImpl\", bentoml.Runner(NLTKSentimentAnalysisRunnable, name=\"nltk_sentiment\")\n   )\nsvc = bentoml.Service(\"sentiment_analyzer\", runners=[nltk_runner])\n@svc.api(input=bentoml.io.Text(), output=bentoml.io.JSON())\n   async def analysis(input_text: str) -> dict[str, bool]:\n       is_positive = await nltk_runner.is_positive.async_run(input_text)\n       polarity_counter.labels(polarity=is_positive).inc()\n       return {\"is_positive\": is_positive}\nOur endpoint `analysis` uses the `polarity_counter` to track the total number of\ninvocation for `analysis` by polarity scores.\n.. tab-set::\n\n\n```.. tab-item:: HTTP\n   :sync: http\n\n   Serve our service:\n\n   .. code-block:: bash\n\n      \u00bb bentoml serve-http --production\n\n   Use the following ``prometheus.yml`` config:\n\n   .. literalinclude:: ../../../examples/custom_runner/nltk_pretrained_model/prometheus/prometheus.http.yml\n      :language: python\n      :caption: `prometheus.yml`\n\n   Startup your Prometheus server in a different terminal session:\n\n   .. code-block:: bash\n\n      \u00bb prometheus --config.file=prometheus.yml\n\n   In a different terminal, send a request to our service:\n\n   .. code-block:: bash\n\n      \u00bb curl -X POST -F \"image=@test_image.png\" \\\n               http://0.0.0.0:3000/predict\n\n.. tab-item:: gRPC\n   :sync: grpc\n\n   Serve our service:\n\n   .. code-block:: bash\n\n      \u00bb bentoml serve-grpc --production --enable-reflection\n\n   Use the following ``prometheus.yml`` config:\n\n   .. literalinclude:: ../../../examples/custom_runner/nltk_pretrained_model/prometheus/prometheus.grpc.yml\n      :language: python\n      :caption: `prometheus.yml`\n\n   Startup your Prometheus server in a different terminal session:\n\n   .. code-block:: bash\n\n      \u00bb prometheus --config.file=prometheus.yml\n\n   In a different terminal, send a request to our service:\n\n   .. code-block:: bash\n\n      \u00bb grpcurl -d @ -plaintext 0.0.0.0:3000 bentoml.grpc.v1.BentoService/Call <<EOT\n        {\n          \"apiName\": \"predict\",\n          \"serializedBytes\": \"...\"\n        }\n        EOT\n```\n\n\nVisit `http://localhost:9090/graph <http://localhost:9090/graph>`_ and use the following query for 95th percentile inference latency:\n.. code-block:: text\nhistogram_quantile(0.95, rate(inference_duration_bucket[1m]))\n.. image:: ../_static/img/prometheus-metrics.png\n.. TODO::\n\n\n```* Grafana dashboard\n```\n\n\n.. admonition:: Help us improve the project!\n\n\n```Found an issue or a TODO item? You're always welcome to make contributions to the\nproject and its documentation. Check out the\n`BentoML development guide <https://github.com/bentoml/BentoML/blob/main/DEVELOPMENT.md>`_\nand `documentation guide <https://github.com/bentoml/BentoML/blob/main/docs/README.md>`_\nto get started.\n```\n\n\n\n.. rubric:: Notes",
    "tag": "bentoml"
  },
  {
    "title": "graph.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/guides/graph.rst",
    "content": "===============\nInference Graph\n===============\nMany ML problems require an ensemble of models to work together to solve. BentoML architecture can support any model inference graph natively in its\n:ref:`Service APIs <concepts/service:Service and APIs>` definition. Users can define parallel and sequential inference graphs with any control flows\nby writing simple Python code. In this guide, we will build a text generation and classification service using model inference graph. The project\nsource code can be found in the BentoML `inference graph <https://github.com/bentoml/BentoML/tree/main/examples/inference_graph>`_ example.\n.. image:: ../_static/img/inference-graph-diagram.png\nAs illustrated in the diagram above, the service performs the following tasks.\n\nAccepts a text input.\nPasses the input to three text generation models in parallel and receives a list of three generated texts.\nPasses the list of generated texts to a text classification model iteratively and receives a list of three classification results.\nReturns the list of generated texts and classification results in a JSON output.\n\n.. code-block:: json\n    :caption: Sample JSON Output\n\n\n```[\n    {\n        \"generated\": \"I have an idea! Please share with, like and subscribe and leave a comment below!\\n\\nIf you like this post, please consider becoming a patron of Reddit or becoming a patron of the author.\",\n        \"score\": 0.5149753093719482\n    },\n    {\n        \"generated\": \"I have an idea! One that won't break your heart but will leave you gasping in awe. A book about the history of magic. And because what's better than magic? Some. It's a celebration of our ancient, universal gift of awe.\\\"\\n\\nThe result was the \\\"Vox Populi: A Memoir of the Ancient World\\\" by E.V. Okello (Ace Books), published in 1999.\\n\\nIn the past 20 years, Okello, professor of history at Ohio State University and author of such titles as \\\"The American Imagination\\\" and \\\"Walking With Elephants\",\n        \"score\": 0.502700924873352\n    },\n    {\n        \"generated\": \"I have an idea! I've been wondering what the name of this thing is. What's the point?\\\" - The Simpsons\\n\\n\\n\\\"It's bigger, bigger than she needs!\\\" - SpongeBob SquarePants\\n\\n\\n\\\"That's a funny thing. It's like my brain is the most gigantic living thing. I just like thinking big.\\\" - Simpsons\\n\\n\\n\\\"Ooookay! Here comes Barty-Icarus himself! (pause)\\\" - A Christmas Tale\\n\\n\\nBackground information Edit\\n\\nFormal name: Homer's Brain.\\n\\nHomer's Brain. Special name: Brain.\\n\\nAppearances Edit\",\n        \"score\": 0.536346971988678\n    }\n]\n```\n\n\nDeclare Runners\n\nCreate :ref:`Runners <concepts/runner:Using Runners>` for the three text generation models and the one text classification model using the `to_runner` function.\n.. code-block:: python\n\n\n```gpt2_generator = bentoml.transformers.get(\"gpt2-generation:latest\").to_runner()\ndistilgpt2_generator = bentoml.transformers.get(\"distilgpt2-generation:latest\").to_runner()\ndistilbegpt2_medium_generator = bentoml.transformers.get(\"gpt2-medium-generation:latest\").to_runner()\nbert_base_uncased_classifier = bentoml.transformers.get(\"bert-base-uncased-classification:latest\").to_runner()\n```\n\n\nCreate Service\n\nCreate a :ref:`Service <concept/service:Service and APIs>` named `inference_graph` and specify the runners created earlier in the `runners` argument.\n.. code-block:: python\n\n\n```svc = bentoml.Service(\n    \"inference_graph\",\n    runners=[\n        gpt2_generator,\n        distilgpt2_generator,\n        distilbegpt2_medium_generator,\n        bert_base_uncased_classifier,\n    ],\n)\n```\n\n\nDefine API\n\nFirst, define an async :ref:`API <concepts/service:Service and APIs>` named `classify_generated_texts` that accepts a :ref:`Text <reference/api_io_descriptors:Texts>`\ninput and returns :ref:`JSON <reference/api_io_descriptors:Structured Data with JSON>` output. Second, pass the input simultaneously to all three text generation\nrunners through `asyncio.gather` and receive a list of three generated texts. Using `asyncio.gather` and Runner's `async_run` allows the inferences to happen\nin parallel. Third, pass the list of generated texts to the text classification runner iteratively using a loop to get the classification score of each generated text.\nFinally, return the list of generated texts and classification results in a dictionary.\n.. tip::\n\n\n```Using asynchronous Service and Runner APIs achives better performance and throughput for IO-intensive workloads.\nSee :ref:`Sync vs Async APIs <concepts/service:Sync vs Async APIs>` for more details.\n```\n\n\n.. code-block:: python\n\n\n```@svc.api(input=Text(), output=JSON())\nasync def classify_generated_texts(original_sentence: str) -> dict:\n    generated_sentences = [\n        result[0][\"generated_text\"]\n        for result in await asyncio.gather(\n            gpt2_generator.async_run(\n                original_sentence,\n                max_length=MAX_LENGTH,\n                num_return_sequences=NUM_RETURN_SEQUENCE,\n            ),\n            distilgpt2_generator.async_run(\n                original_sentence,\n                max_length=MAX_LENGTH,\n                num_return_sequences=NUM_RETURN_SEQUENCE,\n            ),\n            distilbegpt2_medium_generator.async_run(\n                original_sentence,\n                max_length=MAX_LENGTH,\n                num_return_sequences=NUM_RETURN_SEQUENCE,\n            ),\n        )\n    ]\n\n    results = []\n    for sentence in generated_sentences:\n        score = (await bert_base_uncased_classifier.async_run(sentence))[0][\"score\"]\n        results.append(\n            {\n                \"generated\": sentence,\n                \"score\": score,\n            }\n        )\n\n    return results\n```\n\n\nInference Graph Trace\n\nThe following tracing waterfall graphs demonstrates the execution flow of the inference graph. Note that the three calls to the text generation\nrunners happen in parallel without blocking each other and the calls to the text classification runner happen sequentially.",
    "tag": "bentoml"
  },
  {
    "title": "migration.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/guides/migration.rst",
    "content": "===================\n1.0 Migration Guide\n===================\nBentoML version 1.0.0 APIs are backward incompatible with version 0.13.1. However, most of the common\nfunctionality can be achieved with the new version. We will guide and demonstrate the migration by\ntransforming the `quickstart <https://github.com/bentoml/BentoML/tree/main/examples/quickstart>`_ gallery project\nfrom BentoML version 0.13.1 to 1.0.0. Complete every migration action denoted like the section below.\n.. admonition:: \ud83d\udca1 Migration Task\nInstall BentoML version 1.0.0 by running the following command.\n.. code-block:: bash\n\n\n```> pip install bentoml\n```\n\n\nTrain Models\nFirst, the quickstart project begins by training a classifier Scikit-Learn model from the iris datasets.\nBy running :code:`python train.py`, we obtain a trained classifier model.\n.. code-block:: python\n    :caption: train.py\n\n\n```from sklearn import svm\nfrom sklearn import datasets\n\n# Load training data\niris = datasets.load_iris()\nX, y = iris.data, iris.target\n\n# Model Training\nclf = svm.SVC(gamma='scale')\nclf.fit(X, y)\n```\n\n\nBentoML version 1.0.0 introduces the model store concept to help improve model management during development.\nOnce we are happy with the model trained, we can save the  model instance with the :code:`save_model()`\nframework API to persist it in the model store. Optionally, you may attach custom labels, metadata, or custom\nobjects like tokenizers to be saved alongside the model. See\n:ref:`Save A Trained Model <concepts/model:Save A Trained Model>` to learn more.\n.. admonition:: \ud83d\udca1 Migration Task\nAppend the model saving logic below to `train.py` and run `python train.py`.\n.. code-block:: python\n\n\n```bentoml.sklearn.save_model(\"iris_clf\", clf)\nprint(f\"Model saved: {saved_model}\")\n```\n\n\nYou can view and manage all saved models via the :code:`bentoml models` CLI command.\n.. code-block:: bash\n\n\n```> bentoml models list\n\nTag                        Module           Size        Creation Time        Path\niris_clf:zy3dfgxzqkjrlgxi  bentoml.sklearn  5.81 KiB    2022-05-19 08:36:52  ~/bentoml/models/iris_clf/zy3dfgxzqkjrlgxi\n```\n\n\nDefine Services\nNext, we will transform the service definition module and breakdown each section into details.\n.. admonition:: \ud83d\udca1 Migration Task\nUpdate the service definition module `service.py` from the BentoML 0.13.1 specification to 1.0.0 specification.\n.. tab-set::\n\n\n```.. tab-item:: 0.13.1\n\n    .. code-block:: python\n        :caption: service.py\n\n        import pandas as pd\n\n        from bentoml import env, artifacts, api, BentoService\n        from bentoml.adapters import DataframeInput\n        from bentoml.frameworks.sklearn import SklearnModelArtifact\n\n        @env(infer_pip_packages=True)\n        @artifacts([SklearnModelArtifact('model')])\n        class IrisClassifier(BentoService):\n            @api(input=DataframeInput(), batch=True)\n            def predict(self, df: pd.DataFrame):\n                return self.artifacts.model.predict(df)\n\n.. tab-item:: 1.0.0\n\n    .. code-block:: python\n        :caption: service.py\n\n        import numpy as np\n        import pandas as pd\n\n        import bentoml\n        from bentoml.io import NumpyNdarray, PandasDataFrame\n\n        iris_clf_runner = bentoml.sklearn.get(\"iris_clf:latest\").to_runner()\n\n        svc = bentoml.Service(\"iris_classifier\", runners=[iris_clf_runner])\n\n        @svc.api(input=PandasDataFrame(), output=NumpyNdarray())\n        def predict(input_series: pd.DataFrame) -> np.ndarray:\n            result = iris_clf_runner.predict.run(input_series)\n            return result\n```\n\n\nEnvironment\n~~~~~~~~~~~\nBentoML version 0.13.1 relies on the :code:`@env`\n`decorator API <https://docs.bentoml.org/en/0.13-lts/concepts.html#defining-service-environment>`_ for defining the\nenvironment settings and dependencies of the service. Typical arguments of the environment decorator includes Python\ndependencies (e.g. :code:`pip_packages`, :code:`pip_index_url`), Conda dependencies (e.g. :code:`conda_channels`,\n:code:`conda_dependencies`), and Docker options (e.g. :code:`setup_sh`, :code:`docker_base_image`).\n.. code-block:: python\n\n\n```@env(pip_packages=[\"scikit-learn\", \"pandas\"])\n```\n\n\nBentoML version 1.0.0 no longer relies on the environment decorator. Environment settings and service dependencies are\ndefined in the :code:`bentofile.yaml` file in the project directory. The contents are used to specify the\n:code:`bentoml build` opations when :ref:`building bentos <concepts/bento:Bento Build Options>`.\n.. admonition:: \ud83d\udca1 Migration Task\nSave the contents below to the `bentofile.yaml` file in the same directory as `service.py`.\n.. code-block:: yaml\n\n\n```service: \"service.py:svc\"\nlabels:\n    owner: bentoml-team\n    project: gallery\ninclude:\n    - \"*.py\"\npython:\n    packages:\n    - scikit-learn\n    - pandas\n```\n\n\nArtifacts\n~~~~~~~~~\nBentoML version 0.13.1 provides the :code:`@artifacts`\n`decorator API <https://docs.bentoml.org/en/0.13-lts/concepts.html#packaging-model-artifacts>`_ for users to specify\nthe trained models required by a BentoService. The specified artifacts are automatically serialized and deserialized\nwhen saving and loading a BentoService.\n.. code-block:: python\n\n\n```@artifacts([SklearnModelArtifact('model')])\n```\n\n\nBentoML 1.0.0 leverages a combination of :ref:`model store <concepts/model:Managing Models>` and\n:ref:`runners <concepts/runner:What is Runner?>` APIs for specifying the required models at runtime. Methods on the\nmodel can be invoked by calling the run function on the runner. Runner represents a unit of computation that can be\nexecuted on a remote Python worker and scales independently.\n.. code-block:: python\n\n\n```iris_clf_runner = bentoml.sklearn.get(\"iris_clf:latest\").to_runner()\n```\n\n\nAPI\n~~~\nBentoML version 0.13.1 defines the inference API through the :code:`@api`\n`decorator <https://docs.bentoml.org/en/0.13-lts/concepts.html#api-function-and-adapters>`_.\nInput and output types can be specified through the adapters. The service will convert the inference request from\nHTTP to the desired format specified by the input adaptor, in this case, a :code:`pandas.DataFrame` object.\n.. code-block:: python\n\n\n```@api(input=DataframeInput(), batch=True)\ndef predict(self, df: pd.DataFrame):\n    return self.artifacts.model.predict(df)\n```\n\n\nBentoML version 1.0.0 also provides a similar :code:`@svc.api` :ref:`decorator <concepts/service:Service APIs>`.\nThe inference API is no longer defined within the service class. The association with the service is declared with the\n:code:`@svc.api` decorator from the :code:`bentoml.Service` class. Input and output specifications are defined by IO\ndescriptor arguments passed to the :code:`@src.api` decorator. Similar to the adaptors, they help describe the expected\ndata types, validate that the input and output conform to the expected format and schema, and convert them from and to\nthe specified native types. In addition, multiple input and output can be defined using the tuple syntax,\ne.g. :code:`input=(image=Image(), metadata=JSON())`.\n.. code-block:: python\n\n\n```@svc.api(input=PandasDataFrame(), output=NumpyNdarray())\ndef predict(input_series: pd.DataFrame) -> np.ndarray:\n    result = iris_clf_runner.predict.run(input_series)\n    return result\n```\n\n\nBentoML version 1.0.0 supports defining inference API as an asynchronous coroutine. Asynchronous APIs are preferred if\nthe processing logic is IO-bound or invokes multiple runners simultaneously which is ideal for fetching features and\ncalling remote APIs.\nTest Services\n~~~~~~~~~~~~~\nTo improve development agility, BentoML version 1.0.0 adds the capability to test the service in development before\nsaving. Executing the :code:`bentoml serve` command will bring up an API server for rapid development iterations. The\n:code:`--reload` option allows the development API server to reload upon every change of the service module.\n.. code-block:: bash\n\n\n```> bentoml serve --reload\n```\n\n\nTo bring up the API server and runners in a production like setting, use the :code:`--production` option. In production\nmode, API servers and runners will run in separate processes to maximize server utility and parallelism.\n.. code-block:: bash\n\n\n```> bentoml serve --production\n```\n\n\nBuilding Bentos\nNext, we will build the service into a bento and save it to the bento store. Building a service to bento is to persist\nthe service for distribution. This operation is unique to BentoML version 1.0.0. The comparable operation in version\n0.13.1 is to save a service to disk by calling the :code:`save()` function on the service instance.\n.. admonition:: \ud83d\udca1 Migration Task\nRun :code:`bentoml build` command from the same directory as `service.py` and `bentofile.yaml`.\n.. tab-set::\n\n\n```.. tab-item:: 0.13.1\n\n    .. code-block:: python\n        :caption: packer.py\n\n        # import the IrisClassifier class defined above\n        from bento_service import IrisClassifier\n\n        # Create a iris classifier service instance\n        iris_classifier_service = IrisClassifier()\n\n        # Pack the newly trained model artifact\n        from sklearn import svm\n        from sklearn import datasets\n\n        # Load training data\n        iris = datasets.load_iris()\n        X, y = iris.data, iris.target\n\n        # Model Training\n        clf = svm.SVC(gamma='scale')\n        clf.fit(X, y)\n\n        iris_classifier_service.pack('model', clf)\n\n        # Save the prediction service to disk for model serving\n        saved_path = iris_classifier_service.save()\n\n.. tab-item:: 1.0.0\n\n    .. code-block:: bash\n\n        > bentoml build\n\n        Building BentoML service \"iris_classifier:6otbsmxzq6lwbgxi\" from build context \"/home/user/gallery/quickstart\"\n        Packing model \"iris_clf:zy3dfgxzqkjrlgxi\"\n        Locking PyPI package versions..\n\n        \u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2557\u2591\u2591\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2591\u2588\u2588\u2588\u2588\u2588\u2557\u2591\u2588\u2588\u2588\u2557\u2591\u2591\u2591\u2588\u2588\u2588\u2557\u2588\u2588\u2557\u2591\u2591\u2591\u2591\u2591\n        \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2588\u2588\u2557\u2591\u2588\u2588\u2551\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2557\u2591\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551\u2591\u2591\u2591\u2591\u2591\n        \u2588\u2588\u2588\u2588\u2588\u2588\u2566\u255d\u2588\u2588\u2588\u2588\u2588\u2557\u2591\u2591\u2588\u2588\u2554\u2588\u2588\u2557\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2588\u2588\u2551\u2588\u2588\u2554\u2588\u2588\u2588\u2588\u2554\u2588\u2588\u2551\u2588\u2588\u2551\u2591\u2591\u2591\u2591\u2591\n        \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u255d\u2591\u2591\u2588\u2588\u2551\u255a\u2588\u2588\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2551\u2591\u2591\u2591\u2591\u2591\n        \u2588\u2588\u2588\u2588\u2588\u2588\u2566\u255d\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551\u2591\u255a\u2588\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2591\u255a\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2591\u255a\u2550\u255d\u2591\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\n        \u255a\u2550\u2550\u2550\u2550\u2550\u255d\u2591\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d\u2591\u2591\u255a\u2550\u2550\u255d\u2591\u2591\u2591\u255a\u2550\u255d\u2591\u2591\u2591\u2591\u255a\u2550\u2550\u2550\u2550\u255d\u2591\u255a\u2550\u255d\u2591\u2591\u2591\u2591\u2591\u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\n        Successfully built Bento(tag=\"iris_classifier:6otbsmxzq6lwbgxi\")\n```\n\n\nYou can view and manage all saved models via the :code:`bentoml` CLI command.\n.. code-block:: bash\n\n\n```> bentoml list\n\nTag                               Size        Creation Time        Path\niris_classifier:6otbsmxzq6lwbgxi  16.48 KiB   2022-07-01 16:03:44  ~/bentoml/bentos/iris_classifier/6otbsmxzq6lwbgxi\n```\n\n\nServe Bentos\n~~~~~~~~~~~~\nWe can serve the saved bentos by running the :code:`bentoml serve` command. We can add :code:`--production` to have\nAPI servers and runners will run in separate processes to maximize server utility and parallelism.\n.. code-block:: bash\n\n\n```> bentoml serve iris_classifier:latest --production\n\n2022-07-06T02:02:30-0700 [INFO] [] Starting production BentoServer from \".\" running on http://0.0.0.0:3000 (Press CTRL+C to quit)\n2022-07-06T02:02:31-0700 [INFO] [runner-iris_clf:1] Setting up worker: set CPU thread count to 10\n```\n\n\nGenerate Docker Images\nSimilar to version 0.13.1, we can generate docker images from bentos using the :code:`bentoml containerize` command in BentoML\nversion 1.0.0, see :ref:`Containerize Bentos <concepts/deploy:Containerize Bentos>` to learn more.\n.. code-block:: bash\n\n\n```> bentoml containerize iris_classifier:latest\n\nBuilding docker image for Bento(tag=\"iris_classifier:6otbsmxzq6lwbgxi\")...\nSuccessfully built docker image \"iris_classifier:6otbsmxzq6lwbgxi\"\n```\n\n\nYou can run the docker image to start the service.\n.. code-block:: bash\n\n\n```> docker run -p 3000:3000 iris_classifier:6otbsmxzq6lwbgxi\n\n2022-07-01T21:57:47+0000 [INFO] [] Service loaded from Bento directory: bentoml.Service(tag=\"iris_classifier:6otbsmxzq6lwbgxi\", path=\"/home/bentoml/bento/\")\n2022-07-01T21:57:47+0000 [INFO] [] Starting production BentoServer from \"/home/bentoml/bento\" running on http://0.0.0.0:3000 (Press CTRL+C to quit)\n2022-07-01T21:57:48+0000 [INFO] [api_server:1] Service loaded from Bento directory: bentoml.Service(tag=\"iris_classifier:6otbsmxzq6lwbgxi\", path=\"/home/bentoml/bento/\")\n2022-07-01T21:57:48+0000 [INFO] [runner-iris_clf:1] Service loaded from Bento directory: bentoml.Service(tag=\"iris_classifier:6otbsmxzq6lwbgxi\", path=\"/home/bentoml/bento/\")\n2022-07-01T21:57:48+0000 [INFO] [api_server:2] Service loaded from Bento directory: bentoml.Service(tag=\"iris_classifier:6otbsmxzq6lwbgxi\", path=\"/home/bentoml/bento/\")\n2022-07-01T21:57:48+0000 [INFO] [runner-iris_clf:1] Setting up worker: set CPU thread count to 4\n2022-07-01T21:57:48+0000 [INFO] [api_server:3] Service loaded from Bento directory: bentoml.Service(tag=\"iris_classifier:6otbsmxzq6lwbgxi\", path=\"/home/bentoml/bento/\")\n2022-07-01T21:57:48+0000 [INFO] [api_server:4] Service loaded from Bento directory: bentoml.Service(tag=\"iris_classifier:6otbsmxzq6lwbgxi\", path=\"/home/bentoml/bento/\")\n```\n\n\nDeploy Bentos\nBentoML version 0.13.1 supports deployment of Bentos to various cloud providers, including Google Cloud Platform, Amazon Web Services,\nand Microsoft Azure. To better support the devops workflows, cloud deployment of Bentos has been moved to a separate project,\n`\ud83d\ude80 bentoctl <https://github.com/bentoml/bentoctl>`_, to better focus on the deployment tasks. :code:`bentoctl` is a CLI tool for\ndeploying your machine-learning models to any cloud platforms.\nManage Bentos\nBentoML version 0.13.1 relies on Yatai as a bento registry to help teams collaborate and manage bentos. In addition to bento management,\n`\ud83e\udd84\ufe0f Yatai <https://github.com/bentoml/Yatai>`_ project has since been expanded into a platform for deploying large scale model\nserving workloads on Kubernetes. Yatai standardizes BentoML deployment and provides UI for managing all your ML models and deployments\nin one place, and enables advanced GitOps and CI/CD workflow.\n\ud83c\udf89\u00a0Ta-da, you have migrated your project to BentoML 1.0.0. Have more questions?",
    "tag": "bentoml"
  },
  {
    "title": "logging.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/guides/logging.rst",
    "content": "=======\nLogging\n=======\ntime expected: 6 minutes\nServer Logging\nBentoML provides a powerful and detailed logging pattern out of the box. Request logs for\nwebservices are logged along with requests to each of the model runner services.\nThe request log format is as follows:\n.. parsed-literal::\n\n\n```time [LEVEL] [component] ClientIP:ClientPort (scheme,method,path,type,length) (status,type,length) Latency (trace,span,sampled)\n```\n\n\nFor example, a log message might look like:\n.. parsed-literal::\n\n\n```2022-06-28T18:07:35-0700 [INFO] [api_server] 127.0.0.1:37386 (scheme=http,method=POST,path=/classify,type=application/json,length=20) (status=200,type=application/json,length=3) 0.005ms (trace=67131233608323295915755120473254509377,span=4151694932783368069,sampled=0)\n```\n\n\nOpenTelemetry Compatible\n^^^^^^^^^^^^^^^^^^^^^^^^\nThe BentoML logging system implements the `OpenTelemetry <https://opentelemetry.io/docs/>` standard\nfor :github:`HTTP <open-telemetry/opentelemetry-specification/blob/main/specification/trace/semantic_conventions/http.md>`\nthroughout the call stack to provide for maximum debuggability. Propogation of the OpenTelemetry\nparameters follows the standard provided `here <https://opentelemetry.lightstep.com/core-concepts/context-propagation/>`\nThe following are parameters which are provided in the logs as well for correlation back to\nparticular requests.\n\n\n`trace_id` is the id of a trace which tracks \u201cthe progression of a single request, as it is handled by services that make up an application.\u201d [#basic_documentation]_\n\n\n`span_id is` the id of a span which is contained within a trace.\n\n\n.. epigraph::\n     \u201cA span is the building block of a trace and is a named, timed operation that represents a piece of the workflow in the distributed system. Multiple spans are pieced together to create a trace.\u201d [#span_documentation]_\n\n`sampled_id is` the number of times this trace has been sampled.\n\n.. epigraph::\n     \u201cSampling is a mechanism to control the noise and overhead introduced by OpenTelemetry by reducing the number of samples of traces collected and sent to the backend.\u201d [#sampling_documentation]_\nLogging Configuration\n^^^^^^^^^^^^^^^^^^^^^\nAccess logs can be configured by setting the appropriate flags in the bento configuration file for\nboth web requests and model serving requests. Read more about how to use a bento configuration file\nhere in the - :ref:`Configuration Guide <guides/configuration>`\nTo configure other logs, please use the `default Python logging configuration <https://docs.python.org/3/howto/logging.html>`_. All BentoML logs are logged under the `bentoml` namespace.\nAPI Service Request Logging\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nFor requests made to the API server, logging can be enabled and disabled using the `api_server.logging.access` parameter at the\ntop level of the `bentoml_configuration.yml`.\n.. code-block:: yaml\n\n\n```api_server:\n  logging:\n    access:\n      enabled: true\n      # whether to log the size of the request body\n      request_content_length: true\n      # whether to log the content type of the request\n      request_content_type: true\n      # whether to log the content length of the response\n      response_content_length: true\n      # whether to log the content type of the response\n      response_content_type: true\n```\n\n\nModel Server Request Logging\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nDepending on how you've configured BentoML, the API server and runner server can be run\nseparately. In either case, BentoML also provides a logging configuration under\n`runners` to allow user to configure the runner server output logs.\nYou may configure the runner access logs under the runners parameter at the top level of your `bentoml_configuration.yml`:\n.. code-block:: yaml\n\n\n```runners:\n  logging:\n    access:\n      enabled: true\n      ...\n```\n\n\nThe available configuration options are the same to the one for API service request logging options above.\nAccess Logging Format\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nYou may configure the format of the Trace and Span IDs in the access logs in `bentoml_configuration.yml`.\nThe default configuration is shown below, where the opentelemetry `trace_id` and `span_id` are logged in\nhexadecimal format, consistent with OpenTelemetry logging instrumentation. You may also configure other format\nspecs, such as decimal `d`.\n.. code-block:: yaml\n\n\n```api_server:\n  logging:\n    access:\n      format:\n        trace_id: 032x\n        span_id: 016x\n```\n\n\nLibrary Logging\nWhen using BentoML as a library, BentoML does not configure any logs. By default, Python will configure a root logger that logs at level WARNING and higher. If you want to see BentoML's DEBUG or INFO logs, register a log handler to the `bentoml` namespace:\n.. code-block:: python\n\n\n```import logging\n\nch = logging.StreamHandler()\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nch.setFormatter(formatter)\n\nbentoml_logger = logging.getLogger(\"bentoml\")\nbentoml_logger.addHandler(ch)\nbentoml_logger.setLevel(logging.DEBUG)\n```\n\n\n\n.. rubric:: Notes\n.. [#basic_documentation] `OpenTelemetry Basic Documentation <https://www.dynatrace.com/support/help/extend-dynatrace/opentelemetry/basics>`_\n.. [#span_documentation] `OpenTelemetry Span Documentation <https://opentelemetry.lightstep.com/spans/>`_\n.. [#sampling_documentation] `OpenTelemetry SDK Documentation <https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/trace/sdk.md>`_",
    "tag": "bentoml"
  },
  {
    "title": "batching.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/guides/batching.rst",
    "content": "=================\nAdaptive Batching\n=================\nBatching is the term used for combining multiple inputs for submission to processing at the same time. The idea is that processing multiple messages is be faster than processing each individual message one at a time. In practice many ML frameworks have optimizations for processing multiple messages at a time thanks to the underlying hardware optimization.\n.. epigraph::\n    \"While serving a TensorFlow model, batching individual model inference requests together can be important for performance. In particular, batching is necessary to unlock the high throughput promised by hardware accelerators such as GPUs.\"\n    -- `TensorFlow documentation <https://github.com/tensorflow/serving/blob/master/tensorflow_serving/batching/README.md>`_\nAdaptive batching is implemented on the server-side. This is advantageous as opposed to client-side batching because it simplifies the client's logic and it is often times more efficient due to traffic volume.\nAs an optimization for a real-time service, batching works off of 2 main concepts.\n\nBatching Window: The maximum time that a service should wait to build a \u201cbatch\u201d before releasing a batch for processing. This is essentially the max latency for processing in a low throughput system. It helps avoid the situation where if very few messages have been submitted (smaller than the max batch size) the batch must wait for a long time to be processed.\nBatch Size: The maximum size that a batch can reach before the batch is release for processing. It puts a cap on the size of the batch in which should optimize for maximum throughput. The concept only applies within the maximum wait time before the batch is released.\n\nBentoML\u2019s adaptive batching works off of these 2 basic concepts and builds on them. Our adaptive batching adapts both the batching window and the max batch size based off of incoming traffic patterns at the time. The dispatching mechanism regresses the recent processing time, wait time and batch sizes to optimize for lowest latency.\nArchitecture\nThe batching mechanism is located on the model runner. Each model runner receives inference requests and batches those requests based on optimal latency.\n.. image:: ../_static/img/batching-architecture.png\nThe load balancer will distribute the requests to each of the running API services. The API services will in turn distribute the inference requests to the model runners. The distribution of requests to the model runners uses a random algorithm which provides for slightly more efficient batch sizes as opposed to round robin. Additional dispatch algorithms are planned for the future.\n.. note::\n    The order of the requests in a batch is not guaranteed.\nModel Batch Dimensions\nIf a trained model supports batched input, the model can be declared as `batchable` in the `save_model` signature parameter. All frameworks by default saves models as `non-batchable` to prevent any inadvertent effects. To gain better performance, it is recommended to enable batching for supported models.\nIn addition to declaring model as batchable, batch dimensions can also be configured in the model signature. Batch dimensions are specified as a 2-tuple representing the input and output batch dimensions respectively. By default, the batch dimension is set to `0`. Adaptive batching will only batch input on the specified batch dimension.\n.. code-block:: python\n    :caption: `train.py`\n\n\n```bentoml.pytorch.save_model(\n    name=\"mnist\",\n    model=model,\n    signatures={\n        \"__call__\": {\n            \"batchable\": True,\n            \"batch_dim\": (0, 0),\n        },\n    },\n)\n```\n\n\nConfiguring Batching\nIf a model supports batching, adaptive batching is enabled by default. To explicitly disable or control adaptive batching behaviors at runtime, configuration can be specified under the `batching` key.\nAdditionally, there are two configurations for customizing batching behaviors, `max_batch_size` and `max_latency_ms`.\nMax Batch Size\n^^^^^^^^^^^^^^\nConfigured through the `max_batch_size` key, max batch size represents the maximum size a batch can reach before releasing for inferencing. Max batch size should be set based on the capacity of the available system resources, e.g. memory or GPU memory.\nMax Latency\n^^^^^^^^^^^\nConfigured through the `max_latency_ms` key, max latency represents the maximum latency in milliseconds that a batch should wait before releasing for inferencing. Max latency should be set based on the service level objective (SLO) of the inference requests.\n.. code-block:: yaml\n    :caption: \u2699\ufe0f `configuration.yml`\n\n\n```runners:\n    iris_clf:\n        batching:\n            enabled: true\n            max_batch_size: 100\n            max_latency_ms: 500\n```\n\n\nMonitoring\nAdaptive batching size is by default exposed as a histogram metric with name, `BENTOML_{runner}_{method}_adaptive_batch_size_bucket`, for observing and debugging adaptive batching behaviors of each model runner.\n.. image:: ../_static/img/adaptive-batching-histogram.png\nError handling",
    "tag": "bentoml"
  },
  {
    "title": "client.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/guides/client.rst",
    "content": "============\nBento Client\n============\nBentoML provides a client implementation that can be used to make requests to a BentoML server.\n.. note:: This feature is only available if both the client and server are running version 1.0.8 of\n   BentoML or newer.\nAfter starting your server, you can initialize a BentoML client by using :obj:`~bentoml.client.Client.from_url`:\n.. code-block:: python\n\n\n```from bentoml.client import Client\n\nclient = Client.from_url(\"http://localhost:3000\")\n```\n\n\nThe client can then be used to make requests to the BentoML server using the api name associated\nwith the function. For example, for the quickstart service endpoint `classify`:\n.. code-block:: python\n\n\n```@svc.api(\n    input=NumpyNdarray.from_sample(np.array([[4.9, 3.0, 1.4, 0.2]], dtype=np.double)),\n    output=NumpyNdarray(),\n)\nasync def classify(input_series: np.ndarray) -> np.ndarray:\n    return await iris_clf_runner.predict.async_run(input_series)\n```\n\n\nThe client can be used in four ways. Calling the client has been made to be as similar to calling\nthe API function manually as possible.\n.. code-block:: python\n\n\n```res = client.classify(np.array([[4.9, 3.0, 1.4, 0.2]]))\nres = client.call(\"classify\", np.array([[4.9, 3.0, 1.4, 0.2]]))\n```\n\n\nNote that even though classify is an `async` function, the `classify` provided by the client is\nstill a synchronous function by default. The synchronous `classify` and `call` cannot be\ncalled within a running async event loop.\nIf inside an async function, use the async versions of the client methods instead:\n.. code-block:: python\n\n\n```res = await client.async_classify(np.array([[4.9, 3.0, 1.4, 0.2]]))\nres = await client.async_call(\"classify\", np.array([[4.9, 3.0, 1.4, 0.2]]))\n```\n\n\nFor multipart requests, all arguments to the function must currently be keyword arguments.\nFor example, for the service API function:\n.. code-block:: python\n    @svc.api(input=Multipart(a=Text(), b=Text()), output=JSON())\n    def combine(a, b) -> dict[typing.Any, typing.Any]:\n        return {a: b}\nThe client call would look like:\n.. code-block:: python",
    "tag": "bentoml"
  },
  {
    "title": "performance.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/guides/performance.rst",
    "content": "=================\nPerformance Guide\n=================\nThis guide is intended to aid advanced BentoML users with a better understanding of the costs and performance overhead of their model serving workload. This guide will also demonstrate BentoML's architecture and provide insights into how users can fine-tune its performance.\n.. TODO::\n\n\n```Performance Guide Todo items:\n\n* basic load testing with locust\n* load testing tips:\n    * the use of --production\n    * enable/disable logging\n    * always run locust client on a separate machine\n\n* performance best practices:\n    * ``bentoml serve`` options: --api-worker, --backlog, --timeout\n    * configure runner resources\n    * configure adaptive batching (max_latency, max_batch_size)\n\n* existing benchmark results and comparisons\n\n* advanced topics:\n    * alternative load testing with grafana k6\n    * setup tracing and dashboard\n    * setup tracing for Yatai and distributed Runner\n    * instrument tracing for user service and runner code\n```\n\n\n.. admonition:: Help us improve the project!\n\n\n```Found an issue or a TODO item? You're always welcome to make contributions to the\nproject and its documentation. Check out the\n`BentoML development guide <https://github.com/bentoml/BentoML/blob/main/DEVELOPMENT.md>`_\nand `documentation guide <https://github.com/bentoml/BentoML/blob/main/docs/README.md>`_\nto get started.\n```\n\n",
    "tag": "bentoml"
  },
  {
    "title": "security.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/guides/security.rst",
    "content": "========\nSecurity\n========\nSecuring Endpoint Access\nServer Side Authentication\n^^^^^^^^^^^^^^^^^^^^^^^^^^\nTo enable authentication for a given BentoServer endpoint, An authentication middleware can be added to :code:`bentoml.Service`'s via :code:`add_asgi_middleware` API. This API supports mounting\nany ASGI middleware to the BentoServer endpoints. And many of the middlewares built by\nthe Python community, provides authentication or security functionality.\nFor example, you may apply HTTPS redirect and set trusted host URLs this way:\n.. code::\n\n\n```from starlette.middleware.httpsredirect import HTTPSRedirectMiddleware\nfrom starlette.middleware.trustedhost import TrustedHostMiddleware\n\nsvc = bentoml.Service('my_service', runners=[...])\n\nsvc.add_asgi_middleware(TrustedHostMiddleware, allowed_hosts=['example.com', '*.example.com'])\nsvc.add_asgi_middleware(HTTPSRedirectMiddleware)\n```\n\n\nFor JWT authentication, check out the `starlette-authlib <https://github.com/aogier/starlette-authlib>`\nand `starlette-auth-toolkit <https://github.com/florimondmanca/starlette-auth-toolkit>`.\nHere's an example with starlette-authlib:\n.. code::\n\n\n```from starlette_authlib.middleware import AuthlibMiddleware as SessionMiddleware\n\nsvc = bentoml.Service('my_service', runners=[...])\n\nsvc.add_asgi_middleware(SessionMiddleware, secret_key='you_secret')\n```\n\n\nCertificates\n^^^^^^^^^^^^\nBentoML supports HTTPS with self-signed certificates. To enable HTTPS, you can to provide SSL certificate and key files as arguments\nto the :code:`bentoml serve` command. Use :code:`bentoml serve --help` to see the full list of options.\n.. code::\n\n\n```bentoml serve iris_classifier:latest --ssl-certfile /path/to/cert.pem --ssl-keyfile /path/to/key.pem\n```\n\n\nReverse Proxy\n^^^^^^^^^^^^^\nIt is a common practice to set up a reverse proxy server to handle rate limiting and authentication for any given backend services.\nService Mesh\n^^^^^^^^^^^^\nFor Kubernetes users looking for advanced authentication, access control, and routing\npolicies, we recommend you to deploy Bentos with `Yatai <https://github.com/bentoml/Yatai>`\nand use Yatai's `Istio <https://istio.io/>` integration.\nSecurity Policy\nTo report a vulnerability, we kindly ask you not to share it publicly on GitHub or in the community slack channel. Instead, contact the BentoML team directly at contact@bentoml.ai\nView the full BentoML\u2019s security policy `here <https://github.com/bentoml/BentoML/security/policy>`_.\n.. TODO::\n\n\n```* Base Image Security\n* Securing Yatai deployment\n* Reverse Proxy setup guide and sample code/config\n```\n\n",
    "tag": "bentoml"
  },
  {
    "title": "grpc_tools.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/guides/snippets/grpc/grpc_tools.rst",
    "content": ".. tab-set::\n.. tab-item:: gRPCurl\n\n\n```  We will use :github:`fullstorydev/grpcurl` to send a CURL-like request to the gRPC BentoServer.\n\n  Note that we will use `docker <https://docs.docker.com/get-docker/>`_ to run the ``grpcurl`` command.\n\n  .. tab-set::\n\n     .. tab-item:: MacOS/Windows\n        :sync: __fullstorydev_macwin\n\n        .. code-block:: bash\n\n           \u00bb docker run -i --rm \\\n                          fullstorydev/grpcurl -d @ -plaintext host.docker.internal:3000 \\\n                          bentoml.grpc.v1.BentoService/Call <<EOT\n           {\n              \"apiName\": \"classify\",\n              \"ndarray\": {\n                 \"shape\": [1, 4],\n                 \"floatValues\": [5.9, 3, 5.1, 1.8]\n              }\n           }\n           EOT\n\n     .. tab-item:: Linux\n        :sync: __fullstorydev_linux\n\n        .. code-block:: bash\n\n           \u00bb docker run -i --rm \\\n                          --network=host \\\n                          fullstorydev/grpcurl -d @ -plaintext 0.0.0.0:3000 \\\n                          bentoml.grpc.v1.BentoService/Call <<EOT\n           {\n              \"apiName\": \"classify\",\n              \"ndarray\": {\n                 \"shape\": [1, 4],\n                 \"floatValues\": [5.9, 3, 5.1, 1.8]\n              }\n           }\n           EOT\n```\n\n\n.. tab-item:: gRPCUI\n\n\n```  We will use :github:`fullstorydev/grpcui` to send request from a web browser.\n\n  Note that we will use `docker <https://docs.docker.com/get-docker/>`_ to run the ``grpcui`` command.\n\n  .. tab-set::\n\n     .. tab-item:: MacOS/Windows\n        :sync: __fullstorydev_macwin\n\n        .. code-block:: bash\n\n           \u00bb docker run --init --rm \\\n                          -p 8080:8080 fullstorydev/grpcui -plaintext host.docker.internal:3000\n\n     .. tab-item:: Linux\n        :sync: __fullstorydev_linux\n\n        .. code-block:: bash\n\n           \u00bb docker run --init --rm \\\n                          -p 8080:8080 \\\n                          --network=host fullstorydev/grpcui -plaintext 0.0.0.0:3000\n\n  Proceed to http://127.0.0.1:8080 in your browser and send test request from the web UI.\n```\n\n",
    "tag": "bentoml"
  },
  {
    "title": "additional_setup.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/guides/snippets/grpc/additional_setup.rst",
    "content": "Since there is no easy way to add additional proto files, we will have to clone some\nrepositories and copy the proto files into our project:\n\n:github:`protocolbuffers/protobuf` - the official repository for the Protocol Buffers. We will need protobuf files that lives under `src/google/protobuf`:\n\n.. code-block:: bash\n\u00bb mkdir -p thirdparty && cd thirdparty\n   \u00bb git clone --depth 1 https://github.com/protocolbuffers/protobuf.git\n\n:github:`bentoml/bentoml` - We need the `service.proto` under `bentoml/grpc/` to build the client, therefore, we will perform\n   a `sparse checkout <https://github.blog/2020-01-17-bring-your-monorepo-down-to-size-with-sparse-checkout/>`_ to only checkout `bentoml/grpc` directory:\n\n.. code-block:: bash\n\u00bb mkdir bentoml && pushd bentoml\n   \u00bb git init\n   \u00bb git remote add -f origin https://github.com/bentoml/BentoML.git\n   \u00bb git config core.sparseCheckout true\n   \u00bb cat <|.git/info/sparse-checkout\n   src/bentoml/grpc\n   EOT\n   \u00bb git pull origin main && mv src/bentoml/grpc .",
    "tag": "bentoml"
  },
  {
    "title": "index.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/concepts/index.rst",
    "content": "=============\nMain Concepts\n=============\nThis is a step-by-step tour that help you dive into the main concepts in BentoML.\n.. grid:: 1 2 2 2\n    :gutter: 3\n    :margin: 0\n    :padding: 3 4 0 0\n\n\n```.. grid-item-card:: :doc:`/concepts/model`\n    :link: /concepts/model\n    :link-type: doc\n\n    Save and version your trained ML model with BentoML's model store\n\n.. grid-item-card:: :doc:`/concepts/service`\n    :link: /concepts/service\n    :link-type: doc\n\n    Create ML service and API endpoints with BentoML\n\n.. grid-item-card:: :doc:`/concepts/bento`\n    :link: /concepts/bento\n    :link-type: doc\n\n    Build and customize Bento for production deployment\n\n.. grid-item-card:: :doc:`/concepts/runner`\n    :link: /concepts/runner\n    :link-type: doc\n\n    Using Runners in BentoML and fine-tune performance\n\n.. grid-item-card:: :doc:`/concepts/deploy`\n    :link: /concepts/deploy\n    :link-type: doc\n\n    Understand different deployment options provided by BentoML\n```\n\n\n.. toctree::\n    :maxdepth: 1\n    :titlesonly:\n    :hidden:\n\n\n```model\nservice\nbento\nrunner\ndeploy\n```\n\n",
    "tag": "bentoml"
  },
  {
    "title": "model.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/concepts/model.rst",
    "content": "================\nPreparing Models\n================\nSave A Trained Model\nA trained ML model instance needs to be saved with BentoML API, in order to serve it\nwith BentoML. For most cases, it will be just one line added to your model training\npipeline, invoking a :code:`save_model` call, as demonstrated in the\n:doc:`tutorial </tutorial>`:\n.. code:: python\n\n\n```saved_model = bentoml.sklearn.save_model(\"iris_clf\", clf)\nprint(f\"Model saved: {saved_model}\")\n\n# Model saved: Model(tag=\"iris_clf:2uo5fkgxj27exuqj\")\n```\n\n\n.. seealso::\nIt is also possible to use pre-trained models directly with BentoML, without\n   saving it to the model store first. Check out the\n   :ref:`Custom Runner <concepts/runner:Custom Runner>` example to learn more.\n.. tip::\nIf you have an existing model saved to file on disk, you will need to load the model\n   in a python session first and then use BentoML's framework specific\n   :code:`save_model` method to put it into the BentoML model store.\nWe recommend always save the model with BentoML as soon as it finished training and\n   validation. By putting the :code:`save_model` call to the end of your training\n   pipeline, all your finalized models can be managed in one place and ready for\n   inference.\nOptionally, you may attach custom labels, metadata, or :code:`custom_objects` to be\nsaved alongside your model in the model store, e.g.:\n.. code:: python\n\n\n```bentoml.pytorch.save_model(\n    \"demo_mnist\",   # model name in the local model store\n    trained_model,  # model instance being saved\n    labels={    # user-defined labels for managing models in Yatai\n        \"owner\": \"nlp_team\",\n        \"stage\": \"dev\",\n    },\n    metadata={  # user-defined additional metadata\n        \"acc\": acc,\n        \"cv_stats\": cv_stats,\n        \"dataset_version\": \"20210820\",\n    },\n    custom_objects={    # save additional user-defined python objects\n        \"tokenizer\": tokenizer_object,\n    }\n)\n```\n\n\n\nlabels: user-defined labels for managing models, e.g. team=nlp, stage=dev.\nmetadata: user-defined metadata for storing model training context information or model evaluation metrics, e.g. dataset version, training parameters, model scores.\ncustom_objects: user-defined additional python objects, e.g. a tokenizer instance, preprocessor function, model configuration json, serialized with cloudpickle. Custom objects will be serialized with `cloudpickle <https://github.com/cloudpipe/cloudpickle>`_.\n\nRetrieve a saved model\nTo load the model instance back into memory, use the framework-specific\n:code:`load_model` method. For example:\n.. code:: python\n\n\n```import bentoml\nfrom sklearn.base import BaseEstimator\n\nmodel: BaseEstimator = bentoml.sklearn.load_model(\"iris_clf:latest\")\n```\n\n\n.. note::\n\n\n```The :code:`load_model` method is only here for testing and advanced customizations.\nFor general model serving use cases, use Runner for running model inference. See the\n:ref:`concepts/model:Using Model Runner` section below to learn more.\n```\n\n\nFor retrieving the model metadata or custom objects, use the :code:`get` method:\n.. code:: python\n\n\n```import bentoml\nbento_model: bentoml.Model = bentoml.models.get(\"iris_clf:latest\")\n\nprint(bento_model.tag)\nprint(bento_model.path)\nprint(bento_model.custom_objects)\nprint(bento_model.info.metadata)\nprint(bento_model.info.labels)\n\nmy_runner: bentoml.Runner = bento_model.to_runner()\n```\n\n\n:code:`bentoml.models.get` returns a :ref:`bentoml.Model <reference/core:Model>`\ninstance, which is a reference to a saved model entry in the BentoML model store. The\n:code:`bentoml.Model` instance then provides access to the model info and the\n:code:`to_runner` API for creating a Runner instance from the model.\n.. note::\n\n\n```BentoML also provides a framework-specific :code:`get` method under each framework\nmodule, e.g.: :code:`benotml.pytorch.get`. It behaves exactly the same as\n:code:`bentoml.models.get`, besides that it verifies if the model found was saved\nwith the same framework.\n```\n\n\nManaging Models\nSaved models are stored in BentoML's model store, which is a local file directory\nmaintained by BentoML. Users can view and manage all saved models via the\n:code:`bentoml models` CLI command:\n.. tab-set::\n\n\n```.. tab-item:: List\n\n    .. code:: bash\n\n        > bentoml models list\n\n        Tag                        Module           Size        Creation Time        Path\n        iris_clf:2uo5fkgxj27exuqj  bentoml.sklearn  5.81 KiB    2022-05-19 08:36:52  ~/bentoml/models/iris_clf/2uo5fkgxj27exuqj\n        iris_clf:nb5vrfgwfgtjruqj  bentoml.sklearn  5.80 KiB    2022-05-17 21:36:27  ~/bentoml/models/iris_clf/nb5vrfgwfgtjruqj\n\n\n.. tab-item:: Get\n\n    .. code:: bash\n\n        > bentoml models get iris_clf:latest\n\n        name: iris_clf\n        version: 2uo5fkgxj27exuqj\n        module: bentoml.sklearn\n        labels: {}\n        options: {}\n        metadata: {}\n        context:\n            framework_name: sklearn\n            framework_versions:\n              scikit-learn: 1.1.0\n            bentoml_version: 1.0.0\n            python_version: 3.8.12\n        signatures:\n            predict:\n              batchable: false\n        api_version: v1\n        creation_time: '2022-05-19T08:36:52.456990+00:00'\n\n.. tab-item:: Delete\n\n    .. code:: bash\n\n        > bentoml models delete iris_clf:latest -y\n\n        INFO [cli] Model(tag=\"iris_clf:2uo5fkgxj27exuqj\") deleted\n```\n\n\nModel Import and Export\n^^^^^^^^^^^^^^^^^^^^^^^\nModels saved with BentoML can be exported to a standalone archive file outside of the\nmodel store, for sharing models between teams or moving models between different build\nstages. For example:\n.. code:: bash\n\n\n```> bentoml models export iris_clf:latest .\n\nModel(tag=\"iris_clf:2uo5fkgxj27exuqj\") exported to ./iris_clf-2uo5fkgxj27exuqj.bentomodel\n```\n\n\n.. code:: bash\n\n\n```> bentoml models import ./iris_clf-2uo5fkgxj27exuqj.bentomodel\n\nModel(tag=\"iris_clf:2uo5fkgxj27exuqj\") imported\n```\n\n\n.. note::\n\n\n```Model can be exported to or import from AWS S3, GCS, FTP, Dropbox, etc. For\nexample:\n\n.. code:: bash\n\n    pip install fs-s3fs  # Additional dependency required for working with s3\n    bentoml models export iris_clf:latest s3://my_bucket/my_prefix/\n```\n\n\nPush and Pull with Yatai\n^^^^^^^^^^^^^^^^^^^^^^^^\n`Yatai <https://github.com/bentoml/Yatai>`_ provides a centralized Model repository\nthat comes with flexible APIs and Web UI for managing all models (and\n:doc:`Bentos </concepts/bento>`) created by your team. It can be configured to store\nmodel files on cloud blob storage such as AWS S3, MinIO or GCS.\nOnce your team have Yatai setup, you can use the :code:`bentoml models push` and\n:code:`bentoml models pull` command to get models to and from Yatai:\n.. code:: bash\n\n\n```> bentoml models push iris_clf:latest\n\nSuccessfully pushed model \"iris_clf:2uo5fkgxj27exuqj\"                                                                                                                                                                                           \u2502\n```\n\n\n.. code:: bash\n\n\n```> bentoml models pull iris_clf:latest\n\nSuccessfully pulled model \"iris_clf:2uo5fkgxj27exuqj\"\n```\n\n\n.. image:: /_static/img/yatai-model-detail.png\n    :alt: Yatai Model Details UI\n.. tip::\n\n\n```Learn more about CLI usage from :code:`bentoml models --help`.\n```\n\n\nModel Management API\n^^^^^^^^^^^^^^^^^^^^\nBesides the CLI commands, BentoML also provides equivalent\n:doc:`Python APIs </reference/stores>` for managing models:\n.. tab-set::\n\n\n```.. tab-item:: Get\n\n    .. code:: python\n\n        import bentoml\n        bento_model: bentoml.Model = bentoml.models.get(\"iris_clf:latest\")\n\n        print(bento_model.path)\n        print(bento_model.info.metadata)\n        print(bento_model.info.labels)\n\n\n.. tab-item:: List\n\n    :code:`bentoml.models.list` returns a list of :ref:`bentoml.Model <reference/core:Model>`:\n\n    .. code:: python\n\n        import bentoml\n        models = bentoml.models.list()\n\n.. tab-item:: Import / Export\n\n    .. code:: python\n\n        import bentoml\n        bentoml.models.export_model('iris_clf:latest', '/path/to/folder/my_model.bentomodel')\n\n    .. code:: python\n\n        bentoml.models.import_model('/path/to/folder/my_model.bentomodel')\n\n    .. note::\n\n        Model can be exported to or import from AWS S3, GCS, FTP, Dropbox, etc. For\n        example:\n\n        .. code:: python\n\n            bentoml.models.import_model('s3://my_bucket/folder/my_model.bentomodel')\n\n\n.. tab-item:: Push / Pull\n\n    If your team has `Yatai <https://github.com/bentoml/Yatai>`_ setup, you can also\n    push local Models to Yatai, it provides APIs and Web UI for managing all Models\n    created by your team and stores model files on cloud blob storage such as AWS S3,\n    MinIO or GCS.\n\n    .. code:: python\n\n        import bentoml\n        bentoml.models.push(\"iris_clf:latest\")\n\n    .. code:: python\n\n        bentoml.models.pull(\"iris_clf:latest\")\n\n\n.. tab-item:: Delete\n\n    .. code:: python\n\n        import bentoml\n        bentoml.models.delete(\"iris_clf:latest\")\n```\n\n\nUsing Model Runner\nThe way to run model inference in the context of a :code:`bentoml.Service`, is via a\nRunner. The Runner abstraction gives BentoServer more flexibility in terms of how to\nschedule the inference computation, how to dynamically batch inference calls and better\ntake advantage of all hardware resource available.\nAs demonstrated in the :doc:`tutorial </tutorial>`, a model runner can be created\nfrom a saved model via the :code:`to_runner` API:\n.. code:: python\n\n\n```iris_clf_runner = bentoml.sklearn.get(\"iris_clf:latest\").to_runner()\n```\n\n\nThe runner instance can then be used for creating a :code:`bentoml.Service`:\n.. code:: python\n\n\n```svc = bentoml.Service(\"iris_classifier\", runners=[iris_clf_runner])\n\n@svc.api(input=NumpyNdarray(), output=NumpyNdarray())\ndef classify(input_series: np.ndarray) -> np.ndarray:\n    result = iris_clf_runner.predict.run(input_series)\n    return result\n```\n\n\nTo test out the runner interface before writing the Service API callback function,\nyou can create a local runner instance outside of a Service:\n.. code:: python\n\n\n```# Create a Runner instance:\niris_clf_runner = bentoml.sklearn.get(\"iris_clf:latest\").to_runner()\n\n# Initializes the runner in current process, this is meant for development and testing only:\niris_clf_runner.init_local()\n\n# This should yield the same result as the loaded model:\niris_clf_runner.predict.run([[5.9, 3., 5.1, 1.8]])\n```\n\n\nTo learn more about Runner usage and its architecture, see :doc:`/concepts/runner`.\nModel Signatures\nA model signature represents a method on a model object that can be called. This\ninformation is used when creating BentoML runners for this model.\nFrom the example above, the :code:`iris_clf_runner.predict.run` call will pass through\nthe function input to the model's :code:`predict` method, running from a remote runner\nprocess.\nFor many :doc:`other ML frameworks <frameworks/index>`, the model object's inference\nmethod may not be called :code:`predict`. Users can customize it by specifying the model\nsignature during :code:`save_model`:\n.. code-block:: python\n   :emphasize-lines: 4-8,13\n\n\n```bentoml.pytorch.save_model(\n    \"demo_mnist\",  # model name in the local model store\n    trained_model,  # model instance being saved\n    signatures={   # model signatures for runner inference\n        \"classify\": {\n            \"batchable\": False,\n        }\n    }\n)\n\nrunner = bentoml.pytorch.get(\"demo_mnist:latest\").to_runner()\nrunner.init_local()\nrunner.classify.run( MODEL_INPUT )\n```\n\n\nA special case here is Python's magic method :code:`__call__`. Similar to the\nPython language convention, the call to :code:`runner.run` will be applied to\nthe model's :code:`__call__` method:\n.. code-block:: python\n   :emphasize-lines: 4-8,13\n\n\n```bentoml.pytorch.save_model(\n    \"demo_mnist\",  # model name in the local model store\n    trained_model,  # model instance being saved\n    signatures={   # model signatures for runner inference\n        \"__call__\": {\n            \"batchable\": False,\n        },\n    }\n)\n\nrunner = bentoml.pytorch.get(\"demo_mnist:latest\").to_runner()\nrunner.init_local()\nrunner.run( MODEL_INPUT )\n```\n\n\nBatching\nFor model inference calls that supports taking a batch input, it is recommended to\nenable batching for the target model signature. In which case, :code:`runner#run` calls\nmade from multiple Service workers can be dynamically merged to a larger batch and run\nas one inference call in the runner worker. Here's an example:\n.. code-block:: python\n   :emphasize-lines: 4-9,14\n\n\n```bentoml.pytorch.save_model(\n    \"demo_mnist\",  # model name in the local model store\n    trained_model,  # model instance being saved\n    signatures={   # model signatures for runner inference\n        \"__call__\": {\n            \"batchable\": True,\n            \"batch_dim\": 0,\n        },\n    }\n)\n\nrunner = bentoml.pytorch.get(\"demo_mnist:latest\").to_runner()\nrunner.init_local()\nrunner.run( MODEL_INPUT )\n```\n\n\n.. tip::\n\n\n```The runner interface is exactly the same, regardless :code:`batchable` was set to\nTrue or False.\n```\n\n\nThe :code:`batch_dim` parameter determines the dimension(s) that contain multiple data\nwhen passing to this run method. The default :code:`batch_dim`, when left unspecified,\nis :code:`0`.\nFor example, if you have two inputs you want to run prediction on, :code:`[1, 2]` and\n:code:`[3, 4]`, if the array you would pass to the predict method would be\n:code:`[[1, 2], [3, 4]]`, then the batch dimension would be :code:`0`. If the array you\nwould pass to the predict method would be :code:`[[1, 3], [2, 4]]`, then the batch\ndimension would be :code:`1`. For example:\n.. code:: python\n\n\n```# Save two models with `predict` method that supports taking input batches on the\n# dimension 0 and the other on dimension 1:\nbentoml.pytorch.save_model(\"demo0\", model_0, signatures={\n    \"predict\": {\"batchable\": True, \"batch_dim\": 0}}\n)\nbentoml.pytorch.save_model(\"demo1\", model_1, signatures={\n    \"predict\": {\"batchable\": True, \"batch_dim\": 1}}\n)\n\n# if the following calls are batched, the input to the actual predict method on the\n# model.predict method would be [[1, 2], [3, 4], [5, 6]]\nrunner0 = bentoml.pytorch.get(\"demo0:latest\").to_runner()\nrunner0.init_local()\nrunner0.predict.run(np.array([[1, 2], [3, 4]]))\nrunner0.predict.run(np.array([[5, 6]]))\n\n# if the following calls are batched, the input to the actual predict method on the\n# model.predict would be [[1, 2, 5], [3, 4, 6]]\nrunner1 = bentoml.pytorch.get(\"demo1:latest\").to_runner()\nrunner1.init_local()\nrunner1.predict.run(np.array([[1, 2], [3, 4]]))\nrunner1.predict.run(np.array([[5], [6]]))\n```\n\n\n.. admonition:: Expert API\n\n\n```If there are multiple arguments to the run method and there is only one batch\ndimension supplied, all arguments will use that batch dimension.\n\nThe batch dimension can also be a tuple of (input batch dimension, output batch\ndimension). For example, if the predict method should have its input batched along\nthe first axis and its output batched along the zeroth axis, :code:`batch_dim`` can\nbe set to :code:`(1, 0)`.\n```\n\n\nFor online serving workloads, adaptive batching is a critical component that contributes\nto the overall performance. If throughput and latency are important to you, learn more\nabout other Runner options and batching configurations in the :doc:`/concepts/runner`\nand :doc:`/guides/batching` doc.\n.. TODO::",
    "tag": "bentoml"
  },
  {
    "title": "service.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/concepts/service.rst",
    "content": "================\nService and APIs\n================\nThe service definition is the manifestation of the\n`Service Oriented Architecture <https://en.wikipedia.org/wiki/Service-oriented_architecture>`_\nand the core building block in BentoML where users define the model serving logic. This\nguide will dissect and explain the key components in the service definition.\nCreating a Service\nA BentoML service is composed of Runners and APIs. Consider the following service\ndefinition from the :doc:`tutorial </tutorial>`:\n.. code-block:: python\n\n\n```import numpy as np\nimport bentoml\nfrom bentoml.io import NumpyNdarray\n\niris_clf_runner = bentoml.sklearn.get(\"iris_clf:latest\").to_runner()\n\nsvc = bentoml.Service(\"iris_classifier\", runners=[iris_clf_runner])\n\n@svc.api(input=NumpyNdarray(), output=NumpyNdarray())\ndef classify(input_series: np.ndarray) -> np.ndarray:\n    result = iris_clf_runner.predict.run(input_series)\n    return result\n```\n\n\nServices are initialized through `bentoml.Service()` call, with the service name and a\nlist of :doc:`Runners </concepts/runner>` required in the service:\n.. code-block:: python\n\n\n```# Create the iris_classifier_service with the ScikitLearn runner\nsvc = bentoml.Service(\"iris_classifier\", runners=[iris_clf_runner])\n```\n\n\n.. note::\n    The service name will become the name of the Bento.\nThe `svc` object created provides a decorator method `svc.api` for defining`\nAPIs in this service:\n.. code-block:: python\n\n\n```@svc.api(input=NumpyNdarray(), output=NumpyNdarray())\ndef classify(input_series: np.ndarray) -> np.ndarray:\n    result = iris_clf_runner.predict.run(input_series)\n    return result\n```\n\n\nRunners\nRunners represent a unit of serving logic that can be scaled horizontally to maximize\nthroughput and resource utilization.\nBentoML provides a convenient way of creating Runner instance from a saved model:\n.. code-block:: python\n\n\n```runner = bentoml.sklearn.get(\"iris_clf:latest\").to_runner()\n```\n\n\n.. tip::\n    Users can also create custom Runners via the :doc:`Runner and Runnable interface <concepts/runner>`.\nRunner created from a model will automatically choose the most optimal Runner\nconfigurations specific for the target ML framework.\nFor example, if an ML framework releases the Python GIL and supports concurrent access\nnatively, BentoML will create a single global instance of the runner worker and route\nall API requests to the global instance; otherwise, BentoML will create multiple\ninstances of runners based on the available system resources. We also let advanced users\nto customize the runtime configurations to fine tune the runner performance. To learn\nmore, please see the :doc:`concepts/runner` guide.\nDebugging Runners\n^^^^^^^^^^^^^^^^^\nRunners must be initialized in order to function. Normally, this is handled by BentoML internally\nwhen `bentoml serve` is called.\nIf you want to import and run a service without using BentoML, this must be done manually. For\nexample, to debug a service called `svc` in `service.py`:\n.. code-block:: python\n\n\n```from service import svc\n\nfor runner in svc.runners:\n    runner.init_local()\n\nresult = svc.apis[\"my_endpoint\"].func(inp)\n```\n\n\nService APIs\nInference APIs define how the service functionality can be called remotely. A service can \nhave one or more APIs. An API consists of its input/output specs and a callback function:\n.. code-block:: python\n\n\n```# Create new API and add it to \"svc\"\n@svc.api(input=NumpyNdarray(), output=NumpyNdarray())  # define IO spec\ndef predict(input_array: np.ndarray) -> np.ndarray:\n    # Define business logic\n    # Define pre-processing logic\n    result = runner.run(input_array)  #  model inference call\n    # Define post-processing logic\n    return result\n```\n\n\nBy decorating a function with ``@svc.api`, we declare that the function shall be`\ninvoked when this API is called. The API function is a great place for defining your\nserving logic, such as feature fetching, pre and post processing, and model inferences \nvia Runners.\nWhen running `bentoml serve` with the example above, this API function is\ntransformed into an HTTP endpoint, `/predict`, that takes in a `np.ndarray` as \ninput, and returns a `np.ndarray` as output. The endpoint can be called with the following\n`curl` command:\n.. code-block:: bash\n\n\n```\u00bb curl -X POST \\\n    -H \"content-type: application/json\" \\\n    --data \"[[5.9, 3, 5.1, 1.8]]\" \\\n    http://127.0.0.1:3000/predict\n\n\"[0]\"\n```\n\n\n.. tip::\n    BentoML also plan to support translating the same Service API definition into a gRPC\n    server endpoint, in addition to the default HTTP server. See :issue:`703`.\nRoute\n^^^^^\nBy default, the function name becomes the endpoint URL. Users can also customize\nthis URL via the `route` option, e.g.:\n.. code-block:: python\n\n\n```@svc.api(\n    input=NumpyNdarray(),\n    output=NumpyNdarray(),\n    route=\"/v2/models/my_model/versions/v0/infer\",\n)\ndef predict(input_array: np.ndarray) -> np.ndarray:\n    return runner.run(input_array)\n```\n\n\n.. note::\n    BentoML aims to parallelize API logic by starting multiple instances of the API\n    server based on available system resources.\nInference Context\n^^^^^^^^^^^^^^^^^\nThe context of an inference call can be accessed through the additional `bentoml.Context`\nargument added to the service API function. Both the request and response contexts can be \naccessed through the inference context for getting and setting the headers, cookies, and\nstatus codes.\n.. code-block:: python\n\n\n```@svc.api(\n    input=NumpyNdarray(),\n    output=NumpyNdarray(),\n)\ndef predict(input_array: np.ndarray, ctx: bentoml.Context) -> np.ndarray:\n    # get request headers\n    request_headers = ctx.request.headers\n\n    result = runner.run(input_array)\n\n    # set response headers, cookies, and status code \n    ctx.response.status_code = 202\n    ctx.response.cookies = [\n        bentoml.Cookie(\n            key=\"key\",\n            value=\"value\",\n            max_age=None,\n            expires=None,\n            path=\"/predict\",\n            domain=None,\n            secure=True,\n            httponly=True,\n            samesite=\"None\"\n        )\n    ]\n    ctx.response.headers.append(\"X-Custom-Header\", \"value\")\n\n    return result\n```\n\n\nIO Descriptors\nIO descriptors are used for defining an API's input and output specifications. It\ndescribes the expected data type, helps validate that the input and output conform to\nthe expected format and schema and convert them from and to the native types. They are\nspecified through the `input` and `output` arguments in the `@svc.api`\ndecorator method.\nRecall the API we created in the :doc:`tutorial </tutorial>`. The `classify` API both accepts\narguments and returns results in the type of\n:ref:`bentoml.io.NumpyNdarray <reference/api_io_descriptors:NumPy \\``ndarray``>`:\n.. code-block:: python\n\n\n```import numpy as np\nfrom bentoml.io import NumpyNdarray\n\n@svc.api(input=NumpyNdarray(), output=NumpyNdarray())\ndef classify(input_array: np.ndarray) -> np.ndarray:\n    ...\n```\n\n\nBesides the `NumpyNdarray` IO descriptor, BentoML supports a variety of IO\ndescriptors including `PandasDataFrame`, `JSON`, `String`,\n`Image`, `Text`, and `File`. For detailed documentation on how to\ndeclare and invoke these descriptors please see the\n:doc:`IO Descriptors </reference/api_io_descriptors>` API reference page.\nSchema and Validation\n^^^^^^^^^^^^^^^^^^^^^\nIO descriptors allow users to define the expected data types, shape, and schema, based \non the type of the input and output descriptor specified. IO descriptors can also be defined \nthrough  examples with the `from_sample` API to simplify the development of service \ndefinitions.\nNumpy\n~~~~~\nThe data type and shape of the `NumpyNdarray` can be specified with the `dtype` \nand `shape` arguments. By setting the `enforce_shape` and `enforce_dtype` \narguments to `True`, the IO descriptor will strictly validate the input and output data \nbased the specified data type and shape. To learn more, see IO descrptor reference for \n:ref:`reference/api_io_descriptors:NumPy ndarray`.\n.. code-block:: python\n\n\n```import numpy as np\n\nfrom bentoml.io import NumpyNdarray\n\nsvc = bentoml.Service(\"iris_classifier\")\n\n# Define IO descriptors through samples\noutput_descriptor = NumpyNdarray.from_sample(np.array([[1.0, 2.0, 3.0, 4.0]]))\n\n@svc.api(\n    input=NumpyNdarray(\n        shape=(-1, 4),\n        dtype=np.float32,\n        enforce_dtype=True,\n        enforce_shape=True\n    ),\n    output=output_descriptor,\n)\ndef classify(input_array: np.ndarray) -> np.ndarray:\n    ...\n```\n\n\nPandas DataFrame\n~~~~~~~~~~~~~~~~\nThe data type and shape of the `PandasDataFrame` can be specified with the `dtype` \nand `shape` arguments. By setting the `enforce_shape` and `enforce_dtype` \narguments to `True`, the IO descriptor will strictly validate the input and output data \nbased the specified data type and shape. To learn more, see IO descrptor reference for \n:ref:`reference/api_io_descriptors:Tabular Data with Pandas`.\n.. code-block:: python\n\n\n```import pandas as pd\n\nfrom bentoml.io import PandasDataFrame\n\nsvc = bentoml.Service(\"iris_classifier\")\n\n# Define IO descriptors through samples\noutput_descriptor = PandasDataFrame.from_sample(pd.DataFrame([[5,4,3,2]]))\n\n@svc.api(\n    input=PandasDataFrame(\n        orient=\"records\",\n        dtype=np.float32,\n        enforce_dtype=True,\n        shape=(-1, 4),\n        enforce_shape=True\n    ),\n    output=output_descriptor,\n)\ndef classify(input_series: pd.DataFrame) -> pd.DataFrame:\n    ...\n```\n\n\nJSON\n~~~~\nThe data type of a JSON IO descriptor can be specified through a Pydantic model. By setting \na pydantic model, the IO descriptor will validate the input based on the specified pydantic\nmodel and return. To learn more, see IO descrptor reference for\n:ref:`reference/api_io_descriptors:Structured Data with JSON`.\n.. code-block:: python\n\n\n```from typing import Dict, Any\nfrom pydantic import BaseModel\n\nsvc = bentoml.Service(\"iris_classifier\")\n\nclass IrisFeatures(BaseModel):\n    sepal_length: float\n    sepal_width: float\n    petal_length: float\n    petal_width: float\n\n@svc.api(\n    input=JSON(pydantic_model=IrisFeatures),\n    output=JSON(),\n)\ndef classify(input_series: IrisFeatures) -> Dict[str, Any]:\n    input_df = pd.DataFrame([input_data.dict()])\n    results = iris_clf_runner.predict.run(input_df).to_list()\n    return {\"predictions\": results}\n```\n\n\nBuilt-in Types\n^^^^^^^^^^^^^^\nBeside `NumpyNdarray`, BentoML supports a variety of other built-in IO descriptor\ntypes under the :doc:`bentoml.io <reference/api_io_descriptors>` module. Each type comes\nwith support of type validation and OpenAPI specification generation. For example:\n+-----------------+---------------------+---------------------+-------------------------+\n| IO Descriptor   | Type                | Arguments           | Schema Type             |\n+=================+=====================+=====================+=========================+\n| NumpyNdarray    | numpy.ndarray       | validate, schema    | numpy.dtype             |\n+-----------------+---------------------+---------------------+-------------------------+\n| PandasDataFrame | pandas.DataFrame    | validate, schema    | pandas.DataFrame.dtypes |\n+-----------------+---------------------+---------------------+-------------------------+\n| Json            | Python native types | validate, schema    | Pydantic.BaseModel      |\n+-----------------+---------------------+---------------------+-------------------------+\n| Image           | PIL.Image.Image     | pilmodel, mime_type |                         |\n+-----------------+---------------------+---------------------+-------------------------+\n| Text            | str                 |                     |                         |\n+-----------------+---------------------+---------------------+-------------------------+\n| File            | BytesIOFile         | kind, mime_type     |                         |\n+-----------------+---------------------+---------------------+-------------------------+\nLearn more about other built-in IO Descriptors :doc:`here </reference/api_io_descriptors>`.\nComposite Types\n^^^^^^^^^^^^^^^\nThe `Multipart` IO descriptors can be used to group multiple IO Descriptor\ninstances, which allows the API function to accept multiple arguments or return multiple\nvalues. Each IO descriptor can be customized with independent schema and validation\nlogic:\n.. code-block:: python\n\n\n```import typing as t\nimport numpy as np\nfrom pydantic import BaseModel\n\nfrom bentoml.io import NumpyNdarray, Json\n\nclass FooModel(BaseModel):\n    field1: int\n    field2: float\n    field3: str\n\nmy_np_input = NumpyNdarray.from_sample(np.ndarray(...))\n\n@svc.api(\n    input=Multipart(\n        arr=NumpyNdarray(schema=np.dtype(int, 4), validate=True),\n        json=Json(pydantic_model=FooModel),\n    )\n    output=NumpyNdarray(schema=np.dtype(int), validate=True),\n)\ndef predict(arr: np.ndarray, json: t.Dict[str, t.Any]) -> np.ndarray:\n    ...\n```\n\n\nSync vs Async APIs\nAPIs can be defined as either synchronous function or asynchronous coroutines in Python.\nThe API we created in the :doc:`tutorial </tutorial>` was a synchronous API. BentoML will\nintelligently create an optimally sized pool of workers to execute the synchronous\nlogic. Synchronous APIs are simple and capable of getting the job done for most model\nserving scenarios.\n.. code-block:: python\n\n\n```@svc.api(input=NumpyNdarray(), output=NumpyNdarray())\ndef predict(input_array: np.ndarray) -> np.ndarray:\n    result = runner.run(input_array)\n    return result\n```\n\n\nSynchronous APIs fall short when we want to maximize the performance and throughput of\nthe service. Asynchronous APIs are preferred if the processing logic is IO-bound or\ninvokes multiple runners simultaneously. The following async API example calls a remote\nfeature store asynchronously, invokes two runners simultaneously, and returns a combined\nresult.\n.. code-block:: python\n\n\n```import aiohttp\nimport asyncio\n\n# Load two runners for two different versions of the ScikitLearn\n# Iris Classifier models we saved before\nrunner1 = bentoml.sklearn.get(\"iris_clf:yftvuwkbbbi6zc\").to_runner()\nrunner2 = bentoml.sklearn.get(\"iris_clf:edq3adsfhzi6zg\").to_runner()\n\n@svc.api(input=NumpyNdarray(), output=NumpyNdarray())\nasync def predict(input_array: np.ndarray) -> np.ndarray:\n    # Call a remote feature store to pre-process the request\n    async with aiohttp.ClientSession() as session:\n        async with session.get('https://features/get', params=input_array[0]) as resp:\n            features = get_features(await resp.text())\n\n    # Invoke both model runners simultaneously\n    results = await asyncio.gather(\n        runner1.predict.async_run(input_array, features),\n        runner2.predict.async_run(input_array, features),\n    )\n    return combine_results(results)\n```\n\n\nThe asynchronous API implementation is more efficient because when an asynchronous\nmethod is invoked, the event loop is released to service other requests while this\nrequest awaits the results of the method. In addition, BentoML will automatically\nconfigure the ideal amount of parallelism based on the available number of CPU cores.\nFurther tuning of event loop configuration is not needed under common use cases.\n.. tip::\n    Blocking logic such as communicating with an API or database without the `await`\n    keyword will block the event loop and prevent it from completing other IO tasks.\n    If you must use a library that does not support asynchronous IO with `await`, you\n    should use the synchronous API instead. If you are not sure, also use the synchronous\n    API to prevent unexpected errors.\n.. TODO:\n\n\n```Running Server:\n    bentoml serve arguments\n    --reload\n    --production\n\n    other options and configs:\n    --api-workers\n    --backlog\n    --timeout\n    --host\n    --port\n\n    Config options:\n    --config\n\nEndpoints:\n    List of Endpoints\n        POST: /{api_name}\n    Open API (Swagger) generation and sample usage\n\nException handling\n    custom error code\n    custom error msg\n```\n\n",
    "tag": "bentoml"
  },
  {
    "title": "deploy.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/concepts/deploy.rst",
    "content": "===============\nDeploying Bento\n===============\nDeployment Overview\nThe three most common deployment options with BentoML are:\n\n\ud83d\udc33 Generate container images from Bento for custom docker deployment\n`\ud83e\udd84\ufe0f Yatai <https://github.com/bentoml/Yatai>`_: Model Deployment at scale on Kubernetes\n`\ud83d\ude80 bentoctl <https://github.com/bentoml/bentoctl>`_: Fast model deployment on any cloud platform\n\nContainerize Bentos\nContainerizing bentos as Docker images allows users to easily distribute and deploy\nbentos. Once services are built as bentos and saved to the bento store, we can\ncontainerize saved bentos with the CLI command :ref:`bentoml containerize <reference/cli:containerize>`.\nStart the Docker engine. Verify using `docker info`.\n.. code-block:: bash\n\n\n```$ docker info\n```\n\n\nRun `bentoml list` to view available bentos in the store.\n.. code-block:: bash\n\n\n```$ bentoml list\n\nTag                               Size        Creation Time        Path\niris_classifier:ejwnswg5kw6qnuqj  803.01 KiB  2022-05-27 00:37:08  ~/bentoml/bentos/iris_classifier/ejwnswg5kw6qnuqj\niris_classifier:h4g6jmw5kc4ixuqj  644.45 KiB  2022-05-27 00:02:08  ~/bentoml/bentos/iris_classifier/h4g6jmw5kc4ixuqj\n```\n\n\nRun `bentoml containerize` to start the containerization process.\n.. code-block:: bash\n\n\n```$ bentoml containerize iris_classifier:latest\n\nINFO [cli] Building docker image for Bento(tag=\"iris_classifier:ejwnswg5kw6qnuqj\")...\n[+] Building 21.2s (20/20) FINISHED\n...\nINFO [cli] Successfully built docker image \"iris_classifier:ejwnswg5kw6qnuqj\"\n```\n\n\n.. dropdown:: For Mac with Apple Silicon\n   :icon: cpu\nSpecify the :code:`--platform` to avoid potential compatibility issues with some\n   Python libraries.\n.. code-block:: bash\n\n\n```  $ bentoml containerize --opt platform=linux/amd64 iris_classifier:latest\n```\n\n\nView the built Docker image:\n.. code-block:: bash\n\n\n```$ docker images\n\nREPOSITORY          TAG                 IMAGE ID       CREATED         SIZE\niris_classifier     ejwnswg5kw6qnuqj    669e3ce35013   1 minutes ago   1.12GB\n```\n\n\nRun the generated docker image:\n.. code-block:: bash\n\n\n```$ docker run -p 3000:3000 iris_classifier:ejwnswg5kw6qnuqj serve --production\n```\n\n\n.. seealso::\n:ref:`guides/containerization:Containerization with different container engines.`\n   goes into more details on our containerization process and how to use different container runtime.\n.. todo::\n\n\n```- Add sample code for working with GPU and --gpu flag\n```\n\n\nDeploy with Yatai\nYatai helps ML teams to deploy large scale model serving workloads on Kubernetes. It\nstandardizes BentoML deployment on Kubernetes, provides UI and APis for managing all\nyour ML models and deployments in one place, and enables advanced GitOps and CI/CD\nworkflows.\nYatai is Kubernetes native, integrates well with other cloud native tools in the K8s\neco-system.\nTo get started, get an API token from Yatai Web UI and login from your :code:`bentoml`\nCLI command:\n.. code-block:: bash\n\n\n```bentoml yatai login --api-token {YOUR_TOKEN_GOES_HERE} --endpoint http://yatai.127.0.0.1.sslip.io\n```\n\n\nPush your local Bentos to yatai:\n.. code-block:: python\n\n\n```bentoml push iris_classifier:latest\n```\n\n\n.. tip::\n    Yatai will automatically start building container images for a new Bento pushed.\nDeploy via Web UI\n^^^^^^^^^^^^^^^^^\nAlthough not always recommended for production workloads, Yatai offers an easy-to-use\nweb UI for quickly creating deployments. This is convenient for data scientists to test\nout Bento deployments end-to-end from a development or testing environment:\n.. image:: /_static/img/yatai-deployment-creation.png\n    :alt: Yatai Deployment creation UI\nThe web UI is also very helpful for viewing system status, monitoring services, and\ndebugging issues.\n.. image:: /_static/img/yatai-deployment-details.png\n    :alt: Yatai Deployment Details UI\nCommonly we recommend using APIs or Kubernetes CRD objects to automate the deployment\npipeline for production workloads.\nDeploy via API\n^^^^^^^^^^^^^^\nYatai's REST API specification can be found under the :code:`/swagger` endpoint. If you\nhave Yatai deployed locally with minikube, visit:\nhttp://yatai.127.0.0.1.sslip.io/swagger/. The Swagger API spec covers all core Yatai\nfunctionalities ranging from model/bento management, cluster management to deployment\nautomation.\n.. note::\n\n\n```Python APIs for creating deployment on Yatai is on our roadmap. See :issue:`2405`.\nCurrent proposal looks like this:\n\n.. code-block:: python\n\n    yatai_client = bentoml.YataiClient.from_env()\n\n    bento = yatai_client.get_bento('my_svc:v1')\n    assert bento and bento.status.is_ready()\n\n    yatai_client.create_deployment('my_deployment', bento.tag, ...)\n\n    # For updating a deployment:\n    yatai_client.update_deployment('my_deployment', bento.tag)\n\n    # check deployment_info.status\n    deployment_info = yatai_client.get_deployment('my_deployment')\n```\n\n\nDeploy via kubectl and CRD\n^^^^^^^^^^^^^^^^^^^^^^^^^^\nFor DevOps managing production model serving workloads along with other kubernetes\nresources, the best option is to use :code:`kubectl` and directly create\n:code:`BentoDeployment` objects in the cluster, which will be handled by the Yatai\ndeployment CRD controller.\n.. code-block:: yaml\n\n\n```# my_deployment.yaml\napiVersion: serving.yatai.ai/v1alpha2\nkind: BentoDeployment\nmetadata:\n  name: demo\nspec:\n  bento_tag: iris_classifier:3oevmqfvnkvwvuqj\n  resources:\n    limits:\n      cpu: 1000m\n    requests:\n      cpu: 500m\n```\n\n\n.. code-block:: bash\n\n\n```kubectl apply -f my_deployment.yaml\n```\n\n\nDeploy with bentoctl\n:code:`bentoctl` is a CLI tool for deploying Bentos to run on any cloud platform. It\nsupports all major cloud providers, including AWS, Azure, Google Cloud, and many more.\nUnderneath, :code:`bentoctl` is powered by Terraform. :code:`bentoctl` adds required\nmodifications to Bento or service configurations, and then generate terraform templates\nfor the target deploy platform for easy deployment.\nThe :code:`bentoctl` deployment workflow is optimized for CI/CD and GitOps. It is highly\ncustomizable, users can fine-tune all configurations provided by the cloud platform. It\nis also extensible, for users to define additional terraform templates to be attached\nto a deployment.\nQuick Tour\n^^^^^^^^^^\nInstall aws-lambda plugin for :code:`bentoctl` as an example:\n.. code-block:: bash\n\n\n```bentoctl operator install aws-lambda\n```\n\n\nInitialize a bentoctl project. This enters an interactive mode asking users for related\ndeployment configurations:\n.. code-block:: bash\n\n\n```$ bentoctl init\n\nBentoctl Interactive Deployment Config Builder\n...\n\ndeployment config generated to: deployment_config.yaml\n\u2728 generated template files.\n  - bentoctl.tfvars\n  - main.tf\n```\n\n\nDeployment config will be saved to :code:`./deployment_config.yaml`:\n.. code-block:: yaml\n\n\n```api_version: v1\nname: quickstart\noperator:\n    name: aws-lambda\ntemplate: terraform\nspec:\n    region: us-west-1\n    timeout: 10\n    memory_size: 512\n```\n\n\nNow, we are ready to build the deployable artifacts required for this deployment. In\nmost cases, this step will product a new docker image specific to the target deployment\nconfiguration:\n.. code-block:: bash\n\n\n```bentoctl build -b iris_classifier:btzv5wfv665trhcu -f ./deployment_config.yaml\n```\n\n\nNext step, use :code:`terraform` CLI command to apply the generated deployment configs\nto AWS. This will require user setting up AWS credentials on the environment.\n.. code-block:: bash\n\n\n```$ terraform init\n$ terraform apply -var-file=bentoctl.tfvars --auto-approve\n\n...\nbase_url = \"https://ka8h2p2yfh.execute-api.us-west-1.amazonaws.com/\"\nfunction_name = \"quickstart-function\"\nimage_tag = \"192023623294.dkr.ecr.us-west-1.amazonaws.com/quickstart:btzv5wfv665trhcu\"\n```\n\n\nTesting the endpoint deployed:\n.. code-block:: bash\n\n\n```URL=$(terraform output -json | jq -r .base_url.value)classify\ncurl -i \\\n    --header \"Content-Type: application/json\" \\\n    --request POST \\\n    --data '[5.1, 3.5, 1.4, 0.2]' \\\n    $URL\n```\n\n\nSupported Cloud Platforms\n^^^^^^^^^^^^^^^^^^^^^^^^^\n\nAWS Lambda: https://github.com/bentoml/aws-lambda-deploy\nAWS SageMaker: https://github.com/bentoml/aws-sagemaker-deploy\nAWS EC2: https://github.com/bentoml/aws-ec2-deploy\nGoogle Cloud Run: https://github.com/bentoml/google-cloud-run-deploy\nGoogle Compute Engine: https://github.com/bentoml/google-compute-engine-deploy\nAzure Functions: https://github.com/bentoml/azure-functions-deploy\nAzure Container Instances: https://github.com/bentoml/azure-container-instances-deploy\nHeroku: https://github.com/bentoml/heroku-deploy\n\n.. TODO::\n    Explain limitations of each platform, e.g. GPU support\n    Explain how to customize the terraform workflow\nAbout Horizontal Auto-scaling\nAuto-scaling is one of the most sought-after features when it comes to deploying models. Autoscaling helps optimize resource usage and cost by automatically provisioning up and scaling down depending on incoming traffic.\nAmong deployment options introduced in this guide, Yatai on Kubernetes is the\nrecommended approach if auto-scaling and resource efficiency are required for your team\u2019s workflow.\nYatai enables users to fine-tune resource requirements and\nauto-scaling policy at the Runner level, which inherently improves interoperability between auto-scaling and data aggregated at Runner's adaptive batching layer in real-time.\nMany of bentoctl\u2019s deployment targets also come with a certain level of auto-scaling",
    "tag": "bentoml"
  },
  {
    "title": "runner.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/concepts/runner.rst",
    "content": "=============\nUsing Runners\n=============\ntime expected: 15 minutes\nThis page articulates on the concept of Runners and demonstrates its role within the\nBentoML architecture.\nWhat is Runner?\nIn BentoML, Runner represents a unit of computation that can be executed on a remote\nPython worker and scales independently.\nRunner allows :ref:`bentoml.Service <reference/core:bentoml.Service>` to parallelize\nmultiple instances of a :ref:`bentoml.Runnable <reference/core:bentoml.Runnable>` class,\neach on its own Python worker. When a BentoServer is launched, a group of runner worker\nprocesses will be created, and :code:`run` method calls made from the\n:code:`bentoml.Service` code will be scheduled among those runner workers.\nRunner also supports :doc:`/guides/batching`. For a\n:ref:`bentoml.Runnable <reference/core:bentoml.Runnable>` configured with batching,\nmultiple :code:`run` method invocations made from other processes can be dynamically\ngrouped into one batch execution in real-time. This is especially beneficial for compute\nintensive workloads such as model inference, helps to bring better performance through\nvectorization or multi-threading.\nPre-built Model Runners\nBentoML provides pre-built Runners implemented for each ML framework supported. These\npre-built runners are carefully configured to work well with each specific ML framework.\nThey handle working with GPU when GPU is available, set the number of threads and number\nof workers automatically, and convert the model signatures to corresponding Runnable\nmethods.\n.. code:: python\n\n\n```trained_model = train()\n\nbentoml.pytorch.save_model(\n    \"demo_mnist\",  # model name in the local model store\n    trained_model,  # model instance being saved\n    signatures={   # model signatures for runner inference\n        \"predict\": {\n            \"batchable\": True,\n            \"batch_dim\": 0,\n        }\n    }\n)\n\nrunner = bentoml.pytorch.get(\"demo_mnist:latest\").to_runner()\nrunner.init_local()\nrunner.predict.run( MODEL_INPUT )\n```\n\n\n.. _custom-runner:\nCustom Runner\nFor more advanced use cases, BentoML also allows users to define their own Runner\nclasses. This is useful when the pre-built Runners do not meet the requirements, or\nwhen the user wants to implement a Runner for a new ML framework.\nCreating a Runnable\n^^^^^^^^^^^^^^^^^^^\nRunner can be created from a :ref:`bentoml.Runnable <reference/core:bentoml.Runnable>`\nclass. By implementing a :code:`Runnable` class, users can create Runner instances that\nruns custom logic. Here's an example, creating an NLTK runner that does sentiment\nanalysis with a pre-trained model:\n.. literalinclude:: ../../../examples/custom_runner/nltk_pretrained_model/service.py\n   :language: python\n   :caption: `service.py`\n.. note::\n\n\n```Full code example can be found :github:`here <bentoml/BentoML/tree/main/examples/custom_runner/nltk_pretrained_model>`.\n```\n\n\nThe constant attribute `SUPPORTED_RESOURCES` indicates which resources this Runnable class\nimplementation supports. The only currently pre-defined resources are `\"cpu\"` and\n`\"nvidia.com/gpu\"`.\nThe constant attribute `SUPPORTS_CPU_MULTI_THREADING` indicates whether or not the runner supports\nCPU multi-threading.\n.. tip::\n\n\n```Neither constant can be set inside of the runner's ``__init__`` or ``__new__`` methods, as they are class-level attributes. The reason being BentoML\u2019s scheduling policy is not invoked in runners\u2019 initialization code, as instantiating runners can be quite expensive.\n```\n\n\nSince NLTK library doesn't support utilizing GPU or multiple CPU cores natively, supported resources\nis specified as :code:`(\"cpu\",)`, and `SUPPORTS_CPU_MULTI_THREADING` is set to False. This is the default configuration.\nThis information is then used by the BentoServer scheduler to determine the worker pool size for this runner.\nThe :code:`bentoml.Runnable.method` decorator is used for creating\n:code:`RunnableMethod` - the decorated method will be exposed as the runner interface\nfor accessing remotely. :code:`RunnableMethod` can be configured with a signature,\nwhich is defined same as the :ref:`concepts/model:Model Signatures`.\nReusable Runnable\n^^^^^^^^^^^^^^^^^\nRunnable class can also take :code:`__init__` parameters to customize its behavior for\ndifferent scenarios. The same Runnable class can also be used to create multiple runners\nand used in the same service. For example:\n.. code-block:: python\n   :caption: `service.py`\n\n\n```import bentoml\nimport torch\n\nclass MyModelRunnable(bentoml.Runnable):\n    SUPPORTED_RESOURCES = (\"nvidia.com/gpu\",)\n    SUPPORTS_CPU_MULTI_THREADING = True\n\n    def __init__(self, model_file):\n        self.model = torch.load_model(model_file)\n\n    @bentoml.Runnable.method(batchable=True, batch_dim=0)\n    def predict(self, input_tensor):\n        return self.model(input_tensor)\n\nmy_runner_1 = bentoml.Runner(\n    MyModelRunnable,\n    name=\"my_runner_1\",\n    runnable_init_params={\n        \"model_file\": \"./saved_model_1.pt\",\n    }\n)\nmy_runner_2 = bentoml.Runner(\n    MyModelRunnable,\n    name=\"my_runner_2\",\n    runnable_init_params={\n        \"model_file\": \"./saved_model_2.pt\",\n    }\n)\n\nsvc = bentoml.Service(__name__, runners=[my_runner_1, my_runner_2])\n```\n\n\n.. epigraph::\n    All runners presented in one `bentoml.Service` object must have unique names.\n.. note::\n\n\n```The default Runner name is the Runnable class name. When using the same Runnable\nclass to create multiple runners and use them in the same service, user must rename\nrunners by specifying the ``name`` parameter when creating the runners. Runner\nname are a key to configuring individual runner at deploy time and to runner related\nlogging and tracing features.\n```\n\n\nCustom Model Runner\n^^^^^^^^^^^^^^^^^^^\nCustom Runnable built with Model from BentoML's model store:\n.. code::\n\n\n```from typing import Any\n\nimport bentoml\nfrom bentoml.io import JSON\nfrom bentoml.io import NumpyNdarray\nfrom numpy.typing import NDArray\n\nbento_model = bentoml.pytorch.get(\"spam_detection:latest\")\n\nclass SpamDetectionRunnable(bentoml.Runnable):\n    SUPPORTED_RESOURCES = (\"cpu\",)\n    SUPPORTS_CPU_MULTI_THREADING = True\n\n    def __init__(self):\n        # load the model instance\n        self.classifier = bentoml.sklearn.load_model(bento_model)\n\n    @bentoml.Runnable.method(batchable=False)\n    def is_spam(self, input_data: NDArray[Any]) -> NDArray[Any]:\n        return self.classifier.predict(input_data)\n\nspam_detection_runner = bentoml.Runner(SpamDetectionRunnable, models=[bento_model])\nsvc = bentoml.Service(\"spam_detector\", runners=[spam_detection_runner])\n\n@svc.api(input=NumpyNdarray(), output=JSON())\ndef analysis(input_text: NDArray[Any]) -> dict[str, Any]:\n    return {\"res\": spam_detection_runner.is_spam.run(input_text)}\n```\n\n\nServing Multiple Models via Runner\nServing multiple models in the same workflow is also a common pattern in BentoML\u2019s prediction framework. This pattern can be achieved by simply instantiating multiple runners up front and passing them to the service that\u2019s being created. Each runner/model will be configured with its\u2019 own resources and run autonomously. If no configuration is passed, BentoML will then determine the optimal resources to allocate to each runner.\nSequential Runs\n^^^^^^^^^^^^^^^\n.. code:: python\n\n\n```import asyncio\nimport bentoml\nimport PIL.Image\n\nimport bentoml\nfrom bentoml.io import Image, Text\n\ntransformers_runner = bentoml.transformers.get(\"sentiment_model:latest\").to_runner()\nocr_runner = bentoml.easyocr.get(\"ocr_model:latest\").to_runner()\n\nsvc = bentoml.Service(\"sentiment_analysis\", runners=[transformers_runner, ocr_runner])\n\n@svc.api(input=Image(),output=Text())\ndef classify(input: PIL.Image.Image) -> str:\n    ocr_text = ocr_runner.run(input)\n    return transformers_runner.run(ocr_text)\n```\n\n\nIt\u2019s as simple as creating two runners and invoking them synchronously in your prediction endpoint. Note that an async endpoint is often preferred in these use cases as the primary event loop is yielded while waiting for other IO-expensive tasks. \nFor example, the same API above can be achieved as an `async` endpoint:\n.. code:: python\n\n\n```@svc.api(input=Image(),output=Text())\nasync def classify_async(input: PIL.Image.Image) -> str:\n    ocr_text = await ocr_runner.async_run(input)\n    return await transformers_runner.async_run(ocr_text)\n```\n\n\nConcurrent Runs\n^^^^^^^^^^^^^^^\nIn cases where certain steps can be executed concurrently, :code:`asyncio.gather` can be used to aggregate results from multiple concurrent runs. For instance, if you are running two models simultaneously, you could invoke `asyncio.gather` as follows:\n.. code-block:: python\n\n\n```import asyncio\nimport PIL.Image\n\nimport bentoml\nfrom bentoml.io import Image, Text\n\npreprocess_runner = bentoml.Runner(MyPreprocessRunnable)\nmodel_a_runner = bentoml.xgboost.get('model_a:latest').to_runner()\nmodel_b_runner = bentoml.pytorch.get('model_b:latest').to_runner()\n\nsvc = bentoml.Service('inference_graph_demo', runners=[\n    preprocess_runner,\n    model_a_runner,\n    model_b_runner\n])\n\n@svc.api(input=Image(), output=Text())\nasync def predict(input_image: PIL.Image.Image) -> str:\n    model_input = await preprocess_runner.async_run(input_image)\n\n    results = await asyncio.gather(\n        model_a_runner.async_run(model_input),\n        model_b_runner.async_run(model_input),\n    )\n\n    return post_process(\n        results[0], # model a result\n        results[1], # model b result\n    )\n```\n\n\nOnce each model completes, the results can be compared and logged as a post processing\nstep.\nRunner Definition\n.. TODO::\n    Document detailed list of Runner options\n\n\n```.. code:: python\n\n    my_runner = bentoml.Runner(\n        MyRunnable,\n        runnable_init_params={\"foo\": foo, \"bar\": bar},\n        name=\"custom_runner_name\",\n        strategy=None, # default strategy will be selected depending on the SUPPORTED_RESOURCES and SUPPORTS_CPU_MULTI_THREADING flag on runnable\n        models=[..],\n\n        # below are also configurable via config file:\n\n        # default configs:\n        max_batch_size=..  # default max batch size will be applied to all run methods, unless override in the runnable_method_configs\n        max_latency_ms=.. # default max latency will be applied to all run methods, unless override in the runnable_method_configs\n\n        runnable_method_configs=[\n            {\n                method_name=\"predict\",\n                max_batch_size=..,\n                max_latency_ms=..,\n            }\n        ],\n    )\n```\n\n\nRunner Configuration\nRunner behaviors and resource allocation can be specified via BentoML :ref:`configuration <guides/configuration:Configuration>`.\nRunners can be both configured individually or in aggregate under the `runners` configuration key. To configure a specific runner, specify its name\nunder the `runners` configuration key. Otherwise, the configuration will be applied to all runners. The examples below demonstrate both\nthe configuration for all runners in aggregate and for an individual runner (`iris_clf`).\nAdaptive Batching\n^^^^^^^^^^^^^^^^^\nIf a model or custom runner supports batching, the :ref:`adaptive batching <guides/batching:Adaptive Batching>` mechanism is enabled by default.\nTo explicitly disable or control adaptive batching behaviors at runtime, configuration can be specified under the `batching` key.\n.. tab-set::\n\n\n```.. tab-item:: All Runners\n   :sync: all_runners\n\n   .. code-block:: yaml\n      :caption: \u2699\ufe0f `configuration.yml`\n\n      runners:\n        batching:\n          enabled: true\n          max_batch_size: 100\n          max_latency_ms: 500\n\n.. tab-item:: Individual Runner\n    :sync: individual_runner\n\n    .. code-block:: yaml\n       :caption: \u2699\ufe0f `configuration.yml`\n\n       runners:\n         iris_clf:\n           batching:\n             enabled: true\n             max_batch_size: 100\n             max_latency_ms: 500\n```\n\n\nResource Allocation\n^^^^^^^^^^^^^^^^^^^\nBy default, a runner will attempt to utilize all available resources in the container. Runner's resource allocation can also be customized\nthrough configuration, with a `float` value for `cpu` and an `int` value for `nvidia.com/gpu`. Fractional GPU is currently not supported.\n.. tab-set::\n\n\n```.. tab-item:: All Runners\n   :sync: all_runners\n\n   .. code-block:: yaml\n      :caption: \u2699\ufe0f `configuration.yml`\n\n      runners:\n        resources:\n          cpu: 0.5\n          nvidia.com/gpu: 1\n\n.. tab-item:: Individual Runner\n    :sync: individual_runner\n\n    .. code-block:: yaml\n       :caption: \u2699\ufe0f `configuration.yml`\n\n       runners:\n         iris_clf:\n           resources:\n             cpu: 0.5\n             nvidia.com/gpu: 1\n```\n\n\nAlternatively, a runner can be mapped to a specific set of GPUs. To specify GPU mapping, instead of defining an `integer` value, a list of device IDs\ncan be specified for the `nvidia.com/gpu` key. For example, the following configuration maps the configured runners to GPU device 2 and 4.\n.. tab-set::\n\n\n```.. tab-item:: All Runners\n   :sync: all_runners\n\n   .. code-block:: yaml\n      :caption: \u2699\ufe0f `configuration.yml`\n\n      runners:\n        resources:\n          nvidia.com/gpu: [2, 4]\n\n.. tab-item:: Individual Runner\n   :sync: individual_runner\n\n   .. code-block:: yaml\n      :caption: \u2699\ufe0f `configuration.yml`\n\n      runners:\n        iris_clf:\n          resources:\n            nvidia.com/gpu: [2, 4]\n```\n\n\nTimeout\n^^^^^^^\nRunner timeout defines the amount of time in seconds to wait before calls a runner is timed out on the API server.\n.. tab-set::\n\n\n```.. tab-item:: All Runners\n   :sync: all_runners\n\n   .. code-block:: yaml\n      :caption: \u2699\ufe0f `configuration.yml`\n\n      runners:\n        timeout: 60\n\n.. tab-item:: Individual Runner\n   :sync: individual_runner\n\n   .. code-block:: yaml\n      :caption: \u2699\ufe0f `configuration.yml`\n\n      runners:\n        iris_clf:\n          timeout: 60\n```\n\n\nAccess Logging\n^^^^^^^^^^^^^^\nSee :ref:`guides/logging:Logging Configuration` for access log customization.\nDistributed Runner with Yatai\n`\ud83e\udd84\ufe0f Yatai <https://github.com/bentoml/Yatai>`_ provides a more advanced Runner\narchitecture specifically designed for running large scale inference workloads on a\nKubernetes cluster.\nWhile the standalone :code:`BentoServer` schedules Runner workers on their own Python\nprocesses, the :code:`BentoDeployment` created by Yatai, scales Runner workers in their\nown group of `Pods <https://kubernetes.io/docs/concepts/workloads/pods/>`_ and made it\npossible to set a different resource requirement for each Runner, and auto-scaling each\nRunner separately based on their workloads.\nSample :code:`BentoDeployment` definition file for deploying in Kubernetes:\n.. code:: yaml\n\n\n```apiVersion: yatai.bentoml.org/v1beta1\nkind: BentoDeployment\nspec:\nbento_tag: 'fraud_detector:dpijemevl6nlhlg6'\nautoscaling:\n    minReplicas: 3\n    maxReplicas: 20\nresources:\n    limits:\n        cpu: 500m\n    requests:\n        cpu: 200m\nrunners:\n- name: model_runner_a\n    autoscaling:\n        minReplicas: 1\n        maxReplicas: 5\n    resources:\n        requests:\n            nvidia.com/gpu: 1\n            cpu: 2000m\n        ...\n```\n\n\n.. TODO::",
    "tag": "bentoml"
  },
  {
    "title": "bento.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/concepts/bento.rst",
    "content": "===============\nBuilding Bentos\n===============\nWhat is a Bento?\n:ref:`Bento \ud83c\udf71 <reference/core:bentoml.Bento>` is a file archive with all the source\ncode, models, data files and dependency configurations required for running a\nuser-defined :ref:`reference/core:bentoml.Service`, packaged into a standardized format.\nWhile `bentoml.Service` standardizes the inference API definition, including the\nserving logic, runners initialization and API input, output types.\n`Bento` standardizes how to reproduce the required environment for running a\n`bentoml.Service` in production.\n.. note::\n    \"Bento Build\" is essentially the build process in traditional software development,\n    where source code files were converted into standalone artifacts that are ready to\n    deploy. BentoML reimagined this process for Machine Learning model delivery, and\n    optimized the workflow both for interactive model development and for working with\n    automated training pipelines.\nThe Build Command\nA Bento can be created with the :ref:`bentoml build <reference/cli:build>` CLI command\nwith a `bentofile.yaml` build file. Here's an example from the\n:doc:`tutorial </tutorial>`:\n.. code-block:: yaml\n\n\n```service: \"service:svc\"  # Same as the argument passed to `bentoml serve`\nlabels:\n    owner: bentoml-team\n    stage: dev\ninclude:\n- \"*.py\"  # A pattern for matching which files to include in the bento\npython:\n    packages:  # Additional pip packages required by the service\n    - scikit-learn\n    - pandas\n```\n\n\n.. code-block:: bash\n\n\n```\u00bb bentoml build\n\nBuilding BentoML service \"iris_classifier:dpijemevl6nlhlg6\" from build context \"/home/user/gallery/quickstart\"\nPacking model \"iris_clf:zy3dfgxzqkjrlgxi\"\nLocking PyPI package versions..\n\n\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2557\u2591\u2591\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2591\u2588\u2588\u2588\u2588\u2588\u2557\u2591\u2588\u2588\u2588\u2557\u2591\u2591\u2591\u2588\u2588\u2588\u2557\u2588\u2588\u2557\u2591\u2591\u2591\u2591\u2591\n\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2588\u2588\u2557\u2591\u2588\u2588\u2551\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2557\u2591\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551\u2591\u2591\u2591\u2591\u2591\n\u2588\u2588\u2588\u2588\u2588\u2588\u2566\u255d\u2588\u2588\u2588\u2588\u2588\u2557\u2591\u2591\u2588\u2588\u2554\u2588\u2588\u2557\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2588\u2588\u2551\u2588\u2588\u2554\u2588\u2588\u2588\u2588\u2554\u2588\u2588\u2551\u2588\u2588\u2551\u2591\u2591\u2591\u2591\u2591\n\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u255d\u2591\u2591\u2588\u2588\u2551\u255a\u2588\u2588\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2551\u2591\u2591\u2591\u2591\u2591\n\u2588\u2588\u2588\u2588\u2588\u2588\u2566\u255d\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551\u2591\u255a\u2588\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2591\u255a\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2591\u255a\u2550\u255d\u2591\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\n\u255a\u2550\u2550\u2550\u2550\u2550\u255d\u2591\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d\u2591\u2591\u255a\u2550\u2550\u255d\u2591\u2591\u2591\u255a\u2550\u255d\u2591\u2591\u2591\u2591\u255a\u2550\u2550\u2550\u2550\u255d\u2591\u255a\u2550\u255d\u2591\u2591\u2591\u2591\u2591\u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\nSuccessfully built Bento(tag=\"iris_classifier:dpijemevl6nlhlg6\")\n```\n\n\nSimilar to :doc:`saving a model </concepts/model>`, a unique version tag will be\nautomatically generated for the newly created Bento.\nIt is also possible to customize the Bento version string by specifying it in the\n:code:`--version` CLI argument. However this is generally not recommended. Only use it\nif your team has a very specific naming convention for deployable artifacts, e.g.:\n.. code-block:: bash\n\n\n```\u00bb bentoml build --version 1.0.1\n```\n\n\n.. note::\n\n\n```The Bento build process requires importing the ``bentoml.Service`` object\ndefined. This means, the build environment must have all its dependencies installed.\nSupport for building from a docker environment is on the roadmap, see :issue:`2495`.\n```\n\n\nAdvanced Project Structure\n^^^^^^^^^^^^^^^^^^^^^^^^^^\nFor projects that are part of a larger codebase and interacts with other local python\nmodules; Or for projects containing multiple Bentos/Services, it may not be possible to\nput all service definition code and `bentofile.yaml` under the project's root\ndirectory.\nBentoML allows placing the service definition file and bentofile anywhere in the project\ndirectory. In this case, the user needs to provide the `build_ctx` and\n`bentofile` argument to the `bentoml build` CLI command.\nbuild_ctx\n    Build context is your Python project's working directory. This is from where you\n    start the Python interpreter during development so that your local python modules\n    can be imported properly. Default to current directory where the\n    `bentoml build` takes place.\nbentofile\n    `bentofile` is a `.yaml` file that specifies the\n    :ref:`concepts/bento:Bento Build Options`. Default to the `bentofile.yaml`\n    file under the build context.\nThey can also be customized via the CLI command, e.g.:\n.. code-block:: bash\n\n\n```\u00bb bentoml build -f ./src/my_project_a/bento_fraud_detect.yaml ./src/\n```\n\n\nManaging Bentos\nBentos are the unit of deployment in BentoML, one of the most important artifact to keep\ntrack of for your model deployment workflow.\nLocal Bento Store\n^^^^^^^^^^^^^^^^^\nSimilar to Models, Bentos built locally can be managed via the\n:doc:`bentoml CLI commands </reference/cli>`:\n.. tab-set::\n\n\n```.. tab-item:: List\n\n   .. code-block:: bash\n\n      \u00bb bentoml list\n\n      Tag                               Size        Creation Time        Path\n      iris_classifier:nvjtj7wwfgsafuqj  16.99 KiB   2022-05-17 21:36:36  ~/bentoml/bentos/iris_classifier/nvjtj7wwfgsafuqj\n      iris_classifier:jxcnbhfv6w6kvuqj  19.68 KiB   2022-04-06 22:02:52  ~/bentoml/bentos/iris_classifier/jxcnbhfv6w6kvuqj\n\n.. tab-item:: Get\n\n   .. code-block:: bash\n\n      \u00bb bentoml get iris_classifier:latest\n\n      service: service:svc\n      name: iris_classifier\n      version: nvjtj7wwfgsafuqj\n      bentoml_version: 1.0.0\n      creation_time: '2022-05-17T21:36:36.436878+00:00'\n      labels:\n        owner: bentoml-team\n        project: gallery\n      models:\n      - tag: iris_clf:nb5vrfgwfgtjruqj\n        module: bentoml.sklearn\n        creation_time: '2022-05-17T21:36:27.656424+00:00'\n      runners:\n      - name: iris_clf\n        runnable_type: SklearnRunnable\n        models:\n        - iris_clf:nb5vrfgwfgtjruqj\n        resource_config:\n          cpu: 4.0\n          nvidia_gpu: 0.0\n      apis:\n      - name: classify\n        input_type: NumpyNdarray\n        output_type: NumpyNdarray\n\n\n.. tab-item:: Delete\n\n   .. code-block:: bash\n\n      \u00bb bentoml delete iris_classifier:latest -y\n\n      Bento(tag=\"iris_classifier:nvjtj7wwfgsafuqj\") deleted\n```\n\n\nImport and Export\n^^^^^^^^^^^^^^^^^\nBentos can be exported to a standalone archive file outside of the store, for sharing\nBentos between teams or moving between different deployment stages. For example:\n.. code:: bash\n\n\n```> bentoml export iris_classifier:latest .\n\nINFO [cli] Bento(tag=\"iris_classifier:nvjtj7wwfgsafuqj\") exported to ./iris_classifier-nvjtj7wwfgsafuqj.bento\n```\n\n\n.. code:: bash\n\n\n```> bentoml import ./iris_classifier-nvjtj7wwfgsafuqj.bento\n\nINFO [cli] Bento(tag=\"iris_classifier:nvjtj7wwfgsafuqj\") imported\n```\n\n\n.. note::\n\n\n```Bentos can be exported to or import from AWS S3, GCS, FTP, Dropbox, etc. For\nexample with S3:\n\n.. code:: bash\n\n    pip install fs-s3fs  # Additional dependency required for working with s3\n    bentoml import s3://bentoml.com/quickstart/iris_classifier.bento\n    bentoml export iris_classifier:latest s3://my_bucket/my_prefix/\n```\n\n\nPush and Pull\n^^^^^^^^^^^^^\n`Yatai <https://github.com/bentoml/Yatai>`_ provides a centralized Bento repository\nthat comes with flexible APIs and Web UI for managing all Bentos created by your team.\nIt can be configured to store Bento files on cloud blob storage such as AWS S3, MinIO\nor GCS, and automatically build docker images when a new Bento was pushed.\n.. code-block:: bash\n\u00bb bentoml push iris_classifier:latest\nSuccessfully pushed Bento \"iris_classifier:nvjtj7wwfgsafuqj\"\n.. code-block:: bash\n\u00bb bentoml pull iris_classifier:nvjtj7wwfgsafuqj\nSuccessfully pulled Bento \"iris_classifier:nvjtj7wwfgsafuqj\"\n.. image:: /_static/img/yatai-bento-repos.png\n   :alt: Yatai Bento Repo UI\nBento Management API\n^^^^^^^^^^^^^^^^^^^^\nSimilar to :ref:`concepts/model:Managing Models`, equivalent Python APIs are also\nprovided for managing Bentos:\n.. tab-set::\n\n\n```.. tab-item:: Get\n\n    .. code-block:: python\n\n        import bentoml\n        bento = bentoml.get(\"iris_classifier:latest\")\n\n        print(bento.tag)\n        print(bento.path)\n        print(bento.info.to_dict())\n\n.. tab-item:: List\n\n    .. code-block:: python\n\n        import bentoml\n        bentos = bentoml.list()\n\n.. tab-item:: Import / Export\n\n    .. code-block:: python\n\n        import bentoml\n        bentoml.export_bento('my_bento:latest', '/path/to/folder/my_bento.bento')\n\n    .. code-block:: bash\n\n        bentoml.import_bento('/path/to/folder/my_bento.bento')\n\n    .. note::\n\n        Bentos can be exported to or import from AWS S3, GCS, FTP, Dropbox, etc. For\n        example: :code:`bentoml.export_bento('my_bento:latest', 's3://my_bucket/folder')`\n\n.. tab-item:: Push / Pull\n\n    If your team has `Yatai <https://github.com/bentoml/Yatai>`_ setup, you can also\n    push local Bentos to Yatai, it provides APIs and Web UI for managing all Bentos\n    created by your team, stores Bento files on cloud blob storage such as AWS S3, MinIO\n    or GCS, and automatically builds docker images when a new Bento was pushed.\n\n    .. code-block:: bash\n\n        import bentoml\n        bentoml.push(\"iris_classifier:nvjtj7wwfgsafuqj\")\n\n    .. code-block:: bash\n\n        bentoml.pull(\"iris_classifier:nvjtj7wwfgsafuqj\")\n\n.. tab-item:: Delete\n\n    .. code-block:: bash\n\n        import bentoml\n        bentoml.delete(\"iris_classifier:nvjtj7wwfgsafuqj\")\n```\n\n\nWhat's inside a Bento\n^^^^^^^^^^^^^^^^^^^^^\nIt is possible to view the generated files in a specific Bento. Simply use the\n:code:`-o/--output` option of the `bentoml get` command to find the file path to\nthe Bento archive directory.\n.. code-block:: bash\n\n\n```\u00bb cd $(bentoml get iris_classifier:latest -o path)\n\u00bb tree\n.\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 apis\n\u2502   \u2514\u2500\u2500 openapi.yaml\n\u251c\u2500\u2500 bento.yaml\n\u251c\u2500\u2500 env\n\u2502   \u251c\u2500\u2500 docker\n\u2502   \u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u2502   \u2514\u2500\u2500 entrypoint.sh\n\u2502   \u2514\u2500\u2500 python\n\u2502       \u251c\u2500\u2500 requirements.lock.txt\n\u2502       \u251c\u2500\u2500 requirements.txt\n\u2502       \u2514\u2500\u2500 version.txt\n\u251c\u2500\u2500 models\n\u2502    \u2514\u2500\u2500 iris_clf\n\u2502       \u251c\u2500\u2500 latest\n\u2502       \u2514\u2500\u2500 nb5vrfgwfgtjruqj\n\u2502           \u251c\u2500\u2500 model.yaml\n\u2502           \u2514\u2500\u2500 saved_model.pkl\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 locustfile.py\n    \u251c\u2500\u2500 service.py\n    \u2514\u2500\u2500 train.py\n```\n\n\n\n\n`src` directory contains files specified under the :ref:`include <concepts/bento:Files to include>` field in the `bentofile.yaml`. These\n  files are relative to user Python code's CWD (current working directory), which makes\n  importing relative modules and file path inside user code possible.\n\n\n`models` directory contains all models required by the Service. This is automatically determined from the `bentoml.Service` object's runners list.\n\n\n`apis` directory contains all API definitions. This directory contains API specs\n  that are generated from the `bentoml.Service` object's API definitions.\n\n\n`env` directory contains all environment-related files which will help boostrap the Bento \ud83c\udf71. This directory contains files that are generated\n  from :ref:`concepts/bento:Bento Build Options` that is specified under `bentofile.yaml`.\n\n\n.. note::\n:bdg-warning:`Warning:` users should never change files in the generated Bento\n   archive, unless it's for debugging purpose.\nBento Build Options\nBuild options are specified in a `.yaml` file, which customizes the final Bento\nproduced.\nBy convention, this file is named `bentofile.yaml`.\nIn this section, we will go over all the build options, including defining\ndependencies, configuring files to include, and customize docker image settings.\nService\n^^^^^^^\n`service` is a required field which specifies where the\n`bentoml.Service` object is defined. \nIn the :doc:`tutorial </tutorial>`, we defined `service: \"service:svc\"`, which can be\ninterpreted as:\n\n`service` refers to the Python module (the `service.py` file)\n`svc` refers to the `bentoml.Service` object created in `service.py`, with `svc = bentoml.Service(...)`\n\n.. tip::\nThis is synonymous to how the :ref:`bentoml serve <reference/cli:serve>` command specifies a `bentoml.Service` target.\n.. code-block:: zsh\n\n\n```                       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524bentofile.yaml\u2502\n      \u2502                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2518\n      \u2502                            \u2502\n      \u2502  service: \"service:svc\"    \u2502\n      \u2502                \u2500\u252c\u2500         \u2502\n      \u2502                 \u2502          \u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502\n                        \u2502\n                        \u2502    \u250c\u2500\u2500\u2500\u2500\u2510\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2524bash\u2502\n  \u2502                     \u2502    \u2514\u2500\u2500\u252c\u2500\u2518\n  \u2502                     \u25bc       \u2502\n  \u2502 \u00bb bentoml serve service:svc \u2502\n  \u2502                             \u2502\n  \u2502                             \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n\nDescription\n^^^^^^^^^^^\n`description` field allows user to customize documentation for any given Bento.\nThe description contents must be plain text, optionally in `Markdown <https://daringfireball.net/projects/markdown/syntax>`_ format. Description\ncan be specified either inline in the `bentofile.yaml`, or via a file path to an\nexisting text file:\n.. tab-set::\n.. tab-item:: Inline\n\n\n```  .. code-block:: yaml\n\n      service: \"service.py:svc\"\n      description: |\n          ## Description For My Bento \ud83c\udf71\n\n          Use **any markdown syntax** here!\n\n          > BentoML is awesome!\n      include:\n          ...\n```\n\n\n.. tab-item:: File path\n\n\n```  .. code-block:: yaml\n\n      service: \"service.py:svc\"\n      description: \"file: ./README.md\"\n      include:\n          ...\n```\n\n\n.. tip::\n    When pointing to a description file, it can be either an absolute path or a relative\n    path. The file must exist on the given path upon `bentoml build` command run,\n    and for relative file path, the current path is set to the `build_ctx`, which\n    default to the directory where `bentoml build` was executed from.\nLabels\n^^^^^^\n`labels` are key-value pairs that are attached to an object.\nIn BentoML, both `Bento` and `Model` can have labels attached to them. Labels are intended to\nbe used to specify identifying attributes of Bentos/Models that are meaningful and\nrelevant to users, but do not directly imply semantics to the rest of the system.\nLabels can be used to organize models and Bentos in `Yatai <https://github.com/bentoml/Yatai>`_,\nwhich also allow users to add or modify labels at any time.\n.. code-block:: yaml\nlabels:\n     owner: bentoml-team\n     stage: not-ready\nFiles to include\n^^^^^^^^^^^^^^^^\nIn the example :ref:`above </concepts/bento:The Build Command>`, the :code:`*.py` includes every Python files under `build_ctx`.\nYou can also include other wildcard and directory pattern matching.\n.. code-block:: yaml\n\n\n```...\ninclude:\n  - \"data/\"\n  - \"**/*.py\"\n  - \"config/*.json\"\n  - \"path/to/a/file.csv\"\n```\n\n\nIf the include field is not specified, BentoML will include all files under the `build_ctx` directory, besides the ones explicitly set to be excluded, as will be demonstrated in :ref:`concepts/bento:Files to exclude`.\n.. seealso::\nBoth `include` and `exclude` fields support `gitignore style pattern\n   matching.  <https://git-scm.com/docs/gitignore#_pattern_format>`_.\nFiles to exclude\n^^^^^^^^^^^^^^^^\nIf there are a lot of files under the working directory, another approach is to\nonly specify which files to be ignored.\n`exclude` field specifies the pathspecs (similar to `.gitignore` files) of files to be excluded in the final Bento build. The pathspecs are relative to\nthe `build_ctx` directory.\n.. code-block:: yaml\n\n\n```...\ninclude:\n- \"data/\"\n- \"**/*.py\"\nexclude:\n- \"tests/\"\n- \"secrets.key\"\n```\n\n\nUsers can also opt to place a `.bentoignore` file in the `build_ctx`\ndirectory. This is what a `.bentoignore` file would look like:\n.. code-block:: bash\n   :caption: .bentoignore\npycache/\n   .py[cod]\n   $py.class\n   .ipynb_checkpoints/\n   training_data/\n.. note::\n\n\n`````exclude`` is always applied after ``include``.\n```\n\n\nPython Packages\n^^^^^^^^^^^^^^^\nRequired Python packages for a given Bento can be specified under the `python.packages` field.\nWhen a package name is left without a version, BentoML will lock the package to the\nversion available under the current environment when running `bentoml build`. User can also specify the\ndesired version, install from a custom PyPI source, or install from a GitHub repo:\n.. code-block:: yaml\n\n\n```python:\n    packages:\n    - \"numpy\"\n    - \"matplotlib==3.5.1\"\n    - \"package>=0.2,<0.3\"\n    - \"torchvision==0.9.2 --extra-index-url https://download.pytorch.org/whl/lts/1.8/cpu\"\n    - \"git+https://github.com/username/mylib.git@main\"\n```\n\n\n.. note::\n    There's no need to specify :code:`bentoml` as a dependency here since BentoML will\n    addd the current version of BentoML to the Bento's dependency list by default. User\n    can override this by specifying a different BentoML version.\nTo use a variant of BentoML with additional features such as gRPC, tracing exporters, pydantic\nvalidation, specify the desired variant in the under `python.packages` field:\n.. tab-set::\n.. tab-item:: gRPC\n\n\n```  .. code-block:: yaml\n\n     python:\n       packages:\n       - \"bentoml[grpc]\"\n```\n\n\n.. tab-item:: AWS\n\n\n```  .. code-block:: yaml\n\n     python:\n       packages:\n       - \"bentoml[aws]\"\n```\n\n\n.. tab-item:: JSON IO\n\n\n```  .. code-block:: yaml\n\n     python:\n       packages:\n       - \"bentoml[io-json]\"\n```\n\n\n.. tab-item:: Image IO\n\n\n```  .. code-block:: yaml\n\n     python:\n       packages:\n       - \"bentoml[io-image]\"\n```\n\n\n.. tab-item:: Pandas IO\n\n\n```  .. code-block:: yaml\n\n     python:\n       packages:\n       - \"bentoml[io-pandas]\"\n```\n\n\n.. tab-item:: JSON IO\n\n\n```  .. code-block:: yaml\n\n     python:\n       packages:\n       - \"bentoml[io-json]\"\n```\n\n\n.. tab-item:: Jaeger\n\n\n```  .. code-block:: yaml\n\n     python:\n       packages:\n       - \"bentoml[tracing-jaeger]\"\n```\n\n\n.. tab-item:: Zipkin\n\n\n```  .. code-block:: yaml\n\n     python:\n       packages:\n       - \"bentoml[tracing-zipkin]\"\n```\n\n\n.. tab-item:: OTLP\n\n\n```  .. code-block:: yaml\n\n     python:\n       packages:\n       - \"bentoml[tracing-otlp]\"\n```\n\n\nIf you already have a\n`requirements.txt <https://pip.pypa.io/en/stable/reference/requirements-file-format/>`_\nfile that defines python packages for your project, you may also supply a path to the\n`requirements.txt` file directly:\n.. code-block:: yaml\n\n\n```python:\n    requirements_txt: \"./project-a/ml-requirements.txt\"\n```\n\n\nPip Install Options\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nAdditional `pip install` arguments can also be provided.\nNote that these arguments will be applied to all packages defined in `python.packages`, as\nwell as the `requirements_txt` file, if provided.\n.. code-block:: yaml\n\n\n```python:\n    requirements_txt: \"./requirements.txt\"\n    index_url: \"https://my.mirror.com/simple\"\n    no_index: False\n    trusted_host:\n    - \"pypi.python.org\"\n    - \"my.mirror.com\"\n    find_links:\n    - \"https://download.pytorch.org/whl/cu80/stable.html\"\n    extra_index_url:\n    - \"https://<other api token>:@my.mirror.com/pypi/simple\"\n    - \"https://pypi.python.org/simple\"\n    pip_args: \"--pre -U --force-reinstall\"\n```\n\n\n.. note::\n\n\n```**BentoML by default will cache pip artifacts across all local image builds to speed\nup the build process**.\n\nIf you want to force a re-download instead of using the cache, you can specify the :code:`pip_args: \"--no-cache-dir\"` option in your\n``bentofile.yaml``, or use the :code:`--no-cache` option in ``bentoml containerize`` command, e.g.:\n\n.. code-block::\n\n    \u00bb bentoml containerize my_bento:latest --no-cache\n```\n\n\nPyPI Package Locking\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nBy default, BentoML automatically locks all package versions, as well as all packages in\ntheir dependency graph, to the version found in the current build environment, and\ngenerates a :code:`requirements.lock.txt` file. This process uses\n`pip-compile <https://github.com/jazzband/pip-tools>`_ under the hood.\nIf you have already specified a version for all packages, you can optionally disable\nthis behavior by setting the `lock_packages` field to False:\n.. code-block:: yaml\n\n\n```python:\n    requirements_txt: \"requirements.txt\"\n    lock_packages: false\n```\n\n\nPython Wheels\n\"\"\"\"\"\"\"\"\"\"\"\"\"\nPython `.whl` files are also supported as a type of dependency to include in a\nBento. Simply provide a path to your `.whl` files under the ``wheels``` field.\n.. code-block:: yaml\n\n\n```python:\n    wheels:\n    - ./lib/my_package.whl\n```\n\n\nIf the wheel is hosted on a local network without TLS, you can indicate\nthat the domain is safe to pip with the `trusted_host` field.\nPython Options Table\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n+-------------------+------------------------------------------------------------------------------------+\n| Field             | Description                                                                        |\n+===================+====================================================================================+\n| requirements_txt  | The path to a custom requirements.txt file                                         |\n+-------------------+------------------------------------------------------------------------------------+\n| packages          | Packages to include in this bento                                                  |\n+-------------------+------------------------------------------------------------------------------------+\n| lock_packages     | Whether to lock the packages or not                                                |\n+-------------------+------------------------------------------------------------------------------------+\n| index_url         | Inputs for the `--index-url` pip argument                                        |\n+-------------------+------------------------------------------------------------------------------------+\n| no_index          | Whether to include the `--no-index` pip argument                                 |\n+-------------------+------------------------------------------------------------------------------------+\n| trusted_host      | List of trusted hosts used as inputs using the `--trusted-host` pip argument     |\n+-------------------+------------------------------------------------------------------------------------+\n| find_links        | List of links to find as inputs using the `--find-links` pip argument            |\n+-------------------+------------------------------------------------------------------------------------+\n| extra_index_url   | List of extra index urls as inputs using the `\u2248` pip argument                    |\n+-------------------+------------------------------------------------------------------------------------+\n| pip_args          | Any additional pip arguments that you would like to add when installing a package  |\n+-------------------+------------------------------------------------------------------------------------+\n| wheels            | List of paths to wheels to include in the bento                                    |\n+-------------------+------------------------------------------------------------------------------------+\nConda Options\n^^^^^^^^^^^^^\nConda dependencies can be specified under `conda` field. For example:\n.. code-block:: yaml\n\n\n```conda:\n    channels:\n    - default\n    dependencies:\n    - h2o\n    pip:\n    - \"scikit-learn==1.2.0\"\n```\n\n\nWhen `channels` filed is left unspecified, BentoML will use the community\nmaintained `conda-forge` channel as the default.\nOptionally, you can export all dependencies from a preexisting conda environment to\nan `environment.yml` file, and provide this file in your `bentofile.yaml`\nconfig:\nExport conda environment:\n.. code-block:: bash\n\n\n```\u00bb conda env export > environment.yml\n```\n\n\nIn your `bentofile.yaml`:\n.. code-block:: yaml\n\n\n```conda:\n    environment_yml: \"./environment.yml\"\n```\n\n\n.. note::\n\n\n```Unlike Python packages, BentoML does not support locking conda packages versions\nautomatically. It is recommended for users to specify a version in the config file.\n```\n\n\n.. seealso::\n\n\n```When ``conda`` options are provided, BentoML will select a docker base image\nthat comes with Miniconda pre-installed in the generated Dockerfile. Note that only\nthe ``debian`` and ``alpine`` distro support ``conda``. Learn more at\nthe :ref:`concepts/bento:Docker Options` section below.\n```\n\n\nConda Options Table\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n+------------------+----------------------------------------------------------------------------------------------------------------------------------+\n| Field            | Description                                                                                                                      |\n+==================+==================================================================================================================================+\n| environment_yml  | Path to a conda environment file to copy into the bento. If specified, this file will overwrite any additional option specified  |\n+------------------+----------------------------------------------------------------------------------------------------------------------------------+\n| channels         | Custom conda channels to use. If not specified will use `conda-forge`                                                          |\n+------------------+----------------------------------------------------------------------------------------------------------------------------------+\n| dependencies     | Custom conda dependencies to include in the environment                                                                          |\n+------------------+----------------------------------------------------------------------------------------------------------------------------------+\n| pip              | The specific `pip` conda dependencies to include                                                                               |\n+------------------+----------------------------------------------------------------------------------------------------------------------------------+\nDocker Options\n^^^^^^^^^^^^^^\nBentoML makes it easy to deploy a Bento to a Docker container. This section discuss the\navailable options for customizing the docker image generated from a Bento.\nHere's a basic Docker options configuration:\n.. code-block:: yaml\n\n\n```docker:\n    distro: debian\n    python_version: \"3.8.12\"\n    cuda_version: \"11.6.2\"\n    system_packages:\n      - libblas-dev\n      - liblapack-dev\n      - gfortran\n    env:\n      - FOO=value1\n      - BAR=value2\n```\n\n\n.. note::\nBentoML leverage `BuildKit <https://github.com/moby/buildkit>`_, a cache-efficient builder toolkit,\n   to containerize Bentos \ud83c\udf71.\nBuildKit comes with `Docker 18.09 <https://docs.docker.com/develop/develop-images/build_enhancements/>`_. This means\n   if you are using Docker via Docker Desktop, BuildKit will be available by default.\nHowever, if you are using a standalone version of Docker, you can install\n   BuildKit by following the instructions `here <https://github.com/docker/buildx#installing>`_.\nOS Distros\n\"\"\"\"\"\"\"\"\"\"\nThe following OS distros are currently supported in BentoML:\n\n`debian`: default, similar to Ubuntu\n`alpine`: A minimal Docker image based on Alpine Linux\n`ubi8`: Red Hat Universal Base Image\n`amazonlinux`: Amazon Linux 2\n\nSome of the distros may not support using conda or specifying CUDA for GPU. Here is the\nsupport matrix for all distros:\n+------------------+-----------------------------+-----------------+----------------------+\n| Distro           |  Available Python Versions  | Conda Support   | CUDA Support (GPU)   |\n+==================+=============================+=================+======================+\n| debian           |  3.7, 3.8, 3.9, 3.10        |  Yes            |  Yes                 |\n+------------------+-----------------------------+-----------------+----------------------+\n| alpine           |  3.7, 3.8, 3.9, 3.10        |  Yes            |  No                  |\n+------------------+-----------------------------+-----------------+----------------------+\n| ubi8             |  3.8, 3.9                   |  No             |  Yes                 |\n+------------------+-----------------------------+-----------------+----------------------+\n| amazonlinux      |  3.7, 3.8                   |  No             |  No                  |\n+------------------+-----------------------------+-----------------+----------------------+\n.. TODO::\n    Document image supported architectures\nGPU support\n\"\"\"\"\"\"\"\"\"\"\"\nThe `cuda_version` field specifies the target CUDA version to install on the\nthe generated docker image. Currently, the following CUDA version are supported:\n\n`\"11.6.2\"`\n`\"11.4.3\"`\n`\"11.2.2\"`\n\nBentoML will also install additional packages required for given target CUDA version.\n.. code-block:: yaml\n\n\n```docker:\n    cuda_version: \"11.6.2\"\n```\n\n\nIf you need a different cuda version that is not currently supported in BentoML, it is\npossible to install it by specifying it in the `system_packages` or via the\n`setup_script`.\n.. dropdown:: Installing custom CUDA version with conda\n   :icon: code\nWe will demonstrate how you can install custom cuda version via conda.\nAdd the following to your `bentofile.yaml`:\n.. code-block:: yaml\n\n\n```  conda:\n    channels:\n    - conda-forge\n    - nvidia\n    - defaults\n    dependencies:\n    - cudatoolkit-dev=10.1\n    - cudnn=7.6.4\n    - cxx-compiler=1.0\n    - mpi4py=3.0 # installs cuda-aware openmpi\n    - matplotlib=3.2\n    - networkx=2.4\n    - numba=0.48\n    - pandas=1.0\n```\n\n\nThen proceed with `bentoml build` and `bentoml containerize` respectively:\n.. code-block:: bash\n\n\n```  \u00bb bentoml build\n\n  \u00bb bentoml containerize <bento>:<tag>\n```\n\n\nSetup Script\n\"\"\"\"\"\"\"\"\"\"\"\"\nFor advanced Docker customization, you can also use a `setup_script` to inject\narbitrary user provided script during the image build process. For example, with NLP\nprojects you can pre-download NLTK data in the image with:\nIn your `bentofile.yaml`:\n.. code-block:: yaml\n\n\n```...\npython:\n  packages:\n    - nltk\ndocker:\n  setup_script: \"./setup.sh\"\n```\n\n\nIn the `setup.sh` file:\n.. code-block:: bash\n\n\n```#!/bin/bash\nset -euxo pipefail\n\necho \"Downloading NLTK data..\"\npython -m nltk.downloader all\n```\n\n\nNow build a new bento and then run `bentoml containerize MY_BENTO --progress plain` to\nview the docker image build progress. The newly built docker image will contain\npre-downloaded NLTK dataset.\n.. tip::\n\n\n```When working with bash scripts, it is recommended to add ``set -euxo pipefail``\nto the beginning. Especially when `set -e` is missing, the script will fail silently\nwithout raising an exception during ``bentoml containerize``. Learn more about\n`Bash Set builtin <https://www.gnu.org/software/bash/manual/html_node/The-Set-Builtin.html>`_.\n```\n\n\nIt is also possible to provide a Python script for initializing the docker image. Here's\nan example:\nIn `bentofile.yaml`:\n.. code-block:: yaml\n\n\n```...\npython:\n  packages:\n      - nltk\ndocker:\n  setup_script: \"./setup.py\"\n```\n\n\nIn the `setup.py` file:\n.. code-block:: python\n\n\n```#!/usr/bin/env python\n\nimport nltk\n\nprint(\"Downloading NLTK data..\")\nnltk.download('treebank')\n```\n\n\n.. note::\n\n\n```Pay attention to ``#!/bin/bash`` and ``#!/usr/bin/env python`` in the\nfirst line of the example scripts above. They are known as `Shebang <https://en.wikipedia.org/wiki/Shebang_(Unix)>`_\nand they are required in a setup script provided to BentoML.\n```\n\n\nSetup script is always executed after the specified Python packages, conda dependencies,\nand system packages are installed. Thus user can import and utilize those libraries in\ntheir setup script for the initialization process.\nEnable features for your Bento\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nUsers can optionally pass in the `--enable-features` flag to `bentoml containerize` to\nenable additional features for the generated Bento container image.\n+---------------------------------------+-------------------------------------------------------------------------------------------------------------------------+\n| `--enable-features`                 | Feature                                                                                                                 |\n+=======================================+=========================================================================================================================+\n| `--enable-features=aws`             | adding AWS interop (currently file upload to S3)                                                                        |\n+---------------------------------------+-------------------------------------------------------------------------------------------------------------------------+\n| `--enable-features=grpc`            | enable gRPC functionalities in BentoML                                                                                  |\n+---------------------------------------+-------------------------------------------------------------------------------------------------------------------------+\n| `--enable-features=grpc-channelz`   | enable `gRPC Channelz <https://grpc.io/blog/a-short-introduction-to-channelz/>`_ for debugging purposes                 |\n+---------------------------------------+-------------------------------------------------------------------------------------------------------------------------+\n| `--enable-features=grpc-reflection` | enable :github:`gRPC Reflection <grpc/grpc/blob/master/doc/server-reflection.md>`                                       |\n+---------------------------------------+-------------------------------------------------------------------------------------------------------------------------+\n| `--enable-features=io-image`        | adding Pillow dependencies to :ref:`Image IO descriptor <reference/api_io_descriptors:Images>`                          |\n+---------------------------------------+-------------------------------------------------------------------------------------------------------------------------+\n| `--enable-features=io-json`         | adding Pydantic validation to :ref:`JSON IO descriptor <reference/api_io_descriptors:Structured Data with JSON>`        |\n+---------------------------------------+-------------------------------------------------------------------------------------------------------------------------+\n| `--enable-features=io-pandas`       | adding Pandas dependencies to :ref:`PandasDataFrame descriptor <reference/api_io_descriptors:Tabular Data with Pandas>` |\n+---------------------------------------+-------------------------------------------------------------------------------------------------------------------------+\n| `--enable-features=tracing-jaeger`  | enable :ref:`Jaeger Exporter <guides/tracing:Tracing>` for distributed tracing                                          |\n+---------------------------------------+-------------------------------------------------------------------------------------------------------------------------+\n| `--enable-features=tracing-otlp`    | enable :ref:`OTLP Exporter <guides/tracing:Tracing>`   for distributed tracing                                          |\n+---------------------------------------+-------------------------------------------------------------------------------------------------------------------------+\n| `--enable-features=tracing-zipkin`  | enable :ref:`Zipkin Exporter <guides/tracing:Tracing>`  for distributed tracing                                         |\n+---------------------------------------+-------------------------------------------------------------------------------------------------------------------------+\nAdvanced Options\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nFor advanced customization for generating docker images, see :doc:`/guides/containerization`:\n\n:ref:`Using base image <guides/containerization:Custom Base Image>`\n:ref:`Using dockerfile template <guides/containerization:Dockerfile Template>`\n\nDocker Options Table\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n+---------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n| Field               | Description                                                                                                                               |\n+=====================+===========================================================================================================================================+\n| distro              | The OS distribution on the Docker image, Default to `debian`.                                                                           |\n+---------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n| python_version      | Specify which python to include on the Docker image [`3.7`, `3.8`, `3.9`, `3.10`]. Default to the Python version in build environment.    |\n+---------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n| cuda_version        | Specify the cuda version to install on the Docker image [:code:`11.6.2`].                                                                 |\n+---------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n| system_packages     | Declare system packages to be installed in the container.                                                                            |\n+---------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n| env                 | Declare environment variables in the generated Dockerfile.                                                                                |\n+---------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n| setup_script        | A python or shell script that executes during docker build time.                                                                          |\n+---------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n| base_image          | A user-provided docker base image. This will override all other custom attributes of the image.                                           |\n+---------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n| dockerfile_template | Customize the generated dockerfile by providing a Jinja2 template that extends the default dockerfile.                                    |",
    "tag": "bentoml"
  },
  {
    "title": "fastai.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/frameworks/fastai.rst",
    "content": "=======\nfast.ai\n=======\nfastai is a popular deep learning library which provides high-level components for practioners to get state-of-the-art results in standard deep learning domains, as well as low-level components\nfor researchers to build new approaches. To learn more about fastai, visit their `documentation <docs.fast.ai>`_.\nBentoML provides native support for `fastai <https://github.com/fastai/fastai>`_, and this guide provides an overview of how to use BentoML with fastai.\nCompatibility\nBentoML requires fastai version 2 or higher to be installed. \nBentoML does not support fastai version 1. If you are using fastai version 1, consider using :ref:`concepts/runner:Custom Runner`.\nSaving a trained fastai learner\nThis example is based on `Transfer Learning with text <https://docs.fast.ai/tutorial.text.html#The-ULMFiT-approach>`_ from fastai.\n.. code-block:: python\nfrom fastai.basics import URLs\n   from fastai.metrics import accuracy\n   from fastai.text.data import DataBlock\n   from fastai.text.data import TextBlock\n   from fastai.text.data import untar_data\n   from fastai.text.data import CategoryBlock\n   from fastai.text.models import AWD_LSTM\n   from fastai.text.learner import text_classifier_learner\n   from fastai.data.transforms import parent_label\n   from fastai.data.transforms import get_text_files\n   from fastai.data.transforms import GrandparentSplitter\n# Download IMDB dataset\n   path = untar_data(URLs.IMDB)\n# Create IMDB DataBlock\n   imdb = DataBlock(\n       blocks=(TextBlock.from_folder(path), CategoryBlock),\n       get_items=get_text_files,\n       get_y=parent_label,\n       splitter=GrandparentSplitter(valid_name=\"test\"),\n   )\n   dls = imdb.dataloaders(path)\n# define a Learner object\n   learner = text_classifier_learner(\n        dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy\n    )\n# quickly fine tune the model\n   learner.fine_tune(4, 1e-2)\n# output:\n   # epoch     train_loss  valid_loss  accuracy  time\n   # 0         0.453252    0.395130    0.822080  36:45\nlearner.predict(\"I really liked that movie!\")\n# output:\n   # ('pos', TensorText(1), TensorText([0.1216, 0.8784]))\nAfter training, use :obj:`~bentoml.fastai.save_model` to save the `Learner <https://docs.fast.ai/learner.html#Learner>`_ instance to BentoML model store.\n.. code-block:: python\nbentoml.fastai.save_model(\"fastai_sentiment\", learner)\nTo verify that the saved learner can be loaded properly:\n.. code-block:: python\nlearner = bentoml.fastai.load_model(\"fastai_sentiment:latest\")\nlearner.predict(\"I really liked that movie!\")\nBuilding a Service using fastai\n.. seealso::\n:ref:`Building a Service <concepts/service:Service and APIs>`: more information on creating a prediction service with BentoML.\n.. code-block:: python\nimport bentoml\nimport numpy as np\nfrom bentoml.io import Text\n   from bentoml.io import NumpyNdarray\nrunner = bentoml.fastai.get(\"fastai_sentiment:latest\").to_runner()\nsvc = bentoml.Service(\"fast_sentiment\", runners=[runner])\n@svc.api(input=Text(), output=NumpyNdarray())\n   async def classify_text(text: str) -> np.ndarray:\n      # returns sentiment score of a given text\n      res = await runner.predict.async_run(text)\n      return np.asarray(res[-1])\nWhen constructing a :ref:`bentofile.yaml <concepts/bento:Bento Build Options>`,\nthere are two ways to include fastai as a dependency, via `python` or\n`conda`:\n.. tab-set::\n.. tab-item:: python\n\n\n```  .. code-block:: yaml\n\n     python:\n   packages:\n     - fastai\n```\n\n\n.. tab-item:: conda\n\n\n```  .. code-block:: yaml\n\n     conda:\n       channels:\n       - fastchan\n       dependencies:\n       - fastai\n```\n\n\nUsing Runners\n.. seealso::\nSee :ref:`concepts/runner:Using Runners` doc for a general introduction to the Runner concept and its usage.\n`runner.predict.run` is generally a drop-in replacement for `learner.predict` regardless of the learner type \nfor executing the prediction in the model runner. A fastai runner will receive the same inputs type as \nthe given learner.\nFor example, Runner created from a `Tabular learner <https://docs.fast.ai/tabular.learner.html>`_ model will\naccept a :obj:`pandas.DataFrame` as input, where as a Text learner based runner will accept a :obj:`str` as input.\nUsing PyTorch layer\nSince fastai is built on top of PyTorch, it is also possible to use PyTorch\nmodels from within a fastai learner directly for inference. Note that by using\nthe PyTorch layer, you will not be able to use the fastai :obj:`Learner`'s\nfeatures such as :code:`.predict()`, :code:`.get_preds()`, etc.\nTo get the PyTorch model, access it via `learner.model`:\n.. code-block:: python\nimport bentoml\nbentoml.pytorch.save_model(\n      \"my_pytorch_model\", learner.model, signatures={\"call\": {\"batchable\": True}}\n   )\nLearn more about using PyTorch with BentoML :ref:`here <frameworks/pytorch:PyTorch>`.\nUsing GPU\nSince fastai doesn't support using GPU for inference, BentoML\ncan only support CPU inference with fastai models.\nAdditionally, if the model uses `mixed_precision`, then the loaded model will also be converted to FP32.\nSee `mixed precision <https://docs.fast.ai/callback.fp16.html>`_ to learn more about mixed precision.\nIf you need to use GPU for inference, you can :ref:`use the PyTorch layer <frameworks/fastai:Using PyTorch layer>`.\nAdaptive batching \n~~~~~~~~~~~~~~~~~\nfastai's `Learner#predict` does not support taking batch input for inference, hence\nthe adaptive batching feature in BentoML is not available for fastai models.\nThe default signature has :code:`batchable` set to :code:`False`.",
    "tag": "bentoml"
  },
  {
    "title": "index.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/frameworks/index.rst",
    "content": "================\nFramework Guides\n================\nHere is the list of supported ML libraries and formats in BentoML. You can also find example\nprojects in the `bentoml/examples <https://github.com/bentoml/BentoML/tree/main/examples>`_ directory.\n.. grid:: 1 2 2 2\n    :gutter: 3\n    :margin: 0\n    :padding: 3 4 0 0\n\n\n```.. grid-item-card:: :doc:`/frameworks/catboost`\n    :link: /frameworks/catboost\n    :link-type: doc\n\n.. grid-item-card:: :doc:`/frameworks/fastai`\n    :link: /frameworks/fastai\n    :link-type: doc\n\n.. grid-item-card:: :doc:`/frameworks/keras`\n    :link: /frameworks/keras\n    :link-type: doc\n\n.. grid-item-card:: :doc:`/frameworks/lightgbm`\n    :link: /frameworks/lightgbm\n    :link-type: doc\n\n.. grid-item-card:: :doc:`/integrations/mlflow`\n    :link: /integrations/mlflow\n    :link-type: doc\n\n.. grid-item-card:: :doc:`/frameworks/onnx`\n    :link: /frameworks/onnx\n    :link-type: doc\n\n.. grid-item-card:: :doc:`/frameworks/pytorch`\n    :link: /frameworks/pytorch\n    :link-type: doc\n\n.. grid-item-card:: :doc:`/frameworks/pytorch_lightning`\n    :link: /frameworks/pytorch_lightning\n    :link-type: doc\n\n.. grid-item-card:: :doc:`/frameworks/sklearn`\n    :link: /frameworks/sklearn\n    :link-type: doc\n\n.. grid-item-card:: :doc:`/frameworks/tensorflow`\n    :link: /frameworks/tensorflow\n    :link-type: doc\n\n.. grid-item-card:: :doc:`/frameworks/transformers`\n    :link: /frameworks/transformers\n    :link-type: doc\n\n.. grid-item-card:: :doc:`/frameworks/xgboost`\n    :link: /frameworks/xgboost\n    :link-type: doc\n```\n\n\nCustom Models\n.. grid:: 1 2 2 2\n    :gutter: 3\n    :margin: 0\n    :padding: 3 4 0 0\n\n\n```.. grid-item-card:: :doc:`/frameworks/picklable`\n    :link: /frameworks/picklable\n    :link-type: doc\n\n.. grid-item-card:: :ref:`concepts/runner:Custom Runner`\n    :link: custom-runner\n    :link-type: ref\n```\n\n\nRoadmap\nThe following frameworks are supported in pre-1.0 BentoML versions and are being migrated to the new 1.0 API. In the meantime, users may use :ref:`Custom Models <frameworks/index:Custom Models>` as a workaround.\n\nDetectron\nEasyOCR\nEvalML\nFastText\nFlax\nGluon\nH2O\nJax\nNeuropod\nONNX-MLIR\nPaddlePaddle\nPyCaret\nPyTorch ignite\nSnapML\nSpacy\nSpark MLlib\nStatsmodels\n\n.. admonition:: Help us improve the project!\n\n\n```Found an issue or a TODO item? You're always welcome to make contributions to the\nproject and its documentation. Check out the\n`BentoML development guide <https://github.com/bentoml/BentoML/blob/main/DEVELOPMENT.md>`_\nand `documentation guide <https://github.com/bentoml/BentoML/blob/main/docs/README.md>`_\nto get started.\n```\n\n\n.. toctree::\n    :hidden:\n\n\n```catboost\nfastai\nkeras\nlightgbm\nonnx\npicklable\npytorch\npytorch_lightning\nsklearn\ntensorflow\ntransformers\n```\n\n",
    "tag": "bentoml"
  },
  {
    "title": "picklable.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/frameworks/picklable.rst",
    "content": "===============\nPicklable Model\n===============\nFor custom ML models created with pure Python code, a simple way to make it work with\nthe BentoML workflow is via `bentoml.picklable_model`.\nBelow is an example of saving a Python function as a model:\n.. code:: python\n\n\n```from typing import List\nimport numpy as np\nimport bentoml\n\ndef my_python_model(input_list: List[int]) -> List[int]:\n    return np.square(np.array(input_list))\n\n# `save_model` saves a given python object or function\nsaved_model = bentoml.picklable_model.save_model(\n    'my_python_model',\n    my_python_model,\n    signatures={\"__call__\": {\"batchable\": True}}\n)\nprint(f\"Model saved: {saved_model}\")\n```\n\n\nLoad the model back to memory for testing:\n.. code:: python\n\n\n```loaded_model = bentoml.picklable_model.load_model(\"my_python_model:latest\")\n\nloaded_model([1, 2, 3])\n# out: array([1, 4, 9])\n```\n\n\nLoad the model as a local Runner to test out its inference API:\n.. code:: python\n\n\n```runner = bentoml.picklable_model.get(\"my_python_model:latest\").to_runner()\nrunner.init_local()\nrunner.run([7])\n```\n\n",
    "tag": "bentoml"
  },
  {
    "title": "catboost.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/frameworks/catboost.rst",
    "content": "========\nCatBoost\n========\nCatBoost is a machine learning algorithm that uses gradient boosting on decision trees. It is available as an open source library.\nTo learn more about CatBoost, visit their `documentation <https://catboost.ai/en/docs/>`_.\nBentoML provides native support for `CatBoost <https://github.com/catboost/catboost>`_, and this guide provides an overview of how to use BentoML with CatBoost.\nSaving a trained CatBoost model\nIn this example, we will train a new model using UCI's `breast cancer dataset <https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)>`_.\n.. code-block:: python\nimport bentoml\nimport catboost as cbt\nfrom sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\nX = cancer.data\n   y = cancer.target\nmodel = cbt.CatBoostClassifier(\n       iterations=2,\n       depth=2,\n       learning_rate=1,\n       loss_function=\"Logloss\",\n       verbose=False,\n   )\n# train the model\n   model.fit(X, y)\nUse :obj:`~bentoml.catboost.save_model` to save the model instance to BentoML model store:\n.. code-block:: python\nbento_model = bentoml.catboost.save_model(\"catboost_cancer_clf\", model)\nTo verify that the saved learner can be loaded properly:\n.. code-block:: python\nmodel = bentoml.catboost.load_model(\"catboost_cancer_clf:latest\")\nmodel.predict(cbt.Pool([[1.308e+01, 1.571e+01, 8.563e+01, 5.200e+02, 1.075e-01, 1.270e-01,\n       4.568e-02, 3.110e-02, 1.967e-01, 6.811e-02, 1.852e-01, 7.477e-01,\n       1.383e+00, 1.467e+01, 4.097e-03, 1.898e-02, 1.698e-02, 6.490e-03,\n       1.678e-02, 2.425e-03, 1.450e+01, 2.049e+01, 9.609e+01, 6.305e+02,\n       1.312e-01, 2.776e-01, 1.890e-01, 7.283e-02, 3.184e-01, 8.183e-02]]))\nBuilding a Service using CatBoost\n.. seealso::\n:ref:`Building a Service <concepts/service:Service and APIs>`: more information on creating a prediction service with BentoML.\n.. code-block:: python\nimport bentoml\nimport numpy as np\nfrom bentoml.io import NumpyNdarray\nrunner = bentoml.catboost.get(\"catboost_cancer_clf:latest\").to_runner()\nsvc = bentoml.Service(\"cancer_clf\", runners=[runner])\n@svc.api(input=NumpyNdarray(), output=NumpyNdarray())\n   async def classify_cancer(input: np.ndarray) -> np.ndarray:\n      # returns sentiment score of a given text\n      res = await runner.predict.async_run(input)\n      return res\nWhen constructing a :ref:`bentofile.yaml <concepts/bento:Bento Build Options>`,\nthere are two ways to include CatBoost as a dependency, via `python` or\n`conda`:\n.. tab-set::\n.. tab-item:: python\n\n\n```  .. code-block:: yaml\n\n     python:\n   packages:\n     - catboost\n```\n\n\n.. tab-item:: conda\n\n\n```  .. code-block:: yaml\n\n     conda:\n       channels:\n       - conda-forge\n       dependencies:\n       - catboost\n```\n\n\nUsing Runners\n.. seealso::\nSee :ref:`concepts/runner:Using Runners` doc for a general introduction to the Runner concept and its usage.\nA CatBoost :obj:`~bentoml.Runner` can be created as follows:\n.. code-block:: python\nrunner = bentoml.catboost.get(\"model_name:model_version\").to_runner()\n`runner.predict.run` is generally a drop-in replacement for `model.predict`.\nWhile a `Pool <https://catboost.ai/en/docs/concepts/python-reference_pool>`_ can be passed to a CatBoost Runner, BentoML does not support adaptive batching for `Pool` objects.\nTo use adaptive batching feature from BentoML, we recommend our users to use either NumPy `ndarray` or Pandas `DataFrame` instead.\n.. note::\nCurrently `staged_predict` callback is not yet supported with :code:`bentoml.catboost`.\nUsing GPU\nCatBoost Runners will automatically use `task_type=GPU` if a GPU is detected.\nThis behavior can be disabled using the :ref:`BentoML configuration file<guides/configuration:Configuration>`:\naccess:\n.. code-block:: yaml\nrunners:\n      # resources can be configured at the top level\n      resources:\n         nvidia.com/gpu: 0\n      # or per runner\n      my_runner_name:\n         resources:\n             nvidia.com/gpu: 0\nAdaptive batching \n~~~~~~~~~~~~~~~~~\n.. seealso::\n:ref:`guides/batching:Adaptive Batching`: a general introduction to adaptive batching in BentoML.\nCatBoost's `model.predict` supports taking batch input for inference. This is disabled by\ndefault, but can be enabled using the appropriate signature when saving your model.\n.. note::\nBentoML does not currently support adaptive batching for `Pool` input. In order to enable\n   batching, use either a NumPy `ndarray` or a Pandas `DataFrame` instead.\n.. code-block:: python\nbento_model = bentoml.catboost.save_model(\n    \"catboost_cancer_clf\", model, signatures={\"predict\": {\"batchable\": True}}",
    "tag": "bentoml"
  },
  {
    "title": "xgboost.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/frameworks/xgboost.rst",
    "content": "=======\nXGBoost\n=======\nXGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. This guide provides an overview of using `XGBoost <https://xgboost.readthedocs.io/en/stable/>`_ with BentoML.\nCompatibility\n~~~~~~~~~~~~~\nBentoML has been validated to work with XGBoost version 0.7post3 and higher.\nSaving a Trained Booster\nFirst, train or load a booster. In this example, we will be training a new booster using UCI's\n`breast cancer dataset <https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)>`_.\nIf you've already saved a model using XGBoost, simply load it back into Python using\n`Booster.load_model`.\n.. code-block:: python\nimport xgboost as xgb\n   from sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\nX = cancer.data\n   y = cancer.target\ndt = xgb.DMatrix(X, label=y)\nparam = {\"max_depth\": 3, \"eta\": 0.3, \"objective\": \"multi:softprob\", \"num_class\": 2}\n   bst = xgb.train(param, dt)\nAfter training, use :obj:`~bentoml.xgboost.save_model()` to save the Booster instance to BentoML model store. XGBoost has no\nframework-specific save options.\n.. code-block:: python\nimport bentoml\n   bento_model = bentoml.xgboost.save_model(\"booster_tree\", bst)\nTo verify that the saved learner can be loaded properly:\n.. code-block:: python\nimport bentoml\n   booster = bentoml.xgboost.load_model(\"booster_tree:latest\")\n   booster.predict(xgb.DMatrix([[1.308e+01, 1.571e+01, 8.563e+01, 5.200e+02, 1.075e-01, 1.270e-01,\n       4.568e-02, 3.110e-02, 1.967e-01, 6.811e-02, 1.852e-01, 7.477e-01,\n       1.383e+00, 1.467e+01, 4.097e-03, 1.898e-02, 1.698e-02, 6.490e-03,\n       1.678e-02, 2.425e-03, 1.450e+01, 2.049e+01, 9.609e+01, 6.305e+02,\n       1.312e-01, 2.776e-01, 1.890e-01, 7.283e-02, 3.184e-01, 8.183e-02]]))\n.. note::\n   `load_model` should only be used when the booster object itself is required. When using a saved\n   booster in a BentoML service, use :obj:`~bentoml.xgboost.get` and create a runner as described\n   below.\nBuilding a Service\n.. seealso::\n:ref:`Building a Service <concepts/service:Service and APIs>`: more information on creating a\n   prediction service with BentoML.\nCreate a `service.py` file separate from your training code that will be used to define the\nBentoML service:\n.. code-block:: python\nimport bentoml\n   from bentoml.io import NumpyNdarray\n   import numpy as np\n# create a runner from the saved Booster\n   runner = bentoml.xgboost.get(\"booster_tree:latest\").to_runner()\n# create a BentoML service\n   svc = bentoml.Service(\"cancer_classifier\", runners=[runner])\n# define a new endpoint on the BentoML service\n   @svc.api(input=NumpyNdarray(), output=NumpyNdarray())\n   async def classify_tumor(input: np.ndarray) -> np.ndarray:\n       # use 'runner.predict.run(input)' instead of 'booster.predict'\n       res = await runner.predict.async_run(input)\n       return res\nTake note of the name of the service (`svc` in this example) and the name of the file.\nYou should also have a `bentofile.yaml` alongside the service file that specifies that\ninformation, as well as the fact that it depends on XGBoost. This can be done using either\n`python` (if using pip), or `conda`:\n.. tab-set::\n   .. tab-item:: pip\n\n\n```  .. code-block:: yaml\n\n     service: \"service:svc\"\n     description: \"My XGBoost service\"\n     python:\n   packages:\n     - xgboost\n```\n\n\n.. tab-item:: conda\n\n\n```  .. code-block:: yaml\n\n     service: \"service:svc\"\n     description: \"My XGBoost service\"\n     conda:\n       channels:\n       - conda-forge\n       dependencies:\n       - xgboost\n```\n\n\nUsing Runners\n~~~~~~~~~~~~~\n.. seealso::\n:ref:`concepts/runner:Using Runners`: a general introduction to the Runner concept and its usage.\nA runner for a Booster is created like so:\n.. code-block:: python\nbentoml.xgboost.get(\"model_name:model_version\").to_runner()\n`runner.predict.run` is generally a drop-in replacement for `booster.predict`. However, while it\nis possible to pass a `DMatrix` as input, BentoML does not support adaptive batching in that case.\nIt is therefore recommended to use a NumPy `ndarray` or Pandas `DataFrame` as input instead.\nThere are no special options for loading XGBoost.\nRunners must to be initialized in order for their `run` methods to work. This is done by BentoML\ninternally when you serve a bento with `bentoml serve`. See the :ref:`runner debugging guide\n<concepts/service:Debugging Runners>` for more information about initializing runners locally.\nGPU Inference\n~~~~~~~~~~~~~\nIf there is a GPU available, the XGBoost Runner will automatically use `gpu_predictor` by default.\nThis can be disabled by using the\n:ref:`BentoML configuration file <guides/configuration:Configuration>` to disable Runner GPU\naccess:\n.. code-block:: yaml\nrunners:\n      # resources can be configured at the top level\n      resources:\n         nvidia.com/gpu: 0\n      # or per runner\n      my_runner_name:\n         resources:\n             nvidia.com/gpu: 0\nAdaptive Batching\n~~~~~~~~~~~~~~~~~\n.. seealso::\n:ref:`guides/batching:Adaptive Batching`: a general introduction to adaptive batching in BentoML.\nXGBoost's `booster.predict` supports taking batch input for inference. This is disabled by\ndefault, but can be enabled using the appropriate signature when saving your booster.\n.. note\nBentoML does not currently support adaptive batching for `DMatrix` input. In order to enable\n   batching, use either a NumPy `ndarray` or a Pandas `DataFrame` instead.\n.. code-block:: python",
    "tag": "bentoml"
  },
  {
    "title": "tensorflow.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/frameworks/tensorflow.rst",
    "content": "==========\nTensorFlow\n==========\nTensorFlow is an open source machine learning library focusing on deep neural networks. BentoML provides native support for \nserving and deploying models trained from TensorFlow.\nPreface\nEven though `bentoml.tensorflow` supports Keras model, we recommend our users to use :ref:`bentoml.keras <frameworks/keras>` for better development experience. \nIf you must use TensorFlow for your Keras model, make sure that your Keras model inference callback (such as `predict`) is decorated with :obj:`~tf.function`.\n.. note::\n\n\n```- Keras is not optimized for production inferencing. There are `known reports <https://github.com/tensorflow/tensorflow/issues?q=is%3Aissue+sort%3Aupdated-desc+keras+memory+leak>`_ of memory leaks during serving at the time of BentoML 1.0 release. The same issue applies to ``bentoml.keras`` as it heavily relies on the Keras APIs.\n- Running Inference with :obj:`~bentoml.tensorflow` usually halves the time comparing with using ``bentoml.keras``.\n- ``bentoml.keras`` performs input casting that resembles the original Keras model input signatures.\n```\n\n\n.. note::\n\n\n```:bdg-info:`Remarks:` We recommend users apply model optimization techniques such as **distillation** or **quantization**. Alternatively, Keras models can also be converted to :ref:`ONNX <frameworks/onnx>` models and leverage different runtimes.\n```\n\n\nCompatibility\nBentoML requires TensorFlow version 2.0 or higher. For TensorFlow version 1.0, consider using a :ref:`concepts/runner:Custom Runner`.\nSaving a Trained Model\n`bentoml.tensorflow` supports saving `tf.Module`, `keras.models.Sequential`, and `keras.Model`.\n.. tab-set::\n.. tab-item:: tf.Module\n\n\n```  .. code-block:: python\n    :caption: `train.py`\n\n    # models created from the tf native API\n\n    class NativeModel(tf.Module):\n        def __init__(self):\n            super().__init__()\n            self.weights = np.asfarray([[1.0], [1.0], [1.0], [1.0], [1.0]])\n            self.dense = lambda inputs: tf.matmul(inputs, self.weights)\n\n        @tf.function(\n            input_signature=[\n                tf.TensorSpec(shape=[None, 5], dtype=tf.float64, name=\"inputs\")\n            ]\n        )\n        def __call__(self, inputs):\n            return self.dense(inputs)\n\n    model = NativeModel()\n\n    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    optimizer = tf.keras.optimizers.Adam()\n\n    EPOCHS = 10\n    for epoch in range(EPOCHS):\n        with tf.GradientTape() as tape:\n            predictions = model(train_x)\n            loss = loss_object(train_y, predictions)\n\n        gradients = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\n    bentoml.tensorflow.save(\n        model,\n        \"my_tf_model\",\n        signatures={\"__call__\": {\"batchable\": True, \"batch_dim\": 0}}\n    )\n```\n\n\n.. tab-item:: keras.Model\n\n\n```  .. code-block:: python\n    :caption: `train.py`\n\n    class Model(keras.Model):\n        def __init__(self):\n            super().__init__()\n            self.dense = keras.layers.Dense(1)\n\n        @tf.function(\n            input_signature=[\n                tf.TensorSpec(shape=[None, 5], dtype=tf.float64, name=\"inputs\")\n            ]\n        )\n        def call(self, inputs):\n            return self.dense(inputs)\n\n    model = Model()\n    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n    model.fit(train_x, train_y, epochs=10)\n\n    bentoml.tensorflow.save(\n        model,\n        \"my_keras_model\",\n        signatures={\"__call__\": {\"batchable\": True, \"batch_dim\": 0}}\n    )\n```\n\n\n.. tab-item:: keras.model.Sequential\n\n\n```  .. code-block:: python\n    :caption: `train.py`\n\n    model = keras.models.Sequential(\n        (\n            keras.layers.Dense(\n                units=1,\n                input_shape=(5,),\n                dtype=tf.float64,\n                use_bias=False,\n                kernel_initializer=keras.initializers.Ones(),\n            ),\n        )\n    )\n    opt = keras.optimizers.Adam(0.002, 0.5)\n    model.compile(optimizer=opt, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n    model.fit(train_x, train_y, epochs=10)\n\n    bentoml.tensorflow.save(\n        model,\n        \"my_keras_model\",\n        signatures={\"__call__\": {\"batchable\": True, \"batch_dim\": 0}}\n    )\n```\n\n\n.. tab-item:: Functional keras.Model\n\n\n```  .. code-block:: python\n    :caption: `train.py`\n\n    x = keras.layers.Input((5,), dtype=tf.float64, name=\"x\")\n    y = keras.layers.Dense(\n        6,\n        name=\"out\",\n        kernel_initializer=keras.initializers.Ones(),\n    )(x)\n    model = keras.Model(inputs=x, outputs=y)\n    opt = keras.optimizers.Adam(0.002, 0.5)\n    model.compile(optimizer=opt, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n    model.fit(train_x, train_y, epochs=10)\n\n    bentoml.tensorflow.save(\n        model,\n        \"my_keras_model\",\n        signatures={\"__call__\": {\"batchable\": True, \"batch_dim\": 0}}\n    )\n```\n\n\n`bentoml.tensorflow` also supports saving models that take multiple tensors as input:\n.. code-block:: python\n   :caption: `train.py`\nclass MultiInputModel(tf.Module):\n       def init(self):\n           ...\n\n\n```   @tf.function(\n       input_signature=[\n           tf.TensorSpec(shape=[None, 5], dtype=tf.float64, name=\"x1\"),\n           tf.TensorSpec(shape=[None, 5], dtype=tf.float64, name=\"x2\"),\n           tf.TensorSpec(shape=(), dtype=tf.float64, name=\"factor\"),\n       ]\n   )\n   def __call__(self, x1: tf.Tensor, x2: tf.Tensor, factor: tf.Tensor):\n       ...\n```\n\n\nmodel = MultiInputModel()\n   ... # training\nbentoml.tensorflow.save(\n       model,\n       \"my_tf_model\",\n       signatures={\"call\": {\"batchable\": True, \"batch_dim\": 0}}\n   )\n.. note::\n\n\n```:obj:`~bentoml.tensorflow.save_model` has two parameters: ``tf_signature`` and ``signatures``.\n\nUse the following arguments to define the model signatures to ensure consistent model behaviors in a Python session and from the BentoML model store:\n\n- ``tf_signatures`` is an alias to `tf.saved_model.save <https://www.tensorflow.org/api_docs/python/tf/saved_model/save>`_ *signatures* field. This optional signatures controls which methods in a given `obj <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/trackable/base.py#L281>`_ will be available to programs that consume `SavedModel's <https://www.tensorflow.org/guide/saved_model>`_, for example, serving APIs. Read more about TensorFlow's signatures behavior `from their API documentation <https://www.tensorflow.org/api_docs/python/tf/saved_model/save>`_.\n\n- ``signatures`` refers to a general :ref:`Model Signatures <concepts/model:Model Signatures>` that dictates which methods can be used for inference in the Runner context. This signatures dictionary will be used during the creation process of a Runner instance.\n```\n\n\n:bdg-info:`Note:` The signatures used for creating a Runner is `{\"__call__\": {\"batchable\": False}}`.\nThis means BentoML\u2019s :ref:`Adaptive Batching <guides/batching:Adaptive Batching>` is disabled when using :obj:`~bentoml.tensorflow.save_model()`.\nIf you want to utilize adaptive batching behavior and know your model's dynamic batching dimension, make sure to pass in `signatures` as follow: \n.. code-block:: python\n\n\n```bentoml.tensorflow.save(model, \"my_model\", signatures={\"__call__\": {\"batch_dim\": 0, \"batchable\": True}})\n```\n\n\nBuilding a Service\nCreate a BentoML service with the previously saved `my_tf_model` pipeline using the :obj:`~bentoml.tensorflow` framework APIs.\n.. code-block:: python\n    :caption: `service.py`\n\n\n```runner = bentoml.tensorflow.get(\"my_tf_model\").to_runner()\n\nsvc = bentoml.Service(name=\"test_service\", runners=[runner])\n\n@svc.api(input=JSON(), output=JSON())\nasync def predict(json_obj: JSONSerializable) -> JSONSerializable:\n    batch_ret = await runner.async_run([json_obj])\n    return batch_ret[0]\n```\n\n\n.. seealso::\n\n\n```The following resources can help you to fine-tune your Tensorflow model:\n\n- |tf_function|_\n\n- Apply :ref:`Adaptive Batching <frameworks/tensorflow:Adaptive Batching>`.\n\n- `Serve on GPUs <https://www.tensorflow.org/guide/gpu>`_ and `GPU optimization <https://www.tensorflow.org/guide/gpu_performance_analysis>`_\n\n- `Graph optimization with Grappler <https://www.tensorflow.org/guide/graph_optimization>`_\n```\n\n\n.. _tf_function: https://www.tensorflow.org/guide/function\n.. |tf_function| replace:: Performance tuning with well-defined `tf.function`\nAdaptive Batching\nMost TensorFlow models can accept batched data as input. If batch inference is supported, it is recommended to enable batching to take advantage of\nthe :ref:`adaptive batching <guides/batching:Adaptive Batching>` capability to improve the throughput and efficiency of the model.\nEnable adaptive batching by overriding `signatures` argument with the method name and providing `batchable` and `batch_dim` configurations when saving the model to the model store:\n.. code-block:: diff\n   :caption: `batch.diff`\ndiff --git a/train.py b/train_batched.py\n   index 3b4bf11f..2d0ea09c 100644\n   --- a/train.py\n   +++ b/train_batched.py\n   @@ -3,15 +3,24 @@ import bentoml\n   class NativeModel(tf.Module):\n       @tf.function(\n           input_signature=[\n   -            tf.TensorSpec(shape=[1, 5], dtype=tf.int64, name=\"inputs\")\n   +            tf.TensorSpec(shape=[None, 5], dtype=tf.float64, name=\"inputs\")\n           ]\n       )\n       def call(self, inputs):\n           ...\nmodel = NativeModel()\n   -bentoml.tensorflow.save(model, \"test_model\")\n   +bentoml.tensorflow.save(\n   +    model,\n   +    \"test_model\",\n   +    signatures={\"call\": {\"batchable\": True, \"batch_dim\": 0}},\n   +)\nrunner = bentoml.tensorflow.get(\"test_model\")\n   runner.init_local()\n   +\n   +#client 1\n   runner.run([[1,2,3,4,5]])\n   +\n   +#client 2\n   +runner.run([[6,7,8,9,0]])\nFrom the diff above, when multiple clients send requests to a given server running this\nmodel, BentoML will automatically batched inbound request and invoke `model([[1,2,3,4,5], [6,7,8,9,0]])`\n.. seealso::\nSee :ref:`Adaptive Batching <guides/batching:Adaptive Batching>` to learn more about\n   the adaptive batching feature in BentoML.\n.. note::\nYou can find more examples for TensorFlow in our :github:`bentoml/examples <bentoml/BentoML/tree/main/examples>` directory.",
    "tag": "bentoml"
  },
  {
    "title": "keras.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/frameworks/keras.rst",
    "content": "=====\nKeras\n=====\n.. note::\nBoth `bentoml.keras` and `bentoml.tensorflow` support Keras\n   models. `bentoml.keras` utilizes the native model format and\n   will give a better development experience to users who are more\n   familiar with Keras models. However, the native model format of Keras is\n   not optimized for production inference. There are `known reports\n   <https://github.com/tensorflow/tensorflow/issues?q=is%3Aissue+sort%3Aupdated-desc+keras+memory+leak>`_\n   of memory leaks during serving time at the time of BentoML 1.0\n   release, so `bentoml.tensorflow` is recommended in production\n   environments. You can read :doc:`bentoml.tensorflow\n   </frameworks/tensorflow>` documentation for more information.\nYou can also convert a Keras model to ONNX model and use\n   `bentoml.onnx` to serve in production. Refer\n   :doc:`bentoml.onnx documentation </frameworks/onnx>` and\n   `tensorflow-onnx (tf2onnx) documentation\n   <https://github.com/onnx/tensorflow-onnx>`_ for more information.\nCompatibility\nBentoML requires TensorFlow version 2.7.3 or higher to be installed.\nSaving a Keras Model\nThe following example loads a pre-trained ResNet50 model.\n.. code-block:: python\nimport tensorflow as tf\n   from tensorflow.keras.applications.resnet50 import ResNet50\n# Use pre-trained ResNet50 weights\n   model = ResNet50(weights='imagenet')\n# try a sample input with created model\n   from tensorflow.keras.preprocessing import image\n   from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\nimg_path = 'ade20k.jpg'\nimg = image.load_img(img_path, target_size=(224, 224))\nx = image.img_to_array(img)\n   x = np.expand_dims(x, axis=0)\n   x = preprocess_input(x)\npreds = model.predict(x)\n   print('Keras Predicted:', decode_predictions(preds, top=3)[0])\n# output:\n   # Keras Predicted: [('n04285008', 'sports_car', 0.3447785)]\nAfter the Keras model is ready, use :obj:`~bentoml.keras.save_model`\nto save the model instance to BentoML model store.\n.. code-block:: python\nbentoml.keras.save_model(\"keras_resnet50\", model)\nKeras model can be loaded with :obj:`~bentoml.keras.load_model` to \nverify that the saved model can be loaded properly.\n.. code-block:: python\nmodel = bentoml.keras.load_model(\"keras_resnet50:latest\")\nprint(decode_predictions(model.predict(x)))\nBuilding a Service using Keras\n.. seealso::\nSee :ref:`Building a Service <concepts/service:Service and APIs>` for more \n   information on creating a prediction service with BentoML.\nThe following service example creates a `predict` API endpoint that accepts an image as input \nand return JSON data as output. Within the API function, Keras model runner created from the \npreviously saved ResNet50 model is used for inference.\n.. code-block:: python\nimport bentoml\nimport numpy as np\n   from bentoml.io import Image\n   from bentoml.io import JSON\nrunner = bentoml.keras.get(\"keras_resnet50:latest\").to_runner()\nsvc = bentoml.Service(\"keras_resnet50\", runners=[runner])\n@svc.api(input=Image(), output=JSON())\n   async def predict(img):\n\n\n```   from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n\n   img = img.resize((224, 224))\n   arr = np.array(img)\n   arr = np.expand_dims(arr, axis=0)\n   arr = preprocess_input(arr)\n   preds = await runner.async_run(arr)\n   return decode_predictions(preds, top=1)[0]\n```\n\n\nWhen constructing a :ref:`bentofile.yaml <concepts/bento:Bento Build\nOptions>`, there are two ways to include Keras as a dependency, via\n`python` (if using pip) or `conda`:\n.. tab-set::\n.. tab-item:: python\n\n\n```  .. code-block:: yaml\n\n python:\n   packages:\n     - tensorflow\n```\n\n\n.. tab-item:: conda\n\n\n```  .. code-block:: yaml\n\n     conda:\n       channels:\n       - conda-forge\n       dependencies:\n       - tensorflow\n```\n\n\nUsing Runners\n.. seealso::\nSee :ref:`concepts/runner:Using Runners` doc for a general introduction to the Runner concept and its usage.\n`runner.predict.run` is generally a drop-in replacement for\n`model.predict` for executing the prediction in the model\nrunner. When `predict` is the only prediction method exposed by\nrunner model, you can just use `runner.run` instead of",
    "tag": "bentoml"
  },
  {
    "title": "pytorch_lightning.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/frameworks/pytorch_lightning.rst",
    "content": "=================\nPyTorch Lightning\n=================\nHere's a simple example of using PyTorch Lightning with BentoML:\n.. code:: python\n\n\n```import bentoml\nimport torch\nimport pytorch_lightning as pl\n\nclass AdditionModel(pl.LightningModule):\n    def forward(self, inputs):\n        return inputs.add(1)\n\n# `save` a given classifier and retrieve coresponding tag:\ntag = bentoml.pytorch_lightning.save_model(\"addition_model\", AdditionModel())\n\n# retrieve metadata with `bentoml.models.get`:\nmetadata = bentoml.models.get(tag)\n\n# `load` the model back in memory:\nmodel = bentoml.pytorch_lightning.load_model(\"addition_model:latest\")\n\n# Run a given model under `Runner` abstraction with `to_runner`\nrunner = bentoml.pytorch_lightning.get(tag).to_runner()\nrunner.init_local()\nrunner.run(torch.from_numpy(np.array([[1,2,3,4]])))\n```\n\n\n.. note::\nYou can find more examples for PyTorch Lightning in our :github:`bentoml/examples <tree/main/examples>` directory.\n.. currentmodule:: bentoml.pytorch_lightning\n.. autofunction:: bentoml.pytorch_lightning.save_model\n.. autofunction:: bentoml.pytorch_lightning.load_model\n.. autofunction:: bentoml.pytorch_lightning.get",
    "tag": "bentoml"
  },
  {
    "title": "transformers.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/frameworks/transformers.rst",
    "content": "============\nTransformers\n============\n`\ud83e\udd17 Transformers <https://huggingface.co/docs/transformers/main/en/index>`_ is a library that helps download and fine-tune popular \npretrained models for common machine learning tasks. BentoML provides native support for serving and deploying models trained from \nTransformers.\nCompatibility\nBentoML requires Transformers version 4 or above. For other versions of Transformers, consider using a \n:ref:`concepts/runner:Custom Runner`.\nWhen constructing a :ref:`bentofile.yaml <concepts/bento:Bento Build Options>`, include `transformers` and the machine learning \nframework of the model, e.g. `pytorch`, `tensorflow`, or `jax`.\n.. tab-set::\n.. tab-item:: PyTorch\n\n\n```  .. code-block:: yaml\n     :caption: `bentofile.yaml`\n\n     service: \"service.py:svc\"\n     labels:\n     owner: bentoml-team\n     project: gallery\n     include:\n     - \"*.py\"\n     python:\n       packages:\n       - transformers\n       - torch\n```\n\n\n.. tab-item:: TensorFlow\n\n\n```  .. code-block:: yaml\n      :caption: `bentofile.yaml`\n\n      service: \"service.py:svc\"\n      labels:\n      owner: bentoml-team\n      project: gallery\n      include:\n      - \"*.py\"\n      python:\n        packages:\n        - transformers\n        - tensorflow\n```\n\n\nFined-tuned Models\nFine-tuning pretrained models is a powerful practice that allows users to save computation cost and adapt state-of-the-art models to their \ndomain specific dataset. Transformers offers a variety of libraries for fine-tuning pretrained models. The example below fine-tunes a BERT \nmodel with Yelp review dataset. To learn more, refer to the Transformers guide on \n`fine-tuning pretrained models <https://huggingface.co/docs/transformers/main/en/training>`_.\n.. code-block:: python\n    :caption: `train.py`\n\n\n```from datasets import load_dataset\nfrom transformers import AutoModelForMaskedLM, AutoTokenizer, Trainer, TrainingArguments\n\ndataset = load_dataset(\"yelp_review_full\")\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\nsmall_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n\nmodel = AutoModelForMaskedLM.from_pretrained(\"bert-base-cased\", num_labels=5)\n\ntraining_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n)\n\ntrainer.train()\n```\n\n\nSaving a Fine-tuned Model\n~~~~~~~~~~~~~~~~~~~~~~~~~\nOnce the model is fine-tuned, create a Transformers \n`Pipeline <https://huggingface.co/docs/transformers/main/en/pipeline_tutorial>`_ with the model and save to the BentoML model \nstore. By design, only Pipelines can be saved with the BentoML Transformers framework APIs. Models, tokenizers, feature extractors, \nand processors, need to be a part of the pipeline first before they can be saved. Transformers pipelines are callable objects therefore \nthe signatures of the model are saved as :code:`__call__` by default.\n.. code-block:: python\n    :caption: `train.py`\n\n\n```import bentoml\nfrom transformers import pipeline\n\nunmasker = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n\nbentoml.transformers.save_model(name=\"unmasker\", pipeline=unmasker)\n```\n\n\nTo load the model for testing and debugging, use :code:`bentoml.transformers.load_model` with the :code:`unmasker:latest` tag.\nServing a Fined-tuned Model\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\nCreate a BentoML service with the previously saved `unmasker` pipeline using the Transformers framework APIs.\n.. seealso::\nSee :ref:`Building a Service <concepts/service:Service and APIs>` to learn more on creating a prediction service with BentoML.\n.. code-block:: python\n    :caption: `service.py`\n\n\n```import bentoml\n\nfrom bentoml.io import Text, JSON\n\nrunner = bentoml.transformers.get(\"unmasker:latest\").to_runner()\n\nsvc = bentoml.Service(\"unmasker_service\", runners=[runner])\n\n@svc.api(input=Text(), output=JSON())\nasync def unmask(input_series: str) -> list:\n    return await runner.async_run(input_series)\n```\n\n\nPretrained Models\nUsing pretrained models from the Hugging Face does not require saving the model first in the BentoML model store. A custom runner \ncan be implemented to download and run pretrained models at runtime.\n.. seealso::\nSee :ref:`Custom Runner <concepts/runner:Custom Runner>` to learn more.\nServing a Pretrained Model\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n.. code-block:: python\n    :caption: `service.py`\n\n\n```import bentoml\n\nfrom bentoml.io import Text, JSON\nfrom transformers import pipeline\n\nclass PretrainedModelRunnable(bentoml.Runnable):\n    SUPPORTED_RESOURCES = (\"cpu\",)\n    SUPPORTS_CPU_MULTI_THREADING = True\n\n    def __init__(self):\n        self.unmasker = pipeline(task=\"fill-mask\", model=\"distilbert-base-uncased\")\n\n    @bentoml.Runnable.method(batchable=False)\n    def __call__(self, input_text):\n        return self.unmasker(input_text)\n\nrunner = bentoml.Runner(PretrainedModelRunnable, name=\"pretrained_unmasker\")\n\nsvc = bentoml.Service('pretrained_unmasker_service', runners=[runner])\n\n@svc.api(input=Text(), output=JSON())\nasync def unmask(input_series: str) -> list:\n    return await runner.async_run(input_series)\n```\n\n\nCustom Pipelines\nTransformers custom pipelines allow users to define their own pre and post-process logic and customize how input data is forwarded to \nthe model for inference.\n.. seealso::\n\n\n````How to add a pipeline <https://huggingface.co/docs/transformers/main/en/add_new_pipeline>`_ from Hugging Face to learn more.\n```\n\n\n.. code-block:: python\n    :caption: `train.py`\n\n\n```from transformers import Pipeline\n\nclass MyClassificationPipeline(Pipeline):\n    def _sanitize_parameters(self, **kwargs):\n        preprocess_kwargs = {}\n        if \"maybe_arg\" in kwargs:\n            preprocess_kwargs[\"maybe_arg\"] = kwargs[\"maybe_arg\"]\n        return preprocess_kwargs, {}, {}\n\n    def preprocess(self, text, maybe_arg=2):\n        input_ids = self.tokenizer(text, return_tensors=\"pt\")\n        return input_ids\n\n    def _forward(self, model_inputs):\n        outputs = self.model(**model_inputs)\n        return outputs\n\n    def postprocess(self, model_outputs):\n        return model_outputs[\"logits\"].softmax(-1).numpy()\n```\n\n\nSaving a Custom Pipeline\n~~~~~~~~~~~~~~~~~~~~~~~~\nA custom pipeline first needs to be added to the Transformers supported tasks, :code:`SUPPORTED_TASKS` before it can be created with \nthe Transformers :code:`pipeline` API.\n.. code-block:: python\n    :caption: `train.py`\n\n\n```from transformers import pipeline\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers.pipelines import SUPPORTED_TASKS\n\nTASK_NAME = \"my-classification-task\"\nTASK_DEFINITION = {\n    \"impl\": MyClassificationPipeline,\n    \"tf\": (),\n    \"pt\": (AutoModelForSequenceClassification,),\n    \"default\": {},\n    \"type\": \"text\",\n}\nSUPPORTED_TASKS[TASK_NAME] = TASK_DEFINITION\n\nclassifier = pipeline(\n    task=TASK_NAME,\n    model=AutoModelForSequenceClassification.from_pretrained(\n        \"distilbert-base-uncased-finetuned-sst-2-english\"\n    ),\n    tokenizer=AutoTokenizer.from_pretrained(\n        \"distilbert-base-uncased-finetuned-sst-2-english\"\n    ),\n)\n```\n\n\nOnce a new pipeline is added to the Transformers supported tasks, it can be saved to the BentoML model store with the additional \narguments of :code:`task_name` and :code:`task_definition`, the same arguments that were added to the Transformers :code:`SUPPORTED_TASKS` \nwhen creating the pipeline. :code:`task_name` and :code:`task_definition` will be saved as model options alongside the model.\n.. code-block:: python\n   :caption: `train.py`\n\n\n```import bentoml\n\nbentoml.transformers.save_model(\n    \"my_classification_model\",\n    pipeline=classifier,\n    task_name=TASK_NAME,\n    task_definition=TASK_DEFINITION,\n)\n```\n\n\nServing a Custom Pipeline\n~~~~~~~~~~~~~~~~~~~~~~~~~\nTo serve a custom pipeline, simply create a runner and service with the previously saved pipeline. :code:`task_name` and \n:code:`task_definition` will be automatically applied when initializing the runner.\n.. code-block:: python\n    :caption: `service.py`\n\n\n```import bentoml\n\nfrom bentoml.io import Text, JSON\n\nrunner = bentoml.transformers.get(\"my_classification_model:latest\").to_runner()\n\nsvc = bentoml.Service(\"my_classification_service\", runners=[runner])\n\n@svc.api(input=Text(), output=JSON())\nasync def classify(input_series: str) -> list:\n    return await runner.async_run(input_series)\n```\n\n\nAdaptive Batching\nIf the model supports batched interence, it is recommended to enable batching to take advantage of the adaptive batching capability \nin BentoML by overriding the :code:`signatures` argument with the method name (:code:`__call__`), :code:`batchable`, and :code:`batch_dim` \nconfigurations when saving the model to the model store . \n.. seealso::\nSee :ref:`Adaptive Batching <guides/batching:Adaptive Batching>` to learn more.\n.. code-block:: python\n    :caption: `train.py`\n\n\n```import bentoml\n\nbentoml.transformers.save_model(\n    name=\"unmasker\",\n    pipeline=unmasker,\n    signatures={\n        \"__call__\": {\n            \"batchable\": True,\n            \"batch_dim\": 0,\n        },\n    },\n)\n```\n\n\n.. Serving on GPU\n.. --------------\n.. BentoML Transformers framework will enable inference on GPU if the hardware is available.\n.. .. seealso::",
    "tag": "bentoml"
  },
  {
    "title": "sklearn.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/frameworks/sklearn.rst",
    "content": "============\nScikit-Learn\n============\nBelow is a simple example of using scikit-learn with BentoML:\n.. code:: python\n\n\n```import bentoml\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.neighbors import KNeighborsClassifier\n\nmodel = KNeighborsClassifier()\niris = load_iris()\nX = iris.data[:, :4]\nY = iris.target\nmodel.fit(X, Y)\n\n# `save` a given classifier and retrieve coresponding tag:\ntag = bentoml.sklearn.save_model('kneighbors', model)\n\n# retrieve metadata with `bentoml.models.get`:\nmetadata = bentoml.models.get(tag)\n\n# load the model back:\nloaded = bentoml.sklearn.load_model(\"kneighbors:latest\")\n\n# Run a given model under `Runner` abstraction with `to_runner`\nrunner = bentoml.sklearn.get(tag).to_runner()\nrunner.init_local()\nrunner.run([[1,2,3,4,5]])\n```\n\n\n.. note::",
    "tag": "bentoml"
  },
  {
    "title": "lightgbm.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/frameworks/lightgbm.rst",
    "content": "========\nLightGBM\n========\nUsers can now use LightGBM with BentoML with the following API: :code:`load_model`,\n:code:`save_model`, and :code:`get` as follow:\n.. code-block:: python\nimport bentoml\n   import lightgbm as lgb\n   import pandas as pd\n# load a dataset\n   df_train = pd.read_csv(\"regression.train\", header=None, sep=\"\\t\")\n   df_test = pd.read_csv(\"regression.test\", header=None, sep=\"\\t\")\ny_train = df_train[0]\n   y_test = df_test[0]\n   X_train = df_train.drop(0, axis=1)\n   X_test = df_test.drop(0, axis=1)\n# create dataset for lightgbm\n   lgb_train = lgb.Dataset(X_train, y_train)\n   lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n# specify your configurations as a dict\n   params = {\n      \"boosting_type\": \"gbdt\",\n      \"objective\": \"regression\",\n      \"metric\": {\"l2\", \"l1\"},\n      \"num_leaves\": 31,\n      \"learning_rate\": 0.05,\n   }\n# train\n   gbm = lgb.train(\n      params, lgb_train, num_boost_round=20, valid_sets=lgb_eval\n   )\n# `save` a given classifier and retrieve coresponding tag:\n   bentoml.lightgbm.save_model(\"my_lightgbm_model\", gbm, booster_params=params)\n# retrieve metadata with `bentoml.models.get`:\n   bento_model = bentoml.models.get(\"my_lightgbm_model:latest\")\n# `load` the model back in memory:\n   loaded_model = bentoml.lightgbm.load_model(\"my_lightgbm_model\")\n# Run a given model under `Runner` abstraction with `to_runner`\n   input_data = pd.from_csv(\"/path/to/csv\")\n   runner = bentoml.lightgbm.get(\"my_lightgbm_model:latest\").to_runner()\n   runner.init_local()\n   runner.run(input_data)\n.. note::\nYou can find more examples for LightGBM in our `bentoml/examples <https://github.com/bentoml/BentoML/tree/main/examples>`_ repo.",
    "tag": "bentoml"
  },
  {
    "title": "pytorch.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/frameworks/pytorch.rst",
    "content": "=======\nPyTorch\n=======\nBentoML provides native support for serving and deploying models trained from PyTorch. For more in-depth tutorials about PyTorch, please visit `PyTorch's official documentation <https://pytorch.org/tutorials/>`_\nPreface\nIf you have already compiled your PyTorch model to TorchScript, you might consider to use :doc:`bentoml.torchscript </reference/frameworks/torchscript>`. BentoML provides first-class support for TorchScript, hence using `bentoml.torchscript` is less prone to compatibility issues during production.\n.. note::\n\n\n```:bdg-info:`Remarks:` We recommend users to apply model optimization techniques such as `distillation <https://arxiv.org/abs/1503.02531>`_ or `quantization <https://pytorch.org/docs/stable/quantization.html#general-quantization-flow>`_ . Alternatively, PyTorch models can also be converted to :doc:`/frameworks/onnx` models and leverage different runtimes (e.g. TensorRT, Apache TVM, etc.) for better performance.\n```\n\n\nSaving a Trained Model\nFor common PyTorch models with single input:\n.. code-block:: python\n    :caption: `train.py`\n\n\n```import bentoml\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\n\ntransform = transforms.Compose(\n[transforms.ToTensor(),\n transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\nbatch_size = 4\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                         shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\nmodel = Net()\n\nimport torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\nfor epoch in range(2):  # a small epoch just for demostration purpose\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs\n        inputs, labels = data\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        print('Epoch: %d, Step: %d, Loss: %.4f' % (epoch, i, loss.item()))\n\nbentoml.pytorch.save(\n    model,\n    \"my_torch_model\",\n    signatures={\"__call__\": {\"batchable\": True, \"batch_dim\": 0}},\n)\n```\n\n\n`bentoml.pytorch` also supports saving models that take multiple tensors as input:\n.. code-block:: python\n    :caption: `train.py`\n\n\n```import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x, y):\n        return x + y\n\n\nmodel = Net()\n... # training\n\nbentoml.pytorch.save(\n    model,\n    \"my_torch_model\",\n    signatures={\"__call__\": {\"batchable\": True, \"batch_dim\": 0}},\n)\n```\n\n\n.. note::\n\n\n```:bdg-info:`Remarks:` External python classes or utility functions required by the model must be referenced in ``<module>.<class>`` format, and such modules should be passed to ``bentoml.pytorch.save`` via ``external_modules``. For example:\n\n.. code-block:: python\n   :caption: `train.py`\n   :emphasize-lines: 1,8\n\n   import my_models\n\n   model = my_models.MyModel()\n   bentoml.pytorch.save(\n       model,\n       \"my_torch_model\",\n       signatures={\"__call__\": {\"batchable\": True, \"batch_dim\": 0}},\n       external_modules=[my_models],\n   )\n\nThis is due to a limitation from PyTorch model serialisation, where PyTorch requires the model's source code to restore it.\n\nA better practice is to compile your model to `TorchScript <https://pytorch.org/docs/stable/jit.html>`_ format.\n```\n\n\n.. note::\n\n\n```:code:`bentoml.pytorch.save_model` has parameter ``signatures``.\nThe ``signatures`` argument of type :ref:`Model Signatures <concepts/model:Model Signatures>` in :obj:`bentoml.pytorch.save_model` is used to determine which methods will be used for inference and exposed in the Runner. The signatures dictionary will then be used during the creation process of a Runner instance.\n```\n\n\nThe signatures used for creating a Runner is `{\"__call__\": {\"batchable\": False}}`. This means by default, BentoML\u2019s `Adaptive Batching <guides/batching:Adaptive Batching>`_ is disabled when using :obj:`~bentoml.pytorch.save_model()`. If you want to utilize adaptive batching behavior and know your model's dynamic batching dimension, make sure to pass in `signatures` as follow: \n.. code-block:: python\n\n\n```bentoml.pytorch.save(model, \"my_model\", signatures={\"__call__\": {\"batch_dim\": 0, \"batchable\": True}})\n```\n\n\nBuilding a Service\nCreate a BentoML service with the previously saved `my_torch_model` pipeline using the :code:`bentoml.pytorch` framework APIs.\n.. code-block:: python\n    :caption: `service.py`\n\n\n```runner = bentoml.pytorch.get(\"my_torch_model\").to_runner()\n\nsvc = bentoml.Service(name=\"test_service\", runners=[runner])\n\n@svc.api(input=JSON(), output=JSON())\nasync def predict(json_obj: JSONSerializable) -> JSONSerializable:\n    batch_ret = await runner.async_run([json_obj])\n    return batch_ret[0]\n```\n\n\n.. note::\n\n\n```Follow the steps to get the best performance out of your PyTorch model.\n#. Apply adaptive batching if possible.\n#. Serve on GPUs if applicable.\n#. See performance guide from `PyTorch Model Opt Doc <https://pytorch.org/tutorials/beginner/profiler.html>`_\n```\n\n\nAdaptive Batching\nMost PyTorch models can accept batched data as input. If batched interence is supported, it is recommended to enable batching to take advantage of \nthe adaptive batching capability to improve the throughput and efficiency of the model. Enable adaptive batching by overriding the :code:`signatures` \nargument with the method name and providing :code:`batchable` and :code:`batch_dim` configurations when saving the model to the model store.\n.. seealso::\nSee :ref:`Adaptive Batching <guides/batching:Adaptive Batching>` to learn more.\n.. note::\nYou can find more examples for PyTorch in our `bentoml/examples <https://github.com/bentoml/BentoML/tree/main/examples>`_ directory.",
    "tag": "bentoml"
  },
  {
    "title": "onnx.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/frameworks/onnx.rst",
    "content": "====\nONNX\n====\nONNX is an open format built to represent machine learning models. ONNX provides `high interoperability <https://onnx.ai/supported-tools.html#buildModel>`  among various frameworks, as well as enable machine learning practitioners to maximize models' performance across `different hardware <https://onnx.ai/supported-tools.html#deployModel>`.\nDue to its high interoperability among frameworks, we recommend you to check out the framework integration with ONNX as it will contain specific recommendation and requirements for that given framework.\n.. tab-set::\n.. tab-item:: PyTorch\n      :sync: pytorch\n\n\n```  - `Quick tutorial about exporting a model from PyTorch to ONNX <https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html>`_ from official PyTorch documentation.\n  - `torch.onnx <https://pytorch.org/docs/stable/onnx.html>`_ from official PyTorch documentation. Pay special attention to section **Avoiding Pitfalls**, **Limitations** and **Frequently Asked Questions**.\n```\n\n\n.. tab-item:: TensorFlow\n      :sync: tensorflow\n\n\n```  - `tensorflow-onnx (tf2onnx) <https://github.com/onnx/tensorflow-onnx>`_ documentation.\n```\n\n\n.. tab-item:: scikit-learn\n      :sync: sklearn\n\n\n```  - `sklearn-onnx (skl2onnx) <https://onnx.ai/sklearn-onnx/>`_ documentation.\n```\n\n\nCompatibility\nBentoML currently only support `ONNX Runtime\n<https://onnxruntime.ai>`_ as ONNX backend. BentoML requires either\n`onnxruntime>=1.9` or `onnxruntime-gpu>=1.9` to be installed.\nConverting model frameworks to ONNX format\n.. tab-set::\n.. tab-item:: PyTorch\n      :sync: pytorch\n\n\n```  First, let\u2019s create a SuperResolution model in PyTorch.\n\n  .. code-block:: python\n :caption: `train.py`\n\n import torch.nn as nn\n import torch.nn.init as init\n\n class SuperResolutionNet(nn.Module):\n     def __init__(self, upscale_factor, inplace=False):\n     super(SuperResolutionNet, self).__init__()\n\n     self.relu = nn.ReLU(inplace=inplace)\n     self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2))\n     self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1))\n     self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1))\n     self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1))\n     self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n\n     self._initialize_weights()\n\n     def forward(self, x):\n     x = self.relu(self.conv1(x))\n     x = self.relu(self.conv2(x))\n     x = self.relu(self.conv3(x))\n     x = self.pixel_shuffle(self.conv4(x))\n     return x\n\n     def _initialize_weights(self):\n     init.orthogonal_(self.conv1.weight, init.calculate_gain('relu'))\n     init.orthogonal_(self.conv2.weight, init.calculate_gain('relu'))\n     init.orthogonal_(self.conv3.weight, init.calculate_gain('relu'))\n     init.orthogonal_(self.conv4.weight)\n\n torch_model = SuperResolutionNet(upscale_factor=3)\n\n  For this tutorial, we will use pre-trained weights provided by the PyTorch team. Note that the model was only partially trained and being used for demonstration purposes.\n\n  .. code-block:: python\n :caption: `train.py`\n\n import torch.utils.model_zoo as model_zoo\n\n # Load pretrained model weights\n model_url = 'https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth'\n\n # Initialize model with the pretrained weights\n map_location = lambda storage, loc: storage\n if torch.cuda.is_available():\n     map_location = None\n torch_model.load_state_dict(model_zoo.load_url(model_url, map_location=map_location))\n\n # set the model to inference mode\n torch_model.eval()\n\n\n  Exporting a model to ONNX in PyTorch works via tracing or\n  scripting (read more at `official PyTorch documentation\n  <https://pytorch.org/docs/stable/onnx.html#tracing-vs-scripting>`_). In\n  this tutorial we will export the model using tracing techniques:\n\n  .. code-block:: python\n :caption: `train.py`\n\n batch_size = 1\n # Tracing input to the model\n x = torch.randn(batch_size, 1, 224, 224, requires_grad=True)\n\n # Export the model\n torch.onnx.export(\n    torch_model,\n    x,\n    \"super_resolution.onnx\",  # where to save the model (can be a file or file-like object)\n    export_params=True,  # store the trained parameter weights inside the model file\n    opset_version=10,  # the ONNX version to export the model to\n    do_constant_folding=True,  # whether to execute constant folding for optimization\n    input_names=[\"input\"],  # the model's input names\n    output_names=[\"output\"],  # the model's output names\n    dynamic_axes={\n       \"input\": {0: \"batch_size\"},  # variable length axes\n       \"output\": {0: \"batch_size\"},\n    },\n )\n\n  Notice from the arguments of ``torch.onnx.export()``, even though we are exporting the model\n  with an input of ``batch_size=1``, the first dimension is still specified as dynamic in ``dynamic_axes``\n  parameter. By doing so, the exported model will accept inputs of size ``[batch_size, 1, 224, 224]`` where\n  ``batch_size`` can vary among inferences.\n\n  We can now compute the output using ONNX Runtime\u2019s Python APIs:\n\n  .. code-block:: python\n\n import onnxruntime\n\n ort_session = onnxruntime.InferenceSession(\"super_resolution.onnx\")\n # compute ONNX Runtime output prediction\n ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\n # ONNX Runtime will return a list of outputs\n ort_outs = ort_session.run(None, ort_inputs)\n print(ort_outs[0])\n```\n\n\n.. tab-item:: TensorFlow\n      :sync: tensorflow\n\n\n```  First let's install `tf2onnx <https://github.com/onnx/tensorflow-onnx>`_\n\n  .. code-block:: bash\n\n pip install tf2onnx\n\n  For this tutorial we will download a pretrained ResNet-50 model:\n\n  .. code-block:: python\n :caption: `train.py`\n\n import tensorflow as tf\n from tensorflow.keras.applications.resnet50 import ResNet50\n\n model = ResNet50(weights='imagenet')\n\n  Notice that we use ``None`` in `TensorSpec <https://www.tensorflow.org/api_docs/python/tf/TensorSpec>`_ to\n  denote the first input dimension as dynamic batch axies, which\n  means this dimension can accept any arbitrary input size:\n\n  .. code-block:: python\n :caption: `train.py`\n\n spec = (tf.TensorSpec((None, 224, 224, 3), tf.float32, name=\"input\"),)\n onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature=spec, opset=13)\n```\n\n\n.. tab-item:: scikit-learn\n      :sync: sklearn\n\n\n```  First let's install `sklearn-onnx <https://onnx.ai/sklearn-onnx/>`_\n\n  .. code-block:: bash\n\n pip install skl2onnx\n\n  For this tutorial we will train a random forest classifier on\n  Iris Data set:\n\n  .. code-block:: python\n :caption: `train.py`\n\n from sklearn.datasets import load_iris\n from sklearn.model_selection import train_test_split\n from sklearn.ensemble import RandomForestClassifier\n\n iris = load_iris()\n X, y = iris.data, iris.target\n X_train, X_test, y_train, y_test = train_test_split(X, y)\n clr = RandomForestClassifier()\n clr.fit(X_train, y_train)\n\n  Then we can use ``skl2onnx`` to export a scikit-learn model to\n  ONNX format:\n\n  .. code-block:: python\n :caption: `train.py`\n\n import skl2onnx\n\n from skl2onnx import convert_sklearn\n from skl2onnx.common.data_types import FloatTensorType\n initial_type = [('float_input', FloatTensorType([None, 4]))]\n model_proto = convert_sklearn(clr, initial_types=initial_type)\n\n  Notice that we use ``None`` in ``initial_type`` to denote the\n  first input dimension as dynamic batch axies, which means this\n  dimension can accept arbitrary input size:\n```\n\n\nSaving ONNX model with BentoML\nTo quickly save any given ONNX model to BentoML's :ref:`Model\nStore<concepts/model:Managing Models>`, use `onnx.load` to\nload the exported ONNX model back into the Python session,\nthen call BentoML's :obj:`~bentoml.onnx.save_model()`:\n.. tab-set::\n.. tab-item:: PyTorch\n      :sync: pytorch\n\n\n```  .. code-block:: python\n :caption: `train.py`\n\n signatures = {\n     \"run\": {\"batchable\": True},\n }\n bentoml.onnx.save_model(\"onnx_super_resolution\", onnx_model, signatures=signatures)\n\n  which will result:\n\n  .. code-block:: bash\n\n Model(tag=\"onnx_super_resolution:lwqr7ah5ocv3rea3\", path=\"~/bentoml/models/onnx_super_resolution/lwqr7ah5ocv3rea3/\")\n```\n\n\n.. tab-item:: TensorFlow\n      :sync: tensorflow\n\n\n```  .. code-block:: python\n :caption: `train.py`\n\n signatures = {\n     \"run\": {\"batchable\": True},\n }\n bentoml.onnx.save_model(\"onnx_resnet50\", onnx_model, signatures=signatures)\n\n  which will result:\n\n  .. code-block:: bash\n\n Model(tag=\"onnx_resnet50:zavavxh6w2v3rea3\", path=\"~/bentoml/models/onnx_resnet50/zavavxh6w2v3rea3/\")\n```\n\n\n.. tab-item:: scikit-learn\n      :sync: sklearn\n\n\n```  .. code-block:: python\n :caption: `train.py`\n\n signatures = {\n     \"run\": {\"batchable\": True},\n }\n bentoml.onnx.save_model(\"onnx_iris\", model_proto, signatures=signatures)\n\n  which will result:\n\n  .. code-block:: bash\n\n Model(tag=\"onnx_iris:sqixlaqf76vv7ea3\", path=\"~/bentoml/models/onnx_iris/sqixlaqf76vv7ea3/\")\n```\n\n\nThe default signature for :obj:`~bentoml.onnx.save_model()` is set to `{\"run\": {\"batchable\": False}}`.\nThis means by default, BentoML's :ref:`guides/batching:Adaptive Batching` is disabled when saving ONNX model.\nIf you want to enable adaptive batching, provide a signature similar to the\naboved example.\nRefer to :ref:`concepts/model:Model Signatures` and :ref:`Batching behaviour <concepts/model:Batching>` for more information.\n.. note::\nBentoML internally use `onnxruntime.InferenceSession\n   <https://onnxruntime.ai/docs/api/python/api_summary.html#inferencesession>`_\n   to run inference. When the original model is converted to ONNX\n   format and loaded by `onnxruntime.InferenceSession`, the\n   inference method of the original model is converted to the `run`\n   method of the `onnxruntime.InferenceSession`. `signatures` in\n   above codes refers to the predict method of\n   `onnxruntime.InferenceSession`, hence the only allowed method\n   name in `signatures` is `run`.\nBuilding a Service for ONNX\n.. seealso::\n:ref:`Building a Service <concepts/service:Service and APIs>` for how to\n   create a prediction service with BentoML.\n.. tab-set::\n.. tab-item:: PyTorch\n      :sync: pytorch\n\n\n```  .. code-block:: python\n :caption: `service.py`\n\n import bentoml\n\n import numpy as np\n from PIL import Image as PIL_Image\n from PIL import ImageOps\n from bentoml.io import Image\n\n runner = bentoml.onnx.get(\"onnx_super_resolution:latest\").to_runner()\n\n svc = bentoml.Service(\"onnx_super_resolution\", runners=[runner])\n\n # for output, we set image io descriptor's pilmode to \"L\" to denote\n # the output is a gray scale image\n @svc.api(input=Image(), output=Image(pilmode=\"L\"))\n async def sr(img) -> np.ndarray:\n     img = img.resize((224, 224))\n     gray_img = ImageOps.grayscale(img)\n     arr = np.array(gray_img) / 255.0  # convert from 0-255 range to 0.0-1.0 range\n     arr = np.expand_dims(arr, (0, 1))  # add batch_size, color_channel dims\n     sr_arr = await runner.run.async_run(arr)\n     sr_arr = np.squeeze(sr_arr)  # remove batch_size, color_channel dims\n     sr_arr = np.uint8(sr_arr * 255)\n     return sr_arr\n```\n\n\n.. tab-item:: TensorFlow\n      :sync: tensorflow\n\n\n```  .. code-block:: python\n :caption: `service.py`\n\n import bentoml\n\n import numpy as np\n from bentoml.io import Image\n from bentoml.io import JSON\n\n runner = bentoml.onnx.get(\"onnx_resnet50:latest\").to_runner()\n\n svc = bentoml.Service(\"onnx_resnet50\", runners=[runner])\n\n @svc.api(input=Image(), output=JSON())\n async def predict(img):\n\n     from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n\n     img = img.resize((224, 224))\n     arr = np.array(img)\n     arr = np.expand_dims(arr, axis=0)\n     arr = preprocess_input(arr)\n     preds = await runner.run.async_run(arr)\n     return decode_predictions(preds, top=1)[0]\n```\n\n\n.. tab-item:: scikit-learn\n      :sync: sklearn\n\n\n```  .. code-block:: python\n :caption: `service.py`\n\n import bentoml\n\n from bentoml.io import JSON\n from bentoml.io import NumpyNdarray\n\n runner = bentoml.onnx.get(\"onnx_iris:latest\").to_runner()\n\n svc = bentoml.Service(\"onnx_iris\", runners=[runner])\n\n @svc.api(input=NumpyNdarray(), output=JSON())\n async def classify(input_array):\n     return await runner.run.async_run(input_array)\n```\n\n\n.. note::\nIn the aboved example, notice there are both `run` and `async_run`  in `runner.run.async_run(input_data)` inside inference code. The distinction between `run` and `async_run` is as follow:\n\nThe `run` refers  to `onnxruntime.InferenceSession <https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/core/session/inference_session.cc>`'s `run` method, which is ONNX Runtime API to run `inference <https://onnxruntime.ai/docs/api/python/api_summary.html#data-inputs-and-outputs>`.\nThe `async_run` refers to BentoML's runner inference API for invoking a model's signature. In the case of ONNX, it happens to have a similar name like the `InferenceSession` endpoint.\n\nWhen constructing a :ref:`bentofile.yaml <concepts/bento:Bento Build\nOptions>`, there are two ways to include ONNX as a dependency, via\n`python` (if using pip) or `conda`:\n.. tab-set::\n.. tab-item:: python\n\n\n```  .. code-block:: yaml\n\n python:\n   packages:\n     - onnx\n     - onnxruntime\n```\n\n\n.. tab-item:: conda\n\n\n```  .. code-block:: yaml\n\n     conda:\n       channels:\n       - conda-forge\n       dependencies:\n       - onnx\n   - onnxruntime\n```\n\n\nUsing Runners\n.. seealso::\n:ref:`Runners<concepts/runner:Using Runners>` for more information on what is\n   a Runner and how to use it.\nTo test ONNX Runner locally, access the model via `get` and\nconvert it to a runner object:\n.. code-block:: python\ntest_input = np.random.randn(2, 1, 244, 244)\nrunner = bentoml.onnx.get(\"onnx_super_resolution\").to_runner()\nrunner.init_local()\nrunner.run.run(test_input)\n.. note::\nYou don't need to cast your input ndarray to `np.float32` for\n   runner input.\nSimilar to `load_model`, you can customize `providers` and `session_options` when creating a runner:\n.. code-block:: python\nproviders=[\"TensorrtExecutionProvider\", \"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\nbento_model = bentoml.onnx.get(\"onnx_super_resolution\")\nrunner = bento_model.with_options(providers=providers).to_runner()\nrunner.init_local()\nLoading an ONNX model with BentoML for local testing\nUse `load_model` to verify that the saved model can be loaded properly:\n.. code-block:: python\nort_session = bentoml.onnx.load_model(\"onnx_super_resolution\")\n.. note::\nBentoML will load an ONNX model back as an\n   `onnxruntime.InferenceSession` object which is ready to do\n   inference\n.. code-block:: python\ntest_input = np.random.randn(2, 1, 244, 244) # can accept arbitrary batch size\n   ort_session.run(None, {\"input\": test_input.astype(np.float32)})\n.. note::\nIn the above snippet, we need explicitly convert input ndarray to\n   float32 since `onnxruntime.InferenceSession` expects only single floats.\nHowever, BentoML will automatically cast the input data automatically via Runners.\nDynamic Batch Size\n.. seealso::\n:ref:`guides/batching:Adaptive Batching`: a general introduction to adaptive batching in BentoML.\nWhen :ref:`guides/batching:Adaptive Batching` is enabled, the exported\nONNX model is REQUIRED to accept dynamic batch size.\nTherefore, dynamic batch axes needs to be specified when the model is exported to the ONNX format.\n.. tab-set::\n.. tab-item:: PyTorch\n      :sync: pytorch\n\n\n```  For PyTorch models, you can achieve this by specifying ``dynamic_axes``\n  when using `torch.onnx.export <https://pytorch.org/docs/stable/onnx.html#torch.onnx.export>`_\n\n  .. code-block:: python\n\n torch.onnx.export(\n    torch_model,\n    x,\n    \"super_resolution.onnx\",  # where to save the model (can be a file or file-like object)\n    export_params=True,  # store the trained parameter weights inside the model file\n    opset_version=10,  # the ONNX version to export the model to\n    do_constant_folding=True,  # whether to execute constant folding for optimization\n    input_names=[\"input\"],  # the model's input names\n    output_names=[\"output\"],  # the model's output names\n    dynamic_axes={\n       \"input\": {0: \"batch_size\"},  # variable length axes\n       \"output\": {0: \"batch_size\"},\n    },\n )\n```\n\n\n.. tab-item:: TensorFlow\n      :sync: tensorflow\n\n\n```  For TensorFlow models, you can achieve this by using ``None`` to denote\n  a dynamic batch axis in `TensorSpec\n  <https://www.tensorflow.org/api_docs/python/tf/TensorSpec>`_ when\n  using either ``tf2onnx.convert.from_keras`` or ``tf2onnx.convert.from_function``\n\n  .. code-block:: python\n\n spec = (tf.TensorSpec((None, 224, 224, 3), tf.float32, name=\"input\"),) # batch_axis = 0\n model_proto, _ = tf2onnx.convert.from_keras(model, input_signature=spec, opset=13)\n```\n\n\n.. tab-item:: scikit-learn\n      :sync: sklearn\n\n\n```  For scikit-learn models, you can achieve this by using ``None``\n  in ``initial_type`` to denote the a dynamic batch axis when\n  using `skl2onnx.convert_sklearn\n  <https://onnx.ai/sklearn-onnx/api_summary.html#converters>`_\n\n  .. code-block:: python\n\n initial_type = [('float_input', FloatTensorType([None, 4]))]\n model_proto = convert_sklearn(clr, initial_types=initial_type)\n```\n\n\nDefault Execution Providers Settings\nWhen a CUDA-compatible GPU is available, BentoML runner will use `[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"]` as the de facto execution providers.\nOtherwise, Runner will use `[\"CPUExecutionProvider\"]` as the default providers.\nIf `onnxruntime-gpu` is installed, using `TensorrtExecutionProvider` may improve inference runtime. You can\noverride the default setting using `with_options` when creating a runner:\n.. code-block:: python\nproviders = [\"TensorrtExecutionProvider\", \"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\nbento_model = bentoml.onnx.get(\"onnx_super_resolution\")\nrunner = bento_model.with_options(providers=providers).to_runner()\n.. seealso::",
    "tag": "bentoml"
  },
  {
    "title": "stores.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/reference/stores.rst",
    "content": "================\nBento Store APIs\n================\nManaging Bentos\n.. autofunction:: bentoml.list\n.. autofunction:: bentoml.get\n.. autofunction:: bentoml.delete\n.. autofunction:: bentoml.export_bento\n.. autofunction:: bentoml.import_bento\nManaging Models\n.. autofunction:: bentoml.models.list\n.. autofunction:: bentoml.models.get\n.. autofunction:: bentoml.models.delete\n.. autofunction:: bentoml.models.export_model\n.. autofunction:: bentoml.models.import_model\nWorking with Yatai\n.. autofunction:: bentoml.push\n.. autofunction:: bentoml.pull\n.. autofunction:: bentoml.models.push",
    "tag": "bentoml"
  },
  {
    "title": "container.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/reference/container.rst",
    "content": "==============\nContainer APIs\n==============\nThe following page includes the Python SDK to build BentoContainer. Refer to :ref:`guides/containerization:Containerization with different container engines.`\nfor more information.\n.. autofunction:: bentoml.container.build\n.. autofunction:: bentoml.container.health\n.. autofunction:: bentoml.container.get_backend\n.. autofunction:: bentoml.container.register_backend\nUtility functions\n~~~~~~~~~~~~~~~~~\nWe also provided some utility functions to help you customize your containerization process.",
    "tag": "bentoml"
  },
  {
    "title": "metrics.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/reference/metrics.rst",
    "content": "===========\nMetrics API\n===========\nBentoML provides metrics API that uses `Prometheus <https://prometheus.io/>`_ under the hood.\nBentoML's `bentoml.metrics` is a drop-in replacement for `prometheus_client` that should be used in BentoML services:\n.. code-block:: diff\ndiff --git a/service.py b/service.py\n   index acd8467e..0f3e6e77 100644\n   --- a/service.py\n   +++ b/service.py\n   @@ -1,11 +1,10 @@\n   -from prometheus_client import Summary\n   +from bentoml.metrics import Summary\n    import random\n    import time\nREQUEST_TIME = Summary(\"request_processing_seconds\", \"Time spent processing request\")\n@REQUEST_TIME.time()\n   def process_request(t):\n       \"\"\"A function that takes some time.\"\"\"\nWhile `bentoml.metrics` contains all API that is offered by `prometheus_client`,\nusers should always use `bentoml.metrics` instead of `prometheus_client` in your service definition.\nThe reason is that BentoML's `bentoml.metrics` will construct metrics lazily and\nensure `multiprocessing mode <https://github.com/prometheus/client_python#multiprocess-mode-eg-gunicorn>`_. are correctly configured.\n.. note::\n`prometheus_client` shouldn't be imported in BentoML services, otherwise it will\n   break multiprocessing mode.\n.. note::\nAll metrics from `bentoml.metrics` will set up `registry` to handle multiprocess mode,\n   which means you SHOULD NOT pass in `registry` argument to metrics initialization:\n.. code-block:: python\n      :caption: service.py\n\n\n```  # THIS WILL NOT WORK\n  from bentoml.metrics import Summary, CollectorRegistry\n  from bentoml.metrics import multiprocess\n\n  registry = CollectorRegistry()\n  multiprocess.MultiProcessCollector(registry)\n  REQUEST_TIME = Summary(\n     \"request_processing_seconds\", \"Time spent processing request\", registry=registry\n  )\n```\n\n\ninstead:\n.. code-block:: python\n      :caption: service.py\n\n\n```  # THIS WILL WORK\n  from bentoml.metrics import Summary\n\n  REQUEST_TIME = Summary(\"request_processing_seconds\", \"Time spent processing request\")\n```\n\n\n\nThe following section will go over the most commonly used metrics API in\n`bentoml.metrics`:\n.. currentmodule:: bentoml.metrics\n.. autofunction:: bentoml.metrics.generate_latest",
    "tag": "bentoml"
  },
  {
    "title": "api_io_descriptors.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/reference/api_io_descriptors.rst",
    "content": "==================\nAPI IO Descriptors\n==================\nIO Descriptors are used for describing the input and output spec of a Service API.\nHere's a list of built-in IO Descriptors and APIs for extending custom IO Descriptor.\nNumPy `ndarray`\n.. note::\nThe :code:`numpy` package is required to use the :obj:`bentoml.io.NumpyNdarray`.\nInstall it with `pip install numpy` and add it to your :code:`bentofile.yaml`'s under either Python or Conda packages list.\nRefer to :ref:`Build Options <concepts/bento:Bento Build Options>`.\n.. tab-set::\n\n\n```  .. tab-item:: pip\n\n     .. code-block:: yaml\n        :caption: `bentofile.yaml`\n\n        ...\n        python:\n          packages:\n            - numpy\n\n  .. tab-item:: conda\n\n     .. code-block:: yaml\n        :caption: `bentofile.yaml`\n\n        ...\n        conda:\n          channels:\n            - conda-forge\n          dependencies:\n            - numpy\n```\n\n\n.. autoclass:: bentoml.io.NumpyNdarray\n.. automethod:: bentoml.io.NumpyNdarray.from_sample\n.. automethod:: bentoml.io.NumpyNdarray.from_proto\n.. automethod:: bentoml.io.NumpyNdarray.from_http_request\n.. automethod:: bentoml.io.NumpyNdarray.to_proto\n.. automethod:: bentoml.io.NumpyNdarray.to_http_response\nTabular Data with Pandas\nTo use the IO descriptor, install bentoml with extra `io-pandas` dependency:\n.. code-block:: bash\n\n\n```pip install \"bentoml[io-pandas]\"\n```\n\n\n.. note::\nThe :code:`pandas` package is required to use the :obj:`bentoml.io.PandasDataFrame`\n   or :obj:`bentoml.io.PandasSeries`. \nInstall it with `pip install pandas` and add it to your :code:`bentofile.yaml`'s under either Python or Conda packages list.\nRefer to :ref:`Build Options <concepts/bento:Bento Build Options>`.\n.. tab-set::\n\n\n```  .. tab-item:: pip\n\n     .. code-block:: yaml\n        :caption: `bentofile.yaml`\n\n        ...\n        python:\n          packages:\n            - pandas\n\n  .. tab-item:: conda\n\n     .. code-block:: yaml\n        :caption: `bentofile.yaml`\n\n        ...\n        conda:\n          channels:\n            - conda-forge\n          dependencies:\n            - pandas\n```\n\n\n.. autoclass:: bentoml.io.PandasDataFrame\n.. automethod:: bentoml.io.PandasDataFrame.from_sample\n.. automethod:: bentoml.io.PandasDataFrame.from_proto\n.. automethod:: bentoml.io.PandasDataFrame.from_http_request\n.. automethod:: bentoml.io.PandasDataFrame.to_proto\n.. automethod:: bentoml.io.PandasDataFrame.to_http_response\n.. autoclass:: bentoml.io.PandasSeries\n.. automethod:: bentoml.io.PandasSeries.from_sample\n.. automethod:: bentoml.io.PandasSeries.from_proto\n.. automethod:: bentoml.io.PandasSeries.from_http_request\n.. automethod:: bentoml.io.PandasSeries.to_proto\n.. automethod:: bentoml.io.PandasSeries.to_http_response\nStructured Data with JSON\n.. note::\nFor common structure data, we recommend using the :obj:`JSON` descriptor, as it provides\n   the most flexibility. Users can also define a schema of the JSON data via a\n   `Pydantic <https://pydantic-docs.helpmanual.io/>`_ model, and use it to for data\n   validation.\nTo use the IO descriptor with pydantic, install bentoml with extra `io-json` dependency:\n.. code-block:: bash\n\n\n```  pip install \"bentoml[io-json]\"\n```\n\n\nThis will include BentoML with `Pydantic <https://pydantic-docs.helpmanual.io/>`_\n   alongside with BentoML\nThen proceed to add it to your :code:`bentofile.yaml`'s under either Python or Conda packages list.\nRefer to :ref:`Build Options <concepts/bento:Bento Build Options>`.\n.. tab-set::\n\n\n```  .. tab-item:: pip\n\n     .. code-block:: yaml\n        :caption: `bentofile.yaml`\n\n        ...\n        python:\n          packages:\n            - pydantic\n\n  .. tab-item:: conda\n\n     .. code-block:: yaml\n        :caption: `bentofile.yaml`\n\n        ...\n        conda:\n          channels:\n            - conda-forge\n          dependencies:\n            - pydantic\n```\n\n\nRefers to :ref:`Build Options <concepts/bento:Bento Build Options>`.\n.. tab-set::\n\n\n```  .. tab-item:: pip\n\n     .. code-block:: yaml\n        :caption: `bentofile.yaml`\n\n        ...\n        python:\n          packages:\n            - pydantic\n\n  .. tab-item:: conda\n\n     .. code-block:: yaml\n        :caption: `bentofile.yaml`\n\n        ...\n        conda:\n          channels:\n            - conda-forge\n          dependencies:\n            - pydantic\n```\n\n\n.. autoclass:: bentoml.io.JSON\n.. automethod:: bentoml.io.JSON.from_sample\n.. automethod:: bentoml.io.JSON.from_proto\n.. automethod:: bentoml.io.JSON.from_http_request\n.. automethod:: bentoml.io.JSON.to_proto\n.. automethod:: bentoml.io.JSON.to_http_response\nTexts\n:code:`bentoml.io.Text` is commonly used for NLP Applications:\n.. autoclass:: bentoml.io.Text\n.. automethod:: bentoml.io.Text.from_proto\n.. automethod:: bentoml.io.Text.from_http_request\n.. automethod:: bentoml.io.Text.to_proto\n.. automethod:: bentoml.io.Text.to_http_response\nImages\nTo use the IO descriptor, install bentoml with extra `io-image` dependency:\n.. code-block:: bash\n\n\n```pip install \"bentoml[io-image]\"\n```\n\n\n.. note::\nThe :code:`Pillow` package is required to use the :obj:`bentoml.io.Image`.\nInstall it with `pip install Pillow` and add it to your :code:`bentofile.yaml`'s under either Python or Conda packages list.\nRefer to :ref:`Build Options <concepts/bento:Bento Build Options>`.\n.. tab-set::\n\n\n```  .. tab-item:: pip\n\n     .. code-block:: yaml\n        :caption: `bentofile.yaml`\n\n        ...\n        python:\n          packages:\n            - Pillow\n\n  .. tab-item:: conda\n\n     .. code-block:: yaml\n        :caption: `bentofile.yaml`\n\n        ...\n        conda:\n          channels:\n            - conda-forge\n          dependencies:\n            - Pillow\n```\n\n\n.. autoclass:: bentoml.io.Image\n.. automethod:: bentoml.io.Image.from_proto\n.. automethod:: bentoml.io.Image.from_http_request\n.. automethod:: bentoml.io.Image.to_proto\n.. automethod:: bentoml.io.Image.to_http_response\nFiles\n.. autoclass:: bentoml.io.File\n.. automethod:: bentoml.io.File.from_proto\n.. automethod:: bentoml.io.File.from_http_request\n.. automethod:: bentoml.io.File.to_proto\n.. automethod:: bentoml.io.File.to_http_response\nMultipart Payloads\n.. note::\n    :code:`io.Multipart` makes it possible to compose a multipart payload from multiple\n    other IO Descriptor instances. For example, you may create a Multipart input that\n    contains a image file and additional metadata in JSON.\n.. autoclass:: bentoml.io.Multipart\n.. automethod:: bentoml.io.Multipart.from_proto\n.. automethod:: bentoml.io.Multipart.from_http_request\n.. automethod:: bentoml.io.Multipart.to_proto\n.. automethod:: bentoml.io.Multipart.to_http_response\nCustom IODescriptor\n.. note::\n    The IODescriptor base class can be extended to support custom data format for your\n    APIs, if the built-in descriptors does not fit your needs.",
    "tag": "bentoml"
  },
  {
    "title": "core.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/reference/core.rst",
    "content": "===============\nCore Components\n===============\nbentoml.Service\n.. autoclass:: bentoml.Service\n    :members: api, runners, apis, mount_asgi_app, mount_wsgi_app, add_asgi_middleware\n    :undoc-members:\n.. autofunction:: bentoml.load\n.. TODO::\n    Add docstring to the following classes/functions\nbentoml.build\n.. autofunction:: bentoml.bentos.build\n.. autofunction:: bentoml.bentos.build_bentofile\n.. autofunction:: bentoml.bentos.containerize\nbentoml.Bento\n.. autoclass:: bentoml.Bento\n    :members: tag, info, path, path_of, doc\n    :undoc-members:\nbentoml.Runner\n.. autoclass:: bentoml.Runner\nbentoml.Runnable\n.. autoclass:: bentoml.Runnable\n    :members: method\n    :undoc-members:\nTag\n.. autoclass:: bentoml.Tag\nModel\n.. autoclass:: bentoml.Model\n    :members: to_runner, to_runnable, info, path, path_of, with_options\n    :undoc-members:\nYataiClient",
    "tag": "bentoml"
  },
  {
    "title": "tensorflow.rst",
    "source": "https://github.com/bentoml/BentoML/tree/main/docs/source/reference/frameworks/tensorflow.rst",
    "content": "==========\nTensorFlow\n==========\n.. admonition:: About this page\nThis is an API reference for TensorFlow in BentoML. Please refer to\n   :doc:`/frameworks/tensorflow` for more information about how to use TensorFlow\n   in BentoML.\n.. note::\nYou can find more examples for TensorFlow in our `bentoml/examples https://github.com/bentoml/BentoML/tree/main/examples`_ directory.\n.. currentmodule:: bentoml.tensorflow\n.. autofunction:: bentoml.tensorflow.save_model\n.. autofunction:: bentoml.tensorflow.load_model",
    "tag": "bentoml"
  }
]